file_path,api_count,code
setup.py,0,"b'import os\n\nfrom setuptools import setup, find_packages\n\ntests_require = [\n    ""pylint==2.4.*"",\n    ""parameterized"",\n    ""nose2""\n]\n\ndef get_long_description():\n    readme_path = os.path.join(os.path.dirname(__file__), ""README.md"")\n    with open(readme_path, encoding=""utf-8"") as readme_file:\n        return readme_file.read()\n\nsetup(\n    name=""OpenNMT-tf"",\n    version=""2.10.1"",\n    license=""MIT"",\n    description=""Neural machine translation and sequence learning using TensorFlow"",\n    long_description=get_long_description(),\n    long_description_content_type=""text/markdown"",\n    author=""OpenNMT"",\n    author_email=""guillaume.klein@systrangroup.com"",\n    url=""https://opennmt.net"",\n    classifiers=[\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.5"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence""\n    ],\n    project_urls={\n        ""Documentation"": ""https://opennmt.net/OpenNMT-tf/"",\n        ""Forum"": ""https://forum.opennmt.net/"",\n        ""Gitter"": ""https://gitter.im/OpenNMT/OpenNMT-tf"",\n        ""Source"": ""https://github.com/OpenNMT/OpenNMT-tf/""\n    },\n    keywords=""tensorflow opennmt nmt neural machine translation"",\n    python_requires=\'>=3.5\',\n    install_requires=[\n        ""ctranslate2>=1.7,<2;platform_system==\'Linux\'"",\n        ""pyonmttok>=1.18.1,<2;platform_system==\'Linux\'"",\n        ""pyyaml>=5.3,<5.4"",\n        ""rouge>=1.0,<2"",\n        ""sacrebleu>=1.4.9,<2"",\n        ""tensorflow>=2.2,<2.3"",\n        ""tensorflow-addons>=0.10,<0.11"",\n        ""pyter3==0.3""\n    ],\n    extras_require={\n        ""tests"": tests_require,\n    },\n    tests_require=tests_require,\n    test_suite=""nose2.collector.collector"",\n    packages=find_packages(exclude=[""bin"", ""*.tests""]),\n    entry_points={\n        ""console_scripts"": [\n            ""onmt-ark-to-records=opennmt.bin.ark_to_records:main"",\n            ""onmt-build-vocab=opennmt.bin.build_vocab:main"",\n            ""onmt-detokenize-text=opennmt.bin.detokenize_text:main"",\n            ""onmt-main=opennmt.bin.main:main"",\n            ""onmt-merge-config=opennmt.bin.merge_config:main"",\n            ""onmt-tokenize-text=opennmt.bin.tokenize_text:main"",\n        ],\n    }\n)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\n\n# General information about the project.\nproject = ""OpenNMT-tf""\nauthor = ""OpenNMT""\nlanguage = ""en""\n\nversion = ""2.10""  # The short X.Y version.\nrelease = ""2.10.1""  # The full version, including alpha/beta/rc tags.\n\nextensions = [\n    ""recommonmark"",\n    ""sphinx_markdown_tables"",\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages""]\n\nsource_suffix = ["".rst"", "".md""]\nexclude_patterns = [""README.md""]\n\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_options = {}\nhtml_static_path = [""_static""]\nhtml_show_sourcelink = False\nhtml_show_copyright = False\nhtml_show_sphinx = False\nhtml_logo = ""_static/logo-alpha.png""\nhtml_favicon = ""_static/favicon.png""\n\nautodoc_member_order = ""bysource""\nnapoleon_include_init_with_doc = True\nnapoleon_include_special_with_doc = True\n\ndef setup(app):\n  app.add_stylesheet(""custom.css"")\n'"
docs/generate-apidoc.py,0,"b'import inspect\nimport numbers\nimport os\nimport sys\nimport six\n\nimport opennmt\n\ndef document_class(output_dir,\n                   class_path,\n                   base_path=None,\n                   children_paths=None):\n  with open(os.path.join(output_dir, ""%s.rst"" % class_path), ""w"") as doc:\n    doc.write(""%s\\n"" % class_path)\n    doc.write(""="" * len(class_path))\n    doc.write(""\\n\\n"")\n    doc.write("".. autoclass:: %s\\n"" % class_path)\n    doc.write(""    :members:\\n"")\n    doc.write(""    :undoc-members:\\n"")\n    if base_path:\n      doc.write(""\\n    **Inherits from:** :class:`%s`\\n"" % base_path)\n    if children_paths:\n      doc.write(""\\n    **Extended by:**\\n\\n"")\n      for path in children_paths:\n        doc.write(""    - :class:`%s`\\n"" % path)\n\ndef document_function(output_dir, function_path):\n  with open(os.path.join(output_dir, ""%s.rst"" % function_path), ""w"") as doc:\n    doc.write(""%s\\n"" % function_path)\n    doc.write(""="" * len(function_path))\n    doc.write(""\\n\\n"")\n    doc.write("".. autofunction:: %s\\n"" % function_path)\n\ndef module_is_public(module):\n  return (module.__name__.startswith(""opennmt"")\n          and hasattr(module, ""__file__"")\n          and module.__file__.endswith(""__init__.py""))\n\ndef get_module_map(module, module_path):\n  """"""Map true modules to exported name""""""\n  if not module_is_public(module):\n    return {}\n  m = {}\n  for symbol_name in dir(module):\n    if symbol_name.startswith(""_""):\n      continue\n    symbol = getattr(module, symbol_name)\n    symbol_path = ""%s.%s"" % (module_path, symbol_name)\n    m[symbol] = symbol_path\n    if inspect.ismodule(symbol):\n      m.update(get_module_map(symbol, symbol_path))\n  return m\n\ndef get_first_public_parent(cls):\n  base = cls.__bases__[0]\n  while base.__name__.startswith(""_""):  # Skip private parent classes.\n    base = base.__bases__[0]\n  if base is not object and base.__bases__[0] is tuple:  # For namedtuples.\n    base = tuple\n  return base\n\ndef annotate_classes(classes):\n  annotations = []\n  child_classes = {}\n  for cls, path in classes:\n    parent = get_first_public_parent(cls)\n    if parent not in child_classes:\n      child_classes[parent] = [cls]\n    else:\n      child_classes[parent].append(cls)\n    annotations.append(dict(\n        cls=cls,\n        path=path,\n        parent=parent))\n  for annotation in annotations:\n    annotation[""children""] = child_classes.get(annotation[""cls""])\n  return annotations\n\ndef document_module(module, module_path, module_map, output_dir):\n  if not module_is_public(module):\n    return False\n  submodules = []\n  classes = []\n  functions = []\n  constants = []\n  for symbol_name in dir(module):\n    if symbol_name.startswith(""_""):\n      continue\n    symbol = getattr(module, symbol_name)\n    symbol_path = ""%s.%s"" % (module_path, symbol_name)\n    if inspect.isclass(symbol):\n      classes.append((symbol, symbol_path))\n    elif inspect.isfunction(symbol) or inspect.ismethod(symbol):\n      functions.append(symbol_path)\n    elif inspect.ismodule(symbol):\n      submodules.append((symbol_path, symbol))\n    elif isinstance(symbol, (numbers.Number, six.string_types)):\n      constants.append(symbol_path)\n\n  with open(os.path.join(output_dir, ""%s.rst"" % module_path), ""w"") as doc:\n    doc.write(""%s module\\n"" % module_path)\n    doc.write(""="" * (len(module_path) + 7))\n    doc.write(""\\n\\n"")\n    doc.write("".. automodule:: %s\\n\\n"" % module_path)\n\n    if submodules:\n      submodules = list(filter(\n        lambda x: document_module(x[1], x[0], module_map, output_dir), submodules))\n      if submodules:\n        doc.write(""Submodules\\n"")\n        doc.write(""----------\\n\\n"")\n        doc.write("".. toctree::\\n\\n"")\n        for module_path, module in submodules:\n          doc.write(""   %s\\n"" % module_path)\n        doc.write(""\\n"")\n\n    if classes:\n      doc.write(""Classes\\n"")\n      doc.write(""-------\\n\\n"")\n      doc.write("".. toctree::\\n\\n"")\n      for class_info in annotate_classes(classes):\n        base = class_info[""parent""]\n        base_path = module_map.get(base, ""%s.%s"" % (base.__module__, base.__name__))\n        children_paths = class_info[""children""]\n        if children_paths:\n          children_paths = [\n              module_map.get(child, ""%s.%s"" % (child.__module__, child.__name__))\n              for child in children_paths]\n        class_path = class_info[""path""]\n        doc.write(""   %s\\n"" % class_path)\n        document_class(\n            output_dir,\n            class_path,\n            base_path=base_path,\n            children_paths=children_paths)\n\n    if functions:\n      doc.write(""Functions\\n"")\n      doc.write(""---------\\n\\n"")\n      doc.write("".. toctree::\\n\\n"")\n      for function_path in functions:\n        doc.write(""   %s\\n"" % function_path)\n        document_function(output_dir, function_path)\n\n    if constants:\n      doc.write(""Constants\\n"")\n      doc.write(""---------\\n\\n"")\n      for constant_path in constants:\n        doc.write(""* %s\\n"" % constant_path)\n\n    return True\n\noutput_dir = sys.argv[1]\nos.makedirs(output_dir)\nmodule_map = get_module_map(opennmt, ""opennmt"")\ndocument_module(opennmt, ""opennmt"", module_map, output_dir)\n'"
opennmt/__init__.py,0,"b'""""""OpenNMT module.""""""\n\n__version__ = ""2.10.1""\n\nfrom opennmt.config import convert_to_v2_config\nfrom opennmt.config import load_config\nfrom opennmt.config import load_model\n\nfrom opennmt.constants import END_OF_SENTENCE_ID\nfrom opennmt.constants import END_OF_SENTENCE_TOKEN\nfrom opennmt.constants import PADDING_ID\nfrom opennmt.constants import PADDING_TOKEN\nfrom opennmt.constants import START_OF_SENTENCE_ID\nfrom opennmt.constants import START_OF_SENTENCE_TOKEN\nfrom opennmt.constants import UNKNOWN_TOKEN\n\nfrom opennmt.runner import Runner\n'"
opennmt/config.py,7,"b'""""""Defines functions related to configuration files.""""""\n\nfrom importlib import import_module\n\nimport copy\nimport os\nimport sys\nimport tensorflow as tf\nimport yaml\n\nfrom opennmt.models import catalog\nfrom opennmt.optimizers import utils as optimizers_lib\nfrom opennmt.schedules import lr_schedules as schedules_lib\nfrom opennmt.utils.misc import merge_dict\n\n\ndef load_model_module(path):\n  """"""Loads a model configuration file.\n\n  Args:\n    path: The relative path to the configuration file.\n\n  Returns:\n    A Python module.\n\n  Raises:\n    ValueError: if :obj:`path` is invalid.\n    ImportError: if the module in :obj:`path` does not define a model.\n  """"""\n  if not os.path.exists(path):\n    raise ValueError(""Model configuration not found in %s"" % path)\n  dirname, filename = os.path.split(path)\n  module_name, _ = os.path.splitext(filename)\n  sys.path.insert(0, os.path.abspath(dirname))\n  module = import_module(module_name)\n  sys.path.pop(0)\n\n  if not hasattr(module, ""model""):\n    raise ImportError(""No model defined in {}"".format(path))\n\n  return module\n\ndef load_model_from_file(path, as_builder=False):\n  """"""Loads a model from a configuration file.\n\n  Args:\n    path: The relative path to the configuration file.\n    as_builder: If ``True``, return a callable building the model on call.\n\n  Returns:\n    A :class:`opennmt.models.Model` instance or a callable returning such\n    instance.\n  """"""\n  module = load_model_module(path)\n  model = module.model\n  if not as_builder:\n    model = model()\n  del sys.path_importer_cache[os.path.dirname(module.__file__)]\n  del sys.modules[module.__name__]\n  return model\n\ndef load_model_from_catalog(name, as_builder=False):\n  """"""Loads a model from the catalog.\n\n  Args:\n    name: The model name.\n    as_builder: If ``True``, return a callable building the model on call.\n\n  Returns:\n    A :class:`opennmt.models.Model` instance or a callable returning such\n    instance.\n\n  Raises:\n    ValueError: if the model :obj:`name` does not exist in the model catalog.\n  """"""\n  return catalog.get_model_from_catalog(name, as_builder=as_builder)\n\ndef load_model(model_dir,\n               model_file=None,\n               model_name=None,\n               serialize_model=True,\n               as_builder=False):\n  """"""Loads the model from the catalog or a definition file.\n\n  Args:\n    model_dir: The model directory.\n    model_file: An optional model configuration.\n      Mutually exclusive with :obj:`model_name`.\n    model_name: An optional model name from the catalog.\n      Mutually exclusive with :obj:`model_file`.\n    serialize_model: Serialize the model definition in the model directory to\n      make it optional for future runs.\n    as_builder: If ``True``, return a callable building the model on call.\n\n  Returns:\n    A :class:`opennmt.models.Model` instance or a callable returning such\n    instance.\n\n  Raises:\n    ValueError: if both :obj:`model_file` and :obj:`model_name` are set.\n  """"""\n  if model_file and model_name:\n    raise ValueError(""only one of model_file and model_name should be set"")\n  model_name_or_path = model_file or model_name\n  model_description_path = os.path.join(model_dir, ""model_description.py"")\n\n  if model_name_or_path:\n    if tf.train.latest_checkpoint(model_dir) is not None:\n      tf.get_logger().warning(\n          ""You provided a model configuration but a checkpoint already exists. ""\n          ""The model configuration must define the same model as the one used for ""\n          ""the initial training. However, you can change non structural values like ""\n          ""dropout."")\n\n    if model_file:\n      model = load_model_from_file(model_file, as_builder=as_builder)\n      if serialize_model:\n        tf.io.gfile.copy(model_file, model_description_path, overwrite=True)\n    elif model_name:\n      model = load_model_from_catalog(model_name, as_builder=as_builder)\n      if serialize_model:\n        with tf.io.gfile.GFile(model_description_path, mode=""w"") as model_description_file:\n          model_description_file.write(\n              ""from opennmt import models\\n""\n              ""model = lambda: models.get_model_from_catalog(\\""%s\\"")\\n"" % model_name)\n  elif tf.io.gfile.exists(model_description_path):\n    tf.get_logger().info(""Loading model description from %s"", model_description_path)\n    model = load_model_from_file(model_description_path, as_builder=as_builder)\n  else:\n    raise RuntimeError(""A model configuration is required: you probably need to ""\n                       ""set --model or --model_type on the command line."")\n\n  return model\n\ndef load_config(config_paths, config=None):\n  """"""Loads YAML configuration files.\n\n  Args:\n    config_paths: A list of configuration files that will be merged to a single\n      configuration. The rightmost configuration takes priority.\n    config: A (possibly non empty) config dictionary to fill.\n\n  Returns:\n    The configuration as Python dictionary.\n  """"""\n  if config is None:\n    config = {}\n\n  for config_path in config_paths:\n    with tf.io.gfile.GFile(config_path, mode=""rb"") as config_file:\n      subconfig = yaml.load(config_file.read(), Loader=yaml.UnsafeLoader)\n      # Add or update section in main configuration.\n      merge_dict(config, subconfig)\n\n  return config\n\n\ndef convert_to_v2_config(v1_config):\n  """"""Converts a V1 configuration to its V2 equivalent.\n\n  Args:\n    v1_config: The V1 configuration.\n\n  Returns:\n    The V2 configuration.\n\n  Raises:\n    ValueError: if the conversion can not be done automatically.\n  """"""\n  config = copy.deepcopy(v1_config)\n\n  _convert_to_v2_params(config)\n\n  data_config = config.get(""data"")\n  if data_config:\n    # Covers most of seq2seq models in the catalog.\n    _rename_opt(data_config, ""source_words_vocabulary"", ""source_vocabulary"")\n    _rename_opt(data_config, ""target_words_vocabulary"", ""target_vocabulary"")\n\n  for section_name in (""train"", ""eval"", ""infer"", ""score"", ""params""):\n    section = config.get(section_name)\n    if section is None:\n      continue\n\n    _delete_opt(section, ""num_threads"")\n    _delete_opt(section, ""prefetch_buffer_size"")\n    _rename_opt(section, ""bucket_width"", ""length_bucket_width"")\n    if section_name == ""train"":\n      _rename_opt(section, ""train_steps"", ""max_step"")\n      _delete_opt(section, ""save_checkpoints_secs"")\n    elif section_name == ""eval"":\n      _delete_opt(section, ""eval_delay"")\n      _delete_opt(section, ""exporters"")\n\n    # Remove empty sections.\n    if not section:\n      config.pop(section_name)\n\n  return config\n\ndef _convert_to_v2_params(config):\n  params = config.get(""params"")\n  if not params:\n    return\n  if ""freeze_variables"" in params:\n    raise ValueError(""params/freeze_variables should be manually converted to ""\n                     ""params/freeze_layers"")\n\n  _convert_to_v2_optimizer(params)\n  _convert_to_v2_lr_schedules(params)\n  _convert_to_v2_step_accumulation(params, config)\n  _delete_opt(params, ""param_init"")\n  _delete_opt(params, ""loss_scale"")\n  _delete_opt(params, ""horovod"")\n  _delete_opt(params, ""maximum_learning_rate"")\n  _rename_opt(params, ""maximum_iterations"", ""maximum_decoding_length"")\n\n  clip_gradients = _delete_opt(params, ""clip_gradients"")\n  if clip_gradients:\n    optimizer_params = params.setdefault(""optimizer_params"", {})\n    optimizer_params[""clipnorm""] = clip_gradients\n\n  weight_decay = _delete_opt(params, ""weight_decay"")\n  if weight_decay:\n    optimizer_params = params.setdefault(""optimizer_params"", {})\n    optimizer_params[""weight_decay""] = weight_decay\n\n# Only covering the most common optimizers.\n_V1_OPTIMIZER_MAP = {\n    ""AdamOptimizer"": ""Adam"",\n    ""GradientDescentOptimizer"": ""SGD"",\n    ""LazyAdamOptimizer"": ""LazyAdam"",\n}\n\ndef _convert_to_v2_optimizer(params):\n  optimizer = params.get(""optimizer"")\n  if optimizer:\n    try:\n      # Check if the optimizer exists in V2.\n      optimizers_lib.get_optimizer_class(optimizer)\n    except ValueError:\n      v2_optimizer = _V1_OPTIMIZER_MAP.get(optimizer)\n      if not v2_optimizer:\n        raise ValueError(""params/optimizer should be manually converted: no registered ""\n                         ""conversion for optimizer %s"" % optimizer)\n      params[""optimizer""] = v2_optimizer\n\n  optimizer_params = params.get(""optimizer_params"")\n  if optimizer_params:\n    _rename_opt(optimizer_params, ""beta1"", ""beta_1"")\n    _rename_opt(optimizer_params, ""beta2"", ""beta_2"")\n\ndef _convert_to_v2_lr_schedules(params):\n  decay_type = params.get(""decay_type"")\n  if not decay_type:\n    return\n  try:\n    # Check if the learning rate schedule exists in V2.\n    schedules_lib.get_lr_schedule_class(decay_type)\n  except ValueError:\n    if decay_type.startswith(""noam_decay""):\n      params[""decay_type""] = ""NoamDecay""\n      if ""decay_params"" not in params:\n        model_dim = _delete_opt(params, ""decay_rate"")\n        warmup_steps = _delete_opt(params, ""decay_steps"")\n        params[""decay_params""] = dict(model_dim=model_dim, warmup_steps=warmup_steps)\n    else:\n      raise ValueError(""params/decay_type should be manually converted: no registered ""\n                       ""conversion for decay type %s"" % decay_type)\n\ndef _convert_to_v2_step_accumulation(params, config):\n  # Try to upgrade step accumulation to train/effective_batch_size.\n  decay_step_duration = _delete_opt(params, ""decay_step_duration"") or 1\n  gradients_accum = _delete_opt(params, ""gradients_accum"") or 1\n  accum_steps = max(decay_step_duration, gradients_accum)\n  if accum_steps > 1:\n    train_config = config.setdefault(""train"", {})\n    batch_size = train_config.get(""batch_size"")\n    if batch_size is not None and batch_size > 0:\n      train_config[""effective_batch_size""] = batch_size * accum_steps\n    else:\n      raise ValueError(""params/decay_step_duration and params/gradients_accum ""\n                       ""should be manually converted to train/effective_batch_size"")\n\ndef _delete_opt(config, name):\n  return config.pop(name, None)\n\ndef _rename_opt(config, name, new_name):\n  value = _delete_opt(config, name)\n  if value is not None:\n    config[new_name] = value\n  return value\n'"
opennmt/constants.py,0,"b'""""""Define constant values used thoughout the project.""""""\n\nPADDING_TOKEN = ""<blank>""\nSTART_OF_SENTENCE_TOKEN = ""<s>""\nEND_OF_SENTENCE_TOKEN = ""</s>""\nUNKNOWN_TOKEN = ""<unk>""\n\nPADDING_ID = 0\nSTART_OF_SENTENCE_ID = 1\nEND_OF_SENTENCE_ID = 2\n'"
opennmt/evaluation.py,14,"b'""""""Evaluation related classes and functions.""""""\n\nimport collections\nimport os\nimport shutil\n\nimport tensorflow as tf\n\nfrom opennmt.utils import exporters\nfrom opennmt.utils import misc\nfrom opennmt.utils import scorers as scorers_lib\n\n\n_SUMMARIES_SCOPE = ""metrics""\n\n\nclass EarlyStopping(\n    collections.namedtuple(""EarlyStopping"",\n                           (""metric"", ""min_improvement"", ""steps""))):\n  """"""Conditions for early stopping.""""""\n\n\nclass Evaluator(object):\n  """"""Model evaluator.""""""\n\n  def __init__(self,\n               model,\n               features_file,\n               labels_file,\n               batch_size,\n               scorers=None,\n               save_predictions=False,\n               early_stopping=None,\n               model_dir=None,\n               export_on_best=None,\n               exporter=None,\n               max_exports_to_keep=5):\n    """"""Initializes the evaluator.\n\n    Args:\n      model: A :class:`opennmt.models.model.Model` to evaluate.\n      features_file: Path to the evaluation features.\n      labels_file: Path to the evaluation labels.\n      batch_size: The evaluation batch size.\n      scorers: A list of scorers, callables taking the path to the reference and\n        the hypothesis and return one or more scores.\n      save_predictions: Save evaluation predictions to a file. This is ``True``\n        when :obj:`external_evaluator` is set.\n      early_stopping: An ``EarlyStopping`` instance.\n      model_dir: The active model directory.\n      export_on_best: Export a model when this evaluation metric has the\n        best value so far.\n      exporter: A :class:`opennmt.utils.Exporter` instance to export the model.\n        Defaults to :class:`opennmt.utils.SavedModelExporter`.\n      max_exports_to_keep: Maximum number of exports to keep. Older exports will\n        be garbage collected. Set to ``None`` to keep all exports.\n\n    Raises:\n      ValueError: If :obj:`save_predictions` is set but the model is not compatible.\n      ValueError: If :obj:`save_predictions` is set but :obj:`model_dir` is ``None``.\n      ValueError: If :obj:`export_on_best` is set but :obj:`model_dir` is ``None``.\n      ValueError: If the :obj:`early_stopping` configuration is invalid.\n    """"""\n    if model_dir is not None:\n      export_dir = os.path.join(model_dir, ""export"")\n      eval_dir = os.path.join(model_dir, ""eval"")\n    else:\n      if save_predictions:\n        raise ValueError(""Saving evaluation predictions requires model_dir to be set"")\n      if export_on_best is not None:\n        raise ValueError(""Exporting models requires model_dir to be set"")\n      export_dir = None\n      eval_dir = None\n\n    if scorers is None:\n      scorers = []\n    if scorers:\n      save_predictions = True\n    if save_predictions:\n      if model.unsupervised:\n        raise ValueError(""This model does not support saving evaluation predictions"")\n      if not tf.io.gfile.exists(eval_dir):\n        tf.io.gfile.makedirs(eval_dir)\n    self._model = model\n    self._labels_file = labels_file\n    self._save_predictions = save_predictions\n    self._scorers = scorers\n    self._eval_dir = eval_dir\n    self._metrics_history = []\n    if eval_dir is not None:\n      self._summary_writer = tf.summary.create_file_writer(eval_dir)\n      summaries = misc.read_summaries(eval_dir)\n      for step, values in summaries:\n        metrics = misc.extract_prefixed_keys(values, _SUMMARIES_SCOPE + ""/"")\n        self._metrics_history.append((step, metrics))\n    else:\n      self._summary_writer = tf.summary.create_noop_writer()\n    dataset = model.examples_inputter.make_evaluation_dataset(\n        features_file,\n        labels_file,\n        batch_size,\n        num_threads=1,\n        prefetch_buffer_size=1)\n\n    self._eval_fn = tf.function(model.evaluate, input_signature=dataset.element_spec)\n    self._dataset = dataset\n\n    self._metrics_name = {""loss"", ""perplexity""}\n    for scorer in self._scorers:\n      self._metrics_name.update(scorer.scores_name)\n    model_metrics = self._model.get_metrics()\n    if model_metrics:\n      self._metrics_name.update(set(model_metrics.keys()))\n\n    if early_stopping is not None:\n      if early_stopping.metric not in self._metrics_name:\n        raise ValueError(""Invalid early stopping metric \'%s\', expected one in %s"" % (\n            early_stopping.metric, str(self._metrics_name)))\n      if early_stopping.steps <= 0:\n        raise ValueError(""Early stopping steps should greater than 0"")\n    self._early_stopping = early_stopping\n\n    self._export_on_best = export_on_best\n    self._exporter = exporter\n    self._export_dir = export_dir\n    self._max_exports_to_keep = max_exports_to_keep\n\n  @classmethod\n  def from_config(cls, model, config, features_file=None, labels_file=None):\n    """"""Creates an evaluator from the configuration.\n\n    Args:\n      model: A :class:`opennmt.models.model.Model` to evaluate.\n      config: The global user configuration.\n      features_file: Optional input features file to evaluate. If not set, will\n        load ``eval_features_file`` from the data configuration.\n      labels_file: Optional output labels file to evaluate. If not set, will load\n        ``eval_labels_file`` from the data configuration.\n\n    Returns:\n      A :class:`opennmt.evaluation.Evaluator` instance.\n\n    Raises:\n      ValueError: if one of :obj:`features_file` and :obj:`labels_file` is set\n        but not the other.\n    """"""\n    if (features_file is None) != (labels_file is None):\n      raise ValueError(""features_file and labels_file should be both set for evaluation"")\n    eval_config = config[""eval""]\n    scorers = eval_config.get(""external_evaluators"")\n    if scorers is not None:\n      scorers = scorers_lib.make_scorers(scorers)\n    early_stopping_config = eval_config.get(""early_stopping"")\n    if early_stopping_config is not None:\n      early_stopping = EarlyStopping(\n          metric=early_stopping_config.get(""metric"", ""loss""),\n          min_improvement=early_stopping_config.get(""min_improvement"", 0),\n          steps=early_stopping_config[""steps""])\n    else:\n      early_stopping = None\n    return cls(\n        model,\n        features_file or config[""data""][""eval_features_file""],\n        labels_file or config[""data""].get(""eval_labels_file""),\n        eval_config[""batch_size""],\n        scorers=scorers,\n        save_predictions=eval_config.get(""save_eval_predictions"", False),\n        early_stopping=early_stopping,\n        model_dir=config[""model_dir""],\n        export_on_best=eval_config.get(""export_on_best""),\n        exporter=exporters.make_exporter(eval_config.get(""export_format"", ""saved_model"")),\n        max_exports_to_keep=eval_config.get(""max_exports_to_keep"", 5))\n\n  @property\n  def predictions_dir(self):\n    """"""The directory containing saved predictions.""""""\n    return self._eval_dir\n\n  @property\n  def export_dir(self):\n    """"""The directory containing exported models.""""""\n    return self._export_dir\n\n  @property\n  def metrics_name(self):\n    """"""The name of the metrics returned by this evaluator.""""""\n    return self._metrics_name\n\n  @property\n  def metrics_history(self):\n    """"""The history of metrics result per evaluation step.""""""\n    return self._metrics_history\n\n  @property\n  def last_evaluated_step(self):\n    """"""The last training step that was evaluated.""""""\n    if not self._metrics_history:\n      return None\n    return self._metrics_history[-1][0]\n\n  def _is_higher_better_for_metric(self, metric):\n    # Look if the metric is produced by a scorer as they define the scores order.\n    for scorer in self._scorers:\n      if metric in scorer.scores_name:\n        return scorer.higher_is_better()\n    # TODO: the condition below is not always true, find a way to set it\n    # correctly for Keras metrics.\n    return metric not in (""loss"", ""perplexity"")\n\n  def _get_metric_history(self, metric):\n    return [\n        metrics[metric] for _, metrics in self._metrics_history if metric in metrics]\n\n  def should_stop(self):\n    """"""Returns ``True`` if early stopping conditions are met.""""""\n    if self._early_stopping is None:\n      return False\n    target_metric = self._early_stopping.metric\n    higher_is_better = self._is_higher_better_for_metric(target_metric)\n    metrics = self._get_metric_history(target_metric)\n    should_stop = early_stop(\n        metrics,\n        self._early_stopping.steps,\n        min_improvement=self._early_stopping.min_improvement,\n        higher_is_better=higher_is_better)\n    if should_stop:\n      tf.get_logger().warning(\n          ""Evaluation metric \'%s\' did not improve more than %f in the last %d evaluations"",\n          target_metric,\n          self._early_stopping.min_improvement,\n          self._early_stopping.steps)\n    return should_stop\n\n  def is_best(self, metric):\n    """"""Returns ``True`` if the latest value of :obj:`metric` is the best so far.\n\n    Args:\n      metric: The metric to consider.\n\n    Returns:\n      A boolean.\n    """"""\n    metric_history = self._get_metric_history(metric)\n    if not metric_history:\n      return False\n    metric_history, latest_value = metric_history[:-1], metric_history[-1]\n    if not metric_history:\n      return True\n    if self._is_higher_better_for_metric(metric):\n      return latest_value > max(metric_history)\n    else:\n      return latest_value < min(metric_history)\n\n  def __call__(self, step):\n    """"""Runs the evaluator.\n\n    Args:\n      step: The current training step.\n\n    Returns:\n      A dictionary of evaluation metrics.\n    """"""\n    tf.get_logger().info(""Running evaluation for step %d"", step)\n    output_file = None\n    output_path = None\n    if self._save_predictions:\n      output_path = os.path.join(self._eval_dir, ""predictions.txt.%d"" % step)\n      output_file = tf.io.gfile.GFile(output_path, ""w"")\n\n    loss_num = 0\n    loss_den = 0\n    metrics = self._model.get_metrics()\n    for source, target in self._dataset:\n      loss, predictions = self._eval_fn(source, target)\n      if isinstance(loss, tuple):\n        loss_num += loss[0]\n        loss_den += loss[1]\n      else:\n        loss_num += loss\n        loss_den += 1\n      if metrics:\n        self._model.update_metrics(metrics, predictions, target)\n      if output_file is not None:\n        predictions = {k:v.numpy() for k, v in predictions.items()}\n        for prediction in misc.extract_batches(predictions):\n          self._model.print_prediction(prediction, stream=output_file)\n    if loss_den == 0:\n      raise RuntimeError(""No examples were evaluated"")\n    loss = loss_num / loss_den\n\n    results = dict(loss=loss, perplexity=tf.math.exp(loss))\n    if metrics:\n      for name, metric in metrics.items():\n        results[name] = metric.result()\n    if self._save_predictions:\n      tf.get_logger().info(""Evaluation predictions saved to %s"", output_path)\n      output_file.close()\n      for scorer in self._scorers:\n        score = scorer(self._labels_file, output_path)\n        if isinstance(score, dict):\n          results.update(score)\n        else:\n          results[scorer.name] = score\n\n    for name, value in results.items():\n      if isinstance(value, tf.Tensor):\n        results[name] = value.numpy()\n\n    self._record_results(step, results)\n    self._maybe_export(step, results)\n    self._maybe_garbage_collect_exports()\n    return results\n\n  def _record_results(self, step, results):\n    # Clear history for steps that are greater than step.\n    while self._metrics_history and self._metrics_history[-1][0] > step:\n      self._metrics_history.pop()\n    self._metrics_history.append((step, dict(results)))\n    tf.get_logger().info(\n        ""Evaluation result for step %d: %s"",\n        step,\n        "" ; "".join(""%s = %f"" % (k, v) for k, v in results.items()))\n    with self._summary_writer.as_default():\n      for key, value in results.items():\n        tf.summary.scalar(""%s/%s"" % (_SUMMARIES_SCOPE, key), value, step=step)\n      self._summary_writer.flush()\n\n  def _maybe_export(self, step, results):\n    if self._export_on_best is None or not self.is_best(self._export_on_best):\n      return\n    export_dir = os.path.join(self._export_dir, str(step))\n    tf.get_logger().info(""Exporting model to %s (best %s so far: %f)"",\n                         export_dir, self._export_on_best, results[self._export_on_best])\n    self._model.export(export_dir, exporter=self._exporter)\n\n  def _maybe_garbage_collect_exports(self):\n    if self._max_exports_to_keep is None or not os.path.exists(self._export_dir):\n      return\n    exported_steps = list(sorted(map(int, os.listdir(self._export_dir))))\n    num_exports = len(exported_steps)\n    if num_exports > self._max_exports_to_keep:\n      steps_to_remove = exported_steps[:num_exports - self._max_exports_to_keep]\n      for step in steps_to_remove:\n        shutil.rmtree(os.path.join(self._export_dir, str(step)))\n\n\ndef early_stop(metrics, steps, min_improvement=0, higher_is_better=False):\n  """"""Early stopping condition.\n\n  Args:\n    metrics: A list of metric values.\n    steps: Consider the improvement over this many steps.\n    min_improvement: Continue if the metric improved less than this value:\n    higher_is_better: Whether a higher value is better for this metric.\n\n  Returns:\n    A boolean.\n  """"""\n  if len(metrics) < steps + 1:\n    return False\n\n  def _did_improve(ref, new):\n    # Returns True if new is improving on ref.\n    if higher_is_better:\n      return new > ref + min_improvement\n    else:\n      return new < ref - min_improvement\n\n  ref_metric = metrics[-steps - 1]\n  for metric in metrics[-steps:]:\n    if _did_improve(ref_metric, metric):\n      return False\n  return True\n'"
opennmt/inference.py,11,"b'""""""Inference related classes and functions.""""""\n\nimport sys\nimport time\n\nimport tensorflow as tf\n\nfrom opennmt.utils import misc\n\n\ndef predict_dataset(model,\n                    dataset,\n                    print_params=None,\n                    predictions_file=None,\n                    log_time=False):\n  """"""Outputs the model predictions for the dataset.\n\n  To run inference on strings directly, see\n  :meth:`opennmt.models.Model.serve_function`.\n\n  Args:\n    model: A :class:`opennmt.models.Model` instance.\n    dataset: A ``tf.data.Dataset`` instance outputting features.\n    print_params: A dictionary of parameters passed to\n      :meth:`opennmt.models.Model.print_prediction`.\n    predictions_file: If set, predictions are saved in this file, otherwise they\n      are printed on the standard output.\n    log_time: If ``True``, several time metrics will be printed in the logs at\n      the end of the inference loop.\n  """"""\n  if predictions_file:\n    stream = open(predictions_file, encoding=""utf-8"", mode=""w"")\n  else:\n    stream = sys.stdout\n\n  infer_fn = tf.function(model.infer, input_signature=(dataset.element_spec,))\n  tf.get_logger().info(""Tracing and optimizing the inference graph..."")\n  infer_fn.get_concrete_function()  # Trace the function now.\n\n  # Inference might return out-of-order predictions. The OrderRestorer utility is\n  # used to write predictions in their original order.\n  write_fn = lambda prediction: (\n      model.print_prediction(prediction, params=print_params, stream=stream))\n  index_fn = lambda prediction: prediction.get(""index"")\n  ordered_writer = misc.OrderRestorer(index_fn, write_fn)\n\n  total_time = 0\n  total_tokens = 0\n  total_examples = 0\n  start_time = time.time()\n\n  # When the inference dataset is bucketized, it can happen that no output is\n  # written in a long time. To avoid confusion and give the impression that\n  # the process is stuck, we ensure that something is logged regularly.\n  max_time_without_output = 10\n  last_output_time = start_time\n\n  for features in dataset:\n    predictions = infer_fn(features)\n    predictions = tf.nest.map_structure(lambda t: t.numpy(), predictions)\n    batch_time = time.time()\n\n    for prediction in misc.extract_batches(predictions):\n      written = ordered_writer.push(prediction)\n      if written:\n        last_output_time = batch_time\n      else:\n        time_without_output = batch_time - last_output_time\n        if time_without_output >= max_time_without_output:\n          tf.get_logger().info(\n              ""%d predictions are buffered, but waiting for the prediction of ""\n              ""line %d to advance the output..."",\n              ordered_writer.buffer_size,\n              ordered_writer.next_index + 1)\n          last_output_time = batch_time\n\n    if log_time:\n      batch_size = next(iter(predictions.values())).shape[0]\n      total_examples += batch_size\n      length = predictions.get(""length"")\n      if length is not None:\n        if len(length.shape) == 2:\n          length = length[:, 0]\n        total_tokens += sum(length)\n\n  if log_time:\n    end_time = time.time()\n    total_time = end_time - start_time\n    tf.get_logger().info(""Total prediction time (s): %f"", total_time)\n    tf.get_logger().info(\n        ""Average prediction time (s): %f"", total_time / total_examples)\n    if total_tokens > 0:\n      tf.get_logger().info(""Tokens per second: %f"", total_tokens / total_time)\n  if predictions_file:\n    stream.close()\n\ndef score_dataset(model,\n                  dataset,\n                  print_params=None,\n                  output_file=None):\n  """"""Outputs the model scores for the dataset.\n\n  Args:\n    model: A :class:`opennmt.models.Model` instance.\n    dataset: A ``tf.data.Dataset`` instance outputting parallel features and\n      labels.\n    print_params: A dictionary of parameters passed to\n      :meth:`opennmt.models.Model.print_score`.\n    output_file: If set, outputs are saved in this file, otherwise they are\n      printed on the standard output.\n  """"""\n  if output_file:\n    stream = open(output_file, encoding=""utf-8"", mode=""w"")\n  else:\n    stream = sys.stdout\n\n  score_fn = tf.function(model.score, input_signature=dataset.element_spec)\n  for features, labels in dataset:\n    results = score_fn(features, labels)\n    results = tf.nest.map_structure(lambda t: t.numpy(), results)\n    for batch in misc.extract_batches(results):\n      model.print_score(batch, params=print_params, stream=stream)\n\n  if output_file:\n    stream.close()\n'"
opennmt/runner.py,10,"b'""""""Main library entrypoint.""""""\n\nimport copy\nimport os\nimport sys\nimport random\nimport math\nimport subprocess\nimport time\nimport tempfile\nimport yaml\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom opennmt import evaluation\nfrom opennmt import inference\nfrom opennmt import models\nfrom opennmt import training as training_util\nfrom opennmt.utils import checkpoint as checkpoint_util\nfrom opennmt.utils import misc\n\n\n# These options require a value but we can fallback to a default one.\n_CONFIG_FALLBACK = {\n    ""params"": {},\n    ""train"": {\n        ""batch_type"": ""examples"",\n        ""length_bucket_width"": 1,\n        ""sample_buffer_size"": 500000,\n        ""save_summary_steps"": 100\n    },\n    ""eval"": {\n        ""batch_size"": 32\n    },\n    ""infer"": {\n        ""length_bucket_width"": None,\n        ""batch_size"": 16\n    },\n    ""score"": {\n        ""batch_size"": 64\n    }\n}\n\nclass Runner(object):\n  """"""Class for running and exporting models.""""""\n\n  def __init__(self,\n               model,\n               config,\n               auto_config=False,\n               mixed_precision=False,\n               seed=None):\n    """"""Initializes the runner parameters.\n\n    Args:\n      model: A :class:`opennmt.models.Model` instance to run or a callable that\n        returns such instance.\n      config: The run configuration.\n      auto_config: If ``True``, use automatic configuration values defined by\n        :obj:`model`.\n      mixed_precision: Enable mixed precision.\n      seed: The random seed to set.\n\n    Raises:\n      TypeError: if :obj:`model` is not a :class:`opennmt.models.Model` instance\n        or a callable.\n    """"""\n    if isinstance(model, models.Model):\n      self._model = model\n      self._model_fn = lambda: misc.clone_layer(model)\n    elif callable(model):\n      self._model = model()\n      self._model_fn = model\n    else:\n      raise TypeError(""model should be a opennmt.models.Model instance or a callable"")\n    self._optimizer = None\n    self._config = copy.deepcopy(config)\n    self._auto_config = auto_config\n    self._mixed_precision = mixed_precision\n    if mixed_precision:\n      tf.config.optimizer.set_experimental_options({""auto_mixed_precision"": True})\n    if seed is not None:\n      np.random.seed(seed)\n      random.seed(seed)\n      tf.random.set_seed(seed)\n\n  @property\n  def model(self):\n    """"""The :class:`opennmt.models.Model` executed by this runner.""""""\n    return self._model\n\n  @property\n  def model_dir(self):\n    """"""The active model directory.""""""\n    return self._config[""model_dir""]\n\n  def _finalize_config(self, training=False, num_replicas=1, num_devices=1):\n    # Configuration priority: user config > auto config > default config.\n    config = copy.deepcopy(_CONFIG_FALLBACK)\n    if self._auto_config:\n      model_config = self._model.auto_config(num_replicas=num_replicas)\n      if not model_config:\n        raise NotImplementedError(""This model does not define any automatic configuration values"")\n      misc.merge_dict(config, model_config)\n    misc.merge_dict(config, self._config)\n\n    config[""params""].setdefault(""num_hypotheses"", config[""infer""].get(""n_best"", 1))\n    config[""params""].setdefault(""average_loss_in_time"", config[""train""][""batch_type""] == ""tokens"")\n\n    if training:\n      train_config = config[""train""]\n      batch_size = train_config.get(""batch_size"")\n\n      # Auto tune batch size.\n      if batch_size is None or batch_size == 0:\n        if train_config[""batch_type""] == ""examples"":\n          raise ValueError(""Batch size autotuning is only supported for the \\""tokens\\"" batch type"")\n        max_batch_size = 16384\n        if train_config.get(""effective_batch_size"") is not None:\n          max_batch_size = min(max_batch_size, train_config[""effective_batch_size""])\n        train_config[""batch_size""] = _auto_tune_batch_size(\n            config,\n            max_batch_size=max_batch_size,\n            num_devices=num_devices,\n            mixed_precision=self._mixed_precision)\n\n    tf.get_logger().info(\n        ""Using parameters:\\n%s"", yaml.dump(config, indent=2, default_flow_style=False))\n    return config\n\n  def _init_model(self, config):\n    model = self._model_fn()\n    model.initialize(config[""data""], params=config[""params""])\n    return model\n\n  def train(self, num_devices=1, with_eval=False, checkpoint_path=None, hvd=None):\n    """"""Runs the training loop.\n\n    Args:\n      num_devices: Number of devices to use for training.\n      with_eval: Enable evaluation during training.\n      checkpoint_path: The checkpoint path to load the model weights from it.\n      hvd: Optional Horovod module.\n\n    Returns:\n      The path to the final model directory.\n    """"""\n    if hvd is None:\n      num_replicas = num_devices\n      is_master = True\n    else:\n      num_replicas = hvd.size()\n      is_master = hvd.rank() == 0\n\n    config = self._finalize_config(\n        training=True,\n        num_replicas=num_replicas,\n        num_devices=num_devices)\n    model = self._init_model(config)\n    optimizer = model.get_optimizer()\n\n    data_config = config[""data""]\n    train_config = config[""train""]\n    eval_config = config[""eval""]\n\n    batch_type = train_config[""batch_type""]\n    if batch_type == ""tokens"" and self._mixed_precision:\n      batch_size_multiple = 8\n    else:\n      batch_size_multiple = 1\n\n    dataset_fn = lambda input_context: model.examples_inputter.make_training_dataset(\n        data_config[""train_features_file""],\n        data_config.get(""train_labels_file""),\n        train_config[""batch_size""],\n        batch_type=batch_type,\n        batch_size_multiple=batch_size_multiple,\n        shuffle_buffer_size=train_config[""sample_buffer_size""],\n        length_bucket_width=train_config[""length_bucket_width""],\n        maximum_features_length=train_config.get(""maximum_features_length""),\n        maximum_labels_length=train_config.get(""maximum_labels_length""),\n        single_pass=train_config.get(""single_pass"", False),\n        num_shards=input_context.num_input_pipelines,\n        shard_index=input_context.input_pipeline_id,\n        prefetch_buffer_size=train_config.get(""prefetch_buffer_size""),\n        cardinality_multiple=input_context.num_replicas_in_sync,\n        weights=data_config.get(""train_files_weights""))\n\n    checkpoint = None\n    evaluator = None\n    if is_master:\n      checkpoint = checkpoint_util.Checkpoint.from_config(config, model, optimizer=optimizer)\n      checkpoint.restore(\n          checkpoint_path=checkpoint_path, weights_only=checkpoint_path is not None)\n      if with_eval:\n        evaluator = evaluation.Evaluator.from_config(model, config)\n\n    # Set gradients accumulation based on the requested effective batch size.\n    if train_config.get(""effective_batch_size"") is not None:\n      accum_steps = _count_batch_accum(\n          train_config[""batch_size""],\n          train_config[""effective_batch_size""],\n          num_replicas=num_replicas)\n      tf.get_logger().info(\n          ""Accumulate gradients of %d iterations to reach effective batch size of %d"",\n          accum_steps,\n          train_config[""effective_batch_size""])\n    else:\n      accum_steps = 1\n\n    if hvd is not None:\n      if num_devices > 1:\n        raise ValueError(""num_devices (or num_gpus) should be set to 1 when using Horovod"")\n      trainer = training_util.HorovodTrainer(\n          model, optimizer, hvd, checkpoint=checkpoint)\n    elif num_devices > 1:\n      devices = misc.get_devices(count=num_devices)\n      trainer = training_util.MirroredStrategyTrainer(\n          model, optimizer, checkpoint=checkpoint, devices=devices)\n    else:\n      trainer = training_util.Trainer(model, optimizer, checkpoint=checkpoint)\n\n    trainer(\n        dataset_fn,\n        max_step=train_config.get(""max_step""),\n        accum_steps=accum_steps,\n        report_steps=train_config.get(""save_summary_steps"", 100),\n        save_steps=train_config.get(""save_checkpoints_steps"", 5000),\n        evaluator=evaluator,\n        eval_steps=eval_config.get(""steps"", 5000),\n        moving_average_decay=train_config.get(""moving_average_decay""))\n\n    if checkpoint is None:\n      return None\n    average_last_checkpoints = train_config.get(""average_last_checkpoints"", 0)\n    if average_last_checkpoints > 0:\n      return self.average_checkpoints(\n          os.path.join(checkpoint.model_dir, ""avg""),\n          max_count=average_last_checkpoints)\n    return checkpoint.model_dir\n\n  def evaluate(self, features_file=None, labels_file=None, checkpoint_path=None):\n    """"""Runs evaluation.\n\n    Args:\n      features_file: The input features file to evaluate. If not set, will load\n        ``eval_features_file`` from the data configuration.\n      labels_file: The output labels file to evaluate. If not set, will load\n        ``eval_labels_file`` from the data configuration.\n      checkpoint_path: The checkpoint path to load the model weights from it.\n\n    Returns:\n      A dict of evaluation metrics.\n    """"""\n    config = self._finalize_config()\n    model = self._init_model(config)\n    checkpoint = checkpoint_util.Checkpoint.from_config(config, model)\n    checkpoint_path = checkpoint.restore(checkpoint_path=checkpoint_path, weights_only=True)\n    step = checkpoint_util.get_step_from_checkpoint_prefix(checkpoint_path)\n    evaluator = evaluation.Evaluator.from_config(\n        model,\n        config,\n        features_file=features_file,\n        labels_file=labels_file)\n    return evaluator(step)\n\n  def average_checkpoints(self, output_dir, max_count=8):\n    """"""Averages checkpoints.\n\n    Args:\n      output_dir: The directory that will contain the averaged checkpoint.\n      max_count: The maximum number of checkpoints to average.\n\n    Returns:\n      The path to the directory containing the averaged checkpoint.\n    """"""\n    config = self._finalize_config()\n    model = self._init_model(config)\n    optimizer = model.get_optimizer()\n    checkpoint = checkpoint_util.Checkpoint.from_config(config, model, optimizer=optimizer)\n    checkpoint.restore()\n    model.create_variables(optimizer=optimizer)\n    trackables = dict(model=model, optimizer=optimizer)\n    output_dir = checkpoint_util.average_checkpoints(\n        checkpoint.model_dir,\n        output_dir,\n        trackables,\n        max_count=max_count)\n    self._config[""model_dir""] = output_dir\n    return output_dir\n\n  def update_vocab(self, output_dir, src_vocab=None, tgt_vocab=None):\n    """"""Updates model vocabularies.\n\n    Args:\n      output_dir: Directory where the update checkpoint will be saved.\n      src_vocab: Path to the new source vocabulary.\n      tgt_vocab: Path to the new tagret vocabulary.\n\n    Returns:\n      Path to the new checkpoint directory.\n    """"""\n    if not isinstance(self._model, models.SequenceToSequence):\n      raise ValueError(""Updating vocabularies is only supported for sequence to sequence models"")\n    config = self._finalize_config()\n    if src_vocab is None and tgt_vocab is None:\n      return config[""model_dir""]\n\n    model = self._init_model(config)\n    optimizer = model.get_optimizer()\n    cur_checkpoint = checkpoint_util.Checkpoint.from_config(config, model, optimizer=optimizer)\n    cur_checkpoint.restore()\n    model.create_variables(optimizer=optimizer)\n\n    self._config[""model_dir""] = output_dir\n    if src_vocab is not None:\n      self._config[""data""][""source_vocabulary""] = src_vocab\n    if tgt_vocab is not None:\n      self._config[""data""][""target_vocabulary""] = tgt_vocab\n    new_config = self._finalize_config()\n    new_model = self._init_model(new_config)\n    new_optimizer = new_model.get_optimizer()\n    new_checkpoint = checkpoint_util.Checkpoint.from_config(\n        new_config, new_model, optimizer=new_optimizer)\n    new_model.create_variables(optimizer=new_optimizer)\n\n    model.transfer_weights(new_model, new_optimizer=new_optimizer, optimizer=optimizer)\n    new_optimizer.iterations.assign(optimizer.iterations)\n    new_checkpoint.save()\n    return output_dir\n\n  def infer(self,\n            features_file,\n            predictions_file=None,\n            checkpoint_path=None,\n            log_time=False):\n    """"""Runs inference.\n\n    Args:\n      features_file: The file(s) to infer from.\n      predictions_file: If set, predictions are saved in this file, otherwise\n        they are printed on the standard output.\n      checkpoint_path: Path of a specific checkpoint to predict. If ``None``,\n        the latest is used.\n      log_time: If ``True``, several time metrics will be printed in the logs at\n        the end of the inference loop.\n    """"""\n    config = self._finalize_config()\n    model = self._init_model(config)\n    checkpoint = checkpoint_util.Checkpoint.from_config(config, model)\n    checkpoint.restore(checkpoint_path=checkpoint_path, weights_only=True)\n    infer_config = config[""infer""]\n    dataset = model.examples_inputter.make_inference_dataset(\n        features_file,\n        infer_config[""batch_size""],\n        length_bucket_width=infer_config[""length_bucket_width""],\n        prefetch_buffer_size=infer_config.get(""prefetch_buffer_size""))\n    inference.predict_dataset(\n        model,\n        dataset,\n        print_params=infer_config,\n        predictions_file=predictions_file,\n        log_time=log_time)\n\n  def export(self, export_dir, checkpoint_path=None, exporter=None):\n    """"""Exports a model.\n\n    Args:\n      export_dir: The export directory.\n      checkpoint_path: The checkpoint path to export. If ``None``, the latest is used.\n      exporter: A :class:`opennmt.utils.Exporter` instance. Defaults to\n        :class:`opennmt.utils.SavedModelExporter`.\n   """"""\n    config = self._finalize_config()\n    model = self._init_model(config)\n    checkpoint = checkpoint_util.Checkpoint.from_config(config, model)\n    checkpoint.restore(checkpoint_path=checkpoint_path, weights_only=True)\n    model.export(export_dir, exporter=exporter)\n\n  def score(self, features_file, predictions_file, checkpoint_path=None, output_file=None):\n    """"""Scores existing predictions.\n\n    Args:\n      features_file: The input file.\n      predictions_file: The predictions file to score.\n      checkpoint_path: Path of a specific checkpoint to use. If ``None``,\n        the latest is used.\n      output_file: The file where the scores are saved. Otherwise, they will be\n        printed on the standard output.\n    """"""\n    config = self._finalize_config()\n    model = self._init_model(config)\n    checkpoint = checkpoint_util.Checkpoint.from_config(config, model)\n    checkpoint.restore(checkpoint_path=checkpoint_path, weights_only=True)\n    score_config = config[""score""]\n    dataset = model.examples_inputter.make_evaluation_dataset(\n        features_file,\n        predictions_file,\n        score_config[""batch_size""],\n        prefetch_buffer_size=score_config.get(""prefetch_buffer_size""))\n    inference.score_dataset(model, dataset, print_params=score_config, output_file=output_file)\n\n\ndef _count_batch_accum(batch_size, target_batch_size, num_replicas=1):\n  """"""Given the current batch size, the number of replicas, and the requested\n  effective batch size, returns the number of gradients to accumulate.\n  """"""\n  return int(math.ceil(float(target_batch_size) / (batch_size * num_replicas)))\n\ndef _auto_tune_batch_size(config,\n                          min_batch_size=1024,\n                          max_batch_size=16384,\n                          min_range=256,\n                          sample_iterations=10,\n                          num_devices=1,\n                          scaling_factor=0.8,\n                          mixed_precision=False):\n  """"""Find the largest token-based batch size that can be used with this\n  configuration.\n\n  This function runs some training iterations and uses out-of-memory errors as\n  search conditions. A binary search is used to converge to a suitable batch\n  size.\n\n  We prefer to run the iterations in a different process so that it does not\n  alter the current context (OOM may not be safe to recover from, see for\n  example https://stackoverflow.com/q/53820713/2529808).\n\n  Args:\n    config: The training configuration.\n    min_batch_size: The smallest batch size to consider.\n    max_batch_size: The largest batch size to consider.\n    min_range: Continue searching while the difference between\n      :obj:`max_batch_size` and :obj:`min_batch_size` is larger than this value.\n    sample_iterations: The number of training iterations.\n    num_devices: The number of devices to use.\n    scaling_factor: Scale the found batch size by this value.\n    mixed_precision: If ``True``, run the autotuning with mixed precision.\n\n  Returns:\n    The autotuned batch size.\n  """"""\n  model_dir = config[""model_dir""]\n  with tempfile.TemporaryDirectory() as tmpdir:\n    config = copy.deepcopy(config)\n    config[""model_dir""] = tmpdir\n    config[""train""][""save_checkpoints_steps""] = None\n    config[""train""][""average_last_checkpoints""] = 0\n    config[""train""][""max_step""] = sample_iterations\n    config_path = os.path.join(config[""model_dir""], ""batch_size_autotuner.yml"")\n    model_description = os.path.join(model_dir, ""model_description.py"")\n\n    args = [\n        sys.executable or ""python"",\n        ""-m"", ""opennmt.bin.main"",\n        ""--config"", config_path,\n        ""--model"", model_description,\n        ""--checkpoint_path"", model_dir,\n    ]\n    if mixed_precision:\n      args.extend([""--mixed_precision""])\n    args.extend([\n        ""train"",\n        ""--num_gpus"", str(num_devices),\n    ])\n\n    tf.get_logger().info(\n        ""Searching the largest batch size between %d and %d with a precision of %d..."",\n        min_batch_size, max_batch_size, min_range)\n\n    while max_batch_size - min_batch_size > min_range:\n      batch_size = (max_batch_size + min_batch_size) // 2\n\n      # Update configuration with current batch size and adjusted gradients\n      # accumulation.\n      config[""train""][""batch_size""] = batch_size\n      with tf.io.gfile.GFile(config_path, mode=""wb"") as config_file:\n        yaml.dump(config, config_file)\n\n      tf.get_logger().info(""Trying training with batch size %d..."", batch_size)\n      time.sleep(1)\n      with open(os.devnull, ""w"") as devnull:\n        process = subprocess.Popen(args, stdout=devnull, stderr=devnull)\n        exit_code = process.wait()\n\n      if exit_code != 0:\n        tf.get_logger().info(""... failed."")\n        max_batch_size = batch_size - 1\n      else:\n        tf.get_logger().info(\n            ""... succeeded, continue until the search range is smaller than %d."", min_range)\n        min_batch_size = batch_size\n\n  batch_size = int(scaling_factor * min_batch_size)\n  tf.get_logger().info(""Batch size auto tuned to %d."", batch_size)\n  return batch_size\n'"
opennmt/training.py,69,"b'""""""Training related classes and functions.""""""\n\nimport contextlib\nimport itertools\nimport time\n\nimport tensorflow as tf\n\nfrom opennmt.optimizers import utils as optimizer_util\nfrom opennmt.utils import misc\n\n\nclass Trainer:\n  """"""Base class for model trainer, implementing single-GPU training.""""""\n\n  def __init__(self, model, optimizer, checkpoint=None, is_master=True):\n    """"""Initializes the trainer.\n\n    Args:\n      model: A :class:`opennmt.models.Model` instance to train.\n      optimizer: A ``tf.keras.optimizers.Optimizer`` instance.\n      checkpoint: A :class:`opennmt.utils.checkpoint.Checkpoint` instance. If\n        not set, no checkpoints will be saved.\n      is_master: Whether this trainer instance is the master trainer.\n    """"""\n    self._checkpoint = checkpoint\n    self._is_master = is_master\n    self._model = model\n    if checkpoint is not None:\n      self._summary_writer = tf.summary.create_file_writer(checkpoint.model_dir)\n    else:\n      self._summary_writer = tf.summary.create_noop_writer()\n    self._words_counters = {}\n    self._gradient_accumulator = optimizer_util.GradientAccumulator()\n\n    if optimizer is None:\n      raise ValueError(""No optimizer is defined"")\n    graph_optimizer_options = tf.config.optimizer.get_experimental_options()\n    mixed_precision_enabled = graph_optimizer_options.get(""auto_mixed_precision"")\n    if (mixed_precision_enabled\n        and not isinstance(optimizer, tf.keras.mixed_precision.experimental.LossScaleOptimizer)):\n      optimizer = _LossScaleOptimizer(optimizer, ""dynamic"")\n    self._optimizer = optimizer\n\n  @property\n  def num_replicas(self):\n    """"""Number of synchronous training replicas.""""""\n    return 1\n\n  def __call__(self,\n               dataset,\n               max_step=None,\n               accum_steps=1,\n               report_steps=100,\n               save_steps=5000,\n               evaluator=None,\n               eval_steps=5000,\n               moving_average_decay=None):\n    """"""Runs the training.\n\n    Args:\n      dataset: A ``tf.data.Dataset`` or a function taking a ``tf.distribute.InputContext``\n        instance and returning a ``tf.data.Dataset``.\n      max_step: The final training step.\n      accum_steps: The number of gradient accumulation steps.\n      report_steps: Report status every this many steps.\n      save_steps: Save a checkpoint every this many steps.\n      evaluator: A :class:`opennmt.evaluation.Evaluator` instance to call for\n        evaluation.\n      eval_steps: Evaluate every this many steps.\n      moving_average_decay: If set, maintain an exponential moving average of the model\n        variables using this decay value (usually close to 1, e.g. 0.9999). See\n        https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage.\n    """"""\n    if max_step is not None and self._optimizer.iterations.numpy() >= max_step:\n      tf.get_logger().warning(""Model already reached max_step = %d. Exiting."", max_step)\n      return\n    if evaluator is not None and evaluator.should_stop():\n      tf.get_logger().warning(""Early stopping conditions are already met. Exiting."")\n      return\n\n    self._gradient_accumulator.reset()\n    self._words_counters.clear()\n\n    with self._summary_writer.as_default():\n      iterations = self._optimizer.iterations\n      tf.summary.experimental.set_step(iterations)\n\n      step = None\n      moving_average = None\n      last_report_step = iterations.numpy()\n      last_report_time = time.time()\n      for loss in self._steps(dataset, accum_steps=accum_steps, report_steps=report_steps):\n        if tf.math.is_nan(loss):\n          raise RuntimeError(""Model diverged with loss = NaN."")\n\n        if moving_average_decay is not None and self._is_master:\n          if moving_average is None:\n            moving_average = MovingAverage(\n                self._model.trainable_variables,\n                iterations,\n                decay=moving_average_decay)\n          else:\n            moving_average.update()\n\n        step = iterations.numpy()\n        if step % report_steps == 0:\n          words_counters = self._get_words_counters()\n          if self._is_master:\n            _report_training_status(\n                step,\n                loss,\n                self._optimizer.learning_rate,\n                words_counters,\n                last_report_step,\n                last_report_time)\n            last_report_step = step\n            last_report_time = time.time()\n        if step == 1 or (save_steps is not None and step % save_steps == 0):\n          self._save_checkpoint(step, moving_average=moving_average)\n        if eval_steps is not None and step % eval_steps == 0:\n          early_stop = self._evaluate(evaluator, step, moving_average=moving_average)\n          if early_stop:\n            tf.get_logger().warning(""Early stopping conditions are met. Exiting."")\n            break\n        if step == max_step:\n          break\n\n      if step is None:\n        raise RuntimeError(""No training steps were executed. This usually means the ""\n                           ""training file is empty or all examples were filtered out. ""\n                           ""For the latter, verify that the maximum_*_length values are ""\n                           ""consistent with your data."")\n      self._save_checkpoint(step, moving_average=moving_average)\n      self._evaluate(evaluator, step, moving_average=moving_average)\n\n  def _save_checkpoint(self, step, moving_average=None):\n    """"""Saves a checkpoint for step.""""""\n    if not self._is_master or self._checkpoint is None or step == self._checkpoint.last_saved_step:\n      return\n    with moving_average.shadow_variables() if moving_average is not None else contextlib.suppress():\n      self._checkpoint.save(step)\n\n  def _evaluate(self, evaluator, step, moving_average=None):\n    """"""Runs evaluation for step. Returns ``True`` is early conditions are met.""""""\n    if not self._is_master or evaluator is None or step == evaluator.last_evaluated_step:\n      return False\n    with moving_average.shadow_variables() if moving_average is not None else contextlib.suppress():\n      evaluator(step)\n      return evaluator.should_stop()\n\n  def _finalize_dataset(self, dataset):\n    """"""Returns the final dataset instance to be used for training.\n\n    Args:\n      dataset: A ``tf.data.Dataset`` or a function taking a ``tf.distribute.InputContext``\n        instance and returning a ``tf.data.Dataset``.\n\n    Returns:\n      A ``tf.data.Dataset``.\n    """"""\n    if callable(dataset):\n      dataset = dataset(tf.distribute.InputContext())\n    return dataset\n\n  def _steps(self, dataset, accum_steps=1, report_steps=None):\n    """"""Returns a generator over training steps (i.e. parameters update).\n\n    Args:\n      dataset: The training dataset.\n      accum_steps: Accumulate the gradients of this many steps/batches.\n      report_steps: Report summary statistics every this many steps. This should\n        typically be used in a ``tf.summary.record_if`` context.\n\n    Returns:\n      A generator that yields a loss value to report for this step.\n    """"""\n    dataset = self._finalize_dataset(dataset)\n    iterator = iter(dataset)\n\n    # Wrap forward and step with tf.function.\n    # We get the next dataset element within the function for increased efficiency.\n\n    if accum_steps is None or accum_steps == 1:\n\n      @tf.function\n      def _forward_and_step():\n        source, target = next(iterator)\n        return self._forward_and_step(\n            source,\n            target,\n            record_summaries=self._should_record_summaries(report_steps, with_accum=False))\n\n      for i in itertools.count():\n        try:\n          loss = _forward_and_step()\n        except tf.errors.OutOfRangeError:  # Dataset iterator exhausted.\n          break\n        if i == 0:\n          self._broadcast_variables()\n        yield loss\n\n    else:\n\n      @tf.function\n      def _forward():\n        source, target = next(iterator)\n        return self._forward(\n            source,\n            target,\n            record_summaries=self._should_record_summaries(report_steps, with_accum=True))\n\n      @tf.function\n      def _step():\n        return self._step()\n\n      for i in itertools.count():\n        try:\n          loss = _forward()\n        except tf.errors.OutOfRangeError:  # Dataset iterator exhausted.\n          break\n        if i == 0 or (i + 1) % accum_steps == 0:\n          _step()\n          if i == 0:\n            self._broadcast_variables()\n          yield loss\n\n  def _run_model(self, source, target):\n    """"""Computes the loss of the given source and target pair.\n\n    Args:\n      source: A nested structure of tensors.\n      target: A nested structure of tensors.\n\n    Returns:\n      A tuple containing,\n\n      - The loss to compute the gradients.\n      - The loss to report.\n    """"""\n    first_call = not self._model.built\n    outputs, _ = self._model(\n        source,\n        labels=target,\n        training=True,\n        step=self._optimizer.iterations)\n    loss = self._model.compute_loss(outputs, target, training=True)\n    if isinstance(loss, tuple):\n      training_loss = loss[0] / loss[1]\n      reported_loss = loss[0] / loss[2] if len(loss) > 2 else training_loss\n    else:\n      training_loss, reported_loss = loss, loss\n    training_loss = self._model.regularize_loss(\n        training_loss, variables=self._model.trainable_variables)\n    self._update_words_counter(""source"", source)\n    if not self._model.unsupervised:\n      self._update_words_counter(""target"", target)\n    if first_call and self._is_master:\n      if self._checkpoint is not None:\n        self._model.visualize(self._checkpoint.model_dir)\n      tf.get_logger().info(""Number of model parameters: %d"", self._model.count_params())\n      tf.get_logger().info(\n          ""Number of model weights: %d (trainable = %d, non trainable = %d)"",\n          len(self._model.weights),\n          len(self._model.trainable_weights),\n          len(self._model.non_trainable_weights))\n    return training_loss, reported_loss\n\n  def _should_record_summaries(self, report_steps, with_accum=False):\n    """"""Returns a boolean tensor to be used in tf.summary.record_if.""""""\n    if report_steps is None or not self._is_master:\n      return False\n    record_summaries = tf.equal(self._optimizer.iterations % report_steps, 0)\n    if with_accum:\n      record_summaries = tf.logical_and(\n          record_summaries,\n          tf.equal(self._gradient_accumulator.step, 0))\n    return record_summaries\n\n  def _compute_gradients(self, source, target, record_summaries=False):\n    """"""Computes the gradient of a training example.""""""\n    with tf.summary.record_if(record_summaries):\n      training_loss, reported_loss = self._run_model(source, target)\n      gradients = self._optimizer.get_gradients(training_loss, self._model.trainable_variables)\n      _summarize_gradients(gradients, record_summaries)\n    return reported_loss, gradients\n\n  def _apply_gradients(self, gradients, scale=1):\n    """"""Applies the gradients.""""""\n    gradient_scale = scale * self.num_replicas\n    if tf.is_tensor(gradient_scale) or gradient_scale != 1:\n      gradients = [\n          self._all_reduce_sum(gradient / tf.cast(gradient_scale, gradient.dtype))\n          for gradient in gradients]\n    self._optimizer.apply_gradients(list(zip(gradients, self._model.trainable_variables)))\n\n  def _forward_and_step(self, source, target, record_summaries=False):\n    """"""Forwards a training example and applies the gradients, without accumulation.""""""\n    loss, gradients = self._compute_gradients(source, target, record_summaries=record_summaries)\n    self._apply_gradients(gradients)\n    return loss\n\n  def _forward(self, source, target, record_summaries=False):\n    """"""Forwards a training example and accumulates the gradients.""""""\n    loss, gradients = self._compute_gradients(source, target, record_summaries=record_summaries)\n    self._gradient_accumulator(gradients)\n    return loss\n\n  def _step(self):\n    """"""Applies gradients and resets accumulation.""""""\n    self._apply_gradients(\n        self._gradient_accumulator.gradients,\n        scale=self._gradient_accumulator.step)\n    self._gradient_accumulator.reset()\n\n  def _update_words_counter(self, name, features):\n    """"""Accumulates number of source and target tokens to report throughput.""""""\n    length = features.get(""length"")\n    if length is None:\n      return\n    num_words = tf.reduce_sum(length)\n    counter = self._words_counters.get(name)\n    if counter is None:\n      counter = tf.Variable(\n          tf.constant(0, dtype=tf.int64),\n          trainable=False,\n          synchronization=tf.VariableSynchronization.ON_READ,\n          aggregation=tf.VariableAggregation.SUM)\n      self._words_counters[name] = counter\n    counter.assign_add(tf.cast(num_words, tf.int64))\n\n  @tf.function\n  def _get_words_counters(self):\n    """"""Returns the accumulated words counters and resets them.\n\n    This is used to report the words per second in the training logs.\n\n    Returns:\n      A dictionary mapping a counter name to a Python value.\n    """"""\n    counters = {}\n    for name, counter in self._words_counters.items():\n      counters[name] = self._all_reduce_sum(counter.read_value())\n      counter.assign(tf.constant(0, dtype=tf.int64))\n    return counters\n\n  def _broadcast_variables(self):\n    """"""Broadcasts variables to other replicas, if required.""""""\n    return\n\n  def _all_reduce_sum(self, value):\n    """"""Reduces the value across all replicas.""""""\n    return value\n\n\nclass HorovodTrainer(Trainer):\n  """"""Trainer compatible with Horovod distributed training.""""""\n\n  def __init__(self, model, optimizer, hvd, checkpoint=None):\n    """"""Initializes the Horovod trainer.\n\n    Args:\n      model: A :class:`opennmt.models.Model` instance to train.\n      optimizer: A ``tf.keras.optimizers.Optimizer`` instance.\n      hvd: The global Horovod object.\n      checkpoint: A :class:`opennmt.utils.checkpoint.Checkpoint` instance. If\n        not set, no checkpoints will be saved.\n    """"""\n    super().__init__(model, optimizer, checkpoint=checkpoint, is_master=hvd.rank() == 0)\n    self._hvd = hvd\n\n  @property\n  def num_replicas(self):\n    return self._hvd.size()\n\n  def _finalize_dataset(self, dataset):\n    if callable(dataset):\n      dataset = dataset(tf.distribute.InputContext(\n          num_input_pipelines=self._hvd.size(),\n          input_pipeline_id=self._hvd.rank(),\n          num_replicas_in_sync=self._hvd.size()))\n    return dataset\n\n  def _broadcast_variables(self):\n    self._hvd.broadcast_variables(self._model.variables, root_rank=0)\n    self._hvd.broadcast_variables(self._optimizer.variables(), root_rank=0)\n\n  def _all_reduce_sum(self, value):\n    return self._hvd.allreduce(value, op=self._hvd.Sum)\n\n\nclass MirroredStrategyTrainer(Trainer):\n  """"""Trainer based on ``tf.distribute.MirroredStrategy`` for local multi-GPU training.""""""\n\n  def __init__(self, model, optimizer, checkpoint=None, devices=None):\n    """"""Initializes the MirroredStrategy trainer.\n\n    Args:\n      model: A :class:`opennmt.models.Model` instance to train.\n      optimizer: A ``tf.keras.optimizers.Optimizer`` instance.\n      checkpoint: A :class:`opennmt.utils.checkpoint.Checkpoint` instance. If\n        not set, no checkpoints will be saved.\n      devices: List of device strings to use for training. If not set, all\n        visible GPUs are used.\n    """"""\n    super().__init__(model, optimizer, checkpoint=checkpoint)\n    self._strategy = tf.distribute.MirroredStrategy(devices=devices)\n    with self._strategy.scope():\n      # Create some variables under the strategy scope.\n      _ = self._optimizer.iterations\n\n  @property\n  def num_replicas(self):\n    return self._strategy.num_replicas_in_sync\n\n  def _finalize_dataset(self, dataset):\n    # We prefer not to use experimental_distribute_dataset here because it\n    # sometimes fails to split the batches (noticed with tokens batch type).\n    # We also assume for now that we are training with a single worker\n    # otherwise we would need to correctly shard the input dataset.\n    dataset_fn = dataset if callable(dataset) else lambda _: dataset\n    return self._strategy.experimental_distribute_datasets_from_function(dataset_fn)\n\n  def _forward_and_step(self, source, target, record_summaries=False):\n    per_replica_loss = self._strategy.run(\n        super()._forward_and_step,\n        args=(source, target),\n        kwargs=dict(record_summaries=record_summaries))\n    return self._strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_loss, None)\n\n  def _forward(self, source, target, record_summaries=False):\n    per_replica_loss = self._strategy.run(\n        super()._forward,\n        args=(source, target),\n        kwargs=dict(record_summaries=record_summaries))\n    # TODO: this reduction could be delayed until _step is called.\n    return self._strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_loss, None)\n\n  def _step(self):\n    self._strategy.run(super()._step)\n\n\ndef _report_training_status(step,\n                            loss,\n                            learning_rate,\n                            words_counters,\n                            last_report_step,\n                            last_report_time):\n  elapsed_time = time.time() - last_report_time\n\n  steps_per_sec = (step - last_report_step) / elapsed_time\n  tf.summary.scalar(""steps_per_sec"", steps_per_sec, description=""Training steps per second"")\n  steps_per_sec_fmt = ""steps/s = %0.2f"" % steps_per_sec\n\n  words_per_sec_fmt = []\n  for name, counter in words_counters.items():\n    avg = int(counter.numpy() / elapsed_time)\n    tf.summary.scalar(\n        ""words_per_sec/%s"" % name,\n        avg,\n        description=""%s words per second"" % name.capitalize())\n    words_per_sec_fmt.append(""%s words/s = %d"" % (name, avg))\n\n  if isinstance(learning_rate, tf.optimizers.schedules.LearningRateSchedule):\n    learning_rate = learning_rate(step)\n  elif isinstance(learning_rate, tf.Variable):\n    learning_rate = learning_rate.value()\n\n  tf.get_logger().info(\n      ""Step = %d ; %s ; Learning rate = %f ; Loss = %f"",\n      step,\n      "", "".join([steps_per_sec_fmt] + list(sorted(words_per_sec_fmt))),\n      learning_rate,\n      loss)\n  tf.summary.scalar(""loss"", loss, description=""Training loss"")\n  tf.summary.scalar(""optim/learning_rate"", learning_rate, description=""Learning rate"")\n\ndef _summarize_gradients(gradients, should_record):\n  # Only compute the gradients global norm when the value is actually recorded.\n  if isinstance(should_record, bool) and not should_record:\n    return\n  tf.summary.scalar(\n      ""gradients/global_norm"",\n      tf.cond(\n          should_record,\n          true_fn=lambda: tf.linalg.global_norm(gradients),\n          false_fn=lambda: tf.constant(0, dtype=gradients[0].dtype)))\n\n\nclass MovingAverage(object):\n  """"""Object holding an exponential moving average of variables.""""""\n\n  def __init__(self, variables, step, decay=0.9999):\n    """"""Initializes the moving average object.\n\n    Args:\n      variables: The list of variable for which to maintain a moving average.\n      step: The training step counter as a ``tf.Variable``.\n      decay: The decay rate of the exponential moving average. Usually close to\n        1, e.g. 0.9999, see the complete formula on\n        https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage.\n\n    Raises:\n      TypeError: is :obj:`step` is not a ``tf.Variable``.\n    """"""\n    if not isinstance(step, tf.Variable):\n      raise TypeError(""step should be a tf.Variable"")\n    if decay < 0.9 or decay > 1:\n      tf.get_logger().warning(""Moving average decay should be close to 1 (e.g. 0.9999) but you ""\n                              ""passed %f, is it correct? See https://www.tensorflow.org/api_docs""\n                              ""/python/tf/train/ExponentialMovingAverage for details about the ""\n                              ""formula and recommended decay values."")\n    self._ema = tf.train.ExponentialMovingAverage(decay, num_updates=step)\n    self._variables = variables\n    self.update()\n\n  @tf.function\n  def update(self):\n    """"""Updates the moving average of the variables.""""""\n    self._ema.apply(var_list=list(map(misc.get_primary_variable, self._variables)))\n\n  @contextlib.contextmanager\n  def shadow_variables(self):\n    """"""Returns a context manager that assigns the variables to their moving\n    average value on enter and restores the previous value on exit.\n\n    Returns:\n      A context manager.\n    """"""\n    # TODO: Do we want to shadow the values on all replicas?\n    previous_values = []\n    for variable in self._variables:\n      previous_values.append(variable.value())\n      variable.assign(self._ema.average(misc.get_primary_variable(variable)))\n    yield\n    for previous_value, variable in zip(previous_values, self._variables):\n      variable.assign(previous_value)\n\n\nclass _LossScaleOptimizer(tf.keras.mixed_precision.experimental.LossScaleOptimizer):\n  # TODO: Remove this wrapper when this fix is released:\n  # https://github.com/tensorflow/tensorflow/commit/d1dd08dd2807ac80a4508686618419826463374b\n\n  def get_unscaled_gradients(self, grads):\n    loss_scale_reciprocal = 1. / self._loss_scale()\n    return [\n        _multiply_gradient(g, loss_scale_reciprocal) if g is not None else None\n        for g in grads\n    ]\n\n\ndef _multiply_gradient(gradient, scale):\n  """"""Multiply a (possibly sparse) gradient by the given scale factor.""""""\n  scale = tf.cast(scale, gradient.dtype)\n  if isinstance(gradient, tf.IndexedSlices):\n    return tf.IndexedSlices(\n        gradient.values * scale,\n        gradient.indices,\n        dense_shape=gradient.dense_shape)\n  else:\n    return gradient * scale\n'"
tools/update_version.py,0,"b'from __future__ import print_function\n\nimport argparse\nimport datetime\nimport re\n\n\nSRC_DIR = "".""\nROOT_INIT = ""%s/opennmt/__init__.py"" % SRC_DIR\nSETUP_PY = ""%s/setup.py"" % SRC_DIR\nDOCS_CONF = ""%s/docs/conf.py"" % SRC_DIR\nCHANGELOG = ""%s/CHANGELOG.md"" % SRC_DIR\n\n\ndef get_current_version():\n  with open(ROOT_INIT, ""r"") as init_file:\n    for line in init_file:\n      version_match = re.search(\'^__version__ = ""(.+)""\', line)\n      if version_match:\n        return version_match.group(1)\n\ndef replace_string_in_file(pattern, replace, path):\n  with open(path, ""r"") as f:\n    content = f.read()\n  with open(path, ""w"") as f:\n    f.write(re.sub(pattern, replace, content))\n\ndef split_version(version):\n  return version.split(""."")\n\ndef join_version(version):\n  return ""."".join(version)\n\ndef get_short_version(version):\n  version = split_version(version)\n  return join_version(version[0:2])\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""version"", help=""New version to release."")\n  args = parser.parse_args()\n\n  current_version = get_current_version()\n  new_version = args.version\n  print(""Updating version strings from %s to %s"" % (current_version, new_version))\n\n  replace_string_in_file(\'__version__ = ""%s""\' % current_version,\n                         \'__version__ = ""%s""\' % new_version,\n                         ROOT_INIT)\n  replace_string_in_file(\'version=""%s""\' % current_version,\n                         \'version=""%s""\' % new_version,\n                         SETUP_PY)\n  replace_string_in_file(\'release = ""%s""\' % current_version,\n                         \'release = ""%s""\' % new_version,\n                         DOCS_CONF)\n  replace_string_in_file(\'version = ""%s""\' % get_short_version(current_version),\n                         \'version = ""%s""\' % get_short_version(new_version),\n                         DOCS_CONF)\n  replace_string_in_file(""## \\[Unreleased\\]"",\n                         ""## [Unreleased]\\n""\n                         ""\\n""\n                         ""### New features\\n""\n                         ""\\n""\n                         ""### Fixes and improvements\\n""\n                         ""\\n""\n                         ""## [%s](https://github.com/OpenNMT/OpenNMT-tf/releases/tag/v%s) (%s)"" % (\n                             new_version, new_version, datetime.datetime.now().strftime(""%Y-%m-%d"")),\n                         CHANGELOG)\n\n  print(\'git tag -a v%s -m ""%s release""\' % (new_version, new_version))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
config/models/multi_features_transformer.py,0,"b'""""""Defines a Transformer model with multiple input features. For example, these\ncould be words, parts of speech, and lemmas that are embedded in parallel and\nconcatenated into a single input embedding.\n\nThe features are separate data files with separate vocabularies. The YAML\nconfiguration file should look like this:\n\ndata:\n  train_features_file:\n    - features_1.txt\n    - features_2.txt\n    - features_3.txt\n  train_labels_file: target.txt\n  source_1_vocabulary: feature_1_vocab.txt\n  source_2_vocabulary: feature_2_vocab.txt\n  source_3_vocabulary: feature_3_vocab.txt\n  target_vocabulary: target_vocab.txt\n""""""\n\nimport tensorflow as tf\nimport opennmt as onmt\n\ndef model():\n  return onmt.models.Transformer(\n      source_inputter=onmt.inputters.ParallelInputter([\n          onmt.inputters.WordEmbedder(embedding_size=512),\n          onmt.inputters.WordEmbedder(embedding_size=16),\n          onmt.inputters.WordEmbedder(embedding_size=64)],\n          reducer=onmt.layers.ConcatReducer()),\n      target_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n      num_layers=6,\n      num_units=512,\n      num_heads=8,\n      ffn_inner_dim=2048,\n      dropout=0.1,\n      attention_dropout=0.1,\n      ffn_dropout=0.1)\n'"
config/models/multi_source_transformer.py,0,"b'""""""Defines a dual source Transformer architecture with serial attention layers\nand parameter sharing between the encoders.\n\nSee for example https://arxiv.org/pdf/1809.00188.pdf.\n\nThe YAML configuration file should look like this:\n\ndata:\n  train_features_file:\n    - source_1.txt\n    - source_2.txt\n  train_labels_file: target.txt\n  source_1_vocabulary: source_1_vocab.txt\n  source_2_vocabulary: source_2_vocab.txt\n  target_vocabulary: target_vocab.txt\n""""""\n\nimport opennmt as onmt\n\nfrom opennmt.utils import misc\n\n\nclass DualSourceTransformer(onmt.models.Transformer):\n\n  def __init__(self):\n    super(DualSourceTransformer, self).__init__(\n      source_inputter=onmt.inputters.ParallelInputter([\n          onmt.inputters.WordEmbedder(embedding_size=512),\n          onmt.inputters.WordEmbedder(embedding_size=512)]),\n      target_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n      num_layers=6,\n      num_units=512,\n      num_heads=8,\n      ffn_inner_dim=2048,\n      dropout=0.1,\n      attention_dropout=0.1,\n      ffn_dropout=0.1,\n      share_encoders=True)\n\n  def auto_config(self, num_replicas=1):\n    config = super(DualSourceTransformer, self).auto_config(num_replicas=num_replicas)\n    max_length = config[""train""][""maximum_features_length""]\n    return misc.merge_dict(config, {\n        ""train"": {\n            ""maximum_features_length"": [max_length, max_length]\n        }\n    })\n\n\nmodel = DualSourceTransformer\n'"
config/models/template.py,0,"b'""""""Template file from model configurations.""""""\n\n# You usually want to import tensorflow and opennmt modules.\nimport tensorflow as tf\nimport opennmt as onmt\n\ndef model():\n  """"""Builds a model.\n\n  Returns:\n    A `opennmt.models.Model`.\n  """"""\n  pass\n'"
config/models/transformer_shared_embedding.py,0,"b'""""""A Transformer model sharing all embeddings and softmax weights.""""""\n\nimport tensorflow as tf\nimport opennmt as onmt\n\ndef model():\n  return onmt.models.Transformer(\n      source_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n      target_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n      num_layers=6,\n      num_units=512,\n      num_heads=8,\n      ffn_inner_dim=2048,\n      dropout=0.1,\n      attention_dropout=0.1,\n      ffn_dropout=0.1,\n      share_embeddings=onmt.models.EmbeddingsSharingLevel.ALL)\n'"
examples/library/custom_transformer_training.py,11,"b'""""""This example demonstrates how to train a Transformer model with a custom\ntraining loop in about 200 lines of code.\n\nThe purpose of this example is to showcase selected lower-level OpenNMT-tf APIs\nthat can be useful in other projects:\n\n* efficient training dataset (with shuffling, bucketing, batching, prefetching, etc.)\n* inputter/encoder/decoder API\n* dynamic decoding API\n\nProducing a SOTA model is NOT a goal: this usually requires extra steps such as\ntraining a bigger model, using a larger batch size via multi GPU training and/or\ngradient accumulation, etc.\n""""""\n\nimport argparse\nimport logging\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport opennmt as onmt\n\ntf.get_logger().setLevel(logging.INFO)\n\n\n# Define the model. For the purpose of this example, the model components\n# (encoder, decoder, etc.) will be called separately.\nmodel = onmt.models.SequenceToSequence(\n    source_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n    target_inputter=onmt.inputters.WordEmbedder(embedding_size=512),\n    encoder=onmt.encoders.SelfAttentionEncoder(\n        num_layers=6,\n        num_units=512,\n        num_heads=8,\n        ffn_inner_dim=2048,\n        dropout=0.1,\n        attention_dropout=0.1,\n        ffn_dropout=0.1),\n    decoder=onmt.decoders.SelfAttentionDecoder(\n        num_layers=6,\n        num_units=512,\n        num_heads=8,\n        ffn_inner_dim=2048,\n        dropout=0.1,\n        attention_dropout=0.1,\n        ffn_dropout=0.1))\n\n\n# Define the learning rate schedule and the optimizer.\nlearning_rate = onmt.schedules.NoamDecay(scale=2.0, model_dim=512, warmup_steps=8000)\noptimizer = tfa.optimizers.LazyAdam(learning_rate)\n\n# Track the model and optimizer weights.\ncheckpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n\n\ndef train(source_file,\n          target_file,\n          checkpoint_manager,\n          maximum_length=100,\n          shuffle_buffer_size=-1,  # Uniform shuffle.\n          train_steps=100000,\n          save_every=1000,\n          report_every=100):\n  """"""Runs the training loop.\n\n  Args:\n    source_file: The source training file.\n    target_file: The target training file.\n    checkpoint_manager: The checkpoint manager.\n    maximum_length: Filter sequences longer than this.\n    shuffle_buffer_size: How many examples to load for shuffling.\n    train_steps: Train for this many iterations.\n    save_every: Save a checkpoint every this many iterations.\n    report_every: Report training progress every this many iterations.\n  """"""\n\n  # Create the training dataset.\n  dataset = model.examples_inputter.make_training_dataset(\n      source_file,\n      target_file,\n      batch_size=3072,\n      batch_type=""tokens"",\n      shuffle_buffer_size=shuffle_buffer_size,\n      length_bucket_width=1,  # Bucketize sequences by the same length for efficiency.\n      maximum_features_length=maximum_length,\n      maximum_labels_length=maximum_length)\n\n  @tf.function(input_signature=dataset.element_spec)\n  def training_step(source, target):\n    # Run the encoder.\n    source_inputs = model.features_inputter(source, training=True)\n    encoder_outputs, _, _ = model.encoder(\n        source_inputs,\n        source[""length""],\n        training=True)\n\n    # Run the decoder.\n    target_inputs = model.labels_inputter(target, training=True)\n    decoder_state = model.decoder.initial_state(\n        memory=encoder_outputs,\n        memory_sequence_length=source[""length""])\n    logits, _, _ = model.decoder(\n        target_inputs,\n        target[""length""],\n        state=decoder_state,\n        training=True)\n\n    # Compute the cross entropy loss.\n    loss_num, loss_den, _ = onmt.utils.cross_entropy_sequence_loss(\n        logits,\n        target[""ids_out""],\n        target[""length""],\n        label_smoothing=0.1,\n        average_in_time=True,\n        training=True)\n    loss = loss_num / loss_den\n\n    # Compute and apply the gradients.\n    variables = model.trainable_variables\n    gradients = optimizer.get_gradients(loss, variables)\n    optimizer.apply_gradients(list(zip(gradients, variables)))\n    return loss\n\n  # Runs the training loop.\n  for source, target in dataset:\n    loss = training_step(source, target)\n    step = optimizer.iterations.numpy()\n    if step % report_every == 0:\n      tf.get_logger().info(\n          ""Step = %d ; Learning rate = %f ; Loss = %f"",\n          step, learning_rate(step), loss)\n    if step % save_every == 0:\n      tf.get_logger().info(""Saving checkpoint for step %d"", step)\n      checkpoint_manager.save(checkpoint_number=step)\n    if step == train_steps:\n      break\n\n\ndef translate(source_file,\n              batch_size=32,\n              beam_size=4):\n  """"""Runs translation.\n\n  Args:\n    source_file: The source file.\n    batch_size: The batch size to use.\n    beam_size: The beam size to use. Set to 1 for greedy search.\n  """"""\n\n  # Create the inference dataset.\n  dataset = model.examples_inputter.make_inference_dataset(source_file, batch_size)\n\n  @tf.function(input_signature=(dataset.element_spec,))\n  def predict(source):\n    # Run the encoder.\n    source_length = source[""length""]\n    batch_size = tf.shape(source_length)[0]\n    source_inputs = model.features_inputter(source)\n    encoder_outputs, _, _ = model.encoder(source_inputs, source_length)\n\n    # Prepare the decoding strategy.\n    if beam_size > 1:\n      encoder_outputs = tfa.seq2seq.tile_batch(encoder_outputs, beam_size)\n      source_length = tfa.seq2seq.tile_batch(source_length, beam_size)\n      decoding_strategy = onmt.utils.BeamSearch(beam_size)\n    else:\n      decoding_strategy = onmt.utils.GreedySearch()\n\n    # Run dynamic decoding.\n    decoder_state = model.decoder.initial_state(\n        memory=encoder_outputs,\n        memory_sequence_length=source_length)\n    decoded = model.decoder.dynamic_decode(\n        model.labels_inputter,\n        tf.fill([batch_size], onmt.START_OF_SENTENCE_ID),\n        end_id=onmt.END_OF_SENTENCE_ID,\n        initial_state=decoder_state,\n        decoding_strategy=decoding_strategy,\n        maximum_iterations=200)\n    target_lengths = decoded.lengths\n    target_tokens = model.labels_inputter.ids_to_tokens.lookup(tf.cast(decoded.ids, tf.int64))\n    return target_tokens, target_lengths\n\n  for source in dataset:\n    batch_tokens, batch_length = predict(source)\n    for tokens, length in zip(batch_tokens.numpy(), batch_length.numpy()):\n      sentence = b"" "".join(tokens[0][:length[0]])\n      print(sentence.decode(""utf-8""))\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(""run"", choices=[""train"", ""translate""],\n                      help=""Run type."")\n  parser.add_argument(""--src"", required=True,\n                      help=""Path to the source file."")\n  parser.add_argument(""--tgt"",\n                      help=""Path to the target file."")\n  parser.add_argument(""--src_vocab"", required=True,\n                      help=""Path to the source vocabulary."")\n  parser.add_argument(""--tgt_vocab"", required=True,\n                      help=""Path to the target vocabulary."")\n  parser.add_argument(""--model_dir"", default=""checkpoint"",\n                      help=""Directory where checkpoint are written."")\n  args = parser.parse_args()\n\n  data_config = {\n      ""source_vocabulary"": args.src_vocab,\n      ""target_vocabulary"": args.tgt_vocab\n  }\n\n  model.initialize(data_config)\n\n  checkpoint_manager = tf.train.CheckpointManager(checkpoint, args.model_dir, max_to_keep=5)\n  if checkpoint_manager.latest_checkpoint is not None:\n    tf.get_logger().info(""Restoring parameters from %s"", checkpoint_manager.latest_checkpoint)\n    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n\n  if args.run == ""train"":\n    train(args.src, args.tgt, checkpoint_manager)\n  elif args.run == ""translate"":\n    translate(args.src)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
examples/library/minimal_transformer_training.py,1,"b'""""""This example demonstrates how to train a standard Transformer model in a few\nlines of code using OpenNMT-tf high-level APIs.\n""""""\n\nimport argparse\nimport logging\n\nimport tensorflow as tf\nimport opennmt as onmt\n\ntf.get_logger().setLevel(logging.INFO)\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(""run"", choices=[""train"", ""translate""],\n                      help=""Run type."")\n  parser.add_argument(""--src"", required=True,\n                      help=""Path to the source file."")\n  parser.add_argument(""--tgt"",\n                      help=""Path to the target file."")\n  parser.add_argument(""--src_vocab"", required=True,\n                      help=""Path to the source vocabulary."")\n  parser.add_argument(""--tgt_vocab"", required=True,\n                      help=""Path to the target vocabulary."")\n  parser.add_argument(""--model_dir"", default=""checkpoint"",\n                      help=""Directory where checkpoint are written."")\n  args = parser.parse_args()\n\n  # See http://opennmt.net/OpenNMT-tf/configuration.html for a complete specification\n  # of the configuration.\n  config = {\n      ""model_dir"": args.model_dir,\n      ""data"": {\n          ""source_vocabulary"": args.src_vocab,\n          ""target_vocabulary"": args.tgt_vocab,\n          ""train_features_file"": args.src,\n          ""train_labels_file"": args.tgt,\n      }\n  }\n\n  model = onmt.models.TransformerBase()\n  runner = onmt.Runner(model, config, auto_config=True)\n\n  if args.run == ""train"":\n    runner.train()\n  elif args.run == ""translate"":\n    runner.infer(args.src)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/__init__.py,0,b''
opennmt/bin/ark_to_records.py,2,"b'""""""ARK data file to TFRecords converter.\n\nThe scripts takes the ARK data file and optionally the indexed target text\nto write aligned source and target data.\n""""""\n\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nfrom opennmt.inputters.record_inputter import write_sequence_record\n\n\ndef consume_next_vector(ark_file):\n  """"""Consumes the next vector.\n\n  Args:\n    ark_file: The ARK data file.\n\n  Returns:\n    The next vector as a 2D Numpy array.\n  """"""\n  idx = None\n  vector = []\n\n  for line in ark_file:\n    line = line.strip()\n    fields = line.split()\n\n    if not idx:\n      idx = fields[0]\n      fields.pop(0)\n      fields.pop(0)\n\n    end = fields and fields[-1] == ""]""\n\n    if end:\n      fields.pop()\n\n    if fields:\n      vector.append(fields)\n\n    if end:\n      break\n\n  return idx, np.asarray(vector, dtype=np.float32)\n\ndef consume_next_text(text_file):\n  """"""Consumes the next text line from `text_file`.""""""\n  idx = None\n  text = text_file.readline()\n\n  if text:\n    tokens = text.strip().split()\n    idx = tokens[0]\n    tokens.pop(0)\n    text = "" "".join(tokens)\n\n  return idx, text\n\ndef write_text(text, writer):\n  """"""Serializes a line of text.""""""\n  writer.write(text)\n  writer.write(""\\n"")\n\ndef ark_to_records_aligned(ark_filename, text_filename, out_prefix, compression_type=None):\n  """"""Converts ARK and text datasets to aligned TFRecords and text datasets.""""""\n  record_filename = ""%s.records"" % out_prefix\n  if compression_type == ""GZIP"":\n    record_filename = ""%s.gz"" % record_filename\n  record_writer = tf.io.TFRecordWriter(record_filename, options=compression_type)\n  text_writer = open(out_prefix + "".txt"", encoding=""utf-8"", mode=""w"")\n\n  ark_buffer = {}\n  text_buffer = {}\n  count = 0\n\n  def _write_example(vector, text):\n    write_sequence_record(vector, record_writer)\n    write_text(text, text_writer)\n\n  def _search_aligned():\n    for idx in ark_buffer:\n      if idx in text_buffer:\n        vector = ark_buffer[idx]\n        text = text_buffer[idx]\n\n        del ark_buffer[idx]\n        del text_buffer[idx]\n\n        return vector, text\n\n    return None, None\n\n  with open(ark_filename, encoding=""utf-8"") as ark_file, open(text_filename, encoding=""utf-8"") as text_file: #pylint: disable=line-too-long\n    while True:\n      ark_idx, vector = consume_next_vector(ark_file)\n      text_idx, text = consume_next_text(text_file)\n\n      if not ark_idx and not text_idx:\n        # Both files are empty.\n        break\n\n      if ark_idx == text_idx:\n        # If the indices match, write the example.\n        _write_example(vector, text)\n        count += 1\n      else:\n        # Otherwise store the entries.\n        if ark_idx:\n          ark_buffer[ark_idx] = vector\n        if text_idx:\n          text_buffer[text_idx] = text\n\n        # Look if we can now find aligned entries.\n        vector, text = _search_aligned()\n\n        if vector is not None:\n          _write_example(vector, text)\n          count += 1\n\n  # Search alignments in stored entries.\n  while True:\n    vector, text = _search_aligned()\n    if vector is None:\n      break\n    _write_example(vector, text)\n    count += 1\n\n  record_writer.close()\n  text_writer.close()\n\n  print(""Saved {} aligned records."".format(count))\n\ndef ark_to_records(ark_filename, out_prefix, compression_type=None):\n  """"""Converts ARK dataset to TFRecords.""""""\n  record_writer = tf.io.TFRecordWriter(out_prefix + "".records"", options=compression_type)\n  count = 0\n\n  with open(ark_filename, encoding=""utf-8"") as ark_file:\n    while True:\n      ark_idx, vector = consume_next_vector(ark_file)\n      if not ark_idx:\n        break\n      write_sequence_record(vector, record_writer)\n      count += 1\n\n  record_writer.close()\n  print(""Saved {} records."".format(count))\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(""--ark"", required=True,\n                      help=""Indexed ARK data file."")\n  parser.add_argument(""--txt"",\n                      help=(""Indexed target text data file ""\n                            ""(must set it to align source and target files).""))\n  parser.add_argument(""--out"", required=True,\n                      help=""Output files prefix (will be suffixed by .records and .txt)."")\n  parser.add_argument(""--compression_type"", default=None, choices=[""GZIP""],\n                      help=""Optional compression type."")\n  args = parser.parse_args()\n\n  if args.txt:\n    ark_to_records_aligned(args.ark, args.txt, args.out, compression_type=args.compression_type)\n  else:\n    ark_to_records(args.ark, args.out, compression_type=args.compression_type)\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/build_vocab.py,0,"b'""""""Standalone script to generate word vocabularies from monolingual corpus.""""""\n\nimport argparse\n\nfrom opennmt import constants\nfrom opennmt import tokenizers\nfrom opennmt import data\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      ""data"", nargs=""*"",\n      help=""Source text file."")\n  parser.add_argument(\n      ""--from_vocab"", default=None,\n      help=""Build from a saved vocabulary (see also --from_format)."")\n  parser.add_argument(\n      ""--from_format"", default=""default"", choices=[""default"", ""sentencepiece""],\n      help=""The format of the saved vocabulary (see also --from_vocab)."")\n  parser.add_argument(\n      ""--save_vocab"", required=True,\n      help=""Output vocabulary file."")\n  parser.add_argument(\n      ""--min_frequency"", type=int, default=1,\n      help=""Minimum word frequency."")\n  parser.add_argument(\n      ""--size"", type=int, default=0,\n      help=""Maximum vocabulary size. If = 0, do not limit vocabulary."")\n  parser.add_argument(\n      ""--size_multiple"", type=int, default=1,\n      help=(""Ensure that the vocabulary size + 1 is a multiple of this value ""\n            ""(+ 1 represents the <unk> token that will be added during the training.""))\n  parser.add_argument(\n      ""--without_sequence_tokens"", default=False, action=""store_true"",\n      help=""If set, do not add special sequence tokens (start, end) in the vocabulary."")\n  parser.add_argument(\n      ""--tokenizer_config"", default=None,\n      help=""Tokenization configuration."")\n  parser.add_argument(\n      ""--sentencepiece"", nargs=""*"", default=None,\n      help=(""Build a SentencePiece model and vocabulary. This option accepts additional ""\n            ""training parameters (e.g. --sentencepiece character_coverage=0.98).""))\n  args = parser.parse_args()\n\n  special_tokens = [constants.PADDING_TOKEN]\n  if not args.without_sequence_tokens:\n    special_tokens.append(constants.START_OF_SENTENCE_TOKEN)\n    special_tokens.append(constants.END_OF_SENTENCE_TOKEN)\n\n  vocab = data.Vocab(special_tokens=special_tokens)\n  num_oov_buckets = 1\n\n  if args.sentencepiece is not None:\n    import pyonmttok  # pylint: disable=import-outside-toplevel\n    if args.size_multiple == 1:\n      vocab_size = args.size\n    else:\n      # Round vocabulary size to the next multiple of args.size_multiple\n      vocab_size = (\n          args.size - (args.size + num_oov_buckets) % args.size_multiple + args.size_multiple)\n    sp_params = dict(map(lambda arg: tuple(arg.split(""="")), args.sentencepiece))\n    sp_trainer = pyonmttok.SentencePieceLearner(\n        keep_vocab=True, vocab_size=vocab_size, **sp_params)\n    for data_file in args.data:\n      sp_trainer.ingest_file(data_file)\n    sp_trainer.learn(args.save_vocab, verbose=True)\n    args.save_vocab = args.save_vocab + "".vocab""\n    vocab.load(args.save_vocab, file_format=""sentencepiece"")\n  else:\n    if args.from_vocab is not None:\n      vocab.load(args.from_vocab, file_format=args.from_format)\n    tokenizer = tokenizers.make_tokenizer(args.tokenizer_config)\n    for data_file in args.data:\n      vocab.add_from_text(data_file, tokenizer=tokenizer)\n    vocab = vocab.prune(max_size=args.size, min_frequency=args.min_frequency)\n    vocab.pad_to_multiple(args.size_multiple, num_oov_buckets=num_oov_buckets)\n\n  vocab.serialize(args.save_vocab)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/detokenize_text.py,0,"b'""""""Standalone script to detokenize a corpus.""""""\n\nimport argparse\n\nfrom opennmt import tokenizers\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      ""--delimiter"", default="" "",\n      help=""Token delimiter used in text serialization."")\n  parser.add_argument(\n      ""--tokenizer_config"", default=None,\n      help=""Tokenization configuration."")\n  args = parser.parse_args()\n\n  tokenizer = tokenizers.make_tokenizer(args.tokenizer_config)\n  tokenizer.detokenize_stream(delimiter=args.delimiter)\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/main.py,10,"b'""""""Main script.""""""\n\nimport argparse\nimport logging\nimport os\nimport sys\n\nimport tensorflow as tf\n\nfrom opennmt import __version__\nfrom opennmt.models import catalog\nfrom opennmt.runner import Runner\nfrom opennmt.config import load_model, load_config\nfrom opennmt.utils import exporters\n\n\n_PYTHON_TO_TENSORFLOW_LOGGING_LEVEL = {\n    logging.CRITICAL: 3,\n    logging.ERROR: 2,\n    logging.WARNING: 1,\n    logging.INFO: 0,\n    logging.DEBUG: 0,\n    logging.NOTSET: 0,\n}\n\ndef _set_log_level(log_level):\n  tf.get_logger().setLevel(log_level)\n  os.environ[""TF_CPP_MIN_LOG_LEVEL""] = str(_PYTHON_TO_TENSORFLOW_LOGGING_LEVEL[log_level])\n\ndef _prefix_paths(prefix, paths):\n  """"""Recursively prefix paths.\n\n  Args:\n    prefix: The prefix to apply.\n    data: A dict of relative paths.\n\n  Returns:\n    The updated dict.\n  """"""\n  if isinstance(paths, dict):\n    for key, path in paths.items():\n      paths[key] = _prefix_paths(prefix, path)\n    return paths\n  elif isinstance(paths, list):\n    for i, path in enumerate(paths):\n      paths[i] = _prefix_paths(prefix, path)\n    return paths\n  else:\n    path = paths\n    new_path = os.path.join(prefix, path)\n    if tf.io.gfile.exists(new_path):\n      return new_path\n    else:\n      return path\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(""-v"", ""--version"", action=""version"", version=""OpenNMT-tf %s"" % __version__)\n  parser.add_argument(""--config"", required=True, nargs=""+"",\n                      help=""List of configuration files."")\n  parser.add_argument(""--auto_config"", default=False, action=""store_true"",\n                      help=""Enable automatic configuration values."")\n  parser.add_argument(""--model_type"", default="""",\n                      choices=list(sorted(catalog.list_model_names_from_catalog())),\n                      help=""Model type from the catalog."")\n  parser.add_argument(""--model"", default="""",\n                      help=""Custom model configuration file."")\n  parser.add_argument(""--run_dir"", default="""",\n                      help=""If set, model_dir will be created relative to this location."")\n  parser.add_argument(""--data_dir"", default="""",\n                      help=""If set, data files are expected to be relative to this location."")\n  parser.add_argument(""--checkpoint_path"", default=None,\n                      help=(""Specific checkpoint or model directory to load ""\n                            ""(when a directory is set, the latest checkpoint is used).""))\n  parser.add_argument(""--log_level"", default=""INFO"",\n                      choices=[""CRITICAL"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""],\n                      help=""Logs verbosity."")\n  parser.add_argument(""--seed"", type=int, default=None,\n                      help=""Random seed."")\n  parser.add_argument(""--gpu_allow_growth"", default=False, action=""store_true"",\n                      help=""Allocate GPU memory dynamically."")\n  parser.add_argument(""--intra_op_parallelism_threads"", type=int, default=0,\n                      help=(""Number of intra op threads (0 means the system picks ""\n                            ""an appropriate number).""))\n  parser.add_argument(""--inter_op_parallelism_threads"", type=int, default=0,\n                      help=(""Number of inter op threads (0 means the system picks ""\n                            ""an appropriate number).""))\n  parser.add_argument(""--mixed_precision"", default=False, action=""store_true"",\n                      help=""Enable mixed precision."")\n\n  subparsers = parser.add_subparsers(help=""Run type."", dest=""run_type"")\n  subparsers.required = True\n  parser_train = subparsers.add_parser(""train"", help=""Training."")\n  parser_train.add_argument(\n      ""--with_eval"", default=False, action=""store_true"",\n      help=""Enable automatic evaluation."")\n  parser_train.add_argument(\n      ""--num_gpus"", type=int, default=1,\n      help=""Number of GPUs to use for in-graph replication."")\n  parser_train.add_argument(\n      ""--horovod"", default=False, action=""store_true"",\n      help=""Enable Horovod training mode."")\n\n  parser_eval = subparsers.add_parser(""eval"", help=""Evaluation."")\n  parser_eval.add_argument(\n      ""--features_file"", nargs=""+"", default=None,\n      help=""Input features files."")\n  parser_eval.add_argument(\n      ""--labels_file"", default=None,\n      help=""Output labels files."")\n\n  parser_infer = subparsers.add_parser(""infer"", help=""Inference."")\n  parser_infer.add_argument(\n      ""--features_file"", nargs=""+"", required=True,\n      help=""Run inference on this file."")\n  parser_infer.add_argument(\n      ""--predictions_file"", default="""",\n      help=(""File used to save predictions. If not set, predictions are printed ""\n            ""on the standard output.""))\n  parser_infer.add_argument(\n      ""--log_prediction_time"", default=False, action=""store_true"",\n      help=""Logs some prediction time metrics."")\n\n  parser_export = subparsers.add_parser(""export"", help=""Model export."")\n  parser_export.add_argument(\n      ""--export_dir"", required=True,\n      help=""The directory of the exported model."")\n  parser_export.add_argument(\n      ""--export_format"", choices=exporters.list_exporters(), default=""saved_model"",\n      help=""Format of the exported model."")\n\n  parser_score = subparsers.add_parser(""score"", help=""Scoring."")\n  parser_score.add_argument(""--features_file"", nargs=""+"", required=True,\n                            help=""Features file."")\n  parser_score.add_argument(""--predictions_file"", default=None,\n                            help=""Predictions to score."")\n\n  parser_average_checkpoints = subparsers.add_parser(\n      ""average_checkpoints"", help=""Checkpoint averaging."")\n  parser_average_checkpoints.add_argument(\n      ""--output_dir"", required=True,\n      help=""The output directory for the averaged checkpoint."")\n  parser_average_checkpoints.add_argument(\n      ""--max_count"", type=int, default=8,\n      help=""The maximal number of checkpoints to average."")\n\n  parser_update_vocab = subparsers.add_parser(\n      ""update_vocab"", help=""Update model vocabularies in checkpoint."")\n  parser_update_vocab.add_argument(\n      ""--output_dir"", required=True,\n      help=""The output directory for the updated checkpoint."")\n  parser_update_vocab.add_argument(\n      ""--src_vocab"", default=None,\n      help=""Path to the new source vocabulary."")\n  parser_update_vocab.add_argument(\n      ""--tgt_vocab"", default=None,\n      help=""Path to the new target vocabulary."")\n\n  # When using an option that takes multiple values just before the run type,\n  # the run type is treated as a value of this option. To fix this issue, we\n  # inject a placeholder option just before the run type to clearly separate it.\n  parser.add_argument(""--placeholder"", action=""store_true"", help=argparse.SUPPRESS)\n  run_types = set(subparsers.choices.keys())\n  args = sys.argv[1:]\n  for i, arg in enumerate(args):\n    if arg in run_types:\n      args.insert(i, ""--placeholder"")\n      break\n\n  args = parser.parse_args(args)\n  if hasattr(args, ""features_file"") and args.features_file and len(args.features_file) == 1:\n    args.features_file = args.features_file[0]\n\n  _set_log_level(getattr(logging, args.log_level))\n  tf.config.threading.set_intra_op_parallelism_threads(args.intra_op_parallelism_threads)\n  tf.config.threading.set_inter_op_parallelism_threads(args.inter_op_parallelism_threads)\n\n  gpus = tf.config.list_physical_devices(device_type=""GPU"")\n  if hasattr(args, ""horovod"") and args.horovod:\n    import horovod.tensorflow as hvd  # pylint: disable=import-outside-toplevel\n    hvd.init()\n    is_master = hvd.rank() == 0\n    if gpus:\n      local_gpu = gpus[hvd.local_rank()]\n      tf.config.experimental.set_visible_devices(local_gpu, device_type=""GPU"")\n      gpus = [local_gpu]\n  else:\n    hvd = None\n    is_master = True\n\n  if args.gpu_allow_growth:\n    for device in gpus:\n      tf.config.experimental.set_memory_growth(device, enable=True)\n\n  # Load and merge run configurations.\n  config = load_config(args.config)\n  if args.run_dir:\n    config[""model_dir""] = os.path.join(args.run_dir, config[""model_dir""])\n  if args.data_dir:\n    config[""data""] = _prefix_paths(args.data_dir, config[""data""])\n\n  if is_master and not tf.io.gfile.exists(config[""model_dir""]):\n    tf.get_logger().info(""Creating model directory %s"", config[""model_dir""])\n    tf.io.gfile.makedirs(config[""model_dir""])\n\n  model = load_model(\n      config[""model_dir""],\n      model_file=args.model,\n      model_name=args.model_type,\n      serialize_model=is_master,\n      as_builder=True)\n  runner = Runner(\n      model,\n      config,\n      auto_config=args.auto_config,\n      mixed_precision=args.mixed_precision,\n      seed=args.seed)\n\n  if args.run_type == ""train"":\n    runner.train(\n        num_devices=args.num_gpus,\n        with_eval=args.with_eval,\n        checkpoint_path=args.checkpoint_path,\n        hvd=hvd)\n  elif args.run_type == ""eval"":\n    metrics = runner.evaluate(\n        checkpoint_path=args.checkpoint_path,\n        features_file=args.features_file,\n        labels_file=args.labels_file)\n    print(metrics)\n  elif args.run_type == ""infer"":\n    runner.infer(\n        args.features_file,\n        predictions_file=args.predictions_file,\n        checkpoint_path=args.checkpoint_path,\n        log_time=args.log_prediction_time)\n  elif args.run_type == ""export"":\n    runner.export(\n        args.export_dir,\n        checkpoint_path=args.checkpoint_path,\n        exporter=exporters.make_exporter(args.export_format))\n  elif args.run_type == ""score"":\n    runner.score(\n        args.features_file,\n        args.predictions_file,\n        checkpoint_path=args.checkpoint_path)\n  elif args.run_type == ""average_checkpoints"":\n    runner.average_checkpoints(args.output_dir, max_count=args.max_count)\n  elif args.run_type == ""update_vocab"":\n    runner.update_vocab(\n        args.output_dir,\n        src_vocab=args.src_vocab,\n        tgt_vocab=args.tgt_vocab)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/merge_config.py,0,"b'""""""Script that merges configurations for debug or simplification.""""""\n\nimport argparse\nimport yaml\n\nfrom opennmt.config import load_config\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(""config"", nargs=""+"", help=""Configuration files."")\n  args = parser.parse_args()\n  config = load_config(args.config)\n  print(yaml.dump(config, default_flow_style=False))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/bin/tokenize_text.py,0,"b'""""""Standalone script to tokenize a corpus.""""""\n\nimport argparse\n\nfrom opennmt import tokenizers\n\n\ndef main():\n  parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      ""--delimiter"", default="" "",\n      help=""Token delimiter for text serialization."")\n  parser.add_argument(\n      ""--tokenizer_config"", default=None,\n      help=""Tokenization configuration."")\n  args = parser.parse_args()\n\n  tokenizer = tokenizers.make_tokenizer(args.tokenizer_config)\n  tokenizer.tokenize_stream(delimiter=args.delimiter)\n\nif __name__ == ""__main__"":\n  main()\n'"
opennmt/data/__init__.py,0,"b'""""""This module exposes classes and functions that help creating and processing data.""""""\n\nfrom opennmt.data.dataset import batch_dataset\nfrom opennmt.data.dataset import batch_sequence_dataset\nfrom opennmt.data.dataset import filter_examples_by_length\nfrom opennmt.data.dataset import filter_irregular_batches\nfrom opennmt.data.dataset import get_dataset_size\nfrom opennmt.data.dataset import inference_pipeline\nfrom opennmt.data.dataset import make_cardinality_multiple_of\nfrom opennmt.data.dataset import random_shard\nfrom opennmt.data.dataset import shuffle_dataset\nfrom opennmt.data.dataset import training_pipeline\n\nfrom opennmt.data.noise import Noise\nfrom opennmt.data.noise import WordDropout\nfrom opennmt.data.noise import WordNoiser\nfrom opennmt.data.noise import WordOmission\nfrom opennmt.data.noise import WordPermutation\nfrom opennmt.data.noise import WordReplacement\n\nfrom opennmt.data.text import alignment_matrix_from_pharaoh\nfrom opennmt.data.text import tokens_to_chars\nfrom opennmt.data.text import tokens_to_words\n\nfrom opennmt.data.vocab import Vocab\n'"
opennmt/data/dataset.py,54,"b'""""""Dataset creation and transformations.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom opennmt.utils import misc\n\n\ndef make_datasets(dataset_cls, filenames):\n  """"""Creates instances of :obj:`dataset_cls`.\n\n  Args:\n    dataset_cls: A class inheriting from ``tf.data.Dataset``.\n    filenames: A list of filenames or a single filename.\n\n  Returns:\n    A list of ``tf.data.Dataset`` instances if multiple :obj:`filenames` are\n    passed, otherwise a single ``tf.data.Dataset``.\n\n  Raises:\n    ValueError: if :obj:`filenames` is empty.\n  """"""\n  if not isinstance(filenames, list):\n    filenames = [filenames]\n  elif not filenames:\n    raise ValueError(""At least one data file is required"")\n  datasets = [\n      dataset_cls(filename, compression_type=""GZIP"" if misc.is_gzip_file(filename) else None)\n      for filename in filenames]\n  if len(datasets) == 1:\n    return datasets[0]\n  return datasets\n\ndef normalize_weights(datasets, weights=None, sizes=None):\n  """"""Returns normalized dataset weights based on datasets sizes.\n\n  Args:\n    datasets: A list of ``tf.data.Dataset`` instances.\n    weights: An optional list of dataset weights.\n    sizes: The size of each dataset, if known.\n\n  Returns:\n    A normalized list of weights that can be used as sampling probabilities.\n\n  Raises:\n    ValueError: if the length of :obj:`weights` or :obj:`sizes` does not match\n      the length of :obj:`datasets`.\n  """"""\n  if not datasets:\n    return []\n  if len(datasets) == 1:\n    return [1.]\n\n  if weights is None:\n    weights = [1 / len(datasets)] * len(datasets)\n  elif len(weights) != len(datasets):\n    raise ValueError(""Got %d datasets but %d weights"" % (len(datasets), len(weights)))\n\n  if sizes is None:\n    sizes = [int(get_dataset_size(dataset)) for dataset in datasets]\n  elif len(sizes) != len(datasets):\n    raise ValueError(""Got %d datasets but %d sizes"" % (len(datasets), len(sizes)))\n\n  # Weights should be normalized by the dataset size relative to the total size.\n  total_size = sum(sizes)\n  weights = [weight * (size / total_size) for weight, size in zip(weights, sizes)]\n\n  # Convert weights to probabilities.\n  logits = tf.math.log(tf.constant(weights, dtype=tf.float32))\n  probabilities = tf.nn.softmax(logits).numpy().tolist()\n  return probabilities\n\ndef _get_output_shapes(dataset):\n  """"""Returns the outputs shapes of the dataset.\n\n  Args:\n    dataset: A ``tf.data.Dataset``.\n\n  Returns:\n    A nested structure of ``tf.TensorShape``\n  """"""\n  return tf.nest.map_structure(lambda spec: spec.shape, dataset.element_spec)\n\ndef get_dataset_size(dataset, batch_size=5000):\n  """"""Get the dataset size.\n\n  Example:\n\n    >>> dataset = tf.data.Dataset.range(5)\n    >>> opennmt.data.get_dataset_size(dataset).numpy()\n    5\n\n  Args:\n    dataset: A dataset.\n    batch_size: The batch size to use to improve the scan performance, or\n      ``None`` to scan the dataset as-is.\n\n  Returns:\n    The dataset size or ``None`` if the dataset is infinite.\n  """"""\n  if tf.data.experimental.cardinality(dataset) == tf.data.experimental.INFINITE_CARDINALITY:\n    return None\n  if batch_size is not None:\n    dataset = dataset.batch(batch_size)\n\n  def _reduce_func(count, element):\n    element = tf.nest.flatten(element)[0]\n    batch_size = tf.shape(element)[0]\n    return count + tf.cast(batch_size, count.dtype)\n\n  return dataset.reduce(tf.constant(0, dtype=tf.int64), _reduce_func)\n\ndef filter_irregular_batches(multiple):\n  """"""Transformation that filters out batches based on their size.\n\n  Example:\n\n    >>> dataset = tf.data.Dataset.range(10).batch(3)\n    >>> dataset = dataset.apply(opennmt.data.filter_irregular_batches(3))\n    >>> len(list(iter(dataset)))\n    3\n\n  Args:\n    multiple: The divisor of the batch size.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n  """"""\n  if multiple == 1:\n    return lambda dataset: dataset\n\n  def _predicate(*x):\n    flat = tf.nest.flatten(x)\n    batch_size = tf.shape(flat[0])[0]\n    return tf.equal(batch_size % multiple, 0)\n\n  return lambda dataset: dataset.filter(_predicate)\n\ndef filter_examples_by_length(maximum_features_length=None,\n                              maximum_labels_length=None,\n                              features_length_fn=None,\n                              labels_length_fn=None):\n  """"""Transformation that filters out examples with zero length or length that is\n  greater than the configured maximum.\n\n  Example:\n\n    >>> dataset = dataset.apply(opennmt.data.filter_examples_by_length(...))\n\n  Args:\n    maximum_features_length: The maximum length or list of maximum lengths of\n      the features sequence(s). ``None`` to not constrain the length.\n    maximum_labels_length: The maximum length or list of maximum lengths of\n      the labels sequence(s). ``None`` to not constrain the length.\n    features_length_fn: A function mapping features to a sequence length.\n    labels_length_fn: A function mapping labels to a sequence length.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n  """"""\n  if features_length_fn is None and labels_length_fn is None:\n    return lambda dataset: dataset\n\n  def _length_constraints(length, maximum_length):\n    # Work with lists of lengths which correspond to the general multi source case.\n    if not isinstance(length, list):\n      length = [length]\n    if not isinstance(maximum_length, list):\n      maximum_length = [maximum_length]\n    # Unset maximum lengths are set to None (i.e. no constraint).\n    maximum_length += [None] * (len(length) - len(maximum_length))\n    constraints = []\n    for l, maxlen in zip(length, maximum_length):\n      constraints.append(tf.greater(l, 0))\n      if maxlen is not None:\n        constraints.append(tf.less_equal(l, maxlen))\n    return constraints\n\n  def _predicate(features, labels):\n    cond = []\n    features_length = features_length_fn(features) if features_length_fn is not None else None\n    labels_length = labels_length_fn(labels) if labels_length_fn is not None else None\n    if features_length is not None:\n      cond.extend(_length_constraints(features_length, maximum_features_length))\n    if labels_length is not None:\n      cond.extend(_length_constraints(labels_length, maximum_labels_length))\n    return tf.reduce_all(cond)\n\n  return lambda dataset: dataset.filter(_predicate)\n\ndef make_cardinality_multiple_of(divisor):\n  """"""Transformation that ensures that the dataset cardinality is a multiple of\n  :obj:`divisor`.\n\n  Example:\n\n    >>> dataset = tf.data.Dataset.range(7)\n    >>> dataset = dataset.apply(opennmt.data.make_cardinality_multiple_of(10))\n    >>> len(list(iter(dataset)))\n    10\n\n  Args:\n    divisor: The value that should divide the dataset size.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n\n  Tip:\n    This transformation is useful when training multiple replicas on a finite\n    dataset. It ensures that each replica receives a non empty batch in the last\n    training iteration.\n  """"""\n  if divisor == 1:\n    return lambda dataset: dataset\n\n  def _continue_iter(num_consumed, element):\n    # Continue iterating if the current element is from the original dataset or\n    # if the number of consumed batches is not a multiple of divisor.\n    is_original = element[0]\n    return tf.math.logical_or(is_original, tf.math.not_equal(num_consumed % divisor, 0))\n\n  def _retrieve_element(num_consumed, element):\n    _ = num_consumed\n    return element[1]\n\n  def _transform(dataset):\n    # Nothing to do for infinite datasets.\n    if tf.data.experimental.cardinality(dataset) == tf.data.experimental.INFINITE_CARDINALITY:\n      return dataset\n\n    # Concatenate extra batches with a flag.\n    extra_batches = dataset.repeat()\n    dataset = dataset.map(lambda *x: (tf.constant(True), x))\n    extra_batches = extra_batches.map(lambda *x: (tf.constant(False), x))\n    dataset = dataset.concatenate(extra_batches)\n\n    # Take all original batches and the number of extra batches required.\n    dataset = dataset.enumerate()\n    dataset = dataset.apply(tf.data.experimental.take_while(_continue_iter))\n    return dataset.map(_retrieve_element)  # Retrieve the element only.\n\n  return _transform\n\ndef random_shard(shard_size, dataset_size):\n  """"""Transformation that shards the dataset in a random order.\n\n  Example:\n\n    >>> dataset = tf.data.Dataset.range(6)\n    >>> dataset = dataset.apply(opennmt.data.random_shard(2, 6)\n    >>> list(dataset.as_numpy_iterator())\n    [0, 1, 4, 5, 2, 3]\n\n  Args:\n    shard_size: The number of examples in each shard.\n    dataset_size: The total number of examples in the dataset.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n  """"""\n  num_shards = -(-dataset_size // shard_size)  # Ceil division.\n  offsets = np.linspace(0, dataset_size, num=num_shards, endpoint=False, dtype=np.int64)\n\n  def _random_shard(dataset):\n    sharded_dataset = tf.data.Dataset.from_tensor_slices(offsets)\n    sharded_dataset = sharded_dataset.shuffle(num_shards)\n    sharded_dataset = sharded_dataset.flat_map(\n        lambda offset: dataset.skip(offset).take(shard_size))\n    return sharded_dataset\n\n  return _random_shard\n\ndef shuffle_dataset(buffer_size, shuffle_shards=True, dataset_size=None):\n  """"""Transformation that shuffles the dataset based on its size.\n\n  Example:\n\n    >>> dataset = tf.data.Dataset.range(6)\n    >>> dataset = dataset.apply(opennmt.data.shuffle_dataset(3))\n    >>> list(dataset.as_numpy_iterator())\n    [2, 3, 1, 0, 4, 5]\n\n  Args:\n    buffer_size: The number of elements from which to sample.\n    shuffle_shards: When :obj:`buffer_size` is smaller than the dataset size,\n      the dataset is first sharded in a random order to add another level of\n      shuffling.\n    dataset_size: If the dataset size is already known, it can be passed here to\n      avoid a slower generic computation of the dataset size later.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n  """"""\n\n  def _shuffle(dataset):\n    sample_size = buffer_size\n    if sample_size < 0 or shuffle_shards:\n      total_size = dataset_size\n      if total_size is None:\n        total_size = get_dataset_size(dataset)\n      tf.get_logger().info(""Training on %d examples"", total_size)\n      if sample_size < 0:\n        sample_size = total_size\n      elif sample_size < total_size:\n        dataset = dataset.apply(random_shard(sample_size, total_size))\n    dataset = dataset.shuffle(sample_size)\n    return dataset\n\n  return _shuffle\n\ndef batch_dataset(batch_size, padded_shapes=None):\n  """"""Transformation that batches a dataset.\n\n  Example:\n\n    >>> dataset = dataset.apply(opennmt.data.batch_dataset(...))\n\n  Args:\n    batch_size: The batch size.\n    padded_shapes: The padded shapes for this dataset. If ``None``, the shapes\n      are automatically inferred from the dataset output shapes.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n\n  See Also:\n    :func:`opennmt.data.batch_sequence_dataset`\n  """"""\n  return lambda dataset: dataset.padded_batch(\n      batch_size, padded_shapes=padded_shapes or _get_output_shapes(dataset))\n\ndef batch_sequence_dataset(batch_size,\n                           batch_type=""examples"",\n                           batch_multiplier=1,\n                           batch_size_multiple=1,\n                           length_bucket_width=None,\n                           length_fn=None,\n                           padded_shapes=None):\n  """"""Transformation that batches a dataset of sequences.\n\n  This implements an example-based and a token-based batching strategy\n  with optional bucketing of sequences.\n\n  Bucketing makes the batches contain sequences of similar lengths to optimize\n  the training efficiency. For example, if :obj:`length_bucket_width` is 5,\n  sequences will be organized by the following length buckets:\n\n  1 - 5 | 6 - 10 | 11 - 15 | ...\n\n  Then when building the next batch, sequences will be selected from the same\n  length bucket.\n\n  If the dataset has parallel elements (e.g. a parallel source and target\n  dataset), the element is assigned to the bucket corresponding to the maximum\n  length of all parallel elements.\n\n  Example:\n\n    >>> dataset = dataset.apply(opennmt.data.batch_sequence_dataset(...))\n\n  Args:\n    batch_size: The batch size.\n    batch_type: The training batching strategy to use: can be ""examples"" or\n      ""tokens"".\n    batch_multiplier: The batch size multiplier.\n    batch_size_multiple: When :obj:`batch_type` is ""tokens"", ensure that the\n      resulting batch size is a multiple of this value.\n    length_bucket_width: The width of the length buckets to select batch\n      candidates from. ``None`` to not constrain batch formation.\n    length_fn: A function or list of functions (in case of a parallel dataset)\n      that take features as argument and return the associated sequence length.\n    padded_shapes: The padded shapes for this dataset. If ``None``, the shapes\n      are automatically inferred from the dataset output shapes.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n\n  Raises:\n    ValueError: if :obj:`batch_type` is not one of ""examples"" or ""tokens"".\n    ValueError: if the number of length functions in :obj:`length_fn` does not\n      match the number of parallel elements.\n\n  See Also:\n    :func:`opennmt.data.batch_dataset`\n  """"""\n  batch_size = batch_size * batch_multiplier\n\n  def _get_bucket_id(features, length_fn):\n    default_id = tf.constant(0, dtype=tf.int32)\n    if length_fn is None:\n      return default_id\n    lengths = length_fn(features)\n    if lengths is None:\n      return default_id\n    if not isinstance(lengths, list):\n      lengths = [lengths]  # Fallback to the general case of parallel inputs.\n    lengths = [length // length_bucket_width for length in lengths]\n    return tf.reduce_max(lengths)\n\n  def _key_func(*args):\n    length_fns = length_fn\n    if length_fns is None:\n      length_fns = [None for _ in args]\n    elif not isinstance(length_fns, (list, tuple)):\n      length_fns = [length_fns]\n    if len(length_fns) != len(args):\n      raise ValueError(""%d length functions were passed but this dataset contains ""\n                       ""%d parallel elements"" % (len(length_fns), len(args)))\n    # Take the highest bucket id.\n    bucket_id = tf.reduce_max([\n        _get_bucket_id(features, length_fn)\n        for features, length_fn in zip(args, length_fns)])\n    return tf.cast(bucket_id, tf.int64)\n\n  def _reduce_func(unused_key, dataset):\n    return dataset.apply(batch_dataset(batch_size, padded_shapes=padded_shapes))\n\n  def _window_size_func(key):\n    if length_bucket_width > 1:\n      key += 1  # For length_bucket_width == 1, key 0 is unassigned.\n    size = batch_size // (key * length_bucket_width)\n    required_multiple = batch_multiplier * batch_size_multiple\n    if required_multiple > 1:\n      size = size + required_multiple - size % required_multiple\n    return tf.cast(tf.maximum(size, required_multiple), tf.int64)\n\n  if length_bucket_width is None:\n    return batch_dataset(batch_size, padded_shapes=padded_shapes)\n\n  if batch_type == ""examples"":\n    return tf.data.experimental.group_by_window(\n        _key_func, _reduce_func, window_size=batch_size)\n  elif batch_type == ""tokens"":\n    return tf.data.experimental.group_by_window(\n        _key_func, _reduce_func, window_size_func=_window_size_func)\n  else:\n    raise ValueError(\n        ""Invalid batch type: \'{}\'; should be \'examples\' or \'tokens\'"".format(batch_type))\n\n\ndef training_pipeline(batch_size,\n                      batch_type=""examples"",\n                      batch_multiplier=1,\n                      batch_size_multiple=1,\n                      process_fn=None,\n                      transform_fns=None,\n                      length_bucket_width=None,\n                      features_length_fn=None,\n                      labels_length_fn=None,\n                      maximum_features_length=None,\n                      maximum_labels_length=None,\n                      single_pass=False,\n                      num_shards=1,\n                      shard_index=0,\n                      num_threads=None,\n                      dataset_size=None,\n                      shuffle_buffer_size=None,\n                      prefetch_buffer_size=None,\n                      cardinality_multiple=1):\n  """"""Transformation that applies most of the dataset operations commonly used\n  for training on sequence data:\n\n  * sharding\n  * shuffling\n  * processing\n  * filtering\n  * bucketization\n  * batching\n  * prefetching\n\n  Example:\n\n    >>> dataset = dataset.apply(opennmt.data.training_pipeline(...))\n\n  Args:\n    batch_size: The batch size to use.\n    batch_type: The training batching stragety to use: can be ""examples"" or\n      ""tokens"".\n    batch_multiplier: The batch size multiplier.\n    batch_size_multiple: When :obj:`batch_type` is ""tokens"", ensure that the\n      resulting batch size is a multiple of this value.\n    process_fn: The processing function to apply on each element.\n    transform_fns: List of dataset transformation functions (applied after\n      :obj:`process_fn` if defined).\n    length_bucket_width: The width of the length buckets to select batch\n      candidates from. ``None`` to not constrain batch formation.\n    features_length_fn: A function mapping features to a sequence length.\n    labels_length_fn: A function mapping labels to a sequence length.\n    maximum_features_length: The maximum length or list of maximum lengths of\n      the features sequence(s). ``None`` to not constrain the length.\n    maximum_labels_length: The maximum length of the labels sequence.\n      ``None`` to not constrain the length.\n    single_pass: If ``True``, makes a single pass over the training data.\n    num_shards: The number of data shards (usually the number of workers in a\n      distributed setting).\n    shard_index: The shard index this data pipeline should read from.\n    num_threads: The number of elements processed in parallel.\n    dataset_size: If the dataset size is already known, it can be passed here to\n      avoid a slower generic computation of the dataset size later.\n    shuffle_buffer_size: The number of elements from which to sample.\n    prefetch_buffer_size: The number of batches to prefetch asynchronously. If\n      ``None``, use an automatically tuned value.\n    cardinality_multiple: Ensure that the dataset cardinality is a multiple of\n      this value when :obj:`single_pass` is ``True``.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n\n  See Also:\n    - :func:`opennmt.data.batch_sequence_dataset`\n    - :func:`opennmt.data.make_cardinality_multiple_of`\n    - :func:`opennmt.data.filter_examples_by_length`\n    - :func:`opennmt.data.filter_irregular_batches`\n    - :func:`opennmt.data.shuffle_dataset`\n  """"""\n  if dataset_size is not None and num_shards > 1:\n    # Update dataset_size based on the shard size.\n    if isinstance(dataset_size, list):\n      dataset_size = [size // num_shards for size in dataset_size]\n    else:\n      dataset_size //= num_shards\n\n  def _make_weighted_dataset(datasets, weights):\n    if single_pass:\n      raise ValueError(""single_pass parameter is not compatible with weighted datasets"")\n    if not datasets:\n      raise ValueError(""At least one dataset is required"")\n    if weights is not None and len(weights) != len(datasets):\n      raise ValueError(""%d dataset weights were provided, but %d were expected to match the ""\n                       ""number of data files"" % (len(weights), len(datasets)))\n    if num_shards > 1:\n      datasets = [dataset.shard(num_shards, shard_index) for dataset in datasets]\n    weights = normalize_weights(datasets, weights=weights, sizes=dataset_size)\n    datasets = [dataset.repeat() for dataset in datasets]\n    dataset = tf.data.experimental.sample_from_datasets(datasets, weights=weights)\n    if shuffle_buffer_size is not None and shuffle_buffer_size != 0:\n      if shuffle_buffer_size < 0:\n        raise ValueError(""shuffle_buffer_size < 0 is not compatible with weighted datasets"")\n      dataset = dataset.shuffle(shuffle_buffer_size)\n    return dataset\n\n  def _make_single_dataset(dataset):\n    if num_shards > 1:\n      dataset = dataset.shard(num_shards, shard_index)\n    if shuffle_buffer_size is not None and shuffle_buffer_size != 0:\n      dataset = dataset.apply(shuffle_dataset(shuffle_buffer_size, dataset_size=dataset_size))\n    return dataset\n\n  def _pipeline(dataset):\n    if isinstance(dataset, tuple):\n      dataset, weights = dataset\n    else:\n      weights = None\n    is_weighted_dataset = isinstance(dataset, list)\n    if is_weighted_dataset:\n      dataset = _make_weighted_dataset(dataset, weights)\n    else:\n      dataset = _make_single_dataset(dataset)\n    if process_fn is not None:\n      dataset = dataset.map(process_fn, num_parallel_calls=num_threads or 4)\n    if transform_fns is not None:\n      for transform_fn in transform_fns:\n        dataset = dataset.apply(transform_fn)\n    dataset = dataset.apply(filter_examples_by_length(\n        maximum_features_length=maximum_features_length,\n        maximum_labels_length=maximum_labels_length,\n        features_length_fn=features_length_fn,\n        labels_length_fn=labels_length_fn))\n    dataset = dataset.apply(batch_sequence_dataset(\n        batch_size,\n        batch_type=batch_type,\n        batch_multiplier=batch_multiplier,\n        batch_size_multiple=batch_size_multiple,\n        length_bucket_width=length_bucket_width,\n        length_fn=[features_length_fn, labels_length_fn]))\n    dataset = dataset.apply(filter_irregular_batches(batch_multiplier))\n    if not single_pass:\n      if not is_weighted_dataset:  # Weighted dataset is repeated before sampling.\n        dataset = dataset.repeat()\n    else:\n      dataset = dataset.apply(make_cardinality_multiple_of(cardinality_multiple))\n    dataset = dataset.prefetch(prefetch_buffer_size)\n    return dataset\n\n  return _pipeline\n\ndef inference_pipeline(batch_size,\n                       process_fn=None,\n                       transform_fns=None,\n                       length_bucket_width=None,\n                       length_fn=None,\n                       num_threads=None,\n                       prefetch_buffer_size=None):\n  """"""Transformation that applies dataset operations for inference.\n\n  Example:\n\n    >>> dataset = dataset.apply(opennmt.data.inference_pipeline(...))\n\n  Args:\n    batch_size: The batch size to use.\n    process_fn: The processing function to apply on each element.\n    transform_fns: List of dataset transformation functions (applied after\n      :obj:`process_fn` if defined).\n    length_bucket_width: The width of the length buckets to select batch\n      candidates from. If set, this means the inference pipeline will be\n      reordered based on the examples length, the application is then\n      responsible to restore the predictions in order. An ""index"" key will be\n      inserted in the examples dictionary.\n    length_fn: A function mapping features to a sequence length.\n    num_threads: The number of elements processed in parallel.\n    prefetch_buffer_size: The number of batches to prefetch asynchronously. If\n      ``None``, use an automatically tuned value.\n\n  Returns:\n    A ``tf.data.Dataset`` transformation.\n\n  Raises:\n    ValueError: if :obj:`length_bucket_width` is set but not :obj:`length_fn`.\n    ValueError: if :obj:`length_bucket_width` is set but the dataset does not\n      output a dictionary structure.\n  """"""\n\n  def _inject_index(index, x):\n    x[""index""] = index\n    return x\n\n  def _pipeline(dataset):\n    if process_fn is not None:\n      dataset = dataset.map(process_fn, num_parallel_calls=num_threads)\n    if transform_fns is not None:\n      for transform_fn in transform_fns:\n        dataset = dataset.apply(transform_fn)\n    if length_bucket_width is not None and length_bucket_width > 0:\n      if length_fn is None:\n        raise ValueError(""length_fn is required when reordering by length"")\n      if not isinstance(_get_output_shapes(dataset), dict):\n        raise ValueError(""Reordering by length expects dataset elements to be Python dicts"")\n      dataset = dataset.enumerate()\n      dataset = dataset.map(_inject_index)\n      dataset = dataset.apply(batch_sequence_dataset(\n          batch_size,\n          length_bucket_width=length_bucket_width,\n          length_fn=length_fn))\n    else:\n      dataset = dataset.apply(batch_dataset(batch_size))\n    dataset = dataset.prefetch(prefetch_buffer_size)\n    return dataset\n\n  return _pipeline\n\ndef function_on_next(dataset, as_numpy=False):\n  """"""Decorator to run a ``tf.function`` on each dataset element.\n\n  The motivation for this construct is to get the next element within the\n  ``tf.function`` for increased efficiency.\n\n  Args:\n    dataset: The dataset to iterate.\n    as_numpy: If ``True``, call `.numpy()` on each output tensors.\n\n  Returns:\n    A function decorator. The decorated function is transformed into a callable\n    that returns a generator over its outputs.\n  """"""\n\n  def decorator(func):\n    def _fun():\n      iterator = iter(dataset)\n\n      @tf.function\n      def _tf_fun():\n        return func(lambda: next(iterator))\n\n      while True:\n        try:\n          outputs = _tf_fun()\n          if as_numpy:\n            outputs = tf.nest.map_structure(lambda x: x.numpy(), outputs)\n          yield outputs\n        except tf.errors.OutOfRangeError:\n          break\n\n    return _fun\n\n  return decorator\n'"
opennmt/data/noise.py,53,"b'# -*- coding: utf-8 -*-\n\n""""""Noise modules.""""""\n\nimport abc\n\nimport tensorflow as tf\n\nfrom opennmt import constants\nfrom opennmt.data import text\nfrom opennmt.utils import misc\n\n\nclass WordNoiser(object):\n  """"""Applies noise to words sequences.""""""\n\n  def __init__(self, noises=None, subword_token=""\xef\xbf\xad"", is_spacer=None):\n    """"""Initializes the noising class.\n\n    Args:\n      noises: A list of :class:`opennmt.data.Noise` instances to apply\n        sequentially.\n      subword_token: The special token used by the subword tokenizer. This is\n        required when the noise should be applied at the word level and not the\n        subword level.\n      is_spacer: Whether :obj:`subword_token` is used as a spacer (as in\n        SentencePiece) or a joiner (as in BPE). If ``None``, will infer\n        directly from :obj:`subword_token`.\n\n    See Also:\n      :func:`opennmt.data.tokens_to_words`\n    """"""\n    if noises is None:\n      noises = []\n    self.noises = noises\n    self.subword_token = subword_token\n    self.is_spacer = is_spacer\n\n  def add(self, noise):\n    """"""Adds a noise to apply.""""""\n    self.noises.append(noise)\n\n  def __call__(self, tokens, sequence_length=None, keep_shape=False):\n    """"""Applies noise on :obj:`tokens`.\n\n    Args:\n      tokens: A string ``tf.Tensor`` or batch of string ``tf.Tensor``.\n      sequence_length: When :obj:`tokens` is a batch, the length of each\n        sequence in the batch.\n      keep_shape: Ensure that the shape is kept. Otherwise, fit the shape to the\n        new lengths.\n\n    Returns:\n      A tuple with the noisy version of :obj:`tokens` and the new lengths.\n\n    Raises:\n      ValueError: if :obj:`tokens` is a batch of string but\n        :obj:`sequence_length` is not passed.\n    """"""\n    with tf.device(""cpu:0""):\n      return self._call(tokens, sequence_length, keep_shape)\n\n  def _call(self, tokens, sequence_length, keep_shape):\n    rank = tokens.shape.ndims\n    if rank == 1:\n      input_length = tf.shape(tokens)[0]\n      if sequence_length is not None:\n        tokens = tokens[:sequence_length]\n      else:\n        tokens = tokens[:tf.math.count_nonzero(tokens)]\n      words = text.tokens_to_words(\n          tokens,\n          subword_token=self.subword_token,\n          is_spacer=self.is_spacer)\n      words = words.to_tensor()\n      for noise in self.noises:\n        words = noise(words)\n      outputs = tf.RaggedTensor.from_tensor(words, padding="""").flat_values\n      output_length = tf.shape(outputs)[0]\n      if keep_shape:\n        outputs = tf.pad(outputs, [[0, input_length - output_length]])\n      return outputs, output_length\n    elif rank == 2:\n      if sequence_length is None:\n        raise ValueError(""sequence_length must be passed for 2D inputs"")\n      tokens, sequence_length = tf.map_fn(\n          lambda arg: self._call(*arg, keep_shape=True),\n          (tokens, sequence_length))\n      if not keep_shape:\n        tokens = tokens[:, :tf.reduce_max(sequence_length)]\n      return tokens, sequence_length\n    else:\n      if sequence_length is None:\n        raise ValueError(""sequence_length must be passed for ND inputs"")\n      original_shape = misc.shape_list(tokens)\n      tokens = tf.reshape(tokens, [-1, original_shape[-1]])\n      sequence_length = tf.reshape(sequence_length, [-1])\n      tokens, sequence_length = self._call(tokens, sequence_length, keep_shape=keep_shape)\n      tokens = tf.reshape(tokens, original_shape[:-1] + [-1])\n      sequence_length = tf.reshape(sequence_length, original_shape[:-1])\n      return tokens, sequence_length\n\n\nclass Noise(abc.ABC):\n  """"""Base class for noise modules.""""""\n\n  def __call__(self, words):\n    """"""Applies noise on a sequence of words.\n\n    Args:\n      words: The sequence of words as a string ``tf.Tensor``. If it has 2\n        dimensions, each row represents a word that possibly contains multiple\n        tokens.\n\n    Returns:\n      A noisy version of :obj:`words`.\n\n    Raises:\n      ValueError: if :obj:`words` has a rank greater than 2.\n    """"""\n    if words.shape.ndims > 2:\n      raise ValueError(""Noise only supports tensors of rank 2 or less"")\n    inputs = words\n    if words.shape.ndims == 1:\n      inputs = tf.expand_dims(inputs, 1)\n    num_words = tf.shape(inputs)[0]\n    outputs = tf.cond(\n        tf.math.equal(num_words, 0),\n        true_fn=lambda: inputs,\n        false_fn=lambda: self._apply(inputs))\n    if words.shape.ndims == 1:\n      outputs = tf.squeeze(outputs, 1)\n    return outputs\n\n  @abc.abstractmethod\n  def _apply(self, words):\n    """"""Applies noise on a sequence of words.\n\n    Args:\n      words: A 2D string ``tf.Tensor`` where each row represents a word that\n        possibly contains multiple tokens.\n\n    Returns:\n      A noisy version of :obj:`words`.\n    """"""\n    raise NotImplementedError()\n\n\nclass WordDropout(Noise):\n  """"""Randomly drops words in a sequence.\n\n  Example:\n\n    >>> noise = opennmt.data.WordDropout(0.5)\n    >>> words = tf.constant([""a"", ""b"", ""c""])\n    >>> noise(words).numpy()\n    array([b\'a\', b\'b\'], dtype=object)\n  """"""\n\n  def __init__(self, dropout):\n    """"""Initializes the noise module.\n\n    Args:\n      dropout: The probability to drop word.\n    """"""\n    self.dropout = dropout\n\n  def _apply(self, words):\n    if self.dropout == 0:\n      return tf.identity(words)\n    num_words = tf.shape(words, out_type=tf.int64)[0]\n    keep_mask = random_mask([num_words], 1 - self.dropout)\n    keep_ind = tf.where(keep_mask)\n    # Keep at least one word.\n    keep_ind = tf.cond(\n        tf.equal(tf.shape(keep_ind)[0], 0),\n        true_fn=lambda: tf.random.uniform([1], maxval=num_words, dtype=tf.int64),\n        false_fn=lambda: tf.squeeze(keep_ind, -1))\n    return tf.gather(words, keep_ind)\n\n\nclass WordOmission(Noise):\n  """"""Randomly omits words in a sequence.\n\n  This is different than :class:`opennmt.data.WordDropout` as it drops a\n  fixed number of words.\n\n  Example:\n\n    >>> noise = opennmt.data.WordOmission(1)\n    >>> words = tf.constant([""a"", ""b"", ""c""])\n    >>> noise(words).numpy()\n    array([b\'b\', b\'c\'], dtype=object)\n  """"""\n\n  def __init__(self, count):\n    """"""Initializes the noise module.\n\n    Args:\n      count: The number of words to omit.\n    """"""\n    self.count = count\n\n  def _apply(self, words):\n    if self.count == 0:\n      return tf.identity(words)\n    num_words = tf.shape(words)[0]\n    indices = tf.range(num_words)\n    shuffle_indices = tf.random.shuffle(indices)\n    keep_count = tf.maximum(num_words - self.count, 1)\n    keep_indices = tf.sort(shuffle_indices[:keep_count])\n    return tf.gather(words, keep_indices)\n\n\nclass WordReplacement(Noise):\n  """"""Randomly replaces words.\n\n  Example:\n\n    >>> noise = opennmt.data.WordReplacement(0.5)\n    >>> words = tf.constant([""a"", ""b"", ""c""])\n    >>> noise(words).numpy()\n    array([b\'a\', b\'<unk>\', b\'c\'], dtype=object)\n  """"""\n\n  def __init__(self, probability, filler=constants.UNKNOWN_TOKEN):\n    """"""Initializes the noise module.\n\n    Args:\n      probability: The probability to replace words.\n      filler: The replacement token.\n    """"""\n    self.probability = probability\n    self.filler = filler\n\n  def _apply(self, words):\n    if self.probability == 0:\n      return tf.identity(words)\n    shape = tf.shape(words)\n    replace_mask = random_mask(shape[:1], self.probability)\n    filler = tf.fill([shape[0], 1], self.filler)\n    filler = tf.pad(filler, [[0, 0], [0, shape[-1] - 1]])\n    return tf.where(\n        tf.broadcast_to(tf.expand_dims(replace_mask, -1), tf.shape(words)),\n        x=filler,\n        y=words)\n\n\nclass WordPermutation(Noise):\n  """"""Randomly permutes words in a sequence with a maximum distance.\n\n  Example:\n\n    >>> noise = opennmt.data.WordPermutation(3)\n    >>> words = tf.constant([""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6""])\n    >>> noise(words).numpy()\n    array([b\'1\', b\'0\', b\'2\', b\'4\', b\'3\', b\'6\', b\'5\'], dtype=object)\n  """"""\n\n  def __init__(self, max_distance):\n    """"""Initializes the noise module.\n\n    Args:\n      max_distance: The maximum permutation distance.\n    """"""\n    self.max_distance = max_distance\n\n  def _apply(self, words):\n    if self.max_distance == 0:\n      return tf.identity(words)\n    num_words = tf.shape(words)[0]\n    offset = tf.random.uniform([num_words], maxval=1) * (self.max_distance + 1)\n    offset = tf.cast(offset, num_words.dtype)\n    new_pos = tf.argsort(tf.range(num_words) + offset)\n    return tf.gather(words, new_pos)\n\n\ndef random_mask(shape, probability):\n  """"""Generates a random boolean mask.\n\n  Args:\n    shape: The mask shape.\n    probability: The probability to select an element.\n\n  Returns:\n    A boolean mask with shape :obj:`shape`.\n  """"""\n  probs = tf.random.uniform(shape, maxval=1)\n  mask = tf.math.less(probs, probability)\n  return mask\n'"
opennmt/data/text.py,37,"b'# -*- coding: utf-8 -*-\n\n""""""Text manipulation.""""""\n\nimport tensorflow as tf\n\n\ndef tokens_to_chars(tokens):\n  """"""Splits tokens into unicode characters.\n\n  Example:\n\n    >>> opennmt.data.tokens_to_chars([""hello"", ""world""])\n    <tf.RaggedTensor [[b\'h\', b\'e\', b\'l\', b\'l\', b\'o\'], [b\'w\', b\'o\', b\'r\', b\'l\', b\'d\']]>\n\n  Args:\n    tokens: A string ``tf.Tensor`` of shape :math:`[T]`.\n\n  Returns:\n    The characters as a 2D string ``tf.RaggedTensor``.\n  """"""\n  return tf.strings.unicode_split(tokens, ""UTF-8"")\n\ndef tokens_to_words(tokens, subword_token=""\xef\xbf\xad"", is_spacer=None):\n  """"""Converts a sequence of tokens to a sequence of words.\n\n  Example:\n\n    >>> opennmt.data.tokens_to_words([""He@@"", ""llo"", ""W@@"", ""orld"", ""@@!""], subword_token=""@@"")\n    <tf.RaggedTensor [[b\'He@@\', b\'llo\'], [b\'W@@\', b\'orld\', b\'@@!\']]>\n\n  Args:\n    tokens: A 1D string ``tf.Tensor``.\n    subword_token: The special token used by the subword tokenizer.\n    is_spacer: Whether :obj:`subword_token` is used as a spacer (as in\n      SentencePiece) or a joiner (as in BPE). If ``None``, will infer\n      directly from :obj:`subword_token`.\n\n  Returns:\n    The words as a 2D string ``tf.RaggedTensor``.\n  """"""\n  if is_spacer is None:\n    is_spacer = subword_token == ""\xe2\x96\x81""\n  if is_spacer:\n    # First token implicitly starts with a spacer.\n    left_and_single = tf.logical_or(\n        tf.strings.regex_full_match(tokens, ""%s.*"" % subword_token),\n        tf.one_hot(0, tf.shape(tokens)[0], on_value=True, off_value=False))\n    right = tf.strings.regex_full_match(tokens, "".+%s"" % subword_token)\n    word_start = tf.logical_or(tf.roll(right, shift=1, axis=0), left_and_single)\n  else:\n    right = tf.strings.regex_full_match(tokens, "".*%s"" % subword_token)\n    left = tf.strings.regex_full_match(tokens, ""%s.*"" % subword_token)\n    subword = tf.logical_or(tf.roll(right, shift=1, axis=0), left)\n    word_start = tf.logical_not(subword)\n  start_indices = tf.squeeze(tf.where(word_start), -1)\n  return tf.RaggedTensor.from_row_starts(tokens, start_indices)\n\ndef alignment_matrix_from_pharaoh(alignment_line,\n                                  source_length,\n                                  target_length,\n                                  dtype=tf.float32):\n  """"""Parse Pharaoh alignments into an alignment matrix.\n\n  Example:\n\n    >>> opennmt.data.alignment_matrix_from_pharaoh(""0-0 1-2 1-3 2-1"", 3, 4)\n    <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n    array([[1., 0., 0.],\n           [0., 0., 1.],\n           [0., 1., 0.],\n           [0., 1., 0.]], dtype=float32)>\n\n  Args:\n    alignment_line: A string ``tf.Tensor`` in the Pharaoh format.\n    source_length: The length of the source sentence, without special symbols.\n    target_length: The length of the target sentence, without special symbols.\n    dtype: The output matrix dtype. Defaults to ``tf.float32`` for convenience\n      when computing the guided alignment loss.\n\n  Returns:\n    The alignment matrix as a 2-D ``tf.Tensor`` of type :obj:`dtype` and shape\n    ``[target_length, source_length]``, where ``[i, j] = 1`` if the ``i`` th\n    target word is aligned with the ``j`` th source word.\n  """"""\n  align_pairs_str = tf.strings.split([alignment_line]).values\n  align_pairs_flat_str = tf.strings.split(align_pairs_str, sep=""-"").values\n  align_pairs_flat = tf.strings.to_number(align_pairs_flat_str, out_type=tf.int64)\n  sparse_indices = tf.reshape(align_pairs_flat, [-1, 2])\n  sparse_values = tf.ones([tf.shape(sparse_indices)[0]], dtype=dtype)\n  source_length = tf.cast(source_length, tf.int64)\n  target_length = tf.cast(target_length, tf.int64)\n  maximum_ids = tf.reduce_max(sparse_indices, axis=0)\n  assert_source_length = _assert_in_range(maximum_ids[0], source_length, alignment_line, ""source"")\n  assert_target_length = _assert_in_range(maximum_ids[1], target_length, alignment_line, ""target"")\n  with tf.control_dependencies([assert_source_length, assert_target_length]):\n    alignment_matrix_sparse = tf.sparse.SparseTensor(\n        sparse_indices, sparse_values, [source_length, target_length])\n    alignment_matrix = tf.sparse.to_dense(alignment_matrix_sparse, validate_indices=False)\n    return tf.transpose(alignment_matrix)\n\n\ndef _assert_in_range(maximum_id, length, line, name):\n  return tf.debugging.assert_less(\n      maximum_id,\n      length,\n      message=tf.strings.format(\n          ""Length mismatch for alignment line {}: actual %s length is {}, but ""\n          ""got %s id {} which is out of range. Please check that the alignment ""\n          ""file is correctly aligned to the training file."" % (name, name),\n          [line, length, maximum_id]))\n'"
opennmt/data/vocab.py,7,"b'""""""Vocabulary utilities for Python scripts.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\n\nclass Vocab(object):\n  """"""Vocabulary class.\n\n  Example:\n\n    >>> vocab = opennmt.data.Vocab.from_file(""wmtende.vocab"")\n    >>> len(vocab)\n    32000\n    >>> ""be"" in vocab\n    True\n    >>> vocab.lookup(""be"")\n    377\n    >>> vocab.lookup(377)\n    \'be\'\n  """"""\n\n  def __init__(self, special_tokens=None):\n    """"""Initializes a vocabulary.\n\n    Args:\n      special_tokens: A list of special tokens (e.g. start of sentence).\n    """"""\n    self._token_to_id = {}\n    self._id_to_token = []\n    self._frequency = []\n\n    if special_tokens is not None:\n      for index, token in enumerate(special_tokens):\n        self._token_to_id[token] = index\n        self._id_to_token.insert(index, token)\n\n        # Set a very high frequency to avoid special tokens to be pruned. Note that Python sort\n        # functions are stable which means that special tokens in pruned vocabularies will have\n        # the same index.\n        self._frequency.insert(index, float(""inf""))\n\n  @classmethod\n  def from_file(cls, path, file_format=""default""):\n    """"""Creates from a vocabulary file.\n\n    Args:\n      path: The path to the vocabulary file.\n      file_format: Define the format of the vocabulary file. Can be: default,\n        sentencepiece. ""default"" is simply one token per line.\n\n    Raises:\n      ValueError: if :obj:`file_format` is invalid.\n    """"""\n    vocab = cls()\n    vocab.load(path, file_format=file_format)\n    return vocab\n\n  @property\n  def size(self):\n    """"""Returns the number of entries of the vocabulary.""""""\n    return len(self._id_to_token)\n\n  @property\n  def words(self):\n    """"""Returns the list of words.""""""\n    return self._id_to_token\n\n  def __len__(self):\n    """"""Returns the number of entries of the vocabulary.""""""\n    return self.size\n\n  def __contains__(self, token):\n    """"""Returns ``True`` if the vocabulary contains :obj:`token`.""""""\n    return self.lookup(token) is not None\n\n  def add_from_text(self, filename, tokenizer=None):\n    """"""Fills the vocabulary from a text file.\n\n    Args:\n      filename: The file to load from.\n      tokenizer: A callable to tokenize a line of text.\n    """"""\n    with tf.io.gfile.GFile(filename, mode=""rb"") as text:\n      for line in text:\n        line = tf.compat.as_text(line.strip())\n        if tokenizer:\n          tokens = tokenizer.tokenize(line)\n        else:\n          tokens = line.split()\n        for token in tokens:\n          self.add(token)\n\n  def serialize(self, path):\n    """"""Writes the vocabulary on disk.\n\n    Args:\n      path: The path where the vocabulary will be saved.\n    """"""\n    with tf.io.gfile.GFile(path, mode=""wb"") as vocab:\n      for token in self._id_to_token:\n        vocab.write(tf.compat.as_bytes(token))\n        vocab.write(b""\\n"")\n\n  def load(self, path, file_format=""default""):\n    """"""Loads a serialized vocabulary.\n\n    Args:\n      path: The path to the vocabulary to load.\n      file_format: Define the format of the vocabulary file. Can be: default,\n        sentencepiece. ""default"" is simply one token per line.\n\n    Raises:\n      ValueError: if :obj:`file_format` is invalid.\n    """"""\n    with tf.io.gfile.GFile(path, mode=""rb"") as vocab:\n      for line in vocab:\n        token = line.rstrip(b""\\r\\n"")\n        if file_format == ""default"":\n          self.add(token)\n        elif file_format == ""sentencepiece"":\n          token, _ = token.split(b""\\t"")\n          if token in (b""<unk>"", b""<s>"", b""</s>""):  # Ignore SentencePiece special tokens.\n            continue\n          self.add(token)\n        else:\n          raise ValueError(""Invalid vocabulary format: %s"" % file_format)\n\n  def add(self, token):\n    """"""Adds a token or increases its frequency.\n\n    Args:\n      token: The string to add.\n    """"""\n    token = tf.compat.as_text(token)\n    if token not in self._token_to_id:\n      index = self.size\n      self._token_to_id[token] = index\n      self._id_to_token.append(token)\n      self._frequency.append(1)\n    else:\n      self._frequency[self._token_to_id[token]] += 1\n\n  def lookup(self, identifier, default=None):\n    """"""Lookups in the vocabulary.\n\n    Args:\n      identifier: A string or an index to lookup.\n      default: The value to return if :obj:`identifier` is not found.\n\n    Returns:\n      The value associated with :obj:`identifier` or :obj:`default`.\n    """"""\n    value = None\n\n    if isinstance(identifier, (bytes, str)):\n      identifier = tf.compat.as_text(identifier)\n      value = self._token_to_id.get(identifier)\n    elif identifier < self.size:\n      value = self._id_to_token[identifier]\n\n    if value is None:\n      return default\n    else:\n      return value\n\n  def prune(self, max_size=0, min_frequency=1):\n    """"""Creates a pruned version of the vocabulary.\n\n    Args:\n      max_size: The maximum vocabulary size.\n      min_frequency: The minimum frequency of each entry.\n\n    Returns:\n      A new vocabulary.\n    """"""\n    sorted_ids = sorted(range(self.size), key=lambda k: self._frequency[k], reverse=True)\n    new_size = len(sorted_ids)\n\n    # Discard words that do not meet frequency requirements.\n    for i in range(new_size - 1, 0, -1):\n      index = sorted_ids[i]\n      if self._frequency[index] < min_frequency:\n        new_size -= 1\n      else:\n        break\n\n    # Limit absolute size.\n    if max_size > 0:\n      new_size = min(new_size, max_size)\n\n    new_vocab = Vocab()\n\n    for i in range(new_size):\n      index = sorted_ids[i]\n      token = self._id_to_token[index]\n      frequency = self._frequency[index]\n\n      new_vocab._token_to_id[token] = i  # pylint: disable=protected-access\n      new_vocab._id_to_token.append(token)  # pylint: disable=protected-access\n      new_vocab._frequency.append(frequency)  # pylint: disable=protected-access\n\n    return new_vocab\n\n  def pad_to_multiple(self, multiple, num_oov_buckets=1):\n    """"""Pads the vocabulary size to a multiple value.\n\n    More specically, this method ensures that:\n\n        ``(vocab_size + num_oov_buckets) % multiple == 0``\n\n    Args:\n      multiple: The multiple value.\n      num_oov_buckets: The number of OOV buckets added during the training.\n        Usually just 1 for the `<unk>` token.\n    """"""\n    i = 0\n    while (self.size + num_oov_buckets) % multiple != 0:\n      self.add(""averyunlikelytoken%d"" % i)\n      i += 1\n\n\ndef get_mapping(current_vocab_path, new_vocab_path, mode=""replace""):\n  """"""Maps vocabulary new indices to old ones. -1 means that the entry is new.""""""\n  mode = mode.lower()\n  if mode not in (""merge"", ""replace""):\n    raise ValueError(""invalid vocab update mode: %s"" % mode)\n  current_vocab = Vocab.from_file(current_vocab_path)\n  new_vocab = Vocab.from_file(new_vocab_path)\n  mapping = []\n  if mode == ""merge"":\n    final_vocab = Vocab.from_file(current_vocab_path)\n    mapping = list(range(current_vocab.size))\n    for new_word in new_vocab.words:\n      if current_vocab.lookup(new_word) is None:\n        mapping.append(-1)\n        final_vocab.add(new_word)\n  elif mode == ""replace"":\n    final_vocab = new_vocab\n    for new_word in new_vocab.words:\n      idx = current_vocab.lookup(new_word)\n      if idx is not None:\n        mapping.append(idx)\n      else:\n        mapping.append(-1)\n  mapping.append(current_vocab.size)  # <unk> token is always the last entry.\n  return mapping, final_vocab\n\ndef update_variable(ref_variable, new_variable, mapping, vocab_axis=0):\n  """"""Update a vocabulary variable, possibly copying previous entries based on\n  mapping.\n  """"""\n  ref = ref_variable.numpy()\n  new = np.zeros(new_variable.shape.as_list(), dtype=new_variable.dtype.as_numpy_dtype)\n  perm = None\n  if vocab_axis != 0:\n    # Make the dimension to index the first.\n    perm = list(range(len(ref.shape)))\n    perm[0], perm[vocab_axis] = perm[vocab_axis], perm[0]\n    ref = np.transpose(ref, axes=perm)\n    new = np.transpose(new, axes=perm)\n  for i, j in enumerate(mapping):\n    if j >= 0:\n      new[i] = ref[j]\n  if perm is not None:\n    new = np.transpose(new, axes=perm)\n  new_variable.assign(new)\n  return new_variable\n\ndef update_variable_and_slots(ref_variable,\n                              new_variable,\n                              ref_optimizer,\n                              new_optimizer,\n                              mapping,\n                              vocab_axis=0):\n  """"""Update a vocabulary variable and its associated optimizer slots (if any).""""""\n  variables = []\n  variables.append(update_variable(ref_variable, new_variable, mapping, vocab_axis=vocab_axis))\n  ref_slot_names = ref_optimizer.get_slot_names()\n  new_slot_names = new_optimizer.get_slot_names()\n  for slot_name in ref_slot_names:\n    if slot_name not in new_slot_names:\n      continue\n    ref_slot = ref_optimizer.get_slot(ref_variable, slot_name)\n    new_slot = new_optimizer.get_slot(new_variable, slot_name)\n    variables.append(update_variable(ref_slot, new_slot, mapping, vocab_axis=vocab_axis))\n  return variables\n'"
opennmt/decoders/__init__.py,0,"b'""""""Module defining decoders.""""""\n\nfrom opennmt.decoders.decoder import Decoder\nfrom opennmt.decoders.decoder import get_sampling_probability\n\nfrom opennmt.decoders.rnn_decoder import AttentionalRNNDecoder\nfrom opennmt.decoders.rnn_decoder import RNMTPlusDecoder\nfrom opennmt.decoders.rnn_decoder import RNNDecoder\n\nfrom opennmt.decoders.self_attention_decoder import SelfAttentionDecoder\n'"
opennmt/decoders/decoder.py,30,"b'""""""Base class and functions for dynamic decoders.""""""\n\nimport abc\n\nimport tensorflow as tf\n\nfrom opennmt import constants\nfrom opennmt.inputters import text_inputter\nfrom opennmt.layers import common\nfrom opennmt.utils import decoding\nfrom opennmt.utils import misc\n\n\ndef get_sampling_probability(step,\n                             read_probability=None,\n                             schedule_type=None,\n                             k=None):\n  """"""Returns the sampling probability as described in\n  https://arxiv.org/abs/1506.03099.\n\n  Args:\n    step: The training step.\n    read_probability: The probability to read from the inputs.\n    schedule_type: The type of schedule: ""constant"", ""linear"", ""exponential"",\n      or ""inverse_sigmoid"".\n    k: The convergence constant.\n\n  Returns:\n    The probability to sample from the output ids as a 0D ``tf.Tensor`` or\n    ``None`` if scheduled sampling is not configured.\n\n  Raises:\n    ValueError: if :obj:`schedule_type` is set but not :obj:`k` or if\n     :obj:`schedule_type` is ``linear`` but an initial :obj:`read_probability`\n     is not set.\n    TypeError: if :obj:`schedule_type` is invalid.\n  """"""\n  if read_probability is None and schedule_type is None:\n    return None\n\n  if schedule_type is not None and schedule_type != ""constant"":\n    if k is None:\n      raise ValueError(""scheduled_sampling_k is required when scheduled_sampling_type is set"")\n\n    step = tf.cast(step, tf.float32)\n    k = tf.constant(k, tf.float32)\n\n    if schedule_type == ""linear"":\n      if read_probability is None:\n        raise ValueError(""Linear schedule requires an initial read probability"")\n      read_probability = min(read_probability, 1.0)\n      read_probability = tf.maximum(read_probability - k * step, 0.0)\n    elif schedule_type == ""exponential"":\n      read_probability = tf.pow(k, step)\n    elif schedule_type == ""inverse_sigmoid"":\n      read_probability = k / (k + tf.exp(step / k))\n    else:\n      raise TypeError(""Unknown scheduled sampling type: {}"".format(schedule_type))\n\n  return 1.0 - read_probability\n\n\nclass Decoder(tf.keras.layers.Layer):\n  """"""Base class for decoders.""""""\n\n  def __init__(self, num_sources=1, **kwargs):\n    """"""Initializes the decoder parameters.\n\n    Args:\n      num_sources: The number of source contexts expected by this decoder.\n      **kwargs: Additional layer arguments.\n\n    Raises:\n      ValueError: if the number of source contexts :obj:`num_sources` is not\n        supported by this decoder.\n    """"""\n    if num_sources < self.minimum_sources or num_sources > self.maximum_sources:\n      raise ValueError(""This decoder accepts between %d and %d source contexts, ""\n                       ""but received %d"" % (\n                           self.minimum_sources, self.maximum_sources, num_sources))\n    super(Decoder, self).__init__(**kwargs)\n    self.num_sources = num_sources\n    self.output_layer = None\n    self.memory = None\n    self.memory_sequence_length = None\n\n  @property\n  def minimum_sources(self):\n    """"""The minimum number of source contexts supported by this decoder.""""""\n    return 1\n\n  @property\n  def maximum_sources(self):\n    """"""The maximum number of source contexts supported by this decoder.""""""\n    return 1\n\n  @property\n  def support_alignment_history(self):\n    """"""Returns ``True`` if this decoder can return the attention as alignment\n    history.""""""\n    return False\n\n  @property\n  def initialized(self):\n    """"""Returns ``True`` if this decoder is initialized.""""""\n    return self.output_layer is not None\n\n  def initialize(self, vocab_size=None, output_layer=None):\n    """"""Initializes the decoder configuration.\n\n    Args:\n      vocab_size: The target vocabulary size.\n      output_layer: The output layer to use.\n\n    Raises:\n      ValueError: if both :obj:`vocab_size` and :obj:`output_layer` are not set.\n    """"""\n    if output_layer is not None:\n      self.output_layer = output_layer\n    else:\n      if vocab_size is None:\n        raise ValueError(""One of vocab_size and output_layer must be set"")\n      self.output_layer = common.Dense(vocab_size)\n\n  def reuse_embeddings(self, embeddings):\n    """"""Reuses embeddings in the decoder output layer.\n\n    Args:\n      embeddings: The embeddings matrix to reuse.\n\n    Raises:\n      RuntimeError: if the decoder was not initialized.\n    """"""\n    self._assert_is_initialized()\n    self.output_layer.set_kernel(embeddings, transpose=True)\n\n  def initial_state(self,\n                    memory=None,\n                    memory_sequence_length=None,\n                    initial_state=None,\n                    batch_size=None,\n                    dtype=None):\n    """"""Returns the initial decoder state.\n\n    Args:\n      memory: Memory values to query.\n      memory_sequence_length: Memory values length.\n      initial_state: An initial state to start from, e.g. the last encoder\n        state.\n      batch_size: The batch size to use.\n      dtype: The dtype of the state.\n\n    Returns:\n      A nested structure of tensors representing the decoder state.\n\n    Raises:\n      RuntimeError: if the decoder was not initialized.\n      ValueError: if one of :obj:`batch_size` or :obj:`dtype` is not set and\n        neither :obj:`initial_state` nor :obj:`memory` are not passed.\n      ValueError: if the number of source contexts (:obj:`memory`) does not\n        match the number defined at the decoder initialization.\n    """"""\n    self._assert_is_initialized()\n    self._assert_memory_is_compatible(memory, memory_sequence_length)\n    self.memory = memory\n    self.memory_sequence_length = memory_sequence_length\n    if batch_size is None or dtype is None:\n      sentinel = tf.nest.flatten(memory)[0]\n      if sentinel is None:\n        sentinel = tf.nest.flatten(initial_state)[0]\n      if sentinel is None:\n        raise ValueError(""If batch_size or dtype are not set, then either ""\n                         ""memory or initial_state should be set"")\n      if batch_size is None:\n        batch_size = tf.shape(sentinel)[0]\n      if dtype is None:\n        dtype = sentinel.dtype\n    return self._get_initial_state(batch_size, dtype, initial_state=initial_state)\n\n  # pylint: disable=arguments-differ\n  def call(self,\n           inputs,\n           length_or_step=None,\n           state=None,\n           input_fn=None,\n           sampling_probability=None,\n           training=None):\n    """"""Runs the decoder layer on either a complete sequence (e.g. for training\n    or scoring), or a single timestep (e.g. for iterative decoding).\n\n    Args:\n      inputs: The inputs to decode, can be a 3D (training) or 2D (iterative\n        decoding) tensor.\n      length_or_step: For 3D :obj:`inputs`, the length of each sequence. For 2D\n        :obj:`inputs`, the current decoding timestep.\n      state: The decoder state.\n      input_fn: A callable taking sampled ids and returning the decoding inputs.\n      sampling_probability: When :obj:`inputs` is the full sequence, the\n        probability to read from the last sample instead of the true target.\n      training: Run in training mode.\n\n    Returns:\n      A tuple with the logits, the decoder state, and an attention vector.\n\n    Raises:\n      RuntimeError: if the decoder was not initialized.\n      ValueError: if the :obj:`inputs` rank is different than 2 or 3.\n      ValueError: if :obj:`length_or_step` is invalid.\n    """"""\n    self._assert_is_initialized()\n    rank = inputs.shape.ndims\n    if rank == 2:\n      if length_or_step.shape.ndims != 0:\n        raise ValueError(""length_or_step should be a scalar with the current timestep"")\n      outputs, state, attention = self.step(\n          inputs,\n          length_or_step,\n          state=state,\n          memory=self.memory,\n          memory_sequence_length=self.memory_sequence_length,\n          training=training)\n      logits = self.output_layer(outputs)\n    elif rank == 3:\n      if length_or_step.shape.ndims != 1:\n        raise ValueError(""length_or_step should contain the length of each sequence"")\n      logits, state, attention = self.forward(\n          inputs,\n          sequence_length=length_or_step,\n          initial_state=state,\n          memory=self.memory,\n          memory_sequence_length=self.memory_sequence_length,\n          input_fn=input_fn,\n          sampling_probability=sampling_probability,\n          training=training)\n    else:\n      raise ValueError(""Unsupported input rank %d"" % rank)\n    return logits, state, attention\n\n  def forward(self,\n              inputs,\n              sequence_length=None,\n              initial_state=None,\n              memory=None,\n              memory_sequence_length=None,\n              input_fn=None,\n              sampling_probability=None,\n              training=None):\n    """"""Runs the decoder on full sequences.\n\n    Args:\n      inputs: The 3D decoder input.\n      sequence_length: The length of each input sequence.\n      initial_state: The initial decoder state.\n      memory: Memory values to query.\n      memory_sequence_length: Memory values length.\n      input_fn: A callable taking sampled ids and returning the decoding inputs.\n      sampling_probability: The probability to read from the last sample instead\n        of the true target.\n      training: Run in training mode.\n\n    Returns:\n      A tuple with the logits, the decoder state, and the attention\n      vector.\n    """"""\n    _ = sequence_length\n    fused_projection = True\n    if sampling_probability is not None:\n      if input_fn is None:\n        raise ValueError(""input_fn is required when a sampling probability is set"")\n      if not tf.is_tensor(sampling_probability) and sampling_probability == 0:\n        sampling_probability = None\n      else:\n        fused_projection = False\n        tf.summary.scalar(""sampling_probability"", sampling_probability)\n\n    batch_size, max_step, _ = misc.shape_list(inputs)\n    inputs_ta = tf.TensorArray(inputs.dtype, size=max_step)\n    inputs_ta = inputs_ta.unstack(tf.transpose(inputs, perm=[1, 0, 2]))\n\n    def _maybe_sample(true_inputs, logits):\n      # Read from samples with a probability.\n      draw = tf.random.uniform([batch_size])\n      read_sample = tf.less(draw, sampling_probability)\n      sampled_ids = tf.random.categorical(logits, 1)\n      sampled_inputs = input_fn(tf.squeeze(sampled_ids, 1))\n      inputs = tf.where(\n          tf.broadcast_to(tf.expand_dims(read_sample, -1), tf.shape(true_inputs)),\n          x=sampled_inputs,\n          y=true_inputs)\n      return inputs\n\n    def _body(step, state, inputs, outputs_ta, attention_ta):\n      outputs, state, attention = self.step(\n          inputs,\n          step,\n          state=state,\n          memory=memory,\n          memory_sequence_length=memory_sequence_length,\n          training=training)\n      next_inputs = tf.cond(\n          step + 1 < max_step,\n          true_fn=lambda: inputs_ta.read(step + 1),\n          false_fn=lambda: tf.zeros_like(inputs))\n      if not fused_projection:\n        outputs = self.output_layer(outputs)\n      if sampling_probability is not None:\n        next_inputs = _maybe_sample(next_inputs, outputs)\n      outputs_ta = outputs_ta.write(step, outputs)\n      if attention is not None:\n        attention_ta = attention_ta.write(step, attention)\n      return step + 1, state, next_inputs, outputs_ta, attention_ta\n\n    step = tf.constant(0, dtype=tf.int32)\n    outputs_ta = tf.TensorArray(inputs.dtype, size=max_step)\n    attention_ta = tf.TensorArray(tf.float32, size=max_step)\n\n    _, state, _, outputs_ta, attention_ta = tf.while_loop(\n        lambda *arg: True,\n        _body,\n        loop_vars=(step, initial_state, inputs_ta.read(0), outputs_ta, attention_ta),\n        parallel_iterations=32,\n        swap_memory=True,\n        maximum_iterations=max_step)\n\n    outputs = tf.transpose(outputs_ta.stack(), perm=[1, 0, 2])\n    logits = self.output_layer(outputs) if fused_projection else outputs\n    attention = None\n    if self.support_alignment_history:\n      attention = tf.transpose(attention_ta.stack(), perm=[1, 0, 2])\n    return logits, state, attention\n\n  @abc.abstractmethod\n  def step(self,\n           inputs,\n           timestep,\n           state=None,\n           memory=None,\n           memory_sequence_length=None,\n           training=None):\n    """"""Runs one decoding step.\n\n    Args:\n      inputs: The 2D decoder input.\n      timestep: The current decoding step.\n      state: The decoder state.\n      memory: Memory values to query.\n      memory_sequence_length: Memory values length.\n      training: Run in training mode.\n\n    Returns:\n      A tuple with the decoder outputs, the decoder state, and the attention\n      vector.\n    """"""\n    raise NotImplementedError()\n\n  def dynamic_decode(self,\n                     embeddings,\n                     start_ids,\n                     end_id=constants.END_OF_SENTENCE_ID,\n                     initial_state=None,\n                     decoding_strategy=None,\n                     sampler=None,\n                     maximum_iterations=None,\n                     minimum_iterations=0):\n    """"""Decodes dynamically from :obj:`start_ids`.\n\n    Args:\n      embeddings: Target embeddings or :class:`opennmt.inputters.WordEmbedder`\n        to apply on decoded ids.\n      start_ids: Initial input IDs of shape :math:`[B]`.\n      end_id: ID of the end of sequence token.\n      initial_state: Initial decoder state.\n      decoding_strategy: A :class:`opennmt.utils.DecodingStrategy`\n        instance that define the decoding logic. Defaults to a greedy search.\n      sampler: A :class:`opennmt.utils.Sampler` instance that samples\n        predictions from the model output. Defaults to an argmax sampling.\n      maximum_iterations: The maximum number of iterations to decode for.\n      minimum_iterations: The minimum number of iterations to decode for.\n\n    Returns:\n      A :class:`opennmt.utils.DecodingResult` instance.\n\n    See Also:\n      :func:`opennmt.utils.dynamic_decode`\n    """"""\n    if isinstance(embeddings, text_inputter.WordEmbedder):\n      input_fn = lambda ids: embeddings({""ids"": ids})\n    else:\n      input_fn = lambda ids: tf.nn.embedding_lookup(embeddings, ids)\n\n    # TODO: find a better way to pass the state reorder flags.\n    if hasattr(decoding_strategy, ""_set_state_reorder_flags""):\n      state_reorder_flags = self._get_state_reorder_flags()\n      decoding_strategy._set_state_reorder_flags(state_reorder_flags)  # pylint: disable=protected-access\n\n    return decoding.dynamic_decode(\n        lambda ids, step, state: self(input_fn(ids), step, state),\n        start_ids,\n        end_id=end_id,\n        initial_state=initial_state,\n        decoding_strategy=decoding_strategy,\n        sampler=sampler,\n        maximum_iterations=maximum_iterations,\n        minimum_iterations=minimum_iterations,\n        attention_history=self.support_alignment_history,\n        attention_size=tf.shape(self.memory)[1] if self.support_alignment_history else None)\n\n  def map_v1_weights(self, weights):\n    return self.output_layer.map_v1_weights(weights[""dense""])\n\n  @abc.abstractmethod\n  def _get_initial_state(self, batch_size, dtype, initial_state=None):\n    """"""Returns the decoder initial state.\n\n    Args:\n      batch_size: The batch size of the returned state.\n      dtype; The data type of the state.\n      initial_state: A state to start from.\n\n    Returns:\n      The decoder state as a nested structure of tensors.\n    """"""\n    raise NotImplementedError()\n\n  def _get_state_reorder_flags(self):\n    """"""Returns a structure that marks states that should be reordered during beam search.\n    By default all states are reordered.\n\n    Returns:\n      The same structure as the decoder state with tensors replaced by booleans.\n    """"""\n    return None\n\n  def _assert_is_initialized(self):\n    """"""Raises an expection if the decoder was not initialized.""""""\n    if not self.initialized:\n      raise RuntimeError(""The decoder was not initialized"")\n\n  def _assert_memory_is_compatible(self, memory, memory_sequence_length):\n    """"""Raises an expection if the memory layout is not compatible with this decoder.""""""\n\n    def _num_elements(obj):\n      if obj is None:\n        return 0\n      elif isinstance(obj, (list, tuple)):\n        return len(obj)\n      else:\n        return 1\n\n    num_memory = _num_elements(memory)\n    num_length = _num_elements(memory_sequence_length)\n    if num_memory != num_length and memory_sequence_length is not None:\n      raise ValueError(""got %d memory values but %d length vectors"" % (num_memory, num_length))\n    if num_memory != self.num_sources:\n      raise ValueError(""expected %d source contexts, but got %d"" % (\n          self.num_sources, num_memory))\n'"
opennmt/decoders/rnn_decoder.py,6,"b'""""""Define RNN-based decoders.""""""\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom opennmt.decoders import decoder\nfrom opennmt.layers import bridge\nfrom opennmt.layers import common\nfrom opennmt.layers import rnn\nfrom opennmt.layers import transformer\nfrom opennmt.layers.rnn import map_v1_weights_to_cell\n\n\nclass RNNDecoder(decoder.Decoder):\n  """"""A basic RNN decoder.""""""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               bridge_class=None,\n               cell_class=None,\n               dropout=0.3,\n               residual_connections=False,\n               **kwargs):\n    """"""Initializes the decoder parameters.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      bridge_class: A :class:`opennmt.layers.Bridge` class to pass the\n        encoder state to the decoder. Default to\n        :class:`opennmt.layers.ZeroBridge`.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a LSTM cell.\n      dropout: The probability to drop units in each layer output.\n      residual_connections: If ``True``, each layer input will be added to its\n        output.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(RNNDecoder, self).__init__(**kwargs)\n    self.dropout = dropout\n    self.cell = rnn.make_rnn_cell(\n        num_layers,\n        num_units,\n        dropout=dropout,\n        residual_connections=residual_connections,\n        cell_class=cell_class)\n    if bridge_class is None:\n      bridge_class = bridge.ZeroBridge\n    self.bridge = bridge_class()\n\n  def _get_initial_state(self, batch_size, dtype, initial_state=None):\n    cell_initial_state = self.cell.get_initial_state(batch_size=batch_size, dtype=dtype)\n    if initial_state is not None:\n      cell_initial_state = self.bridge(initial_state, cell_initial_state)\n    return cell_initial_state\n\n  def step(self,\n           inputs,\n           timestep,\n           state=None,\n           memory=None,\n           memory_sequence_length=None,\n           training=None):\n    outputs, state = self.cell(inputs, state, training=training)\n    return outputs, state, None\n\n\nclass AttentionalRNNDecoder(RNNDecoder):\n  """"""A RNN decoder with attention.""""""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               bridge_class=None,\n               attention_mechanism_class=None,\n               cell_class=None,\n               dropout=0.3,\n               residual_connections=False,\n               first_layer_attention=False,\n               attention_layer_activation=tf.math.tanh,\n               **kwargs):\n    """"""Initializes the decoder parameters.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      bridge: A :class:`opennmt.layers.Bridge` to pass the encoder state\n        to the decoder.\n      attention_mechanism_class: A class inheriting from\n        ``tfa.seq2seq.AttentionMechanism``. Defaults to\n        ``tfa.seq2seq.LuongAttention``.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell.\n      dropout: The probability to drop units in each layer output.\n      residual_connections: If ``True``, each layer input will be added to its\n        output.\n      first_layer_attention: If ``True``, output attention after the first layer.\n      attention_layer_activation: The activation to produce the attentional hidden\n        state. Defaults to tanh following Luong paper (equation (5) in\n        https://arxiv.org/abs/1508.04025).\n      **kwargs: Additional layer arguments.\n    """"""\n    super(AttentionalRNNDecoder, self).__init__(\n        num_layers,\n        num_units,\n        bridge_class=bridge_class,\n        cell_class=cell_class,\n        dropout=dropout,\n        residual_connections=residual_connections,\n        **kwargs)\n    if attention_mechanism_class is None:\n      attention_mechanism_class = tfa.seq2seq.LuongAttention\n    self.attention_mechanism = attention_mechanism_class(self.cell.output_size)\n\n    def _add_attention(cell):\n      # Produce Luong-style attentional hidden states.\n      attention_layer = common.Dense(\n          cell.output_size,\n          use_bias=False,\n          activation=attention_layer_activation)\n      wrapper = tfa.seq2seq.AttentionWrapper(\n          cell,\n          self.attention_mechanism,\n          attention_layer=attention_layer)\n      return wrapper\n\n    if first_layer_attention:\n      self.cell.cells[0] = _add_attention(self.cell.cells[0])\n    else:\n      self.cell = _add_attention(self.cell)\n    self.dropout = dropout\n    self.first_layer_attention = first_layer_attention\n\n  @property\n  def support_alignment_history(self):\n    return True\n\n  def _get_initial_state(self, batch_size, dtype, initial_state=None):\n    # Reset memory of attention mechanism.\n    self.attention_mechanism.setup_memory(\n        self.memory, memory_sequence_length=self.memory_sequence_length)\n    decoder_state = self.cell.get_initial_state(batch_size=batch_size, dtype=dtype)\n    if initial_state is not None:\n      if self.first_layer_attention:\n        cell_state = list(decoder_state)\n        cell_state[0] = decoder_state[0].cell_state\n        cell_state = self.bridge(initial_state, cell_state)\n        cell_state[0] = decoder_state[0].clone(cell_state=cell_state[0])\n        decoder_state = tuple(cell_state)\n      else:\n        cell_state = self.bridge(initial_state, decoder_state.cell_state)\n        decoder_state = decoder_state.clone(cell_state=cell_state)\n    return decoder_state\n\n  def step(self,\n           inputs,\n           timestep,\n           state=None,\n           memory=None,\n           memory_sequence_length=None,\n           training=None):\n    outputs, state = self.cell(inputs, state, training=training)\n    outputs = common.dropout(outputs, self.dropout, training=training)\n    if self.first_layer_attention:\n      attention = state[0].alignments\n    else:\n      attention = state.alignments\n    return outputs, state, attention\n\n  def map_v1_weights(self, weights):\n    if (self.first_layer_attention\n        or not isinstance(self.attention_mechanism, tfa.seq2seq.LuongAttention)):\n      raise ValueError(""Can only map V1 weights for RNN decoder with Luong attention ""\n                       ""on the last layer"")\n    m = super(AttentionalRNNDecoder, self).map_v1_weights(weights)\n    m += common.Dense.map_v1_weights(\n        self.attention_mechanism.memory_layer, weights[""memory_layer""])\n    weights = weights[""decoder""][""attention_wrapper""]\n    # pylint: disable=protected-access\n    m += common.Dense.map_v1_weights(\n        self.cell._attention_layers[0], weights[""attention_layer""])\n    m += map_v1_weights_to_cell(self.cell._cell, weights)\n    # pylint: enable=protected-access\n    return m\n\n\nclass RNMTPlusDecoder(decoder.Decoder):\n  """"""The RNMT+ decoder described in https://arxiv.org/abs/1804.09849.""""""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               num_heads,\n               dropout=0.3,\n               cell_class=None,\n               **kwargs):\n    """"""Initializes the decoder parameters.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      num_heads: The number of attention heads.\n      dropout: The probability to drop units from the decoder input and in each\n        layer output.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a layer normalized LSTM cell.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(RNMTPlusDecoder, self).__init__(**kwargs)\n    if cell_class is None:\n      cell_class = tfa.rnn.LayerNormLSTMCell\n    self.num_heads = num_heads\n    self.num_units = num_units\n    self.dropout = dropout\n    self.cells = [cell_class(num_units) for _ in range(num_layers)]\n    self.multi_head_attention = transformer.MultiHeadAttention(\n        num_heads,\n        num_units,\n        dropout=dropout,\n        return_attention=True)\n\n  @property\n  def support_alignment_history(self):\n    return True\n\n  def _get_initial_state(self, batch_size, dtype, initial_state=None):\n    return tuple(\n        cell.get_initial_state(batch_size=batch_size, dtype=dtype)\n        for cell in self.cells)\n\n  def step(self,\n           inputs,\n           timestep,\n           state=None,\n           memory=None,\n           memory_sequence_length=None,\n           training=None):\n    inputs = common.dropout(inputs, rate=self.dropout, training=training)\n\n    new_states = []\n    last_outputs, state_0 = self.cells[0](inputs, state[0])\n    new_states.append(state_0)\n\n    if memory_sequence_length is not None:\n      memory_mask = tf.sequence_mask(memory_sequence_length, maxlen=tf.shape(memory)[1])\n    else:\n      memory_mask = None\n\n    context, _, attention = self.multi_head_attention(\n        tf.expand_dims(last_outputs, 1),\n        memory=memory,\n        mask=memory_mask,\n        training=training)\n    attention = attention[:, 0, 0]  # Use the first head for the attention vector.\n    context = tf.squeeze(context, axis=1)\n\n    for i in range(1, len(self.cells)):\n      inputs = tf.concat([last_outputs, context], axis=-1)\n      outputs, state_i = self.cells[i](inputs, state[i], training=training)\n      new_states.append(state_i)\n      outputs = common.dropout(outputs, rate=self.dropout, training=training)\n      if i >= 2:\n        outputs += last_outputs\n      last_outputs = outputs\n\n    final = tf.concat([last_outputs, context], -1)\n    return final, tuple(new_states), attention\n'"
opennmt/decoders/self_attention_decoder.py,10,"b'""""""Define self-attention decoder.""""""\n\nimport tensorflow as tf\n\nfrom opennmt.decoders import decoder\nfrom opennmt.layers import common, transformer\nfrom opennmt.layers.position import SinusoidalPositionEncoder\n\n\nclass SelfAttentionDecoder(decoder.Decoder):\n  """"""Encoder using self-attention as described in\n  https://arxiv.org/abs/1706.03762.\n  """"""\n\n  def __init__(self,\n               num_layers,\n               num_units=512,\n               num_heads=8,\n               ffn_inner_dim=2048,\n               dropout=0.1,\n               attention_dropout=0.1,\n               ffn_dropout=0.1,\n               ffn_activation=tf.nn.relu,\n               position_encoder_class=SinusoidalPositionEncoder,\n               num_sources=1,\n               maximum_relative_position=None,\n               **kwargs):\n    """"""Initializes the parameters of the decoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of hidden units.\n      num_heads: The number of heads in the multi-head attention.\n      ffn_inner_dim: The number of units of the inner linear transformation\n        in the feed forward layer.\n      dropout: The probability to drop units from the outputs.\n      attention_dropout: The probability to drop units from the attention.\n      ffn_dropout: The probability to drop units from the activation output in\n        the feed forward layer.\n      ffn_activation: The activation function to apply between the two linear\n        transformations of the feed forward layer.\n      position_encoder_class: The :class:`opennmt.layers.PositionEncoder`\n        class to use for position encoding (or a callable that returns an\n        instance).\n      num_sources: The number of source contexts expected by this decoder.\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n      **kwargs: Additional layer arguments.\n    """"""\n    super(SelfAttentionDecoder, self).__init__(num_sources=num_sources, **kwargs)\n    self.num_units = num_units\n    self.num_heads = num_heads\n    self.dropout = dropout\n    self.position_encoder = None\n    if position_encoder_class is not None:\n      self.position_encoder = position_encoder_class()\n    self.layer_norm = common.LayerNorm()\n    self.layers = [\n        transformer.SelfAttentionDecoderLayer(\n            self.num_units,\n            self.num_heads,\n            ffn_inner_dim,\n            num_sources=num_sources,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            ffn_dropout=ffn_dropout,\n            ffn_activation=ffn_activation,\n            maximum_relative_position=maximum_relative_position)\n        for i in range(num_layers)]\n\n  @property\n  def minimum_sources(self):\n    return 0\n\n  @property\n  def maximum_sources(self):\n    return 1e6  # An arbitrary large number.\n\n  @property\n  def support_alignment_history(self):\n    return True\n\n  def map_v1_weights(self, weights):\n    m = super(SelfAttentionDecoder, self).map_v1_weights(weights)\n    m += self.layer_norm.map_v1_weights(weights[""LayerNorm""])\n    for i, layer in enumerate(self.layers):\n      m += layer.map_v1_weights(weights[""layer_%d"" % i])\n    return m\n\n  def _run(self,\n           inputs,\n           sequence_length=None,\n           cache=None,\n           memory=None,\n           memory_sequence_length=None,\n           step=None,\n           training=None):\n    # Process inputs.\n    inputs *= self.num_units**0.5\n    if self.position_encoder is not None:\n      inputs = self.position_encoder(inputs, position=step + 1 if step is not None else None)\n    inputs = common.dropout(inputs, self.dropout, training=training)\n\n    # Prepare query mask.\n    mask = None\n    if step is None:\n      maximum_length = tf.shape(inputs)[1]\n      if sequence_length is None:\n        batch_size = tf.shape(inputs)[0]\n        sequence_length = tf.fill([batch_size], maximum_length)\n      mask = transformer.future_mask(sequence_length, maximum_length=maximum_length)\n\n    # Prepare memory mask.\n    memory_mask = None\n    if memory is not None:\n      if not isinstance(memory, (list, tuple)):\n        memory = (memory,)\n    if memory_sequence_length is not None:\n      if not isinstance(memory_sequence_length, (list, tuple)):\n        memory_sequence_length = (memory_sequence_length,)\n      memory_mask = [\n          tf.sequence_mask(mem_length, maxlen=tf.shape(mem)[1])\n          for mem, mem_length in zip(memory, memory_sequence_length)]\n\n    # Run each layer.\n    new_cache = []\n    for i, layer in enumerate(self.layers):\n      inputs, layer_cache, attention = layer(\n          inputs,\n          mask=mask,\n          memory=memory,\n          memory_mask=memory_mask,\n          cache=cache[i] if cache is not None else None,\n          training=training)\n      new_cache.append(layer_cache)\n    outputs = self.layer_norm(inputs)\n    return outputs, new_cache, attention\n\n  def forward(self,\n              inputs,\n              sequence_length=None,\n              initial_state=None,\n              memory=None,\n              memory_sequence_length=None,\n              input_fn=None,\n              sampling_probability=None,\n              training=None):\n    _ = initial_state\n    _ = input_fn\n    if sampling_probability is not None:\n      raise ValueError(""Scheduled sampling is not supported by this decoder"")\n    outputs, state, attention = self._run(\n        inputs,\n        sequence_length=sequence_length,\n        memory=memory,\n        memory_sequence_length=memory_sequence_length,\n        training=training)\n    logits = self.output_layer(outputs)\n    return logits, state, attention\n\n  def step(self,\n           inputs,\n           timestep,\n           state=None,\n           memory=None,\n           memory_sequence_length=None,\n           training=None):\n    inputs = tf.expand_dims(inputs, 1)\n    outputs, state, attention = self._run(\n        inputs,\n        cache=state,\n        memory=memory,\n        memory_sequence_length=memory_sequence_length,\n        step=timestep,\n        training=training)\n    outputs = tf.squeeze(outputs, axis=1)\n    if attention is not None:\n      attention = tf.squeeze(attention, axis=1)\n    return outputs, state, attention\n\n  def _get_initial_state(self, batch_size, dtype, initial_state=None):\n    # The decoder state contains the keys and values projections of the previous timesteps.\n    _ = initial_state\n    cache = []\n    for _ in self.layers:\n      shape = [batch_size, self.num_heads, 0, self.num_units // self.num_heads]\n      self_kv = (tf.zeros(shape, dtype=dtype), tf.zeros(shape, dtype=dtype))\n      memory_kv = [\n          (tf.zeros(shape, dtype=dtype), tf.zeros(shape, dtype=dtype))\n          for _ in range(self.num_sources)]\n      cache.append(dict(self_kv=self_kv, memory_kv=memory_kv))\n    return cache\n\n  def _get_state_reorder_flags(self):\n    # We don\'t need to reorder memory_kv as it is the same for all beams.\n    return [\n        {\n            ""self_kv"": (True, True),\n            ""memory_kv"": [(False, False) for _ in range(self.num_sources)]\n        } for _ in self.layers]\n'"
opennmt/encoders/__init__.py,0,"b'""""""Module defining encoders.""""""\n\nfrom opennmt.encoders.conv_encoder import ConvEncoder\n\nfrom opennmt.encoders.encoder import Encoder\nfrom opennmt.encoders.encoder import ParallelEncoder\nfrom opennmt.encoders.encoder import SequentialEncoder\n\nfrom opennmt.encoders.mean_encoder import MeanEncoder\n\nfrom opennmt.encoders.rnn_encoder import GNMTEncoder\nfrom opennmt.encoders.rnn_encoder import LSTMEncoder\nfrom opennmt.encoders.rnn_encoder import PyramidalRNNEncoder\nfrom opennmt.encoders.rnn_encoder import RNMTPlusEncoder\nfrom opennmt.encoders.rnn_encoder import RNNEncoder\n\nfrom opennmt.encoders.self_attention_encoder import SelfAttentionEncoder\n'"
opennmt/encoders/conv_encoder.py,4,"b'""""""Define convolution-based encoders.""""""\n\nimport tensorflow as tf\n\nfrom opennmt.encoders.encoder import Encoder\nfrom opennmt.layers import common\nfrom opennmt.layers import position\n\n\nclass ConvEncoder(Encoder):\n  """"""An encoder that applies a convolution over the input sequence\n  as described in https://arxiv.org/abs/1611.02344.\n  """"""\n\n  def __init__(self,\n               num_layers_a,\n               num_layers_c,\n               num_units,\n               kernel_size=3,\n               dropout=0.3,\n               position_encoder_class=position.PositionEmbedder):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers_a: The number of layers in CNN-a.\n      num_layers_c: The number of layers in CNN-c.\n      num_units: The number of output filters.\n      kernel_size: The kernel size.\n      dropout: The probability to drop units from the inputs.\n      position_encoder_class: The :class:`opennmt.layers.PositionEncoder`\n        class to use for position encoding (or a callable that returns an\n        instance).\n    """"""\n    super(ConvEncoder, self).__init__()\n    self.dropout = dropout\n    self.position_encoder = None\n    if position_encoder_class is not None:\n      self.position_encoder = position_encoder_class()\n    self.cnn_a = [\n        tf.keras.layers.Conv1D(num_units, kernel_size, padding=""same"")\n        for _ in range(num_layers_a)]\n    self.cnn_c = [\n        tf.keras.layers.Conv1D(num_units, kernel_size, padding=""same"")\n        for _ in range(num_layers_c)]\n\n  def call(self, inputs, sequence_length=None, training=None):\n    if self.position_encoder is not None:\n      inputs = self.position_encoder(inputs)\n    inputs = common.dropout(inputs, self.dropout, training=training)\n\n    cnn_a = _cnn_stack(self.cnn_a, inputs)\n    cnn_c = _cnn_stack(self.cnn_c, inputs)\n\n    outputs = cnn_a\n    state = tf.reduce_mean(cnn_c, axis=1)\n    return (outputs, state, sequence_length)\n\ndef _cnn_stack(layers, inputs):\n  next_input = inputs\n\n  for l, layer in enumerate(layers):\n    outputs = layer(next_input)\n    # Add residual connections past the first layer.\n    if l > 0:\n      outputs += next_input\n    next_input = tf.tanh(outputs)\n\n  return next_input\n'"
opennmt/encoders/encoder.py,4,"b'""""""Base class for encoders and generic multi encoders.""""""\n\nimport abc\nimport itertools\n\nimport tensorflow as tf\n\nfrom opennmt.layers.reducer import JoinReducer\n\n\nclass Encoder(tf.keras.layers.Layer):\n  """"""Base class for encoders.""""""\n\n  def build_mask(self, inputs, sequence_length=None, dtype=tf.bool):\n    """"""Builds a boolean mask for :obj:`inputs`.""""""\n    if sequence_length is None:\n      return None\n    return tf.sequence_mask(sequence_length, maxlen=tf.shape(inputs)[1], dtype=dtype)\n\n  @abc.abstractmethod\n  def call(self, inputs, sequence_length=None, training=None):  # pylint: disable=arguments-differ\n    """"""Encodes an input sequence.\n\n    Args:\n      inputs: The inputs to encode of shape :math:`[B, T, ...]`.\n      sequence_length: The length of each input with shape :math:`[B]`.\n      training: Run in training mode.\n\n    Returns:\n      A tuple ``(outputs, state, sequence_length)``.\n    """"""\n    raise NotImplementedError()\n\n\nclass SequentialEncoder(Encoder):\n  """"""An encoder that executes multiple encoders sequentially with optional\n  transition layers.\n\n  See for example ""Cascaded Encoder"" in https://arxiv.org/abs/1804.09849.\n  """"""\n\n  def __init__(self, encoders, states_reducer=JoinReducer(), transition_layer_fn=None):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      encoders: A list of :class:`opennmt.encoders.Encoder`.\n      states_reducer: A :class:`opennmt.layers.Reducer` to merge all\n        states.\n      transition_layer_fn: A callable or list of callables applied to the\n        output of an encoder before passing it as input to the next. If it is a\n        single callable, it is applied between every encoders. Otherwise, the\n        ``i`` th callable will be applied between encoders ``i`` and ``i + 1``.\n\n    Raises:\n      ValueError: if :obj:`transition_layer_fn` is a list with a size not equal\n        to the number of encoder transitions ``len(encoders) - 1``.\n    """"""\n    if (transition_layer_fn is not None and isinstance(transition_layer_fn, list)\n        and len(transition_layer_fn) != len(encoders) - 1):\n      raise ValueError(""The number of transition layers must match the number of encoder ""\n                       ""transitions, expected %d layers but got %d.""\n                       % (len(encoders) - 1, len(transition_layer_fn)))\n    super(SequentialEncoder, self).__init__()\n    self.encoders = encoders\n    self.states_reducer = states_reducer\n    self.transition_layer_fn = transition_layer_fn\n\n  def call(self, inputs, sequence_length=None, training=None):\n    encoder_state = []\n\n    for i, encoder in enumerate(self.encoders):\n      if i > 0 and self.transition_layer_fn is not None:\n        if isinstance(self.transition_layer_fn, list):\n          inputs = self.transition_layer_fn[i - 1](inputs)\n        else:\n          inputs = self.transition_layer_fn(inputs)\n      inputs, state, sequence_length = encoder(\n          inputs,\n          sequence_length=sequence_length,\n          training=training)\n      encoder_state.append(state)\n\n    return (\n        inputs,\n        self.states_reducer(encoder_state),\n        sequence_length)\n\n\nclass ParallelEncoder(Encoder):\n  """"""An encoder that encodes its input with several encoders and reduces the\n  outputs and states together. Additional layers can be applied on each encoder\n  output and on the combined output (e.g. to layer normalize each encoder\n  output).\n\n  If the input is a single ``tf.Tensor``, the same input will be encoded by\n  every encoders. Otherwise, when the input is a Python sequence (e.g. the non\n  reduced output of a :class:`opennmt.inputters.ParallelInputter`),\n  each encoder will encode its corresponding input in the sequence.\n\n  See for example ""Multi-Columnn Encoder"" in https://arxiv.org/abs/1804.09849.\n  """"""\n\n  def __init__(self,\n               encoders,\n               outputs_reducer=None,\n               states_reducer=None,\n               outputs_layer_fn=None,\n               combined_output_layer_fn=None):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      encoders: A list of :class:`opennmt.encoders.Encoder` or a single\n        one, in which case the same encoder is applied to each input.\n      outputs_reducer: A :class:`opennmt.layers.Reducer` to merge all\n        outputs. If ``None``, defaults to\n        :class:`opennmt.layers.JoinReducer`.\n      states_reducer: A :class:`opennmt.layers.Reducer` to merge all\n        states. If ``None``, defaults to\n        :class:`opennmt.layers.JoinReducer`.\n      outputs_layer_fn: A callable or list of callables applied to the\n        encoders outputs If it is a single callable, it is on each encoder\n        output. Otherwise, the ``i`` th callable is applied on encoder ``i``\n        output.\n      combined_output_layer_fn: A callable to apply on the combined output\n        (i.e. the output of :obj:`outputs_reducer`).\n\n    Raises:\n      ValueError: if :obj:`outputs_layer_fn` is a list with a size not equal\n        to the number of encoders.\n    """"""\n    if (isinstance(encoders, list)\n        and outputs_layer_fn is not None and isinstance(outputs_layer_fn, list)\n        and len(outputs_layer_fn) != len(encoders)):\n      raise ValueError(""The number of output layers must match the number of encoders; ""\n                       ""expected %d layers but got %d.""\n                       % (len(encoders), len(outputs_layer_fn)))\n    super(ParallelEncoder, self).__init__()\n    self.encoders = encoders\n    self.outputs_reducer = outputs_reducer if outputs_reducer is not None else JoinReducer()\n    self.states_reducer = states_reducer if states_reducer is not None else JoinReducer()\n    self.outputs_layer_fn = outputs_layer_fn\n    self.combined_output_layer_fn = combined_output_layer_fn\n\n  def call(self, inputs, sequence_length=None, training=None):\n    all_outputs = []\n    all_states = []\n    all_sequence_lengths = []\n    parallel_inputs = isinstance(inputs, (list, tuple))\n    parallel_encoders = isinstance(self.encoders, (list, tuple))\n\n    if parallel_encoders and parallel_inputs and len(inputs) != len(self.encoders):\n      raise ValueError(""ParallelEncoder expects as many inputs as parallel encoders"")\n    if parallel_encoders:\n      encoders = self.encoders\n    else:\n      encoders = itertools.repeat(self.encoders, len(inputs) if parallel_inputs else 1)\n\n    for i, encoder in enumerate(encoders):\n      if parallel_inputs:\n        encoder_inputs = inputs[i]\n        length = sequence_length[i]\n      else:\n        encoder_inputs = inputs\n        length = sequence_length\n\n      outputs, state, length = encoder(\n          encoder_inputs,\n          sequence_length=length,\n          training=training)\n\n      if self.outputs_layer_fn is not None:\n        if isinstance(self.outputs_layer_fn, list):\n          outputs = self.outputs_layer_fn[i](outputs)\n        else:\n          outputs = self.outputs_layer_fn(outputs)\n\n      all_outputs.append(outputs)\n      all_states.append(state)\n      all_sequence_lengths.append(length)\n\n    outputs, sequence_length = self.outputs_reducer(\n        all_outputs, sequence_length=all_sequence_lengths)\n\n    if self.combined_output_layer_fn is not None:\n      outputs = self.combined_output_layer_fn(outputs)\n\n    return (outputs, self.states_reducer(all_states), sequence_length)\n'"
opennmt/encoders/mean_encoder.py,3,"b'""""""Define a minimal encoder.""""""\n\nimport tensorflow as tf\n\nfrom opennmt.encoders.encoder import Encoder\n\n\nclass MeanEncoder(Encoder):\n  """"""A simple encoder that takes the mean of its inputs.""""""\n\n  def call(self, inputs, sequence_length=None, training=None):\n    outputs = tf.identity(inputs)\n    if sequence_length is not None:\n      inputs = tf.RaggedTensor.from_tensor(inputs, lengths=sequence_length)\n    state = tf.reduce_mean(inputs, axis=1)\n    return (outputs, state, sequence_length)\n'"
opennmt/encoders/rnn_encoder.py,7,"b'""""""Define RNN-based encoders.""""""\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom opennmt.encoders.encoder import Encoder, SequentialEncoder\nfrom opennmt.layers.reducer import ConcatReducer, JoinReducer, pad_in_time\nfrom opennmt.layers import common\nfrom opennmt.layers import rnn\n\n\nclass _RNNEncoderBase(Encoder):\n  """"""Base class for RNN encoders.""""""\n\n  def __init__(self, rnn_layer, **kwargs):\n    """"""Initializes the encoder.\n\n    Args:\n      rnn_layer: The RNN layer used to encode the inputs.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(_RNNEncoderBase, self).__init__(**kwargs)\n    self.rnn = rnn_layer\n\n  def call(self, inputs, sequence_length=None, training=None):\n    mask = self.build_mask(inputs, sequence_length=sequence_length)\n    outputs, states = self.rnn(inputs, mask=mask, training=training)\n    return outputs, states, sequence_length\n\n\nclass RNNEncoder(_RNNEncoderBase):\n  """"""A RNN sequence encoder.""""""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               bidirectional=False,\n               residual_connections=False,\n               dropout=0.3,\n               reducer=ConcatReducer(),\n               cell_class=None,\n               **kwargs):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      bidirectional: Use a bidirectional RNN.\n      residual_connections: If ``True``, each layer input will be added to its\n        output.\n      dropout: The probability to drop units in each layer output.\n      reducer: A :class:`opennmt.layers.Reducer` instance to merge\n        bidirectional state and outputs.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a LSTM cell.\n      **kwargs: Additional layer arguments.\n    """"""\n    cell = rnn.make_rnn_cell(\n        num_layers,\n        num_units,\n        dropout=dropout,\n        residual_connections=residual_connections,\n        cell_class=cell_class)\n    rnn_layer = rnn.RNN(cell, bidirectional=bidirectional, reducer=reducer)\n    super(RNNEncoder, self).__init__(rnn_layer, **kwargs)\n\n  def map_v1_weights(self, weights):\n    return self.rnn.map_v1_weights(weights)\n\n\nclass LSTMEncoder(_RNNEncoderBase):\n  """"""A LSTM sequence encoder.\n\n  See Also:\n    :class:`opennmt.layers.LSTM` for differences between this encoder and\n    :class:`opennmt.encoders.RNNEncoder` with a `LSTMCell`.\n  """"""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               bidirectional=False,\n               residual_connections=False,\n               dropout=0.3,\n               reducer=ConcatReducer(),\n               **kwargs):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer output.\n      bidirectional: Make each LSTM layer bidirectional.\n      residual_connections: If ``True``, each layer input will be added to its\n        output.\n      dropout: The probability to drop units in each layer output.\n      reducer: A :class:`opennmt.layers.Reducer` instance to merge\n        bidirectional state and outputs.\n      **kwargs: Additional layer arguments.\n    """"""\n    lstm_layer = rnn.LSTM(\n        num_layers,\n        num_units,\n        bidirectional=bidirectional,\n        reducer=reducer,\n        dropout=dropout,\n        residual_connections=residual_connections)\n    super(LSTMEncoder, self).__init__(lstm_layer, **kwargs)\n\n\nclass GNMTEncoder(SequentialEncoder):\n  """"""The RNN encoder used in GNMT as described in\n  https://arxiv.org/abs/1609.08144.\n  """"""\n\n  def __init__(self, num_layers, num_units, dropout=0.3):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      dropout: The probability to drop units in each layer output.\n\n    Raises:\n      ValueError: if :obj:`num_layers` < 2.\n    """"""\n    if num_layers < 2:\n      raise ValueError(""GNMTEncoder requires at least 2 layers"")\n    bidirectional = LSTMEncoder(\n        1,\n        num_units,\n        bidirectional=True,\n        dropout=dropout)\n    unidirectional = LSTMEncoder(\n        num_layers - 1,\n        num_units,\n        dropout=dropout,\n        residual_connections=True)\n    super(GNMTEncoder, self).__init__([bidirectional, unidirectional])\n\n\nclass RNMTPlusEncoder(SequentialEncoder):\n  """"""The RNMT+ encoder described in https://arxiv.org/abs/1804.09849.""""""\n\n  def __init__(self,\n               num_layers=6,\n               num_units=1024,\n               cell_class=None,\n               dropout=0.3):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each RNN layer and the final output.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a layer normalized LSTM cell.\n      dropout: The probability to drop units in each layer output.\n    """"""\n    if cell_class is None:\n      cell_class = tfa.rnn.LayerNormLSTMCell\n    layers = [\n        RNNEncoder(\n            1,\n            num_units,\n            bidirectional=True,\n            dropout=0.0,\n            cell_class=cell_class)\n        for _ in range(num_layers)]\n    layers = [\n        common.LayerWrapper(layer, output_dropout=dropout, residual_connection=True)\n        for layer in layers]\n    super(RNMTPlusEncoder, self).__init__(layers)\n    self.dropout = dropout\n    self.projection = tf.keras.layers.Dense(num_units)\n\n  def call(self, inputs, sequence_length=None, training=None):\n    inputs = common.dropout(inputs, self.dropout, training=training)\n    outputs, state, sequence_length = super(RNMTPlusEncoder, self).call(\n        inputs, sequence_length=sequence_length, training=training)\n    projected = self.projection(outputs)\n    return (projected, state, sequence_length)\n\n\nclass PyramidalRNNEncoder(Encoder):\n  """"""An encoder that reduces the time dimension after each bidirectional layer.""""""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               reduction_factor=2,\n               cell_class=None,\n               dropout=0.3):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of units in each layer.\n      reduction_factor: The time reduction factor.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a LSTM cell.\n      dropout: The probability to drop units in each layer output.\n    """"""\n    super(PyramidalRNNEncoder, self).__init__()\n    self.reduction_factor = reduction_factor\n    self.state_reducer = JoinReducer()\n    self.layers = [\n        RNNEncoder(\n            1,\n            num_units // 2,\n            bidirectional=True,\n            reducer=ConcatReducer(),\n            cell_class=cell_class,\n            dropout=dropout)\n        for _ in range(num_layers)]\n\n  def call(self, inputs, sequence_length=None, training=None):\n    encoder_state = []\n\n    for layer_index, layer in enumerate(self.layers):\n      input_depth = inputs.shape[-1]\n\n      if layer_index == 0:\n        # For the first input, make the number of timesteps a multiple of the\n        # total reduction factor.\n        total_reduction_factor = pow(self.reduction_factor, len(self.layers) - 1)\n\n        current_length = tf.shape(inputs)[1]\n        factor = tf.cast(current_length, tf.float32) / total_reduction_factor\n        new_length = tf.cast(tf.math.ceil(factor), tf.int32) * total_reduction_factor\n        inputs = pad_in_time(inputs, new_length - current_length)\n\n        # Lengths should not be smaller than the total reduction factor.\n        sequence_length = tf.maximum(sequence_length, total_reduction_factor)\n      else:\n        # In other cases, reduce the time dimension.\n        inputs = tf.reshape(\n            inputs,\n            [tf.shape(inputs)[0], -1, input_depth * self.reduction_factor])\n        if sequence_length is not None:\n          sequence_length //= self.reduction_factor\n\n      outputs, state, sequence_length = layer(\n          inputs,\n          sequence_length=sequence_length,\n          training=training)\n\n      encoder_state.append(state)\n      inputs = outputs\n\n    return (\n        outputs,\n        self.state_reducer(encoder_state),\n        sequence_length)\n'"
opennmt/encoders/self_attention_encoder.py,1,"b'""""""Define the self-attention encoder.""""""\n\nimport tensorflow as tf\n\nfrom opennmt.layers import transformer\n\nfrom opennmt.encoders.encoder import Encoder\nfrom opennmt.layers.position import SinusoidalPositionEncoder\nfrom opennmt.layers import common\n\n\nclass SelfAttentionEncoder(Encoder):\n  """"""Encoder using self-attention as described in\n  https://arxiv.org/abs/1706.03762.\n  """"""\n\n  def __init__(self,\n               num_layers,\n               num_units=512,\n               num_heads=8,\n               ffn_inner_dim=2048,\n               dropout=0.1,\n               attention_dropout=0.1,\n               ffn_dropout=0.1,\n               ffn_activation=tf.nn.relu,\n               position_encoder_class=SinusoidalPositionEncoder,\n               maximum_relative_position=None,\n               **kwargs):\n    """"""Initializes the parameters of the encoder.\n\n    Args:\n      num_layers: The number of layers.\n      num_units: The number of hidden units.\n      num_heads: The number of heads in the multi-head attention.\n      ffn_inner_dim: The number of units of the inner linear transformation\n        in the feed forward layer.\n      dropout: The probability to drop units from the outputs.\n      attention_dropout: The probability to drop units from the attention.\n      ffn_dropout: The probability to drop units from the activation output in\n        the feed forward layer.\n      ffn_activation: The activation function to apply between the two linear\n        transformations of the feed forward layer.\n      position_encoder_class: The :class:`opennmt.layers.PositionEncoder`\n        class to use for position encoding (or a callable that returns an\n        instance).\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n      **kwargs: Additional layer arguments.\n    """"""\n    super(SelfAttentionEncoder, self).__init__(**kwargs)\n    self.num_units = num_units\n    self.dropout = dropout\n    self.position_encoder = None\n    if position_encoder_class is not None:\n      self.position_encoder = position_encoder_class()\n    self.layer_norm = common.LayerNorm()\n    self.layers = [\n        transformer.SelfAttentionEncoderLayer(\n            num_units,\n            num_heads,\n            ffn_inner_dim,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            ffn_dropout=ffn_dropout,\n            ffn_activation=ffn_activation,\n            maximum_relative_position=maximum_relative_position)\n        for i in range(num_layers)]\n\n  def call(self, inputs, sequence_length=None, training=None):\n    inputs *= self.num_units**0.5\n    if self.position_encoder is not None:\n      inputs = self.position_encoder(inputs)\n    inputs = common.dropout(inputs, self.dropout, training=training)\n    mask = self.build_mask(inputs, sequence_length=sequence_length)\n    for layer in self.layers:\n      inputs = layer(inputs, mask=mask, training=training)\n    outputs = self.layer_norm(inputs)\n    return outputs, None, sequence_length\n\n  def map_v1_weights(self, weights):\n    m = []\n    m += self.layer_norm.map_v1_weights(weights[""LayerNorm""])\n    for i, layer in enumerate(self.layers):\n      m += layer.map_v1_weights(weights[""layer_%d"" % i])\n    return m\n'"
opennmt/inputters/__init__.py,0,"b'""""""Module defining inputters.\n\nInputters implement the logic of transforming raw data to vectorized inputs,\ne.g., from a line of text to a sequence of embeddings.\n""""""\n\nfrom opennmt.inputters.inputter import ExampleInputter\nfrom opennmt.inputters.inputter import ExampleInputterAdapter\nfrom opennmt.inputters.inputter import Inputter\nfrom opennmt.inputters.inputter import MixedInputter\nfrom opennmt.inputters.inputter import MultiInputter\nfrom opennmt.inputters.inputter import ParallelInputter\n\nfrom opennmt.inputters.record_inputter import SequenceRecordInputter\nfrom opennmt.inputters.record_inputter import create_sequence_records\nfrom opennmt.inputters.record_inputter import write_sequence_record\n\nfrom opennmt.inputters.text_inputter import CharEmbedder\nfrom opennmt.inputters.text_inputter import CharConvEmbedder\nfrom opennmt.inputters.text_inputter import CharRNNEmbedder\nfrom opennmt.inputters.text_inputter import TextInputter\nfrom opennmt.inputters.text_inputter import WordEmbedder\nfrom opennmt.inputters.text_inputter import add_sequence_controls\nfrom opennmt.inputters.text_inputter import load_pretrained_embeddings\n'"
opennmt/inputters/inputter.py,14,"b'""""""Define generic inputters.""""""\n\nimport abc\n\nimport tensorflow as tf\n\nfrom opennmt.data import dataset as dataset_util\nfrom opennmt.layers import common\nfrom opennmt.layers.reducer import ConcatReducer, JoinReducer\nfrom opennmt.utils import misc\n\n\nclass Inputter(tf.keras.layers.Layer):\n  """"""Base class for inputters.""""""\n\n  def __init__(self, **kwargs):\n    super(Inputter, self).__init__(**kwargs)\n    self.asset_prefix = None\n\n  @property\n  def num_outputs(self):\n    """"""The number of parallel outputs produced by this inputter.""""""\n    return 1\n\n  def initialize(self, data_config, asset_prefix=""""):\n    """"""Initializes the inputter.\n\n    Args:\n      data_config: A dictionary containing the data configuration set\n        by the user.\n      asset_prefix: The prefix to attach to assets filename.\n    """"""\n    _ = data_config\n    _ = asset_prefix\n    return\n\n  def export_assets(self, asset_dir, asset_prefix=""""):\n    """"""Exports assets used by this tokenizer.\n\n    Args:\n      asset_dir: The directory where assets can be written.\n      asset_prefix: The prefix to attach to assets filename.\n\n    Returns:\n      A dictionary containing additional assets used by the inputter.\n    """"""\n    _ = asset_dir\n    _ = asset_prefix\n    return {}\n\n  @abc.abstractmethod\n  def make_dataset(self, data_file, training=None):\n    """"""Creates the base dataset required by this inputter.\n\n    Args:\n      data_file: The data file.\n      training: Run in training mode.\n\n    Returns:\n      A ``tf.data.Dataset`` instance or a list of ``tf.data.Dataset`` instances.\n    """"""\n    raise NotImplementedError()\n\n  def get_dataset_size(self, data_file):\n    """"""Returns the dataset size.\n\n    If the inputter can efficiently compute the dataset size from a training\n    file on disk, it can optionally override this method. Otherwise, we may\n    compute the size later with a generic and slower approach (iterating over\n    the dataset instance).\n\n    Args:\n      data_file: The data file.\n\n    Returns:\n      The dataset size or ``None``.\n    """"""\n    _ = data_file\n    return None\n\n  def make_inference_dataset(self,\n                             features_file,\n                             batch_size,\n                             length_bucket_width=None,\n                             num_threads=1,\n                             prefetch_buffer_size=None):\n    """"""Builds a dataset to be used for inference.\n\n    For evaluation and training datasets, see\n    :class:`opennmt.inputters.ExampleInputter`.\n\n    Args:\n      features_file: The test file.\n      batch_size: The batch size to use.\n      length_bucket_width: The width of the length buckets to select batch\n        candidates from (for efficiency). Set ``None`` to not constrain batch\n        formation.\n      num_threads: The number of elements processed in parallel.\n      prefetch_buffer_size: The number of batches to prefetch asynchronously. If\n        ``None``, use an automatically tuned value.\n\n    Returns:\n      A ``tf.data.Dataset``.\n\n    See Also:\n      :func:`opennmt.data.inference_pipeline`\n    """"""\n\n    def _map_fn(*arg):\n      features = self.make_features(element=misc.item_or_tuple(arg), training=False)\n      if isinstance(features, (list, tuple)):\n        # Special case for unsupervised inputters that always return a tuple (features, labels).\n        return features[0]\n      return features\n\n    transform_fns = [lambda dataset: dataset.map(_map_fn, num_parallel_calls=num_threads or 1)]\n\n    dataset = self.make_dataset(features_file, training=False)\n    dataset = dataset.apply(dataset_util.inference_pipeline(\n        batch_size,\n        transform_fns=transform_fns,\n        length_bucket_width=length_bucket_width,\n        length_fn=self.get_length,\n        num_threads=num_threads,\n        prefetch_buffer_size=prefetch_buffer_size))\n    return dataset\n\n  @abc.abstractmethod\n  def input_signature(self):\n    """"""Returns the input signature of this inputter.""""""\n    raise NotImplementedError()\n\n  def get_length(self, features, ignore_special_tokens=False):\n    """"""Returns the length of the input features, if defined.\n\n    Args:\n      features: The dictionary of input features.\n      ignore_special_tokens: Ignore special tokens that were added by the\n        inputter (e.g. <s> and/or </s>).\n\n    Returns:\n      The length.\n    """"""\n    _ = ignore_special_tokens\n    return features.get(""length"")\n\n  @abc.abstractmethod\n  def make_features(self, element=None, features=None, training=None):\n    """"""Creates features from data.\n\n    This is typically called in a data pipeline (such as ``Dataset.map``).\n    Common transformation includes tokenization, parsing, vocabulary lookup,\n    etc.\n\n    This method accept both a single :obj:`element` from the dataset or a\n    partially built dictionary of :obj:`features`.\n\n    Args:\n      element: An element from the dataset returned by\n        :meth:`opennmt.inputters.Inputter.make_dataset`.\n      features: An optional and possibly partial dictionary of features to\n        augment.\n      training: Run in training mode.\n\n    Returns:\n      A dictionary of ``tf.Tensor``.\n    """"""\n    raise NotImplementedError()\n\n  def keep_for_training(self, features, maximum_length=None):\n    """"""Returns ``True`` if this example should be kept for training.\n\n    Args:\n      features: A dictionary of ``tf.Tensor``.\n      maximum_length: The maximum length used for training.\n\n    Returns:\n      A boolean.\n    """"""\n    if isinstance(features, (list, tuple)):\n      # Special case for unsupervised inputters that always return a tuple (features, labels).\n      features = features[0]\n    length = self.get_length(features)\n    if length is None:\n      return True\n    is_valid = tf.greater(length, 0)\n    if maximum_length is not None:\n      is_valid = tf.logical_and(is_valid, tf.less_equal(length, maximum_length))\n    return is_valid\n\n  def call(self, features, training=None):  # pylint: disable=arguments-differ\n    """"""Creates the model input from the features (e.g. word embeddings).\n\n    Args:\n      features: A dictionary of ``tf.Tensor``, the output of\n        :meth:`opennmt.inputters.Inputter.make_features`.\n      training: Run in training mode.\n\n    Returns:\n      The model input.\n    """"""\n    _ = training\n    return features\n\n  def visualize(self, model_root, log_dir):\n    """"""Visualizes the transformation, usually embeddings.\n\n    Args:\n      model_root: The root model object.\n      log_dir: The active log directory.\n    """"""\n    _ = model_root\n    _ = log_dir\n    return\n\n\nclass MultiInputter(Inputter):\n  """"""An inputter that gathers multiple inputters, possibly nested.""""""\n\n  def __init__(self, inputters, reducer=None):\n    if not isinstance(inputters, list) or not inputters:\n      raise ValueError(""inputters must be a non empty list"")\n    dtype = inputters[0].dtype\n    for inputter in inputters:\n      if inputter.dtype != dtype:\n        raise TypeError(""All inputters must have the same dtype"")\n    super(MultiInputter, self).__init__(dtype=dtype)\n    self.inputters = inputters\n    self.reducer = reducer\n\n  @property\n  def num_outputs(self):\n    if self.reducer is None or isinstance(self.reducer, JoinReducer):\n      return len(self.inputters)\n    return 1\n\n  def get_leaf_inputters(self):\n    """"""Returns a list of all leaf Inputter instances.""""""\n    inputters = []\n    for inputter in self.inputters:\n      if isinstance(inputter, MultiInputter):\n        inputters.extend(inputter.get_leaf_inputters())\n      else:\n        inputters.append(inputter)\n    return inputters\n\n  def __getattribute__(self, name):\n    if name == ""built"":\n      return all(inputter.built for inputter in self.inputters)\n    else:\n      return super(MultiInputter, self).__getattribute__(name)\n\n  def initialize(self, data_config, asset_prefix=""""):\n    for i, inputter in enumerate(self.inputters):\n      inputter.initialize(\n          data_config, asset_prefix=_get_asset_prefix(asset_prefix, inputter, i))\n\n  def export_assets(self, asset_dir, asset_prefix=""""):\n    assets = {}\n    for i, inputter in enumerate(self.inputters):\n      assets.update(inputter.export_assets(\n          asset_dir, asset_prefix=_get_asset_prefix(asset_prefix, inputter, i)))\n    return assets\n\n  @abc.abstractmethod\n  def make_dataset(self, data_file, training=None):\n    raise NotImplementedError()\n\n  def get_dataset_size(self, data_file):\n    for inputter, data in zip(self.inputters, data_file):\n      size = inputter.get_dataset_size(data)\n      if size is not None:\n        return size\n    return None\n\n  def visualize(self, model_root, log_dir):\n    for inputter in self.inputters:\n      inputter.visualize(model_root, log_dir)\n\n\ndef _get_asset_prefix(prefix, inputter, i):\n  return ""%s%s_"" % (prefix, inputter.asset_prefix or str(i + 1))\n\n\nclass ParallelInputter(MultiInputter):\n  """"""A multi inputter that processes parallel data.""""""\n\n  def __init__(self,\n               inputters,\n               reducer=None,\n               share_parameters=False,\n               combine_features=True):\n    """"""Initializes a parallel inputter.\n\n    Args:\n      inputters: A list of :class:`opennmt.inputters.Inputter`.\n      reducer: A :class:`opennmt.layers.Reducer` to merge all inputs. If\n        set, parallel inputs are assumed to have the same length.\n      share_parameters: Share the inputters parameters.\n      combine_features: Combine each inputter features in a single dict or\n        return them separately. This is typically ``True`` for multi source\n        inputs but ``False`` for features/labels parallel data.\n\n    Raises:\n      ValueError: if :obj:`share_parameters` is set but the child inputters are\n        not of the same type.\n    """"""\n    super(ParallelInputter, self).__init__(inputters, reducer=reducer)\n    self.combine_features = combine_features\n    self.share_parameters = share_parameters\n    if self.share_parameters:\n      leaves = self.get_leaf_inputters()\n      for inputter in leaves[1:]:\n        if type(inputter) is not type(leaves[0]):\n          raise ValueError(""Each inputter must be of the same type for parameter sharing"")\n\n  def _structure(self):\n    """"""Returns the nested structure that represents this parallel inputter.""""""\n    return [\n        inputter._structure()  # pylint: disable=protected-access\n        if isinstance(inputter, ParallelInputter) else None\n        for inputter in self.inputters]\n\n  def make_dataset(self, data_file, training=None):\n    if not isinstance(data_file, list):\n      data_file = [data_file]\n\n    # For evaluation and inference, accept a flat list of data files for nested inputters.\n    # This is needed when nesting can\'t easily be represented (e.g. on the command line).\n    if not training:\n      try:\n        data_file = tf.nest.pack_sequence_as(self._structure(), tf.nest.flatten(data_file))\n      except ValueError:\n        data_file = []  # This will raise the error below.\n\n    if len(data_file) != len(self.inputters):\n      raise ValueError(""The number of data files must be the same as the number of inputters"")\n\n    num_files = -1\n    datasets = []\n    for i, (inputter, data) in enumerate(zip(self.inputters, data_file)):\n      dataset = inputter.make_dataset(data, training=training)\n      if not isinstance(dataset, list):\n        dataset = [dataset]\n      datasets.append(dataset)\n      if num_files < 0:\n        num_files = len(dataset)\n      elif len(dataset) != num_files:\n        raise ValueError(""All parallel inputs must have the same number of data files, ""\n                         ""saw %d files for input 0 but got %d files for input %d"" % (\n                             num_files, len(dataset), i))\n\n    parallel_datasets = [\n        tf.data.Dataset.zip(tuple(parallel_dataset)) for parallel_dataset in zip(*datasets)]\n    if len(parallel_datasets) == 1:\n      return parallel_datasets[0]\n    if not training:\n      raise ValueError(""Only training data can be configured to multiple files"")\n    return parallel_datasets\n\n  def input_signature(self):\n    if self.combine_features:\n      signature = {}\n      for i, inputter in enumerate(self.inputters):\n        for key, value in inputter.input_signature().items():\n          signature[""{}_{}"".format(key, i)] = value\n      return signature\n    else:\n      return tuple(inputter.input_signature() for inputter in self.inputters)\n\n  def _index_features(self, features, index):\n    if self.combine_features:\n      return misc.extract_prefixed_keys(features, ""inputter_{}_"".format(index))\n    else:\n      return features[index]\n\n  def get_length(self, features, ignore_special_tokens=False):\n    lengths = [\n        inputter.get_length(\n            self._index_features(features, i),\n            ignore_special_tokens=ignore_special_tokens)\n        for i, inputter in enumerate(self.inputters)]\n    if self.reducer is None:\n      return lengths\n    else:\n      return lengths[0]\n\n  def make_features(self, element=None, features=None, training=None):\n    if self.combine_features:\n      if features is None:\n        features = {}\n      for i, inputter in enumerate(self.inputters):\n        prefix = ""inputter_%d_"" % i\n        sub_features = misc.extract_prefixed_keys(features, prefix)\n        if not sub_features:\n          # Also try to read the format produced by the serving features.\n          sub_features = misc.extract_suffixed_keys(features, ""_%d"" % i)\n        sub_features = inputter.make_features(\n            element=element[i] if element is not None else None,\n            features=sub_features,\n            training=training)\n        for key, value in sub_features.items():\n          features[""%s%s"" % (prefix, key)] = value\n      return features\n    else:\n      if features is None:\n        features = [{} for _ in self.inputters]\n      else:\n        features = list(features)\n      for i, inputter in enumerate(self.inputters):\n        features[i] = inputter.make_features(\n            element=element[i] if element is not None else None,\n            features=features[i],\n            training=training)\n      return tuple(features)\n\n  def keep_for_training(self, features, maximum_length=None):\n    if not isinstance(maximum_length, list):\n      maximum_length = [maximum_length]\n    # Unset maximum lengths are set to None (i.e. no constraint).\n    maximum_length += [None] * (len(self.inputters) - len(maximum_length))\n    constraints = []\n    for i, inputter in enumerate(self.inputters):\n      keep = inputter.keep_for_training(\n          self._index_features(features, i), maximum_length=maximum_length[i])\n      if isinstance(keep, bool):\n        if not keep:\n          return False\n        continue\n      constraints.append(keep)\n    if not constraints:\n      return True\n    return tf.reduce_all(constraints)\n\n  def build(self, input_shape):\n    if self.share_parameters:\n      # When sharing parameters, build the first leaf inputter and then set\n      # all attributes with parameters to the other inputters.\n      leaves = self.get_leaf_inputters()\n      first, others = leaves[0], leaves[1:]\n      first.build(input_shape)\n      for name, attr in first.__dict__.copy().items():\n        if isinstance(attr, tf.Variable) or (isinstance(attr, tf.Module) and attr.variables):\n          for inputter in others:\n            setattr(inputter, name, attr)\n            inputter.built = True\n    else:\n      for inputter in self.inputters:\n        inputter.build(input_shape)\n    super(ParallelInputter, self).build(input_shape)\n\n  def call(self, features, training=None):\n    transformed = [\n        inputter(self._index_features(features, i), training=training)\n        for i, inputter in enumerate(self.inputters)]\n    if self.reducer is not None:\n      transformed = self.reducer(transformed)\n    return transformed\n\n\nclass MixedInputter(MultiInputter):\n  """"""An multi inputter that applies several transformation on the same data\n  (e.g. combine word-level and character-level embeddings).\n  """"""\n\n  def __init__(self,\n               inputters,\n               reducer=ConcatReducer(),\n               dropout=0.0):\n    """"""Initializes a mixed inputter.\n\n    Args:\n      inputters: A list of :class:`opennmt.inputters.Inputter`.\n      reducer: A :class:`opennmt.layers.Reducer` to merge all inputs.\n      dropout: The probability to drop units in the merged inputs.\n    """"""\n    super(MixedInputter, self).__init__(inputters, reducer=reducer)\n    self.dropout = dropout\n\n  def make_dataset(self, data_file, training=None):\n    datasets = [\n        inputter.make_dataset(data_file, training=training)\n        for inputter in self.inputters]\n    for dataset in datasets[1:]:\n      if not isinstance(dataset, datasets[0].__class__):\n        raise ValueError(""All inputters should use the same dataset in a MixedInputter setting"")\n    return datasets[0]\n\n  def input_signature(self):\n    signature = {}\n    for inputter in self.inputters:\n      signature.update(inputter.input_signature())\n    return signature\n\n  def get_length(self, features, ignore_special_tokens=False):\n    return self.inputters[0].get_length(features, ignore_special_tokens=ignore_special_tokens)\n\n  def make_features(self, element=None, features=None, training=None):\n    if features is None:\n      features = {}\n    for inputter in self.inputters:\n      features = inputter.make_features(\n          element=element, features=features, training=training)\n    return features\n\n  def build(self, input_shape):\n    for inputter in self.inputters:\n      inputter.build(input_shape)\n    super(MixedInputter, self).build(input_shape)\n\n  def call(self, features, training=None):\n    transformed = []\n    for inputter in self.inputters:\n      transformed.append(inputter(features, training=training))\n    outputs = self.reducer(transformed)\n    outputs = common.dropout(outputs, self.dropout, training=training)\n    return outputs\n\n\nclass ExampleInputterAdapter:\n  """"""Extends an inputter with methods to build evaluation and training datasets.""""""\n\n  def make_evaluation_dataset(self,\n                              features_file,\n                              labels_file,\n                              batch_size,\n                              num_threads=1,\n                              prefetch_buffer_size=None):\n    """"""Builds a dataset to be used for evaluation.\n\n    Args:\n      features_file: The evaluation source file.\n      labels_file: The evaluation target file.\n      batch_size: The batch size to use.\n      num_threads: The number of elements processed in parallel.\n      prefetch_buffer_size: The number of batches to prefetch asynchronously. If\n        ``None``, use an automatically tuned value.\n\n    Returns:\n      A ``tf.data.Dataset``.\n\n    See Also:\n      :func:`opennmt.data.inference_pipeline`\n    """"""\n    if labels_file is not None:\n      data_files = [features_file, labels_file]\n    else:\n      data_files = features_file\n\n    map_fn = lambda *arg: self.make_features(element=misc.item_or_tuple(arg), training=False)\n    transform_fns = [lambda dataset: dataset.map(map_fn, num_parallel_calls=num_threads or 1)]\n\n    dataset = self.make_dataset(data_files, training=False)\n    dataset = dataset.apply(dataset_util.inference_pipeline(\n        batch_size,\n        transform_fns=transform_fns,\n        num_threads=num_threads,\n        prefetch_buffer_size=prefetch_buffer_size))\n    return dataset\n\n  def make_training_dataset(self,\n                            features_file,\n                            labels_file,\n                            batch_size,\n                            batch_type=""examples"",\n                            batch_multiplier=1,\n                            batch_size_multiple=1,\n                            shuffle_buffer_size=None,\n                            length_bucket_width=None,\n                            maximum_features_length=None,\n                            maximum_labels_length=None,\n                            single_pass=False,\n                            num_shards=1,\n                            shard_index=0,\n                            num_threads=4,\n                            prefetch_buffer_size=None,\n                            cardinality_multiple=1,\n                            weights=None):\n    """"""Builds a dataset to be used for training. It supports the full training\n    pipeline, including:\n\n    * sharding\n    * shuffling\n    * filtering\n    * bucketing\n    * prefetching\n\n    Args:\n      features_file: The source file or a list of training source files.\n      labels_file: The target file or a list of training target files.\n      batch_size: The batch size to use.\n      batch_type: The training batching stragety to use: can be ""examples"" or\n        ""tokens"".\n      batch_multiplier: The batch size multiplier to prepare splitting accross\n         replicated graph parts.\n      batch_size_multiple: When :obj:`batch_type` is ""tokens"", ensure that the\n        resulting batch size is a multiple of this value.\n      shuffle_buffer_size: The number of elements from which to sample.\n      length_bucket_width: The width of the length buckets to select batch\n        candidates from (for efficiency). Set ``None`` to not constrain batch\n        formation.\n      maximum_features_length: The maximum length or list of maximum lengths of\n        the features sequence(s). ``None`` to not constrain the length.\n      maximum_labels_length: The maximum length of the labels sequence.\n        ``None`` to not constrain the length.\n      single_pass: If ``True``, makes a single pass over the training data.\n      num_shards: The number of data shards (usually the number of workers in a\n        distributed setting).\n      shard_index: The shard index this data pipeline should read from.\n      num_threads: The number of elements processed in parallel.\n      prefetch_buffer_size: The number of batches to prefetch asynchronously. If\n        ``None``, use an automatically tuned value.\n      cardinality_multiple: Ensure that the dataset cardinality is a multiple of\n        this value when :obj:`single_pass` is ``True``.\n      weights: An optional list of weights to create a weighted dataset out of\n        multiple training files.\n\n    Returns:\n      A ``tf.data.Dataset``.\n\n    See Also:\n      :func:`opennmt.data.training_pipeline`\n    """"""\n    if labels_file is not None:\n      data_files = [features_file, labels_file]\n      maximum_length = [maximum_features_length, maximum_labels_length]\n      features_length_fn = self.features_inputter.get_length\n      labels_length_fn = self.labels_inputter.get_length\n    else:\n      data_files = features_file\n      maximum_length = maximum_features_length\n      features_length_fn = self.get_length\n      labels_length_fn = None\n\n    map_fn = lambda *arg: self.make_features(element=misc.item_or_tuple(arg), training=True)\n    filter_fn = lambda *arg: (\n        self.keep_for_training(misc.item_or_tuple(arg), maximum_length=maximum_length))\n    transform_fns = [\n        lambda dataset: dataset.map(map_fn, num_parallel_calls=num_threads or 4),\n        lambda dataset: dataset.filter(filter_fn)]\n\n    dataset = self.make_dataset(data_files, training=True)\n    if weights is not None:\n      dataset = (dataset, weights)\n    dataset = dataset_util.training_pipeline(\n        batch_size,\n        batch_type=batch_type,\n        batch_multiplier=batch_multiplier,\n        batch_size_multiple=batch_size_multiple,\n        transform_fns=transform_fns,\n        length_bucket_width=length_bucket_width,\n        features_length_fn=features_length_fn,\n        labels_length_fn=labels_length_fn,\n        single_pass=single_pass,\n        num_shards=num_shards,\n        shard_index=shard_index,\n        num_threads=num_threads,\n        dataset_size=self.get_dataset_size(data_files),\n        shuffle_buffer_size=shuffle_buffer_size,\n        prefetch_buffer_size=prefetch_buffer_size,\n        cardinality_multiple=cardinality_multiple)(dataset)\n    return dataset\n\n\nclass ExampleInputter(ParallelInputter, ExampleInputterAdapter):\n  """"""An inputter that returns training examples (parallel features and labels).""""""\n\n  def __init__(self, features_inputter, labels_inputter, share_parameters=False):\n    """"""Initializes this inputter.\n\n    Args:\n      features_inputter: An inputter producing the features (source).\n      labels_inputter: An inputter producing the labels (target).\n      share_parameters: Share the inputters parameters.\n    """"""\n    self.features_inputter = features_inputter\n    self.features_inputter.asset_prefix = ""source""\n    self.labels_inputter = labels_inputter\n    self.labels_inputter.asset_prefix = ""target""\n    super(ExampleInputter, self).__init__(\n        [self.features_inputter, self.labels_inputter],\n        share_parameters=share_parameters,\n        combine_features=False)\n\n  def make_inference_dataset(self,\n                             features_file,\n                             batch_size,\n                             length_bucket_width=None,\n                             num_threads=1,\n                             prefetch_buffer_size=None):\n    return self.features_inputter.make_inference_dataset(\n        features_file,\n        batch_size,\n        length_bucket_width=length_bucket_width,\n        num_threads=num_threads,\n        prefetch_buffer_size=prefetch_buffer_size)\n'"
opennmt/inputters/record_inputter.py,14,"b'""""""Define inputters reading from TFRecord files.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.data import dataset as dataset_util\nfrom opennmt.inputters.inputter import Inputter\n\n\nclass SequenceRecordInputter(Inputter):\n  """"""Inputter that reads ``tf.train.SequenceExample``.\n\n  See Also:\n    :func:`opennmt.inputters.create_sequence_records` to generate a compatible\n    dataset.\n  """"""\n\n  def __init__(self, input_depth, **kwargs):\n    """"""Initializes the parameters of the record inputter.\n\n    Args:\n      input_depth: The depth dimension of the input vectors.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(SequenceRecordInputter, self).__init__(**kwargs)\n    self.input_depth = input_depth\n\n  def make_dataset(self, data_file, training=None):\n    return dataset_util.make_datasets(tf.data.TFRecordDataset, data_file)\n\n  def input_signature(self):\n    return {\n        ""tensor"": tf.TensorSpec([None, None, self.input_depth], self.dtype),\n        ""length"": tf.TensorSpec([None], tf.int32)\n    }\n\n  def make_features(self, element=None, features=None, training=None):\n    if features is None:\n      features = {}\n    if ""tensor"" in features:\n      return features\n    _, feature_lists = tf.io.parse_single_sequence_example(element, sequence_features={\n        ""values"": tf.io.FixedLenSequenceFeature([self.input_depth], dtype=tf.float32)})\n    tensor = feature_lists[""values""]\n    features[""length""] = tf.shape(tensor)[0]\n    features[""tensor""] = tf.cast(tensor, self.dtype)\n    return features\n\n  def call(self, features, training=None):\n    return features[""tensor""]\n\n\ndef write_sequence_record(vector, writer):\n  """"""Writes a sequence vector as a TFRecord.\n\n  Args:\n    vector: A 2D Numpy float array of shape :math:`[T, D]`.\n    writer: A ``tf.io.TFRecordWriter``.\n\n  See Also:\n    - :class:`opennmt.inputters.SequenceRecordInputter`\n    - :func:`opennmt.inputters.create_sequence_records`\n  """"""\n  feature_list = tf.train.FeatureList(feature=[\n      tf.train.Feature(float_list=tf.train.FloatList(value=values))\n      for values in vector.astype(np.float32)])\n  feature_lists = tf.train.FeatureLists(feature_list={""values"": feature_list})\n  example = tf.train.SequenceExample(feature_lists=feature_lists)\n  writer.write(example.SerializeToString())\n\ndef create_sequence_records(vectors, path, compression=None):\n  """"""Creates a TFRecord file of sequence vectors.\n\n  Args:\n    vectors: An iterable of 2D Numpy float arrays of shape :math:`[T, D]`.\n    path: The output TFRecord file.\n    compression: Optional compression type, can be ""GZIP"".\n\n  Returns:\n    Path to the TFRecord file. In most cases this is the same as :obj:`path` but\n    if GZIP compression is enabled, the "".gz"" extension is added if not already\n    present.\n\n  Raises:\n    ValueError: if :obj:`compression` is invalid.\n\n  See Also:\n    - :class:`opennmt.inputters.SequenceRecordInputter`\n    - :func:`opennmt.inputters.write_sequence_record`\n  """"""\n  if compression is not None:\n    if compression not in (""GZIP"",):\n      raise ValueError(""invalid compression type: %s"" % compression)\n    if compression == ""GZIP"" and not path.endswith("".gz""):\n      path = ""%s.gz"" % path\n  writer = tf.io.TFRecordWriter(path, options=compression)\n  for vector in vectors:\n    write_sequence_record(vector, writer)\n  writer.close()\n  return path\n'"
opennmt/inputters/text_inputter.py,53,"b'""""""Define word-based embedders.""""""\n\nimport abc\nimport collections\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom google.protobuf import text_format\n\nfrom opennmt import constants, tokenizers\nfrom opennmt.data import dataset as dataset_util\nfrom opennmt.data import text\nfrom opennmt.data.vocab import Vocab\nfrom opennmt.inputters.inputter import Inputter\nfrom opennmt.layers import common\nfrom opennmt.utils import misc\n\n\ndef save_embeddings_metadata(log_dir, variable_name, vocabulary_file, num_oov_buckets=1):\n  """"""Registers an embedding variable for visualization in TensorBoard.\n\n  This function registers :obj:`variable_name` in the ``projector_config.pbtxt``\n  file and generates metadata from :obj:`vocabulary_file` to attach a label\n  to each word ID.\n\n  Args:\n    log_dir: The active log directory.\n    variable_name: The variable name in the checkpoint.\n    vocabulary_file: The associated vocabulary file.\n    num_oov_buckets: The number of additional unknown tokens.\n  """"""\n  # Assume it ends with /.ATTRIBUTES/VALUE\n  filename = ""%s.txt"" % ""_"".join(variable_name.split(""/"")[:-2])\n  metadata_path = os.path.join(log_dir, filename)\n\n  with tf.io.gfile.GFile(vocabulary_file, mode=""rb"") as src, \\\n       tf.io.gfile.GFile(metadata_path, mode=""wb"") as dst:\n    ws_index = 0\n    for line in src:\n      # The TensorBoard code checks line.trim().length == 0 when loading the\n      # metadata file so make sure lines are not dropped.\n      if not line.decode(""utf-8"").replace(u""\\uFEFF"", u"""").strip():\n        dst.write(tf.compat.as_bytes(""<whitespace%d>\\n"" % ws_index))\n        ws_index += 1\n      else:\n        dst.write(line)\n    if num_oov_buckets == 1:\n      dst.write(b""<unk>\\n"")\n    else:\n      for i in range(num_oov_buckets):\n        dst.write(tf.compat.as_bytes(""<unk%d>\\n"" % i))\n\n  config = projector.ProjectorConfig()\n\n  # If the projector file exists, load it.\n  config_path = os.path.join(log_dir, ""projector_config.pbtxt"")\n  if tf.io.gfile.exists(config_path):\n    with tf.io.gfile.GFile(config_path, mode=""rb"") as config_file:\n      text_format.Merge(config_file.read(), config)\n\n  # If this embedding is already registered, just update the metadata path.\n  exists = False\n  for meta in config.embeddings:\n    if meta.tensor_name == variable_name:\n      meta.metadata_path = filename\n      exists = True\n      break\n\n  if not exists:\n    embedding = config.embeddings.add()\n    embedding.tensor_name = variable_name\n    embedding.metadata_path = filename\n\n  with tf.io.gfile.GFile(config_path, ""w"") as config_file:\n    config_file.write(text_format.MessageToString(config))\n\ndef load_pretrained_embeddings(embedding_file,\n                               vocabulary_file,\n                               num_oov_buckets=1,\n                               with_header=True,\n                               case_insensitive_embeddings=True):\n  """"""Returns pretrained embeddings relative to the vocabulary.\n\n  The :obj:`embedding_file` must have the following format:\n\n  .. code-block:: text\n\n      N M\n      word1 val1 val2 ... valM\n      word2 val1 val2 ... valM\n      ...\n      wordN val1 val2 ... valM\n\n  or if :obj:`with_header` is ``False``:\n\n  .. code-block:: text\n\n      word1 val1 val2 ... valM\n      word2 val1 val2 ... valM\n      ...\n      wordN val1 val2 ... valM\n\n  This function will iterate on each embedding in :obj:`embedding_file` and\n  assign the pretrained vector to the associated word in :obj:`vocabulary_file`\n  if found. Otherwise, the embedding is ignored.\n\n  If :obj:`case_insensitive_embeddings` is ``True``, word embeddings are assumed\n  to be trained on lowercase data. In that case, word alignments are case\n  insensitive meaning the pretrained word embedding for ""the"" will be assigned\n  to ""the"", ""The"", ""THE"", or any other case variants included in\n  :obj:`vocabulary_file`.\n\n  Args:\n    embedding_file: Path the embedding file. Entries will be matched against\n      :obj:`vocabulary_file`.\n    vocabulary_file: The vocabulary file containing one word per line.\n    num_oov_buckets: The number of additional unknown tokens.\n    with_header: ``True`` if the embedding file starts with a header line like\n      in GloVe embedding files.\n    case_insensitive_embeddings: ``True`` if embeddings are trained on lowercase\n      data.\n\n  Returns:\n    A Numpy array of shape ``[vocabulary_size + num_oov_buckets, embedding_size]``.\n  """"""\n  # Map words to ids from the vocabulary.\n  word_to_id = collections.defaultdict(list)\n  with tf.io.gfile.GFile(vocabulary_file, mode=""rb"") as vocabulary:\n    count = 0\n    for word in vocabulary:\n      word = word.strip()\n      if case_insensitive_embeddings:\n        word = word.lower()\n      word_to_id[word].append(count)\n      count += 1\n\n  # Fill pretrained embedding matrix.\n  with tf.io.gfile.GFile(embedding_file, mode=""rb"") as embedding:\n    pretrained = None\n\n    if with_header:\n      next(embedding)\n\n    for line in embedding:\n      fields = line.strip().split()\n      word = fields[0]\n\n      if pretrained is None:\n        pretrained = np.random.normal(\n            size=(count + num_oov_buckets, len(fields) - 1))\n\n      # Lookup word in the vocabulary.\n      if word in word_to_id:\n        ids = word_to_id[word]\n        for index in ids:\n          pretrained[index] = np.asarray(fields[1:])\n\n  return pretrained\n\ndef add_sequence_controls(ids, length, start_id=None, end_id=None):\n  """"""Adds sequence control tokens.\n\n  Args:\n    ids: Sequence of ids as 1D or 2D (batch) tensor.\n    length: Sequence length as 0D or 1D (batch) tensor.\n    start_id: Id to prepend to the sequence (set ``None`` to disable).\n    end_id: Id to append to the sequence (set ``None`` to disable).\n\n  Returns:\n    A tuple ``(ids, length)``.\n  """"""\n  rank = ids.shape.rank\n  if rank not in (1, 2):\n    raise ValueError(""Unsupported rank %d (expected 1 or 2)"" % rank)\n  batch_size = tf.shape(ids)[0] if rank == 2 else None\n\n  def _make_column(value):\n    value = tf.constant(value, dtype=ids.dtype)\n    if batch_size is not None:\n      value = tf.fill([batch_size], value)\n    return tf.expand_dims(value, -1)\n\n  if start_id is not None:\n    start_ids = _make_column(constants.START_OF_SENTENCE_ID)\n    ids = tf.concat([start_ids, ids], axis=-1)\n    length += 1\n\n  if end_id is not None:\n    end_ids = _make_column(constants.END_OF_SENTENCE_ID)\n    if batch_size is not None:\n      # Run concat on RaggedTensor to handle sequences with variable length.\n      ids = tf.RaggedTensor.from_tensor(ids, lengths=length)\n    ids = tf.concat([ids, end_ids], axis=-1)\n    if batch_size is not None:\n      ids = ids.to_tensor()\n    length += 1\n\n  return ids, length\n\ndef _get_field(config, key, prefix=None, default=None, required=False):\n  if prefix:\n    key = ""%s%s"" % (prefix, key)\n  value = config.get(key, default)\n  if value is None and required:\n    raise ValueError(""Missing field \'%s\' in the data configuration"" % key)\n  return value\n\ndef _create_vocabulary_tables(vocabulary_file, num_oov_buckets, as_asset=True):\n  vocabulary = Vocab.from_file(vocabulary_file)\n  vocabulary_size = len(vocabulary)\n  if as_asset:\n    tokens_to_ids_initializer = tf.lookup.TextFileInitializer(\n        vocabulary_file,\n        tf.string,\n        tf.lookup.TextFileIndex.WHOLE_LINE,\n        tf.int64,\n        tf.lookup.TextFileIndex.LINE_NUMBER,\n        vocab_size=vocabulary_size)\n    ids_to_tokens_initializer = tf.lookup.TextFileInitializer(\n        vocabulary_file,\n        tf.int64,\n        tf.lookup.TextFileIndex.LINE_NUMBER,\n        tf.string,\n        tf.lookup.TextFileIndex.WHOLE_LINE,\n        vocab_size=vocabulary_size)\n  else:\n    tokens = tf.constant(vocabulary.words, dtype=tf.string)\n    ids = tf.constant(list(range(vocabulary_size)), dtype=tf.int64)\n    tokens_to_ids_initializer = tf.lookup.KeyValueTensorInitializer(tokens, ids)\n    ids_to_tokens_initializer = tf.lookup.KeyValueTensorInitializer(ids, tokens)\n  if num_oov_buckets > 0:\n    tokens_to_ids = tf.lookup.StaticVocabularyTable(tokens_to_ids_initializer, num_oov_buckets)\n  else:\n    tokens_to_ids = tf.lookup.StaticHashTable(tokens_to_ids_initializer, 0)\n  ids_to_tokens = tf.lookup.StaticHashTable(ids_to_tokens_initializer, constants.UNKNOWN_TOKEN)\n  return vocabulary_size + num_oov_buckets, tokens_to_ids, ids_to_tokens\n\n\nclass TextInputter(Inputter):\n  """"""An abstract inputter that processes text.""""""\n\n  def __init__(self, num_oov_buckets=1, **kwargs):\n    super(TextInputter, self).__init__(**kwargs)\n    self.num_oov_buckets = num_oov_buckets\n    self.noiser = None\n    self.in_place_noise = False\n    self.noise_probability = None\n\n  def initialize(self, data_config, asset_prefix=""""):\n    self.vocabulary_file = _get_field(\n        data_config, ""vocabulary"", prefix=asset_prefix, required=True)\n    self.vocabulary_size, self.tokens_to_ids, self.ids_to_tokens = _create_vocabulary_tables(\n        self.vocabulary_file,\n        self.num_oov_buckets,\n        as_asset=data_config.get(""export_vocabulary_assets"", True))\n    tokenizer_config = _get_field(data_config, ""tokenization"", prefix=asset_prefix)\n    self.tokenizer = tokenizers.make_tokenizer(tokenizer_config)\n\n  def set_noise(self, noiser, in_place=True, probability=None):\n    """"""Enables noise to be applied to the input features.\n\n    Args:\n      noiser: A :class:`opennmt.data.WordNoiser` instance.\n      in_place: If ``False``, the noisy version of the input will be stored as\n        a separate feature prefixed with ``noisy_``.\n      probability: When :obj:`in_place` is enabled, the probability to apply the\n        noise.\n\n    Raises:\n      ValueError: if :obj:`in_place` is enabled but a :obj:`probability` is not\n        set.\n    """"""\n    if in_place and probability is None:\n      raise ValueError(""In-place noise requires a probability"")\n    self.noiser = noiser\n    self.in_place_noise = in_place\n    self.noise_probability = probability\n\n  def export_assets(self, asset_dir, asset_prefix=""""):\n    return self.tokenizer.export_assets(asset_dir, asset_prefix=asset_prefix)\n\n  def make_dataset(self, data_file, training=None):\n    return dataset_util.make_datasets(tf.data.TextLineDataset, data_file)\n\n  def get_dataset_size(self, data_file):\n    if isinstance(data_file, list):\n      return list(map(misc.count_lines, data_file))\n    return misc.count_lines(data_file)\n\n  def make_features(self, element=None, features=None, training=None):\n    """"""Tokenizes raw text.""""""\n    if features is None:\n      features = {}\n    if ""tokens"" in features:\n      return features\n    if ""text"" in features:\n      element = features.pop(""text"")\n    tokens = self.tokenizer.tokenize(element)\n    if isinstance(tokens, tf.RaggedTensor):\n      length = tokens.row_lengths()\n      tokens = tokens.to_tensor()\n    else:\n      length = tf.shape(tokens)[0]\n    if training and self.noiser is not None:\n      noisy_tokens, noisy_length = self.noiser(tokens, keep_shape=False)\n      if self.in_place_noise:\n        tokens, length = tf.cond(\n            tf.random.uniform([]) < self.noise_probability,\n            true_fn=lambda: (noisy_tokens, noisy_length),\n            false_fn=lambda: (tokens, length))\n      else:\n        # Call make_features again to fill the remaining noisy features.\n        noisy_features = dict(tokens=noisy_tokens, length=noisy_length)\n        noisy_features = self.make_features(features=noisy_features, training=training)\n        for key, value in noisy_features.items():\n          features[""noisy_%s"" % key] = value\n    features[""length""] = length\n    features[""tokens""] = tokens\n    return features\n\n  def input_signature(self):\n    if self.tokenizer.in_graph:\n      return {\n          ""text"": tf.TensorSpec([None], tf.string)\n      }\n    else:\n      return {\n          ""tokens"": tf.TensorSpec([None, None], tf.string),\n          ""length"": tf.TensorSpec([None], tf.int32)\n      }\n\n\nclass WordEmbedder(TextInputter):\n  """"""Simple word embedder.""""""\n\n  def __init__(self, embedding_size=None, dropout=0.0, **kwargs):\n    """"""Initializes the parameters of the word embedder.\n\n    Args:\n      embedding_size: The size of the resulting embedding.\n        If ``None``, an embedding file must be provided.\n      dropout: The probability to drop units in the embedding.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(WordEmbedder, self).__init__(**kwargs)\n    self.embedding_size = embedding_size\n    self.embedding_file = None\n    self.dropout = dropout\n    self.decoder_mode = False\n    self.mark_start = None\n    self.mark_end = None\n\n  def set_decoder_mode(self, enable=True, mark_start=None, mark_end=None):\n    """"""Make this inputter produce sequences for a decoder.\n\n    In this mode, the returned ""ids_out"" feature is the decoder output sequence\n    and ""ids"" is the decoder input sequence.\n\n    Args:\n      enable: Enable the decoder mode.\n      mark_start: Mark the sequence start. If ``None``, keep the current value.\n      mark_end: Mark the sequence end. If ``None``, keep the current value.\n    """"""\n    self.decoder_mode = enable\n    if mark_start is not None:\n      self.mark_start = mark_start\n    if mark_end is not None:\n      self.mark_end = mark_end\n\n  def get_length(self, features, ignore_special_tokens=False):\n    length = features[""length""]\n    if ignore_special_tokens:\n      # Decoder mode shifts the sequences by one timesteps.\n      num_special_tokens = -1 if self.decoder_mode else 0\n      if self.mark_start:\n        num_special_tokens += 1\n      if self.mark_end:\n        num_special_tokens += 1\n      length -= num_special_tokens\n    return length\n\n  def initialize(self, data_config, asset_prefix=""""):\n    super(WordEmbedder, self).initialize(data_config, asset_prefix=asset_prefix)\n    embedding = _get_field(data_config, ""embedding"", prefix=asset_prefix)\n    if embedding is None and self.embedding_size is None:\n      raise ValueError(""embedding_size must be set"")\n    if embedding is not None:\n      self.embedding_file = embedding[""path""]\n      self.trainable = embedding.get(""trainable"", True)\n      self.embedding_file_with_header = embedding.get(""with_header"", True)\n      self.case_insensitive_embeddings = embedding.get(""case_insensitive"", True)\n    sequence_controls = _get_field(data_config, ""sequence_controls"", prefix=asset_prefix)\n    if sequence_controls:\n      self.mark_start = sequence_controls[""start""]\n      self.mark_end = sequence_controls[""end""]\n\n  def make_features(self, element=None, features=None, training=None):\n    """"""Converts words tokens to ids.""""""\n    features = super(WordEmbedder, self).make_features(\n        element=element, features=features, training=training)\n    if ""ids"" not in features:\n      features[""ids""] = self.tokens_to_ids.lookup(features[""tokens""])\n      if self.mark_start or self.mark_end:\n        features[""ids""], features[""length""] = add_sequence_controls(\n            features[""ids""],\n            features[""length""],\n            start_id=constants.START_OF_SENTENCE_ID if self.mark_start else None,\n            end_id=constants.END_OF_SENTENCE_ID if self.mark_end else None)\n    if self.decoder_mode:\n      features[""ids_out""] = features[""ids""][1:]\n      features[""ids""] = features[""ids""][:-1]\n      features[""length""] -= 1\n    return features\n\n  def build(self, input_shape):\n    if self.embedding_file:\n      pretrained = load_pretrained_embeddings(\n          self.embedding_file,\n          self.vocabulary_file,\n          num_oov_buckets=self.num_oov_buckets,\n          with_header=self.embedding_file_with_header,\n          case_insensitive_embeddings=self.case_insensitive_embeddings)\n      self.embedding_size = pretrained.shape[-1]\n      initializer = tf.constant_initializer(value=pretrained.astype(self.dtype))\n    else:\n      initializer = None\n    self.embedding = self.add_weight(\n        ""embedding"",\n        [self.vocabulary_size, self.embedding_size],\n        initializer=initializer,\n        trainable=self.trainable)\n    super(WordEmbedder, self).build(input_shape)\n\n  def call(self, features, training=None):\n    outputs = tf.nn.embedding_lookup(self.embedding, features[""ids""])\n    outputs = common.dropout(outputs, self.dropout, training=training)\n    return outputs\n\n  def visualize(self, model_root, log_dir):\n    save_embeddings_metadata(\n        log_dir,\n        misc.get_variable_name(self.embedding, model_root),\n        self.vocabulary_file,\n        num_oov_buckets=self.num_oov_buckets)\n\n  def map_v1_weights(self, weights):\n    return [(self.embedding, weights[""w_embs""])]\n\n\nclass CharEmbedder(TextInputter):\n  """"""Base class for character-aware inputters.""""""\n\n  def __init__(self, embedding_size, dropout=0.0, **kwargs):\n    """"""Initializes the parameters of the character embedder.\n\n    Args:\n      embedding_size: The size of the character embedding.\n      dropout: The probability to drop units in the embedding.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(CharEmbedder, self).__init__(**kwargs)\n    self.embedding_size = embedding_size\n    self.embedding = None\n    self.dropout = dropout\n\n  def make_features(self, element=None, features=None, training=None):\n    """"""Converts words to characters.""""""\n    if features is None:\n      features = {}\n    if ""char_ids"" in features:\n      return features\n    if ""chars"" in features:\n      chars = features[""chars""]\n    else:\n      features = super(CharEmbedder, self).make_features(\n          element=element, features=features, training=training)\n      chars = text.tokens_to_chars(features[""tokens""])\n      chars = chars.to_tensor(default_value=constants.PADDING_TOKEN)\n    features[""char_ids""] = self.tokens_to_ids.lookup(chars)\n    return features\n\n  def build(self, input_shape):\n    self.embedding = self.add_weight(\n        ""char_embedding"", [self.vocabulary_size, self.embedding_size])\n    super(CharEmbedder, self).build(input_shape)\n\n  @abc.abstractmethod\n  def call(self, features, training=None):\n    raise NotImplementedError()\n\n  def visualize(self, model_root, log_dir):\n    save_embeddings_metadata(\n        log_dir,\n        misc.get_variable_name(self.embedding, model_root),\n        self.vocabulary_file,\n        num_oov_buckets=self.num_oov_buckets)\n\n  def _embed(self, inputs, training):\n    mask = tf.math.not_equal(inputs, 0)\n    outputs = tf.nn.embedding_lookup(self.embedding, inputs)\n    outputs = common.dropout(outputs, self.dropout, training=training)\n    return outputs, mask\n\n\nclass CharConvEmbedder(CharEmbedder):\n  """"""An inputter that applies a convolution on characters embeddings.""""""\n\n  def __init__(self,\n               embedding_size,\n               num_outputs,\n               kernel_size=5,\n               stride=3,\n               dropout=0.0,\n               **kwargs):\n    """"""Initializes the parameters of the character convolution embedder.\n\n    Args:\n      embedding_size: The size of the character embedding.\n      num_outputs: The dimension of the convolution output space.\n      kernel_size: Length of the convolution window.\n      stride: Length of the convolution stride.\n      dropout: The probability to drop units in the embedding.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(CharConvEmbedder, self).__init__(\n        embedding_size,\n        dropout=dropout,\n        **kwargs)\n    self.output_size = num_outputs\n    self.conv = tf.keras.layers.Conv1D(\n        num_outputs,\n        kernel_size,\n        strides=stride,\n        padding=""same"")\n\n  def call(self, features, training=None):\n    inputs = features[""char_ids""]\n    flat_inputs = tf.reshape(inputs, [-1, tf.shape(inputs)[-1]])\n    outputs, _ = self._embed(flat_inputs, training)\n    outputs = self.conv(outputs)\n    outputs = tf.reduce_max(outputs, axis=1)\n    outputs = tf.reshape(outputs, [-1, tf.shape(inputs)[1], self.output_size])\n    return outputs\n\n\nclass CharRNNEmbedder(CharEmbedder):\n  """"""An inputter that runs a single RNN layer over character embeddings.""""""\n\n  def __init__(self,\n               embedding_size,\n               num_units,\n               dropout=0.2,\n               cell_class=None,\n               **kwargs):\n    """"""Initializes the parameters of the character RNN embedder.\n\n    Args:\n      embedding_size: The size of the character embedding.\n      num_units: The number of units in the RNN layer.\n      dropout: The probability to drop units in the embedding and the RNN\n        outputs.\n      cell_class: The inner cell class or a callable taking :obj:`num_units` as\n        argument and returning a cell. Defaults to a LSTM cell.\n      **kwargs: Additional layer keyword arguments.\n\n    Raises:\n      ValueError: if :obj:`encoding` is invalid.\n    """"""\n    super(CharRNNEmbedder, self).__init__(\n        embedding_size,\n        dropout=dropout,\n        **kwargs)\n    if cell_class is None:\n      cell_class = tf.keras.layers.LSTMCell\n    self.rnn = tf.keras.layers.RNN(cell_class(num_units))\n    self.num_units = num_units\n\n  def call(self, features, training=None):\n    inputs = features[""char_ids""]\n    flat_inputs = tf.reshape(inputs, [-1, tf.shape(inputs)[-1]])\n    embeddings, mask = self._embed(flat_inputs, training)\n    outputs = self.rnn(embeddings, mask=mask, training=training)\n    outputs = tf.reshape(outputs, [-1, tf.shape(inputs)[1], self.num_units])\n    return outputs\n'"
opennmt/layers/__init__.py,0,"b'""""""Module defining reusable and model specific layers.""""""\n\nfrom opennmt.layers.bridge import Bridge\nfrom opennmt.layers.bridge import CopyBridge\nfrom opennmt.layers.bridge import DenseBridge\nfrom opennmt.layers.bridge import ZeroBridge\n\nfrom opennmt.layers.common import Dense\nfrom opennmt.layers.common import LayerNorm\nfrom opennmt.layers.common import LayerWrapper\nfrom opennmt.layers.common import dropout\nfrom opennmt.layers.common import gelu\n\nfrom opennmt.layers.position import PositionEmbedder\nfrom opennmt.layers.position import PositionEncoder\nfrom opennmt.layers.position import SinusoidalPositionEncoder\n\nfrom opennmt.layers.reducer import ConcatReducer\nfrom opennmt.layers.reducer import DenseReducer\nfrom opennmt.layers.reducer import JoinReducer\nfrom opennmt.layers.reducer import MultiplyReducer\nfrom opennmt.layers.reducer import Reducer\nfrom opennmt.layers.reducer import SumReducer\n\nfrom opennmt.layers.rnn import LSTM\nfrom opennmt.layers.rnn import RNN\nfrom opennmt.layers.rnn import RNNCellWrapper\nfrom opennmt.layers.rnn import make_rnn_cell\n\nfrom opennmt.layers.transformer import FeedForwardNetwork\nfrom opennmt.layers.transformer import MultiHeadAttention\nfrom opennmt.layers.transformer import SelfAttentionDecoderLayer\nfrom opennmt.layers.transformer import SelfAttentionEncoderLayer\nfrom opennmt.layers.transformer import TransformerLayerWrapper\nfrom opennmt.layers.transformer import combine_heads\nfrom opennmt.layers.transformer import future_mask\nfrom opennmt.layers.transformer import split_heads\n'"
opennmt/layers/bridge.py,13,"b'""""""Define bridges: logic of passing the encoder state to the decoder.""""""\n\nimport abc\n\nimport tensorflow as tf\n\n\ndef assert_state_is_compatible(expected_state, state):\n  """"""Asserts that states are compatible.\n\n  Args:\n    expected_state: The reference state.\n    state: The state that must be compatible with :obj:`expected_state`.\n\n  Raises:\n    ValueError: if the states are incompatible.\n  """"""\n  # Check structure compatibility.\n  tf.nest.assert_same_structure(expected_state, state)\n\n  # Check shape compatibility.\n  expected_state_flat = tf.nest.flatten(expected_state)\n  state_flat = tf.nest.flatten(state)\n\n  for x, y in zip(expected_state_flat, state_flat):\n    if tf.is_tensor(x):\n      expected_depth = x.shape[-1]\n      depth = y.shape[-1]\n      if depth != expected_depth:\n        raise ValueError(""Tensor in state has shape %s which is incompatible ""\n                         ""with the target shape %s"" % (y.shape, x.shape))\n\n\nclass Bridge(tf.keras.layers.Layer):\n  """"""Base class for bridges.""""""\n\n  def __call__(self, encoder_state, decoder_zero_state):  # pylint: disable=arguments-differ\n    """"""Returns the initial decoder state.\n\n    Args:\n      encoder_state: The encoder state.\n      decoder_zero_state: The default decoder state.\n\n    Returns:\n      The decoder initial state.\n    """"""\n    return super(Bridge, self).__call__([encoder_state, decoder_zero_state])\n\n  @abc.abstractmethod\n  def call(self, states):  # pylint: disable=arguments-differ\n    raise NotImplementedError()\n\n\nclass CopyBridge(Bridge):\n  """"""A bridge that passes the encoder state as is.""""""\n\n  def call(self, states):\n    encoder_state, decoder_state = states\n    assert_state_is_compatible(encoder_state, decoder_state)\n    flat_encoder_state = tf.nest.flatten(encoder_state)\n    return tf.nest.pack_sequence_as(decoder_state, flat_encoder_state)\n\n\nclass ZeroBridge(Bridge):\n  """"""A bridge that does not pass information from the encoder.""""""\n\n  def call(self, states):\n    # Simply return the default decoder state.\n    return states[1]\n\n\nclass DenseBridge(Bridge):\n  """"""A bridge that applies a parameterized linear transformation from the\n  encoder state to the decoder state size.\n  """"""\n\n  def __init__(self, activation=None):\n    """"""Initializes the bridge.\n\n    Args:\n      activation: Activation function (a callable).\n        Set it to ``None`` to maintain a linear activation.\n    """"""\n    super(DenseBridge, self).__init__()\n    self.activation = activation\n    self.decoder_state_sizes = None\n    self.linear = None\n\n  def build(self, input_shape):\n    decoder_shape = input_shape[1]\n    self.decoder_state_sizes = [\n        shape[-1] for shape in tf.nest.flatten(decoder_shape)]\n    self.linear = tf.keras.layers.Dense(\n        sum(self.decoder_state_sizes), activation=self.activation)\n\n  def call(self, states):\n    encoder_state, decoder_state = states\n    encoder_state_flat = tf.nest.flatten(encoder_state)\n    encoder_state_single = tf.concat(encoder_state_flat, 1)\n    transformed = self.linear(encoder_state_single)\n    splitted = tf.split(transformed, self.decoder_state_sizes, axis=1)\n    return tf.nest.pack_sequence_as(decoder_state, splitted)\n'"
opennmt/layers/common.py,13,"b'""""""Defines common layers.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.utils.misc import shape_list\n\n\ndef dropout(x, rate, training=None):\n  """"""Simple dropout layer.""""""\n  if not training or rate == 0:\n    return x\n  return tf.nn.dropout(x, rate)\n\ndef gelu(x):\n  """"""Gaussian Error Linear Unit activation function described in\n  https://arxiv.org/abs/1606.08415.\n  """"""\n  return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\n\nclass Dense(tf.keras.layers.Dense):\n  """"""Small ``tf.keras.layers.Dense`` extension to possibly reuse an existing weight\n  matrix.\n  """"""\n\n  def __init__(self, units, weight=None, transpose=False, **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      unit: Positive integer, dimensionality of the output space.\n      weight: The weight to reuse.\n      transpose: Whether :obj:`weight` should be transposed or not.\n      kwargs: Additional layers arguments.\n    """"""\n    super(Dense, self).__init__(units, **kwargs)\n    self.set_kernel(weight, transpose=transpose)\n\n  def set_kernel(self, weight, transpose=False):\n    """"""Use :obj:`weight` as the kernel weights matrix.\n\n    Args:\n      weight: The weight ot use.\n      transpose: Whether :obj:`weight` should be transposed or not.\n\n    Raises:\n      ValueError: if the layer is already built.\n    """"""\n    if self.built:\n      raise ValueError(""The layer is already built"")\n    self.weight = weight\n    self.transpose = transpose\n\n  def add_weight(self, name, *args, **kwargs):  # pylint: disable=arguments-differ\n    if self.weight is not None and name == ""kernel"":\n      return self.weight\n    return super(Dense, self).add_weight(name, *args, **kwargs)\n\n  def call(self, inputs):\n    shape = shape_list(inputs)\n    rank = len(shape)\n    if rank > 2:\n      inputs = tf.reshape(inputs, [-1, shape[-1]])\n    outputs = tf.matmul(inputs, self.kernel, transpose_b=self.transpose)\n    if self.use_bias:\n      outputs = tf.nn.bias_add(outputs, self.bias)\n    if self.activation is not None:\n      outputs = self.activation(outputs)  # pylint: disable=not-callable\n    if rank > 2:\n      outputs = tf.reshape(outputs, shape[:-1] + [self.units])\n    return outputs\n\n  def map_v1_weights(self, weights):\n    m = [(self.kernel, weights[""kernel""])]\n    if self.use_bias:\n      m.append((self.bias, weights[""bias""]))\n    return m\n\n\nclass LayerNorm(tf.keras.layers.LayerNormalization):\n  """"""Layer normalization.""""""\n\n  def map_v1_weights(self, weights):\n    return [\n        (self.beta, weights[""beta""]),\n        (self.gamma, weights[""gamma""])\n    ]\n\n\nclass LayerWrapper(tf.keras.layers.Layer):\n  """"""Layer wrapper for input/output normalization, input/output dropout and\n  residual connection.\n  """"""\n\n  def __init__(self,\n               layer,\n               normalize_input=False,\n               normalize_output=False,\n               input_dropout=0,\n               output_dropout=0,\n               residual_connection=False,\n               **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      layer: The layer to wrap.\n      normalize_input: Apply layer normalization on the input.\n      normalize_output: Apply layer normalization on the output.\n      input_dropout: The probability to drop units in the layer input.\n      output_dropout: The probability to drop units in the layer output.\n      residual_connection: Add the inputs to layer outputs (if their shape are\n        compatible).\n      kwargs: Additional layer arguments.\n    """"""\n    super(LayerWrapper, self).__init__(**kwargs)\n    self.layer = layer\n    self.input_layer_norm = LayerNorm() if normalize_input else None\n    self.output_layer_norm = LayerNorm() if normalize_output else None\n    self.input_dropout = input_dropout\n    self.output_dropout = output_dropout\n    self.residual_connection = residual_connection\n\n  def call(self, inputs, *args, **kwargs):  # pylint: disable=arguments-differ\n    """"""Runs the wrapper.""""""\n    training = kwargs.get(""training"")\n    x = inputs\n    if self.input_layer_norm is not None:\n      x = self.input_layer_norm(x)  # pylint: disable=not-callable\n    x = dropout(x, self.input_dropout, training=training)\n\n    all_outputs = self.layer(x, *args, **kwargs)\n    if isinstance(all_outputs, tuple):\n      outputs = all_outputs[0]\n      extra_outputs = list(all_outputs)[1:]\n    else:\n      outputs = all_outputs\n      extra_outputs = None\n\n    outputs = dropout(outputs, self.output_dropout, training=training)\n    if self.residual_connection and outputs.shape[-1] == inputs.shape[-1]:\n      outputs += inputs\n    if self.output_layer_norm is not None:\n      outputs = self.output_layer_norm(outputs)  # pylint: disable=not-callable\n\n    if extra_outputs:\n      return tuple([outputs] + extra_outputs)\n    return outputs\n\n  # The wrapper should be serializable to be used in tf.keras.layers.Bidirectional.\n\n  def get_config(self):\n    """"""Returns the layer wrapper configuration.""""""\n    config = {\n        ""layer"": tf.keras.layers.serialize(self.layer),\n        ""normalize_input"": self.input_layer_norm is not None,\n        ""normalize_output"": self.output_layer_norm is not None,\n        ""input_dropout"": self.input_dropout,\n        ""output_dropout"": self.output_dropout,\n        ""residual_connection"": self.residual_connection\n    }\n    base_config = super(LayerWrapper, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  @classmethod\n  def from_config(cls, config):\n    """"""Creates a layer wrapper from its configuration.""""""\n    layer = tf.keras.layers.deserialize(config.pop(""layer""))\n    return cls(layer, **config)\n'"
opennmt/layers/position.py,16,"b'""""""Define position encoder classes.""""""\n\nimport math\nimport abc\n\nimport tensorflow as tf\n\nfrom opennmt.layers.reducer import SumReducer\n\n\nclass PositionEncoder(tf.keras.layers.Layer):\n  """"""Base class for position encoders.""""""\n\n  def __init__(self, reducer=None, **kwargs):\n    """"""Initializes the position encoder.\n\n    Args:\n      reducer: A :class:`opennmt.layers.Reducer` to merge inputs and position\n        encodings. Defaults to :class:`opennmt.layers.SumReducer`.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(PositionEncoder, self).__init__(**kwargs)\n    if reducer is None:\n      reducer = SumReducer(dtype=kwargs.get(""dtype""))\n    self.reducer = reducer\n\n  def call(self, inputs, position=None):  # pylint: disable=arguments-differ\n    """"""Add position encodings to :obj:`inputs`.\n\n    Args:\n      inputs: The inputs to encode.\n      position: The single position to encode, to use when this layer is called\n        step by step.\n\n    Returns:\n      A ``tf.Tensor`` whose shape depends on the configured ``reducer``.\n    """"""\n    batch_size = tf.shape(inputs)[0]\n    timesteps = tf.shape(inputs)[1]\n    input_dim = inputs.shape[-1]\n    positions = tf.range(timesteps) + 1 if position is None else [position]\n    position_encoding = self._encode([positions], input_dim)\n    position_encoding = tf.tile(position_encoding, [batch_size, 1, 1])\n    return self.reducer([inputs, position_encoding])\n\n  @abc.abstractmethod\n  def _encode(self, positions, depth):\n    """"""Creates position encodings.\n\n    Args:\n      positions: The positions to encode of shape :math:`[B, ...]`.\n      depth: The encoding depth :math:`D`.\n\n    Returns:\n      A ``tf.Tensor`` of shape :math:`[B, ..., D]`.\n    """"""\n    raise NotImplementedError()\n\n\nclass PositionEmbedder(PositionEncoder):\n  """"""Encodes position with a lookup table.""""""\n\n  def __init__(self, maximum_position=128, reducer=None, **kwargs):\n    """"""Initializes the position encoder.\n\n    Args:\n      maximum_position: The maximum position to embed. Positions greater\n        than this value will be set to :obj:`maximum_position`.\n      reducer: A :class:`opennmt.layers.Reducer` to merge inputs and position\n        encodings. Defaults to :class:`opennmt.layers.SumReducer`.\n      **kwargs: Additional layer keyword arguments.\n    """"""\n    super(PositionEmbedder, self).__init__(reducer=reducer, **kwargs)\n    self.maximum_position = maximum_position\n    self.embedding = None\n\n  def build(self, input_shape):\n    shape = [self.maximum_position + 1, input_shape[-1]]\n    self.embedding = self.add_weight(""position_embedding"", shape)\n    super(PositionEmbedder, self).build(input_shape)\n\n  def _encode(self, positions, depth):\n    positions = tf.minimum(positions, self.maximum_position)\n    return tf.nn.embedding_lookup(self.embedding, positions)\n\n\nclass SinusoidalPositionEncoder(PositionEncoder):\n  """"""Encodes positions with sine waves as described in\n  https://arxiv.org/abs/1706.03762.\n  """"""\n\n  def _encode(self, positions, depth):\n    if depth % 2 != 0:\n      raise ValueError(""SinusoidalPositionEncoder expects the depth to be divisble ""\n                       ""by 2 but got %d"" % depth)\n\n    batch_size = tf.shape(positions)[0]\n    positions = tf.cast(positions, tf.float32)\n\n    log_timescale_increment = math.log(10000) / (depth / 2 - 1)\n    inv_timescales = tf.exp(tf.range(depth / 2, dtype=tf.float32) * -log_timescale_increment)\n    inv_timescales = tf.reshape(tf.tile(inv_timescales, [batch_size]), [batch_size, -1])\n    scaled_time = tf.expand_dims(positions, -1) * tf.expand_dims(inv_timescales, 1)\n    encoding = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=2)\n    return tf.cast(encoding, self.dtype)\n'"
opennmt/layers/reducer.py,30,"b'""""""Define reducers: objects that merge inputs.""""""\n\nimport abc\nimport functools\n\nimport tensorflow as tf\n\nfrom opennmt.utils import tensor as tensor_util\n\n\ndef pad_in_time(x, padding_length):\n  """"""Helper function to pad a tensor in the time dimension and retain the static depth dimension.""""""\n  return tf.pad(x, [[0, 0], [0, padding_length], [0, 0]])\n\ndef align_in_time(x, length):\n  """"""Aligns the time dimension of :obj:`x` with :obj:`length`.""""""\n  time_dim = tf.shape(x)[1]\n  return tf.cond(\n      tf.less(time_dim, length),\n      true_fn=lambda: pad_in_time(x, length - time_dim),\n      false_fn=lambda: x[:, :length])\n\ndef pad_with_identity(x, sequence_length, max_sequence_length, identity_values=0, maxlen=None):\n  """"""Pads a tensor with identity values up to :obj:`max_sequence_length`.\n\n  Args:\n    x: A ``tf.Tensor`` of shape ``[batch_size, time, depth]``.\n    sequence_length: The true sequence length of :obj:`x`.\n    max_sequence_length: The sequence length up to which the tensor must contain\n      :obj:`identity values`.\n    identity_values: The identity value.\n    maxlen: Size of the output time dimension. Default is the maximum value in\n      obj:`max_sequence_length`.\n\n  Returns:\n    A ``tf.Tensor`` of shape ``[batch_size, maxlen, depth]``.\n  """"""\n  if maxlen is None:\n    maxlen = tf.reduce_max(max_sequence_length)\n\n  mask = tf.sequence_mask(sequence_length, maxlen=maxlen, dtype=x.dtype)\n  mask = tf.expand_dims(mask, axis=-1)\n  mask_combined = tf.sequence_mask(max_sequence_length, maxlen=maxlen, dtype=x.dtype)\n  mask_combined = tf.expand_dims(mask_combined, axis=-1)\n\n  identity_mask = mask_combined * (1.0 - mask)\n\n  x = pad_in_time(x, maxlen - tf.shape(x)[1])\n  x = x * mask + (identity_mask * identity_values)\n\n  return x\n\ndef pad_n_with_identity(inputs, sequence_lengths, identity_values=0):\n  """"""Pads each input tensors with identity values up to\n  ``max(sequence_lengths)`` for each batch.\n\n  Args:\n    inputs: A list of ``tf.Tensor``.\n    sequence_lengths: A list of sequence length.\n    identity_values: The identity value.\n\n  Returns:\n    A tuple ``(padded, max_sequence_length)`` which are respectively a list of\n    ``tf.Tensor`` where each tensor are padded with identity and the combined\n    sequence length.\n  """"""\n  max_sequence_length = tf.reduce_max(sequence_lengths, axis=0)\n  maxlen = tf.reduce_max([tf.shape(x)[1] for x in inputs])\n  padded = [\n      pad_with_identity(\n          x, length, max_sequence_length, identity_values=identity_values, maxlen=maxlen)\n      for x, length in zip(inputs, sequence_lengths)]\n  return padded, max_sequence_length\n\n\nclass Reducer(tf.keras.layers.Layer):\n  """"""Base class for reducers.""""""\n\n  def zip_and_reduce(self, x, y):\n    """"""Zips the :obj:`x` with :obj:`y` structures together and reduces all\n    elements. If the structures are nested, they will be flattened first.\n\n    Args:\n      x: The first structure.\n      y: The second structure.\n\n    Returns:\n      The same structure as :obj:`x` and :obj:`y` where each element from\n      :obj:`x` is reduced with the correspond element from :obj:`y`.\n\n    Raises:\n      ValueError: if the two structures are not the same.\n    """"""\n    tf.nest.assert_same_structure(x, y)\n    x_flat = tf.nest.flatten(x)\n    y_flat = tf.nest.flatten(y)\n    reduced = list(map(self, zip(x_flat, y_flat)))\n    return tf.nest.pack_sequence_as(x, reduced)\n\n  def call(self, inputs, sequence_length=None):  # pylint: disable=arguments-differ\n    """"""Reduces all input elements.\n\n    Args:\n      inputs: A list of ``tf.Tensor``.\n      sequence_length: The length of each input, if reducing sequences.\n\n    Returns:\n      If :obj:`sequence_length` is set, a tuple\n      ``(reduced_input, reduced_length)``, otherwise a reduced ``tf.Tensor``\n      only.\n    """"""\n    if sequence_length is None:\n      return self.reduce(inputs)\n    else:\n      return self.reduce_sequence(inputs, sequence_lengths=sequence_length)\n\n  @abc.abstractmethod\n  def reduce(self, inputs):\n    """"""See :meth:`opennmt.layers.Reducer.__call__`.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def reduce_sequence(self, inputs, sequence_lengths):\n    """"""See :meth:`opennmt.layers.Reducer.__call__`.""""""\n    raise NotImplementedError()\n\n\nclass SumReducer(Reducer):\n  """"""A reducer that sums the inputs.""""""\n\n  def reduce(self, inputs):\n    return tf.add_n(inputs)\n\n  def reduce_sequence(self, inputs, sequence_lengths):\n    padded, combined_length = pad_n_with_identity(inputs, sequence_lengths, identity_values=0)\n    return self.reduce(padded), combined_length\n\n\nclass MultiplyReducer(Reducer):\n  """"""A reducer that multiplies the inputs.""""""\n\n  def reduce(self, inputs):\n    return functools.reduce(lambda a, x: a * x, inputs)\n\n  def reduce_sequence(self, inputs, sequence_lengths):\n    padded, combined_length = pad_n_with_identity(inputs, sequence_lengths, identity_values=1)\n    return self.reduce(padded), combined_length\n\n\nclass ConcatReducer(Reducer):\n  """"""A reducer that concatenates the inputs.""""""\n\n  def __init__(self, axis=-1, **kwargs):\n    """"""Initializes the concat reducer.\n\n    Args:\n      axis: Dimension along which to concatenate. This reducer supports\n        concatenating in depth or in time.\n      **kwargs: Additional layer arguments.\n    """"""\n    super().__init__(**kwargs)\n    self.axis = axis\n\n  def reduce(self, inputs):\n    return tf.concat(inputs, self.axis)\n\n  def reduce_sequence(self, inputs, sequence_lengths):\n    axis = self.axis % inputs[0].shape.ndims\n\n    if axis == 2:\n      padded, combined_length = pad_n_with_identity(inputs, sequence_lengths)\n      return self.reduce(padded), combined_length\n    elif axis == 1:\n      # Align all input tensors to the maximum combined length.\n      combined_length = tf.add_n(sequence_lengths)\n      maxlen = tf.reduce_max(combined_length)\n      aligned = [align_in_time(x, maxlen) for x in inputs]\n\n      current_length = None\n      accumulator = None\n\n      for elem, length in zip(aligned, sequence_lengths):\n        # Make sure padding are 0 vectors as it is required for the next step.\n        mask = tf.sequence_mask(length, maxlen=maxlen, dtype=elem.dtype)\n        elem = elem * tf.expand_dims(mask, -1)\n\n        if accumulator is None:\n          accumulator = elem\n          current_length = length\n        else:\n          accumulator += tensor_util.roll_sequence(elem, current_length)\n          current_length += length\n\n      return accumulator, combined_length\n    else:\n      raise ValueError(""Unsupported concatenation on axis {}"".format(axis))\n\n\nclass JoinReducer(Reducer):\n  """"""A reducer that joins its inputs in a single tuple.""""""\n\n  def reduce(self, inputs):\n    output = []\n    for elem in inputs:\n      if isinstance(elem, tuple) and not hasattr(elem, ""_fields""):\n        for e in elem:\n          output.append(e)\n      else:\n        output.append(elem)\n    return tuple(output)\n\n  def reduce_sequence(self, inputs, sequence_lengths):\n    return self.reduce(inputs), self.reduce(sequence_lengths)\n\n\nclass DenseReducer(ConcatReducer):\n  """"""A reducer that concatenates its inputs in depth and applies a linear transformation.""""""\n\n  def __init__(self, output_size, activation=None, **kwargs):\n    """"""Initializes the reducer.\n\n    Args:\n      output_size: The output size of the linear transformation.\n      activation: Activation function (a callable).\n        Set it to ``None`` to maintain a linear activation.\n      **kwargs: Additional layer arguments.\n    """"""\n    super().__init__(axis=-1, **kwargs)\n    self.dense = tf.keras.layers.Dense(output_size, activation=activation)\n\n  def reduce(self, inputs):\n    inputs = super().reduce(inputs)\n    return self.dense(inputs)\n'"
opennmt/layers/rnn.py,13,"b'""""""RNN functions and classes for TensorFlow 2.0.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.layers import common\nfrom opennmt.layers import reducer as reducer_lib\n\n\nclass RNNCellWrapper(common.LayerWrapper):\n  """"""A wrapper for RNN cells.""""""\n\n  def __init__(self,\n               cell,\n               input_dropout=0,\n               output_dropout=0,\n               residual_connection=False,\n               **kwargs):\n    """"""Initializes the wrapper.\n\n    Args:\n      cell: The cell to wrap.\n      input_dropout: The probability to drop units in the cell input.\n      output_dropout: The probability to drop units in the cell output.\n      residual_connection: Add the inputs to cell outputs (if their shape are\n        compatible).\n      kwargs: Additional layer arguments.\n    """"""\n    super(RNNCellWrapper, self).__init__(\n        cell,\n        input_dropout=input_dropout,\n        output_dropout=output_dropout,\n        residual_connection=residual_connection,\n        **kwargs)\n    self.cell = cell\n\n  @property\n  def state_size(self):\n    """"""The cell state size.""""""\n    return self.cell.state_size\n\n  @property\n  def output_size(self):\n    """"""The cell output size.""""""\n    return self.cell.output_size\n\n  def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n    """"""Returns the initial cell state.""""""\n    return self.cell.get_initial_state(\n        inputs=inputs, batch_size=batch_size, dtype=dtype)\n\ndef make_rnn_cell(num_layers,\n                  num_units,\n                  dropout=0,\n                  residual_connections=False,\n                  cell_class=None,\n                  **kwargs):\n  """"""Convenience function to build a multi-layer RNN cell.\n\n  Args:\n    num_layers: The number of layers.\n    num_units: The number of units in each layer.\n    dropout: The probability to drop units in each layer output.\n    residual_connections: If ``True``, each layer input will be added to its output.\n    cell_class: The inner cell class or a callable taking :obj:`num_units` as\n      argument and returning a cell. Defaults to a LSTM cell.\n    kwargs: Additional arguments passed to the cell constructor.\n\n  Returns:\n    A ``tf.keras.layers.StackedRNNCells`` instance.\n\n  See Also:\n    :class:`opennmt.layers.RNNCellWrapper`\n  """"""\n  if cell_class is None:\n    cell_class = tf.keras.layers.LSTMCell\n  cells = []\n  for _ in range(num_layers):\n    cell = cell_class(num_units, **kwargs)\n    if dropout > 0 or residual_connections:\n      cell = RNNCellWrapper(\n          cell, output_dropout=dropout, residual_connection=residual_connections)\n    cells.append(cell)\n  return tf.keras.layers.StackedRNNCells(cells)\n\n\nclass _RNNWrapper(tf.keras.layers.Layer):\n  """"""Extend a RNN layer to possibly make it bidirectional and format its outputs.""""""\n\n  def __init__(self, rnn, bidirectional=False, reducer=reducer_lib.ConcatReducer(), **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      rnn: The RNN layer to extend, built with ``return_sequences`` and\n        ``return_state`` enabled.\n      bidirectional: Make this layer bidirectional.\n      reducer: A :class:`opennmt.layers.Reducer` instance to merge\n        bidirectional states and outputs.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(_RNNWrapper, self).__init__(**kwargs)\n    self.rnn = rnn\n    self.reducer = reducer\n    self.bidirectional = bidirectional\n    if bidirectional:\n      self.rnn = tf.keras.layers.Bidirectional(self.rnn, merge_mode=None)\n\n  def call(self, *args, **kwargs):  # pylint: disable=arguments-differ\n    """"""Forwards the arguments to the RNN layer.\n\n    Args:\n      *args: Positional arguments of the RNN layer.\n      **kwargs: Keyword arguments of the RNN layer.\n\n    Returns:\n      A tuple with the output sequences and the states.\n    """"""\n    outputs = self.rnn(*args, **kwargs)\n    if self.bidirectional:\n      sequences = outputs[0:2]\n      states = outputs[2:]\n      fwd_states = states[:len(states)//2]\n      bwd_states = states[len(states)//2:]\n      if self.reducer is not None:\n        sequences = self.reducer(sequences)\n        states = tuple(self.reducer.zip_and_reduce(fwd_states, bwd_states))\n      else:\n        sequences = tuple(sequences)\n        states = (fwd_states, bwd_states)\n    else:\n      sequences = outputs[0]\n      states = tuple(outputs[1:])\n    return sequences, states\n\n\nclass RNN(_RNNWrapper):\n  """"""A simple RNN layer.""""""\n\n  def __init__(self, cell, bidirectional=False, reducer=reducer_lib.ConcatReducer(), **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      cell: The RNN cell to use.\n      bidirectional: Make this layer bidirectional.\n      reducer: A :class:`opennmt.layers.Reducer` instance to merge\n        bidirectional states and outputs.\n      **kwargs: Additional layer arguments.\n\n    See Also:\n      :func:`opennmt.layers.make_rnn_cell`\n    """"""\n    rnn = tf.keras.layers.RNN(cell, return_sequences=True, return_state=True)\n    super(RNN, self).__init__(rnn, bidirectional=bidirectional, reducer=reducer, **kwargs)\n\n  def map_v1_weights(self, weights):\n    m = []\n    if self.bidirectional:\n      weights = weights[""bidirectional_rnn""]\n      m += map_v1_weights_to_cell(self.rnn.forward_layer.cell, weights[""fw""])\n      m += map_v1_weights_to_cell(self.rnn.backward_layer.cell, weights[""bw""])\n    else:\n      weights = weights[""rnn""]\n      m += map_v1_weights_to_cell(self.rnn.cell, weights)\n    return m\n\n\nclass LSTM(tf.keras.layers.Layer):\n  """"""A multi-layer LSTM.\n\n  This differs from using :class:`opennmt.layers.RNN` with a ``LSTMCell`` in 2\n  ways:\n\n  - It uses ``tf.keras.layers.LSTM`` which is possibly accelerated by cuDNN on\n    GPU.\n  - Bidirectional outputs of each layer are reduced before feeding them to the\n    next layer.\n  """"""\n\n  def __init__(self,\n               num_layers,\n               num_units,\n               bidirectional=False,\n               reducer=reducer_lib.ConcatReducer(),\n               dropout=0,\n               residual_connections=False,\n               **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      num_layers: Number of stacked LSTM layers.\n      num_units: Dimension of the output space of each LSTM.\n      bidirectional: Make each layer bidirectional.\n      reducer: A :class:`opennmt.layers.Reducer` instance to merge\n        the bidirectional states and outputs of each layer.\n      dropout: The probability to drop units in each layer output.\n      residual_connections: If ``True``, each layer input will be added to its\n        output.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(LSTM, self).__init__(**kwargs)\n    rnn_layers = [\n        _RNNWrapper(\n            tf.keras.layers.LSTM(num_units, return_sequences=True, return_state=True),\n            bidirectional=bidirectional,\n            reducer=reducer)\n        for _ in range(num_layers)]\n    self.layers = [\n        common.LayerWrapper(\n            layer,\n            output_dropout=dropout,\n            residual_connection=residual_connections)\n        for layer in rnn_layers]\n\n  def call(self, inputs, mask=None, training=None, initial_state=None):  # pylint: disable=arguments-differ\n    all_states = []\n    for i, layer in enumerate(self.layers):\n      outputs, states = layer(\n          inputs,\n          mask=mask,\n          training=training,\n          initial_state=initial_state[i] if initial_state is not None else None)\n      all_states.append(states)\n      inputs = outputs\n    return outputs, tuple(all_states)\n\n\ndef map_v1_weights_to_cell(cell, weights):\n  """"""Maps V1 weights to V2 RNN cell.""""""\n  if isinstance(cell, RNNCellWrapper):\n    cell = cell.cell\n  if isinstance(cell, tf.keras.layers.StackedRNNCells):\n    return _map_v1_weights_to_stacked_cells(cell, weights)\n  elif isinstance(cell, (tf.keras.layers.LSTMCell, tf.compat.v1.keras.layers.LSTMCell)):\n    return _map_v1_weights_to_lstmcell(cell, weights)\n  else:\n    raise ValueError(""Cannot restore V1 weights for cell %s"" % str(cell))\n  return m\n\ndef _map_v1_weights_to_stacked_cells(stacked_cells, weights):\n  weights = weights[""multi_rnn_cell""]\n  m = []\n  for i, cell in enumerate(stacked_cells.cells):\n    m += map_v1_weights_to_cell(cell, weights[""cell_%d"" % i])\n  return m\n\ndef _map_v1_weights_to_lstmcell(cell, weights):\n  weights = weights[""lstm_cell""]\n\n  def _upgrade_weight(weight):\n    is_bias = len(weight.shape) == 1\n    i, j, f, o = np.split(weight, 4, axis=-1)\n    if is_bias:  # Add forget_bias which is part of the LSTM formula in TensorFlow 1.\n      f += 1\n    return np.concatenate((i, f, j, o), axis=-1)  # Swap 2nd and 3rd projection.\n\n  def _split_kernel(index):\n    # TensorFlow 1 had a single kernel of shape [input_dim + units, 4 * units],\n    # but TensorFlow 2 splits it into ""kernel"" and ""recurrent_kernel"".\n    return tf.nest.map_structure(\n        lambda w: np.split(w, [w.shape[0] - cell.units])[index],\n        weights[""kernel""])\n\n  weights = tf.nest.map_structure(_upgrade_weight, weights)\n  m = []\n  m.append((cell.kernel, _split_kernel(0)))\n  m.append((cell.recurrent_kernel, _split_kernel(1)))\n  if cell.use_bias:\n    m.append((cell.bias, weights[""bias""]))\n  return m\n'"
opennmt/layers/transformer.py,52,"b'""""""Define layers related to the Google\'s Transformer model.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.layers import common\nfrom opennmt.utils import misc\n\n\ndef _lower_triangle_mask(sequence_length, maximum_length=None, dtype=tf.bool):\n  batch_size = tf.shape(sequence_length)[0]\n  if maximum_length is None:\n    maximum_length = tf.reduce_max(sequence_length)\n  mask = tf.ones([batch_size, maximum_length, maximum_length], dtype=dtype)\n  mask = tf.linalg.band_part(mask, -1, 0)\n  return mask\n\ndef future_mask(sequence_length, maximum_length=None, dtype=tf.bool):\n  """"""Builds the dot product mask for future positions.\n\n  Args:\n    sequence_length: The sequence length.\n    maximum_length: Optional size of the returned time dimension. Otherwise\n      it is the maximum of :obj:`sequence_length`.\n    dtype: The type of the mask tensor.\n\n  Returns:\n    A ``tf.Tensor`` of type :obj:`dtype` and shape\n    ``[batch_size, max_length, max_length]``.\n  """"""\n  sequence_mask = tf.sequence_mask(sequence_length, maxlen=maximum_length, dtype=dtype)\n  sequence_mask = tf.expand_dims(sequence_mask, axis=1)\n  mask = _lower_triangle_mask(sequence_length, maximum_length=maximum_length, dtype=dtype)\n  if dtype is tf.bool:\n    return tf.math.logical_and(mask, sequence_mask)\n  else:\n    return mask * sequence_mask\n\ndef split_heads(inputs, num_heads):\n  """"""Splits a tensor in depth.\n\n  Args:\n    inputs: A ``tf.Tensor`` of shape :math:`[B, T, D]`.\n    num_heads: The number of heads :math:`H`.\n\n  Returns:\n    A ``tf.Tensor`` of shape :math:`[B, H, T, D / H]`.\n  """"""\n  shape = misc.shape_list(inputs)\n  outputs = tf.reshape(inputs, [shape[0], shape[1], num_heads, shape[2] // num_heads])\n  outputs = tf.transpose(outputs, perm=[0, 2, 1, 3])\n  return outputs\n\ndef combine_heads(inputs):\n  """"""Concatenates heads.\n\n  Args:\n    inputs: A ``tf.Tensor`` of shape :math:`[B, H, T, D]`.\n\n  Returns:\n    A ``tf.Tensor`` of shape :math:`[B, T, D * H]`.\n  """"""\n  shape = misc.shape_list(inputs)\n  outputs = tf.transpose(inputs, perm=[0, 2, 1, 3])\n  outputs = tf.reshape(outputs, [shape[0], shape[2], shape[1] * shape[3]])\n  return outputs\n\ndef relative_positions(length, maximum_position, with_cache=False):\n  """"""Builds the relative positions.\n\n  Args:\n    length: The maximum length of the sequence.\n    maximum_position: The maximum relative position to represent.\n    with_cache: Set to ``True`` if this function is called from a multi-head\n      attention layer with cache states.\n\n  Returns:\n    Positive relative positions with shape :math:`[T or 1, T]`.\n  """"""\n  if with_cache:\n    distance = tf.expand_dims(tf.range(-length + 1, 1, delta=1), 0)\n  else:\n    arange = tf.range(length)\n    distance = tf.expand_dims(arange, 0) - tf.expand_dims(arange, 1)  # Distance to the diagonal.\n  distance = tf.clip_by_value(distance, -maximum_position, maximum_position)\n  return distance + maximum_position  # Return positive indices.\n\ndef matmul_with_relative_representations(a, b, transpose_b=False):  # pylint: disable=invalid-name\n  """"""Multiplies :obj:`a` with the relative representations :obj:`b`.\n\n  Args:\n    a: Tensor with shape :math:`[B, H, T, _]`.\n    b: Tensor with shape :math:`[T, T, _]`.\n\n  Returns:\n    Tensor with shape :math:`[B, H, T, T]`.\n  """"""\n  batch, head, time, _ = misc.shape_list(a)\n  a = tf.transpose(a, perm=[2, 0, 1, 3])\n  a = tf.reshape(a, [time, batch * head, -1])\n  c = tf.matmul(a, b, transpose_b=transpose_b)\n  c = tf.reshape(c, [time, batch, head, -1])\n  c = tf.transpose(c, perm=[1, 2, 0, 3])\n  return c\n\n\nclass FeedForwardNetwork(tf.keras.layers.Layer):\n  """"""Implements the Transformer\'s ""Feed Forward"" layer.\n\n  .. math::\n\n      ffn(x) = max(0, x*W_1 + b_1)*W_2 + b_2\n  """"""\n\n  def __init__(self,\n               inner_dim,\n               output_dim,\n               dropout=0.1,\n               activation=tf.nn.relu,\n               **kwargs):\n    """"""Initializes this layer.\n\n    Args:\n      inner_dim: The number of units of the inner linear transformation.\n      output_dim: The number of units of the ouput linear transformation.\n      dropout: The probability to drop units from the activation output.\n      activation: The activation function to apply between the two linear\n        transformations.\n      kwargs: Additional layer arguments.\n    """"""\n    super(FeedForwardNetwork, self).__init__(**kwargs)\n    self.inner = common.Dense(inner_dim, activation=activation)\n    self.outer = common.Dense(output_dim)\n    self.dropout = dropout\n\n  def call(self, inputs, training=None):  # pylint: disable=arguments-differ\n    """"""Runs the layer.""""""\n    inner = self.inner(inputs)\n    inner = common.dropout(inner, self.dropout, training=training)\n    return self.outer(inner)\n\n  def map_v1_weights(self, weights):\n    # V1 used conv1d layers that have a leading dimensions.\n    weights = tf.nest.map_structure(np.squeeze, weights)\n    m = []\n    m += self.inner.map_v1_weights(weights[""conv1d""])\n    m += self.outer.map_v1_weights(weights[""conv1d_1""])\n    return m\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  """"""Computes the multi-head attention as described in\n  https://arxiv.org/abs/1706.03762.\n  """"""\n\n  def __init__(self,\n               num_heads,\n               num_units,\n               dropout=0.1,\n               return_attention=False,\n               maximum_relative_position=None,\n               **kwargs):\n    """"""Initializes this layer.\n\n    Args:\n      num_heads: The number of attention heads.\n      num_units: The number of hidden units.\n      dropout: The probability to drop units from the inputs.\n      return_attention: If ``True``, also return the attention weights of the\n        first head.\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n      kwargs: Additional layer arguments.\n    """"""\n    super(MultiHeadAttention, self).__init__(**kwargs)\n    if num_units % num_heads != 0:\n      raise ValueError(""Multi head attention requires that num_units is a""\n                       "" multiple of %s"" % num_heads)\n    self.num_heads = num_heads\n    self.num_units_per_head = num_units // num_heads\n    self.linear_queries = common.Dense(num_units)\n    self.linear_keys = common.Dense(num_units)\n    self.linear_values = common.Dense(num_units)\n    self.linear_output = common.Dense(num_units)\n    self.dropout = dropout\n    self.return_attention = return_attention\n    self.maximum_relative_position = maximum_relative_position\n\n  def map_v1_weights(self, weights):\n    # V1 used conv1d layers that have a leading dimensions.\n    weights = tf.nest.map_structure(np.squeeze, weights)\n\n    # V1 used fused linear projections, so the weights should be split accordingly.\n    def _partial_weights(key, num_splits, index):\n      return tf.nest.map_structure(\n          lambda w: np.split(w, num_splits, axis=0 if w.ndim == 1 else 1)[index],\n          weights[key])\n\n    m = []\n    if ""conv1d_2"" not in weights:  # Case self-attention.\n      m += self.linear_queries.map_v1_weights(_partial_weights(""conv1d"", 3, 0))\n      m += self.linear_keys.map_v1_weights(_partial_weights(""conv1d"", 3, 1))\n      m += self.linear_values.map_v1_weights(_partial_weights(""conv1d"", 3, 2))\n      m += self.linear_output.map_v1_weights(weights[""conv1d_1""])\n    else:\n      m += self.linear_queries.map_v1_weights(weights[""conv1d""])\n      m += self.linear_keys.map_v1_weights(_partial_weights(""conv1d_1"", 2, 0))\n      m += self.linear_values.map_v1_weights(_partial_weights(""conv1d_1"", 2, 1))\n      m += self.linear_output.map_v1_weights(weights[""conv1d_2""])\n    return m\n\n  def build(self, input_shape):\n    if self.maximum_relative_position is not None:\n      relative_window_size = self.maximum_relative_position * 2 + 1\n      relative_repr_shape = [relative_window_size, self.num_units_per_head]\n      self.relative_position_keys = self.add_weight(\n          name=""relative_position_keys"", shape=relative_repr_shape)\n      self.relative_position_values = self.add_weight(\n          name=""relative_position_values"", shape=relative_repr_shape)\n    super(MultiHeadAttention, self).build(input_shape)\n\n  def call(self, inputs, memory=None, mask=None, cache=None, training=None):  # pylint: disable=arguments-differ\n    """"""Runs the layer.\n\n    Args:\n      inputs: The sequence of queries. A tensor of shape :math:`[B, T_1, ...]`.\n      memory: The sequence to attend. A tensor of shape :math:`[B, T_2, ...]`.\n        If ``None``, computes self-attention.\n      mask: The dot product mask. A boolean tensor of shape :math:`[B, T_2]` or\n        :math:`[B, T_1, T_2]`.\n      cache: An optional tuple containing projected keys and values from the\n        previous step. Tensors of shape :math:`[B, H, T_2, D / H]`.\n      training: Run in training mode.\n\n    Returns:\n      A tuple with the attention context, the updated cache and the attention\n      probabilities of the first head (if :obj:`return_attention` is ``True``).\n    """"""\n\n    def _compute_kv(x):\n      keys = self.linear_keys(x)\n      keys = split_heads(keys, self.num_heads)\n      values = self.linear_values(x)\n      values = split_heads(values, self.num_heads)\n      return keys, values\n\n    # Compute queries.\n    queries = self.linear_queries(inputs)\n    queries = split_heads(queries, self.num_heads)\n    queries *= self.num_units_per_head**-0.5\n\n    # Compute keys and values.\n    if memory is None:\n      keys, values = _compute_kv(inputs)\n      if cache:\n        keys = tf.concat([cache[0], keys], axis=2)\n        values = tf.concat([cache[1], values], axis=2)\n    else:\n      if cache:\n        keys, values = tf.cond(\n            tf.equal(tf.shape(cache[0])[2], 0),\n            true_fn=lambda: _compute_kv(memory),\n            false_fn=lambda: cache)\n      else:\n        keys, values = _compute_kv(memory)\n\n    if self.maximum_relative_position is not None:\n      if memory is not None:\n        raise ValueError(""Relative position representations only supports self-attention"")\n      keys_length = tf.shape(keys)[2]\n      relative_pos = relative_positions(\n          keys_length,\n          self.maximum_relative_position,\n          with_cache=bool(cache))\n      relative_repr_keys = tf.gather(self.relative_position_keys, relative_pos)\n      relative_repr_values = tf.gather(self.relative_position_values, relative_pos)\n    else:\n      relative_repr_keys = None\n      relative_repr_values = None\n\n    cache = (keys, values)\n\n    # Dot product attention.\n    dot = tf.matmul(queries, keys, transpose_b=True)\n    if relative_repr_keys is not None:\n      dot += matmul_with_relative_representations(queries, relative_repr_keys, transpose_b=True)\n    if mask is not None:\n      mask = tf.cast(mask, tf.float32)\n      if mask.shape.rank == 2:\n        mask = tf.expand_dims(mask, 1)  # Broadcast on time dimension.\n      mask = tf.expand_dims(mask, 1)  # Broadcast on head dimension.\n      dot = tf.cast(tf.cast(dot, tf.float32) * mask + ((1.0 - mask) * tf.float32.min), dot.dtype)\n    attn = tf.cast(tf.nn.softmax(tf.cast(dot, tf.float32)), dot.dtype)\n    drop_attn = common.dropout(attn, self.dropout, training=training)\n    heads = tf.matmul(drop_attn, values)\n    if relative_repr_values is not None:\n      heads += matmul_with_relative_representations(drop_attn, relative_repr_values)\n\n    # Concatenate all heads output.\n    combined = combine_heads(heads)\n    outputs = self.linear_output(combined)\n    if self.return_attention:\n      return outputs, cache, attn\n    return outputs, cache\n\n\nclass TransformerLayerWrapper(common.LayerWrapper):\n  """"""Layer wrapper that applies a standard Transformer preprocessing and\n  postprocessing:\n\n  .. code-block:: text\n\n      y = layer_norm(x)\n      y = dropout(layer(y)) + x\n  """"""\n\n  def __init__(self, layer, output_dropout, **kwargs):\n    """"""Initializes the wrapper.\n\n    Args:\n      layer: The Transformer layer to wrap.\n      output_dropout: The dropout to apply on the layer output.\n      **kwargs: Additional layer arguments.\n    """"""\n    super(TransformerLayerWrapper, self).__init__(\n        layer,\n        normalize_input=True,\n        output_dropout=output_dropout,\n        residual_connection=True,\n        **kwargs)\n\n  def map_v1_weights(self, weights):\n    m = []\n    m += self.input_layer_norm.map_v1_weights(weights[""LayerNorm""])\n    m += self.layer.map_v1_weights(weights)\n    return m\n\n\nclass SelfAttentionEncoderLayer(tf.keras.layers.Layer):\n  """"""Implements one self-attention encoding layer.""""""\n\n  def __init__(self,\n               num_units,\n               num_heads,\n               ffn_inner_dim,\n               dropout=0.1,\n               attention_dropout=0.1,\n               ffn_dropout=0.1,\n               ffn_activation=tf.nn.relu,\n               maximum_relative_position=None,\n               **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      num_units: The number of hidden units.\n      num_heads: The number of heads in the multi-head attention.\n      ffn_inner_dim: The number of units of the inner linear transformation\n        in the feed forward layer.\n      dropout: The probability to drop units from the outputs.\n      attention_dropout: The probability to drop units from the attention.\n      ffn_dropout: The probability to drop units from the activation output in\n        the feed forward layer.\n      ffn_activation: The activation function to apply between the two linear\n        transformations of the feed forward layer.\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n      kwargs: Additional layer arguments.\n    """"""\n    super(SelfAttentionEncoderLayer, self).__init__(**kwargs)\n    self.self_attention = MultiHeadAttention(\n        num_heads,\n        num_units,\n        dropout=attention_dropout,\n        maximum_relative_position=maximum_relative_position)\n    self.self_attention = TransformerLayerWrapper(\n        self.self_attention, dropout)\n    self.ffn = FeedForwardNetwork(\n        ffn_inner_dim,\n        num_units,\n        dropout=ffn_dropout,\n        activation=ffn_activation)\n    self.ffn = TransformerLayerWrapper(\n        self.ffn, dropout)\n\n  def call(self, x, mask=None, training=None):  # pylint: disable=arguments-differ\n    """"""Runs the encoder layer.""""""\n    y, _ = self.self_attention(x, mask=mask, training=training)\n    y = self.ffn(y, training=training)\n    return y\n\n  def map_v1_weights(self, weights):\n    m = []\n    m += self.self_attention.map_v1_weights(weights[""multi_head""])\n    m += self.ffn.map_v1_weights(weights[""ffn""])\n    return m\n\n\nclass SelfAttentionDecoderLayer(tf.keras.layers.Layer):\n  """"""Implements one self-attention decoding layer.""""""\n\n  def __init__(self,\n               num_units,\n               num_heads,\n               ffn_inner_dim,\n               num_sources=1,\n               dropout=0.1,\n               attention_dropout=0.1,\n               ffn_dropout=0.1,\n               ffn_activation=tf.nn.relu,\n               maximum_relative_position=None,\n               **kwargs):\n    """"""Initializes the layer.\n\n    Args:\n      num_units: The number of hidden units.\n      num_heads: The number of heads in the multi-head attention.\n      ffn_inner_dim: The number of units of the inner linear transformation\n        in the feed forward layer.\n      num_sources: The number of source contexts.\n      dropout: The probability to drop units from the outputs.\n      attention_dropout: The probability to drop units from the attention.\n      ffn_dropout: The probability to drop units from the activation output in\n        the feed forward layer.\n      ffn_activation: The activation function to apply between the two linear\n        transformations of the feed forward layer.\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n      **kwargs: Additional layer arguments.\n    """"""\n    super(SelfAttentionDecoderLayer, self).__init__(**kwargs)\n    self.self_attention = MultiHeadAttention(\n        num_heads,\n        num_units,\n        dropout=attention_dropout,\n        maximum_relative_position=maximum_relative_position)\n    self.self_attention = TransformerLayerWrapper(\n        self.self_attention, dropout)\n    self.attention = []\n    for i in range(num_sources):\n      attention = MultiHeadAttention(\n          num_heads,\n          num_units,\n          dropout=attention_dropout,\n          return_attention=i == 0)\n      attention = TransformerLayerWrapper(\n          attention, dropout)\n      self.attention.append(attention)\n    self.ffn = FeedForwardNetwork(\n        ffn_inner_dim,\n        num_units,\n        dropout=ffn_dropout,\n        activation=ffn_activation)\n    self.ffn = TransformerLayerWrapper(\n        self.ffn, dropout)\n\n  def map_v1_weights(self, weights):\n    m = []\n    m += self.self_attention.map_v1_weights(weights[""masked_multi_head""])\n    m += self.attention[0].map_v1_weights(weights[""multi_head""])\n    m += self.ffn.map_v1_weights(weights[""ffn""])\n    return m\n\n  # pylint: disable=arguments-differ\n  def call(self,\n           inputs,\n           mask=None,\n           memory=None,\n           memory_mask=None,\n           cache=None,\n           training=None):\n    """"""Runs the decoder layer.""""""\n    if cache is None:\n      cache = {}\n\n    outputs, self_kv = self.self_attention(\n        inputs,\n        mask=mask,\n        cache=cache.get(""self_kv""),\n        training=training)\n\n    attention = None\n    memory_kv = []\n    if memory is not None:\n      memory_cache = cache.get(""memory_kv"")\n      if memory_cache is None:\n        memory_cache = [None] * len(self.attention)\n      for layer, mem, mem_mask, mem_cache in zip(\n          self.attention, memory, memory_mask, memory_cache):\n        result = layer(\n            outputs,\n            memory=mem,\n            mask=mem_mask,\n            cache=mem_cache,\n            training=training)\n        if len(result) == 3:\n          outputs, memory_kv_i, attention = result\n          attention = attention[:, 0]  # Use the first head for the attention vector.\n        else:\n          outputs, memory_kv_i = result\n        memory_kv.append(memory_kv_i)\n\n    outputs = self.ffn(outputs, training=training)\n    cache = dict(self_kv=self_kv, memory_kv=memory_kv)\n    return outputs, cache, attention\n'"
opennmt/models/__init__.py,0,"b'""""""Module defining models.""""""\n\nfrom opennmt.models.catalog import GPT2Small\nfrom opennmt.models.catalog import ListenAttendSpell\nfrom opennmt.models.catalog import LstmCnnCrfTagger\nfrom opennmt.models.catalog import LuongAttention\nfrom opennmt.models.catalog import TransformerBase\nfrom opennmt.models.catalog import TransformerBaseRelative\n# Export TransformerRelative for backward compatibility.\nfrom opennmt.models.catalog import TransformerBaseRelative as TransformerRelative  # pylint: disable=reimported\nfrom opennmt.models.catalog import TransformerBig\nfrom opennmt.models.catalog import TransformerBigRelative\nfrom opennmt.models.catalog import get_model_from_catalog\nfrom opennmt.models.catalog import register_model_in_catalog\n\nfrom opennmt.models.language_model import LanguageModel\nfrom opennmt.models.language_model import LanguageModelInputter\n\nfrom opennmt.models.model import Model\nfrom opennmt.models.model import SequenceGenerator\n\nfrom opennmt.models.sequence_classifier import ClassInputter\nfrom opennmt.models.sequence_classifier import SequenceClassifier\n\nfrom opennmt.models.sequence_tagger import SequenceTagger\nfrom opennmt.models.sequence_tagger import TagsInputter\n\nfrom opennmt.models.sequence_to_sequence import EmbeddingsSharingLevel\nfrom opennmt.models.sequence_to_sequence import SequenceToSequence\nfrom opennmt.models.sequence_to_sequence import SequenceToSequenceInputter\n\nfrom opennmt.models.transformer import Transformer\n'"
opennmt/models/catalog.py,11,"b'""""""Catalog of predefined models.""""""\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom opennmt import decoders\nfrom opennmt import encoders\nfrom opennmt import inputters\nfrom opennmt import layers\nfrom opennmt.models import language_model\nfrom opennmt.models import model\nfrom opennmt.models import sequence_tagger\nfrom opennmt.models import sequence_to_sequence\nfrom opennmt.models import transformer\nfrom opennmt.utils import misc\n\n\n_CATALOG_MODELS_REGISTRY = misc.ClassRegistry(base_class=model.Model)\n\nregister_model_in_catalog = _CATALOG_MODELS_REGISTRY.register  # pylint: disable=invalid-name\n\ndef list_model_names_from_catalog():\n  """"""Lists the models name registered in the catalog.""""""\n  return _CATALOG_MODELS_REGISTRY.class_names\n\ndef get_model_from_catalog(name, as_builder=False):\n  """"""Gets a model from the catalog.\n\n  Args:\n    name: The model name in the catalog.\n    as_builder: If ``True``, return a callable building the model on call.\n\n  Returns:\n    A :class:`opennmt.models.Model` instance or a callable returning such\n    instance.\n\n  Raises:\n    ValueError: if the model :obj:`name` does not exist in the catalog.\n  """"""\n  model_class = _CATALOG_MODELS_REGISTRY.get(name)\n  if model_class is None:\n    raise ValueError(""The model \'%s\' does not exist in the model catalog"" % name)\n  if as_builder:\n    return model_class\n  return model_class()\n\n\n@register_model_in_catalog\nclass ListenAttendSpell(sequence_to_sequence.SequenceToSequence):\n  """"""Defines a model similar to the ""Listen, Attend and Spell"" model described\n  in https://arxiv.org/abs/1508.01211.\n  """"""\n  def __init__(self):\n    super(ListenAttendSpell, self).__init__(\n        source_inputter=inputters.SequenceRecordInputter(input_depth=40),\n        target_inputter=inputters.WordEmbedder(\n            embedding_size=50),\n        encoder=encoders.PyramidalRNNEncoder(\n            num_layers=3,\n            num_units=512,\n            reduction_factor=2,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.3),\n        decoder=decoders.AttentionalRNNDecoder(\n            num_layers=3,\n            num_units=512,\n            attention_mechanism_class=tfa.seq2seq.LuongMonotonicAttention,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.3,\n            residual_connections=False,\n            first_layer_attention=True))\n\n  def auto_config(self, num_replicas=1):\n    config = super(ListenAttendSpell, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""params"": {\n            ""optimizer"": ""SGD"",\n            ""learning_rate"": 0.2,\n            ""scheduled_sampling_type"": ""constant"",\n            ""scheduled_sampling_read_probability"": 0.9\n        },\n        ""train"": {\n            ""batch_size"": 32,\n            ""length_bucket_width"": 15,\n            ""maximum_features_length"": 2450,\n            ""maximum_labels_length"": 330\n        }\n    })\n\nclass _RNNBase(sequence_to_sequence.SequenceToSequence):\n  """"""Base class for RNN based NMT models.""""""\n\n  def auto_config(self, num_replicas=1):\n    config = super(_RNNBase, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""params"": {\n            ""optimizer"": ""Adam"",\n            ""learning_rate"": 0.0002\n        },\n        ""train"": {\n            ""batch_size"": 64,\n            ""maximum_features_length"": 80,\n            ""maximum_labels_length"": 80\n        }\n    })\n\n@register_model_in_catalog\nclass LuongAttention(_RNNBase):\n  """"""Defines a LSTM encoder-decoder model as described in https://arxiv.org/abs/1508.04025.""""""\n  def __init__(self):\n    super(LuongAttention, self).__init__(\n        source_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        target_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        encoder=encoders.RNNEncoder(\n            num_layers=4,\n            num_units=1000,\n            dropout=0.2,\n            residual_connections=False,\n            cell_class=tf.keras.layers.LSTMCell),\n        decoder=decoders.AttentionalRNNDecoder(\n            num_layers=4,\n            num_units=1000,\n            bridge_class=layers.CopyBridge,\n            attention_mechanism_class=tfa.seq2seq.LuongAttention,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.2,\n            residual_connections=False))\n\n@register_model_in_catalog\nclass NMTBigV1(_RNNBase):\n  """"""Defines a bidirectional LSTM encoder-decoder model.\n\n  Note:\n    For compatibility with OpenNMT-tf v1.\n  """"""\n  def __init__(self):\n    super(NMTBigV1, self).__init__(\n        source_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        target_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        encoder=encoders.RNNEncoder(\n            num_layers=4,\n            num_units=512,\n            bidirectional=True,\n            residual_connections=False,\n            dropout=0.3,\n            reducer=layers.ConcatReducer(),\n            cell_class=tf.keras.layers.LSTMCell),\n        decoder=decoders.AttentionalRNNDecoder(\n            num_layers=4,\n            num_units=1024,\n            bridge_class=layers.CopyBridge,\n            attention_mechanism_class=tfa.seq2seq.LuongAttention,\n            attention_layer_activation=None,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.3,\n            residual_connections=False))\n\n@register_model_in_catalog\nclass NMTMediumV1(_RNNBase):\n  """"""Defines a medium-sized bidirectional LSTM encoder-decoder model.\n\n  Note:\n    For compatibility with OpenNMT-tf v1.\n  """"""\n  def __init__(self):\n    super(NMTMediumV1, self).__init__(\n        source_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        target_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        encoder=encoders.RNNEncoder(\n            num_layers=4,\n            num_units=256,\n            bidirectional=True,\n            residual_connections=False,\n            dropout=0.3,\n            reducer=layers.ConcatReducer(),\n            cell_class=tf.keras.layers.LSTMCell),\n        decoder=decoders.AttentionalRNNDecoder(\n            num_layers=4,\n            num_units=512,\n            bridge_class=layers.CopyBridge,\n            attention_mechanism_class=tfa.seq2seq.LuongAttention,\n            attention_layer_activation=None,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.3,\n            residual_connections=False))\n\n@register_model_in_catalog\nclass NMTSmallV1(_RNNBase):\n  """"""Defines a small unidirectional LSTM encoder-decoder model.\n\n  Note:\n    For compatibility with OpenNMT-tf v1.\n  """"""\n  def __init__(self):\n    super(NMTSmallV1, self).__init__(\n        source_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        target_inputter=inputters.WordEmbedder(\n            embedding_size=512),\n        encoder=encoders.RNNEncoder(\n            num_layers=2,\n            num_units=512,\n            residual_connections=False,\n            dropout=0.3,\n            cell_class=tf.keras.layers.LSTMCell),\n        decoder=decoders.AttentionalRNNDecoder(\n            num_layers=2,\n            num_units=512,\n            bridge_class=layers.CopyBridge,\n            attention_mechanism_class=tfa.seq2seq.LuongAttention,\n            attention_layer_activation=None,\n            cell_class=tf.keras.layers.LSTMCell,\n            dropout=0.3,\n            residual_connections=False))\n\n@register_model_in_catalog\nclass LstmCnnCrfTagger(sequence_tagger.SequenceTagger):\n  """"""Defines a bidirectional LSTM-CNNs-CRF as described in https://arxiv.org/abs/1603.01354.""""""\n  def __init__(self):\n    # pylint: disable=bad-continuation\n    super(LstmCnnCrfTagger, self).__init__(\n        inputter=inputters.MixedInputter([\n            inputters.WordEmbedder(\n                embedding_size=100),\n            inputters.CharConvEmbedder(\n                embedding_size=30,\n                num_outputs=30,\n                kernel_size=3,\n                stride=1,\n                dropout=0.5)],\n            dropout=0.5),\n        encoder=encoders.RNNEncoder(\n            num_layers=1,\n            num_units=400,\n            bidirectional=True,\n            dropout=0.5,\n            residual_connections=False,\n            cell_class=tf.keras.layers.LSTMCell),\n        crf_decoding=True)\n\n  def auto_config(self, num_replicas=1):\n    config = super(LstmCnnCrfTagger, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""params"": {\n            ""optimizer"": ""Adam"",\n            ""learning_rate"": 0.001\n        },\n        ""train"": {\n            ""batch_size"": 32\n        }\n    })\n\nclass _DefaultTransformer(transformer.Transformer):\n  def __init__(self, big=False, relative=False):\n    if big:\n      num_units = 1024\n      num_heads = 16\n      ffn_inner_dim = 4096\n    else:\n      num_units = 512\n      num_heads = 8\n      ffn_inner_dim = 2048\n    if relative:\n      position_encoder_class = None\n      maximum_relative_position = 20\n    else:\n      position_encoder_class = layers.SinusoidalPositionEncoder\n      maximum_relative_position = None\n    super(_DefaultTransformer, self).__init__(\n        source_inputter=inputters.WordEmbedder(embedding_size=num_units),\n        target_inputter=inputters.WordEmbedder(embedding_size=num_units),\n        num_layers=6,\n        num_units=num_units,\n        num_heads=num_heads,\n        ffn_inner_dim=ffn_inner_dim,\n        dropout=0.1,\n        attention_dropout=0.1,\n        ffn_dropout=0.1,\n        position_encoder_class=position_encoder_class,\n        maximum_relative_position=maximum_relative_position)\n\n@register_model_in_catalog(alias=""Transformer"")\nclass TransformerBase(_DefaultTransformer):\n  """"""Defines a Transformer model as decribed in https://arxiv.org/abs/1706.03762.""""""\n\n@register_model_in_catalog(alias=""TransformerRelative"")\nclass TransformerBaseRelative(_DefaultTransformer):\n  """"""Defines a Transformer model using relative position representations as\n  described in https://arxiv.org/abs/1803.02155.\n  """"""\n  def __init__(self):\n    super(TransformerRelative, self).__init__(relative=True)\n\n# Backward compatibility with model descriptions that directly accessed the catalog module.\nTransformer = TransformerBase\nTransformerRelative = TransformerBaseRelative\n\n@register_model_in_catalog\nclass TransformerBig(_DefaultTransformer):\n  """"""Defines a large Transformer model as decribed in https://arxiv.org/abs/1706.03762.""""""\n  def __init__(self):\n    super(TransformerBig, self).__init__(big=True)\n\n@register_model_in_catalog\nclass TransformerBigRelative(_DefaultTransformer):\n  """"""Defines a large Transformer model using relative position representations as\n  described in https://arxiv.org/abs/1803.02155.\n  """"""\n  def __init__(self):\n    super(TransformerBigRelative, self).__init__(big=True, relative=True)\n\n@register_model_in_catalog\nclass GPT2Small(language_model.LanguageModel):\n  """"""GPT-2 language model (small version) as described in:\n\n  https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n  """"""\n\n  def __init__(self):\n    super(GPT2Small, self).__init__(\n        decoder=decoders.SelfAttentionDecoder(\n            num_layers=12,\n            num_units=768,\n            num_heads=12,\n            ffn_inner_dim=3072,\n            ffn_activation=layers.gelu,\n            position_encoder_class=lambda: layers.PositionEmbedder(maximum_position=1024),\n            num_sources=0),\n        embedding_size=768)\n\n  def auto_config(self, num_replicas=1):\n    config = super(GPT2Small, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""params"": {\n            ""average_loss_in_time"": True,\n            ""optimizer"": ""Adam"",\n            ""learning_rate"": 2.5e-4,\n            ""decay_type"": ""CosineAnnealing"",\n            ""decay_params"": {\n                ""max_step"": 1000000,\n                ""warmup_steps"": 2000,\n            }\n        },\n        ""train"": {\n            # Below options are from GPT-1.\n            ""batch_size"": 64,\n            ""maximum_features_length"": 512\n        }\n    })\n'"
opennmt/models/language_model.py,13,"b'""""""Language model.""""""\n\nimport tensorflow as tf\n\nfrom opennmt import inputters\nfrom opennmt.models import model\nfrom opennmt.utils import decoding\nfrom opennmt.utils import losses\nfrom opennmt.utils import misc\n\n\nclass LanguageModel(model.SequenceGenerator):\n  """"""A language model.""""""\n\n  def __init__(self,\n               decoder,\n               embedding_size=None,\n               reuse_embedding=True):\n    """"""Initializes the language model.\n\n    Args:\n      decoder: A :class:`opennmt.decoders.Decoder` instance.\n      embedding_size: The size of the word embedding. If not set, pretrained\n        embeddings should be defined in the configuration.\n      reuse_embedding: If ``True``, reuse the embedding weights in the output\n        layer.\n\n    Raises:\n      ValueError: if the decoder type is invalid.\n    """"""\n    inputter = LanguageModelInputter(embedding_size=embedding_size)\n    super(LanguageModel, self).__init__(inputter)\n    self.decoder = decoder\n    self.reuse_embedding = reuse_embedding\n\n  def auto_config(self, num_replicas=1):\n    config = super(LanguageModel, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""infer"": {\n            ""length_bucket_width"": 1  # To ensure fixed length in each batch.\n        }\n    })\n\n  def initialize(self, data_config, params=None):\n    super(LanguageModel, self).initialize(data_config, params=params)\n    self.decoder.initialize(vocab_size=self.examples_inputter.vocabulary_size)\n\n  def build(self, input_shape):\n    super(LanguageModel, self).build(input_shape)\n    if self.reuse_embedding:\n      self.decoder.reuse_embeddings(self.examples_inputter.embedding)\n\n  def call(self, features, labels=None, training=None, step=None):\n    outputs, predictions = None, None\n\n    ids, length = features[""ids""], features[""length""]\n    if labels is not None:\n      # For training and evaluation, forward the full sequence.\n      logits, _ = self._decode(\n          labels.get(""ids"", ids),\n          labels.get(""length"", length),\n          training=training)\n      outputs = dict(logits=logits)\n    else:\n      assert_fixed_length = tf.debugging.Assert(\n          tf.reduce_all(tf.equal(length, tf.reduce_max(length))),\n          [""Language model does not support variable length contexts during ""\n           ""generation, consider setting batch_size or length_bucket_width to 1""])\n      assert_non_empty_start = tf.debugging.Assert(\n          tf.math.not_equal(tf.math.reduce_max(length), 0),\n          [""The language model requires a context sequence to initialize the decoding. ""\n           ""If you want nonconditional sequence generation, you should configure the ""\n           ""sequence_controls parameter before training.""])\n\n      # Run decoder on the context, if any.\n      with tf.control_dependencies([assert_fixed_length, assert_non_empty_start]):\n        context_ids, start_ids = tf.split(ids, [tf.shape(ids)[1] - 1, 1], axis=1)\n        context_length = length - 1\n        batch_size = tf.shape(context_length)[0]\n        state = tf.cond(\n            tf.equal(tf.reduce_sum(context_length), 0),\n            true_fn=lambda: self.decoder.initial_state(batch_size=batch_size, dtype=self.dtype),\n            false_fn=lambda: self._decode(context_ids, context_length)[1])\n\n      params = self.params\n\n      def _decode_with_step_offset(ids, step, state):\n        return self._decode(ids, step + context_length[0], state)\n\n      # Iteratively decode from the last decoder state.\n      sampled_ids, sampled_length, _, _, _ = decoding.dynamic_decode(\n          _decode_with_step_offset,\n          tf.squeeze(start_ids, 1),\n          initial_state=state,\n          sampler=decoding.Sampler.from_params(params),\n          maximum_iterations=params.get(""maximum_decoding_length"", 250),\n          minimum_iterations=params.get(""minimum_decoding_length"", 0))\n      sampled_ids = tf.reshape(sampled_ids, [batch_size, -1])\n      sampled_length = tf.reshape(sampled_length, [batch_size])\n\n      # Build the full prediction.\n      if self.features_inputter.mark_start:\n        # Remove leading <s> if included in the context sequence.\n        ids = ids[:, 1:]\n        length -= 1\n      full_ids = tf.concat([ids, sampled_ids], 1)\n      full_length = length + sampled_length\n      tokens = self.features_inputter.ids_to_tokens.lookup(full_ids)\n      predictions = dict(tokens=tokens, length=full_length)\n\n    return outputs, predictions\n\n  def _decode(self, ids, length_or_step, state=None, training=None):\n    # Decode from ids.\n    inputs = self.examples_inputter({""ids"": ids}, training=training)\n    logits, state, _ = self.decoder(inputs, length_or_step, state=state, training=training)\n    return logits, state\n\n  def compute_loss(self, outputs, labels, training=True):\n    return losses.cross_entropy_sequence_loss(\n        outputs[""logits""],\n        labels[""ids_out""],\n        labels[""length""],\n        label_smoothing=self.params.get(""label_smoothing"", 0.0),\n        average_in_time=self.params.get(""average_loss_in_time"", False),\n        training=training)\n\n  def print_prediction(self, prediction, params=None, stream=None):\n    target_length = prediction[""length""]\n    tokens = prediction[""tokens""][:target_length]\n    sentence = self.examples_inputter.tokenizer.detokenize(tokens)\n    sentence = misc.format_translation_output(sentence)\n    misc.print_as_bytes(sentence, stream=stream)\n\n\nclass LanguageModelInputter(inputters.WordEmbedder, inputters.ExampleInputterAdapter):\n  """"""A special inputter for language modeling.\n\n  This is a single word embedder that simply produces labels by shifting the\n  input sequence.\n  """"""\n\n  def initialize(self, data_config, asset_prefix=""""):\n    super(LanguageModelInputter, self).initialize(data_config, asset_prefix=asset_prefix)\n    # Set default sequence controls for backward compatibility.\n    if self.mark_start is None:\n      self.mark_start = False\n    if self.mark_end is None:\n      self.mark_end = True\n\n  def make_features(self, element=None, features=None, training=None):\n    base_features = features if features is not None else {}\n\n    # Features define the decoder context during inference. As the context is a prefix,\n    # we should disable the end sequence control token.\n    saved_mark_end = self.mark_end\n    self.set_decoder_mode(enable=False, mark_end=False)\n    features = super(LanguageModelInputter, self).make_features(\n        element=element, features=base_features.copy(), training=training)\n\n    # Labels define the decoder input/output sequences during training and evaluation.\n    self.set_decoder_mode(enable=True, mark_end=saved_mark_end)\n    labels = super(LanguageModelInputter, self).make_features(\n        element=element, features=base_features.copy(), training=training)\n\n    return features, labels\n'"
opennmt/models/model.py,23,"b'""""""Base class for models.""""""\n\nimport abc\n\nimport tensorflow as tf\n\nfrom opennmt import optimizers\nfrom opennmt import schedules\nfrom opennmt.utils import exporters\nfrom opennmt.utils import losses\nfrom opennmt.utils import misc\n\n\nclass Model(tf.keras.layers.Layer):\n  """"""Base class for models.""""""\n\n  def __init__(self, examples_inputter):\n    super(Model, self).__init__()\n    self.examples_inputter = examples_inputter\n    self.params = {}\n\n  @property\n  def unsupervised(self):\n    """"""Unsupervised model.""""""\n    return self.labels_inputter is None\n\n  @property\n  def features_inputter(self):\n    """"""The inputter producing features.""""""\n    return getattr(self.examples_inputter, ""features_inputter"", self.examples_inputter)\n\n  @property\n  def labels_inputter(self):\n    """"""The inputter producing labels.""""""\n    return getattr(self.examples_inputter, ""labels_inputter"", None)\n\n  @property\n  def ctranslate2_spec(self):\n    """"""The equivalent CTranslate2 model specification.""""""\n    return None\n\n  def auto_config(self, num_replicas=1):\n    """"""Returns automatic configuration values specific to this model.\n\n    Args:\n      num_replicas: The number of synchronous model replicas used for the\n        training.\n\n    Returns:\n      A partial training configuration.\n    """"""\n    _ = num_replicas\n    return {}\n\n  def initialize(self, data_config, params=None):\n    """"""Initializes the model from the data configuration.\n\n    Args:\n      data_config: A dictionary containing the data configuration set\n        by the user (e.g. vocabularies, tokenization, pretrained embeddings,\n        etc.).\n      params: A dictionary of hyperparameters.\n    """"""\n    if params is None:\n      params = {}\n    self.params.update(params)\n    dropout = self.params.get(""dropout"")\n    if dropout is not None:\n      misc.set_dropout(self, dropout)\n    self.examples_inputter.initialize(data_config)\n\n  def build(self, input_shape):\n    freeze_layers = self.params.get(""freeze_layers"")\n    if freeze_layers:\n      if not isinstance(freeze_layers, list):\n        freeze_layers = [freeze_layers]\n      for layer_path in freeze_layers:\n        layer = misc.index_structure(self, layer_path)\n        layer.trainable = False\n        misc.set_dropout(layer, 0)  # Disable dropout in frozen layers.\n    self.examples_inputter.build(input_shape)\n    self.built = True\n\n  @abc.abstractmethod\n  def call(self, features, labels=None, training=None, step=None):  # pylint: disable=arguments-differ\n    """"""Runs the model.\n\n    Args:\n      features: A nested structure of features ``tf.Tensor``.\n      labels: A nested structure of labels ``tf.Tensor``.\n      training: If ``True``, run in training mode.\n      step: The current training step.\n\n    Returns:\n      A tuple containing,\n\n      - The model outputs (usually unscaled probabilities).\n      - The model predictions.\n    """"""\n    raise NotImplementedError()\n\n  def infer(self, features):\n    """"""Runs inference on :obj:`features`.\n\n    This is a small convenience wrapper around\n    :meth:`opennmt.models.Model.call`.\n\n    Args:\n      features: A nested structure of features ``tf.Tensor``.\n\n    Returns:\n      The model predictions.\n    """"""\n    _, predictions = self(features)\n    if ""index"" in features:\n      predictions[""index""] = features[""index""]\n    return predictions\n\n  def evaluate(self, features, labels):\n    """"""Evaluates :obj:`features` predictions against `labels`.\n\n    Args:\n      features: A nested structure of features ``tf.Tensor``.\n      labels: A nested structure of labels ``tf.Tensor``.\n\n    Returns:\n      A tuple with the loss and the model predictions.\n    """"""\n    outputs, predictions = self(features, labels=labels)\n    loss = self.compute_loss(outputs, labels, training=False)\n    return loss, predictions\n\n  def score(self, features, labels):\n    """"""Scores labels.\n\n    Args:\n      features: A nested structure of features ``tf.Tensor``.\n      labels: A nested structure of labels ``tf.Tensor``.\n\n    Returns:\n      The score results.\n    """"""\n    raise NotImplementedError(""This model does not define a score function"")\n\n  @abc.abstractmethod\n  def compute_loss(self, outputs, labels, training=True):\n    """"""Computes the loss.\n\n    Args:\n      outputs: The model outputs (usually unscaled probabilities).\n      labels: The dict of labels ``tf.Tensor``.\n      training: If ``True``, compute the loss for training.\n\n    Returns:\n      The loss or a tuple ``(numerator, train_denominator, stats_denominator)``\n      to use a different normalization for training compared to reporting (e.g.\n      batch-normalized for training vs. token-normalized for reporting).\n    """"""\n    raise NotImplementedError()\n\n  def regularize_loss(self, loss, variables=None):\n    """"""Regularizes the loss.\n\n    Args:\n      loss: The loss.\n      variables: List of variables.\n\n    Returns:\n      The regularized loss.\n    """"""\n    if variables is None:\n      variables = self.trainable_variables\n    regularization = self.params.get(""regularization"")\n    if regularization is not None:\n      loss += losses.regularization_penalty(\n          regularization[""type""], regularization[""scale""], variables)\n    return loss\n\n  def get_metrics(self):\n    """"""Returns the metrics for this model.\n\n    Returns:\n      A dictionary of ``tf.keras.metrics.Metric`` metrics.\n    """"""\n    return None\n\n  def update_metrics(self, metrics, predictions, labels):  # pylint: disable=unused-argument\n    """"""Computes additional metrics on the predictions.\n\n    Args:\n      metrics: A dictionary of metrics to update.\n      predictions: The model predictions.\n      labels: The dict of labels ``tf.Tensor``.\n    """"""\n    return\n\n  def get_optimizer(self):\n    """"""Returns the optimizer for this model.\n\n    Returns:\n      A ``tf.keras.optimizers.Optimizer`` instance or ``None`` if no optimizer\n      is configured.\n    """"""\n    params = self.params\n    optimizer_name = params.get(""optimizer"")\n    if optimizer_name is None:\n      return None\n    learning_rate = tf.constant(params[""learning_rate""], dtype=tf.float32)\n    if params.get(""decay_type"") is not None:\n      schedule_params = params.get(""decay_params"", {})\n      learning_rate = schedules.make_learning_rate_schedule(\n          learning_rate,\n          params[""decay_type""],\n          schedule_params=schedule_params,\n          start_step=params.get(""start_decay_steps"", 0),\n          minimum_learning_rate=params.get(""minimum_learning_rate"", 0))\n    optimizer_params = params.get(""optimizer_params"")\n    if optimizer_params is None:\n      optimizer_params = {}\n    optimizer = optimizers.make_optimizer(\n        optimizer_name, learning_rate, **optimizer_params)\n    return optimizer\n\n  def serve_function(self):\n    """"""Returns a function for serving this model.\n\n    Returns:\n      A ``tf.function``.\n    """"""\n    # Set name attribute of the input TensorSpec.\n    input_signature = {\n        name:tf.TensorSpec.from_spec(spec, name=name)\n        for name, spec in self.features_inputter.input_signature().items()}\n\n    @tf.function(input_signature=(input_signature,))\n    def _run(features):\n      features = self.features_inputter.make_features(features=features.copy())\n      if isinstance(features, (list, tuple)):\n        # Special case for unsupervised inputters that always return a tuple (features, labels).\n        features = features[0]\n      _, predictions = self(features)\n      return predictions\n\n    return _run\n\n  def export(self, export_dir, exporter=None):\n    """"""Exports the model for serving.\n\n    Args:\n      export_dir: The output directory.\n      exporter: A :class:`opennmt.utils.Exporter` instance. Defaults to\n        :class:`opennmt.utils.SavedModelExporter`.\n    """"""\n    if exporter is None:\n      exporter = exporters.SavedModelExporter()\n    exporter.export(self, export_dir)\n\n  def create_variables(self, optimizer=None):\n    """"""Creates the model variables by running it once.\n\n    Args:\n      optimizer: If set, also create the optimizer variables.\n    """"""\n    # Create input features from the input signatures. We remove the leading\n    # batch dimension as sometimes assumed by make_features methods and set\n    # unspecified dimensions to 1.\n    features = tf.nest.map_structure(\n        lambda spec: tf.fill(\n            [dim or 1 for dim in spec.shape.as_list()[1:]],\n            tf.constant(""a"" if spec.dtype is tf.string else 1, dtype=spec.dtype)),\n        self.examples_inputter.input_signature())\n    features = self.examples_inputter.make_features(features=features)\n\n    # Add the batch dimension back before calling the model.\n    features, labels = tf.nest.map_structure(lambda x: tf.expand_dims(x, 0), features)\n    _ = self(features, labels=labels, training=True, step=0)\n\n    if optimizer is not None:\n      _ = optimizer.iterations\n      optimizer._create_hypers()  # pylint: disable=protected-access\n      optimizer._create_slots(self.trainable_variables)  # pylint: disable=protected-access\n\n  def transfer_weights(self, new_model, new_optimizer=None, optimizer=None, ignore_weights=None):\n    """"""Transfers weights (and optionally optimizer slots) from this model to\n    another.\n\n    This default implementation assumes that :obj:`self` and :obj:`new_model`\n    have exactly the same variables. Subclasses can override this method to\n    transfer weights to another model type or architecture. For example,\n    :class:`opennmt.models.SequenceToSequence` can transfer weights to a model\n    with a different vocabulary.\n\n    All model and optimizer variables are expected to be initialized.\n\n    Args:\n      new_model: The new model to transfer weights to.\n      new_optimizer: The new optimizer.\n      optimizer: The optimizer used for the current model.\n      ignore_weights: Optional list of weights to not transfer.\n    """"""\n    if type(self) is not type(new_model):\n      raise ValueError(""Transferring weights to another model type is not supported"")\n    if ignore_weights is None:\n      ignore_weights = set()\n    ignore_weights_ref = set(weight.ref() for weight in ignore_weights)\n    weights = self.weights\n    new_weights = new_model.weights\n    for weight, new_weight in zip(weights, new_weights):\n      if new_weight.ref() not in ignore_weights_ref:\n        new_weight.assign(weight)\n        if new_optimizer is not None and optimizer is not None:\n          for slot_name in new_optimizer.get_slot_names():\n            if slot_name not in optimizer.get_slot_names():\n              continue\n            new_slot = new_optimizer.get_slot(new_weight, slot_name)\n            slot = optimizer.get_slot(weight, slot_name)\n            new_slot.assign(slot)\n\n  def map_v1_weights(self, weights):\n    """"""Maps current weights to V1 weights.\n\n    Args:\n      weights: A nested dictionary following the scope names used in V1. The\n        leaves are tuples with the variable value and optionally the optimizer\n        slots.\n\n    Returns:\n      A list of tuples associating variables and their V1 equivalent.\n    """"""\n    raise NotImplementedError(""This model can not restore V1 checkpoints"")\n\n  def export_assets(self, asset_dir):\n    """"""Exports additional assets used by this model.\n\n    Args:\n      asset_dir: The directory where assets can be written.\n\n    Returns:\n      A dictionary of additional assets.\n    """"""\n    return self.examples_inputter.export_assets(asset_dir)\n\n  def visualize(self, log_dir):\n    """"""Setups model visualization (e.g. word embedding projections).\n\n    Args:\n      log_dir: The log directory.\n    """"""\n    self.features_inputter.visualize(self, log_dir)\n    if not self.unsupervised:\n      self.labels_inputter.visualize(self, log_dir)\n\n  def print_prediction(self, prediction, params=None, stream=None):\n    """"""Prints the model prediction.\n\n    Args:\n      prediction: The model prediction.\n      params: (optional) Dictionary of formatting parameters.\n      stream: (optional) The stream to print to.\n    """"""\n    _ = params\n    print(prediction, file=stream)\n\n  def print_score(self, score, params=None, stream=None):\n    """"""Prints the score result.\n\n    Args:\n      score: The score result (output of :meth:`opennmt.models.Model.score`).\n      params: (optional) Dictionary of formatting parameters.\n      stream: (optional) The stream to print to.\n    """"""\n    _ = params\n    print(score, file=stream)\n\n\nclass SequenceGenerator(Model):\n  """"""Base class for models generating sequences.""""""\n\n  @property\n  def decoder_inputter(self):\n    """"""The inputter used on the decoder side.""""""\n    return (\n        self.labels_inputter if not self.unsupervised\n        else self.features_inputter)\n\n  def score(self, features, labels):\n    outputs, _ = self(features, labels=labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels[""ids_out""], outputs[""logits""])\n    weights = tf.sequence_mask(labels[""length""], dtype=cross_entropy.dtype)\n    masked_cross_entropy = cross_entropy * weights\n    scores = tf.reduce_sum(masked_cross_entropy, axis=1)\n    results = {\n        ""cross_entropy"": cross_entropy,\n        ""score"": scores,\n        ""tokens"": labels[""tokens""],\n        ""length"": self.decoder_inputter.get_length(labels, ignore_special_tokens=True)\n    }\n    if ""attention"" in outputs:\n      results[""attention""] = outputs[""attention""]\n    return results\n\n  def print_score(self, score, params=None, stream=None):\n    if params is None:\n      params = {}\n    length = score[""length""]\n    tokens = score[""tokens""][:length]\n    sentence = self.decoder_inputter.tokenizer.detokenize(tokens)\n    token_level_scores = None\n    attention = None\n    if params.get(""with_token_level""):\n      token_level_scores = score[""cross_entropy""][:length]\n    if ""attention"" in score:\n      attention = score[""attention""][:length]\n    alignment_type = params.get(""with_alignments"")\n    sentence = misc.format_translation_output(\n        sentence,\n        score=score[""score""],\n        token_level_scores=token_level_scores,\n        attention=attention,\n        alignment_type=alignment_type)\n    misc.print_as_bytes(sentence, stream=stream)\n'"
opennmt/models/sequence_classifier.py,7,"b'""""""Sequence classifier.""""""\n\nimport tensorflow as tf\n\nfrom opennmt import inputters\nfrom opennmt.models.model import Model\nfrom opennmt.utils import misc\nfrom opennmt.utils.losses import cross_entropy_loss\n\n\nclass SequenceClassifier(Model):\n  """"""A sequence classifier.""""""\n\n  def __init__(self, inputter, encoder):\n    """"""Initializes a sequence classifier.\n\n    Args:\n      inputter: A :class:`opennmt.inputters.Inputter` to process the\n        input data.\n      encoder: A :class:`opennmt.encoders.Encoder` to encode the input.\n\n    Raises:\n      ValueError: if :obj:`encoding` is invalid.\n    """"""\n    example_inputter = inputters.ExampleInputter(inputter, ClassInputter())\n    super(SequenceClassifier, self).__init__(example_inputter)\n    self.encoder = encoder\n\n  def build(self, input_shape):\n    super(SequenceClassifier, self).build(input_shape)\n    self.output_layer = tf.keras.layers.Dense(self.labels_inputter.vocabulary_size)\n\n  def call(self, features, labels=None, training=None, step=None):\n    inputs = self.features_inputter(features, training=training)\n    outputs, state, outputs_length = self.encoder(\n        inputs,\n        sequence_length=self.features_inputter.get_length(features),\n        training=training)\n\n    if state is None:\n      if outputs_length is not None:\n        outputs = tf.RaggedTensor.from_tensor(outputs, lengths=outputs_length)\n      encoding = tf.reduce_mean(outputs, axis=1)\n    else:\n      last_state = state[-1] if isinstance(state, (list, tuple)) else state\n      encoding = last_state if not isinstance(state, (list, tuple)) else last_state[0]\n    logits = self.output_layer(encoding)\n\n    if not training:\n      classes_prob = tf.nn.softmax(logits)\n      classes_id = tf.argmax(classes_prob, axis=1)\n      predictions = {\n          ""classes"": self.labels_inputter.ids_to_tokens.lookup(classes_id),\n          ""classes_id"": classes_id\n      }\n    else:\n      predictions = None\n\n    return logits, predictions\n\n  def compute_loss(self, outputs, labels, training=True):\n    return cross_entropy_loss(\n        outputs,\n        labels[""classes_id""],\n        label_smoothing=self.params.get(""label_smoothing"", 0.0),\n        training=training)\n\n  def get_metrics(self):\n    return {""accuracy"": tf.keras.metrics.Accuracy()}\n\n  def update_metrics(self, metrics, predictions, labels):\n    metrics[""accuracy""].update_state(labels[""classes_id""], predictions[""classes_id""])\n\n  def print_prediction(self, prediction, params=None, stream=None):\n    misc.print_as_bytes(prediction[""classes""], stream=stream)\n\n\nclass ClassInputter(inputters.TextInputter):\n  """"""Reading class from a text file.""""""\n\n  def __init__(self):\n    super(ClassInputter, self).__init__(num_oov_buckets=0)\n\n  def make_features(self, element=None, features=None, training=None):\n    if features is None:\n      features = {}\n    if ""classes"" not in features:\n      features[""classes""] = element\n    features[""classes_id""] = self.tokens_to_ids.lookup(features[""classes""])\n    return features\n\n  def input_signature(self):\n    return {""classes"": tf.TensorSpec([None], tf.string)}\n'"
opennmt/models/sequence_tagger.py,16,"b'""""""Sequence tagger.""""""\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\n\nfrom opennmt import inputters\nfrom opennmt.models.model import Model\nfrom opennmt.utils import misc\nfrom opennmt.utils.losses import cross_entropy_sequence_loss\n\n\nclass SequenceTagger(Model):\n  """"""A sequence tagger.""""""\n\n  def __init__(self, inputter, encoder, crf_decoding=False):\n    """"""Initializes a sequence tagger.\n\n    Args:\n      inputter: A :class:`opennmt.inputters.Inputter` to process the\n        input data.\n      encoder: A :class:`opennmt.encoders.Encoder` to encode the input.\n      crf_decoding: If ``True``, add a CRF layer after the encoder.\n    """"""\n    example_inputter = inputters.ExampleInputter(inputter, TagsInputter())\n    super(SequenceTagger, self).__init__(example_inputter)\n    self.encoder = encoder\n    self.crf_decoding = crf_decoding\n    self.tagging_scheme = None\n    self.transition_params = None\n\n  def initialize(self, data_config, params=None):\n    self.tagging_scheme = data_config.get(""tagging_scheme"")\n    if self.tagging_scheme:\n      self.tagging_scheme = self.tagging_scheme.lower()\n    super(SequenceTagger, self).initialize(data_config, params=params)\n\n  def build(self, input_shape):\n    super(SequenceTagger, self).build(input_shape)\n    num_tags = self.labels_inputter.vocabulary_size\n    self.output_layer = tf.keras.layers.Dense(num_tags)\n    if self.crf_decoding:\n      self.transition_params = self.add_weight(""transition_params"", [num_tags, num_tags])\n\n  def call(self, features, labels=None, training=None, step=None):\n    length = self.features_inputter.get_length(features)\n    inputs = self.features_inputter(features, training=training)\n    outputs, _, length = self.encoder(\n        inputs, sequence_length=length, training=training)\n    logits = self.output_layer(outputs)\n    if not training:\n      if self.crf_decoding:\n        tags_id, _ = tfa.text.crf_decode(logits, self.transition_params, length)\n        tags_id = tf.cast(tags_id, tf.int64)\n      else:\n        tags_prob = tf.nn.softmax(logits)\n        tags_id = tf.argmax(tags_prob, axis=2)\n      predictions = {\n          ""length"": tf.identity(length),\n          ""tags"": self.labels_inputter.ids_to_tokens.lookup(tags_id),\n          ""tags_id"": tags_id\n      }\n    else:\n      predictions = None\n    return logits, predictions\n\n  def compute_loss(self, outputs, labels, training=True):\n    if self.crf_decoding:\n      log_likelihood, _ = tfa.text.crf_log_likelihood(\n          outputs,\n          tf.cast(labels[""tags_id""], tf.int32),\n          labels[""length""],\n          transition_params=self.transition_params)\n      batch_size = tf.shape(log_likelihood)[0]\n      return tf.reduce_sum(-log_likelihood) / tf.cast(batch_size, log_likelihood.dtype)\n    else:\n      return cross_entropy_sequence_loss(\n          outputs,\n          labels[""tags_id""],\n          labels[""length""],\n          label_smoothing=self.params.get(""label_smoothing"", 0.0),\n          average_in_time=self.params.get(""average_loss_in_time"", False),\n          training=training)\n\n  def get_metrics(self):\n    metrics = {""accuracy"": tf.keras.metrics.Accuracy()}\n    if self.tagging_scheme in (""bioes"",):\n      f1 = F1()\n      metrics[""f1""] = f1\n      metrics[""precision""] = f1.precision\n      metrics[""recall""] = f1.recall\n    return metrics\n\n  def update_metrics(self, metrics, predictions, labels):\n    weights = tf.sequence_mask(\n        labels[""length""], maxlen=tf.shape(labels[""tags""])[1], dtype=tf.float32)\n\n    metrics[""accuracy""].update_state(\n        labels[""tags_id""], predictions[""tags_id""], sample_weight=weights)\n\n    if self.tagging_scheme in (""bioes"",):\n      flag_fn = None\n      if self.tagging_scheme == ""bioes"":\n        flag_fn = flag_bioes_tags\n\n      gold_flags, predicted_flags = tf.numpy_function(\n          flag_fn,\n          [labels[""tags""], predictions[""tags""], labels[""length""]],\n          [tf.bool, tf.bool])\n\n      metrics[""f1""].update_state(gold_flags, predicted_flags)\n\n  def print_prediction(self, prediction, params=None, stream=None):\n    tags = prediction[""tags""][:prediction[""length""]]\n    sent = b"" "".join(tags)\n    misc.print_as_bytes(sent, stream=stream)\n\n\nclass TagsInputter(inputters.TextInputter):\n  """"""Reading space-separated tags.""""""\n\n  def __init__(self):\n    super(TagsInputter, self).__init__(num_oov_buckets=0)\n\n  def make_features(self, element=None, features=None, training=None):\n    features = super(TagsInputter, self).make_features(\n        element=element, features=features, training=training)\n    return {\n        ""length"": features[""length""],\n        ""tags"": features[""tokens""],\n        ""tags_id"": self.tokens_to_ids.lookup(features[""tokens""])\n    }\n\n\nclass F1(tf.keras.metrics.Metric):\n  """"""Defines a F1 metric.""""""\n\n  def __init__(self, **kwargs):\n    """"""Initializes the metric.\n\n    Args:\n      **kwargs: Base class arguments.\n    """"""\n    super(F1, self).__init__(**kwargs)\n    self.precision = tf.keras.metrics.Precision()\n    self.recall = tf.keras.metrics.Recall()\n\n  @property\n  def updates(self):\n    """"""Metric update operations.""""""\n    return self.precision.updates + self.recall.updates\n\n  def update_state(self, y_true, y_pred):  # pylint: disable=arguments-differ\n    """"""Updates the metric state.""""""\n    self.precision.update_state(y_true, y_pred)\n    self.recall.update_state(y_true, y_pred)\n\n  def result(self):\n    """"""Returns the metric result.""""""\n    precision = self.precision.result()\n    recall = self.recall.result()\n    return (2 * precision * recall) / (recall + precision)\n\n\ndef flag_bioes_tags(gold, predicted, sequence_length=None):\n  """"""Flags chunk matches for the BIOES tagging scheme.\n\n  This function will produce the gold flags and the predicted flags. For each aligned\n  gold flag ``g`` and predicted flag ``p``:\n\n  * when ``g == p == True``, the chunk has been correctly identified (true positive).\n  * when ``g == False and p == True``, the chunk has been incorrectly identified (false positive).\n  * when ``g == True and p == False``, the chunk has been missed (false negative).\n  * when ``g == p == False``, the chunk has been correctly ignored (true negative).\n\n  Args:\n    gold: The gold tags as a Numpy 2D string array.\n    predicted: The predicted tags as a Numpy 2D string array.\n    sequence_length: The length of each sequence as Numpy array.\n\n  Returns:\n    A tuple ``(gold_flags, predicted_flags)``.\n  """"""\n  gold_flags = []\n  predicted_flags = []\n\n  def _add_true_positive():\n    gold_flags.append(True)\n    predicted_flags.append(True)\n  def _add_false_positive():\n    gold_flags.append(False)\n    predicted_flags.append(True)\n  def _add_true_negative():\n    gold_flags.append(False)\n    predicted_flags.append(False)\n  def _add_false_negative():\n    gold_flags.append(True)\n    predicted_flags.append(False)\n\n  def _match(ref, hyp, index, length):\n    if ref[index].startswith(b""B""):\n      match = True\n      while index < length and not ref[index].startswith(b""E""):\n        if ref[index] != hyp[index]:\n          match = False\n        index += 1\n      match = match and index < length and ref[index] == hyp[index]\n      return match, index\n    return ref[index] == hyp[index], index\n\n  for b in range(gold.shape[0]):\n    length = sequence_length[b] if sequence_length is not None else gold.shape[1]\n\n    # First pass to detect true positives and true/false negatives.\n    index = 0\n    while index < length:\n      gold_tag = gold[b][index]\n      match, index = _match(gold[b], predicted[b], index, length)\n      if match:\n        if gold_tag == b""O"":\n          _add_true_negative()\n        else:\n          _add_true_positive()\n      else:\n        if gold_tag != b""O"":\n          _add_false_negative()\n      index += 1\n\n    # Second pass to detect false postives.\n    index = 0\n    while index < length:\n      pred_tag = predicted[b][index]\n      match, index = _match(predicted[b], gold[b], index, length)\n      if not match and pred_tag != b""O"":\n        _add_false_positive()\n      index += 1\n\n  return np.array(gold_flags), np.array(predicted_flags)\n'"
opennmt/models/sequence_to_sequence.py,24,"b'# -*- coding: utf-8 -*-\n\n""""""Standard sequence-to-sequence model.""""""\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom opennmt import constants\nfrom opennmt import inputters\n\nfrom opennmt.data import noise\nfrom opennmt.data import text\nfrom opennmt.data import vocab\nfrom opennmt.layers import reducer\nfrom opennmt.models import model\nfrom opennmt.utils import decoding\nfrom opennmt.utils import losses\nfrom opennmt.utils import misc\nfrom opennmt.decoders import decoder as decoder_util\n\n\nclass EmbeddingsSharingLevel(object):\n  """"""Level of embeddings sharing.\n\n  Possible values are:\n\n   * ``NONE``: no sharing (default)\n   * ``SOURCE_TARGET_INPUT``: share source and target word embeddings\n   * ``TARGET``: share target word embeddings and softmax weights\n   * ``ALL``: share words embeddings and softmax weights\n  """"""\n  NONE = 0\n  SOURCE_TARGET_INPUT = 1\n  TARGET = 2\n  ALL = 3\n\n  @staticmethod\n  def share_input_embeddings(level):\n    """"""Returns ``True`` if input embeddings should be shared at :obj:`level`.""""""\n    return level in (EmbeddingsSharingLevel.SOURCE_TARGET_INPUT, EmbeddingsSharingLevel.ALL)\n\n  @staticmethod\n  def share_target_embeddings(level):\n    """"""Returns ``True`` if target embeddings should be shared at :obj:`level`.""""""\n    return level in (EmbeddingsSharingLevel.TARGET, EmbeddingsSharingLevel.ALL)\n\n\nclass SequenceToSequence(model.SequenceGenerator):\n  """"""A sequence to sequence model.""""""\n\n  def __init__(self,\n               source_inputter,\n               target_inputter,\n               encoder,\n               decoder,\n               share_embeddings=EmbeddingsSharingLevel.NONE):\n    """"""Initializes a sequence-to-sequence model.\n\n    Args:\n      source_inputter: A :class:`opennmt.inputters.Inputter` to process\n        the source data.\n      target_inputter: A :class:`opennmt.inputters.Inputter` to process\n        the target data. Currently, only the\n        :class:`opennmt.inputters.WordEmbedder` is supported.\n      encoder: A :class:`opennmt.encoders.Encoder` to encode the source.\n      decoder: A :class:`opennmt.decoders.Decoder` to decode the target.\n      share_embeddings: Level of embeddings sharing, see\n        :class:`opennmt.models.EmbeddingsSharingLevel`\n        for possible values.\n\n    Raises:\n      TypeError: if :obj:`target_inputter` is not a\n        :class:`opennmt.inputters.WordEmbedder` (same for\n        :obj:`source_inputter` when embeddings sharing is enabled) or if\n        :obj:`source_inputter` and :obj:`target_inputter` do not have the same\n        ``dtype``.\n    """"""\n    if not isinstance(target_inputter, inputters.WordEmbedder):\n      raise TypeError(""Target inputter must be a WordEmbedder"")\n    if EmbeddingsSharingLevel.share_input_embeddings(share_embeddings):\n      if isinstance(source_inputter, inputters.ParallelInputter):\n        source_inputters = source_inputter.inputters\n      else:\n        source_inputters = [source_inputter]\n      for inputter in source_inputters:\n        if not isinstance(inputter, inputters.WordEmbedder):\n          raise TypeError(""Sharing embeddings requires all inputters to be a ""\n                          ""WordEmbedder"")\n\n    examples_inputter = SequenceToSequenceInputter(\n        source_inputter,\n        target_inputter,\n        share_parameters=EmbeddingsSharingLevel.share_input_embeddings(share_embeddings))\n    super(SequenceToSequence, self).__init__(examples_inputter)\n    self.encoder = encoder\n    self.decoder = decoder\n    self.share_embeddings = share_embeddings\n\n  def auto_config(self, num_replicas=1):\n    config = super(SequenceToSequence, self).auto_config(num_replicas=num_replicas)\n    return misc.merge_dict(config, {\n        ""params"": {\n            ""beam_width"": 4\n        },\n        ""train"": {\n            ""sample_buffer_size"": -1,\n            ""max_step"": 500000\n        },\n        ""infer"": {\n            ""batch_size"": 32,\n            ""length_bucket_width"": 5\n        }\n    })\n\n  def map_v1_weights(self, weights):\n    if not isinstance(self.features_inputter, inputters.WordEmbedder):\n      raise ValueError(""Can not restore V1 model with multi features or multi source inputs"")\n    weights = weights[""seq2seq""]\n    m = []\n    m += self.features_inputter.map_v1_weights(weights[""encoder""])\n    m += self.labels_inputter.map_v1_weights(weights[""decoder""])\n    m += self.encoder.map_v1_weights(weights[""encoder""])\n    m += self.decoder.map_v1_weights(weights[""decoder""])\n    return m\n\n  def initialize(self, data_config, params=None):\n    super(SequenceToSequence, self).initialize(data_config, params=params)\n    self.decoder.initialize(vocab_size=self.labels_inputter.vocabulary_size)\n    if self.params.get(""contrastive_learning""):\n      # Use the simplest and most effective CL_one from the paper.\n      # https://www.aclweb.org/anthology/P19-1623\n      noiser = noise.WordNoiser(\n          noises=[noise.WordOmission(1)],\n          subword_token=self.params.get(""decoding_subword_token"", ""\xef\xbf\xad""),\n          is_spacer=self.params.get(""decoding_subword_token_is_spacer""))\n      self.labels_inputter.set_noise(noiser, in_place=False)\n\n  def build(self, input_shape):\n    super(SequenceToSequence, self).build(input_shape)\n    if EmbeddingsSharingLevel.share_target_embeddings(self.share_embeddings):\n      self.decoder.reuse_embeddings(self.labels_inputter.embedding)\n\n  def call(self, features, labels=None, training=None, step=None):\n    # Encode the source.\n    source_length = self.features_inputter.get_length(features)\n    source_inputs = self.features_inputter(features, training=training)\n    encoder_outputs, encoder_state, encoder_sequence_length = self.encoder(\n        source_inputs, sequence_length=source_length, training=training)\n\n    outputs = None\n    predictions = None\n\n    # When a target is provided, compute the decoder outputs for it.\n    if labels is not None:\n      outputs = self._decode_target(\n          labels,\n          encoder_outputs,\n          encoder_state,\n          encoder_sequence_length,\n          step=step,\n          training=training)\n\n    # When not in training, also compute the model predictions.\n    if not training:\n      predictions = self._dynamic_decode(\n          features,\n          encoder_outputs,\n          encoder_state,\n          encoder_sequence_length)\n\n    return outputs, predictions\n\n  def _decode_target(self,\n                     labels,\n                     encoder_outputs,\n                     encoder_state,\n                     encoder_sequence_length,\n                     step=None,\n                     training=None):\n    params = self.params\n    target_inputs = self.labels_inputter(labels, training=training)\n    input_fn = lambda ids: self.labels_inputter({""ids"": ids}, training=training)\n\n    sampling_probability = None\n    if training:\n      sampling_probability = decoder_util.get_sampling_probability(\n          step,\n          read_probability=params.get(""scheduled_sampling_read_probability""),\n          schedule_type=params.get(""scheduled_sampling_type""),\n          k=params.get(""scheduled_sampling_k""))\n\n    initial_state = self.decoder.initial_state(\n        memory=encoder_outputs,\n        memory_sequence_length=encoder_sequence_length,\n        initial_state=encoder_state)\n    logits, _, attention = self.decoder(\n        target_inputs,\n        self.labels_inputter.get_length(labels),\n        state=initial_state,\n        input_fn=input_fn,\n        sampling_probability=sampling_probability,\n        training=training)\n    outputs = dict(logits=logits, attention=attention)\n\n    noisy_ids = labels.get(""noisy_ids"")\n    if noisy_ids is not None and params.get(""contrastive_learning""):\n      # In case of contrastive learning, also forward the erroneous\n      # translation to compute its log likelihood later.\n      noisy_inputs = self.labels_inputter({""ids"": noisy_ids}, training=training)\n      noisy_logits, _, _ = self.decoder(\n          noisy_inputs,\n          labels[""noisy_length""],\n          state=initial_state,\n          input_fn=input_fn,\n          sampling_probability=sampling_probability,\n          training=training)\n      outputs[""noisy_logits""] = noisy_logits\n    return outputs\n\n  def _dynamic_decode(self, features, encoder_outputs, encoder_state, encoder_sequence_length):\n    params = self.params\n    batch_size = tf.shape(tf.nest.flatten(encoder_outputs)[0])[0]\n    start_ids = tf.fill([batch_size], constants.START_OF_SENTENCE_ID)\n    beam_size = params.get(""beam_width"", 1)\n\n    if beam_size > 1:\n      # Tile encoder outputs to prepare for beam search.\n      encoder_outputs = tfa.seq2seq.tile_batch(encoder_outputs, beam_size)\n      encoder_sequence_length = tfa.seq2seq.tile_batch(encoder_sequence_length, beam_size)\n      encoder_state = tf.nest.map_structure(\n          lambda state: tfa.seq2seq.tile_batch(state, beam_size) if state is not None else None,\n          encoder_state)\n\n    # Dynamically decodes from the encoder outputs.\n    initial_state = self.decoder.initial_state(\n        memory=encoder_outputs,\n        memory_sequence_length=encoder_sequence_length,\n        initial_state=encoder_state)\n    sampled_ids, sampled_length, log_probs, alignment, _ = self.decoder.dynamic_decode(\n        self.labels_inputter,\n        start_ids,\n        initial_state=initial_state,\n        decoding_strategy=decoding.DecodingStrategy.from_params(params),\n        sampler=decoding.Sampler.from_params(params),\n        maximum_iterations=params.get(""maximum_decoding_length"", 250),\n        minimum_iterations=params.get(""minimum_decoding_length"", 0))\n    target_tokens = self.labels_inputter.ids_to_tokens.lookup(tf.cast(sampled_ids, tf.int64))\n\n    # Maybe replace unknown targets by the source tokens with the highest attention weight.\n    if params.get(""replace_unknown_target"", False):\n      if alignment is None:\n        raise TypeError(""replace_unknown_target is not compatible with decoders ""\n                        ""that don\'t return alignment history"")\n      if not isinstance(self.features_inputter, inputters.WordEmbedder):\n        raise TypeError(""replace_unknown_target is only defined when the source ""\n                        ""inputter is a WordEmbedder"")\n      source_tokens = features[""tokens""]\n      if beam_size > 1:\n        source_tokens = tfa.seq2seq.tile_batch(source_tokens, beam_size)\n      # Merge batch and beam dimensions.\n      original_shape = tf.shape(target_tokens)\n      target_tokens = tf.reshape(target_tokens, [-1, original_shape[-1]])\n      align_shape = misc.shape_list(alignment)\n      attention = tf.reshape(\n          alignment, [align_shape[0] * align_shape[1], align_shape[2], align_shape[3]])\n      # We don\'t have attention for </s> but ensure that the attention time dimension matches\n      # the tokens time dimension.\n      attention = reducer.align_in_time(attention, tf.shape(target_tokens)[1])\n      replaced_target_tokens = replace_unknown_target(target_tokens, source_tokens, attention)\n      target_tokens = tf.reshape(replaced_target_tokens, original_shape)\n\n    # Maybe add noise to the predictions.\n    decoding_noise = params.get(""decoding_noise"")\n    if decoding_noise:\n      target_tokens, sampled_length = _add_noise(\n          target_tokens,\n          sampled_length,\n          decoding_noise,\n          params.get(""decoding_subword_token"", ""\xef\xbf\xad""),\n          params.get(""decoding_subword_token_is_spacer""))\n      alignment = None  # Invalidate alignments.\n\n    predictions = {""log_probs"": log_probs}\n    if self.labels_inputter.tokenizer.in_graph:\n      detokenized_text = self.labels_inputter.tokenizer.detokenize(\n          tf.reshape(target_tokens, [batch_size * beam_size, -1]),\n          sequence_length=tf.reshape(sampled_length, [batch_size * beam_size]))\n      predictions[""text""] = tf.reshape(detokenized_text, [batch_size, beam_size])\n    else:\n      predictions[""tokens""] = target_tokens\n      predictions[""length""] = sampled_length\n      if alignment is not None:\n        predictions[""alignment""] = alignment\n\n    # Maybe restrict the number of returned hypotheses based on the user parameter.\n    num_hypotheses = params.get(""num_hypotheses"", 1)\n    if num_hypotheses > 0:\n      if num_hypotheses > beam_size:\n        raise ValueError(""n_best cannot be greater than beam_width"")\n      for key, value in predictions.items():\n        predictions[key] = value[:, :num_hypotheses]\n    return predictions\n\n  def compute_loss(self, outputs, labels, training=True):\n    params = self.params\n    if not isinstance(outputs, dict):\n      outputs = dict(logits=outputs)\n    logits = outputs[""logits""]\n    noisy_logits = outputs.get(""noisy_logits"")\n    attention = outputs.get(""attention"")\n    if noisy_logits is not None and params.get(""contrastive_learning""):\n      return losses.max_margin_loss(\n          logits,\n          labels[""ids_out""],\n          labels[""length""],\n          noisy_logits,\n          labels[""noisy_ids_out""],\n          labels[""noisy_length""],\n          eta=params.get(""max_margin_eta"", 0.1))\n    loss, loss_normalizer, loss_token_normalizer = losses.cross_entropy_sequence_loss(\n        logits,\n        labels[""ids_out""],\n        labels[""length""],\n        label_smoothing=params.get(""label_smoothing"", 0.0),\n        average_in_time=params.get(""average_loss_in_time"", False),\n        training=training)\n    if training:\n      gold_alignments = labels.get(""alignment"")\n      guided_alignment_type = params.get(""guided_alignment_type"")\n      if gold_alignments is not None and guided_alignment_type is not None:\n        if attention is None:\n          tf.get_logger().warning(""This model did not return attention vectors; ""\n                                  ""guided alignment will not be applied"")\n        else:\n          loss += losses.guided_alignment_cost(\n              attention[:, :-1],  # Do not constrain last timestep.\n              gold_alignments,\n              sequence_length=self.labels_inputter.get_length(labels, ignore_special_tokens=True),\n              cost_type=guided_alignment_type,\n              weight=params.get(""guided_alignment_weight"", 1))\n    return loss, loss_normalizer, loss_token_normalizer\n\n  def print_prediction(self, prediction, params=None, stream=None):\n    if params is None:\n      params = {}\n    with_scores = params.get(""with_scores"")\n    alignment_type = params.get(""with_alignments"")\n    if alignment_type and ""alignment"" not in prediction:\n      raise ValueError(""with_alignments is set but the model did not return alignment information"")\n    num_hypotheses = len(prediction[""log_probs""])\n    for i in range(num_hypotheses):\n      if ""tokens"" in prediction:\n        target_length = prediction[""length""][i]\n        tokens = prediction[""tokens""][i][:target_length]\n        sentence = self.labels_inputter.tokenizer.detokenize(tokens)\n      else:\n        sentence = prediction[""text""][i]\n      score = None\n      attention = None\n      if with_scores:\n        score = prediction[""log_probs""][i]\n      if alignment_type:\n        attention = prediction[""alignment""][i][:target_length]\n      sentence = misc.format_translation_output(\n          sentence,\n          score=score,\n          attention=attention,\n          alignment_type=alignment_type)\n      misc.print_as_bytes(sentence, stream=stream)\n\n  def transfer_weights(self, new_model, new_optimizer=None, optimizer=None, ignore_weights=None):\n    updated_variables = []\n\n    def _map_variables(inputter_fn, vars_fn):\n      mapping, _ = vocab.get_mapping(\n          inputter_fn(self).vocabulary_file,\n          inputter_fn(new_model).vocabulary_file)\n      vars_a, vocab_axes = vars_fn(self)\n      vars_b, _ = vars_fn(new_model)\n      for var_a, var_b, vocab_axis in zip(vars_a, vars_b, vocab_axes):\n        if new_optimizer is not None and optimizer is not None:\n          variables = vocab.update_variable_and_slots(\n              var_a,\n              var_b,\n              optimizer,\n              new_optimizer,\n              mapping,\n              vocab_axis=vocab_axis)\n        else:\n          variables = [vocab.update_variable(var_a, var_b, mapping, vocab_axis=vocab_axis)]\n        updated_variables.extend(variables)\n      return vars_b\n\n    _map_variables(\n        lambda model: model.features_inputter,\n        lambda model: ([model.features_inputter.embedding], [0]))\n    _map_variables(\n        lambda model: model.labels_inputter,\n        lambda model: ([\n            model.labels_inputter.embedding,\n            model.decoder.output_layer.kernel,\n            model.decoder.output_layer.bias], [0, 1, 0]))\n\n    return super(SequenceToSequence, self).transfer_weights(\n        new_model,\n        new_optimizer=new_optimizer,\n        optimizer=optimizer,\n        ignore_weights=updated_variables)\n\n\nclass SequenceToSequenceInputter(inputters.ExampleInputter):\n  """"""A custom :class:`opennmt.inputters.ExampleInputter` for sequence to\n  sequence models.\n  """"""\n\n  def __init__(self,\n               features_inputter,\n               labels_inputter,\n               share_parameters=False):\n    super(SequenceToSequenceInputter, self).__init__(\n        features_inputter, labels_inputter, share_parameters=share_parameters)\n    labels_inputter.set_decoder_mode(mark_start=True, mark_end=True)\n    self.alignment_file = None\n\n  def initialize(self, data_config, asset_prefix=""""):\n    super(SequenceToSequenceInputter, self).initialize(data_config, asset_prefix=asset_prefix)\n    self.alignment_file = data_config.get(""train_alignments"")\n\n  def make_dataset(self, data_file, training=None):\n    dataset = super(SequenceToSequenceInputter, self).make_dataset(\n        data_file, training=training)\n    if self.alignment_file is None or not training:\n      return dataset\n    if not isinstance(dataset, list):\n      return tf.data.Dataset.zip((dataset, tf.data.TextLineDataset(self.alignment_file)))\n    datasets = dataset\n    alignment_files = self.alignment_file\n    if not isinstance(alignment_files, list):\n      alignment_files = [alignment_files]\n    if len(alignment_files) != len(datasets):\n      raise ValueError(""%d alignment files were provided, but %d were expected to match the ""\n                       ""number of data files"" % (len(alignment_files), len(datasets)))\n    return [\n        tf.data.Dataset.zip((dataset, tf.data.TextLineDataset(alignment_file)))\n        for dataset, alignment_file in zip(datasets, alignment_files)]\n\n  def make_features(self, element=None, features=None, training=None):\n    if training and self.alignment_file is not None:\n      element, alignment = element\n    else:\n      alignment = None\n    features, labels = super(SequenceToSequenceInputter, self).make_features(\n        element=element, features=features, training=training)\n    if alignment is not None:\n      labels[""alignment""] = text.alignment_matrix_from_pharaoh(\n          alignment,\n          self.features_inputter.get_length(features, ignore_special_tokens=True),\n          self.labels_inputter.get_length(labels, ignore_special_tokens=True))\n    return features, labels\n\n\ndef align_tokens_from_attention(tokens, attention):\n  """"""Returns aligned tokens from the attention.\n\n  Args:\n    tokens: The tokens on which the attention is applied as a string\n      ``tf.Tensor`` of shape :math:`[B, T_s]`.\n    attention: The attention vector of shape :math:`[B, T_t, T_s]`.\n\n  Returns:\n    The aligned tokens as a string ``tf.Tensor`` of shape :math:`[B, T_t]`.\n  """"""\n  alignment = tf.argmax(attention, axis=-1, output_type=tf.int32)\n  return tf.gather(tokens, alignment, axis=1, batch_dims=1)\n\ndef replace_unknown_target(target_tokens,\n                           source_tokens,\n                           attention,\n                           unknown_token=constants.UNKNOWN_TOKEN):\n  """"""Replaces all target unknown tokens by the source token with the highest\n  attention.\n\n  Args:\n    target_tokens: A a string ``tf.Tensor`` of shape :math:`[B, T_t]`.\n    source_tokens: A a string ``tf.Tensor`` of shape :math:`[B, T_s]`.\n    attention: The attention vector of shape :math:`[B, T_t, T_s]`.\n    unknown_token: The target token to replace.\n\n  Returns:\n    A string ``tf.Tensor`` with the same shape and type as :obj:`target_tokens`\n    but will all instances of :obj:`unknown_token` replaced by the aligned source\n    token.\n  """"""\n  aligned_source_tokens = align_tokens_from_attention(source_tokens, attention)\n  return tf.where(\n      tf.equal(target_tokens, unknown_token),\n      x=aligned_source_tokens,\n      y=target_tokens)\n\ndef _add_noise(tokens, lengths, params, subword_token, is_spacer=None):\n  if not isinstance(params, list):\n    raise ValueError(""Expected a list of noise modules"")\n  noises = []\n  for module in params:\n    noise_type, args = next(iter(module.items()))\n    if not isinstance(args, list):\n      args = [args]\n    noise_type = noise_type.lower()\n    if noise_type == ""dropout"":\n      noise_class = noise.WordDropout\n    elif noise_type == ""replacement"":\n      noise_class = noise.WordReplacement\n    elif noise_type == ""permutation"":\n      noise_class = noise.WordPermutation\n    else:\n      raise ValueError(""Invalid noise type: %s"" % noise_type)\n    noises.append(noise_class(*args))\n  noiser = noise.WordNoiser(noises=noises, subword_token=subword_token, is_spacer=is_spacer)\n  return noiser(tokens, lengths, keep_shape=True)\n'"
opennmt/models/transformer.py,2,"b'""""""Define the Google\'s Transformer model.""""""\n\nimport tensorflow as tf\n\nfrom opennmt.models.sequence_to_sequence import SequenceToSequence, EmbeddingsSharingLevel\nfrom opennmt.encoders.encoder import ParallelEncoder\nfrom opennmt.encoders.self_attention_encoder import SelfAttentionEncoder\nfrom opennmt.decoders.self_attention_decoder import SelfAttentionDecoder\nfrom opennmt.layers.position import SinusoidalPositionEncoder\nfrom opennmt.utils.misc import merge_dict\n\n\nclass Transformer(SequenceToSequence):\n  """"""Attention-based sequence-to-sequence model as described in\n  https://arxiv.org/abs/1706.03762.\n  """"""\n\n  def __init__(self,\n               source_inputter,\n               target_inputter,\n               num_layers,\n               num_units,\n               num_heads,\n               ffn_inner_dim,\n               dropout=0.1,\n               attention_dropout=0.1,\n               ffn_dropout=0.1,\n               ffn_activation=tf.nn.relu,\n               position_encoder_class=SinusoidalPositionEncoder,\n               share_embeddings=EmbeddingsSharingLevel.NONE,\n               share_encoders=False,\n               maximum_relative_position=None):\n    """"""Initializes a Transformer model.\n\n    Args:\n      source_inputter: A :class:`opennmt.inputters.Inputter` to process\n        the source data. If this inputter returns parallel inputs, a multi\n        source Transformer architecture will be constructed.\n      target_inputter: A :class:`opennmt.inputters.Inputter` to process\n        the target data. Currently, only the\n        :class:`opennmt.inputters.WordEmbedder` is supported.\n      num_layers: The number of layers or a 2-tuple with the number of encoder\n        layers and decoder layers.\n      num_units: The number of hidden units.\n      num_heads: The number of heads in each self-attention layers.\n      ffn_inner_dim: The inner dimension of the feed forward layers.\n      dropout: The probability to drop units in each layer output.\n      attention_dropout: The probability to drop units from the attention.\n      ffn_dropout: The probability to drop units from the ReLU activation in\n        the feed forward layer.\n      ffn_activation: The activation function to apply between the two linear\n        transformations of the feed forward layer.\n      position_encoder_class: The :class:`opennmt.layers.PositionEncoder`\n        class to use for position encoding (or a callable that returns an\n        instance).\n      share_embeddings: Level of embeddings sharing, see\n        :class:`opennmt.models.EmbeddingsSharingLevel` for possible values.\n      share_encoders: In case of multi source architecture, whether to share the\n        separate encoders parameters or not.\n      maximum_relative_position: Maximum relative position representation\n        (from https://arxiv.org/abs/1803.02155).\n    """"""\n    if isinstance(num_layers, (list, tuple)):\n      num_encoder_layers, num_decoder_layers = num_layers\n    else:\n      num_encoder_layers, num_decoder_layers = num_layers, num_layers\n    encoders = [\n        SelfAttentionEncoder(\n            num_encoder_layers,\n            num_units=num_units,\n            num_heads=num_heads,\n            ffn_inner_dim=ffn_inner_dim,\n            dropout=dropout,\n            attention_dropout=attention_dropout,\n            ffn_dropout=ffn_dropout,\n            ffn_activation=ffn_activation,\n            position_encoder_class=position_encoder_class,\n            maximum_relative_position=maximum_relative_position)\n        for _ in range(source_inputter.num_outputs)]\n    if len(encoders) > 1:\n      encoder = ParallelEncoder(\n          encoders if not share_encoders else encoders[0],\n          outputs_reducer=None,\n          states_reducer=None)\n    else:\n      encoder = encoders[0]\n    decoder = SelfAttentionDecoder(\n        num_decoder_layers,\n        num_units=num_units,\n        num_heads=num_heads,\n        ffn_inner_dim=ffn_inner_dim,\n        dropout=dropout,\n        attention_dropout=attention_dropout,\n        ffn_dropout=ffn_dropout,\n        ffn_activation=ffn_activation,\n        position_encoder_class=position_encoder_class,\n        num_sources=source_inputter.num_outputs,\n        maximum_relative_position=maximum_relative_position)\n\n    self._num_units = num_units\n    self._num_encoder_layers = num_encoder_layers\n    self._num_decoder_layers = num_decoder_layers\n    self._num_heads = num_heads\n    self._with_relative_position = maximum_relative_position is not None\n    self._is_ct2_compatible = (\n        isinstance(encoder, SelfAttentionEncoder)\n        and ffn_activation is tf.nn.relu\n        and ((self._with_relative_position and position_encoder_class is None)\n             or (not self._with_relative_position\n                 and position_encoder_class == SinusoidalPositionEncoder)))\n    super(Transformer, self).__init__(\n        source_inputter,\n        target_inputter,\n        encoder,\n        decoder,\n        share_embeddings=share_embeddings)\n\n  @property\n  def ctranslate2_spec(self):\n    if not self._is_ct2_compatible:\n      return None\n    import ctranslate2  # pylint: disable=import-outside-toplevel\n    return ctranslate2.specs.TransformerSpec(\n        (self._num_encoder_layers, self._num_decoder_layers),\n        self._num_heads,\n        with_relative_position=self._with_relative_position)\n\n  def auto_config(self, num_replicas=1):\n    config = super(Transformer, self).auto_config(num_replicas=num_replicas)\n    return merge_dict(config, {\n        ""params"": {\n            ""average_loss_in_time"": True,\n            ""label_smoothing"": 0.1,\n            ""optimizer"": ""LazyAdam"",\n            ""optimizer_params"": {\n                ""beta_1"": 0.9,\n                ""beta_2"": 0.998\n            },\n            ""learning_rate"": 2.0,\n            ""decay_type"": ""NoamDecay"",\n            ""decay_params"": {\n                ""model_dim"": self._num_units,\n                ""warmup_steps"": 8000\n            }\n        },\n        ""train"": {\n            ""effective_batch_size"": 25000,\n            ""batch_size"": 3072,\n            ""batch_type"": ""tokens"",\n            ""maximum_features_length"": 100,\n            ""maximum_labels_length"": 100,\n            ""keep_checkpoint_max"": 8,\n            ""average_last_checkpoints"": 8\n        }\n    })\n\n  def map_v1_weights(self, weights):\n    weights[""seq2seq""] = weights.pop(""transformer"")\n    return super(Transformer, self).map_v1_weights(weights)\n'"
opennmt/optimizers/__init__.py,0,"b'""""""Module defining custom optimizers.""""""\n\nfrom opennmt.optimizers.utils import make_optimizer\nfrom opennmt.optimizers.utils import register_optimizer\n'"
opennmt/optimizers/utils.py,13,"b'""""""Optimization utilities.""""""\n\nimport inspect\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow_addons.optimizers.weight_decay_optimizers import DecoupledWeightDecayExtension\n\nfrom opennmt.utils import misc\n\n\n_OPTIMIZERS_REGISTRY = misc.ClassRegistry(base_class=tf.keras.optimizers.Optimizer)\n\nregister_optimizer = _OPTIMIZERS_REGISTRY.register  # pylint: disable=invalid-name\n\ndef get_optimizer_class(name):\n  """"""Returns the optimizer class.\n\n  Args:\n    name: The optimizer name.\n\n  Returns:\n    A class extending ``tf.keras.optimizers.Optimizer``.\n\n  Raises:\n    ValueError: if :obj:`name` can not be resolved to an optimizer class.\n  """"""\n  optimizer_class = None\n  if optimizer_class is None:\n    optimizer_class = getattr(tf.keras.optimizers, name, None)\n  if optimizer_class is None:\n    optimizer_class = getattr(tfa.optimizers, name, None)\n  if optimizer_class is None:\n    optimizer_class = _OPTIMIZERS_REGISTRY.get(name)\n  if optimizer_class is None:\n    raise ValueError(""Unknown optimizer class: %s"" % name)\n  return optimizer_class\n\ndef make_optimizer(name, learning_rate, **kwargs):\n  """"""Creates the optimizer.\n\n  Args:\n    name: The name of the optimizer class in ``tf.keras.optimizers`` or\n      ``tfa.optimizers`` as a string.\n    learning_rate: The learning rate or learning rate schedule to use.\n    **kwargs: Additional optimizer arguments. If ``weight_decay`` is set, the\n      optimizer will be extended with decoupled weight decay.\n\n  Returns:\n    A ``tf.keras.optimizers.Optimizer`` instance.\n\n  Raises:\n    ValueError: if :obj:`name` can not be resolved to an optimizer class.\n  """"""\n  optimizer_class = get_optimizer_class(name)\n  if ""weight_decay"" in kwargs:\n    if DecoupledWeightDecayExtension not in inspect.getmro(optimizer_class):\n      optimizer_class = tfa.optimizers.extend_with_decoupled_weight_decay(optimizer_class)\n  optimizer = optimizer_class(learning_rate=learning_rate, **kwargs)\n  return optimizer\n\n\nclass GradientAccumulator(object):\n  """"""Gradient accumulation utility.\n\n  When used with a distribution strategy, the accumulator should be called in a\n  replica context. Gradients will be accumulated locally on each replica and\n  without synchronization. Users should then call ``.gradients``, scale the\n  gradients if required, and pass the result to ``apply_gradients``.\n  """"""\n\n  # We use the ON_READ synchronization policy so that no synchronization is\n  # performed on assignment. To get the value, we call .value() which returns the\n  # value on the current replica without synchronization.\n\n  def __init__(self):\n    """"""Initializes the accumulator.""""""\n    self._gradients = []\n    self._accum_steps = None\n\n  @property\n  def step(self):\n    """"""Number of accumulated steps.""""""\n    if self._accum_steps is None:\n      self._accum_steps = tf.Variable(\n          tf.constant(0, dtype=tf.int64),\n          trainable=False,\n          synchronization=tf.VariableSynchronization.ON_READ,\n          aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n    return self._accum_steps.value()\n\n  @property\n  def gradients(self):\n    """"""The accumulated gradients on the current replica.""""""\n    if not self._gradients:\n      raise ValueError(""The accumulator should be called first to initialize the gradients"")\n    return list(gradient.value() for gradient in self._gradients)\n\n  def __call__(self, gradients):\n    """"""Accumulates :obj:`gradients` on the current replica.""""""\n    if not self._gradients:\n      _ = self.step  # Create the step variable.\n      self._gradients.extend([\n          tf.Variable(\n              tf.zeros_like(gradient),\n              trainable=False,\n              synchronization=tf.VariableSynchronization.ON_READ)\n          for gradient in gradients])\n    if len(gradients) != len(self._gradients):\n      raise ValueError(""Expected %s gradients, but got %d"" % (\n          len(self._gradients), len(gradients)))\n\n    for accum_gradient, gradient in zip(self._gradients, gradients):\n      accum_gradient.assign_add(gradient)\n    self._accum_steps.assign_add(1)\n\n  def reset(self):\n    """"""Resets the accumulated gradients on the current replica.""""""\n    if not self._gradients:\n      return\n    self._accum_steps.assign(0)\n    for gradient in self._gradients:\n      gradient.assign(tf.zeros(gradient.shape, dtype=gradient.dtype))\n'"
opennmt/schedules/__init__.py,0,"b'""""""Module defining learning rate schedules.""""""\n\nfrom opennmt.schedules.lr_schedules import CosineAnnealing\nfrom opennmt.schedules.lr_schedules import NoamDecay\nfrom opennmt.schedules.lr_schedules import RNMTPlusDecay\nfrom opennmt.schedules.lr_schedules import RsqrtDecay\nfrom opennmt.schedules.lr_schedules import ScheduleWrapper\nfrom opennmt.schedules.lr_schedules import make_learning_rate_schedule\nfrom opennmt.schedules.lr_schedules import register_learning_rate_schedule\n'"
opennmt/schedules/lr_schedules.py,40,"b'""""""Define learning rate decay functions.""""""\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.utils import misc\n\n\n_LR_SCHEDULES_REGISTRY = misc.ClassRegistry(\n    base_class=tf.keras.optimizers.schedules.LearningRateSchedule)\n\nregister_learning_rate_schedule = _LR_SCHEDULES_REGISTRY.register  # pylint: disable=invalid-name\n\ndef get_lr_schedule_class(name):\n  """"""Returns the learning rate schedule class.\n\n  Args:\n    name: The schedule class name.\n\n  Returns:\n    A class extending ``tf.keras.optimizers.schedules.LearningRateSchedule``.\n\n  Raises:\n    ValueError: if :obj:`name` can not be resolved to an existing schedule.\n  """"""\n  schedule_class = None\n  if schedule_class is None:\n    schedule_class = getattr(tf.keras.optimizers.schedules, name, None)\n  if schedule_class is None:\n    schedule_class = _LR_SCHEDULES_REGISTRY.get(name)\n  if schedule_class is None:\n    raise ValueError(""Unknown learning rate schedule: %s"" % name)\n  return schedule_class\n\ndef make_learning_rate_schedule(initial_learning_rate,\n                                schedule_type,\n                                schedule_params=None,\n                                schedule_step_duration=1,\n                                start_step=0,\n                                minimum_learning_rate=0):\n  """"""Creates the learning rate schedule.\n\n  Args:\n    initial_learning_rate: The initial learning rate value or scale.\n    schedule_type: The type of decay. A function from\n      ``tf.keras.optimizers.schedules``\n      or :mod:`opennmt.schedules` as a string.\n    schedule_params: Additional parameters for the decay function.\n    schedule_step_duration: The number of training steps that make 1 decay step.\n    start_step: Start the schedule after this many steps.\n    minimum_learning_rate: Do not decay past this learning rate value.\n\n  Returns:\n    A ``tf.keras.optimizers.schedules.LearningRateSchedule`` instance.\n\n  Raises:\n    ValueError: if :obj:`schedule_type` can not be resolved to an existing\n      schedule.\n\n  See Also:\n    :class:`opennmt.schedules.ScheduleWrapper`\n  """"""\n  if schedule_params is None:\n    schedule_params = {}\n  schedule_class = get_lr_schedule_class(schedule_type)\n  schedule = schedule_class(initial_learning_rate, **schedule_params)\n  schedule = ScheduleWrapper(\n      schedule,\n      step_start=start_step,\n      step_duration=schedule_step_duration,\n      minimum_learning_rate=minimum_learning_rate)\n  return schedule\n\n\nclass ScheduleWrapper(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Wrapper to augment a learning rate scheduler behavior.""""""\n\n  def __init__(self,\n               schedule,\n               step_start=0,\n               step_duration=1,\n               minimum_learning_rate=0):\n    """"""Initializes the decay function.\n\n    Args:\n      schedule: A ``tf.keras.optimizers.schedules.LearningRateSchedule``.\n      step_duration: The number of training steps that make 1 decay step.\n      start_step: Start decay after this many steps.\n      minimum_learning_rate: Do not decay past this learning rate value.\n\n    See Also:\n      :class:`opennmt.schedules.make_learning_rate_schedule`\n    """"""\n    self.schedule = schedule\n    self.step_start = step_start\n    self.step_duration = step_duration\n    self.minimum_learning_rate = minimum_learning_rate\n\n  def __call__(self, step):\n    # Map the training step to a decay step.\n    step = tf.maximum(step - self.step_start, 0)\n    step //= self.step_duration\n    learning_rate = self.schedule(step)\n    return tf.maximum(learning_rate, self.minimum_learning_rate)\n\n\n@register_learning_rate_schedule\nclass NoamDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Defines the decay function described in https://arxiv.org/abs/1706.03762.""""""\n\n  def __init__(self, scale, model_dim, warmup_steps):\n    """"""Initializes the decay function.\n\n    Args:\n      scale: The scale constant.\n      model_dim: The model dimension.\n      warmup_steps: The number of warmup steps.\n    """"""\n    self.scale = tf.cast(scale, tf.float32)\n    self.model_dim = tf.cast(model_dim, tf.float32)\n    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n\n  def __call__(self, step):\n    step = tf.cast(step + 1, tf.float32)\n    return (self.scale\n            * tf.pow(self.model_dim, -0.5)\n            * tf.minimum(tf.pow(step, -0.5), step * tf.pow(self.warmup_steps, -1.5)))\n\n\n@register_learning_rate_schedule\nclass RsqrtDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Decay based on the reciprocal of the step square root.""""""\n\n  def __init__(self, scale, warmup_steps):\n    """"""Initializes the decay function.\n\n    Args:\n      scale: The scale constant.\n      warmup_steps: The number of warmup steps.\n    """"""\n    self.scale = tf.cast(scale, tf.float32)\n    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n\n  def __call__(self, step):\n    step = tf.cast(step, tf.float32)\n    return self.scale * tf.math.rsqrt(tf.maximum(step, self.warmup_steps))\n\n\n@register_learning_rate_schedule\nclass CosineAnnealing(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Decay using a cosine annealing schedule.""""""\n\n  def __init__(self, eta_max, eta_min=0, max_step=1000000, warmup_steps=None):\n    """"""Initializes the decay function.\n\n    Args:\n      eta_max: Maximum learning rate.\n      eta_min: Minimum learning rate.\n      max_step: The last step of the scedule.\n      warmup_steps: The number of steps to increment the learning rate linearly\n        from 0 to :obj:`scale` before annealing.\n    """"""\n    self.eta_max = tf.cast(eta_max, tf.float32)\n    self.eta_min = tf.cast(eta_min, tf.float32)\n    self.max_step = tf.cast(max_step, tf.float32)\n    self.warmup_steps = tf.cast(warmup_steps, tf.float32) if warmup_steps is not None else None\n\n  def __call__(self, step):\n    step = tf.cast(step, tf.float32)\n    annealing = lambda: (\n        self.eta_min\n        + 0.5 * (self.eta_max - self.eta_min) * (1 + tf.cos(np.pi * step / self.max_step)))\n    linear = lambda: self.eta_max * step / tf.cast(self.warmup_steps, tf.float32)\n    if self.warmup_steps is None:\n      return annealing()\n    return tf.cond(tf.less(step, self.warmup_steps), true_fn=linear, false_fn=annealing)\n\n\n@register_learning_rate_schedule\nclass RNMTPlusDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n  """"""Defines the decay function described in https://arxiv.org/abs/1804.09849.""""""\n\n  def __init__(self,\n               scale,\n               num_replicas,\n               warmup_steps=500,\n               start_step=600000,\n               end_step=1200000):\n    """"""Initializes the decay function.\n\n    Args:\n      scale: The scale constant.\n      num_replicas: The number of concurrent model replicas.\n      warmup_steps: The number of warmup steps.\n      start_step: The start step of the exponential decay.\n      end_step: The end step of the exponential decay.\n    """"""\n    self.scale = tf.cast(scale, tf.float32)\n    self.num_replicas = tf.cast(num_replicas, tf.float32)\n    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n    self.start_step = tf.cast(start_step, tf.float32)\n    self.end_step = tf.cast(end_step, tf.float32)\n\n  def __call__(self, step):\n    t = tf.cast(step, tf.float32)\n    n = self.num_replicas\n    p = self.warmup_steps\n    s = self.start_step\n    e = self.end_step\n    return self.scale * tf.minimum(\n        tf.minimum(1 + (t * (n - 1)) / (n * p), n),\n        n * tf.pow(2 * n, (s - n * t) / (e - s)))\n'"
opennmt/tests/__init__.py,0,b''
opennmt/tests/api_test.py,2,"b'import inspect\nimport tensorflow as tf\nimport opennmt\n\n\nclass APITest(tf.test.TestCase):\n\n  def testSubmodules(self):\n    def _check(module):\n      self.assertTrue(inspect.ismodule(module))\n\n    _check(opennmt.data)\n    _check(opennmt.decoders)\n    _check(opennmt.encoders)\n    _check(opennmt.inputters)\n    _check(opennmt.layers)\n    _check(opennmt.models)\n    _check(opennmt.optimizers)\n    _check(opennmt.schedules)\n    _check(opennmt.tokenizers)\n    _check(opennmt.utils)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/bridge_test.py,3,"b'import tensorflow as tf\n\nfrom opennmt.layers import bridge\n\n\ndef _build_state(num_layers, num_units, batch_size):\n  return [\n      [tf.zeros([batch_size, num_units]), tf.zeros([batch_size, num_units])]\n      for _ in range(num_layers)]\n\n\nclass BridgeTest(tf.test.TestCase):\n\n  def testZeroBridge(self):\n    encoder_state = _build_state(4, 20, 6)\n    decoder_state = _build_state(3, 60, 6)\n    state = bridge.ZeroBridge()(encoder_state, decoder_state)\n    self.assertAllEqual(decoder_state, state)\n\n  def testCopyBridge(self):\n    encoder_state = _build_state(3, 20, 6)\n    decoder_state = _build_state(3, 20, 6)\n    state = bridge.CopyBridge()(encoder_state, decoder_state)\n    self.assertAllEqual(encoder_state, state)\n\n  def testCopyBridgeLayerMismatch(self):\n    encoder_state = _build_state(3, 20, 6)\n    decoder_state = _build_state(4, 20, 6)\n    with self.assertRaises(ValueError):\n      _ = bridge.CopyBridge()(encoder_state, decoder_state)\n\n  def testCopyBridgeSizeMismatch(self):\n    encoder_state = _build_state(3, 20, 6)\n    decoder_state = _build_state(3, 30, 6)\n    with self.assertRaises(ValueError):\n      _ = bridge.CopyBridge()(encoder_state, decoder_state)\n\n  def testDenseBridge(self):\n    encoder_state = _build_state(3, 20, 6)\n    decoder_state = _build_state(4, 30, 6)\n    state = bridge.DenseBridge()(encoder_state, decoder_state)\n    bridge.assert_state_is_compatible(decoder_state, state)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/checkpoint_test.py,16,"b'import os\n\nimport tensorflow as tf\n\nfrom opennmt.utils import checkpoint as checkpoint_util\n\n\nclass _CustomDense(tf.keras.layers.Dense):\n\n  def add_weight(self, name, *args, **kwargs):\n    # This is to test the case where the variable name is different than the attribute name.\n    name += ""_1""\n    return super(_CustomDense, self).add_weight(name, *args, **kwargs)\n\nclass _DummyModel(tf.keras.layers.Layer):\n\n  def __init__(self):\n    super(_DummyModel, self).__init__()\n    self.layers = [tf.keras.layers.Dense(20), _CustomDense(20)]\n\n  def call(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n\n\nclass CheckpointTest(tf.test.TestCase):\n\n  def testLastSavedStep(self):\n    model = _DummyModel()\n    model(tf.random.uniform([4, 10]))\n    model_dir = os.path.join(self.get_temp_dir(), ""model"")\n    checkpoint = checkpoint_util.Checkpoint(model, model_dir=model_dir)\n    self.assertIsNone(checkpoint.last_saved_step)\n    checkpoint.save(10)\n    self.assertEqual(checkpoint.last_saved_step, 10)\n    checkpoint.save(20)\n    self.assertEqual(checkpoint.last_saved_step, 20)\n\n    # Property should not be bound to an instance.\n    checkpoint = checkpoint_util.Checkpoint(model, model_dir=model_dir)\n    self.assertEqual(checkpoint.last_saved_step, 20)\n\n  def testCheckpointAveraging(self):\n    model = _DummyModel()\n    optimizer = tf.keras.optimizers.Adam()\n\n    @tf.function\n    def _build_model():\n      x = tf.random.uniform([4, 10])\n      y = model(x)\n      loss = tf.reduce_mean(y)\n      gradients = optimizer.get_gradients(loss, model.trainable_variables)\n      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    def _assign_var(var, scalar):\n      var.assign(tf.ones_like(var) * scalar)\n\n    def _all_equal(var, scalar):\n      return tf.size(tf.where(tf.not_equal(var, scalar))).numpy() == 0\n\n    def _get_var_list(checkpoint_path):\n      return [name for name, _ in tf.train.list_variables(checkpoint_path)]\n\n    _build_model()\n\n    # Write some checkpoint with all variables set to the step value.\n    steps = [10, 20, 30, 40]\n    num_checkpoints = len(steps)\n    avg_value = sum(steps) / num_checkpoints\n    directory = os.path.join(self.get_temp_dir(), ""src"")\n    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n    checkpoint_manager = tf.train.CheckpointManager(\n        checkpoint, directory, max_to_keep=num_checkpoints)\n    for step in steps:\n      _assign_var(model.layers[0].kernel, step)\n      _assign_var(model.layers[0].bias, step)\n      checkpoint_manager.save(checkpoint_number=step)\n\n    output_dir = os.path.join(self.get_temp_dir(), ""dst"")\n    checkpoint_util.average_checkpoints(\n        directory, output_dir, dict(model=model, optimizer=optimizer))\n    avg_checkpoint = tf.train.latest_checkpoint(output_dir)\n    self.assertIsNotNone(avg_checkpoint)\n    self.assertEqual(checkpoint_util.get_step_from_checkpoint_prefix(avg_checkpoint), steps[-1])\n    checkpoint.restore(avg_checkpoint)\n    self.assertTrue(_all_equal(model.layers[0].kernel, avg_value))\n    self.assertTrue(_all_equal(model.layers[0].bias, avg_value))\n    self.assertListEqual(\n        _get_var_list(avg_checkpoint),\n        _get_var_list(checkpoint_manager.latest_checkpoint))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/common_layers_test.py,10,"b'from parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.layers import common\n\n\nclass CommonLayersTest(tf.test.TestCase):\n\n  @parameterized.expand([\n      ([5, 10], [3, 5], False),\n      ([10, 5], [3, 5], True),\n  ])\n  def testDense(self, weight_shape, input_shape, transpose):\n    weight = tf.zeros(weight_shape)\n    layer = common.Dense(10, weight=weight, transpose=transpose)\n    x = tf.ones(input_shape)\n    y = layer(x)\n    self.assertEqual(layer.kernel.ref(), weight.ref())\n    self.assertEqual(self.evaluate(tf.reduce_sum(y)), 0)\n\n  def testLayerNorm(self):\n    layer_norm = common.LayerNorm()\n    x = tf.random.uniform([4, 10])\n    y = layer_norm(x)\n    self.assertEqual(y.shape, x.shape)\n\n  def testLayerWrapper(self):\n    layer = common.LayerWrapper(tf.keras.layers.Dense(10))\n    x = tf.random.uniform([4, 5, 10])\n    y = layer(x)\n    self.assertEqual(y.shape, x.shape)\n\n  def testLayerWrapperInputOutputDepthMismatch(self):\n    layer = common.LayerWrapper(tf.keras.layers.Dense(10))\n    x = tf.random.uniform([4, 5, 5])\n    y = layer(x)\n    self.assertListEqual(y.shape.as_list(), [4, 5, 10])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/compat_test.py,2,"b'import tensorflow as tf\n\nfrom opennmt.utils import compat\n\n\nclass MiscTest(tf.test.TestCase):\n\n  def testTFSupports(self):\n    self.assertTrue(compat.tf_supports(""data""))\n    self.assertTrue(compat.tf_supports(""data.Dataset""))\n    self.assertFalse(compat.tf_supports(""data.UnknwonClass""))\n    self.assertFalse(compat.tf_supports(""unknown_module""))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/config_test.py,2,"b'import copy\nimport os\nimport filecmp\nimport yaml\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt import config\nfrom opennmt.models.model import Model\n\n\nclass ConfigTest(tf.test.TestCase):\n\n  def testConfigOverride(self):\n    config1 = {""model_dir"": ""foo"", ""train"": {""batch_size"": 32, ""steps"": 42}}\n    config2 = {""model_dir"": ""bar"", ""train"": {""batch_size"": 64}}\n    config_file_1 = os.path.join(self.get_temp_dir(), ""config1.yml"")\n    config_file_2 = os.path.join(self.get_temp_dir(), ""config2.yml"")\n\n    with open(config_file_1, mode=""w"") as config_file:\n      config_file.write(yaml.dump(config1))\n    with open(config_file_2, mode=""w"") as config_file:\n      config_file.write(yaml.dump(config2))\n\n    loaded_config = config.load_config([config_file_1, config_file_2])\n\n    self.assertDictEqual(\n        {""model_dir"": ""bar"", ""train"": {""batch_size"": 64, ""steps"": 42}},\n        loaded_config)\n\n  def _writeCustomModel(self, filename=""test_model.py"", return_value=42):\n    model_path = os.path.join(self.get_temp_dir(), filename)\n    with open(model_path, mode=""w"") as model_file:\n      model_file.write(""model = lambda: %d"" % return_value)\n    return model_path\n\n  def testLoadModelModule(self):\n    model_path = self._writeCustomModel()\n    model_module = config.load_model_module(model_path)\n    model = model_module.model()\n    self.assertEqual(42, model)\n\n  def testLoadModelFromCatalog(self):\n    model_name = ""Transformer""\n    model = config.load_model_from_catalog(model_name)\n    self.assertIsInstance(model, Model)\n\n  @parameterized.expand([\n      (""Transformer"", False),\n      (""TransformerBase"", True),\n  ])\n  def testLoadModel(self, model_name, as_builder):\n\n    def _check_model(model):\n      if as_builder:\n        self.assertTrue(model, callable)\n        model = model()\n      self.assertIsInstance(model, Model)\n\n    model_dir = self.get_temp_dir()\n    _check_model(config.load_model(model_dir, model_name=model_name, as_builder=as_builder))\n    self.assertTrue(os.path.exists(os.path.join(model_dir, ""model_description.py"")))\n    _check_model(config.load_model(model_dir, as_builder=as_builder))\n\n  def testLoadModelDescriptionCompat(self):\n    model_dir = self.get_temp_dir()\n    description = os.path.join(model_dir, ""model_description.py"")\n    with open(description, ""w"") as description_file:\n      description_file.write(""from opennmt.models import catalog\\n"")\n      description_file.write(""model = catalog.Transformer\\n"")\n    model = config.load_model(model_dir)\n    self.assertIsInstance(model, Model)\n\n  def testLoadModelFile(self):\n    model_file = self._writeCustomModel()\n    model_dir = self.get_temp_dir()\n    model = config.load_model(model_dir, model_file=model_file)\n    saved_description_path = os.path.join(model_dir, ""model_description.py"")\n    self.assertTrue(os.path.exists(saved_description_path))\n    self.assertTrue(filecmp.cmp(model_file, saved_description_path))\n    self.assertEqual(model, 42)\n    model = config.load_model(model_dir)\n    self.assertEqual(model, 42)\n\n  def testLoadModelFileOverride(self):\n    model_dir = self.get_temp_dir()\n    saved_description_path = os.path.join(model_dir, ""model_description.py"")\n    model_file = self._writeCustomModel(filename=""test_model1.py"", return_value=1)\n    model = config.load_model(model_dir, model_file=model_file)\n    self.assertTrue(filecmp.cmp(model_file, saved_description_path))\n    model_file = self._writeCustomModel(filename=""test_model2.py"", return_value=2)\n    model = config.load_model(model_dir, model_file=model_file)\n    self.assertTrue(filecmp.cmp(model_file, saved_description_path))\n\n  def testLoadModelInvalidArguments(self):\n    with self.assertRaises(ValueError):\n      config.load_model(self.get_temp_dir(), model_file=""a"", model_name=""b"")\n\n  def testLoadModelInvalidInvalidName(self):\n    with self.assertRaisesRegex(ValueError, ""does not exist""):\n      config.load_model(self.get_temp_dir(), model_name=""b"")\n\n  def testLoadModelInvalidInvalidFile(self):\n    with self.assertRaisesRegex(ValueError, ""not found""):\n      config.load_model(self.get_temp_dir(), model_file=""a"")\n\n  def testLoadModelMissingModel(self):\n    with self.assertRaises(RuntimeError):\n      config.load_model(self.get_temp_dir())\n\n  def testConvertToV2Config(self):\n    v1_config = {\n        ""data"": {\n            ""source_words_vocabulary"": ""a.txt"",\n            ""target_words_vocabulary"": ""b.txt"",\n        },\n        ""params"": {\n            ""optimizer"": ""LazyAdamOptimizer"",\n            ""optimizer_params"": {\n                ""beta1"": 0.9,\n                ""beta2"": 0.998,\n            },\n            ""param_init"": 0.1,\n            ""loss_scale"": 2,\n            ""horovod"": {},\n            ""maximum_learning_rate"": 4,\n            ""maximum_iterations"": 250,\n            ""clip_gradients"": 5,\n            ""weight_decay"": 0.1,\n            ""decay_step_duration"": 8,\n            ""gradients_accum"": 9,\n            ""decay_type"": ""noam_decay"",\n            ""decay_rate"": 512,\n            ""decay_steps"": 4000\n        },\n        ""train"": {\n            ""batch_size"": 64,\n            ""num_threads"": 4,\n            ""prefetch_buffer_size"": 1,\n            ""bucket_width"": 1,\n            ""train_steps"": 500000,\n            ""save_checkpoints_secs"": 600,\n        },\n        ""eval"": {\n            ""batch_size"": 32,\n            ""eval_delay"": 3600,\n            ""exporters"": ""best"",\n            ""num_threads"": 4,\n            ""prefetch_buffer_size"": 1,\n        },\n        ""infer"": {\n            ""batch_size"": 32,\n            ""num_threads"": 4,\n            ""prefetch_buffer_size"": 1,\n            ""bucket_width"": 1,\n        },\n        ""score"": {\n            ""num_threads"": 4,\n            ""prefetch_buffer_size"": 1,\n        },\n    }\n\n    expected_v2_config = {\n        ""data"": {\n            ""source_vocabulary"": ""a.txt"",\n            ""target_vocabulary"": ""b.txt"",\n        },\n        ""params"": {\n            ""optimizer"": ""LazyAdam"",\n            ""optimizer_params"": {\n                ""beta_1"": 0.9,\n                ""beta_2"": 0.998,\n                ""clipnorm"": 5,\n                ""weight_decay"": 0.1\n            },\n            ""decay_type"": ""NoamDecay"",\n            ""decay_params"": {\n                ""model_dim"": 512,\n                ""warmup_steps"": 4000\n            },\n            ""maximum_decoding_length"": 250,\n        },\n        ""train"": {\n            ""batch_size"": 64,\n            ""effective_batch_size"": 64 * max(8, 9),\n            ""length_bucket_width"": 1,\n            ""max_step"": 500000,\n        },\n        ""eval"": {\n            ""batch_size"": 32,\n        },\n        ""infer"": {\n            ""batch_size"": 32,\n            ""length_bucket_width"": 1,\n        },\n    }\n\n    original_v1_config = copy.deepcopy(v1_config)\n    v2_config = config.convert_to_v2_config(v1_config)\n    self.assertDictEqual(v2_config, expected_v2_config)\n    self.assertDictEqual(v1_config, original_v1_config)\n    self.assertDictEqual(config.convert_to_v2_config(v2_config), expected_v2_config)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/dataset_test.py,23,"b'import os\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.data import dataset as dataset_util\nfrom opennmt.tests import test_util\n\n\nclass DatasetTest(tf.test.TestCase):\n\n  @parameterized.expand([\n      [(4, 2), None, (4 / 6, 2 / 6)],\n      [(4, 2), (0.2, 0.4), (0.5, 0.5)],\n  ])\n  def testNormalizeWeights(self, sizes, weights, expected_weights):\n    datasets = list(map(tf.data.Dataset.range, sizes))\n    weights = dataset_util.normalize_weights(datasets, weights=weights)\n    for weight, expected_weight in zip(weights, expected_weights):\n      self.assertAllClose(weight, expected_weight)\n\n  @parameterized.expand([\n      [(1, 1, 1), None, (1/3, 1/3, 1/3)],\n      [(1, 2, 6), None, (1/9, 2/9, 6/9)],\n      [(1, 1, 1), (0.2, 0.7, 0.1), (0.2, 0.7, 0.1)],\n      [(1, 1, 1), (0.4, 1.4, 0.2), (0.2, 0.7, 0.1)],\n  ])\n  def testDatasetWeighting(self, sizes, weights, target_distribution):\n    datasets = [\n        tf.data.Dataset.from_tensor_slices([i] * size)\n        for i, size in enumerate(sizes)]\n    if weights is not None:\n      datasets = (datasets, weights)\n    dataset = dataset_util.training_pipeline(batch_size=20, shuffle_buffer_size=5000)(datasets)\n    counts = [0] * len(sizes)\n    # Check that after 2000 batches we are close to the target distribution.\n    for x in dataset.take(2000):\n      for i, _ in enumerate(counts):\n        counts[i] += int(tf.math.count_nonzero(x == i))\n    total_count = sum(counts)\n    for count, freq in zip(counts, target_distribution):\n      self.assertNear(count / total_count, freq, 0.05)\n\n  def testDatasetSize(self):\n    path = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""file.txt""),\n        list(map(str, range(15))))\n    dataset = tf.data.TextLineDataset(path)\n    size = dataset_util.get_dataset_size(dataset)\n    self.assertEqual(self.evaluate(size), 15)\n\n  def testDatasetSizeInfinite(self):\n    dataset = tf.data.Dataset.range(5).repeat()\n    self.assertIsNone(dataset_util.get_dataset_size(dataset))\n\n  def testIrregularBatches(self):\n    batch_size = 12\n    dataset = tf.data.Dataset.range(batch_size * 2 - 1)\n    dataset = dataset.map(lambda x: {""x"": x, ""y"": x + 1})\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.apply(dataset_util.filter_irregular_batches(batch_size))\n    iterator = iter(dataset)\n    single_element = next(iterator)\n    self.assertEqual(batch_size, single_element[""x""].shape[0])\n    with self.assertRaises(StopIteration):\n      next(iterator)\n\n  @parameterized.expand([\n      [11, 5, 15],\n      [10, 5, 10],\n      [5, 20, 20]\n  ])\n  def testMakeCardinalityMultipleOf(self, dataset_size, divisor, expected_size):\n    dataset = tf.data.Dataset.range(dataset_size)\n    dataset = dataset.apply(dataset_util.make_cardinality_multiple_of(divisor))\n    self.assertLen(list(iter(dataset)), expected_size)\n\n  def testRandomShard(self):\n    dataset_size = 42\n    shard_size = 3\n\n    dataset = tf.data.Dataset.range(dataset_size)\n    dataset = dataset.apply(dataset_util.random_shard(shard_size, dataset_size))\n    gather = list(iter(dataset))\n    self.assertAllEqual(list(range(dataset_size)), sorted(gather))\n\n  def _testFilterByLength(self,\n                          features_length,\n                          labels_length,\n                          maximum_features_length=None,\n                          maximum_labels_length=None,\n                          filtered=True):\n    dataset = tf.data.Dataset.zip((\n        tf.data.Dataset.from_tensors(tf.constant(features_length)),\n        tf.data.Dataset.from_tensors(tf.constant(labels_length))))\n    dataset = dataset.apply(dataset_util.filter_examples_by_length(\n        maximum_features_length=maximum_features_length,\n        maximum_labels_length=maximum_labels_length,\n        features_length_fn=lambda _: features_length,\n        labels_length_fn=lambda _: labels_length))\n\n    iterator = iter(dataset)\n    if filtered:\n      with self.assertRaises(StopIteration):\n        next(iterator)\n    else:\n      next(iterator)\n\n  def testFilterByLengthNoUpperBound(self):\n    self._testFilterByLength(1, 1, filtered=False)\n    self._testFilterByLength(0, 1, filtered=True)\n    self._testFilterByLength(1, 0, filtered=True)\n\n  def testFilterExamplesByLength(self):\n    self._testFilterByLength(\n        1, 1, maximum_features_length=1, maximum_labels_length=1, filtered=False)\n    self._testFilterByLength(\n        2, 1, maximum_features_length=1, maximum_labels_length=1, filtered=True)\n    self._testFilterByLength(\n        1, 2, maximum_features_length=1, maximum_labels_length=1, filtered=True)\n\n  def testFilterExamplesByLengthMultiSource(self):\n    self._testFilterByLength(\n        [1, 1], 1, maximum_features_length=1, maximum_labels_length=1, filtered=False)\n    self._testFilterByLength(\n        [1, 2], 1, maximum_features_length=1, maximum_labels_length=1, filtered=False)\n    self._testFilterByLength(\n        [1, 0], 1, maximum_features_length=1, maximum_labels_length=1, filtered=True)\n    self._testFilterByLength(\n        [1, 2], 1, maximum_features_length=[1, 1], maximum_labels_length=1, filtered=True)\n\n  def _testBatchTrainDataset(self, check_fn, batch_size, **kwargs):\n    num_examples = 1000\n    features = tf.random.normal([num_examples], mean=12, stddev=6, seed=42)\n    labels_diff = tf.random.normal([num_examples], mean=0, stddev=3, seed=42)\n    labels = features + labels_diff\n\n    features = tf.maximum(tf.cast(1, tf.int32), tf.cast(features, tf.int32))\n    labels = tf.maximum(tf.cast(1, tf.int32), tf.cast(labels, tf.int32))\n\n    dataset = tf.data.Dataset.zip((\n        tf.data.Dataset.from_tensor_slices(features),\n        tf.data.Dataset.from_tensor_slices(labels)))\n    dataset = dataset.apply(dataset_util.batch_sequence_dataset(\n        batch_size,\n        length_fn=[lambda x: x, lambda x: x],\n        **kwargs))\n\n    iterator = iter(dataset)\n    check_fn(iterator)\n\n  def testBatchTrainDatasetSimple(self):\n    def _check_fn(iterator):\n      features, labels = next(iterator)\n      self.assertEqual(64, features.shape[0])\n    self._testBatchTrainDataset(_check_fn, 64)\n\n  def testBatchTrainDatasetMultiplier(self):\n    def _check_fn(iterator):\n      features, labels = next(iterator)\n      self.assertEqual(30, features.shape[0])\n    self._testBatchTrainDataset(_check_fn, 10, batch_multiplier=3)\n\n  def testBatchTrainDatasetMultiple(self):\n    def _check_fn(iterator):\n      features, labels = next(iterator)\n      self.assertEqual(features.shape[0] % 3, 0)\n    self._testBatchTrainDataset(\n        _check_fn,\n        1024,\n        batch_type=""tokens"",\n        batch_size_multiple=3,\n        length_bucket_width=10)\n\n  def testBatchTrainDatasetBucket(self):\n    def _check_fn(iterator):\n      for _ in range(20):\n        features, labels = next(iterator)\n        length = [max(f, l) for f, l in zip(features, labels)]\n        self.assertGreater(3, max(length) - min(length))\n        self.assertGreaterEqual(64, features.shape[0])\n    self._testBatchTrainDataset(_check_fn, 64, length_bucket_width=3)\n\n  def testBatchTrainDatasetTokens(self):\n    def _check_fn(iterator):\n      for _ in range(20):\n        features, labels = next(iterator)\n        batch_size = features.shape[0]\n        max_length = max(list(features) + list(labels))\n        self.assertGreaterEqual(256, batch_size * max_length)\n    self._testBatchTrainDataset(_check_fn, 256, batch_type=""tokens"", length_bucket_width=1)\n\n  def testReorderInferDataset(self):\n    dataset = tf.data.Dataset.from_tensor_slices([8, 2, 5, 6, 7, 1, 3, 9])\n    dataset = dataset.map(lambda x: {""length"": x})\n    dataset = dataset.apply(dataset_util.inference_pipeline(\n        3, length_bucket_width=3, length_fn=lambda x: x[""length""]))\n    elements = list(iter(dataset))\n\n    def _check_element(element, length, index):\n      self.assertAllEqual(element[""length""], length)\n      self.assertAllEqual(element[""index""], index)\n\n    self.assertEqual(len(elements), 4)\n    _check_element(elements[0], [8, 6, 7], [0, 3, 4])\n    _check_element(elements[1], [2, 1], [1, 5])\n    _check_element(elements[2], [5, 3], [2, 6])\n    _check_element(elements[3], [9], [7])\n\n  def testFunctionOnNext(self):\n    dataset = tf.data.Dataset.range(5)\n\n    @dataset_util.function_on_next(dataset)\n    def _identity(next_fn):\n      return tf.identity(next_fn())\n\n    def _check_one_pass():\n      values = list(_identity())\n      self.assertEqual(len(values), 5)\n      for i, value in enumerate(values):\n        self.assertEqual(value, i)\n\n    _check_one_pass()\n    _check_one_pass()  # The function could be called again.\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/decoder_test.py,17,"b'import math\n\nimport tensorflow as tf\n\nfrom opennmt import decoders\nfrom opennmt.layers import bridge\n\n\ndef _generate_source_context(batch_size,\n                             depth,\n                             num_sources=1,\n                             dtype=tf.float32):\n  memory_sequence_length = [\n      tf.random.uniform([batch_size], minval=1, maxval=20, dtype=tf.int32)\n      for _ in range(num_sources)]\n  memory_time = [tf.reduce_max(length) for length in memory_sequence_length]\n  memory = [\n      tf.random.uniform([batch_size, time, depth], dtype=dtype)\n      for time in memory_time]\n  initial_state = tuple(None for _ in range(num_sources))\n  if num_sources == 1:\n    memory_sequence_length = memory_sequence_length[0]\n    memory = memory[0]\n    initial_state = initial_state[0]\n  return memory, memory_sequence_length, initial_state\n\n\nclass DecoderTest(tf.test.TestCase):\n\n  def testSamplingProbability(self):\n    step = tf.constant(5, dtype=tf.int64)\n    large_step = tf.constant(1000, dtype=tf.int64)\n    self.assertIsNone(decoders.get_sampling_probability(step))\n    with self.assertRaises(ValueError):\n      decoders.get_sampling_probability(step, schedule_type=""linear"")\n    with self.assertRaises(ValueError):\n      decoders.get_sampling_probability(step, schedule_type=""linear"", k=1)\n    with self.assertRaises(TypeError):\n      decoders.get_sampling_probability(step, schedule_type=""foo"", k=1)\n\n    constant_sample_prob = decoders.get_sampling_probability(\n        step, read_probability=0.9)\n    linear_sample_prob = decoders.get_sampling_probability(\n        step, read_probability=1.0, schedule_type=""linear"", k=0.1)\n    linear_sample_prob_same = decoders.get_sampling_probability(\n        step, read_probability=2.0, schedule_type=""linear"", k=0.1)\n    linear_sample_prob_inf = decoders.get_sampling_probability(\n        large_step, read_probability=1.0, schedule_type=""linear"", k=0.1)\n    exp_sample_prob = decoders.get_sampling_probability(\n        step, schedule_type=""exponential"", k=0.8)\n    inv_sig_sample_prob = decoders.get_sampling_probability(\n        step, schedule_type=""inverse_sigmoid"", k=1)\n\n    self.assertAlmostEqual(0.1, constant_sample_prob)\n    self.assertAlmostEqual(0.5, self.evaluate(linear_sample_prob))\n    self.assertAlmostEqual(0.5, self.evaluate(linear_sample_prob_same))\n    self.assertAlmostEqual(1.0, self.evaluate(linear_sample_prob_inf))\n    self.assertAlmostEqual(1.0 - pow(0.8, 5), self.evaluate(exp_sample_prob))\n    self.assertAlmostEqual(\n        1.0 - (1.0 / (1.0 + math.exp(5.0 / 1.0))), self.evaluate(inv_sig_sample_prob))\n\n  def _testDecoder(self,\n                   decoder,\n                   initial_state_fn=None,\n                   num_sources=1,\n                   dtype=tf.float32):\n    batch_size = 4\n    vocab_size = 10\n    time_dim = 5\n    depth = 6\n    memory, memory_sequence_length, initial_state = _generate_source_context(\n        batch_size,\n        depth,\n        num_sources=num_sources,\n        dtype=dtype)\n\n    if initial_state_fn is not None:\n      initial_state = initial_state_fn(batch_size, dtype)\n    decoder.initialize(vocab_size=vocab_size)\n    initial_state = decoder.initial_state(\n        memory=memory,\n        memory_sequence_length=memory_sequence_length,\n        initial_state=initial_state,\n        dtype=dtype)\n\n    # Test 3D inputs.\n    inputs = tf.random.uniform([batch_size, time_dim, depth], dtype=dtype)\n    # Allow max(sequence_length) to be less than time_dim.\n    sequence_length = tf.constant([1, 3, 4, 2], dtype=tf.int32)\n    outputs, _, attention = decoder(\n        inputs,\n        sequence_length,\n        state=initial_state,\n        training=True)\n    self.assertEqual(outputs.dtype, dtype)\n    output_time_dim = tf.shape(outputs)[1]\n    if decoder.support_alignment_history:\n      self.assertIsNotNone(attention)\n    else:\n      self.assertIsNone(attention)\n    output_time_dim_val = self.evaluate(output_time_dim)\n    self.assertEqual(time_dim, output_time_dim_val)\n    if decoder.support_alignment_history:\n      first_memory = memory[0] if isinstance(memory, list) else memory\n      attention_val, memory_time = self.evaluate([attention, tf.shape(first_memory)[1]])\n      self.assertAllEqual([batch_size, time_dim, memory_time], attention_val.shape)\n\n    # Test 2D inputs.\n    inputs = tf.random.uniform([batch_size, depth], dtype=dtype)\n    step = tf.constant(0, dtype=tf.int32)\n    outputs, _, attention = decoder(\n        inputs,\n        step,\n        state=initial_state)\n    self.assertEqual(outputs.dtype, dtype)\n    if decoder.support_alignment_history:\n      self.assertIsNotNone(attention)\n    else:\n      self.assertIsNone(attention)\n    self.evaluate(outputs)\n\n  def testRNNDecoder(self):\n    decoder = decoders.RNNDecoder(2, 20)\n    self._testDecoder(decoder)\n\n  def testAttentionalRNNDecoder(self):\n    decoder = decoders.AttentionalRNNDecoder(2, 20)\n    self._testDecoder(decoder)\n\n  def testAttentionalRNNDecoderWithDenseBridge(self):\n    decoder = decoders.AttentionalRNNDecoder(2, 36, bridge_class=bridge.DenseBridge)\n    encoder_cell = tf.keras.layers.StackedRNNCells(\n        [tf.keras.layers.LSTMCell(5), tf.keras.layers.LSTMCell(5)])\n    initial_state_fn = lambda batch_size, dtype: encoder_cell.get_initial_state(\n        batch_size=batch_size, dtype=dtype)\n    self._testDecoder(decoder, initial_state_fn=initial_state_fn)\n\n  def testAttentionalRNNDecoderFirstLayer(self):\n    decoder = decoders.AttentionalRNNDecoder(2, 20, first_layer_attention=True)\n    self._testDecoder(decoder)\n\n  def testRNMTPlusDecoder(self):\n    decoder = decoders.RNMTPlusDecoder(2, 20, 4)\n    self._testDecoder(decoder)\n\n  def testSelfAttentionDecoder(self):\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=6, num_heads=2, ffn_inner_dim=12)\n    self._testDecoder(decoder)\n\n  def testSelfAttentionDecoderMultiSource(self):\n    num_sources = 2\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=6, num_heads=2, ffn_inner_dim=12, num_sources=num_sources)\n    self._testDecoder(decoder, num_sources=num_sources)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/decoding_test.py,7,"b'import tensorflow as tf\nimport numpy as np\n\nfrom opennmt.utils import decoding\n\n\ndef _generate_logits_fn(vocab_size, to_generate):\n  to_generate = tf.convert_to_tensor(to_generate, dtype=tf.int32)\n\n  def _logits_fn(symbols, step, state):\n    logits = tf.one_hot(to_generate[:, step], vocab_size, dtype=tf.float32)\n    return logits, state\n\n  return _logits_fn\n\n\nclass DecodingTest(tf.test.TestCase):\n\n  def testSamplerFromParams(self):\n    self.assertIsInstance(decoding.Sampler.from_params({}), decoding.BestSampler)\n    self.assertIsInstance(decoding.Sampler.from_params({""sampling_topk"": 1}), decoding.BestSampler)\n    self.assertIsInstance(decoding.Sampler.from_params({""sampling_topk"": 2}), decoding.RandomSampler)\n\n  def testDecodingStrategyFromParams(self):\n    self.assertIsInstance(\n        decoding.DecodingStrategy.from_params({}), decoding.GreedySearch)\n    self.assertIsInstance(\n        decoding.DecodingStrategy.from_params({""beam_width"": 1}), decoding.GreedySearch)\n    self.assertIsInstance(\n        decoding.DecodingStrategy.from_params({""beam_width"": 2}), decoding.BeamSearch)\n\n  def testPenalizeToken(self):\n    log_probs = tf.zeros([4, 6])\n    token_id = 1\n    log_probs = decoding._penalize_token(log_probs, token_id)\n    log_probs = self.evaluate(log_probs)\n    self.assertTrue(np.all(log_probs[:, token_id] < 0))\n    non_penalized = np.delete(log_probs, 1, token_id)\n    self.assertEqual(np.sum(non_penalized), 0)\n\n  def testGreedyDecode(self):\n    logits_fn = _generate_logits_fn(10, [[4, 5, 6, 2], [3, 8, 2, 8]])\n    ids, lengths, _, _, _ = decoding.dynamic_decode(logits_fn, [1, 1], end_id=2)\n    self.assertAllEqual(self.evaluate(ids), [[[4, 5, 6, 2]], [[3, 8, 2, 2]]])\n    self.assertAllEqual(self.evaluate(lengths), [[3], [2]])\n\n  def testGreedyDecodeWithMaximumIterations(self):\n    logits_fn = _generate_logits_fn(10, [[4, 5, 6, 2], [3, 8, 2, 8]])\n    ids, lengths, _, _, _ = decoding.dynamic_decode(\n        logits_fn, [1, 1], end_id=2, maximum_iterations=2)\n    self.assertAllEqual(self.evaluate(ids), [[[4, 5]], [[3, 8]]])\n    self.assertAllEqual(self.evaluate(lengths), [[2], [2]])\n\n  def testGreedyDecodeWithMinimumIterations(self):\n    logits_fn = _generate_logits_fn(10, [[4, 2, 2, 2], [3, 8, 7, 2]])\n    ids, lengths, _, _, _ = decoding.dynamic_decode(\n        logits_fn, [1, 1], end_id=2, minimum_iterations=2)\n    self.assertAllEqual(self.evaluate(lengths), [[2], [3]])\n\n  def testGatherFromWordIndices(self):\n    tensor = tf.reshape(tf.range(30, dtype=tf.int32), [3, 10])\n    indices = tf.constant([[5, 3], [0, 8], [4, 2]], dtype=tf.int32)\n    output = decoding._gather_from_word_indices(tensor, indices)\n    self.assertAllEqual(self.evaluate(output), [[5, 3], [10, 18], [24, 22]])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/encoder_test.py,43,"b'from parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt import encoders\nfrom opennmt.layers import reducer\n\n\nclass DenseEncoder(encoders.Encoder):\n\n  def __init__(self, num_layers, num_units):\n    super(DenseEncoder, self).__init__()\n    self.layers = [tf.keras.layers.Dense(num_units) for _ in range(num_layers)]\n\n  def call(self, inputs, sequence_length=None, training=None):\n    states = []\n    for layer in self.layers:\n      inputs = layer(inputs)\n      states.append(inputs[:, -1])\n    return inputs, tuple(states), sequence_length\n\n\nclass EncoderTest(tf.test.TestCase):\n\n  def testMeanEncoder(self):\n    inputs = tf.concat([tf.ones([1, 5, 1]), 2*tf.ones([1, 5, 1])], 0)\n    length = tf.constant([2, 4], dtype=tf.int32)\n    mask = tf.sequence_mask(length, maxlen=tf.shape(inputs)[1], dtype=inputs.dtype)\n    inputs *= tf.expand_dims(mask, -1)\n    encoder = encoders.MeanEncoder()\n    _, state, _ = encoder(inputs, sequence_length=length)\n    state = self.evaluate(state)\n    self.assertEqual(state[0][0], 1)\n    self.assertEqual(state[1][0], 2)\n\n  def testConvEncoder(self):\n    sequence_length = [17, 21, 20]\n    inputs = tf.zeros([3, 21, 5])\n    encoder = encoders.ConvEncoder(3, 3, 10)\n    outputs, _, encoded_length = encoder(\n        inputs, sequence_length=tf.constant(sequence_length))\n    outputs, encoded_length = self.evaluate([outputs, encoded_length])\n    self.assertAllEqual([3, 21, 10], outputs.shape)\n    self.assertAllEqual(sequence_length, encoded_length)\n\n  def testPyramidalEncoder(self):\n    sequence_length = [17, 21, 20]\n    inputs = tf.zeros([3, 21, 5])\n    encoder = encoders.PyramidalRNNEncoder(3, 10, reduction_factor=2)\n    outputs, state, encoded_length = encoder(\n        inputs, sequence_length=sequence_length)\n    self.assertEqual(3, len(state))\n    outputs, encoded_length = self.evaluate([outputs, encoded_length])\n    self.assertAllEqual([3, 6, 10], outputs.shape)\n    self.assertAllEqual([4, 5, 5], encoded_length)\n\n  def testPyramidalEncoderShortSequences(self):\n    sequence_length = [3, 4, 2]\n    inputs = tf.zeros([3, 4, 5])\n    encoder = encoders.PyramidalRNNEncoder(3, 10, reduction_factor=2)\n    outputs, state, encoded_length = encoder(\n        inputs, sequence_length=sequence_length)\n    encoded_length = self.evaluate(encoded_length)\n    self.assertAllEqual([1, 1, 1], encoded_length)\n\n  @parameterized.expand([[None], [tf.identity], [[tf.identity]]])\n  def testSequentialEncoder(self, transition_layer_fn):\n    inputs = tf.zeros([3, 5, 10])\n    encoder = encoders.SequentialEncoder(\n        [DenseEncoder(1, 20), DenseEncoder(3, 20)],\n        transition_layer_fn=transition_layer_fn)\n    outputs, states, _ = encoder(inputs)\n    self.assertEqual(len(states), 4)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual(outputs.shape, [3, 5, 20])\n\n  def testSequentialEncoderWithTooManyTransitionLayers(self):\n    with self.assertRaises(ValueError):\n      _ = encoders.SequentialEncoder(\n          [DenseEncoder(1, 20), DenseEncoder(3, 20)],\n          transition_layer_fn=[tf.identity, tf.identity])\n\n  def testRNMTPlusEncoder(self):\n    sequence_length = [4, 6, 5]\n    inputs = tf.zeros([3, 6, 5])\n    encoder = encoders.RNMTPlusEncoder(6, 10)\n    outputs, state, _ = encoder(\n        inputs, sequence_length=sequence_length)\n    self.assertEqual(6, len(state))\n    self.assertEqual(10 * 2, tf.nest.flatten(state)[0].shape[-1])\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual([3, max(sequence_length), 10], outputs.shape)\n\n  def testParallelEncoder(self):\n    sequence_lengths = [[3, 5, 2], [6, 6, 4]]\n    inputs = [tf.zeros([3, 5, 10]), tf.zeros([3, 6, 10])]\n    encoder = encoders.ParallelEncoder(\n        [DenseEncoder(1, 20), DenseEncoder(2, 20)],\n        outputs_reducer=reducer.ConcatReducer(axis=1))\n    outputs, state, encoded_length = encoder(\n        inputs, sequence_length=sequence_lengths)\n    self.assertEqual(len(state), 3)\n    outputs, encoded_length = self.evaluate([outputs, encoded_length])\n    self.assertAllEqual([3, 11, 20], outputs.shape)\n    self.assertAllEqual([9, 11, 6], encoded_length)\n\n  def _encodeInParallel(self,\n                        inputs,\n                        sequence_length=None,\n                        outputs_layer_fn=None,\n                        combined_output_layer_fn=None):\n    columns = [DenseEncoder(1, 20), DenseEncoder(1, 20)]\n    encoder = encoders.ParallelEncoder(\n        columns,\n        outputs_reducer=reducer.ConcatReducer(),\n        outputs_layer_fn=outputs_layer_fn,\n        combined_output_layer_fn=combined_output_layer_fn)\n    outputs, _, _ = encoder(inputs, sequence_length=sequence_length)\n    return self.evaluate(outputs)\n\n  def testParallelEncoderSameInput(self):\n    sequence_length = tf.constant([2, 5, 4], dtype=tf.int32)\n    inputs = tf.zeros([3, 5, 10])\n    outputs = self._encodeInParallel(inputs, sequence_length=sequence_length)\n    self.assertAllEqual(outputs.shape, [3, 5, 40])\n\n  def testParallelEncoderCombinedOutputLayer(self):\n    sequence_length = tf.constant([2, 5, 4], dtype=tf.int32)\n    inputs = tf.zeros([3, 5, 10])\n    outputs = self._encodeInParallel(\n        inputs,\n        sequence_length=sequence_length,\n        combined_output_layer_fn=tf.keras.layers.Dense(15))\n    self.assertEqual(outputs.shape[-1], 15)\n\n  def _encodeAndProjectInParallel(self, outputs_size):\n    sequence_length = tf.constant([2, 5, 4], dtype=tf.int32)\n    inputs = tf.zeros([3, 5, 10])\n    if isinstance(outputs_size, list):\n      outputs_layer_fn = [tf.keras.layers.Dense(s) for s in outputs_size]\n      combined_output_size = sum(outputs_size)\n    else:\n      outputs_layer_fn = tf.keras.layers.Dense(outputs_size)\n      combined_output_size = outputs_size * 2\n    outputs = self._encodeInParallel(\n        inputs,\n        sequence_length=sequence_length,\n        outputs_layer_fn=outputs_layer_fn)\n    self.assertEqual(outputs.shape[-1], combined_output_size)\n\n  def testParallelEncoderSameOutputsLayer(self):\n    self._encodeAndProjectInParallel(15)\n\n  def testParallelEncoderOutputsLayer(self):\n    self._encodeAndProjectInParallel([14, 15])\n\n  def testParallelEncoderOutputsLayerInvalid(self):\n    with self.assertRaises(ValueError):\n      self._encodeAndProjectInParallel([15])\n\n  def testParallelEncoderReuse(self):\n    lengths = [tf.constant([2, 5, 4], dtype=tf.int32), tf.constant([6, 6, 3], dtype=tf.int32)]\n    inputs = [tf.zeros([3, 5, 10]), tf.zeros([3, 6, 10])]\n    encoder = encoders.ParallelEncoder(DenseEncoder(2, 20), outputs_reducer=None)\n    outputs, _, _ = encoder(inputs, sequence_length=lengths)\n    outputs = self.evaluate(outputs)\n    self.assertIsInstance(outputs, tuple)\n    self.assertEqual(len(outputs), 2)\n\n  @parameterized.expand([[tf.float32], [tf.float16]])\n  def testSelfAttentionEncoder(self, dtype):\n    tf.keras.backend.set_floatx(dtype.name)\n    encoder = encoders.SelfAttentionEncoder(\n        3, num_units=20, num_heads=4, ffn_inner_dim=40)\n    inputs = tf.random.uniform([4, 5, 10], dtype=dtype)\n    lengths = tf.constant([4, 3, 5, 2])\n    outputs, _, _ = encoder(inputs, sequence_length=lengths, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 20])\n    self.assertEqual(outputs.dtype, dtype)\n    tf.keras.backend.set_floatx(""float32"")\n\n  def testLSTMEncoder(self):\n    encoder = encoders.LSTMEncoder(3, 20)\n    inputs = tf.random.uniform([4, 5, 10])\n    lengths = tf.constant([4, 3, 5, 2])\n    outputs, states, _ = encoder(inputs, sequence_length=lengths, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 20])\n    self.assertEqual(len(states), 3)\n\n  @parameterized.expand([[tf.keras.layers.LSTMCell], [tf.keras.layers.GRUCell]])\n  def testUnidirectionalRNNEncoder(self, cell_class):\n    encoder = encoders.RNNEncoder(3, 20, cell_class=cell_class)\n    inputs = tf.random.uniform([4, 5, 10])\n    lengths = tf.constant([4, 3, 5, 2])\n    outputs, states, _ = encoder(inputs, sequence_length=lengths, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 20])\n    self.assertEqual(len(states), 3)\n\n  @parameterized.expand([[tf.keras.layers.LSTMCell], [tf.keras.layers.GRUCell]])\n  def testBidirectionalRNNEncoder(self, cell_class):\n    encoder = encoders.RNNEncoder(3, 20, bidirectional=True, cell_class=cell_class)\n    inputs = tf.random.uniform([4, 5, 10])\n    lengths = tf.constant([4, 3, 5, 2])\n    outputs, states, _ = encoder(inputs, sequence_length=lengths, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 40])\n    self.assertEqual(len(states), 3)\n\n  def testGNMTEncoder(self):\n    encoder = encoders.GNMTEncoder(3, 20)\n    inputs = tf.random.uniform([4, 5, 10])\n    lengths = tf.constant([4, 3, 5, 2])\n    outputs, states, _ = encoder(inputs, sequence_length=lengths, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 20])\n    self.assertEqual(len(states), 3)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/evaluation_test.py,6,"b'import math\nimport os\n\nimport tensorflow as tf\n\nfrom opennmt import evaluation\nfrom opennmt import inputters\nfrom opennmt import models\nfrom opennmt.utils import exporters\nfrom opennmt.utils import scorers\n\n\n# Define some dummy components to simply return the loss and metrics we want to test.\n\nclass TestMetric(object):\n  def __init__(self, result_history):\n    self.result_history = result_history\n\n  def result(self):\n    result = self.result_history[0]\n    self.result_history.pop(0)\n    return result\n\nclass TestInputter(inputters.Inputter):\n  def make_dataset(self, data_file, training=None):\n    return tf.data.TextLineDataset(data_file)\n\n  def input_signature(self):\n    return None\n\n  def make_features(self, element=None, features=None, training=None):\n    return tf.strings.to_number(element)\n\nclass TestModel(models.Model):\n  def __init__(self, metrics_history=None):\n    example_inputter = inputters.ExampleInputter(TestInputter(), TestInputter())\n    super(TestModel, self).__init__(example_inputter)\n    if metrics_history is None:\n      metrics_history = {}\n    self.metrics_history = metrics_history\n    self.next_loss = tf.Variable(0)\n\n  def call(self, features, labels=None, training=None, step=None):\n    return features, None\n\n  def get_metrics(self):\n    return {name:TestMetric(history) for name, history in self.metrics_history.items()}\n\n  def compute_loss(self, outputs, labels, training=True):\n    return self.next_loss\n\nclass TestExporter(exporters.Exporter):\n  def _export_model(self, model, export_dir):\n    tf.io.gfile.makedirs(export_dir)  # Just create an empty directory.\n\n\nclass EvaluationTest(tf.test.TestCase):\n\n  def _assertMetricsEqual(self, metrics, expected):\n    self.assertLen(metrics, len(expected))\n    for name in expected.keys():\n      self.assertIn(name, metrics)\n      self.assertAllClose(metrics[name], expected[name])\n\n  def testEvaluationMetric(self):\n    features_file = os.path.join(self.get_temp_dir(), ""features.txt"")\n    labels_file = os.path.join(self.get_temp_dir(), ""labels.txt"")\n    model_dir = self.get_temp_dir()\n    with open(features_file, ""w"") as features, open(labels_file, ""w"") as labels:\n      features.write(""1\\n2\\n"")\n      labels.write(""1\\n2\\n"")\n    model = TestModel({""a"": [2, 5, 8], ""b"": [3, 6, 9]})\n    early_stopping = evaluation.EarlyStopping(metric=""loss"", min_improvement=0, steps=1)\n    evaluator = evaluation.Evaluator(\n        model,\n        features_file,\n        labels_file,\n        batch_size=1,\n        early_stopping=early_stopping,\n        model_dir=model_dir,\n        export_on_best=""loss"",\n        exporter=TestExporter())\n    self.assertSetEqual(evaluator.metrics_name, {""loss"", ""perplexity"", ""a"", ""b""})\n    model.next_loss.assign(1)\n    metrics_5 = evaluator(5)\n    self._assertMetricsEqual(\n        metrics_5, {""loss"": 1.0, ""perplexity"": math.exp(1.0), ""a"": 2, ""b"": 3})\n    self.assertFalse(evaluator.should_stop())\n    self.assertTrue(evaluator.is_best(""loss""))\n    self.assertTrue(os.path.isdir(os.path.join(evaluator.export_dir, str(5))))\n    model.next_loss.assign(4)\n    metrics_10 = evaluator(10)\n    self._assertMetricsEqual(\n        metrics_10, {""loss"": 4.0, ""perplexity"": math.exp(4.0), ""a"": 5, ""b"": 6})\n    self.assertTrue(evaluator.should_stop())\n    self.assertFalse(evaluator.is_best(""loss""))\n    self.assertFalse(os.path.isdir(os.path.join(evaluator.export_dir, str(10))))\n    self.assertLen(evaluator.metrics_history, 2)\n    self._assertMetricsEqual(evaluator.metrics_history[0][1], metrics_5)\n    self._assertMetricsEqual(evaluator.metrics_history[1][1], metrics_10)\n\n    # Recreating the evaluator should load the metrics history from the eval directory.\n    evaluator = evaluation.Evaluator(\n        model,\n        features_file,\n        labels_file,\n        batch_size=1,\n        model_dir=model_dir,\n        export_on_best=""loss"",\n        exporter=TestExporter())\n    self.assertLen(evaluator.metrics_history, 2)\n    self._assertMetricsEqual(evaluator.metrics_history[0][1], metrics_5)\n    self._assertMetricsEqual(evaluator.metrics_history[1][1], metrics_10)\n\n    # Evaluating previous steps should clear future steps in the history.\n    model.next_loss.assign(7)\n    self._assertMetricsEqual(\n        evaluator(7), {""loss"": 7.0, ""perplexity"": math.exp(7.0), ""a"": 8, ""b"": 9})\n    self.assertFalse(evaluator.is_best(""loss""))\n    self.assertFalse(os.path.isdir(os.path.join(evaluator.export_dir, str(10))))\n    recorded_steps = list(step for step, _ in evaluator.metrics_history)\n    self.assertListEqual(recorded_steps, [5, 7])\n\n  def testEvaluationWithRougeScorer(self):\n    features_file = os.path.join(self.get_temp_dir(), ""features.txt"")\n    labels_file = os.path.join(self.get_temp_dir(), ""labels.txt"")\n    model_dir = self.get_temp_dir()\n    with open(features_file, ""w"") as features, open(labels_file, ""w"") as labels:\n      features.write(""1\\n2\\n"")\n      labels.write(""1\\n2\\n"")\n    model = TestModel()\n    evaluator = evaluation.Evaluator(\n        model,\n        features_file,\n        labels_file,\n        batch_size=1,\n        scorers=[scorers.ROUGEScorer()],\n        model_dir=model_dir)\n    self.assertNotIn(""rouge"", evaluator.metrics_name)\n    self.assertIn(""rouge-1"", evaluator.metrics_name)\n    self.assertIn(""rouge-2"", evaluator.metrics_name)\n    self.assertIn(""rouge-l"", evaluator.metrics_name)\n\n  def testExportsGarbageCollection(self):\n    features_file = os.path.join(self.get_temp_dir(), ""features.txt"")\n    labels_file = os.path.join(self.get_temp_dir(), ""labels.txt"")\n    model_dir = self.get_temp_dir()\n    with open(features_file, ""w"") as features, open(labels_file, ""w"") as labels:\n      features.write(""1\\n2\\n"")\n      labels.write(""1\\n2\\n"")\n    model = TestModel()\n    exporter = TestExporter()\n    evaluator = evaluation.Evaluator(\n        model,\n        features_file,\n        labels_file,\n        batch_size=1,\n        model_dir=model_dir,\n        export_on_best=""loss"",\n        exporter=exporter,\n        max_exports_to_keep=2)\n\n    # Generate some pre-existing exports.\n    for step in (5, 10, 15):\n      exporter.export(model, os.path.join(evaluator.export_dir, str(step)))\n\n    def _eval_step(step, loss, expected_exported_steps):\n      model.next_loss.assign(loss)\n      evaluator(step)\n      exported_steps = list(sorted(map(int, os.listdir(evaluator.export_dir))))\n      self.assertListEqual(exported_steps, expected_exported_steps)\n\n    _eval_step(20, 3, [15, 20])  # Exports 5 and 10 should be removed.\n    _eval_step(25, 2, [20, 25])  # Export 15 should be removed.\n\n  def testEarlyStop(self):\n    self.assertFalse(\n        evaluation.early_stop([3.1, 2.7, 2.6], 4))\n    self.assertFalse(\n        evaluation.early_stop([3.1, 2.7, 2.6, 2.5], 4))\n    self.assertTrue(\n        evaluation.early_stop([3.1, 3.1, 3.0, 2.9], 3, min_improvement=0.3))\n    self.assertTrue(\n        evaluation.early_stop([32, 33, 32, 33, 32], 3, min_improvement=2, higher_is_better=False))\n    self.assertFalse(\n        evaluation.early_stop([32, 35, 32, 33, 32], 3, min_improvement=2, higher_is_better=False))\n    self.assertTrue(\n        evaluation.early_stop(\n            [50.349343, 50.553991, 50.436176, 50.419565, 50.219028, 50.375434],\n            4, min_improvement=0.01, higher_is_better=True))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/exporters_test.py,3,"b'import os\n\nimport tensorflow as tf\n\nfrom opennmt import models\nfrom opennmt.tests import test_util\nfrom opennmt.utils import exporters\n\n\nclass ExportersTest(tf.test.TestCase):\n\n  def testCheckpointExport(self):\n    vocab = test_util.make_vocab(os.path.join(self.get_temp_dir(), ""vocab.txt""), [""a"", ""b"", ""c""])\n\n    model = models.TransformerBase()\n    model.initialize(dict(source_vocabulary=vocab, target_vocabulary=vocab))\n    model.create_variables()\n    original_embedding = model.features_inputter.embedding.numpy()\n\n    export_dir = self.get_temp_dir()\n    exporter = exporters.CheckpointExporter()\n    exporter.export(model, export_dir)\n\n    model = models.TransformerBase()\n    model.initialize(dict(source_vocabulary=vocab, target_vocabulary=vocab))\n    model.create_variables()\n\n    checkpoint = tf.train.Checkpoint(model=model)\n    checkpoint.restore(os.path.join(export_dir, ""ckpt""))\n\n    restored_embedding = model.features_inputter.embedding.numpy()\n    self.assertAllEqual(restored_embedding, original_embedding)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/inputter_test.py,8,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport gzip\nimport io\nimport yaml\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorboard.plugins import projector\nfrom google.protobuf import text_format\nfrom parameterized import parameterized\n\nfrom opennmt import inputters\nfrom opennmt import tokenizers\nfrom opennmt.data import dataset as dataset_util\nfrom opennmt.data import noise\nfrom opennmt.inputters import inputter, text_inputter, record_inputter\nfrom opennmt.layers import reducer\nfrom opennmt.tests import test_util\nfrom opennmt.utils.misc import item_or_tuple, count_lines\n\n\nclass InputterTest(tf.test.TestCase):\n\n  def testSaveEmbeddingMetadata(self):\n    log_dir = os.path.join(self.get_temp_dir(), ""log"")\n    if not os.path.exists(log_dir):\n      os.mkdir(log_dir)\n\n    def _create_vocab(vocab_filename, vocab_size=10):\n      vocab_file = os.path.join(self.get_temp_dir(), vocab_filename)\n      with open(vocab_file, mode=""w"") as vocab:\n        for i in range(vocab_size):\n          vocab.write(""%d\\n"" % i)\n      return vocab_file\n\n    def _visualize(embedding, vocab_file, num_oov_buckets=1):\n      text_inputter.save_embeddings_metadata(\n          log_dir, embedding, vocab_file, num_oov_buckets=num_oov_buckets)\n      projector_config = projector.ProjectorConfig()\n      projector_config_path = os.path.join(log_dir, ""projector_config.pbtxt"")\n      self.assertTrue(os.path.exists(projector_config_path))\n      with open(projector_config_path) as projector_config_file:\n        text_format.Merge(projector_config_file.read(), projector_config)\n      return projector_config\n\n    def _check_vocab(config, filename, expected_size):\n      self.assertEqual(config.metadata_path, filename)\n      self.assertEqual(count_lines(os.path.join(log_dir, filename)), expected_size)\n\n    # Register an embedding variable.\n    src_embedding = ""model/src_emb/.ATTRIBUTES/VALUE""\n    src_vocab_file = _create_vocab(""src_vocab.txt"")\n    projector_config = _visualize(src_embedding, src_vocab_file)\n    self.assertEqual(1, len(projector_config.embeddings))\n    self.assertEqual(src_embedding, projector_config.embeddings[0].tensor_name)\n    _check_vocab(projector_config.embeddings[0], ""model_src_emb.txt"", 10 + 1)\n\n    # Register a second embedding variable.\n    tgt_embedding = ""model/tgt_emb/.ATTRIBUTES/VALUE""\n    tgt_vocab_file = _create_vocab(""tgt_vocab.txt"")\n    projector_config = _visualize(tgt_embedding, tgt_vocab_file, num_oov_buckets=2)\n    self.assertEqual(2, len(projector_config.embeddings))\n    self.assertEqual(tgt_embedding, projector_config.embeddings[1].tensor_name)\n    _check_vocab(projector_config.embeddings[1], ""model_tgt_emb.txt"", 10 + 2)\n\n    # Update an existing variable.\n    src_vocab_file = _create_vocab(""src_vocab.txt"", vocab_size=20)\n    projector_config = _visualize(src_embedding, src_vocab_file)\n    self.assertEqual(2, len(projector_config.embeddings))\n    self.assertEqual(src_embedding, projector_config.embeddings[0].tensor_name)\n    _check_vocab(projector_config.embeddings[0], ""model_src_emb.txt"", 20 + 1)\n\n  def _makeTextFile(self, name, lines, compress=False):\n    path = os.path.join(self.get_temp_dir(), name)\n    if compress:\n      path = ""%s.gz"" % path\n    with (gzip if compress else io).open(path, mode=""wt"", encoding=""utf-8"") as f:\n      for line in lines:\n        f.write(""%s\\n"" % line)\n    return path\n\n  def _makeEmbeddingsFile(self, vectors, name=""embedding"", header=False):\n    path = os.path.join(self.get_temp_dir(), name)\n    with open(path, ""w"") as embs:\n      if header:\n        embs.write(""%d %d\\n"" % (len(vectors), len(vectors[0][1])))\n      for word, vector in vectors:\n        embs.write(""%s %s\\n"" % (word, "" "".join(str(v) for v in vector)))\n    return path\n\n  def testPretrainedEmbeddingsLoading(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""Toto"", ""tOTO"", ""tata"", ""tete""])\n    embedding_file = self._makeEmbeddingsFile(\n        [(""toto"", [1, 1]), (""titi"", [2, 2]), (""tata"", [3, 3])])\n\n    embeddings = text_inputter.load_pretrained_embeddings(\n        embedding_file,\n        vocab_file,\n        num_oov_buckets=1,\n        with_header=False,\n        case_insensitive_embeddings=True)\n    self.assertAllEqual([5, 2], embeddings.shape)\n    self.assertAllEqual([1, 1], embeddings[0])\n    self.assertAllEqual([1, 1], embeddings[1])\n    self.assertAllEqual([3, 3], embeddings[2])\n\n    embeddings = text_inputter.load_pretrained_embeddings(\n        embedding_file,\n        vocab_file,\n        num_oov_buckets=2,\n        with_header=False,\n        case_insensitive_embeddings=False)\n    self.assertAllEqual([6, 2], embeddings.shape)\n    self.assertAllEqual([3, 3], embeddings[2])\n\n  def testPretrainedEmbeddingsWithHeaderLoading(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""Toto"", ""tOTO"", ""tata"", ""tete""])\n    embedding_file = self._makeEmbeddingsFile(\n        [(""toto"", [1, 1]), (""titi"", [2, 2]), (""tata"", [3, 3])], header=True)\n\n    embeddings = text_inputter.load_pretrained_embeddings(\n        embedding_file,\n        vocab_file,\n        num_oov_buckets=1,\n        case_insensitive_embeddings=True)\n    self.assertAllEqual([5, 2], embeddings.shape)\n    self.assertAllEqual([1, 1], embeddings[0])\n    self.assertAllEqual([1, 1], embeddings[1])\n    self.assertAllEqual([3, 3], embeddings[2])\n\n  @parameterized.expand([\n      [[3, 4], 2, 1, 2, [1, 3, 4, 2], 4],\n      [[[3, 4], [5, 6]], [2, 1], 1, None, [[1, 3, 4], [1, 5, 6]], [3, 2]],\n      [[[3, 4], [5, 6]], [2, 1], None, 2, [[3, 4, 2], [5, 2, 0]], [3, 2]],\n  ])\n  def testAddSequenceControls(self, ids, length, start_id, end_id, expected_ids, expected_length):\n    ids = tf.constant(ids, dtype=tf.int64)\n    length = tf.constant(length, dtype=tf.int32)\n    ids, length = inputters.add_sequence_controls(ids, length, start_id=start_id, end_id=end_id)\n    self.assertAllEqual(self.evaluate(ids), expected_ids)\n    self.assertAllEqual(self.evaluate(length), expected_length)\n\n  def _checkFeatures(self, features, expected_shapes):\n    for name, expected_shape in expected_shapes.items():\n      self.assertIn(name, features)\n      self.assertTrue(features[name].shape.is_compatible_with(expected_shape))\n\n  def _testServing(self, inputter):\n    @tf.function(input_signature=(inputter.input_signature(),))\n    def _serving_fun(features):\n      features = inputter.make_features(features=features.copy())\n      inputs = inputter(features)\n      return inputs\n    _serving_fun.get_concrete_function()\n\n  def _makeDataset(self, inputter, data_file, data_config=None, dataset_size=1, shapes=None):\n    if data_config is not None:\n      inputter.initialize(data_config)\n    dataset = inputter.make_dataset(data_file)\n    dataset = dataset.map(lambda *arg: inputter.make_features(item_or_tuple(arg), training=True))\n    dataset = dataset.apply(dataset_util.batch_dataset(1))\n    features = iter(dataset).next()\n    if shapes is not None:\n      self._checkFeatures(features, shapes)\n    inputs = inputter(features, training=True)\n    if not isinstance(inputter, inputters.ExampleInputter):\n      self._testServing(inputter)\n    return self.evaluate((features, inputs))\n\n  def testWordEmbedder(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config={""vocabulary"": vocab_file},\n        shapes={""tokens"": [None, None], ""ids"": [None, None], ""length"": [None]})\n\n    self.assertAllEqual([3], features[""length""])\n    self.assertAllEqual([[2, 1, 4]], features[""ids""])\n    self.assertAllEqual([1, 3, 10], transformed.shape)\n\n  def testWordEmbedderForDecoder(self):\n    vocab_file = test_util.make_vocab(\n        os.path.join(self.get_temp_dir(), ""vocab.txt""), [""the"", ""world"", ""hello"", ""toto""])\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    embedder.set_decoder_mode(mark_start=True, mark_end=True)\n    embedder.initialize({""vocabulary"": vocab_file})\n    features = self.evaluate(embedder.make_features(tf.constant(""hello world"")))\n    self.assertEqual(features[""length""], 3)\n    self.assertEqual(embedder.get_length(features, ignore_special_tokens=True), 2)\n    self.assertAllEqual(features[""ids""], [1, 5, 4])\n    self.assertAllEqual(features[""ids_out""], [5, 4, 2])\n\n  def testWordEmbedderWithTokenizer(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""\xef\xbf\xad""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world!""])\n    tokenization = {\n        ""mode"": ""aggressive"",\n        ""joiner_annotate"": True,\n        ""joiner_new"": True\n    }\n    tokenization_config_path = os.path.join(self.get_temp_dir(), ""tok.yml"")\n    with open(tokenization_config_path, ""w"") as tokenization_config_file:\n      yaml.dump(tokenization, tokenization_config_file)\n\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    data_config = {\n        ""vocabulary"": vocab_file,\n        ""tokenization"": tokenization_config_path\n    }\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config=data_config,\n        shapes={""tokens"": [None, None], ""ids"": [None, None], ""length"": [None]})\n\n    self.assertAllEqual([4], features[""length""])\n    self.assertAllEqual([[2, 1, 3, 4]], features[""ids""])\n\n  def testWordEmbedderWithInGraphTokenizer(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""\xef\xbf\xad""])\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    data_config = {\n        ""vocabulary"": vocab_file,\n        ""tokenization"": {""type"": ""CharacterTokenizer""}\n    }\n    embedder.initialize(data_config)\n    self.assertIn(""text"", embedder.input_signature())\n    self._testServing(embedder)\n\n  def testWordEmbedderWithCompression(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""\xef\xbf\xad""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !"", ""how are you ?""], compress=True)\n    inputter = text_inputter.WordEmbedder(embedding_size=10)\n    inputter.initialize(dict(vocabulary=vocab_file))\n    dataset = inputter.make_inference_dataset(data_file, batch_size=1)\n    iterator = iter(dataset)\n    self.assertAllEqual(next(iterator)[""tokens""].numpy()[0], [b""hello"", b""world"", b""!""])\n\n  def testWordEmbedderWithNoise(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n    noiser = noise.WordNoiser(noises=[noise.WordOmission(1)])\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    embedder.set_noise(noiser, in_place=False)\n    expected_shapes = {\n        ""tokens"": [None, None],\n        ""ids"": [None, None],\n        ""length"": [None],\n        ""noisy_tokens"": [None, None],\n        ""noisy_ids"": [None, None],\n        ""noisy_length"": [None]\n    }\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config={""vocabulary"": vocab_file},\n        shapes=expected_shapes)\n    self.assertEqual(features[""noisy_length""][0], features[""length""][0] - 1)\n\n  @parameterized.expand([[1], [0]])\n  def testWordEmbedderWithInPlaceNoise(self, probability):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n    noiser = noise.WordNoiser(noises=[noise.WordOmission(1)])\n    embedder = text_inputter.WordEmbedder(embedding_size=10)\n    embedder.set_noise(noiser, probability=probability)\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config={""vocabulary"": vocab_file},\n        shapes={""tokens"": [None, None], ""ids"": [None, None], ""length"": [None]})\n    self.assertEqual(features[""length""][0], 3 if probability == 0 else 2)\n\n  def testWordEmbedderWithPretrainedEmbeddings(self):\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    embedding_file = self._makeEmbeddingsFile(\n        [(""hello"", [1, 1]), (""world"", [2, 2]), (""toto"", [3, 3])])\n\n    embedder = text_inputter.WordEmbedder()\n    data = {\n        ""vocabulary"": vocab_file,\n        ""embedding"": {""path"": embedding_file, ""with_header"": False}\n    }\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config=data,\n        shapes={""tokens"": [None, None], ""ids"": [None, None], ""length"": [None]})\n\n    self.assertAllEqual([1, 1], transformed[0][0])\n    self.assertAllEqual([2, 2], transformed[0][1])\n\n  def testCharConvEmbedder(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""h"", ""e"", ""l"", ""w"", ""o""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    embedder = text_inputter.CharConvEmbedder(10, 5)\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config={""vocabulary"": vocab_file},\n        shapes={""char_ids"": [None, None, None], ""length"": [None]})\n\n    self.assertAllEqual([3], features[""length""])\n    self.assertAllEqual(\n        [[[0, 1, 2, 2, 4], [3, 4, 5, 2, 5], [5, 5, 5, 5, 5]]],\n        features[""char_ids""])\n    self.assertAllEqual([1, 3, 5], transformed.shape)\n\n  def testCharRNNEmbedder(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""h"", ""e"", ""l"", ""w"", ""o""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    embedder = text_inputter.CharRNNEmbedder(10, 5)\n    features, transformed = self._makeDataset(\n        embedder,\n        data_file,\n        data_config={""vocabulary"": vocab_file},\n        shapes={""char_ids"": [None, None, None], ""length"": [None]})\n\n    self.assertAllEqual([1, 3, 5], transformed.shape)\n\n  def testParallelInputter(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    data_files = [data_file, data_file]\n\n    parallel_inputter = inputter.ParallelInputter([\n        text_inputter.WordEmbedder(embedding_size=10),\n        text_inputter.WordEmbedder(embedding_size=5)])\n    self.assertEqual(parallel_inputter.num_outputs, 2)\n    features, transformed = self._makeDataset(\n        parallel_inputter,\n        data_files,\n        data_config={""1_vocabulary"": vocab_file, ""2_vocabulary"": vocab_file},\n        shapes={""inputter_0_ids"": [None, None], ""inputter_0_length"": [None],\n                ""inputter_1_ids"": [None, None], ""inputter_1_length"": [None]})\n\n    self.assertEqual(2, len(parallel_inputter.get_length(features)))\n    self.assertEqual(2, len(transformed))\n    self.assertAllEqual([1, 3, 10], transformed[0].shape)\n    self.assertAllEqual([1, 3, 5], transformed[1].shape)\n\n  def testParallelInputterShareParameters(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_config = {""1_vocabulary"": vocab_file, ""2_vocabulary"": vocab_file}\n    inputters = [\n        text_inputter.WordEmbedder(embedding_size=10),\n        text_inputter.WordEmbedder(embedding_size=10)]\n    parallel_inputter = inputter.ParallelInputter(inputters, share_parameters=True)\n    parallel_inputter.initialize(data_config)\n    parallel_inputter.build(None)\n    self.assertEqual(\n        inputters[0].embedding.ref(),\n        inputters[1].embedding.ref())\n\n  def testNestedParallelInputterShareParameters(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_config = {\n        ""1_1_vocabulary"": vocab_file,\n        ""1_2_vocabulary"": vocab_file,\n        ""2_vocabulary"": vocab_file\n    }\n    source_inputters = [\n        text_inputter.WordEmbedder(embedding_size=10),\n        text_inputter.WordEmbedder(embedding_size=10)]\n    target_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    inputters = [\n        inputter.ParallelInputter(source_inputters, share_parameters=True),\n        target_inputter]\n    parallel_inputter = inputter.ParallelInputter(inputters, share_parameters=True)\n    parallel_inputter.initialize(data_config)\n    parallel_inputter.build(None)\n    self.assertEqual(\n        source_inputters[0].embedding.ref(),\n        target_inputter.embedding.ref())\n    self.assertEqual(\n        source_inputters[1].embedding.ref(),\n        target_inputter.embedding.ref())\n\n  def testNestedInputtersWithFlatDataFiles(self):\n    inputters = inputter.ParallelInputter(\n        [\n            record_inputter.SequenceRecordInputter(10),\n            record_inputter.SequenceRecordInputter(10),\n        ],\n        reducer=reducer.SumReducer())\n    inputters = inputter.ParallelInputter(\n        [\n            record_inputter.SequenceRecordInputter(10),\n            inputters,\n        ],\n        reducer=reducer.ConcatReducer())\n\n    self.assertListEqual(inputters._structure(), [None, [None, None]])\n\n    empty_file = os.path.join(self.get_temp_dir(), ""test.txt"")\n    with open(empty_file, ""w""):\n      pass\n\n    with self.assertRaises(ValueError):\n      inputters.make_inference_dataset([empty_file, empty_file], batch_size=2)\n    inputters.make_inference_dataset([empty_file, empty_file, empty_file], batch_size=2)\n\n  def testExampleInputter(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    source_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    target_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    example_inputter = inputter.ExampleInputter(source_inputter, target_inputter)\n    self.assertEqual(example_inputter.num_outputs, 2)\n\n    features, transformed = self._makeDataset(\n        example_inputter,\n        [data_file, data_file],\n        data_config={""source_vocabulary"": vocab_file, ""target_vocabulary"": vocab_file})\n\n    self.assertIsInstance(features, tuple)\n    self.assertEqual(len(features), 2)\n    self.assertEqual(len(transformed), 2)\n    features, labels = features\n    for field in (""ids"", ""length"", ""tokens""):\n      self.assertIn(field, features)\n    for field in (""ids"", ""length"", ""tokens""):\n      self.assertIn(field, labels)\n\n  def testExampleInputterFiltering(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""a"", ""b"", ""c"", ""d""])\n    features_file = self._makeTextFile(""features.txt"", [""a b c d"", ""a b c"", ""a a c"", ""a""])\n    labels_file = self._makeTextFile(""labels.txt"", [""a b c d"", ""a"", ""a a c d d"", """"])\n\n    example_inputter = inputter.ExampleInputter(\n        text_inputter.WordEmbedder(embedding_size=10),\n        text_inputter.WordEmbedder(embedding_size=10))\n    example_inputter.initialize({""source_vocabulary"": vocab_file, ""target_vocabulary"": vocab_file})\n\n    dataset = example_inputter.make_training_dataset(\n        features_file,\n        labels_file,\n        batch_size=1,\n        maximum_features_length=3,\n        maximum_labels_length=4,\n        single_pass=True)\n    examples = list(iter(dataset))\n    self.assertLen(examples, 1)\n    self.assertAllEqual(examples[0][0][""ids""], [[0, 1, 2]])\n    self.assertAllEqual(examples[0][1][""ids""], [[0]])\n\n  def testWeightedDataset(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n    source_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    target_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    example_inputter = inputter.ExampleInputter(source_inputter, target_inputter)\n    example_inputter.initialize({\n        ""source_vocabulary"": vocab_file,\n        ""target_vocabulary"": vocab_file})\n    with self.assertRaisesRegex(ValueError, ""same number""):\n      example_inputter.make_training_dataset(\n          [data_file, data_file], [data_file], batch_size=16)\n    with self.assertRaisesRegex(ValueError, ""expected to match""):\n      example_inputter.make_training_dataset(\n          [data_file, data_file], [data_file, data_file], batch_size=16, weights=[0.5])\n    dataset = example_inputter.make_training_dataset(\n        [data_file, data_file],\n        [data_file, data_file],\n        batch_size=16)\n    self.assertIsInstance(dataset, tf.data.Dataset)\n    dataset = example_inputter.make_training_dataset(\n        [data_file, data_file],\n        [data_file, data_file],\n        batch_size=16,\n        weights=[0.2, 0.8])\n    self.assertIsInstance(dataset, tf.data.Dataset)\n\n  def testExampleInputterAsset(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    source_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    target_inputter = text_inputter.WordEmbedder(embedding_size=10)\n    example_inputter = inputter.ExampleInputter(source_inputter, target_inputter)\n    example_inputter.initialize({\n        ""source_vocabulary"": vocab_file,\n        ""target_vocabulary"": vocab_file,\n        ""source_tokenization"": {""mode"": ""conservative""}\n    })\n    self.assertIsInstance(source_inputter.tokenizer, tokenizers.OpenNMTTokenizer)\n\n  def testMixedInputter(self):\n    vocab_file = self._makeTextFile(""vocab.txt"", [""the"", ""world"", ""hello"", ""toto""])\n    vocab_alt_file = self._makeTextFile(""vocab_alt.txt"", [""h"", ""e"", ""l"", ""w"", ""o""])\n    data_file = self._makeTextFile(""data.txt"", [""hello world !""])\n\n    mixed_inputter = inputter.MixedInputter([\n        text_inputter.WordEmbedder(embedding_size=10),\n        text_inputter.CharConvEmbedder(10, 5)],\n        reducer=reducer.ConcatReducer())\n    self.assertEqual(mixed_inputter.num_outputs, 1)\n    features, transformed = self._makeDataset(\n        mixed_inputter,\n        data_file,\n        data_config={""1_vocabulary"": vocab_file, ""2_vocabulary"": vocab_alt_file},\n        shapes={""char_ids"": [None, None, None], ""ids"": [None, None], ""length"": [None]})\n    self.assertAllEqual([1, 3, 15], transformed.shape)\n\n  def testSequenceRecord(self):\n    vector = np.array([[0.2, 0.3], [0.4, 0.5]], dtype=np.float32)\n\n    record_file = os.path.join(self.get_temp_dir(), ""data.records"")\n    record_inputter.create_sequence_records([vector], record_file)\n\n    inputter = record_inputter.SequenceRecordInputter(2)\n    features, transformed = self._makeDataset(\n        inputter,\n        record_file,\n        shapes={""tensor"": [None, None, 2], ""length"": [None]})\n\n    self.assertEqual([2], features[""length""])\n    self.assertAllEqual([vector], features[""tensor""])\n    self.assertAllEqual([vector], transformed)\n\n  def testSequenceRecordWithCompression(self):\n    vector = np.array([[0.2, 0.3], [0.4, 0.5]], dtype=np.float32)\n    compression = ""GZIP""\n    record_file = os.path.join(self.get_temp_dir(), ""data.records"")\n    record_file = record_inputter.create_sequence_records(\n        [vector], record_file, compression=compression)\n    inputter = record_inputter.SequenceRecordInputter(2)\n    dataset = inputter.make_inference_dataset(record_file, batch_size=1)\n    iterator = iter(dataset)\n    self.assertAllEqual(next(iterator)[""tensor""].numpy()[0], vector)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/losses_test.py,7,"b'from parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.utils import losses\n\n\nclass LossesTest(tf.test.TestCase):\n\n  @parameterized.expand([\n      [""l1"", 1e-4],\n      [""L1"", 1e-4],\n      [""l1"", 1],\n      [""l2"", 1e-4],\n      [""l1_l2"", (1e-4, 1e-4)],\n  ])\n  def testRegularization(self, type, scale):\n    layer = tf.keras.layers.Dense(256)\n    layer.build([None, 128])\n    regularization = losses.regularization_penalty(\n        type, scale, layer.trainable_variables)\n    self.assertEqual(0, len(regularization.shape))\n    self.evaluate(regularization)\n\n  def testRegulaizationInvalidType(self):\n    with self.assertRaises(ValueError):\n      losses.regularization_penalty(""l3"", 1e-4, [])\n\n  def testRegulaizationMissingScaleValue(self):\n    with self.assertRaises(ValueError):\n      losses.regularization_penalty(""l1_l2"", 1e-4, [])\n\n  @parameterized.expand([\n      [""ce"", False],\n      [""mse"", False],\n      [""mse"", True],\n  ])\n  def testGuidedAlignmentCostUnderDistributionStrategy(self, cost_type, with_length):\n    strategy = tf.distribute.MirroredStrategy(devices=[""/cpu:0""])\n    attention_probs = tf.random.uniform([2, 5, 6])\n    gold_alignment = tf.random.uniform([2, 5, 6])\n    if with_length:\n      sequence_length = tf.constant([4, 5], dtype=tf.int32)\n    else:\n      sequence_length = None\n    with strategy.scope():\n      losses.guided_alignment_cost(\n          attention_probs,\n          gold_alignment,\n          sequence_length=sequence_length,\n          cost_type=cost_type)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/lr_schedules_test.py,6,"b'import tensorflow as tf\n\nfrom opennmt.schedules import lr_schedules\n\n\nclass _IdentitySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n\n  def __call__(self, step):\n    return step\n\n\nclass LRSchedulesTest(tf.test.TestCase):\n\n  def _testSchedule(self, schedule, expected_values):\n    for i, expected_value in enumerate(expected_values):\n      step = tf.constant(i, dtype=tf.int64)\n      value = schedule(step)\n      self.assertEqual(self.evaluate(value), expected_value)\n\n  def _testNoError(self, schedule):\n    step = tf.constant(1, dtype=tf.int64)\n    schedule(step)\n\n  def testGetScheduleClass(self):\n    with self.assertRaises(ValueError):\n      lr_schedules.get_lr_schedule_class(""ScheduleWrapper"")\n    lr_schedules.get_lr_schedule_class(""NoamDecay"") == lr_schedules.NoamDecay\n\n  def testMakeSchedule(self):\n    wrapper = lr_schedules.make_learning_rate_schedule(\n        2.0, ""ExponentialDecay"", dict(decay_steps=1000, decay_rate=0.7))\n    self.assertIsInstance(wrapper.schedule, tf.keras.optimizers.schedules.ExponentialDecay)\n    wrapper = lr_schedules.make_learning_rate_schedule(\n        2.0, ""NoamDecay"", dict(model_dim=512, warmup_steps=4000))\n    self.assertIsInstance(wrapper.schedule, lr_schedules.NoamDecay)\n    with self.assertRaises(ValueError):\n      lr_schedules.make_learning_rate_schedule(2.0, ""InvalidScheduleName"")\n\n  def testScheduleWrapper(self):\n    self._testSchedule(\n        lr_schedules.ScheduleWrapper(_IdentitySchedule()),\n        [0, 1, 2, 3, 4])\n    self._testSchedule(\n        lr_schedules.ScheduleWrapper(_IdentitySchedule(), step_start=2),\n        [0, 0, 0, 1, 2, 3])\n    self._testSchedule(\n        lr_schedules.ScheduleWrapper(_IdentitySchedule(), step_duration=2),\n        [0, 0, 1, 1, 2, 2])\n    self._testSchedule(\n        lr_schedules.ScheduleWrapper(_IdentitySchedule(), minimum_learning_rate=2),\n        [2, 2, 2, 3, 4, 5])\n\n  def testNoamDecay(self):\n    self._testNoError(lr_schedules.NoamDecay(2.0, 512, 4000))\n  def testRsqrtDecay(self):\n    self._testNoError(lr_schedules.RsqrtDecay(2.0, 4000))\n  def testCosineAnnealing(self):\n    self._testNoError(lr_schedules.CosineAnnealing(2.5e-4, max_step=1000000, warmup_steps=4000))\n  def testRNMTPlusDecay(self):\n    self._testNoError(lr_schedules.RNMTPlusDecay(1.0, 2))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/misc_test.py,15,"b'import tensorflow as tf\nimport numpy as np\n\nfrom opennmt import models\nfrom opennmt.utils import misc\n\n\nclass MiscTest(tf.test.TestCase):\n\n  def testGetVariableName(self):\n\n    class Layer(tf.Module):\n      def __init__(self):\n        super(Layer, self).__init__()\n        self.variable = tf.Variable(0)\n\n    class Model(tf.Module):\n      def __init__(self):\n        super(Model, self).__init__()\n        self.layers = [Layer()]\n\n    model = Model()\n    variable = model.layers[0].variable\n    expected_name = ""model/layers/0/variable/.ATTRIBUTES/VARIABLE_VALUE""\n    variable_name = misc.get_variable_name(variable, model)\n    self.assertEqual(variable_name, expected_name)\n\n    variables_to_names, names_to_variables = misc.get_variables_name_mapping(model, root_key=""model"")\n    self.assertDictEqual(variables_to_names, {variable.ref(): expected_name})\n    self.assertDictEqual(names_to_variables, {expected_name: variable})\n\n  def testSetDropout(self):\n\n    class Layer(tf.keras.layers.Layer):\n\n      def __init__(self):\n        super(Layer, self).__init__()\n        self.dropout = 0.3\n        self.x = tf.keras.layers.Dropout(0.2)\n\n    class Model(tf.keras.layers.Layer):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        self.output_dropout = 0.1\n        self.layer = Layer()\n        self.layers = [Layer(), Layer()]\n\n    model = Model()\n    misc.set_dropout(model, 0.5)\n    self.assertEqual(model.output_dropout, 0.5)\n    self.assertEqual(model.layer.dropout, 0.5)\n    self.assertEqual(model.layer.x.rate, 0.5)\n    self.assertEqual(model.layers[1].dropout, 0.5)\n    self.assertEqual(model.layers[1].x.rate, 0.5)\n\n  def testFormatTranslationOutput(self):\n    self.assertEqual(\n        misc.format_translation_output(""hello world""),\n        ""hello world"")\n    self.assertEqual(\n        misc.format_translation_output(""hello world"", score=42),\n        ""%f ||| hello world"" % 42)\n    self.assertEqual(\n        misc.format_translation_output(""hello world"", score=42, token_level_scores=[24, 64]),\n        ""%f ||| hello world ||| %f %f"" % (42, 24, 64))\n    self.assertEqual(\n        misc.format_translation_output(""hello world"", token_level_scores=[24, 64]),\n        ""hello world ||| %f %f"" % (24, 64))\n    self.assertEqual(\n        misc.format_translation_output(""hello world"", attention=[[0.1, 0.7, 0.2], [0.5, 0.3, 0.2]]),\n        ""hello world"")\n    self.assertEqual(\n        misc.format_translation_output(\n            ""hello world"",\n            attention=np.array([[0.1, 0.7, 0.2], [0.5, 0.3, 0.2]]),\n            alignment_type=""hard""),\n        ""hello world ||| 1-0 0-1"")\n    self.assertEqual(\n        misc.format_translation_output(\n            ""hello world"",\n            attention=np.array([[0.1, 0.7, 0.2], [0.5, 0.3, 0.2]]),\n            alignment_type=""soft""),\n        ""hello world ||| 0.100000 0.700000 0.200000 ; 0.500000 0.300000 0.200000"")\n\n  def testReadSummaries(self):\n    event_dir = self.get_temp_dir()\n    summary_writer = tf.summary.create_file_writer(event_dir)\n    with summary_writer.as_default():\n      tf.summary.scalar(""values/a"", 1, step=0)\n      tf.summary.scalar(""values/b"", 2, step=0)\n      tf.summary.scalar(""values/a"", 3, step=5)\n      tf.summary.scalar(""values/b"", 4, step=5)\n      tf.summary.scalar(""values/a"", 5, step=10)\n      tf.summary.scalar(""values/b"", 6, step=10)\n      summary_writer.flush()\n    summaries = misc.read_summaries(event_dir)\n    self.assertLen(summaries, 3)\n    steps, values = zip(*summaries)\n    self.assertListEqual(list(steps), [0, 5, 10])\n    values = list(values)\n    self.assertDictEqual(values[0], {""values/a"": 1, ""values/b"": 2})\n    self.assertDictEqual(values[1], {""values/a"": 3, ""values/b"": 4})\n    self.assertDictEqual(values[2], {""values/a"": 5, ""values/b"": 6})\n\n  def testEventOrderRestorer(self):\n    events = []\n    restorer = misc.OrderRestorer(\n        index_fn=lambda x: x[0],\n        callback_fn=lambda x: events.append(x))\n    self.assertFalse(restorer.push((2, ""toto"")))\n    self.assertFalse(restorer.push((1, ""tata"")))\n    self.assertFalse(restorer.push((3, ""foo"")))\n    self.assertTrue(restorer.push((0, ""bar"")))\n    self.assertTrue(restorer.push((4, ""titi"")))\n    with self.assertRaises(ValueError):\n      restorer.push((2, ""invalid""))\n    self.assertEqual(len(events), 5)\n    self.assertTupleEqual(events[0], (0, ""bar""))\n    self.assertTupleEqual(events[1], (1, ""tata""))\n    self.assertTupleEqual(events[2], (2, ""toto""))\n    self.assertTupleEqual(events[3], (3, ""foo""))\n    self.assertTupleEqual(events[4], (4, ""titi""))\n\n  def testClassRegistry(self):\n    registry = misc.ClassRegistry(base_class=models.Model)\n    self.assertIsNone(registry.get(""TransformerBig""))\n    registry.register(models.TransformerBig)\n    self.assertEqual(registry.get(""TransformerBig""), models.TransformerBig)\n    registry.register(models.TransformerBig, name=""TransformerLarge"")\n    self.assertEqual(registry.get(""TransformerLarge""), models.TransformerBig)\n    self.assertSetEqual(registry.class_names, set([""TransformerBig"", ""TransformerLarge""]))\n\n    registry.register(models.TransformerBaseRelative, alias=""TransformerRelative"")\n    self.assertEqual(registry.get(""TransformerBaseRelative""), models.TransformerBaseRelative)\n    self.assertEqual(registry.get(""TransformerRelative""), models.TransformerBaseRelative)\n\n    with self.assertRaises(ValueError):\n      registry.register(models.TransformerBig)\n    with self.assertRaises(TypeError):\n      registry.register(misc.ClassRegistry)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/model_test.py,35,"b'# -*- coding: utf-8 -*-\n\nimport os\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt import decoders\nfrom opennmt import encoders\nfrom opennmt import inputters\nfrom opennmt import models\nfrom opennmt.tests import test_util\nfrom opennmt.utils import misc\n\n\ndef _seq2seq_model(training=None):\n  model = models.SequenceToSequence(\n      inputters.WordEmbedder(16),\n      inputters.WordEmbedder(16),\n      encoders.SelfAttentionEncoder(2, 16, 4, 32),\n      decoders.SelfAttentionDecoder(2, 16, 4, 32))\n  params = {}\n  if training:\n    params[""optimizer""] = ""SGD""\n    params[""learning_rate""] = 0.1\n  return model, params\n\n\nclass ModelTest(tf.test.TestCase):\n\n  def _makeToyEnDeData(self, with_alignments=False):\n    data_config = {}\n    features_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src.txt""),\n        [""Parliament Does Not Support Amendment Freeing Tymoshenko"",\n         ""Today , the Ukraine parliament dismissed , within the Code of Criminal Procedure ""\n         ""amendment , the motion to revoke an article based on which the opposition leader , ""\n         ""Yulia Tymoshenko , was sentenced ."",\n         ""The amendment that would lead to freeing the imprisoned former Prime Minister was ""\n         ""revoked during second reading of the proposal for mitigation of sentences for ""\n         ""economic offences .""])\n    labels_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""tgt.txt""),\n        [""Keine befreiende Novelle f\xc3\xbcr Tymoshenko durch das Parlament"",\n         ""Das ukrainische Parlament verweigerte heute den Antrag , im Rahmen einer Novelle ""\n         ""des Strafgesetzbuches denjenigen Paragrafen abzuschaffen , auf dessen Grundlage die ""\n         ""Oppositionsf\xc3\xbchrerin Yulia Timoshenko verurteilt worden war ."",\n         ""Die Neuregelung , die den Weg zur Befreiung der inhaftierten Expremierministerin h\xc3\xa4tte ""\n         ""ebnen k\xc3\xb6nnen , lehnten die Abgeordneten bei der zweiten Lesung des Antrags auf Milderung ""\n         ""der Strafen f\xc3\xbcr wirtschaftliche Delikte ab .""])\n    data_config[""source_vocabulary""] = test_util.make_vocab_from_file(\n        os.path.join(self.get_temp_dir(), ""src_vocab.txt""), features_file)\n    data_config[""target_vocabulary""] = test_util.make_vocab_from_file(\n        os.path.join(self.get_temp_dir(), ""tgt_vocab.txt""), labels_file)\n    if with_alignments:\n      # Dummy and incomplete alignments.\n      data_config[""train_alignments""] = test_util.make_data_file(\n          os.path.join(self.get_temp_dir(), ""aligne.txt""),\n          [""0-0 1-0 2-2 3-4 4-4 5-6"",\n           ""0-1 1-1 1-3 2-3 4-4"",\n           ""0-0 1-0 2-2 3-4 4-4 5-6""])\n    return features_file, labels_file, data_config\n\n  def _makeToyLMData(self):\n    features_file, _, data_config = self._makeToyEnDeData()\n    return features_file, {""vocabulary"": data_config[""source_vocabulary""]}\n\n  def _makeToyTaggerData(self):\n    data_config = {}\n    features_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src.txt""),\n        [""M . Smith went to Washington ."",\n         ""I live in New Zealand .""])\n    labels_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""labels.txt""),\n        [""B-PER I-PER E-PER O O S-LOC O"",\n         ""O O O B-LOC E-LOC O""])\n    data_config[""source_vocabulary""] = test_util.make_vocab_from_file(\n        os.path.join(self.get_temp_dir(), ""src_vocab.txt""), features_file)\n    data_config[""target_vocabulary""] = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""labels_vocab.txt""),\n        [""O"", ""B-LOC"", ""I-LOC"", ""E-LOC"", ""S-LOC"", ""B-PER"", ""I-PER"", ""E-PER"", ""S-PER""])\n    return features_file, labels_file, data_config\n\n  def _makeToyClassifierData(self):\n    data_config = {}\n    features_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src.txt""),\n        [""This product was not good at all , it broke on the first use !"",\n         ""Perfect , it does everything I need ."",\n         ""How do I change the battery ?""])\n    labels_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""labels.txt""), [""negative"", ""positive"", ""neutral""])\n    data_config[""source_vocabulary""] = test_util.make_vocab_from_file(\n        os.path.join(self.get_temp_dir(), ""src_vocab.txt""), features_file)\n    data_config[""target_vocabulary""] = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""labels_vocab.txt""), [""negative"", ""positive"", ""neutral""])\n    return features_file, labels_file, data_config\n\n  def _testGenericModel(self,\n                        model,\n                        mode,\n                        features_file,\n                        labels_file=None,\n                        data_config=None,\n                        batch_size=16,\n                        prediction_heads=None,\n                        metrics=None,\n                        params=None):\n    # Mainly test that the code does not throw.\n    if params is None:\n      params = model.auto_config()[""params""]\n    if data_config is None:\n      data_config = {}\n    model.initialize(data_config, params=params)\n    model.create_variables()\n    # Build a dataset for mode.\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      dataset = model.examples_inputter.make_inference_dataset(\n          features_file, batch_size)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n      dataset = model.examples_inputter.make_evaluation_dataset(\n          features_file, labels_file, batch_size)\n    elif mode == tf.estimator.ModeKeys.TRAIN:\n      dataset = model.examples_inputter.make_training_dataset(\n          features_file, labels_file, batch_size)\n    # Forward first batch into the model.\n    data = iter(dataset).next()\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      features, labels = data\n    else:\n      features, labels = data, None\n    training = mode == tf.estimator.ModeKeys.TRAIN\n    outputs, predictions = model(features, labels=labels, training=training)\n    if mode != tf.estimator.ModeKeys.PREDICT:\n      loss = model.compute_loss(outputs, labels, training=training)\n      if mode == tf.estimator.ModeKeys.EVAL:\n        # Check that returned evaluation metrics are expected.\n        eval_metrics = model.get_metrics()\n        if eval_metrics is not None:\n          model.update_metrics(eval_metrics, predictions, labels)\n          for metric in metrics:\n            self.assertIn(metric, eval_metrics)\n        try:\n          # Check that scores can be computed and printed without errors.\n          scores = model.score(features, labels)\n          first_score = tf.nest.map_structure(\n              lambda x: x.numpy(),\n              next(misc.extract_batches(scores)))\n          with open(os.devnull, ""w"") as devnull:\n            model.print_score(first_score, stream=devnull)\n        except NotImplementedError:\n          pass\n    else:\n      # Check that all prediction heads are returned.\n      self.assertIsInstance(predictions, dict)\n      if prediction_heads is not None:\n        for head in prediction_heads:\n          self.assertIn(head, predictions)\n      # Check that the prediction can be printed without errors.\n      first_prediction = tf.nest.map_structure(\n          lambda x: x.numpy(),\n          next(misc.extract_batches(predictions)))\n      with open(os.devnull, ""w"") as devnull:\n        model.print_prediction(first_prediction, stream=devnull)\n\n  @parameterized.expand([\n      [tf.estimator.ModeKeys.TRAIN],\n      [tf.estimator.ModeKeys.EVAL],\n      [tf.estimator.ModeKeys.PREDICT]])\n  def testSequenceToSequence(self, mode):\n    model, params = _seq2seq_model(mode)\n    features_file, labels_file, data_config = self._makeToyEnDeData()\n    self._testGenericModel(\n        model,\n        mode,\n        features_file,\n        labels_file,\n        data_config,\n        prediction_heads=[""tokens"", ""length"", ""log_probs""],\n        params=params)\n\n  def testSequenceToSequenceWithSharedEmbedding(self):\n    model = models.SequenceToSequence(\n        inputters.WordEmbedder(16),\n        inputters.WordEmbedder(16),\n        encoders.SelfAttentionEncoder(2, 16, 4, 32),\n        decoders.SelfAttentionDecoder(2, 16, 4, 32),\n        share_embeddings=models.EmbeddingsSharingLevel.ALL)\n    _, _, data_config = self._makeToyEnDeData()\n    data_config[""target_vocabulary""] = data_config[""source_vocabulary""]\n    model.initialize(data_config)\n    self.assertTrue(model.decoder.initialized)\n    model.build(None)\n    self.assertEqual(\n        model.labels_inputter.embedding.ref(),\n        model.decoder.output_layer.weight.ref())\n\n  @parameterized.expand([\n      [tf.estimator.ModeKeys.EVAL],\n      [tf.estimator.ModeKeys.PREDICT]])\n  def testSequenceToSequenceWithInGraphTokenizer(self, mode):\n    model, params = _seq2seq_model(mode)\n    features_file, labels_file, data_config = self._makeToyEnDeData()\n    tokenization_config = {""type"": ""SpaceTokenizer""}\n    data_config[""source_tokenization""] = tokenization_config\n    data_config[""target_tokenization""] = tokenization_config\n    self._testGenericModel(\n        model,\n        mode,\n        features_file,\n        labels_file,\n        data_config,\n        prediction_heads=[""text"", ""log_probs""],\n        params=params)\n\n  @parameterized.expand([[""ce""], [""mse""]])\n  def testSequenceToSequenceWithGuidedAlignment(self, ga_type):\n    model, params = _seq2seq_model(training=True)\n    params[""guided_alignment_type""] = ga_type\n    features_file, labels_file, data_config = self._makeToyEnDeData(with_alignments=True)\n    model.initialize(data_config, params=params)\n    model.create_variables()\n    dataset = model.examples_inputter.make_training_dataset(features_file, labels_file, 16)\n    features, labels = next(iter(dataset))\n    self.assertIn(""alignment"", labels)\n    outputs, _ = model(features, labels=labels, training=True)\n    loss = model.compute_loss(outputs, labels, training=True)\n    loss = loss[0] / loss[1]\n\n  def testSequenceToSequenceWithGuidedAlignmentAndWeightedDataset(self):\n    model, _ = _seq2seq_model()\n    features_file, labels_file, data_config = self._makeToyEnDeData(with_alignments=True)\n    model.initialize(data_config)\n    with self.assertRaisesRegex(ValueError, ""expected to match""):\n      model.examples_inputter.make_training_dataset(\n          [features_file, features_file], [labels_file, labels_file], 16)\n    data_config[""train_alignments""] = [\n        data_config[""train_alignments""], data_config[""train_alignments""]]\n    model.initialize(data_config)\n    dataset = model.examples_inputter.make_training_dataset(\n        [features_file, features_file], [labels_file, labels_file], 16)\n    self.assertIsInstance(dataset, tf.data.Dataset)\n\n  def testSequenceToSequenceWithReplaceUnknownTarget(self):\n    model, params = _seq2seq_model()\n    params[""replace_unknown_target""] = True\n    features_file, labels_file, data_config = self._makeToyEnDeData()\n    model.initialize(data_config, params=params)\n    dataset = model.examples_inputter.make_inference_dataset(features_file, 16)\n    features = next(iter(dataset))\n    _, predictions = model(features)\n\n  def testSequenceToSequenceWithScheduledSampling(self):\n    model = models.SequenceToSequence(\n        inputters.WordEmbedder(16),\n        inputters.WordEmbedder(16),\n        encoders.SelfAttentionEncoder(2, 16, 4, 32),\n        decoders.RNNDecoder(2, 16))\n    params = {\n        ""scheduled_sampling_type"": ""linear"",\n        ""scheduled_sampling_read_probability"": 0.8,\n        ""scheduled_sampling_k"": 0.1\n    }\n    features_file, labels_file, data_config = self._makeToyEnDeData()\n    model.initialize(data_config, params=params)\n    dataset = model.examples_inputter.make_training_dataset(features_file, labels_file, 16)\n    features, labels = next(iter(dataset))\n    with self.assertRaises(ValueError):\n      model(features, labels=labels, training=True)  # step argument is required.\n    outputs, _ = model(features, labels=labels, training=True, step=10)\n    self.assertEqual(outputs[""logits""].shape[1], labels[""ids""].shape[1])\n\n  def testSequenceToSequenceWithContrastiveLearning(self):\n    model, params = _seq2seq_model()\n    params[""contrastive_learning""] = True\n    features_file, labels_file, data_config = self._makeToyEnDeData()\n    model.initialize(data_config, params=params)\n    dataset = model.examples_inputter.make_training_dataset(features_file, labels_file, 16)\n    features, labels = next(iter(dataset))\n    self.assertIn(""noisy_ids"", labels)\n    self.assertIn(""noisy_ids_out"", labels)\n    self.assertIn(""noisy_length"", labels)\n    outputs, _ = model(features, labels=labels, training=True)\n    self.assertIn(""noisy_logits"", outputs)\n    loss = model.compute_loss(outputs, labels, training=True)\n    self.assertGreaterEqual(self.evaluate(loss), 0)\n\n  def testSequenceToSequenceServing(self):\n    # Test that serving features can be forwarded into the model.\n    _, _, data_config = self._makeToyEnDeData()\n    model, params = _seq2seq_model()\n    model.initialize(data_config, params=params)\n    function = model.serve_function()\n    function.get_concrete_function()\n\n  @parameterized.expand([\n      [tf.estimator.ModeKeys.TRAIN],\n      [tf.estimator.ModeKeys.EVAL],\n      [tf.estimator.ModeKeys.PREDICT]])\n  def testLanguageModel(self, mode):\n    # Mainly test that the code does not throw.\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=16, num_heads=4, ffn_inner_dim=32, num_sources=0)\n    model = models.LanguageModel(decoder, embedding_size=16)\n    features_file, data_config = self._makeToyLMData()\n    params = {\n        ""optimizer"": ""SGD"",\n        ""learning_rate"": 0.1}\n    self._testGenericModel(\n        model,\n        mode,\n        features_file,\n        data_config=data_config,\n        batch_size=1 if mode == tf.estimator.ModeKeys.PREDICT else 16,\n        prediction_heads=[""tokens"", ""length""],\n        params=params)\n\n  def testLanguageModelServing(self):\n    _, data_config = self._makeToyLMData()\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=16, num_heads=4, ffn_inner_dim=32, num_sources=0)\n    model = models.LanguageModel(decoder, embedding_size=16)\n    model.initialize(data_config)\n    function = model.serve_function()\n    function.get_concrete_function()\n\n  def testLanguageModelInputter(self):\n    vocabulary_path = test_util.make_vocab(\n        os.path.join(self.get_temp_dir(), ""vocab.txt""), [""a"", ""b"", ""c""])\n\n    inputter = models.LanguageModelInputter(embedding_size=10)\n    inputter.initialize({\n        ""vocabulary"": vocabulary_path,\n        ""sequence_controls"": {""start"": True, ""end"": False}})\n    features, labels = self.evaluate(inputter.make_features(tf.constant(""a b c"")))\n    self.assertAllEqual(features[""ids""], [1, 3, 4, 5])\n    self.assertEqual(features[""length""], 4)\n    self.assertAllEqual(labels[""ids""], [1, 3, 4])\n    self.assertAllEqual(labels[""ids_out""], [3, 4, 5])\n    self.assertEqual(labels[""length""], 3)\n\n    # Backward compatibility mode.\n    inputter = models.LanguageModelInputter(embedding_size=10)\n    inputter.initialize({""vocabulary"": vocabulary_path})\n    features, labels = self.evaluate(inputter.make_features(tf.constant(""a b c"")))\n    self.assertAllEqual(features[""ids""], [3, 4, 5])\n    self.assertEqual(features[""length""], 3)\n    self.assertAllEqual(labels[""ids""], [3, 4, 5])\n    self.assertAllEqual(labels[""ids_out""], [4, 5, 2])\n    self.assertEqual(labels[""length""], 3)\n\n  def testLanguageModelWithMissingStart(self):\n    _, data_config = self._makeToyLMData()\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=16, num_heads=4, ffn_inner_dim=32, num_sources=0)\n    model = models.LanguageModel(decoder, embedding_size=16)\n    model.initialize(data_config)\n    features, _ = model.features_inputter.make_features(tf.constant(""""))\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      model(features)\n\n  def testLanguageModelWithStartOfSentence(self):\n    _, data_config = self._makeToyLMData()\n    data_config[""sequence_controls""] = dict(start=True, end=False)\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=16, num_heads=4, ffn_inner_dim=32, num_sources=0)\n    model = models.LanguageModel(decoder, embedding_size=16)\n    model.initialize(data_config, params={""maximum_decoding_length"": 1})\n    features, _ = model.features_inputter.make_features(tf.constant(""""))\n    features = tf.nest.map_structure(lambda t: tf.expand_dims(t, 0), features)  # Add batch dim.\n    _, predictions = self.evaluate(model(features))\n    # Predictions should not include the leading <s>.\n    self.assertEqual(predictions[""length""][0], 1)\n    self.assertTupleEqual(predictions[""tokens""].shape, (1, 1))\n\n  @parameterized.expand([\n      [tf.estimator.ModeKeys.TRAIN],\n      [tf.estimator.ModeKeys.EVAL],\n      [tf.estimator.ModeKeys.PREDICT]])\n  def testSequenceTagger(self, mode):\n    model = models.SequenceTagger(\n        inputters.WordEmbedder(10),\n        encoders.MeanEncoder(),\n        crf_decoding=True)\n    features_file, labels_file, data_config = self._makeToyTaggerData()\n    data_config[""tagging_scheme""] = ""bioes""\n    params = {\n        ""optimizer"": ""SGD"",\n        ""learning_rate"": 0.1}\n    self._testGenericModel(\n        model,\n        mode,\n        features_file,\n        labels_file,\n        data_config,\n        prediction_heads=[""tags"", ""length""],\n        metrics=[""accuracy"", ""precision"", ""recall"", ""f1""],\n        params=params)\n\n  @parameterized.expand([\n      [tf.estimator.ModeKeys.TRAIN],\n      [tf.estimator.ModeKeys.EVAL],\n      [tf.estimator.ModeKeys.PREDICT]])\n  def testSequenceClassifier(self, mode):\n    model = models.SequenceClassifier(inputters.WordEmbedder(10), encoders.MeanEncoder())\n    features_file, labels_file, data_config = self._makeToyClassifierData()\n    params = {\n        ""optimizer"": ""SGD"",\n        ""learning_rate"": 0.1}\n    self._testGenericModel(\n        model,\n        mode,\n        features_file,\n        labels_file,\n        data_config,\n        prediction_heads=[""classes""],\n        metrics=[""accuracy""],\n        params=params)\n\n  def testSequenceClassifierWithSelfAttentionEncoder(self):\n    # SelfAttentionEncoder does not return a state, so test that the classifier\n    # does not crash on this.\n    model = models.SequenceClassifier(\n        inputters.WordEmbedder(10),\n        encoders.SelfAttentionEncoder(num_layers=2, num_units=16, num_heads=4, ffn_inner_dim=32))\n    features_file, labels_file, data_config = self._makeToyClassifierData()\n    model.initialize(data_config)\n    dataset = model.examples_inputter.make_training_dataset(features_file, labels_file, 16)\n    features, labels = iter(dataset).next()\n    model(features, labels, training=True)\n\n  def testCreateVariables(self):\n    _, _, data_config = self._makeToyEnDeData()\n    model, params = _seq2seq_model()\n    model.initialize(data_config, params=params)\n    model.create_variables()\n    self.assertTrue(len(model.trainable_variables) > 0)\n\n  def testCreateVariablesLanguageModel(self):\n    _, data_config = self._makeToyLMData()\n    decoder = decoders.SelfAttentionDecoder(\n        2, num_units=16, num_heads=4, ffn_inner_dim=32, num_sources=0)\n    model = models.LanguageModel(decoder, embedding_size=16)\n    model.initialize(data_config)\n    model.create_variables()\n    self.assertTrue(len(model.trainable_variables) > 0)\n\n  def testInitializeWithDropoutOverride(self):\n    model = models.SequenceToSequence(\n        inputters.WordEmbedder(16),\n        inputters.WordEmbedder(16),\n        encoders.SelfAttentionEncoder(2, 16, 4, 32),\n        decoders.SelfAttentionDecoder(2, 16, 4, 32))\n    self.assertEqual(model.encoder.dropout, 0.1)\n    _, _, data_config = self._makeToyClassifierData()\n    params = dict(dropout=0.3)\n    model.initialize(data_config, params=params)\n    self.assertEqual(model.encoder.dropout, 0.3)\n\n  def testFreezeLayers(self):\n    model, _ = _seq2seq_model(training=True)\n    params = {""freeze_layers"": [""decoder/output_layer"", ""encoder/layers/0""]}\n    _, _, data_config = self._makeToyEnDeData()\n    model.initialize(data_config, params=params)\n    model.create_variables()\n    trainable_variables = model.trainable_variables\n    self.assertNotEmpty(trainable_variables)\n    trainable_variables_ref = set(variable.ref() for variable in trainable_variables)\n\n    def _assert_layer_not_trainable(layer):\n      self.assertFalse(layer.trainable)\n      for variable in layer.variables:\n        self.assertNotIn(variable.ref(), trainable_variables_ref)\n\n    _assert_layer_not_trainable(model.decoder.output_layer)\n    _assert_layer_not_trainable(model.encoder.layers[0])\n    self.assertEqual(model.encoder.layers[0].ffn.output_dropout, 0)\n    self.assertEqual(model.encoder.layers[0].self_attention.output_dropout, 0)\n\n  def testTransferWeightsNewVocab(self):\n\n    def _make_model(name, src_vocab, tgt_vocab, random_slots=False):\n      model, _ = _seq2seq_model(training=True)\n      optimizer = tf.keras.optimizers.Adam()\n      data = {}\n      data[""source_vocabulary""] = test_util.make_data_file(\n          os.path.join(self.get_temp_dir(), ""%s-src-vocab.txt"" % name),\n          src_vocab)\n      data[""target_vocabulary""] = test_util.make_data_file(\n          os.path.join(self.get_temp_dir(), ""%s-tgt-vocab.txt"" % name),\n          tgt_vocab)\n      model.initialize(data)\n      model.create_variables(optimizer=optimizer)\n      if random_slots:\n        for variable in model.trainable_variables:\n          for slot_name in optimizer.get_slot_names():\n            slot = optimizer.get_slot(variable, slot_name)\n            slot.assign(tf.random.uniform(slot.shape))\n      return model, optimizer\n\n    model_a, optimizer_a = _make_model(\n        ""a"", [""a"", ""b"", ""c"", ""d"", ""e""], [""1"", ""2"", ""3"", ""4"", ""5"", ""6""], random_slots=True)\n    model_b, optimizer_b = _make_model(\n        ""b"", [""c"", ""a"", ""e"", ""f""], [""1"", ""3"", ""2"", ""6"", ""7""])\n    src_mapping = [2, 0, 4, -1]\n    tgt_mapping = [0, 2, 1, 5, -1]\n\n    def _check_weight(weight_a, weight_b, mapping, vocab_axis=0):\n      weight_a = self.evaluate(weight_a)\n      weight_b = self.evaluate(weight_b)\n      if vocab_axis != 0:\n        perm = list(range(len(weight_a.shape)))\n        perm[0], perm[vocab_axis] = perm[vocab_axis], perm[0]\n        weight_a = np.transpose(weight_a, axes=perm)\n        weight_b = np.transpose(weight_b, axes=perm)\n      self.assertEqual(weight_b.shape[0], len(mapping) + 1)\n      for index_b, index_a in enumerate(mapping):\n        if index_a >= 0:\n          self.assertAllEqual(weight_b[index_b], weight_a[index_a])\n\n    def _check_weight_and_slots(weight_fn, mapping, vocab_axis=0):\n      weight_a = weight_fn(model_a)\n      weight_b = weight_fn(model_b)\n      _check_weight(weight_a, weight_b, mapping, vocab_axis=vocab_axis)\n      for slot_name in optimizer_b.get_slot_names():\n        slot_a = optimizer_a.get_slot(weight_a, slot_name)\n        slot_b = optimizer_b.get_slot(weight_b, slot_name)\n        _check_weight(slot_a, slot_b, mapping, vocab_axis=vocab_axis)\n\n    model_a.transfer_weights(model_b, new_optimizer=optimizer_b, optimizer=optimizer_a)\n    _check_weight_and_slots(\n        lambda model: model.features_inputter.embedding, src_mapping)\n    _check_weight_and_slots(\n        lambda model: model.labels_inputter.embedding, tgt_mapping)\n    _check_weight_and_slots(\n        lambda model: model.decoder.output_layer.bias, tgt_mapping)\n    _check_weight_and_slots(\n        lambda model: model.decoder.output_layer.kernel, tgt_mapping, vocab_axis=1)\n\n  @parameterized.expand([\n      [models.TransformerBase()],\n      [models.TransformerBaseRelative()],\n      [models.TransformerBig()],\n      [models.TransformerBigRelative()],\n      [models.Transformer(\n          inputters.WordEmbedder(32),\n          inputters.WordEmbedder(32),\n          num_layers=(6, 3),\n          num_units=32,\n          num_heads=8,\n          ffn_inner_dim=64)],\n  ])\n  def testCTranslate2Spec(self, model):\n    try:\n      self.assertIsNotNone(model.ctranslate2_spec)\n    except ImportError:\n      self.skipTest(""ctranslate2 module is not available"")\n\n  def testTransformerWithDifferentEncoderDecoderLayers(self):\n    model = models.Transformer(\n        inputters.WordEmbedder(32),\n        inputters.WordEmbedder(32),\n        num_layers=(6, 3),\n        num_units=32,\n        num_heads=8,\n        ffn_inner_dim=64)\n    self.assertLen(model.encoder.layers, 6)\n    self.assertLen(model.decoder.layers, 3)\n\n  def testBeamSearchWithMultiSourceEncoder(self):\n    shared_vocabulary = test_util.make_vocab(\n        os.path.join(self.get_temp_dir(), ""vocab.txt""), [""1"", ""2"", ""3""])\n    data_config = {\n        ""source_1_vocabulary"": shared_vocabulary,\n        ""source_2_vocabulary"": shared_vocabulary,\n        ""target_vocabulary"": shared_vocabulary,\n    }\n    params = {\n        ""beam_width"": 2,\n    }\n    model = models.Transformer(\n        inputters.ParallelInputter([\n            inputters.WordEmbedder(32),\n            inputters.WordEmbedder(32)]),\n        inputters.WordEmbedder(32),\n        num_layers=3,\n        num_units=32,\n        num_heads=8,\n        ffn_inner_dim=64)\n    model.initialize(data_config, params=params)\n    model.serve_function().get_concrete_function()\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/noise_test.py,11,"b'# -*- coding: utf-8 -*-\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.data import noise\n\n\nclass NoiseTest(tf.test.TestCase):\n\n  @parameterized.expand([\n    [[""a"", ""b"", ""c"", ""e"", ""f""]],\n    [[[""a"", ""b"", """"], [""c"", ""e"", ""f""]]],\n  ])\n  def testWordDropoutNone(self, words):\n    x = tf.constant(words)\n    y = noise.WordDropout(0)(x)\n    x, y = self.evaluate([x, y])\n    self.assertAllEqual(x, y)\n\n  @parameterized.expand([\n    [[]],\n    [[""a"", ""b"", ""c"", ""e"", ""f""]],\n    [[[""a"", ""b"", """"], [""c"", ""e"", ""f""]]],\n  ])\n  def testWordDropoutAll(self, words):\n    x = tf.constant(words, dtype=tf.string)\n    y = noise.WordDropout(1)(x)\n    y = self.evaluate(y)\n    self.assertEqual(y.shape[0], 1 if words else 0)  # At least one is not dropped.\n\n  @parameterized.expand([[1], [2], [4], [5]])\n  def testWordOmission(self, count):\n    words = [[""a"", ""b"", """"], [""c"", """", """"], [""d"", ""e"", ""f""], [""g"", """", """"]]\n    x = tf.constant(words, dtype=tf.string)\n    y = noise.WordOmission(count)(x)\n    y = self.evaluate(y)\n    expected_omit_count = min(count, len(words) - 1)\n    self.assertEqual(y.shape[0], len(words) - expected_omit_count)\n\n  @parameterized.expand([\n    [[""a"", ""b"", ""c""], [""d"", ""d"", ""d""]],\n    [[[""a"", ""b"", """"], [""c"", ""e"", ""f""]], [[""d"", """", """"], [""d"", """", """"]]],\n  ])\n  def testWordReplacement(self, words, expected):\n    expected = tf.constant(expected)\n    words = tf.constant(words)\n    words = noise.WordReplacement(1, filler=""d"")(words)\n    words, expected = self.evaluate([words, expected])\n    self.assertAllEqual(words, expected)\n\n  @parameterized.expand([[0], [1], [3]])\n  def testWordPermutation(self, k):\n    x = tf.constant(""0 1 2 3 4 5 6 7 8 9 10 11 12 13"".split())\n    y = noise.WordPermutation(k)(x)\n    x, y = self.evaluate([x, y])\n    if k == 0:\n      self.assertAllEqual(y, x)\n    else:\n      for i, v in enumerate(y.tolist()):\n        self.assertLess(abs(int(v) - i), k)\n\n  @parameterized.expand([\n      [True, [[""a\xef\xbf\xad"", ""b"", ""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xade""], [""a"", ""b"", ""c"", """", """"]], [5, 3]],\n      [False, [[""a\xef\xbf\xad"", ""b"", ""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xade""], [""a"", ""b"", ""c"", """", """"]], [5, 3]],\n      [False, [""a\xef\xbf\xad"", ""b"", ""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xade""], None]\n  ])\n  def testWordNoising(self, as_function, tokens, lengths):\n    tokens = tf.constant(tokens)\n    if lengths is not None:\n      lengths = tf.constant(lengths, dtype=tf.int32)\n    noiser = noise.WordNoiser()\n    noiser.add(noise.WordDropout(0.1))\n    noiser.add(noise.WordReplacement(0.1))\n    noiser.add(noise.WordPermutation(3))\n    noiser_fn = tf.function(noiser) if as_function else noiser\n    noisy_tokens, noisy_lengths = noiser_fn(tokens, sequence_length=lengths, keep_shape=True)\n    tokens, noisy_tokens = self.evaluate([tokens, noisy_tokens])\n    self.assertAllEqual(noisy_tokens.shape, tokens.shape)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/optimizer_test.py,15,"b'import tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow_addons.optimizers.weight_decay_optimizers import DecoupledWeightDecayExtension\n\nfrom opennmt.optimizers import utils\nfrom opennmt.tests import test_util\n\n\nclass OptimizerTest(tf.test.TestCase):\n\n  def testMakeLazyAdam(self):\n    lazy_adam = utils.make_optimizer(""LazyAdam"", 0.002, beta_1=0.8)\n    self.assertIsInstance(lazy_adam, tfa.optimizers.LazyAdam)\n    self.assertEqual(lazy_adam.learning_rate, 0.002)\n    self.assertEqual(lazy_adam.beta_1, 0.8)\n\n  def testMakeAdamW(self):\n    adam_w = utils.make_optimizer(""AdamW"", 0.002, weight_decay=0.1)\n    self.assertIsInstance(adam_w, tfa.optimizers.AdamW)\n    adam_w = utils.make_optimizer(""Adam"", 0.002, weight_decay=0.1)\n    self.assertIsInstance(adam_w, tf.keras.optimizers.Adam)\n    self.assertIsInstance(adam_w, DecoupledWeightDecayExtension)\n\n  def testCustomOptimizerRegistration(self):\n\n    @utils.register_optimizer\n    class MyCustomAdam(tf.keras.optimizers.Adam):\n      pass\n\n    optimizer = utils.make_optimizer(""MyCustomAdam"", 0.002)\n    self.assertIsInstance(optimizer, MyCustomAdam)\n\n  def testGradientAccumulator(self):\n    accumulator = utils.GradientAccumulator()\n    accumulator([tf.constant([1.0, 2.0])])\n    accumulator([tf.constant([-2.0, 1.0])])\n    accumulator([tf.constant([-1.0, 2.0])])\n    with self.assertRaises(ValueError):\n      accumulator([tf.constant([1.0, 1.0]), tf.constant([2.0, 2.0])])\n    self.assertEqual(accumulator.step, 3)\n    self.assertEqual(len(accumulator.gradients), 1)\n    self.assertAllEqual(accumulator.gradients[0], [-2.0, 5.0])\n    accumulator.reset()\n    self.assertEqual(accumulator.step, 0)\n    self.assertAllEqual(accumulator.gradients[0], [0.0, 0.0])\n\n  @test_util.run_with_two_cpu_devices\n  def testGradientAccumulatorDistributionStrategy(self):\n    devices = tf.config.list_logical_devices(device_type=""CPU"")\n    strategy = tf.distribute.MirroredStrategy(devices=devices[:2])\n\n    with strategy.scope():\n      accumulator = utils.GradientAccumulator()\n      variable = tf.Variable([4.0, 3.0])\n      sgd = tf.keras.optimizers.SGD(1.0)\n      gradient_placeholder = tf.Variable([0.0, 0.0], trainable=False)\n\n    def accumulate_on_replica(gradient):\n      accumulator([gradient])\n\n    def apply_on_replica():\n      sgd.apply_gradients(list(zip(accumulator.gradients, [variable])))\n\n    @tf.function\n    def accumulate(grad1, grad2):\n      with strategy.scope():\n        local_variables = strategy.experimental_local_results(gradient_placeholder)\n        local_variables[0].assign(grad1)\n        local_variables[1].assign(grad2)\n        strategy.run(accumulate_on_replica, args=(gradient_placeholder,))\n\n    @tf.function\n    def apply_grad():\n      with strategy.scope():\n        strategy.run(apply_on_replica)\n\n    def _check_local_values(grad1, grad2):\n      values = strategy.experimental_local_results(accumulator._gradients[0])\n      self.assertAllEqual(values[0].value(), grad1)\n      self.assertAllEqual(values[1].value(), grad2)\n\n    accumulate([1.0, 2.0], [-1.0, 1.0])\n    accumulate([3.0, -1.0], [-1.0, -1.0])\n    accumulate([-2.0, 2.0], [3.0, -2.0])\n    self.assertEqual(accumulator.step, 3)\n    _check_local_values([2.0, 3.0], [1.0, -2.0])\n    apply_grad()\n    self.assertAllEqual(variable.value(), [1.0, 2.0])  # [4.0 - (2.0 + 1.0), 3.0 - (3.0 - 2.0)]\n    accumulator.reset()\n    self.assertEqual(accumulator.step, 0)\n    _check_local_values([0.0, 0.0], [0.0, 0.0])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/position_test.py,14,"b'from parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.layers import position\n\n\nclass _DummyPositionEncoder(position.PositionEncoder):\n  """"""Encoder that simply forwards the position indices.""""""\n\n  def _encode(self, positions, depth):\n    positions = tf.expand_dims(positions, 2)\n    positions = tf.tile(positions, [1, 1, depth])\n    return tf.cast(positions, self.dtype)\n\n\nclass PositionTest(tf.test.TestCase):\n\n  def testApplyOneEncoding(self):\n    encoder = _DummyPositionEncoder()\n    inputs = tf.zeros([2, 1, 3])\n    outputs = encoder(inputs, 2)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual(outputs, [[[2, 2, 2]], [[2, 2, 2]]])\n\n  def testApplyPositionEncoding(self):\n    encoder = _DummyPositionEncoder()\n    sequence_length = tf.constant([2, 3])\n    inputs = tf.zeros([2, 4, 3])\n    outputs = encoder(inputs)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual(outputs, [\n        [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n        [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]\n    ])\n\n  def testApplyPositionEncodingWithoutSequenceLength(self):\n    encoder = _DummyPositionEncoder()\n    inputs = tf.zeros([2, 4, 3])\n    outputs = encoder(inputs)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual(outputs, [\n        [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n        [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]]\n    ])\n\n  def _testSinusoidalPositionEncoder(self, depth, dtype=tf.float32):\n    encoder = position.SinusoidalPositionEncoder(dtype=dtype)\n    inputs = tf.zeros([2, 6, depth], dtype=dtype)\n    outputs = encoder(inputs)\n    self.assertEqual(dtype, outputs.dtype.base_dtype)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual([2, 6, depth], outputs.shape)\n\n  def testSinusoidalPositionEncoder(self):\n    self._testSinusoidalPositionEncoder(10)\n  def testSinusoidalPositionEncoderFloat16(self):\n    self._testSinusoidalPositionEncoder(10, dtype=tf.float16)\n  def testSinusoidalPositionEncoderInvalidDepth(self):\n    with self.assertRaises(ValueError):\n      self._testSinusoidalPositionEncoder(5)\n\n  @parameterized.expand([[tf.float32], [tf.float16]])\n  def testPositionEmbedder(self, dtype):\n    encoder = position.PositionEmbedder(dtype=dtype)\n    inputs = tf.zeros([3, 5, 10], dtype=dtype)\n    outputs = encoder(inputs)\n    self.assertEqual(outputs.dtype, dtype)\n    self.assertEqual(encoder.embedding.dtype.base_dtype, dtype)\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual(outputs.shape, [3, 5, 10])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/reducer_test.py,29,"b'import collections\n\nimport tensorflow as tf\n\nfrom parameterized import parameterized\nfrom opennmt.layers import reducer\n\n\nclass ReducerTest(tf.test.TestCase):\n\n  def testAlignInTimeSame(self):\n    a = [\n        [[1], [0], [0]],\n        [[1], [2], [3]]]\n    length = 3\n    b = reducer.align_in_time(tf.constant(a, dtype=tf.float32), tf.constant(length))\n    self.assertEqual(1, b.shape[-1])\n    self.assertAllEqual(a, self.evaluate(b))\n\n  def testAlignInTimeLarger(self):\n    a = [\n        [[1], [0], [0]],\n        [[1], [2], [3]]]\n    length = 4\n    b = [\n        [[1], [0], [0], [0]],\n        [[1], [2], [3], [0]]]\n    c = reducer.align_in_time(tf.constant(a, dtype=tf.float32), tf.constant(length))\n    self.assertEqual(1, c.shape[-1])\n    self.assertAllEqual(b, self.evaluate(c))\n\n  def testAlignInTimeSmaller(self):\n    a = [\n        [[1], [0], [0]],\n        [[1], [2], [0]]]\n    length = 2\n    b = [\n        [[1], [0]],\n        [[1], [2]]]\n    c = reducer.align_in_time(tf.constant(a, dtype=tf.float32), tf.constant(length))\n    self.assertEqual(1, c.shape[-1])\n    self.assertAllEqual(b, self.evaluate(c))\n\n  def testPadWithIdentity(self):\n    tensor = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    expected = [\n        [[1], [1], [1], [1], [0]],\n        [[1], [2], [3], [1], [1]],\n        [[1], [2], [0], [0], [0]]]\n    lengths = [1, 3, 2]\n    max_lengths = [4, 5, 2]\n\n    padded = reducer.pad_with_identity(\n        tf.constant(tensor, dtype=tf.float32),\n        tf.constant(lengths),\n        tf.constant(max_lengths),\n        identity_values=1)\n\n    self.assertEqual(1, padded.shape[-1])\n    self.assertAllEqual(expected, self.evaluate(padded))\n\n  def testPadWithIdentityWithMaxTime(self):\n    tensor = [\n        [[1], [-1], [-1], [-1]],\n        [[1], [2], [3], [-1]],\n        [[1], [2], [-1], [-1]]]\n    expected = [\n        [[1], [1], [1], [1], [0], [0]],\n        [[1], [2], [3], [1], [1], [0]],\n        [[1], [2], [0], [0], [0], [0]]]\n    lengths = [1, 3, 2]\n    max_lengths = [4, 5, 2]\n    maxlen = 6\n\n    padded = reducer.pad_with_identity(\n        tf.constant(tensor, dtype=tf.float32),\n        tf.constant(lengths),\n        tf.constant(max_lengths),\n        identity_values=1,\n        maxlen=maxlen)\n\n    self.assertEqual(1, padded.shape[-1])\n    self.assertAllEqual(expected, self.evaluate(padded))\n\n  def testPadNWithIdentity(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4]],\n        [[1], [2], [-1], [-1]],\n        [[1], [2], [-1], [-1]]]\n    expected_a = [\n        [[1], [1], [1], [1]],\n        [[1], [2], [3], [0]],\n        [[1], [2], [0], [0]]]\n    expected_b = [\n        [[1], [2], [3], [4]],\n        [[1], [2], [1], [0]],\n        [[1], [2], [0], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    (padded_a, padded_b), length = reducer.pad_n_with_identity(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)],\n        identity_values=1)\n\n    padded_a, padded_b, length = self.evaluate([padded_a, padded_b, length])\n    self.assertAllEqual([4, 3, 2], length)\n    self.assertAllEqual(expected_a, padded_a)\n    self.assertAllEqual(expected_b, padded_b)\n\n  def testPadNWithIdentityWithMaxTime(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4], [-1]],\n        [[1], [2], [-1], [-1], [-1]],\n        [[1], [2], [-1], [-1], [-1]]]\n    expected_a = [\n        [[1], [1], [1], [1], [0]],\n        [[1], [2], [3], [0], [0]],\n        [[1], [2], [0], [0], [0]]]\n    expected_b = [\n        [[1], [2], [3], [4], [0]],\n        [[1], [2], [1], [0], [0]],\n        [[1], [2], [0], [0], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    (padded_a, padded_b), length = reducer.pad_n_with_identity(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)],\n        identity_values=1)\n\n    padded_a, padded_b, length = self.evaluate([padded_a, padded_b, length])\n    self.assertAllEqual([4, 3, 2], length)\n    self.assertAllEqual(expected_a, padded_a)\n    self.assertAllEqual(expected_b, padded_b)\n\n  def testMultiplyReducerWithSequence(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4]],\n        [[1], [2], [-1], [-1]],\n        [[1], [2], [-1], [-1]]]\n    expected = [\n        [[1], [2], [3], [4]],\n        [[1], [4], [3], [0]],\n        [[1], [4], [0], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    reduced, length = reducer.MultiplyReducer()(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)])\n\n    reduced, length = self.evaluate([reduced, length])\n    self.assertAllEqual(expected, reduced)\n    self.assertAllEqual([4, 3, 2], length)\n\n  def testMultiplyReducerWithSequenceAndMaxTime(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4], [-1]],\n        [[1], [2], [-1], [-1], [-1]],\n        [[1], [2], [-1], [-1], [-1]]]\n    expected = [\n        [[1], [2], [3], [4], [0]],\n        [[1], [4], [3], [0], [0]],\n        [[1], [4], [0], [0], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    reduced, length = reducer.MultiplyReducer()(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)])\n\n    reduced, length = self.evaluate([reduced, length])\n    self.assertAllEqual(expected, reduced)\n    self.assertAllEqual([4, 3, 2], length)\n\n  def testConcatInDepthWithSequence(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4]],\n        [[1], [2], [-1], [-1]],\n        [[1], [2], [-1], [-1]]]\n    expected = [\n        [[1, 1], [0, 2], [0, 3], [0, 4]],\n        [[1, 1], [2, 2], [3, 0], [0, 0]],\n        [[1, 1], [2, 2], [0, 0], [0, 0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    reduced, length = reducer.ConcatReducer()(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)])\n\n    self.assertEqual(2, reduced.shape[-1])\n    reduced, length = self.evaluate([reduced, length])\n    self.assertAllEqual(expected, reduced)\n    self.assertAllEqual([4, 3, 2], length)\n\n  def testConcatInTimeWithSequence(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4]],\n        [[1], [2], [-1], [-1]],\n        [[1], [2], [-1], [-1]]]\n    expected = [\n        [[1], [1], [2], [3], [4]],\n        [[1], [2], [3], [1], [2]],\n        [[1], [2], [1], [2], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    reduced, length = reducer.ConcatReducer(axis=1)(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)])\n\n    self.assertEqual(1, reduced.shape[-1])\n    reduced, length = self.evaluate([reduced, length])\n    self.assertAllEqual(expected, reduced)\n    self.assertAllEqual([5, 5, 4], length)\n\n  def testConcatInTimeWithSequenceAndMaxTimeMismatch(self):\n    a = [\n        [[1], [-1], [-1]],\n        [[1], [2], [3]],\n        [[1], [2], [-1]]]\n    b = [\n        [[1], [2], [3], [4], [-1], [-1]],\n        [[1], [2], [-1], [-1], [-1], [-1]],\n        [[1], [2], [-1], [-1], [-1], [-1]]]\n    expected = [\n        [[1], [1], [2], [3], [4]],\n        [[1], [2], [3], [1], [2]],\n        [[1], [2], [1], [2], [0]]]\n    length_a = [1, 3, 2]\n    length_b = [4, 2, 2]\n\n    reduced, length = reducer.ConcatReducer(axis=1)(\n        [tf.constant(a, dtype=tf.float32), tf.constant(b, dtype=tf.float32)],\n        [tf.constant(length_a), tf.constant(length_b)])\n\n    self.assertEqual(1, reduced.shape[-1])\n    reduced, length = self.evaluate([reduced, length])\n    self.assertAllEqual(expected, reduced)\n    self.assertAllEqual([5, 5, 4], length)\n\n  def testJoinReducer(self):\n    self.assertTupleEqual((1, 2, 3), reducer.JoinReducer()([1, 2, 3]))\n    self.assertTupleEqual((1, 2, 3), reducer.JoinReducer()([(1,), (2,), (3,)]))\n    self.assertTupleEqual((1, 2, 3), reducer.JoinReducer()([1, (2, 3)]))\n\n    # Named tuples should not be unpacked.\n    State = collections.namedtuple(""State"", [""h"", ""c""])\n    self.assertTupleEqual((State(h=1, c=2), State(h=3, c=4), State(h=5, c=6)),\n                          reducer.JoinReducer()([\n                              State(h=1, c=2), (State(h=3, c=4), State(h=5, c=6))]))\n\n  @parameterized.expand([\n      [reducer.SumReducer(), [1, 2, 3], [4, 5, 6], [5, 7, 9]],\n      [reducer.SumReducer(), (1, 2, 3), (4, 5, 6), (5, 7, 9)],\n      [reducer.SumReducer(), 1, 2, 3]\n  ])\n  def testZipAndReduce(self, reducer, x, y, expected_z):\n    z = reducer.zip_and_reduce(x, y)\n    z = self.evaluate(z)\n    self.assertAllEqual(z, expected_z)\n\n  def testDenseReducer(self):\n    inputs = [\n        tf.random.uniform([3, 4]),\n        tf.random.uniform([3, 12]),\n        tf.random.uniform([3, 6]),\n    ]\n    dense_reducer = reducer.DenseReducer(10, activation=tf.nn.relu)\n    output = dense_reducer(inputs)\n    self.assertTrue(dense_reducer.built)\n    self.assertNotEmpty(dense_reducer.variables)\n    self.assertListEqual(output.shape.as_list(), [3, 10])\n    self.assertAllGreaterEqual(output, 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/rnn_test.py,9,"b'import tensorflow as tf\n\nfrom opennmt.layers import rnn\n\n\nclass RNNTest(tf.test.TestCase):\n\n  def testRNNCell(self):\n    cell = rnn.make_rnn_cell(3, 10, dropout=0.1, residual_connections=True)\n    inputs = tf.random.uniform([4, 5])\n    states = cell.get_initial_state(inputs=inputs)\n    outputs, states = cell(inputs, states, training=True)\n    self.assertEqual(len(states), 3)\n    self.assertListEqual(outputs.shape.as_list(), [4, 10])\n\n  def testRNN(self):\n    cell = rnn.make_rnn_cell(3, 10, dropout=0.1, residual_connections=True)\n    rnn_layer = rnn.RNN(cell)\n    inputs = tf.random.uniform([4, 5, 5])\n    outputs, states = rnn_layer(inputs, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 10])\n    self.assertIsInstance(states, tuple)\n    self.assertEqual(len(states), 3)\n\n  def testBRNN(self):\n    cell = rnn.make_rnn_cell(3, 10, dropout=0.1, residual_connections=True)\n    rnn_layer = rnn.RNN(cell, bidirectional=True)\n    inputs = tf.random.uniform([4, 5, 5])\n    outputs, states = rnn_layer(inputs, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 20])\n    self.assertIsInstance(states, tuple)\n    self.assertEqual(len(states), 3)\n    self.assertEqual(len(states[0]), 2)\n    self.assertListEqual(states[0][0].shape.as_list(), [4, 20])\n\n  def testLSTM(self):\n    lstm = rnn.LSTM(3, 12)\n    inputs = tf.random.uniform([4, 5, 5])\n    outputs, states = lstm(inputs, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 12])\n    self.assertIsInstance(states, tuple)\n    self.assertEqual(len(states), 3)\n\n  def testLSTMWithMask(self):\n    lstm = rnn.LSTM(3, 12)\n    inputs = tf.random.uniform([3, 4, 5])\n    lengths = [4, 2, 3]\n    mask = tf.sequence_mask(lengths)\n    outputs, states = lstm(inputs, mask=mask, training=True)\n    last_hidden = states[-1][0]\n    for i, length in enumerate(lengths):\n      self.assertAllClose(last_hidden[i], outputs[i, length - 1])\n\n  def testBLSTM(self):\n    lstm = rnn.LSTM(3, 12, dropout=0.5, bidirectional=True)\n    inputs = tf.random.uniform([4, 5, 5])\n    outputs, states = lstm(inputs, training=True)\n    self.assertListEqual(outputs.shape.as_list(), [4, 5, 24])\n    self.assertIsInstance(states, tuple)\n    self.assertEqual(len(states), 3)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/runner_test.py,14,"b'# -*- coding: utf-8 -*-\n\nimport copy\nimport os\nimport unittest\nimport shutil\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt import decoders\nfrom opennmt import models\nfrom opennmt import Runner\nfrom opennmt.config import load_model\nfrom opennmt.utils import exporters\nfrom opennmt.utils import misc\nfrom opennmt.tests import test_util\n\n\ntest_dir = os.path.dirname(os.path.realpath(__file__))\nroot_dir = os.path.join(test_dir, "".."", "".."")\ntest_data = os.path.join(root_dir, ""testdata"")\n\n\n@unittest.skipIf(not os.path.isdir(test_data), ""Missing test data directory"")\nclass RunnerTest(tf.test.TestCase):\n\n  def _getTransliterationRunner(self,\n                                base_config=None,\n                                model_version=""v2"",\n                                pass_model_builder=False):\n    model_dir = os.path.join(self.get_temp_dir(), ""model"")\n    shutil.copytree(os.path.join(test_data, ""transliteration-aren-v2"", model_version), model_dir)\n    config = {}\n    config[""model_dir""] = model_dir\n    config[""data""] = {\n        ""source_vocabulary"": os.path.join(model_dir, ""ar.vocab""),\n        ""target_vocabulary"": os.path.join(model_dir, ""en.vocab""),\n    }\n    if base_config is not None:\n      config = misc.merge_dict(config, base_config)\n    model = load_model(model_dir, as_builder=pass_model_builder)\n    runner = Runner(model, config)\n    return runner\n\n  def _makeTransliterationData(self):\n    ar = [\n      ""\xd8\xa2 \xd8\xaa \xd8\xb2 \xd9\x85 \xd9\x88 \xd9\x86"",\n      ""\xd8\xa2 \xd8\xaa \xd8\xb4 \xd9\x8a \xd8\xb3 \xd9\x88 \xd9\x86"",\n      ""\xd8\xa2 \xd8\xb1 \xd8\xa8 \xd8\xa7 \xd9\x83 \xd9\x87"",\n      ""\xd8\xa2 \xd8\xb1 \xd8\xab \xd8\xb1"",\n      ""\xd8\xa2 \xd8\xb2 \xd8\xa7"",\n    ]\n    en = [\n        ""a t z m o n"",\n        ""a c h e s o n"",\n        ""a a r b a k k e"",\n        ""a r t h u r"",\n        ""a s a""\n    ]\n    ar_file = test_util.make_data_file(os.path.join(self.get_temp_dir(), ""ar.txt""), ar)\n    en_file = test_util.make_data_file(os.path.join(self.get_temp_dir(), ""en.txt""), en)\n    return ar_file, en_file\n\n  @parameterized.expand([[True], [False]])\n  def testTrain(self, pass_model_builder):\n    ar_file, en_file  = self._makeTransliterationData()\n    config = {\n        ""data"": {\n            ""train_features_file"": ar_file,\n            ""train_labels_file"": en_file\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""batch_size"": 10,\n            ""average_last_checkpoints"": 4,\n            ""save_checkpoints_steps"": 1,\n            ""max_step"": 145002  # Just train for 2 steps.\n        }\n    }\n    runner = self._getTransliterationRunner(config, pass_model_builder=pass_model_builder)\n    avg_dir = runner.train()\n    self.assertEqual(runner.model_dir, avg_dir)\n    self.assertEndsWith(tf.train.latest_checkpoint(avg_dir), ""145002"")\n    self.assertLen(tf.train.get_checkpoint_state(avg_dir).all_model_checkpoint_paths, 1)\n    model_dir = os.path.dirname(avg_dir)\n    self.assertEndsWith(tf.train.latest_checkpoint(model_dir), ""145002"")\n    self.assertLen(tf.train.get_checkpoint_state(model_dir).all_model_checkpoint_paths, 3)\n\n    # Check that the averaged checkpoint is usable.\n    ar_file, _ = self._makeTransliterationData()\n    en_file = os.path.join(self.get_temp_dir(), ""output.txt"")\n    runner.infer(ar_file, predictions_file=en_file, checkpoint_path=avg_dir)\n    with open(en_file) as f:\n      self.assertEqual(next(f).strip(), ""a t z m o n"")\n\n  @test_util.run_with_two_cpu_devices\n  def testTrainDistribute(self):\n    ar_file, en_file  = self._makeTransliterationData()\n    config = {\n        ""data"": {\n            ""train_features_file"": ar_file,\n            ""train_labels_file"": en_file\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""batch_size"": 2,\n            ""length_bucket_width"": None,\n            ""max_step"": 145003,\n            ""single_pass"": True,  # Test we do not fail when a batch is missing for a replica.\n        }\n    }\n    runner = self._getTransliterationRunner(config)\n    runner.train(num_devices=2)\n\n  @test_util.run_with_two_cpu_devices\n  def testTrainDistributeWithGradientAccumulation(self):\n    ar_file, en_file  = self._makeTransliterationData()\n    config = {\n        ""data"": {\n            ""train_features_file"": ar_file,\n            ""train_labels_file"": en_file\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""batch_size"": 2,\n            ""effective_batch_size"": 8,\n            ""length_bucket_width"": None,\n            ""max_step"": 145003,\n        }\n    }\n    runner = self._getTransliterationRunner(config)\n    runner.train(num_devices=2)\n\n  def testTrainWithEval(self):\n    ar_file, en_file  = self._makeTransliterationData()\n    config = {\n        ""data"": {\n            ""train_features_file"": ar_file,\n            ""train_labels_file"": en_file,\n            ""eval_features_file"": ar_file,\n            ""eval_labels_file"": en_file\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""batch_size"": 10,\n            ""max_step"": 145002  # Just train for 2 steps.\n        },\n        ""eval"": {\n            ""export_on_best"": ""loss""\n        }\n    }\n    runner = self._getTransliterationRunner(config)\n    model_dir = runner.train(with_eval=True)\n    export_dir = os.path.join(model_dir, ""export"", ""145002"")\n    self.assertTrue(os.path.exists(export_dir))\n    self.assertTrue(tf.saved_model.contains_saved_model(export_dir))\n\n  def testTrainLanguageModel(self):\n    src = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src.txt""),\n        [""1 2 3 4"", ""5 6 7 8 9"", ""3 2""])\n    vocab = test_util.make_vocab(\n        os.path.join(self.get_temp_dir(), ""vocab.txt""),\n        list(map(str, range(10))))\n    config = {\n        ""data"": {\n            ""train_features_file"": src,\n            ""vocabulary"": vocab,\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""batch_size"": 10,\n            ""max_step"": 2,\n        },\n    }\n    model = models.LanguageModel(\n        decoders.SelfAttentionDecoder(2, num_units=32, ffn_inner_dim=32),\n        embedding_size=16,\n        reuse_embedding=False)\n    runner = Runner(model, config)\n    runner.train()\n\n  def testEvaluate(self):\n    ar_file, en_file  = self._makeTransliterationData()\n    config = {\n        ""data"": {\n            ""eval_features_file"": ar_file,\n            ""eval_labels_file"": en_file\n        },\n        ""eval"": {\n            ""external_evaluators"": ""BLEU""\n        }\n    }\n    runner = self._getTransliterationRunner(config)\n    metrics = runner.evaluate()\n    self.assertIn(""loss"", metrics)\n    self.assertIn(""bleu"", metrics)\n\n  @parameterized.expand([[1, ""v2""], [4, ""v2""], [1, ""v1""]])\n  def testInfer(self, beam_size, model_version):\n    config = {\n        ""params"": {\n            ""beam_width"": beam_size\n        }\n    }\n    runner = self._getTransliterationRunner(config, model_version)\n    ar_file, _ = self._makeTransliterationData()\n    en_file = os.path.join(self.get_temp_dir(), ""output.txt"")\n    runner.infer(ar_file, predictions_file=en_file)\n    self.assertTrue(os.path.exists(en_file))\n    with open(en_file) as f:\n      lines = f.readlines()\n    self.assertEqual(len(lines), 5)\n    self.assertEqual(lines[0].strip(), ""a t z m o n"")\n\n  def testUpdateVocab(self):\n    ar_file, en_file = self._makeTransliterationData()\n    max_step = 145002\n    config = {\n        ""data"": {\n            ""train_features_file"": ar_file,\n            ""train_labels_file"": en_file\n        },\n        ""params"": {\n            ""learning_rate"": 0.0005,\n            ""optimizer"": ""Adam""\n        },\n        ""train"": {\n            ""max_step"": max_step,\n            ""batch_size"": 10\n        }\n    }\n    runner = self._getTransliterationRunner(config)\n\n    # Reverse order of non special tokens and add a new token.\n    new_en_vocab = os.path.join(self.get_temp_dir(), ""en.vocab.new"")\n    with open(os.path.join(runner._config[""model_dir""], ""en.vocab"")) as en_vocab, \\\n         open(new_en_vocab, ""w"") as new_vocab:\n      tokens = en_vocab.readlines()\n      for token in tokens[:3]:\n        new_vocab.write(token)\n      for token in reversed(tokens[3:]):\n        new_vocab.write(token)\n      new_vocab.write(""anewtoken\\n"")\n\n    output_dir = os.path.join(self.get_temp_dir(), ""updated_vocab"")\n    self.assertEqual(runner.update_vocab(output_dir, tgt_vocab=new_en_vocab), output_dir)\n    self.assertEqual(runner.model_dir, output_dir)\n\n    # Check that the translation is unchanged.\n    en_file = os.path.join(self.get_temp_dir(), ""output.txt"")\n    runner.infer(ar_file, predictions_file=en_file)\n    with open(en_file) as f:\n      self.assertEqual(next(f).strip(), ""a t z m o n"")\n\n    # We should be able to continue training without error or NaN loss.\n    output_dir = runner.train()\n    self.assertEndsWith(tf.train.latest_checkpoint(output_dir), str(max_step))\n\n  def testScore(self):\n    runner = self._getTransliterationRunner()\n    ar_file, en_file = self._makeTransliterationData()\n    score_file = os.path.join(self.get_temp_dir(), ""scores.txt"")\n    runner.score(ar_file, en_file, output_file=score_file)\n    self.assertTrue(os.path.exists(score_file))\n    with open(score_file) as f:\n      lines = f.readlines()\n    self.assertEqual(len(lines), 5)\n\n  @parameterized.expand([[True], [False]])\n  def testExport(self, export_vocabulary_assets):\n    config = {\n        ""data"": {\n            ""export_vocabulary_assets"": export_vocabulary_assets,\n            ""source_tokenization"": {\n                ""mode"": ""char""\n            }\n        }\n    }\n    export_dir = os.path.join(self.get_temp_dir(), ""export"")\n    runner = self._getTransliterationRunner(config)\n    runner.export(export_dir)\n    self.assertTrue(tf.saved_model.contains_saved_model(export_dir))\n\n    # Check assets directories.\n    assets = os.listdir(os.path.join(export_dir, ""assets""))\n    if export_vocabulary_assets:\n      self.assertLen(assets, 2)\n    else:\n      self.assertLen(assets, 0)\n    extra_assets_dir = os.path.join(export_dir, ""assets.extra"")\n    self.assertTrue(os.path.isdir(extra_assets_dir))\n    self.assertLen(os.listdir(extra_assets_dir), 1)\n\n    # Export directory could be relocated and does not reference the original vocabulary files.\n    shutil.rmtree(runner.model_dir)\n    export_dir_2 = os.path.join(self.get_temp_dir(), ""export_2"")\n    os.rename(export_dir, export_dir_2)\n    self.assertTrue(tf.saved_model.contains_saved_model(export_dir_2))\n    imported = tf.saved_model.load(export_dir_2)\n    translate_fn = imported.signatures[""serving_default""]\n    outputs = translate_fn(\n        tokens=tf.constant([[""\xd8\xa2"" ,""\xd8\xaa"" ,""\xd8\xb2"" ,""\xd9\x85"" ,""\xd9\x88"" ,""\xd9\x86""]]),\n        length=tf.constant([6], dtype=tf.int32))\n    result = tf.nest.map_structure(lambda x: x[0, 0], outputs)\n    tokens = result[""tokens""][:result[""length""]]\n    self.assertAllEqual(tokens, [b""a"", b""t"", b""z"", b""m"", b""o"", b""n""])\n\n  def testCTranslate2Export(self):\n    try:\n      import ctranslate2\n    except ImportError:\n      self.skipTest(""ctranslate2 module is not available"")\n    export_dir = os.path.join(self.get_temp_dir(), ""export"")\n    runner = self._getTransliterationRunner()\n    runner.export(export_dir, exporter=exporters.make_exporter(""ctranslate2""))\n    self.assertTrue(ctranslate2.contains_model(export_dir))\n    translator = ctranslate2.Translator(export_dir)\n    output = translator.translate_batch([[""\xd8\xa2"" ,""\xd8\xaa"" ,""\xd8\xb2"" ,""\xd9\x85"" ,""\xd9\x88"" ,""\xd9\x86""]])\n    self.assertListEqual(output[0][0][""tokens""], [""a"", ""t"", ""z"", ""m"", ""o"", ""n""])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/scorers_test.py,2,"b'import os\n\nimport tensorflow as tf\n\nfrom opennmt.tests import test_util\nfrom opennmt.utils import scorers\n\n\nclass ScorersTest(tf.test.TestCase):\n\n  def _run_scorer(self, scorer, refs, hyps):\n    ref_path = test_util.make_data_file(os.path.join(self.get_temp_dir(), ""ref.txt""), refs)\n    hyp_path = test_util.make_data_file(os.path.join(self.get_temp_dir(), ""hyp.txt""), hyps)\n    return scorer(ref_path, hyp_path)\n\n  def testBLEUScorer(self):\n    refs = [""Hello world !"", ""How is it going ?""]\n    scorer = scorers.BLEUScorer()\n    score = self._run_scorer(scorer, refs, refs)\n    self.assertEqual(100, int(score))\n\n  def testROUGEScorer(self):\n    refs = [""Hello world !"", ""How is it going ?""]\n    scorer = scorers.ROUGEScorer()\n    score = self._run_scorer(scorer, refs, refs)\n    self.assertIsInstance(score, dict)\n    self.assertIn(""rouge-l"", score)\n    self.assertIn(""rouge-1"", score)\n    self.assertIn(""rouge-2"", score)\n    self.assertAlmostEqual(1.0, score[""rouge-1""])\n\n  def testPRFScorer(self):\n    scorer = scorers.PRFScorer()\n    score = self._run_scorer(scorer, refs=[""TAG O TAG O O TAG TAG""], hyps=[""TAG O O O TAG TAG O""])\n    expected_precision = 2 / 3\n    expected_recall = 2 / 4\n    expected_fscore = (\n        2 * (expected_precision * expected_recall) / (expected_precision + expected_recall))\n    self.assertAlmostEqual(score[""precision""], expected_precision, places=6)\n    self.assertAlmostEqual(score[""recall""], expected_recall, places=6)\n    self.assertAlmostEqual(score[""fmeasure""], expected_fscore, places=6)\n\n  def testPRFScorerEmptyLine(self):\n    scorer = scorers.PRFScorer()\n    self._run_scorer(scorer, [""""], [""O TAG""])\n    self._run_scorer(scorer, [""O TAG""], [""""])\n\n  def testMakeScorers(self):\n\n    def _check_scorers(scorers, instances):\n      self.assertLen(scorers, len(instances))\n      for scorer, instance in zip(scorers, instances):\n        self.assertIsInstance(scorer, instance)\n\n    _check_scorers(scorers.make_scorers(""bleu""), [scorers.BLEUScorer])\n    _check_scorers(scorers.make_scorers(""BLEU""), [scorers.BLEUScorer])\n    _check_scorers(\n        scorers.make_scorers([""BLEU"", ""rouge""]),\n        [scorers.BLEUScorer, scorers.ROUGEScorer])\n    _check_scorers(scorers.make_scorers(""prf""), [scorers.PRFScorer])\n    _check_scorers(scorers.make_scorers(""prfmeasure""), [scorers.PRFScorer])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/sequence_classifier_test.py,2,"b'import tensorflow as tf\n\nfrom opennmt.models import sequence_classifier\n\n\nclass SequenceClassifierTest(tf.test.TestCase):\n  pass\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/sequence_tagger_test.py,7,"b'import tensorflow as tf\nimport numpy as np\n\nfrom opennmt.models import sequence_tagger\n\n\nclass SequenceTaggerTest(tf.test.TestCase):\n\n  def _testTagSchemeFlags(self,\n                          tag_fn,\n                          labels,\n                          predicted,\n                          expected_true_positives,\n                          expected_false_positives,\n                          expected_false_negatives):\n    labels = np.array([[tf.compat.as_bytes(c) for c in labels]])\n    predicted = np.array([[tf.compat.as_bytes(c) for c in predicted]])\n    gold_flags, predicted_flags = tag_fn(labels, predicted)\n\n    true_positives = tf.keras.metrics.TruePositives()\n    false_positives = tf.keras.metrics.FalsePositives()\n    false_negatives = tf.keras.metrics.FalseNegatives()\n    true_positives.update_state(gold_flags, predicted_flags)\n    false_positives.update_state(gold_flags, predicted_flags)\n    false_negatives.update_state(gold_flags, predicted_flags)\n\n    tp = self.evaluate(true_positives.result())\n    fp = self.evaluate(false_positives.result())\n    fn = self.evaluate(false_negatives.result())\n\n    self.assertEqual(expected_true_positives, tp, msg=""true positives mismatch"")\n    self.assertEqual(expected_false_positives, fp, msg=""false positives mismatch"")\n    self.assertEqual(expected_false_negatives, fn, msg=""false negatives mismatch"")\n\n  def testBIOESFlags(self):\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""S-LOC""], [""S-ORG""],\n      expected_true_positives=0,\n      expected_false_positives=1,\n      expected_false_negatives=1)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""B-LOC"", ""I-LOC"", ""E-LOC""], [""B-LOC"", ""I-LOC"", ""E-LOC""],\n      expected_true_positives=1,\n      expected_false_positives=0,\n      expected_false_negatives=0)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""O"", ""B-LOC"", ""I-LOC"", ""E-LOC""], [""B-LOC"", ""I-LOC"", ""E-LOC"", ""O""],\n      expected_true_positives=0,\n      expected_false_positives=1,\n      expected_false_negatives=1)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""B-LOC"", ""I-LOC"", ""E-LOC""], [""B-LOC"", ""E-LOC"", ""S-LOC""],\n      expected_true_positives=0,\n      expected_false_positives=2,\n      expected_false_negatives=1)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""B-LOC"", ""I-LOC"", ""E-LOC""], [""S-LOC"", ""O"", ""O""],\n      expected_true_positives=0,\n      expected_false_positives=1,\n      expected_false_negatives=1)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""S-LOC"", ""O""], [""B-LOC"", ""E-LOC""],\n      expected_true_positives=0,\n      expected_false_positives=1,\n      expected_false_negatives=1)\n    self._testTagSchemeFlags(\n      sequence_tagger.flag_bioes_tags,\n      [""B-ORG"", ""E-ORG"",     ""O"", ""B-PER"", ""E-PER"", ""O"", ""O"", ""O"", ""O"", ""B-MISC"", ""E-MISC"",      ""O""],\n      [""B-ORG"", ""E-ORG"", ""S-PER"", ""S-PER"",     ""O"", ""O"", ""O"", ""O"", ""O"",      ""O"",      ""O"", ""S-MISC""],\n      expected_true_positives=1,\n      expected_false_positives=3,\n      expected_false_negatives=2)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/sequence_to_sequence_test.py,2,"b'import os\n\nimport tensorflow as tf\n\nfrom opennmt.inputters import text_inputter\nfrom opennmt.models import sequence_to_sequence\nfrom opennmt.tests import test_util\n\n\nclass SequenceToSequenceTest(tf.test.TestCase):\n\n  def testReplaceUnknownTarget(self):\n    target_tokens = [\n      [""Hello"", ""world"", ""!"", """", """", """"],\n      [""<unk>"", ""name"", ""is"", ""<unk>"", ""."", """"]]\n    source_tokens = [\n      [""Bonjour"", ""le"", ""monde"", ""!"", """"],\n      [""Mon"", ""nom"", ""est"", ""Max"", "".""]]\n    attention = [\n      [[0.9, 0.1, 0.0, 0.0, 0.0],\n       [0.2, 0.1, 0.7, 0.0, 0.0],\n       [0.0, 0.1, 0.1, 0.8, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0],\n       [0.0, 0.0, 0.0, 0.0, 0.0]],\n      [[0.8, 0.1, 0.1, 0.0, 0.0],\n       [0.1, 0.9, 0.0, 0.0, 0.0],\n       [0.0, 0.1, 0.8, 0.1, 0.0],\n       [0.1, 0.1, 0.2, 0.6, 0.0],\n       [0.0, 0.1, 0.1, 0.3, 0.5],\n       [0.0, 0.0, 0.0, 0.0, 0.0]]]\n    replaced_target_tokens = sequence_to_sequence.replace_unknown_target(\n        target_tokens,\n        source_tokens,\n        attention,\n        unknown_token=""<unk>"")\n    replaced_target_tokens = self.evaluate(replaced_target_tokens)\n    self.assertNotIn(b""<unk>"", replaced_target_tokens.flatten().tolist())\n    self.assertListEqual(\n        [b""Hello"", b""world"", b""!"", b"""", b"""", b""""], replaced_target_tokens[0].tolist())\n    self.assertListEqual(\n        [b""Mon"", b""name"", b""is"", b""Max"", b""."", b""""], replaced_target_tokens[1].tolist())\n\n  def testSequenceToSequenceInputter(self):\n    source_vocabulary = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src_vocab.txt""),\n        [""<blank>"", ""<s>"", ""</s>"", ""a"", ""b"", ""c"", ""d""])\n    target_vocabulary = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""tgt_vocab.txt""),\n        [""<blank>"", ""<s>"", ""</s>"", ""e"", ""f"", ""g"", ""h""])\n    source_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""src.txt""), [""a c c"", ""b d"", ""a e""])\n    target_file = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""tgt.txt""), [""f h g"", ""e h"", ""a e""])\n    inputter = sequence_to_sequence.SequenceToSequenceInputter(\n        text_inputter.WordEmbedder(embedding_size=20),\n        text_inputter.WordEmbedder(embedding_size=20))\n    inputter.initialize(dict(\n        source_vocabulary=source_vocabulary, target_vocabulary=target_vocabulary))\n    dataset = inputter.make_dataset([source_file, target_file])\n    element = iter(dataset).next()\n    features, labels = inputter.make_features(element)\n    self.assertIn(""ids_out"", labels)\n    self.assertAllEqual(labels[""ids""], [1, 4, 6, 5])\n    self.assertAllEqual(labels[""ids_out""], [4, 6, 5, 2])\n    self.assertEqual(labels[""length""], 4)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/tensor_test.py,2,"b'import tensorflow as tf\n\nfrom opennmt.utils import tensor as tensor_util\n\n\nclass TensorTest(tf.test.TestCase):\n\n  def testRollSequence(self):\n    offset = [2, 3, 3]\n    tensor = [\n        [1, 2, 3, 0, 0, 6, 0],\n        [1, 2, 3, 4, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 7]]\n    expected = [\n        [6, 0, 1, 2, 3, 0, 0],\n        [0, 0, 0, 1, 2, 3, 4],\n        [0, 0, 7, 1, 0, 0, 0]]\n\n    rolled = tensor_util.roll_sequence(tensor, offset)\n    self.assertAllEqual(expected, self.evaluate(rolled))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/test_util.py,5,"b'import unittest\n\nimport tensorflow as tf\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import ops\n\nfrom opennmt import constants\nfrom opennmt.data import vocab\nfrom opennmt.utils import compat\n\n\ndef skip_if_unsupported(symbol):\n  return unittest.skipIf(not compat.tf_supports(symbol), ""tf.%s is not supported"")\n\ndef make_data_file(path, lines):\n  with open(path, ""w"") as data:\n    for line in lines:\n      data.write(""%s\\n"" % line)\n  return path\n\ndef make_vocab(path, tokens):\n  vocabulary = vocab.Vocab(special_tokens=[\n      constants.PADDING_TOKEN,\n      constants.START_OF_SENTENCE_TOKEN,\n      constants.END_OF_SENTENCE_TOKEN])\n  for token in tokens:\n    vocabulary.add(token)\n  vocabulary.serialize(path)\n  return path\n\ndef make_vocab_from_file(path, data_file):\n  vocabulary = vocab.Vocab(special_tokens=[\n      constants.PADDING_TOKEN,\n      constants.START_OF_SENTENCE_TOKEN,\n      constants.END_OF_SENTENCE_TOKEN])\n  vocabulary.add_from_text(data_file)\n  vocabulary.serialize(path)\n  return path\n\ndef _reset_context():\n  # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/config_test.py\n  # TODO: find a way to achieve that without relying on TensorFlow private APIs.\n  context._context = None\n  ops.enable_eager_execution_internal()\n\ndef run_with_two_cpu_devices(fn):\n  """"""Defines 2 logical devices before running :obj:`fn`.""""""\n  def decorator(*args, **kwargs):\n    _reset_context()\n    physical_devices = tf.config.list_physical_devices(""CPU"")\n    if len(physical_devices) == 1:\n      tf.config.set_logical_device_configuration(\n          physical_devices[0],\n          [tf.config.LogicalDeviceConfiguration(),\n           tf.config.LogicalDeviceConfiguration()])\n    try:\n      return fn(*args, **kwargs)\n    finally:\n      _reset_context()\n  return decorator\n'"
opennmt/tests/text_test.py,14,"b'# -*- coding: utf-8 -*-\n\nfrom parameterized import parameterized\n\nimport tensorflow as tf\n\nfrom opennmt.data import text\n\n\nclass TextTest(tf.test.TestCase):\n\n  def _testTokensToChars(self, tokens, expected_chars):\n    expected_chars = tf.nest.map_structure(tf.compat.as_bytes, expected_chars)\n    chars = text.tokens_to_chars(tf.constant(tokens, dtype=tf.string))\n    self.assertListEqual(chars.to_list(), expected_chars)\n\n  def testTokensToCharsEmpty(self):\n    self._testTokensToChars([], [])\n\n  def testTokensToCharsSingle(self):\n    self._testTokensToChars([""Hello""], [[""H"", ""e"", ""l"", ""l"", ""o""]])\n\n  def testTokensToCharsMixed(self):\n    self._testTokensToChars(\n        [""Just"", ""a"", ""\xe6\xb5\x8b\xe8\xaf\x95""], [[""J"", ""u"", ""s"", ""t""], [""a""], [""\xe6\xb5\x8b"", ""\xe8\xaf\x95""]])\n\n  @parameterized.expand([\n    [[""a\xef\xbf\xad"", ""b"", ""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xade""], [[""a\xef\xbf\xad"", ""b""], [""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xade""]]],\n    [[""a"", ""\xef\xbf\xad"", ""b"", ""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xad"", ""e""], [[""a"", ""\xef\xbf\xad"", ""b""], [""c\xef\xbf\xad"", ""d"", ""\xef\xbf\xad"", ""e""]]],\n  ])\n  def testToWordsWithJoiner(self, tokens, expected):\n    expected = tf.nest.map_structure(tf.compat.as_bytes, expected)\n    tokens = tf.constant(tokens)\n    words = text.tokens_to_words(tokens)\n    self.assertAllEqual(words.to_list(), expected)\n\n  @parameterized.expand([\n    [[""\xe2\x96\x81a"", ""b"", ""\xe2\x96\x81c"", ""d"", ""e""], [[""\xe2\x96\x81a"", ""b""], [""\xe2\x96\x81c"", ""d"", ""e""]]],\n    [[""\xe2\x96\x81"", ""a"", ""b"", ""\xe2\x96\x81"", ""c"", ""d"", ""e""], [[""\xe2\x96\x81"", ""a"", ""b""], [""\xe2\x96\x81"", ""c"", ""d"", ""e""]]],\n    [[""a\xe2\x96\x81"", ""b"", ""c\xe2\x96\x81"", ""d"", ""e""], [[""a\xe2\x96\x81""], [""b"", ""c\xe2\x96\x81""], [""d"", ""e""]]],\n    [[""a"", ""\xe2\x96\x81b\xe2\x96\x81"", ""c"", ""d"", ""\xe2\x96\x81"", ""e""], [[""a""], [""\xe2\x96\x81b\xe2\x96\x81""], [""c"", ""d""], [""\xe2\x96\x81"", ""e""]]],\n  ])\n  def testToWordsWithSpacer(self, tokens, expected):\n    expected = tf.nest.map_structure(tf.compat.as_bytes, expected)\n    tokens = tf.constant(tokens)\n    words = text.tokens_to_words(tokens, subword_token=""\xe2\x96\x81"", is_spacer=True)\n    self.assertAllEqual(words.to_list(), expected)\n\n  def _testPharaohAlignments(self, line, lengths, expected_matrix):\n    matrix = text.alignment_matrix_from_pharaoh(\n        tf.constant(line), lengths[0], lengths[1], dtype=tf.int32)\n    self.assertListEqual(expected_matrix, self.evaluate(matrix).tolist())\n\n  def testPharaohAlignments(self):\n    self._testPharaohAlignments("""", [0, 0], [])\n    self._testPharaohAlignments(""0-0"", [1, 1], [[1]])\n    self._testPharaohAlignments(\n        ""0-0 1-1 2-2 3-3"", [4, 4], [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]])\n    self._testPharaohAlignments(\n        ""0-0 1-1 2-3 3-2"", [4, 4], [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])\n    self._testPharaohAlignments(\n        ""0-0 1-2 1-1"", [2, 3], [[1, 0], [0, 1], [0, 1]])\n    self._testPharaohAlignments(\n        ""0-0 1-2 1-1 2-4"", [3, 5], [[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1]])\n\n  @parameterized.expand([[True], [False]])\n  def testInvalidPharaohAlignments(self, run_as_function):\n    func = text.alignment_matrix_from_pharaoh\n    if run_as_function:\n      func = tf.function(func)\n\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, ""source""):\n      func(tf.constant(""0-0 1-1 2-3 3-2""), 2, 4)\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError, ""target""):\n      func(tf.constant(""0-0 1-2 1-1 2-4""), 3, 4)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/tokenizer_test.py,22,"b'# -*- coding: utf-8 -*-\n\nimport os\nimport yaml\n\nimport tensorflow as tf\n\nfrom opennmt import tokenizers\n\n\nclass TokenizerTest(tf.test.TestCase):\n\n  def _testTokenizerOnTensor(self, tokenizer, text, ref_tokens):\n    ref_tokens = [tf.compat.as_bytes(token) for token in ref_tokens]\n    text = tf.constant(text)\n    tokens = tokenizer.tokenize(text)\n    self.assertIsInstance(tokens, tf.Tensor)\n    self.assertAllEqual(ref_tokens, self.evaluate(tokens))\n\n  def _testTokenizerOnBatchTensor(self, tokenizer, text, ref_tokens):\n    text = tf.constant(text)\n    tokens = tokenizer.tokenize(text)\n    self.assertIsInstance(tokens, tf.RaggedTensor)\n    self.assertAllEqual(tokens.to_list(), tf.nest.map_structure(tf.compat.as_bytes, ref_tokens))\n\n  def _testTokenizerOnString(self, tokenizer, text, ref_tokens):\n    tokens = tokenizer.tokenize(text)\n    self.assertAllEqual(ref_tokens, tokens)\n\n  def _testTokenizer(self, tokenizer, text, ref_tokens):\n    self.assertAllEqual(tokenizer.tokenize(text), ref_tokens)\n    self._testTokenizerOnBatchTensor(tokenizer, text, ref_tokens)\n    for txt, ref in zip(text, ref_tokens):\n      self._testTokenizerOnTensor(tokenizer, txt, ref)\n      self._testTokenizerOnString(tokenizer, txt, ref)\n\n  def _testDetokenizerOnTensor(self, tokenizer, tokens, ref_text):\n    ref_text = tf.compat.as_bytes(ref_text)\n    tokens = tf.constant(tokens)\n    text = tokenizer.detokenize(tokens)\n    self.assertEqual(ref_text, self.evaluate(text))\n\n  def _testDetokenizerOnBatchTensor(self, tokenizer, tokens, ref_text):\n    ref_text = [tf.compat.as_bytes(t) for t in ref_text]\n    sequence_length = [len(x) for x in tokens]\n    max_length = max(sequence_length)\n    tokens = [tok + [""""] * (max_length - len(tok)) for tok in tokens]\n    tokens = tf.constant(tokens)\n    sequence_length = tf.constant(sequence_length)\n    text = tokenizer.detokenize(tokens, sequence_length=sequence_length)\n    self.assertAllEqual(ref_text, self.evaluate(text))\n\n  def _testDetokenizerOnBatchRaggedTensor(self, tokenizer, tokens, ref_text):\n    lengths = tf.constant([len(x) for x in tokens])\n    flat_tokens = tf.constant(tf.nest.flatten(tokens))\n    ragged_tokens = tf.RaggedTensor.from_row_lengths(flat_tokens, lengths)\n    text = tokenizer.detokenize(ragged_tokens)\n    self.assertAllEqual(self.evaluate(text), tf.nest.map_structure(tf.compat.as_bytes, ref_text))\n\n  def _testDetokenizerOnString(self, tokenizer, tokens, ref_text):\n    text = tokenizer.detokenize(tokens)\n    self.assertAllEqual(ref_text, text)\n\n  def _testDetokenizer(self, tokenizer, tokens, ref_text):\n    self.assertAllEqual(tokenizer.detokenize(tokens), ref_text)\n    self._testDetokenizerOnBatchTensor(tokenizer, tokens, ref_text)\n    self._testDetokenizerOnBatchRaggedTensor(tokenizer, tokens, ref_text)\n    for tok, ref in zip(tokens, ref_text):\n      self._testDetokenizerOnTensor(tokenizer, tok, ref)\n      self._testDetokenizerOnString(tokenizer, tok, ref)\n\n  def testSpaceTokenizer(self):\n    self._testTokenizer(\n        tokenizers.SpaceTokenizer(),\n        [""Hello world !"", ""How are you ?"", ""Good !""],\n        [[""Hello"", ""world"", ""!""], [""How"", ""are"", ""you"", ""?""], [""Good"", ""!""]])\n    self._testDetokenizer(\n        tokenizers.SpaceTokenizer(),\n        [[""Hello"", ""world"", ""!""], [""Test""], [""My"", ""name""]],\n        [""Hello world !"", ""Test"", ""My name""])\n\n  def testCharacterTokenizer(self):\n    self._testTokenizer(\n        tokenizers.CharacterTokenizer(),\n        [""a b"", ""cd e""],\n        [[""a"", ""\xe2\x96\x81"", ""b""], [""c"", ""d"", ""\xe2\x96\x81"", ""e""]])\n    self._testDetokenizer(\n        tokenizers.CharacterTokenizer(),\n        [[""a"", ""\xe2\x96\x81"", ""b""], [""c"", ""d"", ""\xe2\x96\x81"", ""e""]],\n        [""a b"", ""cd e""])\n    self._testTokenizer(\n        tokenizers.CharacterTokenizer(),\n        [""\xe4\xbd\xa0\xe5\xa5\xbd\xef\xbc\x8c\xe4\xb8\x96\xe7\x95\x8c\xef\xbc\x81""],\n        [[""\xe4\xbd\xa0"", ""\xe5\xa5\xbd"", ""\xef\xbc\x8c"", ""\xe4\xb8\x96"", ""\xe7\x95\x8c"", ""\xef\xbc\x81""]])\n\n  def testOpenNMTTokenizer(self):\n    self._testTokenizer(\n        tokenizers.OpenNMTTokenizer(),\n        [""Hello world!"", ""How are you?""],\n        [[""Hello"", ""world"", ""!""], [""How"", ""are"", ""you"", ""?""]])\n    self._testDetokenizer(\n        tokenizers.OpenNMTTokenizer(),\n        [[""Hello"", ""world"", ""\xef\xbf\xad!""], [""Test""], [""My"", ""name""]],\n        [""Hello world!"", ""Test"", ""My name""])\n\n  def testOpenNMTTokenizerEmptyTensor(self):\n    tokenizer = tokenizers.OpenNMTTokenizer()\n    tokens = tokenizer.tokenize(tf.constant(""""))\n    self.assertIs(tokens.dtype, tf.string)\n    self.assertListEqual(tokens.shape.as_list(), [0])\n    text = tokenizer.detokenize(tokens)\n    self.assertIs(text.dtype, tf.string)\n    self.assertListEqual(text.shape.as_list(), [])\n\n  def testOpenNMTTokenizerInFunction(self):\n    tokenizer = tokenizers.OpenNMTTokenizer()\n\n    @tf.function\n    def _tokenize(text):\n      return tokenizer.tokenize(text)\n\n    tokens = _tokenize(tf.constant(""Hello world!""))\n    self.assertAllEqual(self.evaluate(tokens), [b""Hello"", b""world"", b""!""])\n\n  def testOpenNMTTokenizerArguments(self):\n    tokenizer = tokenizers.OpenNMTTokenizer(\n        mode=""aggressive"", spacer_annotate=True, spacer_new=True)\n    self._testTokenizer(tokenizer, [""Hello World-s""], [[""Hello"", ""\xe2\x96\x81"", ""World"", ""-"", ""s""]])\n\n  def testOpenNMTTokenizerAssets(self):\n    asset_dir = self.get_temp_dir()\n    # Write a dummy BPE model.\n    bpe_model_path = os.path.join(asset_dir, ""model.bpe"")\n    with open(bpe_model_path, ""w"") as bpe_model_file:\n      bpe_model_file.write(""#version: 0.2\\ne s</w>\\n"")\n\n    tokenizer = tokenizers.OpenNMTTokenizer(mode=""conservative"", bpe_model_path=bpe_model_path)\n\n    # Generated assets are prefixed but not existing resources.\n    assets = tokenizer.export_assets(asset_dir, asset_prefix=""source_"")\n    self.assertIn(""source_tokenizer_config.yml"", assets)\n    self.assertTrue(os.path.exists(assets[""source_tokenizer_config.yml""]))\n    self.assertIn(""model.bpe"", assets)\n    self.assertTrue(os.path.exists(assets[""model.bpe""]))\n\n    # The tokenization configuration should not contain absolute paths to resources.\n    with open(assets[""source_tokenizer_config.yml""], ""rb"") as config_file:\n      asset_config = yaml.load(config_file.read(), Loader=yaml.UnsafeLoader)\n    self.assertDictEqual(asset_config, {""mode"": ""conservative"", ""bpe_model_path"": ""model.bpe""})\n\n  def testMakeTokenizer(self):\n    tokenizer = tokenizers.make_tokenizer()\n    self.assertIsInstance(tokenizer, tokenizers.SpaceTokenizer)\n    self.assertFalse(tokenizer.in_graph)\n    tokenizer = tokenizers.make_tokenizer({""type"": ""SpaceTokenizer""})\n    self.assertIsInstance(tokenizer, tokenizers.SpaceTokenizer)\n    self.assertTrue(tokenizer.in_graph)\n    self.assertIsInstance(\n        tokenizers.make_tokenizer({""mode"": ""conservative""}),\n        tokenizers.OpenNMTTokenizer)\n    self.assertIsInstance(\n        tokenizers.make_tokenizer({""type"": ""OpenNMTTokenizer"", ""params"": {""mode"": ""conservative""}}),\n        tokenizers.OpenNMTTokenizer)\n    config_path = os.path.join(self.get_temp_dir(), ""tok_config.yml"")\n    with open(config_path, ""w"") as config_file:\n      yaml.dump({""mode"": ""conservative""}, config_file)\n    self.assertIsInstance(\n        tokenizers.make_tokenizer(config_path),\n        tokenizers.OpenNMTTokenizer)\n    with self.assertRaisesRegex(ValueError, ""is not in list of""):\n      tokenizers.make_tokenizer({""type"": ""UnknownTokenizer""})\n    with self.assertRaisesRegex(ValueError, ""is not in list of""):\n      tokenizers.make_tokenizer({""type"": ""Tokenizer""})\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/training_test.py,9,"b'import os\n\nimport tensorflow as tf\n\nfrom opennmt import inputters\nfrom opennmt import models\nfrom opennmt import training\nfrom opennmt.tests import test_util\n\n\ndef _make_seq2seq_model(temp_dir):\n  vocab = test_util.make_vocab(os.path.join(temp_dir, ""vocab.txt""), [""1"", ""2"", ""3"", ""4""])\n  model = models.Transformer(\n      source_inputter=inputters.WordEmbedder(20),\n      target_inputter=inputters.WordEmbedder(20),\n      num_layers=3,\n      num_units=20,\n      num_heads=4,\n      ffn_inner_dim=40)\n  model.initialize(dict(source_vocabulary=vocab, target_vocabulary=vocab))\n  return model\n\n\nclass TrainingTest(tf.test.TestCase):\n\n  def testMovingAverage(self):\n    step = tf.Variable(0, trainable=False, dtype=tf.int64)\n    variables = [tf.Variable(1.0), tf.Variable(2.0)]\n    moving_average = training.MovingAverage(variables, step)\n    variables[0].assign(3.0)\n    variables[1].assign(4.0)\n    moving_average.update()\n    self.assertAllEqual(self.evaluate(variables), [3.0, 4.0])\n    with moving_average.shadow_variables():\n      self.assertAllClose(self.evaluate(variables), [2.8, 3.8])\n    self.assertAllEqual(self.evaluate(variables), [3.0, 4.0])\n\n  @test_util.run_with_two_cpu_devices\n  def testMovingAverageDistributionStrategy(self):\n    devices = tf.config.experimental.list_logical_devices(device_type=""CPU"")\n    strategy = tf.distribute.MirroredStrategy(devices=devices)\n\n    with strategy.scope():\n      variables = [tf.Variable(1.0), tf.Variable(2.0)]\n      step = tf.Variable(0, trainable=False, dtype=tf.int64)\n\n    moving_average = training.MovingAverage(variables, step)\n    variables[0].assign(3.0)\n    variables[1].assign(4.0)\n    moving_average.update()\n    self.assertAllEqual(self.evaluate(variables), [3.0, 4.0])\n    with moving_average.shadow_variables():\n      self.assertAllClose(self.evaluate(variables), [2.8, 3.8])\n    self.assertAllEqual(self.evaluate(variables), [3.0, 4.0])\n\n  def testEmptyTrainingDataset(self):\n    model = _make_seq2seq_model(self.get_temp_dir())\n    optimizer = tf.keras.optimizers.SGD(1.0)\n    trainer = training.Trainer(model, optimizer)\n\n    empty_file = os.path.join(self.get_temp_dir(), ""train.txt"")\n    with open(empty_file, ""w""):\n      pass\n    dataset = model.examples_inputter.make_training_dataset(empty_file, empty_file, 32)\n\n    with self.assertRaisesRegex(RuntimeError, ""No training steps""):\n      trainer(dataset)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/transformer_test.py,32,"b'from parameterized import parameterized\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.layers import transformer\n\n\nclass TransformerTest(tf.test.TestCase):\n\n  @parameterized.expand([[tf.bool], [tf.float32]])\n  def testBuildFutureMask(self, dtype):\n    length = [2, 4, 3]\n    expected = np.array([\n        [[1, 0, 0, 0],\n         [1, 1, 0, 0],\n         [1, 1, 0, 0],\n         [1, 1, 0, 0]],\n        [[1, 0, 0, 0],\n         [1, 1, 0, 0],\n         [1, 1, 1, 0],\n         [1, 1, 1, 1]],\n        [[1, 0, 0, 0],\n         [1, 1, 0, 0],\n         [1, 1, 1, 0],\n         [1, 1, 1, 0]]]).astype(dtype.as_numpy_dtype)\n\n    mask = transformer.future_mask(tf.constant(length), dtype=dtype)\n    self.assertIs(mask.dtype, dtype)\n    mask = self.evaluate(mask)\n    self.assertTupleEqual(mask.shape, (len(length), max(length), max(length)))\n    self.assertAllEqual(mask, expected)\n\n  @parameterized.expand([[tf.bool], [tf.float32]])\n  def testBuildFutureMaskWithMaxLen(self, dtype):\n    length = [2, 4, 3]\n    maximum_length = 5\n    expected = np.array([\n        [[1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0],\n         [1, 1, 0, 0, 0],\n         [1, 1, 0, 0, 0],\n         [1, 1, 0, 0, 0]],\n        [[1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0],\n         [1, 1, 1, 1, 0],\n         [1, 1, 1, 1, 0]],\n        [[1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0],\n         [1, 1, 1, 0, 0],\n         [1, 1, 1, 0, 0]]]).astype(dtype.as_numpy_dtype)\n\n    mask = transformer.future_mask(\n        tf.constant(length), maximum_length=maximum_length, dtype=dtype)\n    self.assertIs(mask.dtype, dtype)\n    mask = self.evaluate(mask)\n    self.assertTupleEqual(mask.shape, (len(length), maximum_length, maximum_length))\n    self.assertAllEqual(mask, expected)\n\n  def testSplitHeads(self):\n    batch_size = 3\n    length = [5, 3, 7]\n    num_heads = 8\n    depth = 20\n\n    inputs = tf.random.normal([batch_size, max(length), depth * num_heads], dtype=tf.float32)\n    outputs = transformer.split_heads(inputs, num_heads)\n\n    static_shape = outputs.shape\n    self.assertEqual(num_heads, static_shape[1])\n    self.assertEqual(depth, static_shape[-1])\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual([batch_size, num_heads, max(length), depth], outputs.shape)\n\n  def testCombineHeads(self):\n    batch_size = 3\n    length = [5, 3, 7]\n    num_heads = 8\n    depth = 20\n\n    inputs = tf.random.normal([batch_size, num_heads, max(length), depth], dtype=tf.float32)\n    outputs = transformer.combine_heads(inputs)\n\n    static_shape = outputs.shape\n    self.assertEqual(depth * num_heads, static_shape[-1])\n    outputs = self.evaluate(outputs)\n    self.assertAllEqual([batch_size, max(length), depth * num_heads], outputs.shape)\n\n  def testSplitAndCombineHeads(self):\n    batch_size = 3\n    length = [5, 3, 7]\n    num_heads = 8\n    depth = 20\n\n    inputs = tf.random.normal([batch_size, max(length), depth * num_heads], dtype=tf.float32)\n    split = transformer.split_heads(inputs, num_heads)\n    combined = transformer.combine_heads(split)\n    inputs, combined = self.evaluate([inputs, combined])\n    self.assertAllEqual(inputs, combined)\n\n  def testRelativePositions(self):\n    positions = transformer.relative_positions(4, 2)\n    self.assertAllEqual(\n        self.evaluate(positions),\n        [[2, 3, 4, 4], [1, 2, 3, 4], [0, 1, 2, 3], [0, 0, 1, 2]])\n\n  def testFeedForwardNetwork(self):\n    ffn = transformer.FeedForwardNetwork(20, 10)\n    x = tf.random.uniform([4, 5, 10])\n    y = ffn(x)\n    self.assertEqual(y.shape, x.shape)\n\n  def testMultiHeadSelfAttention(self):\n    attention = transformer.MultiHeadAttention(4, 20)\n    queries = tf.random.uniform([4, 5, 10])\n    mask = tf.sequence_mask([4, 3, 5, 2])\n    context, _ = attention(queries, mask=mask)\n    self.assertListEqual(context.shape.as_list(), [4, 5, 20])\n\n  def testMultiHeadSelfAttentionWithCache(self):\n    cache = (tf.zeros([4, 4, 0, 5]), tf.zeros([4, 4, 0, 5]))\n    attention = transformer.MultiHeadAttention(4, 20)\n    x = tf.random.uniform([4, 1, 10])\n    _, cache = attention(x, cache=cache)\n    self.assertEqual(cache[0].shape[2], 1)\n    self.assertEqual(cache[1].shape[2], 1)\n    _, cache = attention(x, cache=cache)\n    self.assertEqual(cache[0].shape[2], 2)\n    self.assertEqual(cache[1].shape[2], 2)\n\n  def testMultiHeadSelfAttentionRelativePositions(self):\n    attention = transformer.MultiHeadAttention(4, 20, maximum_relative_position=6)\n    x = tf.random.uniform([2, 9, 10])\n    mask = tf.sequence_mask([9, 7])\n    y = attention(x, mask=mask)\n\n  def testMultiHeadSelfAttentionRelativePositionsWithCache(self):\n    attention = transformer.MultiHeadAttention(4, 20, maximum_relative_position=6)\n    x = tf.random.uniform([4, 1, 10])\n    cache = (tf.zeros([4, 4, 0, 5]), tf.zeros([4, 4, 0, 5]))\n    _, cache = attention(x, cache=cache)\n\n  def testMultiHeadAttention(self):\n    attention = transformer.MultiHeadAttention(4, 20)\n    queries = tf.random.uniform([4, 5, 10])\n    memory = tf.random.uniform([4, 3, 10])\n    mask = tf.sequence_mask([1, 3, 2, 2])\n    context, _ = attention(queries, memory=memory, mask=mask)\n    self.assertListEqual(context.shape.as_list(), [4, 5, 20])\n\n  def testMultiHeadAttentionWithCache(self):\n    cache = (tf.zeros([4, 4, 0, 5]), tf.zeros([4, 4, 0, 5]))\n    attention = transformer.MultiHeadAttention(4, 20)\n    memory = tf.random.uniform([4, 3, 10])\n    mask = tf.sequence_mask([1, 3, 2, 2])\n    x = tf.random.uniform([4, 1, 10])\n    y1, cache = attention(x, memory=memory, mask=mask, cache=cache)\n    self.assertEqual(cache[0].shape[2], 3)\n    self.assertEqual(cache[1].shape[2], 3)\n    y2, cache = attention(x, memory=memory, mask=mask, cache=cache)\n    self.assertAllEqual(y1, y2)\n\n  def testMultiHeadAttentionMask(self):\n    attention = transformer.MultiHeadAttention(4, 20, return_attention=True)\n    queries = tf.random.uniform([4, 5, 10])\n    memory = tf.random.uniform([4, 3, 10])\n    mask = tf.sequence_mask([1, 3, 2, 2])\n    _, _, attention = attention(queries, memory=memory, mask=mask)\n    attention = tf.reshape(attention, [4, -1, 3])\n    mask = tf.broadcast_to(tf.expand_dims(mask, 1), attention.shape)\n    padding = tf.boolean_mask(attention, tf.logical_not(mask))\n    self.assertAllEqual(tf.reduce_sum(padding), 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tests/vocab_test.py,4,"b'# -*- coding: utf-8 -*-\n\nimport os\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom opennmt.data import vocab as vocab_lib\nfrom opennmt.tests import test_util\n\n\nclass VocabTest(tf.test.TestCase):\n\n  def testSimpleVocab(self):\n    vocab = vocab_lib.Vocab()\n\n    self.assertEqual(0, vocab.size)\n\n    vocab.add(""toto"")\n    vocab.add(""toto"")\n    vocab.add(""toto"")\n    vocab.add(""titi"")\n    vocab.add(""titi"")\n    vocab.add(""tata"")\n\n    self.assertEqual(3, vocab.size)\n    self.assertEqual(1, vocab.lookup(""titi""))\n    self.assertEqual(""titi"", vocab.lookup(1))\n\n    pruned_size = vocab.prune(max_size=2)\n\n    self.assertEqual(2, pruned_size.size)\n    self.assertEqual(None, pruned_size.lookup(""tata""))\n\n    pruned_frequency = vocab.prune(min_frequency=3)\n\n    self.assertEqual(1, pruned_frequency.size)\n    self.assertEqual(0, pruned_frequency.lookup(""toto""))\n\n  def testVocabWithSpecialTokens(self):\n    vocab = vocab_lib.Vocab(special_tokens=[""foo"", ""bar""])\n\n    self.assertEqual(2, vocab.size)\n\n    vocab.add(""toto"")\n    vocab.add(""toto"")\n    vocab.add(""toto"")\n    vocab.add(""titi"")\n    vocab.add(""titi"")\n    vocab.add(""tata"")\n\n    self.assertEqual(5, vocab.size)\n    self.assertEqual(3, vocab.lookup(""titi""))\n\n    pruned_size = vocab.prune(max_size=3)\n\n    self.assertEqual(3, pruned_size.size)\n    self.assertEqual(0, pruned_size.lookup(""foo""))\n    self.assertEqual(1, pruned_size.lookup(""bar""))\n\n  def testVocabSaveAndLoad(self):\n    vocab1 = vocab_lib.Vocab(special_tokens=[""foo"", ""bar""])\n    vocab1.add(""toto"")\n    vocab1.add(""toto"")\n    vocab1.add(""toto"")\n    vocab1.add(""titi"")\n    vocab1.add(""titi"")\n    vocab1.add(""tata"")\n\n    vocab_file = os.path.join(self.get_temp_dir(), ""vocab.txt"")\n\n    vocab1.serialize(vocab_file)\n    vocab2 = vocab_lib.Vocab.from_file(vocab_file)\n\n    self.assertEqual(vocab1.size, vocab2.size)\n    self.assertEqual(vocab1.lookup(""titi""), vocab2.lookup(""titi""))\n\n  def testLoadSentencePieceVocab(self):\n    vocab_path = test_util.make_data_file(\n        os.path.join(self.get_temp_dir(), ""vocab_sp""),\n        [\n            ""<unk>\t0"",\n            ""<s>\t0"",\n            ""</s>\t0"",\n            "",\t-3.0326"",\n            "".\t-3.41093"",\n            ""\xe2\x96\x81the\t-3.85169"",\n            ""s\t-4.05468"",\n            ""\xe2\x96\x81die\t-4.15914"",\n            ""\xe2\x96\x81in\t-4.2419"",\n            ""\xe2\x96\x81der\t-4.36135""\n        ])\n\n    vocab = vocab_lib.Vocab.from_file(vocab_path, file_format=""sentencepiece"")\n    self.assertEqual(len(vocab), 7)\n    self.assertNotIn(""<unk>"", vocab)\n    self.assertNotIn(""<s>"", vocab)\n    self.assertNotIn(""</s>"", vocab)\n    self.assertIn(""\xe2\x96\x81the"", vocab)\n\n  def testVocabPadding(self):\n    vocab = vocab_lib.Vocab()\n    vocab.add(""toto"")\n    vocab.add(""titi"")\n    vocab.add(""tata"")\n    self.assertEqual(vocab.size, 3)\n    vocab.pad_to_multiple(6, num_oov_buckets=1)\n    self.assertEqual(vocab.size, 6 - 1)\n\n  def testVocabNoPadding(self):\n    vocab = vocab_lib.Vocab()\n    vocab.add(""toto"")\n    vocab.add(""titi"")\n    vocab.add(""tata"")\n    self.assertEqual(vocab.size, 3)\n    vocab.pad_to_multiple(4, num_oov_buckets=1)\n    self.assertEqual(vocab.size, 3)\n\n  def _saveVocab(self, name, words):\n    vocab = vocab_lib.Vocab()\n    for word in words:\n      vocab.add(str(word))\n    vocab_file = os.path.join(self.get_temp_dir(), name)\n    vocab.serialize(vocab_file)\n    return vocab_file\n\n  def testVocabMappingMerge(self):\n    old = self._saveVocab(""old"", [""1"", ""2"", ""3"", ""4""])\n    new = self._saveVocab(""new"", [""1"", ""6"", ""3"", ""5"", ""7""])\n    mapping, new_vocab = vocab_lib.get_mapping(old, new, ""merge"")\n    self.assertEqual(4 + 5 - 2 + 1, len(mapping))  # old + new - common + <unk>\n    self.assertAllEqual([0, 1, 2, 3, -1, -1, -1, 4], mapping)\n    self.assertAllEqual([""1"", ""2"", ""3"", ""4"", ""6"", ""5"", ""7""], new_vocab.words)\n\n  def testVocabMappingReplace(self):\n    old = self._saveVocab(""old"", [""1"", ""2"", ""3"", ""4""])\n    new = self._saveVocab(""new"", [""1"", ""6"", ""5"", ""3"", ""7""])\n    mapping, new_vocab = vocab_lib.get_mapping(old, new, ""replace"")\n    self.assertEqual(5 + 1, len(mapping))  # new + <unk>\n    self.assertAllEqual([0, -1, -1, 2, -1, 4], mapping)\n    self.assertAllEqual([""1"", ""6"", ""5"", ""3"", ""7""], new_vocab.words)\n\n  def testVocabVariableUpdate(self):\n    ref_variable, ref_optimizer = _create_variable_and_slots([1, 2, 3, 4, 5, 6, 7])\n    new_variable, new_optimizer = _create_variable_and_slots([0, 0, 0, 0, 0, 0])\n    mapping = [0, -1, -1, 4, -1, 2]\n    expected = [1, 0, 0, 5, 0, 3]\n    vocab_lib.update_variable_and_slots(\n        ref_variable,\n        new_variable,\n        ref_optimizer,\n        new_optimizer,\n        mapping)\n    variables = [new_variable] + [new_optimizer.get_slot(new_variable, slot) for slot in (""m"", ""v"")]\n    for variable in self.evaluate(variables):\n      self.assertAllEqual(variable, expected)\n\n\ndef _create_variable_and_slots(values):\n  variable = tf.Variable(tf.constant(values, dtype=tf.float32))\n  optimizer = tf.keras.optimizers.Adam()\n  optimizer._create_slots([variable])\n  for slot in (""m"", ""v""):\n    optimizer.get_slot(variable, slot).assign(variable)\n  return variable, optimizer\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
opennmt/tokenizers/__init__.py,1,"b'""""""Module defining tokenizers.\n\nTokenizers can work on string ``tf.Tensor`` as in-graph transformation.\n""""""\n\ntry:\n  import pyonmttok\n  from opennmt.tokenizers.opennmt_tokenizer import OpenNMTTokenizer\nexcept ImportError:\n  pass\n\nfrom opennmt.tokenizers.tokenizer import Tokenizer\nfrom opennmt.tokenizers.tokenizer import SpaceTokenizer\nfrom opennmt.tokenizers.tokenizer import CharacterTokenizer\nfrom opennmt.tokenizers.tokenizer import make_tokenizer\nfrom opennmt.tokenizers.tokenizer import register_tokenizer\n'"
opennmt/tokenizers/opennmt_tokenizer.py,2,"b'""""""Define the OpenNMT tokenizer.""""""\n\nimport os\nimport yaml\n\nimport tensorflow as tf\n\nimport pyonmttok\n\nfrom opennmt.tokenizers import tokenizer\n\n\n@tokenizer.register_tokenizer\nclass OpenNMTTokenizer(tokenizer.Tokenizer):\n  """"""Uses the OpenNMT tokenizer.""""""\n\n  def __init__(self, **kwargs):\n    kwargs.setdefault(""mode"", ""conservative"")\n    self._config = kwargs\n    self._tokenizer = pyonmttok.Tokenizer(**kwargs)\n\n  def export_assets(self, asset_dir, asset_prefix=""""):\n    assets = {}\n\n    # Extract asset files from the configuration.\n    config = self._config.copy()\n    for key, value in config.items():\n      if isinstance(value, str) and tf.io.gfile.exists(value):\n        basename = os.path.basename(value)\n        config[key] = basename  # Only save the basename.\n        assets[basename] = value\n\n    # Save the tokenizer configuration.\n    config_name = ""%stokenizer_config.yml"" % asset_prefix\n    config_path = os.path.join(asset_dir, config_name)\n    assets[config_name] = config_path\n    with tf.io.gfile.GFile(config_path, ""w"") as config_file:\n      yaml.dump(config, stream=config_file, default_flow_style=False)\n\n    return assets\n\n  def _tokenize_string(self, text):\n    tokens, _ = self._tokenizer.tokenize(text)\n    return tokens\n\n  def _detokenize_string(self, tokens):\n    return self._tokenizer.detokenize(tokens)\n'"
opennmt/tokenizers/tokenizer.py,46,"b'# -*- coding: utf-8 -*-\n\n""""""Define base tokenizers.""""""\n\nimport sys\nimport abc\nimport yaml\n\nimport tensorflow as tf\n\nfrom opennmt.utils import misc\n\n\nclass Tokenizer(abc.ABC):\n  """"""Base class for tokenizers.""""""\n\n  @property\n  def in_graph(self):\n    """"""Returns ``True`` if this tokenizer can be run in graph (i.e. uses TensorFlow ops).""""""\n    return False\n\n  def export_assets(self, asset_dir, asset_prefix=""""):  # pylint: disable=unused-argument\n    """"""Exports assets for this tokenizer.\n\n    Args:\n      asset_dir: The directory where assets can be written.\n      asset_prefix: The prefix to attach to assets filename.\n\n    Returns:\n      A dictionary containing additional assets used by the tokenizer.\n    """"""\n    return {}\n\n  def tokenize_stream(self, input_stream=sys.stdin, output_stream=sys.stdout, delimiter="" ""):\n    """"""Tokenizes a stream of sentences.\n\n    Args:\n      input_stream: The input stream.\n      output_stream: The output stream.\n      delimiter: The token delimiter to use for text serialization.\n    """"""\n    for line in input_stream:\n      line = line.strip()\n      tokens = self.tokenize(line)\n      merged_tokens = delimiter.join(tokens)\n      misc.print_as_bytes(merged_tokens, stream=output_stream)\n\n  def detokenize_stream(self, input_stream=sys.stdin, output_stream=sys.stdout, delimiter="" ""):\n    """"""Detokenizes a stream of sentences.\n\n    Args:\n      input_stream: The input stream.\n      output_stream: The output stream.\n      delimiter: The token delimiter used for text serialization.\n    """"""\n    for line in input_stream:\n      tokens = line.strip().split(delimiter)\n      string = self.detokenize(tokens)\n      misc.print_as_bytes(string, stream=output_stream)\n\n  def tokenize(self, text):\n    """"""Tokenizes text.\n\n    Args:\n      text: A string or batch of strings to tokenize as a ``tf.Tensor`` or\n        Python values.\n\n    Returns:\n      - If :obj:`text` is a Python string, a list of Python strings.\n      - If :obj:`text` is a list of Python strings, a list of list of Python\n        strings.\n      - If :obj:`text` is a 0-D ``tf.Tensor``, a 1-D ``tf.Tensor``.\n      - If :obj:`text` is a 1-D ``tf.Tensor``, a 2-D ``tf.RaggedTensor``.\n\n    Raises:\n      ValueError: if the rank of :obj:`text` is greater than 1.\n    """"""\n    with tf.device(""cpu:0""):\n      return self._tokenize(text)\n\n  def _tokenize(self, text):\n    if tf.is_tensor(text):\n      rank = len(text.shape)\n      if rank == 0:\n        return self._tokenize_tensor(text)\n      elif rank == 1:\n        return self._tokenize_batch_tensor(text)\n      else:\n        raise ValueError(""Unsupported tensor rank %d for tokenization"" % rank)\n    elif isinstance(text, list):\n      return list(map(self.tokenize, text))\n    else:\n      text = tf.compat.as_text(text)\n      return self._tokenize_string(text)\n\n  def detokenize(self, tokens, sequence_length=None):\n    """"""Detokenizes tokens.\n\n    The Tensor version supports batches of tokens.\n\n    Args:\n      tokens: Tokens or batch of tokens as a ``tf.Tensor``, ``tf.RaggedTensor``,\n        or Python values.\n      sequence_length: The length of each sequence. Required if :obj:`tokens`\n        is a dense 2-D ``tf.Tensor``.\n\n    Returns:\n      - If :obj:`tokens` is a list of list of Python strings, a list of Python strings.\n      - If :obj:`tokens` is a list of Python strings, a Python string.\n      - If :obj:`tokens` is a N-D ``tf.Tensor`` (or ``tf.RaggedTensor``), a\n        (N-1)-D ``tf.Tensor``.\n\n    Raises:\n      ValueError: if the rank of :obj:`tokens` is greater than 2.\n      ValueError: if :obj:`tokens` is a 2-D dense ``tf.Tensor`` and\n        :obj:`sequence_length` is not set.\n    """"""\n    with tf.device(""cpu:0""):\n      return self._detokenize(tokens, sequence_length)\n\n  def _detokenize(self, tokens, sequence_length):\n    if tf.is_tensor(tokens):\n      rank = len(tokens.shape)\n      if rank == 1:\n        return self._detokenize_tensor(tokens)\n      elif rank == 2:\n        if sequence_length is None:\n          raise ValueError(""sequence_length is required for Tensor detokenization"")\n        return self._detokenize_batch_tensor(tokens, sequence_length)\n      else:\n        raise ValueError(""Unsupported tensor rank %d for detokenization"" % rank)\n    elif isinstance(tokens, tf.RaggedTensor):\n      rank = len(tokens.shape)\n      if rank == 1:\n        return self._detokenize_tensor(tokens.values)\n      elif rank == 2:\n        return self._detokenize_ragged_tensor(tokens)\n      else:\n        raise ValueError(""Unsupported RaggedTensor rank %d for detokenization"" % rank)\n    elif isinstance(tokens, list) and tokens and isinstance(tokens[0], list):\n      return list(map(self.detokenize, tokens))\n    else:\n      tokens = [tf.compat.as_text(token) for token in tokens]\n      return self._detokenize_string(tokens)\n\n  def _tokenize_tensor(self, text):\n    """"""Tokenizes a tensor.\n\n    When not overriden, this default implementation calls the string-based\n    tokenization.\n\n    Args:\n      text: A 1-D string ``tf.Tensor``.\n\n    Returns:\n      A 1-D string ``tf.Tensor``.\n    """"""\n    def _python_wrapper(string_t):\n      string = tf.compat.as_text(string_t.numpy())\n      tokens = self._tokenize_string(string)\n      return tf.constant(tokens, dtype=tf.string)\n    tokens = tf.py_function(_python_wrapper, [text], tf.string)\n    tokens.set_shape([None])\n    return tokens\n\n  def _tokenize_batch_tensor(self, text):\n    """"""Tokenizes a batch of texts.\n\n    When not overriden, this default implementation calls _tokenize_tensor on\n    each tensor within the batch.\n\n    Args:\n      text: A 1-D string ``tf.Tensor``.\n\n    Returns:\n      A 2-D string ``tf.RaggedTensor``.\n    """"""\n    # map_fn expects each output element to have the same shape, so join tokens with\n    # spaces first and then split on spaces with a function returning a RaggedTensor.\n    tokens = tf.map_fn(\n        lambda x: tf.strings.reduce_join(self._tokenize_tensor(x), axis=0, separator="" ""), text)\n    return tf.strings.split(tokens, sep="" "")\n\n  def _detokenize_tensor(self, tokens):\n    """"""Detokenizes tokens.\n\n    When not overriden, this default implementation calls the string-based\n    detokenization.\n\n    Args:\n      tokens: A 1-D ``tf.Tensor``.\n\n    Returns:\n      A 0-D string ``tf.Tensor``.\n    """"""\n    def _python_wrapper(tokens_t):\n      tokens = [tf.compat.as_text(s) for s in tokens_t.numpy()]\n      string = self._detokenize_string(tokens)\n      return tf.constant(string)\n    text = tf.py_function(_python_wrapper, [tokens], tf.string)\n    text.set_shape([])\n    return text\n\n  def _detokenize_batch_tensor(self, tokens, sequence_length):\n    """"""Detokenizes a batch of tokens.\n\n    When not overriden, this default implementation calls _detokenize_tensor on\n    each tensor within the batch.\n\n    Args:\n      tokens: A 2-D ``tf.Tensor``.\n\n    Returns:\n      A 1-D string ``tf.Tensor``.\n    """"""\n    return tf.map_fn(\n        lambda x: self._detokenize_tensor(x[0][:x[1]]),\n        (tokens, sequence_length),\n        dtype=tf.string)\n\n  def _detokenize_ragged_tensor(self, tokens):\n    """"""Detokenizes a batch of tokens as a ``tf.RaggedTensor``\n\n    When not overriden, this default implementation calls _detokenize_batch_tensor\n    on the dense representation.\n\n    Args:\n      tokens: A 2-D ``tf.RaggedTensor``.\n\n    Returns:\n      A 1-D string ``tf.Tensor``.\n    """"""\n    return self._detokenize_batch_tensor(tokens.to_tensor(), tokens.row_lengths())\n\n  @abc.abstractmethod\n  def _tokenize_string(self, text):\n    """"""Tokenizes a Python unicode string.\n\n    This method should be thread-safe.\n\n    Args:\n      text: A Python unicode string.\n\n    Returns:\n      A list of Python unicode strings.\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def _detokenize_string(self, tokens):\n    """"""Detokenizes tokens.\n\n    Args:\n      tokens: A list of Python unicode strings.\n\n    Returns:\n      A unicode Python string.\n    """"""\n    raise NotImplementedError()\n\n\n_TOKENIZERS_REGISTRY = misc.ClassRegistry(base_class=Tokenizer)\n\nregister_tokenizer = _TOKENIZERS_REGISTRY.register  # pylint: disable=invalid-name\n\ndef make_tokenizer(config=None):\n  """"""Creates a tokenizer instance from the configuration.\n\n  Args:\n    config: Path to a configuration file or the configuration dictionary.\n\n  Returns:\n    A :class:`opennmt.tokenizers.Tokenizer` instance.\n\n  Raises:\n    ValueError: if :obj:`config` is invalid.\n  """"""\n  if config:\n    if isinstance(config, str) and tf.io.gfile.exists(config):\n      with tf.io.gfile.GFile(config, mode=""rb"") as config_file:\n        config = yaml.load(config_file, Loader=yaml.UnsafeLoader)\n    if isinstance(config, dict):\n      tokenizer_type = config.get(""type"")\n      if tokenizer_type is None:\n        tokenizer_type = ""OpenNMTTokenizer""\n        tokenizer_params = config\n      else:\n        tokenizer_params = config.get(""params"", {})\n      tokenizer_class = _TOKENIZERS_REGISTRY.get(tokenizer_type)\n      if tokenizer_class is None:\n        raise ValueError(""%s is not in list of accepted tokenizers: %s"" % (\n            tokenizer_type, "", "".join(sorted(_TOKENIZERS_REGISTRY.class_names))))\n      tokenizer = tokenizer_class(**tokenizer_params)\n    else:\n      raise ValueError(""Invalid tokenization configuration: %s"" % str(config))\n  else:\n    # If the tokenization was not configured, we assume that an external tokenization\n    # was used and we don\'t include the tokenizer in the exported graph.\n    tokenizer = SpaceTokenizer(in_graph=False)\n  return tokenizer\n\n@register_tokenizer\nclass SpaceTokenizer(Tokenizer):\n  """"""A tokenizer that splits on spaces.""""""\n\n  def __init__(self, in_graph=True):\n    """"""Initializes the tokenizer.\n\n    Args:\n      in_graph: If ``True``, this tokenizer should be integrated in the exported graph.\n    """"""\n    self._in_graph = in_graph\n\n  @property\n  def in_graph(self):\n    return self._in_graph\n\n  def _tokenize_tensor(self, text):\n    return self._tokenize_batch_tensor(text)\n\n  def _tokenize_batch_tensor(self, text):\n    return tf.strings.split(text)\n\n  def _detokenize_tensor(self, tokens):\n    return self._detokenize_ragged_tensor(tokens)\n\n  def _detokenize_batch_tensor(self, tokens, sequence_length):\n    ragged = tf.RaggedTensor.from_tensor(tokens, lengths=sequence_length)\n    return self._detokenize_ragged_tensor(ragged)\n\n  def _detokenize_ragged_tensor(self, tokens):\n    return tf.strings.reduce_join(tokens, axis=tokens.shape.rank - 1, separator="" "")\n\n  def _tokenize_string(self, text):\n    return text.split()\n\n  def _detokenize_string(self, tokens):\n    return "" "".join(tokens)\n\n\n@register_tokenizer\nclass CharacterTokenizer(Tokenizer):\n  """"""A tokenizer that splits unicode characters.""""""\n\n  @property\n  def in_graph(self):\n    return True\n\n  def _tokenize_tensor(self, text):\n    return self._tokenize_batch_tensor(text)\n\n  def _tokenize_batch_tensor(self, text):\n    text = tf.strings.regex_replace(text, "" "", ""\xe2\x96\x81"")\n    return tf.strings.unicode_split(text, ""UTF-8"")\n\n  def _detokenize_tensor(self, tokens):\n    return self._detokenize_ragged_tensor(tokens)\n\n  def _detokenize_batch_tensor(self, tokens, sequence_length):\n    _ = sequence_length\n    return self._detokenize_ragged_tensor(tokens)\n\n  def _detokenize_ragged_tensor(self, tokens):\n    text = tf.strings.reduce_join(tokens, axis=tokens.shape.rank - 1)\n    return tf.strings.regex_replace(text, ""\xe2\x96\x81"", "" "")\n\n  def _tokenize_string(self, text):\n    return list(text.replace("" "", u""\xe2\x96\x81""))\n\n  def _detokenize_string(self, tokens):\n    return """".join(tokens).replace(u""\xe2\x96\x81"", "" "")\n'"
opennmt/utils/__init__.py,0,"b'""""""Module defining various utilities.""""""\n\nfrom opennmt.utils.checkpoint import average_checkpoints\nfrom opennmt.utils.checkpoint import average_checkpoints_into_layer\nfrom opennmt.utils.checkpoint import is_v1_checkpoint\n\nfrom opennmt.utils.decoding import BeamSearch\nfrom opennmt.utils.decoding import BestSampler\nfrom opennmt.utils.decoding import DecodingResult\nfrom opennmt.utils.decoding import DecodingStrategy\nfrom opennmt.utils.decoding import GreedySearch\nfrom opennmt.utils.decoding import RandomSampler\nfrom opennmt.utils.decoding import Sampler\nfrom opennmt.utils.decoding import dynamic_decode\n\nfrom opennmt.utils.exporters import CTranslate2Exporter\nfrom opennmt.utils.exporters import Exporter\nfrom opennmt.utils.exporters import SavedModelExporter\nfrom opennmt.utils.exporters import register_exporter\n\nfrom opennmt.utils.losses import cross_entropy_loss\nfrom opennmt.utils.losses import cross_entropy_sequence_loss\nfrom opennmt.utils.losses import guided_alignment_cost\nfrom opennmt.utils.losses import max_margin_loss\nfrom opennmt.utils.losses import regularization_penalty\n\nfrom opennmt.utils.misc import format_translation_output\n\nfrom opennmt.utils.scorers import BLEUScorer\nfrom opennmt.utils.scorers import PRFScorer\nfrom opennmt.utils.scorers import ROUGEScorer\nfrom opennmt.utils.scorers import Scorer\nfrom opennmt.utils.scorers import TERScorer\nfrom opennmt.utils.scorers import WERScorer\nfrom opennmt.utils.scorers import register_scorer\n\nfrom opennmt.utils.tensor import roll_sequence\n'"
opennmt/utils/checkpoint.py,20,"b'""""""Checkpoint utilities.""""""\n\nimport copy\nimport os\nimport tempfile\n\nimport tensorflow as tf\n\nfrom opennmt.utils import misc\n\n\nclass Checkpoint(object):\n  """"""Wrapper around TensorFlow checkpoints utilities.""""""\n\n  def __init__(self, model, optimizer=None, model_dir=None, keep_checkpoint_max=8):\n    """"""Initializes the wrapper.\n\n    Args:\n      model: A :class:`opennmt.models.Model` to save.\n      optimizer: The optimizer instance.\n      model_dir: The directory where checkpoints will be saved. If not set, a\n        temporary directory will be used.\n      keep_checkpoint_max: The maximum number of checkpoints to keep.\n    """"""\n    if model_dir is None:\n      model_dir = tempfile.mkdtemp()\n    trackables = {}\n    trackables[""model""] = model\n    if optimizer is not None:\n      trackables[""optimizer""] = optimizer\n    self._model = model\n    self._optimizer = optimizer\n    self._model_dir = model_dir\n    self._checkpoint = tf.train.Checkpoint(**trackables)\n    self._checkpoint_manager = tf.train.CheckpointManager(\n        self._checkpoint, model_dir, keep_checkpoint_max)\n\n  @classmethod\n  def from_config(cls, config, model, optimizer=None):\n    """"""Creates a checkpoint wrapper from the configuration.\n\n    Args:\n      config: The user configuration.\n      model: A :class:`opennmt.models.Model` to save.\n      optimizer: The optimizer instance.\n\n    Returns:\n      A :class:`openmt.utils.Checkpoint` instance.\n    """"""\n    train_config = config.get(""train"")\n    if train_config is None:\n      train_config = {}\n    return cls(\n        model,\n        optimizer=optimizer,\n        model_dir=config.get(""model_dir""),\n        keep_checkpoint_max=train_config.get(""keep_checkpoint_max"", 8))\n\n  @property\n  def model(self):\n    """"""The managed model.""""""\n    return self._model\n\n  @property\n  def optimizer(self):\n    """"""The managed optimizer.""""""\n    return self._optimizer\n\n  @property\n  def model_dir(self):\n    """"""The model directory.""""""\n    return self._model_dir\n\n  @property\n  def last_saved_step(self):\n    """"""The last training step that was saved.""""""\n    latest_checkpoint = self._checkpoint_manager.latest_checkpoint\n    if latest_checkpoint is None:\n      return None\n    return get_step_from_checkpoint_prefix(latest_checkpoint)\n\n  def save(self, step=None):\n    """"""Saves a checkpoint.\n\n    Args:\n      step: The step to save for. If ``None``, get the value from ``optimizer.iterations``.\n\n    Returns:\n      The path to the saved checkpoint.\n    """"""\n    if step is None:\n      step = self._optimizer.iterations\n    path = self._checkpoint_manager.save(checkpoint_number=step)\n    tf.get_logger().info(""Saved checkpoint %s"", path)\n    return path\n\n  def restore(self, checkpoint_path=None, weights_only=False):\n    """"""Restores a checkpoint.\n\n    Args:\n      checkpoint_path: Path a checkpoint to restore. If not set, the latest\n        checkpoint from :obj:`model_dir` will be restored.\n      weights_only: Only restore model weights.\n\n    Returns:\n      Path to the restored checkpoint.\n    """"""\n    if weights_only:\n      checkpoint = tf.train.Checkpoint(model=self._model)\n    else:\n      checkpoint = self._checkpoint\n    if checkpoint_path is not None:\n      if tf.io.gfile.isdir(checkpoint_path):\n        checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n    elif self._checkpoint_manager.latest_checkpoint is not None:\n      checkpoint_path = self._checkpoint_manager.latest_checkpoint\n    if checkpoint_path is None:\n      tf.get_logger().warning(""No checkpoint to restore in %s"", self._model_dir)\n      return None\n    if is_v1_checkpoint(checkpoint_path):\n      tf.get_logger().info(""Upgrading V1 checkpoint..."")\n      # Work with copies of model and optimizer as the downstream task might\n      # need to create the variable differently (e.g. under a distribution\n      # strategy scope).\n      tmp_model = misc.clone_layer(self._model)\n      tmp_optimizer = copy.deepcopy(self._optimizer) if self._optimizer is not None else None\n      tmp_model.create_variables(optimizer=tmp_optimizer)\n      step = _restore_v1_checkpoint(\n          checkpoint_path, tmp_model, optimizer=tmp_optimizer)\n      # Save an updated checkpoint in the model directory and restore this one instead.\n      tmp_checkpoint = Checkpoint(\n          tmp_model, optimizer=tmp_optimizer, model_dir=self._model_dir)\n      checkpoint_path = tmp_checkpoint.save(step)\n      return self.restore(checkpoint_path=checkpoint_path, weights_only=weights_only)\n    load_status = checkpoint.restore(checkpoint_path)\n    if weights_only:\n      load_status.expect_partial()\n    tf.get_logger().info(""Restored checkpoint %s"", checkpoint_path)\n    return checkpoint_path\n\n\ndef get_step_from_checkpoint_prefix(prefix):\n  """"""Extracts the training step from the checkpoint file prefix.""""""\n  return int(prefix.split(""-"")[-1])\n\ndef is_v1_checkpoint(checkpoint_path):\n  """"""Returns ``True`` if the checkpoint at :obj:`checkpoint_path` has been\n  trained with OpenNMT-tf v1.\n  """"""\n  if tf.io.gfile.isdir(checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n  return os.path.basename(checkpoint_path).startswith(""model"")\n\ndef get_checkpoint_variables(checkpoint_path):\n  """"""Returns variables included in a checkpoint.\n\n  Args:\n    checkpoint_path: Path to the checkpoint.\n\n  Returns:\n    A dictionary mapping variables name to value.\n  """"""\n  reader = tf.train.load_checkpoint(checkpoint_path)\n  return {\n      name:reader.get_tensor(name)\n      for name in reader.get_variable_to_shape_map().keys()}\n\ndef average_checkpoints(model_dir,\n                        output_dir,\n                        trackables,\n                        max_count=8,\n                        model_key=""model""):\n  """"""Averages object-based checkpoints.\n\n  Args:\n    model_dir: The directory containing checkpoints.\n    output_dir: The directory that will contain the averaged checkpoint.\n    trackables: A dictionary containing the trackable objects included in the\n      checkpoint.\n    max_count: The maximum number of checkpoints to average.\n    model_key: The key in :obj:`trackables` that references the model.\n\n  Returns:\n    The path to the directory containing the averaged checkpoint.\n\n  Raises:\n    ValueError: if :obj:`output_dir` is the same as :obj:`model_dir`.\n    ValueError: if a model is not found in :obj:`trackables` or is not already\n      built.\n    ValueError: if no checkpoints are found in :obj:`model_dir`.\n\n  See Also:\n    :func:`opennmt.utils.average_checkpoints_into_layer`\n  """"""\n  if model_dir == output_dir:\n    raise ValueError(""Model and output directory must be different"")\n  model = trackables.get(model_key)\n  if model is None:\n    raise ValueError(""%s not found in trackables %s"" % (model_key, trackables))\n\n  checkpoint_state = tf.train.get_checkpoint_state(model_dir)\n  if checkpoint_state is None:\n    raise ValueError(""No checkpoints found in %s"" % model_dir)\n  checkpoints_path = checkpoint_state.all_model_checkpoint_paths\n  if len(checkpoints_path) > max_count:\n    checkpoints_path = checkpoints_path[-max_count:]\n\n  average_checkpoints_into_layer(checkpoints_path, model, model_key)\n\n  last_step = get_step_from_checkpoint_prefix(checkpoints_path[-1])\n  checkpoint = tf.train.Checkpoint(**trackables)\n  new_checkpoint_manager = tf.train.CheckpointManager(checkpoint, output_dir, max_to_keep=None)\n  new_checkpoint_manager.save(checkpoint_number=last_step)\n  return output_dir\n\ndef average_checkpoints_into_layer(checkpoints, layer, layer_prefix):\n  """"""Updates the layer weights with their average value in the checkpoints.\n\n  Args:\n    checkpoints: A non empty list of checkpoint paths.\n    layer: A ``tf.keras.layers.Layer`` instance.\n    layer_prefix: The name/scope that prefixes the layer variables names in the\n      checkpoints.\n\n  Raises:\n    ValueError: if :obj:`checkpoints` is empty.\n    ValueError: if :obj:`layer` is not already built.\n\n  See Also:\n    :func:`opennmt.utils.average_checkpoints`\n  """"""\n  if not checkpoints:\n    raise ValueError(""There should be at least one checkpoint"")\n  if not layer.built:\n    raise ValueError(""The layer should be built before calling this function"")\n\n  # Reset the layer variables to 0.\n  for variable in layer.variables:\n    variable.assign(tf.zeros_like(variable))\n\n  # Get a map from variable names in the checkpoint to variables in the layer.\n  _, names_to_variables = misc.get_variables_name_mapping(layer, root_key=layer_prefix)\n\n  num_checkpoints = len(checkpoints)\n  tf.get_logger().info(""Averaging %d checkpoints..."", num_checkpoints)\n  for checkpoint_path in checkpoints:\n    tf.get_logger().info(""Reading checkpoint %s..."", checkpoint_path)\n    reader = tf.train.load_checkpoint(checkpoint_path)\n    for path in reader.get_variable_to_shape_map().keys():\n      if not path.startswith(layer_prefix) or "".OPTIMIZER_SLOT"" in path:\n        continue\n      variable = names_to_variables[path]\n      value = reader.get_tensor(path)\n      variable.assign_add(value / num_checkpoints)\n\n\n_V1_OPTIM_SCOPE = ""optim""\n_V1_SLOTS_MAPPING = {\n    ""Adam"": ""m"",\n    ""Adam_1"": ""v""\n}\n\n\ndef _restore_v1_checkpoint(checkpoint_path, model, optimizer=None):\n  v1_variables = get_checkpoint_variables(checkpoint_path)\n  v1_structure = _variables_to_structure(v1_variables)\n  step = v1_structure[""global_step""]\n  if optimizer is not None:\n    optimizer.iterations.assign(step)\n    if _V1_OPTIM_SCOPE in v1_structure:\n      slots = v1_structure[_V1_OPTIM_SCOPE]\n      del v1_structure[_V1_OPTIM_SCOPE]\n      v1_structure = _merge_optimizer_slots(v1_structure, slots)\n  mapping = model.map_v1_weights(v1_structure)\n  existing_variables = set(variable.ref() for variable in model.variables)\n  mapped_variables = set(variable.ref() for variable, _ in mapping)\n  missing_mapping = existing_variables.difference(mapped_variables)\n  if missing_mapping:\n    raise ValueError(""The following variables were not mapped: %s"" % (\n        "", "".join(var.name for var in missing_mapping)))\n  # Assign each variable and possibly the optimizer slots.\n  for v2_variable, v1_variable in mapping:\n    if isinstance(v1_variable, tuple):\n      v1_variable, v1_slots = v1_variable\n    else:\n      v1_slots = None\n    v2_variable.assign(v1_variable)\n    if v1_slots is not None:\n      for slot_name, value in v1_slots.items():\n        v2_slot = optimizer.get_slot(v2_variable, slot_name)\n        v2_slot.assign(value)\n  return step\n\ndef _variables_to_structure(variables):\n  """"""Represents variables a nested dictionary with scope names as keys.""""""\n  structure = {}\n  for name, value in variables.items():\n    fields = name.split(""/"")\n    cur = structure\n    for i, key in enumerate(fields):\n      if key not in cur:\n        if i + 1 == len(fields):\n          cur[key] = value\n          break\n        cur[key] = {}\n      cur = cur[key]\n  return structure\n\ndef _merge_optimizer_slots(variables, slots):\n  """"""Replaces leaves in the variables structure by tuples of\n  (variable, dict of optimizer slots).\n  """"""\n  if isinstance(variables, dict):\n    merged = {}\n    for key, value in variables.items():\n      if key not in slots:\n        merged[key] = copy.deepcopy(value)\n      else:\n        merged[key] = _merge_optimizer_slots(value, slots[key])\n    return merged\n  else:\n    new_slots = {}\n    for name, value in slots.items():\n      name = _V1_SLOTS_MAPPING.get(name)\n      if name is None:\n        # Just ignore the optimizer slots if their name is not listed.\n        return variables\n      new_slots[name] = value\n    return (variables, new_slots)\n'"
opennmt/utils/compat.py,0,"b'""""""Functions for compatibility with different TensorFlow versions.""""""\n\nimport tensorflow as tf\n\n\ndef tf_supports(symbol):\n  """"""Returns ``True`` if TensorFlow defines :obj:`symbol`.""""""\n  return _string_to_tf_symbol(symbol) is not None\n\ndef tf_any(*symbols):\n  """"""Returns the first supported symbol.""""""\n  for symbol in symbols:\n    module = _string_to_tf_symbol(symbol)\n    if module is not None:\n      return module\n  return None\n\ndef _string_to_tf_symbol(symbol):\n  modules = symbol.split(""."")\n  namespace = tf\n  for module in modules:\n    namespace = getattr(namespace, module, None)\n    if namespace is None:\n      return None\n  return namespace\n'"
opennmt/utils/decoding.py,81,"b'""""""Dynamic decoding utilities.""""""\n\nimport abc\nimport collections\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom opennmt import constants\nfrom opennmt.utils import misc\n\n\nclass Sampler(abc.ABC):\n  """"""Base class for samplers.""""""\n\n  @abc.abstractmethod\n  def __call__(self, scores, num_samples=1):\n    """"""Samples predictions.\n\n    Args:\n      scores: The scores to sample from, a tensor of shape\n        ``[batch_size, vocab_size]``.\n      num_samples: The number of samples per batch to produce.\n\n    Returns:\n      A tuple ``(sample_ids, sample_scores)``.\n    """"""\n    raise NotImplementedError()\n\n  @staticmethod\n  def from_params(params):\n    """"""Constructs a sampler based on user parameters.\n\n    Args:\n      params: A dictionary of user parameters.\n\n    Returns:\n      A :class:`opennmt.utils.Sampler` instance.\n    """"""\n    sampling_topk = params.get(""sampling_topk"", 1)\n    if sampling_topk == 1:\n      return BestSampler()\n    else:\n      return RandomSampler(\n          from_top_k=sampling_topk,\n          temperature=params.get(""sampling_temperature""))\n\nclass RandomSampler(Sampler):\n  """"""Randomly samples from model outputs.""""""\n\n  def __init__(self, from_top_k=None, temperature=None):\n    """"""Initializes the random sampler.\n\n    Args:\n      from_top_k: Sample from the top K predictions instead of the full\n        distribution.\n      temperature: Divide logits by this value. High temperatures generate more\n        random samples.\n    """"""\n    if from_top_k is not None and from_top_k <= 0:\n      from_top_k = None\n    self.from_top_k = from_top_k\n    self.temperature = temperature\n\n  def __call__(self, scores, num_samples=1):\n    if self.from_top_k is None:\n      sample_ids = _sample_from(scores, num_samples, temperature=self.temperature)\n    else:\n      top_scores, top_ids = tf.nn.top_k(scores, k=self.from_top_k)\n      sample_ids = _sample_from(top_scores, num_samples, temperature=self.temperature)\n      sample_ids = _gather_from_word_indices(top_ids, sample_ids)\n    sample_scores = _gather_from_word_indices(scores, sample_ids)\n    return sample_ids, sample_scores\n\nclass BestSampler(Sampler):\n  """"""Sample the best predictions.""""""\n\n  def __call__(self, scores, num_samples=1):\n    sample_scores, sample_ids = tf.nn.top_k(scores, k=num_samples)\n    return sample_ids, sample_scores\n\n\nclass DecodingStrategy(abc.ABC):\n  """"""Base class for decoding strategies.""""""\n\n  @property\n  def num_hypotheses(self):\n    """"""The number of hypotheses returned by this strategy.""""""\n    return 1\n\n  @staticmethod\n  def from_params(params):\n    """"""Constructs a decoding strategy based on user parameters.\n\n    Args:\n      params: A dictionary of user parameters.\n\n    Returns:\n      A :class:`opennmt.utils.DecodingStrategy` instance.\n    """"""\n    beam_size = params.get(""beam_width"", 1)\n    if beam_size > 1:\n      return BeamSearch(\n          beam_size,\n          length_penalty=params.get(""length_penalty"", 0),\n          coverage_penalty=params.get(""coverage_penalty"", 0))\n    else:\n      return GreedySearch()\n\n  @abc.abstractmethod\n  def _initialize(self, batch_size, start_ids, attention_size=None):\n    """"""Initializes the strategy.\n\n    Args:\n      batch_size: The batch size.\n      start_ids: The start decoding ids.\n      attention_size: If known, the size of the attention vectors (i.e. the\n        maximum source length).\n\n    Returns:\n      A tuple containing,\n\n      - The (possibly transformed) start decoding ids.\n      - The tensor of finished flags.\n      - Initial log probabilities per batch.\n      - A sequence of additional tensors used during the decoding.\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def _step(self,\n            step,\n            sampler,\n            log_probs,\n            cum_log_probs,\n            finished,\n            state,\n            extra_vars,\n            attention=None):\n    """"""Updates the strategy state.\n\n    Args:\n      step: The current decoding step.\n      sampler: The sampler that produces predictions.\n      log_probs: The model log probabilities.\n      cum_log_probs: The cumulated log probabilities per batch.\n      finished: The current finished flags.\n      state: The decoder state.\n      extra_vars: Additional tensors from this decoding strategy.\n      attention: The attention vector for the current step.\n\n    Returns:\n      A tuple containing,\n\n      - The predicted word ids.\n      - The new cumulated log probabilities.\n      - The updated finished flags.\n      - The update decoder state.\n      - Additional tensors from this decoding strategy.\n    """"""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def _finalize(self, outputs, end_id, extra_vars, attention=None):\n    """"""Finalize the predictions.\n\n    Args:\n      outputs: The array of sampled ids.\n      end_id: The end token id.\n      extra_vars: Additional tensors from this decoding strategy.\n      attention: The array of attention outputs.\n\n    Returns:\n      A tuple containing,\n\n      - The final predictions as a tensor of shape [B, H, T].\n      - The final attention history of shape [B, H, T, S].\n      - The final sequence lengths of shape [B, H].\n    """"""\n    raise NotImplementedError()\n\n\nclass GreedySearch(DecodingStrategy):\n  """"""A basic greedy search strategy.""""""\n\n  def _initialize(self, batch_size, start_ids, attention_size=None):\n    finished = tf.zeros([batch_size], dtype=tf.bool)\n    initial_log_probs = tf.zeros([batch_size], dtype=tf.float32)\n    return start_ids, finished, initial_log_probs, []\n\n  def _step(self,\n            step,\n            sampler,\n            log_probs,\n            cum_log_probs,\n            finished,\n            state,\n            extra_vars,\n            attention=None):\n    sample_ids, sample_log_probs = sampler(log_probs)\n    sample_ids = tf.reshape(sample_ids, [-1])\n    sample_log_probs = tf.reshape(sample_log_probs, [-1])\n    cum_log_probs += sample_log_probs\n    return sample_ids, cum_log_probs, finished, state, extra_vars\n\n  def _finalize(self, outputs, end_id, extra_vars, attention=None):\n    ids = tf.transpose(outputs.stack())\n    ids = tf.expand_dims(ids, 1)\n    lengths = _lengths_from_ids(ids, end_id)\n    if attention is not None:\n      attention = tf.transpose(attention.stack(), perm=[1, 0, 2])\n      attention = tf.expand_dims(attention, 1)\n    return ids, attention, lengths\n\n\nclass BeamSearch(DecodingStrategy):\n  """"""A beam search strategy.""""""\n\n  def __init__(self, beam_size, length_penalty=0, coverage_penalty=0):\n    """"""Initializes the decoding strategy.\n\n    Args:\n      beam_size: The number of paths to consider per batch.\n      length_penalty: Length penalty, see https://arxiv.org/abs/1609.08144.\n      coverage_penalty: Coverage penalty, see https://arxiv.org/abs/1609.08144.\n    """"""\n    self.beam_size = beam_size\n    self.length_penalty = length_penalty\n    self.coverage_penalty = coverage_penalty\n    self._state_reorder_flags = None\n\n  @property\n  def num_hypotheses(self):\n    return self.beam_size\n\n  def _set_state_reorder_flags(self, state_reorder_flags):\n    """"""Sets state reorder flags, a structure matching the decoder state that\n    indicates which tensor should be reorded during beam search.\n    """"""\n    self._state_reorder_flags = state_reorder_flags\n\n  def _initialize(self, batch_size, start_ids, attention_size=None):\n    start_ids = tfa.seq2seq.tile_batch(start_ids, self.beam_size)\n    finished = tf.zeros([batch_size * self.beam_size], dtype=tf.bool)\n    # Give all probability to first beam for the first iteration.\n    initial_log_probs = tf.tile([0.] + [-float(""inf"")] * (self.beam_size - 1), [batch_size])\n    parent_ids = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n    sequence_lengths = tf.zeros([batch_size * self.beam_size], dtype=tf.int32)\n    extra_vars = [parent_ids, sequence_lengths]\n    if self.coverage_penalty != 0:\n      if attention_size is None:\n        raise ValueError(""The attention size should be known to support coverage penalty"")\n      accumulated_attention = tf.zeros([batch_size * self.beam_size, attention_size])\n      extra_vars.append(accumulated_attention)\n    return start_ids, finished, initial_log_probs, tuple(extra_vars)\n\n  def _get_scores(self, log_probs, sequence_lengths, finished, accumulated_attention=None):\n    scores = log_probs\n    if self.length_penalty != 0:\n      expand_sequence_lengths = tf.expand_dims(sequence_lengths, 1)\n      scores /= tf.pow(((5. + tf.cast(expand_sequence_lengths + 1, scores.dtype)) / 6.),\n                       self.length_penalty)\n    if self.coverage_penalty != 0:\n      # Mask out of range steps with ones (log(1) == 0).\n      accumulated_attention = tf.where(\n          tf.equal(accumulated_attention, 0.0),\n          x=tf.ones_like(accumulated_attention),\n          y=accumulated_attention)\n      coverage_penalty = tf.reduce_sum(\n          tf.math.log(tf.minimum(accumulated_attention, 1.0)), 1)\n      # Apply coverage penalty to finished predictions.\n      coverage_penalty *= tf.cast(finished, coverage_penalty.dtype)\n      scores += self.coverage_penalty * tf.expand_dims(coverage_penalty, 1)\n    return scores\n\n  def _step(self,\n            step,\n            sampler,\n            log_probs,\n            cum_log_probs,\n            finished,\n            state,\n            extra_vars,\n            attention=None):\n    parent_ids = extra_vars[0]\n    sequence_lengths = extra_vars[1]\n\n    if self.coverage_penalty != 0:\n      if attention is None:\n        raise ValueError(""Coverage penalty is enabled but the model did not ""\n                         ""return an attention vector"")\n      not_finished = tf.math.logical_not(finished)\n      attention *= tf.expand_dims(tf.cast(not_finished, attention.dtype), 1)\n      accumulated_attention = extra_vars[2] + attention\n    else:\n      accumulated_attention = None\n\n    # Compute scores from log probabilities.\n    vocab_size = log_probs.shape[-1]\n    total_probs = log_probs + tf.expand_dims(cum_log_probs, 1)  # Add current beam probability.\n    scores = self._get_scores(\n        total_probs,\n        sequence_lengths,\n        finished,\n        accumulated_attention=accumulated_attention)\n    scores = tf.reshape(scores, [-1, self.beam_size * vocab_size])\n    total_probs = tf.reshape(total_probs, [-1, self.beam_size * vocab_size])\n\n    # Sample predictions.\n    sample_ids, sample_scores = sampler(scores, num_samples=self.beam_size)\n    cum_log_probs = tf.reshape(_gather_from_word_indices(total_probs, sample_ids), [-1])\n    sample_ids = tf.reshape(sample_ids, [-1])\n    sample_scores = tf.reshape(sample_scores, [-1])\n\n    # Resolve beam origin and word ids.\n    word_ids = sample_ids % vocab_size\n    beam_ids = sample_ids // vocab_size\n    beam_indices = (\n        (tf.range(tf.shape(word_ids)[0]) // self.beam_size)\n        * self.beam_size + beam_ids)\n\n    # Update sequence_length of unfinished sequence.\n    sequence_lengths = tf.where(finished, x=sequence_lengths, y=sequence_lengths + 1)\n\n    # Update state and flags.\n    finished = tf.gather(finished, beam_indices)\n    sequence_lengths = tf.gather(sequence_lengths, beam_indices)\n    parent_ids = parent_ids.write(step, beam_ids)\n    extra_vars = [parent_ids, sequence_lengths]\n    if accumulated_attention is not None:\n      accumulated_attention = tf.gather(accumulated_attention, beam_indices)\n      extra_vars.append(accumulated_attention)\n    state = _reorder_state(state, beam_indices, reorder_flags=self._state_reorder_flags)\n    return word_ids, cum_log_probs, finished, state, tuple(extra_vars)\n\n  def _finalize(self, outputs, end_id, extra_vars, attention=None):\n    parent_ids = extra_vars[0]\n    sequence_lengths = extra_vars[1]\n    maximum_lengths = tf.reduce_max(tf.reshape(sequence_lengths, [-1, self.beam_size]), axis=-1)\n    max_time = outputs.size()\n    array_shape = [max_time, -1, self.beam_size]\n    step_ids = tf.reshape(outputs.stack(), array_shape)\n    parent_ids = tf.reshape(parent_ids.stack(), array_shape)\n    ids = tfa.seq2seq.gather_tree(step_ids, parent_ids, maximum_lengths, end_id)\n    ids = tf.transpose(ids, perm=[1, 2, 0])\n    lengths = _lengths_from_ids(ids, end_id)\n    if attention is not None:\n      attention = tfa.seq2seq.gather_tree_from_array(\n          attention.stack(), parent_ids, lengths)\n      attention = tf.transpose(attention, perm=[1, 0, 2])\n      attention = tf.reshape(\n          attention, [tf.shape(ids)[0], self.beam_size, max_time, -1])\n    return ids, attention, lengths\n\n\nclass DecodingResult(\n    collections.namedtuple(""DecodingResult"",\n                           (""ids"", ""lengths"", ""log_probs"", ""attention"", ""state""))):\n  """"""Final decoding result.\n\n  Args:\n    ids: The predicted ids of shape :math:`[B, H, T]`.\n    lengths: The produced sequences length of shape :math:`[B, H]`.\n    log_probs: The cumulated log probabilities of shape :math:`[B, H]`.\n    attention: The attention history of shape :math:`[B, H, T_t, T_s]`.\n    state: The final decoding state.\n  """"""\n\n\ndef dynamic_decode(symbols_to_logits_fn,\n                   start_ids,\n                   end_id=constants.END_OF_SENTENCE_ID,\n                   initial_state=None,\n                   decoding_strategy=None,\n                   sampler=None,\n                   maximum_iterations=None,\n                   minimum_iterations=0,\n                   attention_history=False,\n                   attention_size=None):\n  """"""Dynamic decoding.\n\n  Args:\n    symbols_to_logits_fn: A callable taking ``(symbols, step, state)`` and\n      returning ``(logits, state, attention)`` (``attention`` is optional).\n    start_ids: Initial input IDs of shape :math:`[B]`.\n    end_id: ID of the end of sequence token.\n    initial_state: Initial decoder state.\n    decoding_strategy: A :class:`opennmt.utils.DecodingStrategy`\n      instance that defines the decoding logic. Defaults to a greedy search.\n    sampler: A :class:`opennmt.utils.Sampler` instance that samples\n      predictions from the model output. Defaults to an argmax sampling.\n    maximum_iterations: The maximum number of iterations to decode for.\n    minimum_iterations: The minimum number of iterations to decode for.\n    attention_history: Gather attention history during the decoding.\n    attention_size: If known, the size of the attention vectors (i.e. the\n      maximum source length).\n\n  Returns:\n    A :class:`opennmt.utils.DecodingResult` instance.\n  """"""\n  if initial_state is None:\n    initial_state = {}\n  if decoding_strategy is None:\n    decoding_strategy = GreedySearch()\n  if sampler is None:\n    sampler = BestSampler()\n\n  def _cond(step, finished, state, inputs, outputs, attention, cum_log_probs, extra_vars):  # pylint: disable=unused-argument\n    return tf.reduce_any(tf.logical_not(finished))\n\n  def _body(step, finished, state, inputs, outputs, attention, cum_log_probs, extra_vars):\n    # Get log probs from the model.\n    result = symbols_to_logits_fn(inputs, step, state)\n    logits, state = result[0], result[1]\n    attn = result[2] if len(result) > 2 else None\n    logits = tf.cast(logits, tf.float32)\n\n    # Penalize or force EOS.\n    batch_size, vocab_size = misc.shape_list(logits)\n    eos_max_prob = tf.one_hot(\n        tf.fill([batch_size], end_id),\n        vocab_size,\n        on_value=logits.dtype.max,\n        off_value=logits.dtype.min)\n    logits = tf.cond(\n        step < minimum_iterations,\n        true_fn=lambda: _penalize_token(logits, end_id),\n        false_fn=lambda: tf.where(\n            tf.broadcast_to(tf.expand_dims(finished, -1), tf.shape(logits)),\n            x=eos_max_prob,\n            y=logits))\n    log_probs = tf.nn.log_softmax(logits)\n\n    # Run one decoding strategy step.\n    output, next_cum_log_probs, finished, state, extra_vars = (\n        decoding_strategy._step(  # pylint: disable=protected-access\n            step,\n            sampler,\n            log_probs,\n            cum_log_probs,\n            finished,\n            state,\n            extra_vars,\n            attention=attn))\n\n    # Update loop vars.\n    if attention_history:\n      if attn is None:\n        raise ValueError(""attention_history is set but the model did not return attention"")\n      attention = attention.write(step, tf.cast(attn, tf.float32))\n    outputs = outputs.write(step, output)\n    cum_log_probs = tf.where(finished, x=cum_log_probs, y=next_cum_log_probs)\n    finished = tf.logical_or(finished, tf.equal(output, end_id))\n    return step + 1, finished, state, output, outputs, attention, cum_log_probs, extra_vars\n\n  start_ids = tf.convert_to_tensor(start_ids)\n  batch_size = tf.shape(start_ids)[0]\n  ids_dtype = start_ids.dtype\n  start_ids = tf.cast(start_ids, tf.int32)\n  start_ids, finished, initial_log_probs, extra_vars = (\n      decoding_strategy._initialize(  # pylint: disable=protected-access\n          batch_size, start_ids, attention_size=attention_size))\n  step = tf.constant(0, dtype=tf.int32)\n  outputs = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n  attention = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n\n  _, _, state, _, outputs, attention, log_probs, extra_vars = tf.while_loop(\n      _cond,\n      _body,\n      loop_vars=(\n          step,\n          finished,\n          initial_state,\n          start_ids,\n          outputs,\n          attention,\n          initial_log_probs,\n          extra_vars),\n      shape_invariants=(\n          step.shape,\n          finished.shape,\n          tf.nest.map_structure(_get_shape_invariants, initial_state),\n          start_ids.shape,\n          tf.TensorShape(None),\n          tf.TensorShape(None),\n          initial_log_probs.shape,\n          tf.nest.map_structure(_get_shape_invariants, extra_vars)),\n      parallel_iterations=1,\n      maximum_iterations=maximum_iterations)\n\n  ids, attention, lengths = decoding_strategy._finalize(  # pylint: disable=protected-access\n      outputs,\n      end_id,\n      extra_vars,\n      attention=attention if attention_history else None)\n  if attention is not None:\n    attention = attention[:, :, :-1]  # Ignore attention for </s>.\n  log_probs = tf.reshape(log_probs, [batch_size, decoding_strategy.num_hypotheses])\n  ids = tf.cast(ids, ids_dtype)\n  return DecodingResult(\n      ids=ids,\n      lengths=lengths,\n      log_probs=log_probs,\n      attention=attention,\n      state=state)\n\n\ndef _reorder_state(state, indices, reorder_flags=None):\n  """"""Gather batch indices from the state tensors.""""""\n\n  def _reorder_one(tensor, reorder=True):\n    if not reorder or isinstance(tensor, tf.TensorArray) or tensor.shape.ndims == 0:\n      return tensor\n    return tf.gather(tensor, indices)\n\n  args = [state]\n  if reorder_flags is not None:\n    tf.nest.assert_same_structure(state, reorder_flags)\n    args.append(reorder_flags)\n  return tf.nest.map_structure(_reorder_one, *args)\n\ndef _get_shape_invariants(tensor):\n  """"""Returns the shape of the tensor but sets middle dims to None.""""""\n  if isinstance(tensor, tf.TensorArray):\n    shape = None\n  else:\n    shape = tensor.shape.as_list()\n    for i in range(1, len(shape) - 1):\n      shape[i] = None\n  return tf.TensorShape(shape)\n\ndef _penalize_token(log_probs, token_id, penalty=-1e7):\n  """"""Penalize token probabilities.""""""\n  depth = log_probs.shape[-1]\n  penalty = tf.one_hot([token_id], depth, on_value=tf.cast(penalty, log_probs.dtype))\n  return log_probs + penalty\n\ndef _sample_from(logits, num_samples, temperature=None):\n  """"""Sample N values from the unscaled probability distribution.""""""\n  if temperature is not None:\n    logits /= tf.cast(temperature, logits.dtype)\n  return tf.random.categorical(logits, num_samples, dtype=tf.int32)\n\ndef _gather_from_word_indices(tensor, indices):\n  """"""Index the depth dim of a 2D tensor.""""""\n  return tf.gather(tensor, indices, axis=-1, batch_dims=1)\n\ndef _lengths_from_ids(ids, end_id):\n  """"""Compute sequence lengths from word ids.""""""\n  lengths = tf.not_equal(ids, end_id)\n  lengths = tf.cast(lengths, tf.int32)\n  lengths = tf.reduce_sum(lengths, axis=-1)\n  return lengths\n'"
opennmt/utils/exporters.py,5,"b'""""""Define model exporters.""""""\n\nimport abc\nimport os\nimport tempfile\n\nimport tensorflow as tf\n\nfrom opennmt.utils import misc\n\n\nclass Exporter(abc.ABC):\n  """"""Base class for model exporters.""""""\n\n  def export(self, model, export_dir):\n    """"""Exports :obj:`model` to :obj:`export_dir`.\n\n    Raises:\n      ValueError: if :obj:`model` is not supported by this exporter.\n    """"""\n    self._export_model(model, export_dir)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n      extra_assets = model.export_assets(tmp_dir)\n      if extra_assets:\n        assets_extra = os.path.join(export_dir, ""assets.extra"")\n        tf.io.gfile.makedirs(assets_extra)\n        for filename, path in extra_assets.items():\n          tf.io.gfile.copy(path, os.path.join(assets_extra, filename), overwrite=True)\n        tf.get_logger().info(""Extra assets written to: %s"", assets_extra)\n\n  @abc.abstractmethod\n  def _export_model(self, model, export_dir):\n    raise NotImplementedError()\n\n\n_EXPORTERS_REGISTRY = misc.ClassRegistry(base_class=Exporter)\nregister_exporter = _EXPORTERS_REGISTRY.register  # pylint: disable=invalid-name\n\ndef make_exporter(name, **kwargs):\n  """"""Creates a new exporter.\n\n  Args:\n    name: The exporter name.\n    **kwargs: Additional arguments to pass to the exporter constructor.\n\n  Returns:\n    A :class:`opennmt.utils.Exporter` instance.\n\n  Raises:\n    ValueError: if :obj:`name` is invalid.\n  """"""\n  exporter_class = _EXPORTERS_REGISTRY.get(name)\n  if exporter_class is None:\n    raise ValueError(""Invalid exporter name: %s"" % name)\n  return exporter_class(**kwargs)\n\ndef list_exporters():\n  """"""Lists the name of registered exporters.""""""\n  return _EXPORTERS_REGISTRY.class_names\n\n\n@register_exporter(name=""saved_model"")\nclass SavedModelExporter(Exporter):\n  """"""SavedModel exporter.""""""\n\n  def _export_model(self, model, export_dir):\n    tf.saved_model.save(model, export_dir, signatures=model.serve_function())\n\n\n@register_exporter(name=""checkpoint"")\nclass CheckpointExporter(Exporter):\n  """"""Checkpoint exporter.""""""\n\n  def _export_model(self, model, export_dir):\n    checkpoint = tf.train.Checkpoint(model=model)\n    checkpoint.write(os.path.join(export_dir, ""ckpt""))\n\n\n@register_exporter(name=""ctranslate2"")\nclass CTranslate2Exporter(Exporter):\n  """"""CTranslate2 exporter.""""""\n\n  def __init__(self, quantization=None):\n    """"""Initializes the exporter.\n\n    Args:\n      quantization: Quantize model weights to this type when exporting the model.\n        Can be ""int16"" or ""int8"". Default is no quantization.\n    """"""\n    # Fail now if ctranslate2 package is missing.\n    import ctranslate2  # pylint: disable=import-outside-toplevel,unused-import\n    self._quantization = quantization\n\n  def _export_model(self, model, export_dir):\n    model_spec = model.ctranslate2_spec\n    if model_spec is None:\n      raise ValueError(""The model does not define an equivalent CTranslate2 model specification"")\n    if not model.built:\n      model.create_variables()\n    _, variables = misc.get_variables_name_mapping(model, root_key=""model"")\n    variables = {\n        name.replace(""/.ATTRIBUTES/VARIABLE_VALUE"", """"):value.numpy()\n        for name, value in variables.items()}\n    import ctranslate2  # pylint: disable=import-outside-toplevel\n    converter = ctranslate2.converters.OpenNMTTFConverter(\n        src_vocab=model.features_inputter.vocabulary_file,\n        tgt_vocab=model.labels_inputter.vocabulary_file,\n        variables=variables)\n    converter.convert(export_dir, model_spec, quantization=self._quantization, force=True)\n'"
opennmt/utils/fmeasure.py,0,"b'""""""Hypotheses file scoring for Precision Recall and F-Measure.""""""\n\ndef fmeasure(ref_path,\n             hyp_path,\n             return_precision_only=False,\n             return_recall_only=False,\n             return_fmeasure_only=False):\n  """"""Compute Precision Recall and F-Measure between two files""""""\n  with open(ref_path) as ref_fp, open(hyp_path) as hyp_fp:\n    list_null_tags = [""X"", ""null"", ""NULL"", ""Null"", ""O""]\n    listtags = []\n    classref = []\n    classrandom = []\n    classhyp = []\n    nbrtagref = {}\n    nbrtaghyp = {}\n    nbrtagok = {}\n    for line in ref_fp:\n      line = line.strip()\n      tabline = line.split(\' \')\n      lineref = []\n      for tag in tabline:\n        lineref.append(tag)\n        if tag in nbrtagref.keys() and tag not in list_null_tags:\n          nbrtagref[tag] = nbrtagref[tag]+1\n        else:\n          nbrtagref[tag] = 1\n      classref.append(lineref)\n    for line, lineref in zip(hyp_fp, classref):\n      line = line.strip()\n      tabline = line.split(\' \')\n      linehyp = []\n      linerandom = []\n      for tagcpt, tag in enumerate(tabline):\n        linehyp.append(tag)\n        if tag not in listtags:\n          listtags.append(tag)\n        linerandom.append(tag)\n        if tagcpt < len(lineref) and tag == lineref[tagcpt]:\n          if tag in nbrtagok.keys():\n            nbrtagok[tag] = nbrtagok[tag]+1\n          else:\n            nbrtagok[tag] = 1\n        if tag in nbrtaghyp.keys():\n          nbrtaghyp[tag] = nbrtaghyp[tag]+1\n        else:\n          nbrtaghyp[tag] = 1\n      classhyp.append(linehyp)\n      classrandom.append(linerandom)\n\n  fullprecision = 0\n  fullrecall = 0\n  precision = {}\n  recall = {}\n  fulltagok = 0.00\n  fulltaghyp = 0.00\n  fulltagref = 0.00\n  for tag in listtags:\n    if tag not in nbrtagok:\n      nbrtagok[tag] = 0\n    if tag not in nbrtaghyp:\n      nbrtaghyp[tag] = 0\n    if tag not in nbrtagref:\n      nbrtagref[tag] = 0\n    if nbrtaghyp[tag] != 0:\n      precision[tag] = nbrtagok[tag]/nbrtaghyp[tag]\n    else:\n      precision[tag] = 0\n    if nbrtagref[tag] != 0:\n      recall[tag] = nbrtagok[tag]/nbrtagref[tag]\n    else:\n      recall[tag] = 0\n    if tag not in list_null_tags:\n      fulltagok = fulltagok+nbrtagok[tag]\n      fulltaghyp = fulltaghyp+nbrtaghyp[tag]\n      fulltagref = fulltagref+nbrtagref[tag]\n  fullprecision = fulltagok / fulltaghyp if fulltaghyp != 0 else 0\n  fullrecall = fulltagok / fulltagref if fulltagref != 0 else 0\n  fullfmeasure = (\n      (2 * fullprecision * fullrecall) / (fullprecision + fullrecall)\n      if (fullprecision + fullrecall) != 0 else 0)\n  if return_precision_only:\n    return fullprecision\n  if return_recall_only:\n    return fullrecall\n  if return_fmeasure_only:\n    return fullfmeasure\n  return fullprecision, fullrecall, fullfmeasure\n'"
opennmt/utils/losses.py,32,"b'""""""Define losses.""""""\n\nimport tensorflow as tf\n\n\ndef _smooth_one_hot_labels(logits, labels, label_smoothing):\n  label_smoothing = tf.constant(label_smoothing, dtype=logits.dtype)\n  num_classes = tf.shape(logits)[-1]\n  return tf.one_hot(\n      tf.cast(labels, tf.int32),\n      num_classes,\n      on_value=1.0 - label_smoothing,\n      off_value=label_smoothing / tf.cast(num_classes - 1, label_smoothing.dtype),\n      dtype=logits.dtype)\n\ndef _softmax_cross_entropy(logits, labels, label_smoothing, training):\n  # Computes the softmax in full precision.\n  if logits.dtype.base_dtype != tf.float32:\n    logits = tf.cast(logits, tf.float32)\n  if training and label_smoothing > 0.0:\n    smoothed_labels = _smooth_one_hot_labels(logits, labels, label_smoothing)\n    return tf.nn.softmax_cross_entropy_with_logits(smoothed_labels, logits)\n  else:\n    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n\ndef cross_entropy_sequence_loss(logits,\n                                labels,\n                                sequence_length,\n                                label_smoothing=0.0,\n                                average_in_time=False,\n                                training=None):\n  """"""Computes the cross entropy loss of sequences.\n\n  Args:\n    logits: The unscaled probabilities.\n    labels: The true labels.\n    sequence_length: The length of each sequence.\n    label_smoothing: The label smoothing value.\n    average_in_time: If ``True``, also average the loss in the time dimension.\n    training: Compute training loss.\n\n  Returns:\n    A tuple (cumulated loss, loss normalizer, token-level normalizer).\n  """"""\n  batch_size = tf.shape(logits)[0]\n  max_time = tf.shape(logits)[1]\n\n  cross_entropy = _softmax_cross_entropy(logits, labels, label_smoothing, training)\n  weights = tf.sequence_mask(\n      sequence_length, maxlen=max_time, dtype=cross_entropy.dtype)\n  loss = tf.reduce_sum(cross_entropy * weights)\n  loss_token_normalizer = tf.reduce_sum(weights)\n\n  if average_in_time or not training:\n    loss_normalizer = loss_token_normalizer\n  else:\n    loss_normalizer = tf.cast(batch_size, loss.dtype)\n\n  return loss, loss_normalizer, loss_token_normalizer\n\ndef cross_entropy_loss(logits,\n                       labels,\n                       label_smoothing=0.0,\n                       training=None):\n  """"""Computes the cross entropy loss.\n\n  Args:\n    logits: The unscaled probabilities.\n    labels: The true labels.\n    label_smoothing: The label smoothing value.\n    training: Compute training loss.\n\n  Returns:\n    The cumulated loss and the loss normalizer.\n  """"""\n  cross_entropy = _softmax_cross_entropy(logits, labels, label_smoothing, training)\n  loss = tf.reduce_sum(cross_entropy)\n  loss_normalizer = tf.cast(tf.shape(cross_entropy)[0], loss.dtype)\n  return loss, loss_normalizer\n\ndef guided_alignment_cost(attention_probs,\n                          gold_alignment,\n                          sequence_length=None,\n                          cost_type=""ce"",\n                          weight=1):\n  """"""Computes the guided alignment cost.\n\n  Args:\n    attention_probs: The attention probabilities, a float ``tf.Tensor`` of shape\n      :math:`[B, T_t, T_s]`.\n    gold_alignment: The true alignment matrix, a float ``tf.Tensor`` of shape\n      :math:`[B, T_t, T_s]`.\n    sequence_length: The length of each sequence.\n    cost_type: The type of the cost function to compute (can be: ce, mse).\n    weight: The weight applied to the cost.\n\n  Returns:\n    The guided alignment cost.\n\n  Raises:\n    ValueError: if :obj:`cost_type` is invalid.\n  """"""\n  if cost_type == ""ce"":\n    loss = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n  elif cost_type == ""mse"":\n    loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n  else:\n    raise ValueError(""invalid guided alignment cost: %s"" % cost_type)\n\n  if sequence_length is not None:\n    sample_weight = tf.sequence_mask(\n        sequence_length,\n        maxlen=tf.shape(attention_probs)[1],\n        dtype=attention_probs.dtype)\n    sample_weight = tf.expand_dims(sample_weight, -1)\n    normalizer = tf.reduce_sum(sequence_length)\n  else:\n    sample_weight = None\n    normalizer = tf.size(attention_probs)\n\n  cost = loss(\n      gold_alignment,\n      attention_probs,\n      sample_weight=sample_weight)\n  cost /= tf.cast(normalizer, cost.dtype)\n  return weight * cost\n\ndef regularization_penalty(regularization_type, scale, weights):\n  """"""Computes the weights regularization penalty.\n\n  Args:\n    regularization_type: The regularization type: ``l1``, ``l2``, or ``l1_l2``.\n    scale: The regularization multiplier. If :obj:`regularization_type` is\n      ``l1_l2``, this should be a list or tuple containing the L1 regularization\n      scale and the L2 regularization scale.\n    weights: The list of weights.\n\n  Returns:\n    The regularization penalty.\n\n  Raises:\n    ValueError: if :obj:`regularization_type` is invalid or is ``l1_l2`` but\n      :obj:`scale` is not a sequence.\n  """"""\n  regularization_type = regularization_type.lower()\n  if regularization_type == ""l1"":\n    regularizer = tf.keras.regularizers.l1(l=float(scale))\n  elif regularization_type == ""l2"":\n    regularizer = tf.keras.regularizers.l2(l=float(scale))\n  elif regularization_type == ""l1_l2"":\n    if not isinstance(scale, (list, tuple)) or len(scale) != 2:\n      raise ValueError(""l1_l2 regularization requires 2 scale values"")\n    regularizer = tf.keras.regularizers.l1_l2(\n        l1=float(scale[0]), l2=float(scale[1]))\n  else:\n    raise ValueError(""invalid regularization type %s"" % regularization_type)\n\n  weights = list(filter(lambda v: not _is_bias(v), weights))\n  penalty = tf.add_n([regularizer(w) for w in weights])\n  return penalty\n\ndef _is_bias(variable):\n  return len(variable.shape) == 1 and variable.name.endswith(""bias:0"")\n\ndef _negative_log_likelihood(logits, labels, sequence_length):\n  nll_num, nll_den, _ = cross_entropy_sequence_loss(\n      logits, labels, sequence_length, average_in_time=True)\n  return nll_num / nll_den\n\ndef max_margin_loss(true_logits,\n                    true_labels,\n                    true_sequence_length,\n                    negative_logits,\n                    negative_labels,\n                    negative_sequence_length,\n                    eta=0.1):\n  """"""Computes the max-margin loss described in\n  https://www.aclweb.org/anthology/P19-1623.\n\n  Args:\n    true_logits: The unscaled probabilities from the true example.\n    negative_logits: The unscaled probabilities from the negative example.\n    true_labels: The true labels.\n    true_sequence_length: The length of each true sequence.\n    negative_labels: The negative labels.\n    negative_sequence_length: The length of each negative sequence.\n    eta: Ensure that the margin is higher than this value.\n\n  Returns:\n    The max-margin loss.\n  """"""\n  true_nll = _negative_log_likelihood(\n      true_logits, true_labels, true_sequence_length)\n  negative_nll = _negative_log_likelihood(\n      negative_logits, negative_labels, negative_sequence_length)\n  margin = true_nll - negative_nll + eta\n  return tf.maximum(margin, 0)\n'"
opennmt/utils/misc.py,17,"b'""""""Various utility functions to use throughout the project.""""""\n\nimport collections\nimport copy\nimport sys\nimport functools\nimport heapq\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.training.tracking import graph_view\n\n\ndef get_devices(count=1, fallback_to_cpu=True):\n  """"""Gets devices.\n\n  Args:\n    count: The number of devices to get.\n    fallback_to_cpu: If ``True``, return CPU devices if no GPU is available.\n\n  Returns:\n    A list of device names.\n\n  Raises:\n    ValueError: if :obj:`count` is greater than the number of visible devices.\n  """"""\n  devices = tf.config.list_logical_devices(device_type=""GPU"")\n  if not devices and fallback_to_cpu:\n    devices = tf.config.list_logical_devices(device_type=""CPU"")\n  if len(devices) < count:\n    raise ValueError(""Requested %d devices but only %d %s visible"" % (\n        count, len(devices), ""is"" if len(devices) == 1 else ""are""))\n  return devices[0:count]\n\ndef get_variables_name_mapping(root, root_key=None):\n  """"""Returns mapping between variables and their name in the object-based\n  representation.\n\n  Args:\n    root: The root layer.\n    root_key: Key that was used to save :obj:`root`, if any.\n\n  Returns:\n    A dict mapping variables ref to names and a dict mapping variables name to\n    variables.\n  """"""\n  # TODO: find a way to implement this function using public APIs.\n  named_variables, _, _ = graph_view.ObjectGraphView(root).serialize_object_graph()\n  variables_to_names = {}\n  names_to_variables = {}\n  for saveable_object in named_variables:\n    variable = saveable_object.op\n    if not hasattr(variable, ""ref""):  # Ignore non Tensor-like objects.\n      continue\n    name = saveable_object.name\n    if root_key is not None:\n      name = ""%s/%s"" % (root_key, name)\n    variables_to_names[variable.ref()] = name\n    names_to_variables[name] = variable\n  return variables_to_names, names_to_variables\n\ndef get_variable_name(variable, root, model_key=""model""):\n  """"""Gets the variable name in the object-based representation.""""""\n  variables_to_names, _ = get_variables_name_mapping(root, root_key=model_key)\n  # In case of a MirroredVariable, look up the primary variable\n  variable = get_primary_variable(variable)\n  return variables_to_names.get(variable.ref())\n\ndef get_primary_variable(variable):\n  """"""If :obj:`variable` is distributed, returns the primary component.""""""\n  # TODO: use a public API to get the primary variable.\n  for attribute_name in (""primary"", ""_primary""):\n    attribute = getattr(variable, attribute_name, None)\n    if attribute is not None:\n      return attribute\n  return variable\n\ndef print_as_bytes(text, stream=None):\n  """"""Prints a string as bytes to non rely on :obj:`stream` default encoding.\n\n  Args:\n    text: The text to print.\n    stream: The stream to print to (``sys.stdout`` if not set).\n  """"""\n  if stream is None:\n    stream = sys.stdout\n  write_buffer = stream.buffer if hasattr(stream, ""buffer"") else stream\n  write_buffer.write(tf.compat.as_bytes(text))\n  write_buffer.write(b""\\n"")\n  stream.flush()\n\ndef format_translation_output(sentence,\n                              score=None,\n                              token_level_scores=None,\n                              attention=None,\n                              alignment_type=None):\n  """"""Formats a translation output with possibly scores, alignments, etc., e.g:\n\n  1.123214 ||| Hello world ||| 0.30907777 0.030488174 ||| 0-0 1-1\n\n  Args:\n    sentence: The translation to output.\n    score: If set, attach the score.\n    token_level_scores: If set, attach the token level scores.\n    attention: The attention vector.\n    alignment_type: The type of alignments to format (can be: ""hard"", ""soft"").\n  """"""\n  if score is not None:\n    sentence = ""%f ||| %s"" % (score, sentence)\n  if token_level_scores is not None:\n    scores_str = "" "".join(""%f"" % s for s in token_level_scores)\n    sentence = ""%s ||| %s"" % (sentence, scores_str)\n  if attention is not None and alignment_type is not None:\n    if alignment_type == ""hard"":\n      source_indices = np.argmax(attention, axis=-1)\n      target_indices = range(attention.shape[0])\n      pairs = (""%d-%d"" % (src, tgt) for src, tgt in zip(source_indices, target_indices))\n      sentence = ""%s ||| %s"" % (sentence, "" "".join(pairs))\n    elif alignment_type == ""soft"":\n      vectors = []\n      for vector in attention:\n        vectors.append("" "".join(""%.6f"" % value for value in vector))\n      sentence = ""%s ||| %s"" % (sentence, "" ; "".join(vectors))\n    else:\n      raise ValueError(""Invalid alignment type %s"" % alignment_type)\n  return sentence\n\ndef item_or_tuple(x):\n  """"""Returns :obj:`x` as a tuple or its single element.""""""\n  x = tuple(x)\n  if len(x) == 1:\n    return x[0]\n  else:\n    return x\n\ndef count_lines(filename, buffer_size=65536):\n  """"""Returns the number of lines of the file :obj:`filename`.""""""\n  with tf.io.gfile.GFile(filename, mode=""rb"") as f:\n    num_lines = 0\n    while True:\n      data = f.read(buffer_size)\n      if not data:\n        return num_lines\n      num_lines += data.count(b""\\n"")\n\ndef is_gzip_file(filename):\n  """"""Returns ``True`` if :obj:`filename` is a GZIP file.""""""\n  return filename.endswith("".gz"")\n\ndef shape_list(x):\n  """"""Return list of dims, statically where possible.""""""\n  x = tf.convert_to_tensor(x)\n\n  # If unknown rank, return dynamic shape\n  if x.shape.dims is None:\n    return tf.shape(x)\n\n  static = x.shape.as_list()\n  shape = tf.shape(x)\n\n  ret = []\n  for i, _ in enumerate(static):\n    dim = static[i]\n    if dim is None:\n      dim = shape[i]\n    ret.append(dim)\n  return ret\n\ndef index_structure(structure, path, path_separator=""/""):\n  """"""Follows :obj:`path` in a nested structure of objects, lists, and dicts.""""""\n  keys = path.split(path_separator)\n  for i, key in enumerate(keys):\n    current_path = ""%s%s"" % (path_separator, path_separator.join(keys[:i]))\n    if isinstance(structure, list):\n      try:\n        index = int(key)\n      except ValueError:\n        raise ValueError(""Object referenced by path \'%s\' is a list, but got non ""\n                         ""integer index \'%s\'"" % (current_path, key))\n      if index < 0 or index >= len(structure):\n        raise ValueError(""List referenced by path \'%s\' has length %d, but got ""\n                         ""out of range index %d"" % (current_path, len(structure), index))\n      structure = structure[index]\n    elif isinstance(structure, dict):\n      structure = structure.get(key)\n      if structure is None:\n        raise ValueError(""Dictionary referenced by path \'%s\' does not have the ""\n                         ""key \'%s\'"" % (current_path, key))\n    else:\n      structure = getattr(structure, key, None)\n      if structure is None:\n        raise ValueError(""Object referenced by path \'%s\' does not have the ""\n                         ""attribute \'%s\'"" % (current_path, key))\n  return structure\n\ndef clone_layer(layer):\n  """"""Clones a layer.""""""\n  return copy.deepcopy(layer)\n\ndef set_dropout(root_layer, dropout):\n  """"""Overrides all dropout values in :obj:`root_layer` and its descendants.\n\n  Args:\n    dropout: The dropout value to set.\n\n  Raises:\n    ValueError: if :obj:`root_layer` is not a ``tf.Module``.\n  """"""\n  if not isinstance(root_layer, tf.Module):\n    raise ValueError(""Layer should be a tf.Module"")\n  for layer in (root_layer,) + root_layer.submodules:\n    for attr, value in layer.__dict__.copy().items():\n      if isinstance(value, tf.keras.layers.Dropout):\n        value.rate = dropout\n      elif ""dropout"" in attr:\n        setattr(layer, attr, dropout)\n\ndef extract_batches(tensors):\n  """"""Returns a generator to iterate on each batch of a Numpy array or dict of\n  Numpy arrays.""""""\n  if not isinstance(tensors, dict):\n    for tensor in tensors:\n      yield tensor\n  else:\n    batch_size = None\n    for value in tensors.values():\n      batch_size = batch_size or value.shape[0]\n    for b in range(batch_size):\n      yield {\n          key: value[b] for key, value in tensors.items()\n      }\n\ndef extract_prefixed_keys(dictionary, prefix):\n  """"""Returns a dictionary with all keys from :obj:`dictionary` that are prefixed\n  with :obj:`prefix`.\n  """"""\n  sub_dict = {}\n  for key, value in dictionary.items():\n    if key.startswith(prefix):\n      original_key = key[len(prefix):]\n      sub_dict[original_key] = value\n  return sub_dict\n\ndef extract_suffixed_keys(dictionary, suffix):\n  """"""Returns a dictionary with all keys from :obj:`dictionary` that are suffixed\n  with :obj:`suffix`.\n  """"""\n  sub_dict = {}\n  for key, value in dictionary.items():\n    if key.endswith(suffix):\n      original_key = key[:-len(suffix)]\n      sub_dict[original_key] = value\n  return sub_dict\n\ndef merge_dict(dict1, dict2):\n  """"""Merges :obj:`dict2` into :obj:`dict1`.\n\n  Args:\n    dict1: The base dictionary.\n    dict2: The dictionary to merge.\n\n  Returns:\n    The merged dictionary :obj:`dict1`.\n  """"""\n  for key, value in dict2.items():\n    if isinstance(value, dict):\n      dict1[key] = merge_dict(dict1.get(key, {}), value)\n    else:\n      dict1[key] = value\n  return dict1\n\ndef read_summaries(event_dir, event_file_pattern=""events.out.tfevents.*""):\n  """"""Reads summaries from TensorFlow event files.\n\n  Args:\n    event_dir: Directory containing event files.\n    event_file_pattern: The pattern to look for event files.\n\n  Returns:\n    A list of tuple (step, dict of summaries), sorted by step.\n  """"""\n  if not tf.io.gfile.exists(event_dir):\n    return []\n  summaries = collections.defaultdict(dict)\n  for event_file in tf.io.gfile.glob(os.path.join(event_dir, event_file_pattern)):\n    for event in tf.compat.v1.train.summary_iterator(event_file):\n      if not event.HasField(""summary""):\n        continue\n      for value in event.summary.value:\n        tensor_proto = value.tensor\n        tensor = tf.io.parse_tensor(\n            tensor_proto.SerializeToString(), tf.as_dtype(tensor_proto.dtype))\n        summaries[event.step][value.tag] = tf.get_static_value(tensor)\n  return list(sorted(summaries.items(), key=lambda x: x[0]))\n\nclass OrderRestorer(object):\n  """"""Helper class to restore out-of-order elements in order.""""""\n\n  def __init__(self, index_fn, callback_fn):\n    """"""Initializes this object.\n\n    Args:\n      index_fn: A callable mapping an element to a unique index.\n      callback_fn: A callable taking an element that will be called in order.\n    """"""\n    self._index_fn = index_fn\n    self._callback_fn = callback_fn\n    self._next_index = 0\n    self._elements = {}\n    self._heap = []\n\n  @property\n  def buffer_size(self):\n    """"""Number of elements waiting to be notified.""""""\n    return len(self._heap)\n\n  @property\n  def next_index(self):\n    """"""The next index to be notified.""""""\n    return self._next_index\n\n  def _try_notify(self):\n    old_index = self._next_index\n    while self._heap and self._heap[0] == self._next_index:\n      index = heapq.heappop(self._heap)\n      value = self._elements.pop(index)\n      self._callback_fn(value)\n      self._next_index += 1\n    return self._next_index != old_index\n\n  def push(self, x):\n    """"""Push event :obj:`x`.""""""\n    index = self._index_fn(x)\n    if index is None:\n      self._callback_fn(x)\n      return True\n    if index < self._next_index:\n      raise ValueError(""Event index %d was already notified"" % index)\n    self._elements[index] = x\n    heapq.heappush(self._heap, index)\n    return self._try_notify()\n\nclass ClassRegistry(object):\n  """"""Helper class to create a registry of classes.""""""\n\n  def __init__(self, base_class=None):\n    """"""Initializes the class registry.\n\n    Args:\n      base_class: Ensure that classes added to this registry are a subclass of\n        :obj:`base_class`.\n    """"""\n    self._base_class = base_class\n    self._registry = {}\n\n  @property\n  def class_names(self):\n    """"""Class names registered in this registry.""""""\n    return set(self._registry.keys())\n\n  def register(self, cls=None, name=None, alias=None):\n    """"""Registers a class.\n\n    Args:\n      cls: The class to register. If not set, this method returns a decorator for\n        registration.\n      name: The class name. Defaults to ``cls.__name__``.\n      alias: An optional alias or list of alias for this class.\n\n    Returns:\n      :obj:`cls` if set, else a class decorator.\n\n    Raises:\n      TypeError: if :obj:`cls` does not extend the expected base class.\n      ValueError: if the class name is already registered.\n    """"""\n    if cls is None:\n      return functools.partial(self.register, name=name, alias=alias)\n    if self._base_class is not None and not issubclass(cls, self._base_class):\n      raise TypeError(""Class %s does not extend %s"" % (cls.__name__, self._base_class.__name__))\n    if name is None:\n      name = cls.__name__\n    self._register(cls, name)\n    if alias is not None:\n      if not isinstance(alias, (list, tuple)):\n        alias = (alias,)\n      for alias_name in alias:\n        self._register(cls, alias_name)\n    return cls\n\n  def _register(self, cls, name):\n    if name in self._registry:\n      raise ValueError(""Class name %s is already registered"" % name)\n    self._registry[name] = cls\n\n  def get(self, name):\n    """"""Returns the class with name :obj:`name` or ``None`` if it does not exist\n    in the registry.\n    """"""\n    return self._registry.get(name)\n'"
opennmt/utils/scorers.py,1,"b'""""""Hypotheses file scoring.""""""\n\nimport abc\n\nimport tensorflow as tf\n\nfrom rouge import FilesRouge\nfrom sacrebleu import corpus_bleu\n\nfrom opennmt.utils import misc\nfrom opennmt.utils.fmeasure import fmeasure\nfrom opennmt.utils.ter import ter\nfrom opennmt.utils.wer import wer\n\n\nclass Scorer(abc.ABC):\n  """"""Scores hypotheses against references.""""""\n\n  def __init__(self, name):\n    self._name = name\n\n  @property\n  def name(self):\n    """"""The scorer name.""""""\n    return self._name\n\n  @property\n  def scores_name(self):\n    """"""The names of returned scores.""""""\n    return {self._name}\n\n  @abc.abstractmethod\n  def __call__(self, ref_path, hyp_path):\n    """"""Scores hypotheses.\n\n    Args:\n      ref_path: Path to the reference.\n      hyp_path: Path to the hypotheses.\n\n    Returns:\n      The score or dictionary of scores.\n    """"""\n    raise NotImplementedError()\n\n  def lower_is_better(self):\n    """"""Returns ``True`` if a lower score is better.""""""\n    return False\n\n  def higher_is_better(self):\n    """"""Returns ``True`` if a higher score is better.""""""\n    return not self.lower_is_better()\n\n\n_SCORERS_REGISTRY = misc.ClassRegistry(base_class=Scorer)\nregister_scorer = _SCORERS_REGISTRY.register  # pylint: disable=invalid-name\n\n\n@register_scorer(name=""rouge"")\nclass ROUGEScorer(Scorer):\n  """"""ROUGE scorer based on https://github.com/pltrdy/rouge.""""""\n\n  def __init__(self):\n    super(ROUGEScorer, self).__init__(""rouge"")\n\n  @property\n  def scores_name(self):\n    return {""rouge-1"", ""rouge-2"", ""rouge-l""}\n\n  def __call__(self, ref_path, hyp_path):\n    scorer = FilesRouge(metrics=list(self.scores_name))\n    rouge_scores = scorer.get_scores(hyp_path, ref_path, avg=True)\n    return {name:rouge_scores[name][""f""] for name in self.scores_name}\n\n\n@register_scorer(name=""bleu"")\nclass BLEUScorer(Scorer):\n  """"""Scorer using sacreBLEU.""""""\n\n  def __init__(self):\n    super(BLEUScorer, self).__init__(""bleu"")\n\n  def __call__(self, ref_path, hyp_path):\n    with tf.io.gfile.GFile(ref_path) as ref_stream, tf.io.gfile.GFile(hyp_path) as sys_stream:\n      bleu = corpus_bleu(sys_stream, [ref_stream], force=True)\n      return bleu.score\n\n\n@register_scorer(name=""wer"")\nclass WERScorer(Scorer):\n  """"""Scorer for WER.""""""\n\n  def __init__(self):\n    super(WERScorer, self).__init__(""wer"")\n\n  def __call__(self, ref_path, hyp_path):\n    wer_score = wer(ref_path, hyp_path)\n    return wer_score\n\n  def lower_is_better(self):\n    """""" Since the score shall be the lower the better """"""\n    return True\n\n\n@register_scorer(name=""ter"")\nclass TERScorer(Scorer):\n  """"""Scorer for TER.""""""\n\n  def __init__(self):\n    super(TERScorer, self).__init__(""ter"")\n\n  def __call__(self, ref_path, hyp_path):\n    ter_score = ter(ref_path, hyp_path)\n    return ter_score\n\n  def lower_is_better(self):\n    """""" Since the score shall be the lower the better """"""\n    return True\n\n\n@register_scorer(name=""prfmeasure"", alias=""prf"")\nclass PRFScorer(Scorer):\n  """"""Scorer for F-measure.""""""\n\n  def __init__(self):\n    super(PRFScorer, self).__init__(""prfmeasure"")\n\n  @property\n  def scores_name(self):\n    return {""precision"", ""recall"", ""fmeasure""}\n\n  def __call__(self, ref_path, hyp_path):\n    precision_score, recall_score, fmeasure_score = fmeasure(ref_path, hyp_path)\n    return {\n        ""precision"": precision_score,\n        ""recall"": recall_score,\n        ""fmeasure"": fmeasure_score\n    }\n\n\ndef make_scorers(names):\n  """"""Returns a list of scorers.\n\n  Args:\n    names: A list of scorer names or a single name.\n\n  Returns:\n    A list of :class:`opennmt.evaluation.Scorer` instances.\n\n  Raises:\n    ValueError: if a scorer name is invalid.\n  """"""\n  if not isinstance(names, list):\n    names = [names]\n  scorers = []\n  for name in names:\n    scorer_class = _SCORERS_REGISTRY.get(name.lower())\n    if scorer_class is None:\n      raise ValueError(""No scorer associated with the name: {}"".format(name))\n    scorers.append(scorer_class())\n  return scorers\n'"
opennmt/utils/tensor.py,8,"b'""""""Various tensor manipulation functions.""""""\n\nimport tensorflow as tf\n\n\ndef roll_sequence(tensor, offsets):\n  """"""Shifts sequences by an offset.\n\n  Args:\n    tensor: A ``tf.Tensor`` of shape :math:`[B, T, ...]`.\n    offsets : The offset of each sequence of shape :math:`[B]`.\n\n  Returns:\n    A ``tf.Tensor`` with the same shape as :obj:`tensor` and with sequences\n    shifted by :obj:`offsets`.\n  """"""\n  batch_size = tf.shape(tensor)[0]\n  time = tf.shape(tensor)[1]\n  cols, rows = tf.meshgrid(tf.range(time), tf.range(batch_size))\n  cols -= tf.expand_dims(offsets, 1)\n  cols %= time\n  indices = tf.stack([rows, cols], axis=-1)\n  return tf.gather_nd(tensor, indices)\n'"
opennmt/utils/ter.py,0,"b'""""""Hypotheses file scoring.""""""\nimport pyter\n\ndef ter(ref_path, hyp_path):\n  """""" Compute Translation Edit Rate between two files """"""\n  with open(ref_path) as ref_fp, open(hyp_path) as hyp_fp:\n    ref_line = ref_fp.readline()\n    hyp_line = hyp_fp.readline()\n    ter_score = 0.0\n    line_cpt = 0.0\n    while ref_line and hyp_line:\n      ter_score = ter_score+(pyter.ter(hyp_line.strip().split(), \\\n                  ref_line.strip().split()))\n      line_cpt = line_cpt+1\n      ref_line = ref_fp.readline()\n      hyp_line = hyp_fp.readline()\n  mean_ter = 1.0\n  if line_cpt > 0:\n    mean_ter = ter_score/line_cpt\n  return mean_ter\n'"
opennmt/utils/wer.py,0,"b'""""""Hypotheses file scoring.""""""\nimport numpy\nimport pyter\n\ndef wer(ref_path, hyp_path):\n  """""" Compute Word Error Rate between two files """"""\n  with open(ref_path) as ref_fp, open(hyp_path) as hyp_fp:\n    ref_line = ref_fp.readline()\n    hyp_line = hyp_fp.readline()\n    wer_score = 0.0\n    line_cpt = 0.0\n    while ref_line and hyp_line:\n      wer_score = wer_score+(pyter.edit_distance(ref_line.strip().split(), \\\n                  hyp_line.strip().split())/len(ref_line.strip().split()))\n      line_cpt = line_cpt+1\n      ref_line = ref_fp.readline()\n      hyp_line = hyp_fp.readline()\n  mean_wer = 1.0\n  if line_cpt > 0:\n    mean_wer = wer_score/line_cpt\n  return mean_wer\n\ndef sentence_wer(ref_sent, hyp_sent):\n  """""" Compute Word Error Rate between two sentences (as list of words) """"""\n  mwer = numpy.zeros((len(ref_sent)+1)*(len(hyp_sent)+1), \\\n         dtype=numpy.uint8).reshape((len(ref_sent)+1, len(hyp_sent)+1))\n  for i in range(len(ref_sent)+1):\n    mwer[i][0] = i\n  for j in range(len(hyp_sent)+1):\n    mwer[0][j] = j\n  for i in range(1, len(ref_sent)+1):\n    for j in range(1, len(hyp_sent)+1):\n      if ref_sent[i-1] == hyp_sent[j-1]:\n        mwer[i][j] = mwer[i-1][j-1]\n      else:\n        substitute = mwer[i-1][j-1] + 1\n        insert = mwer[i][j-1] + 1\n        delete = mwer[i-1][j] + 1\n        mwer[i][j] = min(substitute, insert, delete)\n  sent_wer = 1.0\n  if len(ref_sent) > 0:\n    sent_wer = mwer[len(ref_sent)][len(hyp_sent)]/len(ref_sent)\n  return sent_wer\n'"
examples/serving/python/ende_client.py,3,"b'import argparse\nimport os\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\ntfa.register_all()  # Register custom ops.\n\nimport pyonmttok\n\n\nclass EnDeTranslator(object):\n\n  def __init__(self, export_dir):\n    imported = tf.saved_model.load(export_dir)\n    self._translate_fn = imported.signatures[""serving_default""]\n    sp_model_path = os.path.join(export_dir, ""assets.extra"", ""wmtende.model"")\n    self._tokenizer = pyonmttok.Tokenizer(""none"", sp_model_path=sp_model_path)\n\n  def translate(self, texts):\n    """"""Translates a batch of texts.""""""\n    inputs = self._preprocess(texts)\n    outputs = self._translate_fn(**inputs)\n    return self._postprocess(outputs)\n\n  def _preprocess(self, texts):\n    all_tokens = []\n    lengths = []\n    max_length = 0\n    for text in texts:\n      tokens, _ = self._tokenizer.tokenize(text)\n      length = len(tokens)\n      all_tokens.append(tokens)\n      lengths.append(length)\n      max_length = max(max_length, length)\n    for tokens, length in zip(all_tokens, lengths):\n      if length < max_length:\n        tokens += [""""] * (max_length - length)\n\n    inputs = {\n        ""tokens"": tf.constant(all_tokens, dtype=tf.string),\n        ""length"": tf.constant(lengths, dtype=tf.int32)}\n    return inputs\n\n  def _postprocess(self, outputs):\n    texts = []\n    for tokens, length in zip(outputs[""tokens""].numpy(), outputs[""length""].numpy()):\n      tokens = tokens[0][:length[0]].tolist()\n      texts.append(self._tokenizer.detokenize(tokens))\n    return texts\n\n\ndef main():\n  parser = argparse.ArgumentParser(description=""Translation client example"")\n  parser.add_argument(""export_dir"", help=""Saved model directory"")\n  args = parser.parse_args()\n\n  translator = EnDeTranslator(args.export_dir)\n\n  while True:\n    text = input(""Source: "")\n    output = translator.translate([text])\n    print(""Target: %s"" % output[0])\n    print("""")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
examples/serving/tensorflow_serving/ende_client.py,6,"b'""""""Example of a translation client.""""""\n\nimport argparse\nimport tensorflow as tf\nimport pyonmttok\nimport grpc\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\ndef pad_batch(batch_tokens):\n  """"""Pads a batch of tokens.""""""\n  lengths = [len(tokens) for tokens in batch_tokens]\n  max_length = max(lengths)\n  for tokens, length in zip(batch_tokens, lengths):\n    if max_length > length:\n      tokens += [""""] * (max_length - length)\n  return batch_tokens, lengths, max_length\n\ndef extract_prediction(result):\n  """"""Parses a translation result.\n\n  Args:\n    result: A `PredictResponse` proto.\n\n  Returns:\n    A generator over the hypotheses.\n  """"""\n  batch_lengths = tf.make_ndarray(result.outputs[""length""])\n  batch_predictions = tf.make_ndarray(result.outputs[""tokens""])\n  for hypotheses, lengths in zip(batch_predictions, batch_lengths):\n    # Only consider the first hypothesis (the best one).\n    best_hypothesis = hypotheses[0].tolist()\n    best_length = lengths[0]\n    if best_hypothesis[best_length - 1] == b""</s>"":\n      best_length -= 1\n    yield best_hypothesis[:best_length]\n\ndef send_request(stub, model_name, batch_tokens, timeout=5.0):\n  """"""Sends a translation request.\n\n  Args:\n    stub: The prediction service stub.\n    model_name: The model to request.\n    tokens: A list of tokens.\n    timeout: Timeout after this many seconds.\n\n  Returns:\n    A future.\n  """"""\n  batch_tokens, lengths, max_length = pad_batch(batch_tokens)\n  batch_size = len(lengths)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.inputs[""tokens""].CopyFrom(tf.make_tensor_proto(\n      batch_tokens, dtype=tf.string, shape=(batch_size, max_length)))\n  request.inputs[""length""].CopyFrom(tf.make_tensor_proto(\n      lengths, dtype=tf.int32, shape=(batch_size,)))\n  return stub.Predict.future(request, timeout)\n\ndef translate(stub, model_name, batch_text, tokenizer, timeout=5.0):\n  """"""Translates a batch of sentences.\n\n  Args:\n    stub: The prediction service stub.\n    model_name: The model to request.\n    batch_text: A list of sentences.\n    tokenizer: The tokenizer to apply.\n    timeout: Timeout after this many seconds.\n\n  Returns:\n    A generator over the detokenized predictions.\n  """"""\n  batch_input = [tokenizer.tokenize(text)[0] for text in batch_text]\n  future = send_request(stub, model_name, batch_input, timeout=timeout)\n  result = future.result()\n  batch_output = [tokenizer.detokenize(prediction) for prediction in extract_prediction(result)]\n  return batch_output\n\ndef main():\n  parser = argparse.ArgumentParser(description=""Translation client example"")\n  parser.add_argument(""--model_name"", required=True,\n                      help=""model name"")\n  parser.add_argument(""--sentencepiece_model"", required=True,\n                      help=""path to the sentence model"")\n  parser.add_argument(""--host"", default=""localhost"",\n                      help=""model server host"")\n  parser.add_argument(""--port"", type=int, default=9000,\n                      help=""model server port"")\n  parser.add_argument(""--timeout"", type=float, default=10.0,\n                      help=""request timeout"")\n  args = parser.parse_args()\n\n  channel = grpc.insecure_channel(""%s:%d"" % (args.host, args.port))\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n  tokenizer = pyonmttok.Tokenizer(""none"", sp_model_path=args.sentencepiece_model)\n\n  while True:\n    text = input(""Source: "")\n    output = translate(stub, args.model_name, [text], tokenizer, timeout=args.timeout)\n    print(""Target: %s"" % output[0])\n    print("""")\n\n\nif __name__ == ""__main__"":\n  main()\n'"
