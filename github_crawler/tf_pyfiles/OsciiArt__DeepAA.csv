file_path,api_count,code
output.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom keras.models import model_from_json\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport pickle\nimport os\n\n\n# parameters\nmodel_path = ""model/model.json"" # or ""model/model_light.json""\nweight_path = ""model/weight.hdf5"" # or ""model/weight_light.json""\nimage_path = \'sample images/original images/21 original.png\' # put the path of the image that you convert.\nnew_width = 0 # adjust the width of the image. the original width is used if new_width = 0.\ninput_shape = [64, 64, 1]\n\n\ndef add_mergin(img, mergin):\n    if mergin!=0:\n        img_new = np.ones([img.shape[0] + 2 * mergin, img.shape[1] + 2 * mergin], dtype=np.uint8) * 255\n        img_new[mergin:-mergin, mergin:-mergin] = img\n    else:\n        img_new = img\n    return img_new\n\n\ndef pickleload(path):\n    with open(path, mode=\'rb\') as f:\n        data = pickle.load(f)\n    return data\n\n\n# load model\njson_string = open(model_path).read()\nmodel = model_from_json(json_string)\nmodel.load_weights(weight_path)\nprint(""model load done"")\n\nchar_list_path = ""data/char_list.csv""\nchar_list = pd.read_csv(char_list_path, encoding=""cp932"")\nprint(""len(char_list)"", len(char_list))\n# print(char_list.head())\nchar_list = char_list[char_list[\'frequency\']>=10]\nchar_list = char_list[\'char\'].as_matrix()\n\nfor k, v in enumerate(char_list):\n    if v=="" "":\n        space = k\n        break\nprint(""class index of 1B space:"", space)\n\n\nmergin = (input_shape[0] - 18) // 2\nimg = Image.open(image_path)\norig_width, orig_height = img.size\nif new_width==0: new_width = orig_width\nnew_height = int(img.size[1] * new_width / img.size[0])\nimg = img.resize((new_width, new_height), Image.LANCZOS)\nimg = np.array(img)\nif len(img.shape) == 3:\n    img = img[:, :, 0]\n\nimg_new = np.ones([img.shape[0]+2*mergin+18, img.shape[1]+2*mergin+18],\n                  dtype=np.uint8) * 255\nimg_new[mergin:mergin+new_height, mergin:mergin+new_width] = img\nimg = (img_new.astype(np.float32)) / 255\n\nchar_dict_path = ""data/char_dict.pkl""\nchar_dict = pickleload(char_dict_path)\n\nprint(""len(char_dict)"", len(char_dict))\n\noutput_dir = ""output/""\nif not os.path.isdir(output_dir):\n    os.makedirs(output_dir)\n\nfor slide in range(18):\n    print(""converting:"", slide)\n    num_line = (img.shape[0] - input_shape[0]) // 18\n    img_width = img.shape[1]\n    new_line = np.ones([1, img_width])\n    img = np.concatenate([new_line, img], axis=0)\n    predicts = []\n    text = []\n    for h in range(num_line):\n        w = 0\n        penalty = 1\n        predict_line = []\n        text_line = """"\n        while w <= img_width - input_shape[1]:\n            input_img = img[h*18:h*18+ input_shape[0], w:w+input_shape[1]]\n            input_img = input_img.reshape([1,input_shape[0], input_shape[1], 1])\n            predict = model.predict(input_img)\n            if penalty: predict[0, space] = 0\n            predict = np.argmax(predict[0])\n            penalty = (predict==space)\n            char = char_list[predict]\n            predict_line.append(char)\n            char_width = char_dict[char].shape[1]\n            w += char_width\n            text_line += char\n        predicts.append(predict_line)\n        text.append(text_line+\'\\r\\n\')\n    # print(text)\n\n    img_aa = np.ones_like(img, dtype=np.uint8) * 255\n\n    for h in range(num_line):\n        w = 0\n        for char in predicts[h]:\n            # print(""w"", w)\n            char_width = char_dict[char].shape[1]\n            char_img = 255 - char_dict[char].astype(np.uint8) * 255\n            img_aa[h*18:h*18+16, w:w+char_width] = char_img\n            w += char_width\n\n    img_aa = Image.fromarray(img_aa)\n    img_aa = img_aa.crop([0,slide,new_width, new_height+slide])\n    save_path = output_dir + os.path.basename(image_path)[:-4] + \'_\'\\\n                + \'w\' + str(new_width) \\\n                + \'_slide\' + str(slide) + \'.png\'\n    img_aa.save(save_path)\n\n    f=open(save_path[:-4] + \'.txt\', \'w\')\n    f.writelines(text)\n    f.close()'"
train.py,0,"b'from keras.models import Model\nfrom keras.layers import Dense, Activation, Reshape, Dropout, Embedding, Input, BatchNormalization\nfrom keras.layers import Concatenate, Multiply, Conv2D, MaxPooling2D, Add, Flatten, GaussianNoise\nfrom keras.models import model_from_json\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint, \\\n    EarlyStopping, CSVLogger, ReduceLROnPlateau\n\nimport time\nimport numpy as np\n\nnp.random.seed(42)\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport math\nfrom multiprocessing import Pool\n\n\ndef CBRD(inputs, filters=64, kernel_size=(3,3), droprate=0.5):\n    x = Conv2D(filters, kernel_size, padding=\'same\',\n               kernel_initializer=\'random_normal\')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    # x = Dropout(droprate)(x)\n    return x\n\n\ndef DBRD(inputs, units=4096, droprate=0.5):\n    x = Dense(units)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\'relu\')(x)\n    x = Dropout(droprate)(x)\n    return x\n\ndef CNN(input_shape=None, classes=1000):\n    inputs = Input(shape=input_shape)\n\n    # Block 1\n    x = GaussianNoise(0.3)(inputs)\n    x = CBRD(x, 64)\n    x = CBRD(x, 64)\n    x = MaxPooling2D()(x)\n\n    # Block 2\n    x = CBRD(x, 128)\n    x = CBRD(x, 128)\n    x = MaxPooling2D()(x)\n\n    # Block 3\n    x = CBRD(x, 256)\n    x = CBRD(x, 256)\n    x = CBRD(x, 256)\n    x = MaxPooling2D()(x)\n\n    # Classification block\n    x = Flatten(name=\'flatten\')(x)\n    x = DBRD(x, 4096)\n    x = DBRD(x, 4096)\n    x = Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n\n    return model\n\n\ndef add_mergin(img, mergin):\n    if mergin!=0:\n        img_new = np.ones([img.shape[0] + 2 * mergin, img.shape[1] + 2 * mergin], dtype=np.uint8) * 255\n        img_new[mergin:-mergin, mergin:-mergin] = img\n    else:\n        img_new = img\n    return img_new\n\n\ndef load_img(args):\n    img_path, x, y, input_size, mergin, slide = args\n    img = np.array(Image.open(img_path))\n    if len(img.shape) == 3:\n        img = img[:, :, 0]\n    img = add_mergin(img, mergin)\n    x += np.random.randint(-slide, slide+1)\n    y += np.random.randint(-slide, slide+1)\n    img = img[y:y + input_size, x:x + input_size]\n    img = img.reshape([1, input_size, input_size, 1])\n    # print(img_path, x, y, input_size, mergin )\n    # print(input_size, img.shape)\n    return img\n\ndef batch_generator(df, img_dir, input_size, batch_size, num_label, slide,\n                    tail=\'line\', shuffle=True):\n    df = df.reset_index()\n    batch_index = 0\n    mergin = (input_size - 18) // 2 + 30\n    n = df.shape[0]\n    pool = Pool()\n    while 1:\n        if batch_index == 0:\n            index_array = np.arange(n)\n            if shuffle:\n                index_array = np.random.permutation(n)\n\n        current_index = (batch_index * batch_size) % n\n        if n >= current_index + batch_size:\n            current_batch_size = batch_size\n            batch_index += 1\n        else:\n            current_batch_size = n - current_index\n            batch_index = 0\n\n        index_array_batch = index_array[current_index: current_index + current_batch_size]\n        batch_img_path = df[\'file_name\'][index_array_batch].apply(\n            lambda x: img_dir + x + tail + \'.png\').as_matrix()\n        # print(batch_img_path)\n        batch_coord_x = (df[\'x\'][index_array_batch] + 30).as_matrix()\n        batch_coord_y = (df[\'y\'][index_array_batch] + 30).as_matrix()\n        # print(batch_img_path[0], batch_coord_x[0], batch_coord_y[0], mergin)\n        batch_x = pool.map(load_img,\n                           [(batch_img_path[i],\n                             batch_coord_x[i],\n                             batch_coord_y[i],\n                             input_size,\n                             mergin,\n                             slide)\n                           for i in range(current_batch_size)])\n        # print(batch_x[0].shape)\n        batch_x = np.concatenate(batch_x, axis=0)\n        batch_x = batch_x.astype(np.float32) / 255\n        # print(batch_x.shape)\n\n        batch_y = df[\'label\'][index_array[current_index: current_index + current_batch_size]].as_matrix()\n        batch_y = np.eye(num_label)[batch_y]\n\n        yield batch_x, batch_y\n\n\ndef train_generator(df, img_dir, input_size, batch_size, num_label, slide,\n                    tail=\'line\', shuffle=True):\n    gen_line = batch_generator(df, img_dir, input_size,\n                               batch_size // 2, num_label, slide, tail=""line_resize"")\n    gen_orig = batch_generator(df, img_dir, input_size,\n                               batch_size // 2, num_label, slide, tail=""orig"")\n    while True:\n        batch1 = next(gen_line)\n        batch2 = next(gen_orig)\n        batch_x = np.concatenate([batch1[0], batch2[0]])\n        batch_y = np.concatenate([batch1[1], batch2[1]])\n        yield batch_x, batch_y\n\n\ndef train():\n    # parameter\n    num_epoch = 256\n    batch_size = 64\n    input_shape = [64,64,1]\n    learning_rate = 0.001\n    df_path = ""data/data_500.csv""\n    char_list_path = ""data/char_list_500.csv""\n    img_dir = ""data/image_500/""\n\n    # load text\n    df = pd.read_csv(df_path, encoding=""cp932"")\n    char_list = pd.read_csv(char_list_path, encoding=""cp932"")\n    num_label = char_list[char_list[\'frequency\']>=10].shape[0]\n    # print(num_label)\n    df = df[df[\'label\']<num_label]\n    df = df.reset_index()\n    input_size = input_shape[0]\n    slide = 1\n    df_train, df_val = train_test_split(df, test_size=0.1, random_state=42)\n    gen = train_generator(df_train, img_dir,\n                          input_size, batch_size, num_label, slide)\n    gen_val = batch_generator(df_val, img_dir, input_size,\n                              batch_size, num_label, 0,\n                              tail=""line_resize"", shuffle=False)\n\n    # build model\n    model = CNN(input_shape=input_shape, classes=num_label)\n    model.compile(loss=\'categorical_crossentropy\', optimizer=\'sgd\', metrics=[\'accuracy\'])\n\n    # train\n    nb_train = df_train.shape[0]\n    nb_val = df_val.shape[0]\n    nb_step = math.ceil(nb_train / batch_size)\n    nb_val_step = math.ceil(nb_val / batch_size)\n\n    format = ""%H%M""\n    ts = time.strftime(format)\n    save_path = ""model/"" + path.splitext(__file__)[0] + ""_"" + ts\n\n    json_string = model.to_json()\n    with open(save_path + \'_model.json\', ""w"") as f:\n        f.write(json_string)\n\n    csv_logger = CSVLogger(save_path + \'_log.csv\', append=True)\n    check_path = save_path + \'_e{epoch:02d}_vl{val_loss:.5f}.hdf5\'\n    save_checkpoint = ModelCheckpoint(filepath=check_path, monitor=\'val_loss\', save_best_only=True)\n    lerning_rate_schedular = ReduceLROnPlateau(patience=8, min_lr=learning_rate * 0.00001)\n    early_stopping = EarlyStopping(monitor=\'val_loss\',\n                                   patience=16,\n                                   verbose=1,\n                                   min_delta=1e-4,\n                                   mode=\'min\')\n    Callbacks = [csv_logger,\n                 save_checkpoint,\n                 lerning_rate_schedular, early_stopping]\n    model.fit_generator(gen,\n                        steps_per_epoch=nb_step,\n                        epochs=num_epoch,\n                        validation_data=gen_val,\n                        validation_steps=nb_val_step,\n                        callbacks=Callbacks\n                        )\n\n\nif __name__ == ""__main__"":\n    train()'"
