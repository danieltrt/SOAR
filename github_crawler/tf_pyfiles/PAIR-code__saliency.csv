file_path,api_count,code
setup.py,0,"b'""""""A setuptools module for the Saliency library.\n\nSee:\nhttps://packaging.python.org/en/latest/distributing.html\n""""""\n\n# Always prefer setuptools over distutils\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'saliency\',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=\'0.0.5\',\n\n    description=\'Saliency methods for TensorFlow\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n\n    # The project\'s main homepage.\n    url=\'https://github.com/pair-code/saliency\',\n\n    # Author details\n    author=\'The saliency authors\',\n    author_email=\'tf-saliency-dev@google.com\',\n\n    # Choose your license\n    license=\'Apache 2.0\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 4 - Beta\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n    ],\n\n    # What does your project relate to?\n    keywords=\'saliency mask neural network deep learning\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=[\'saliency\'],\n    #package_dir={\'\': \'.\'},\n    #packages=[\'\'],\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #py_modules=[\'saliency\'],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'numpy\', \'tensorflow\'],\n)\n'"
saliency/__init__.py,0,b'from .base import *\nfrom .visualization import *\nfrom .guided_backprop import *\nfrom .occlusion import *\nfrom .integrated_gradients import *\nfrom .grad_cam import *\nfrom .xrai import *\nfrom .blur_ig import *\n'
saliency/base.py,2,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to compute SaliencyMasks.""""""\nimport numpy as np\nimport tensorflow as tf\n\nclass SaliencyMask(object):\n  """"""Base class for saliency masks. Alone, this class doesn\'t do anything.""""""\n  def __init__(self, graph, session, y, x):\n    """"""Constructs a SaliencyMask by computing dy/dx.\n\n    Args:\n      graph: The TensorFlow graph to evaluate masks on.\n      session: The current TensorFlow session.\n      y: The output tensor to compute the SaliencyMask against. This tensor\n          should be of size 1.\n      x: The input tensor to compute the SaliencyMask against. The outer\n          dimension should be the batch size.\n    """"""\n\n    # y must be of size one, otherwise the gradient we get from tf.gradients\n    # will be summed over all ys.\n    size = 1\n    for shape in y.shape:\n      size *= shape\n    assert size == 1\n\n    self.graph = graph\n    self.session = session\n    self.y = y\n    self.x = x\n\n  def GetMask(self, x_value, feed_dict={}):\n    """"""Returns an unsmoothed mask.\n\n    Args:\n      x_value: Input value, not batched.\n      feed_dict: (Optional) feed dictionary to pass to the session.run call.\n    """"""\n    raise NotImplementedError(\'A derived class should implemented GetMask()\')\n\n  def GetSmoothedMask(\n      self, x_value, feed_dict={}, stdev_spread=.15, nsamples=25,\n      magnitude=True, **kwargs):\n    """"""Returns a mask that is smoothed with the SmoothGrad method.\n\n    Args:\n      x_value: Input value, not batched.\n      feed_dict: (Optional) feed dictionary to pass to the session.run call.\n      stdev_spread: Amount of noise to add to the input, as fraction of the\n                    total spread (x_max - x_min). Defaults to 15%.\n      nsamples: Number of samples to average across to get the smooth gradient.\n      magnitude: If true, computes the sum of squares of gradients instead of\n                 just the sum. Defaults to true.\n    """"""\n    stdev = stdev_spread * (np.max(x_value) - np.min(x_value))\n\n    total_gradients = np.zeros_like(x_value)\n    for i in range(nsamples):\n      noise = np.random.normal(0, stdev, x_value.shape)\n      x_plus_noise = x_value + noise\n      grad = self.GetMask(x_plus_noise, feed_dict, **kwargs)\n      if magnitude:\n        total_gradients += (grad * grad)\n      else:\n        total_gradients += grad\n\n    return total_gradients / nsamples\n\nclass GradientSaliency(SaliencyMask):\n  r""""""A SaliencyMask class that computes saliency masks with a gradient.""""""\n\n  def __init__(self, graph, session, y, x):\n    super(GradientSaliency, self).__init__(graph, session, y, x)\n    self.gradients_node = tf.gradients(y, x)[0]\n\n  def GetMask(self, x_value, feed_dict={}):\n    """"""Returns a vanilla gradient mask.\n\n    Args:\n      x_value: Input value, not batched.\n      feed_dict: (Optional) feed dictionary to pass to the session.run call.\n    """"""\n    feed_dict[self.x] = [x_value]\n    return self.session.run(self.gradients_node, feed_dict=feed_dict)[0]\n'"
saliency/blur_ig.py,0,"b'# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to compute Blur IG SaliencyMask.\n\nImplementation of Integrated Gradients along the blur path.\n""""""\n\nimport tensorflow.compat.v1 as tf\nimport math\nimport numpy as np\nfrom scipy import ndimage\nfrom .base import GradientSaliency\n\ndef gaussian_blur(image, sigma):\n  """"""Returns Gaussian blur filtered 3d (WxHxC) image.\n\n  image: 3 dimensional ndarray / input image (W x H x C).\n  sigma: Standard deviation for Gaussian blur kernel.\n  """"""\n  if sigma == 0:\n    return image\n  return ndimage.gaussian_filter(image,\n                                 sigma=[sigma, sigma, 0],\n                                 mode=\'constant\')\n\nclass BlurIG(GradientSaliency):\n  """"""A SaliencyMask class that implements integrated gradients along blur path.\n\n  Generates a saliency mask by computing integrated gradients for a given input\n  and prediction label using a path that successively blurs the image.\n  TODO(vsubhashini): Add link to paper after it goes up on arxiv.\n  """"""\n\n  def GetMask(self, x_value, feed_dict={}, max_sigma=50, steps=100,\n              grad_step=0.01, sqrt=False):\n    """"""Returns an integrated gradients mask.\n\n    TODO(vsubhashini): Decide if we want to restrict and find explanation\n      between points of maximum information.\n\n    Args:\n      x_value: Input ndarray.\n      max_sigma: Maximum size of the gaussian blur kernel.\n      steps: Number of successive blur applications between x and until fully\n       blurred image (with kernel max_sigma).\n      grad_step: Gaussian gradient step size.\n      sqrt: Chooses square root when deciding spacing between sigma. (Full\n            mathematical implication remains to be understood).\n    """"""\n\n    if sqrt:\n      sigmas = [math.sqrt(float(i)*max_sigma/float(steps)\n                          ) for i in range(0, steps+1)]\n    else:\n      sigmas = [float(i)*max_sigma/float(steps) for i in range(0, steps+1)]\n    step_vector_diff = [sigmas[i+1] - sigmas[i] for i in range(0, steps)]\n\n    total_gradients = np.zeros_like(x_value)\n    for i in range(steps):\n      x_step = gaussian_blur(x_value, sigmas[i])\n      gaussian_gradient = (gaussian_blur(x_value, sigmas[i] + grad_step)\n                           - x_step) / grad_step\n      total_gradients += step_vector_diff[i] * np.multiply(\n          gaussian_gradient, super(BlurIG, self).GetMask(x_step, feed_dict))\n\n    total_gradients *= -1.0\n    return total_gradients\n'"
saliency/blur_ig_test.py,3,"b'# Copyright 2020 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Tests completeness axiom for blur_ig.\n""""""\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.python.platform import googletest\n\nfrom . import blur_ig\n\n\nclass BlurIgTest(googletest.TestCase):\n  """"""\n  To run:\n  ""python -m saliency.blur_ig_test"" from the PAIR-code/saliency directory.\n  """"""\n\n  def testBlurIGGetMask(self):\n    max_sigma = 10\n    with tf.Graph().as_default() as graph:\n      x = tf.placeholder(shape=[None, 5, 5, 1], dtype=tf.float32)\n      # Define function to just look at center pixel.\n      y = x[:, 2, 2, 0] * 1.0\n      with tf.Session() as sess:\n        # All black except 1 white pixel at the center.\n        x_input_val = np.array([[0.0, 0.0, 0.0, 0.0, 0.0],\n                                [0.0, 0.0, 0.0, 0.0, 0.0],\n                                [0.0, 0.0, 1.0, 0.0, 0.0],\n                                [0.0, 0.0, 0.0, 0.0, 0.0],\n                                [0.0, 0.0, 0.0, 0.0, 0.0],\n                               ], dtype=np.float)\n        x_input_val = x_input_val.reshape((5, 5, 1))\n        # Calculate the value of `y` at the input.\n        y_input_val = sess.run(y, feed_dict={x: [x_input_val]})\n\n        # Baseline is the fully blurred version of the input.\n        x_baseline_val = blur_ig.gaussian_blur(x_input_val, sigma=max_sigma)\n        y_baseline_val = sess.run(y, feed_dict={x: [x_baseline_val]})\n\n        # Test if completeness axiom is satisfied.\n        # The expected BlurIG value is equal to the difference between\n        # the `y` value at the input and the `y` value at the baseline.\n        expected_val = y_input_val[0] - y_baseline_val[0]\n\n        # Calculate the Blur IG attribution of the input.\n        blur_ig_instance = blur_ig.BlurIG(graph, sess, y[0], x)\n        mask = blur_ig_instance.GetMask(\n            x_input_val, feed_dict={}, max_sigma=max_sigma, steps=200)\n        # Verify the result for completeness..\n        # Expected (for max_sigma=10): 0.9984083\n        # mask.sum (for max_sigma=10): 0.99832882...\n        self.assertAlmostEqual(expected_val, mask.sum(), places=3)\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
saliency/grad_cam.py,2,"b'# Copyright 2017 Ruth Fong. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to compute SaliencyMasks.""""""\nimport numpy as np\nimport tensorflow as tf\n\nfrom .base import SaliencyMask\n\nclass GradCam(SaliencyMask):\n    """"""A SaliencyMask class that computes saliency masks with Grad-CAM.\n  \n    https://arxiv.org/abs/1610.02391\n\n    Example usage (based on Examples.ipynb):\n\n    grad_cam = GradCam(graph, sess, y, images, conv_layer = end_points[\'Mixed_7c\'])\n    grad_mask_2d = grad_cam.GetMask(im, feed_dict = {neuron_selector: prediction_class}, \n                                    should_resize = False, \n                                    three_dims = False)\n\n    The Grad-CAM paper suggests using the last convolutional layer, which would \n    be \'Mixed_5c\' in inception_v2 and \'Mixed_7c\' in inception_v3.\n\n    """"""\n    def __init__(self, graph, session, y, x, conv_layer):\n        super(GradCam, self).__init__(graph, session, y, x)\n        self.conv_layer = conv_layer\n        self.gradients_node = tf.gradients(y, conv_layer)[0]\n\n    def GetMask(self, x_value, feed_dict={}, should_resize = True, three_dims = True):\n        """"""\n        Returns a Grad-CAM mask.\n        \n        Modified from https://github.com/Ankush96/grad-cam.tensorflow/blob/master/main.py#L29-L62\n\n        Args:\n          x_value: Input value, not batched.\n          feed_dict: (Optional) feed dictionary to pass to the session.run call.\n          should_resize: boolean that determines whether a low-res Grad-CAM mask should be \n              upsampled to match the size of the input image\n          three_dims: boolean that determines whether the grayscale mask should be converted\n              into a 3D mask by copying the 2D mask value\'s into each color channel\n            \n        """"""\n        feed_dict[self.x] = [x_value]\n        (output, grad) = self.session.run([self.conv_layer, self.gradients_node], \n                                               feed_dict=feed_dict)\n        output = output[0]\n        grad = grad[0]\n\n        weights = np.mean(grad, axis=(0,1))\n        grad_cam = np.zeros(output.shape[0:2], dtype=np.float32)\n\n        # weighted average\n        for i, w in enumerate(weights):\n            grad_cam += w * output[:, :, i]\n\n        # pass through relu\n        grad_cam = np.maximum(grad_cam, 0)\n\n        # resize heatmap to be the same size as the input\n        if should_resize:\n            grad_cam = grad_cam / np.max(grad_cam) # values need to be [0,1] to be resized\n            with self.graph.as_default():\n                grad_cam = np.squeeze(tf.image.resize_bilinear(\n                    np.expand_dims(np.expand_dims(grad_cam, 0), 3), \n                    x_value.shape[:2]).eval(session=self.session))\n\n        # convert grayscale to 3-D\n        if three_dims:\n            grad_cam = np.expand_dims(grad_cam, axis=2)\n            grad_cam = np.tile(grad_cam,[1,1,3])\n\n        return grad_cam\n'"
saliency/grad_cam_test.py,12,"b'# Copyright 2018 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom .grad_cam import GradCam\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.platform import googletest\n\n\nclass GradCamTest(googletest.TestCase):\n  """"""\n  To run: \n  ""python -m saliency.grad_cam_test"" from the PAIR-code/saliency directory.\n  """"""\n\n  def testGradCamGetMask(self):\n    """"""\n    Simple test case where the network contains one convolutional layer that\n    acts as a horizontal line detector and the input image is a 5x5 matrix with\n    a centered 3x3 grid of 1s and 0s elsewhere.\n\n    The computed GradCAM mask should detect the pixels of highest importance to\n    be along the two horizontal lines in the image (exact expected values stored\n    in ref_mask).\n    """"""\n    with tf.Graph().as_default() as graph:\n      # Input placeholder\n      num_pix = 5 # width and height of input images in pixels\n      images = tf.placeholder(tf.float32, shape=(1, num_pix, num_pix, 1))\n\n      # Horizontal line detector filter\n      horiz_detector = np.array([[-1,-1,-1],\n                                 [ 2, 2, 2],\n                                 [-1,-1,-1]])\n      conv1 = tf.layers.conv2d(\n          inputs = images,\n          filters = 1,\n          kernel_size = 3,\n          kernel_initializer = tf.constant_initializer(horiz_detector),\n          padding = ""same"",\n          name = ""Conv"")\n\n      # Compute logits and do prediction with pre-defined weights\n      flat = tf.reshape(conv1,[-1,num_pix*num_pix])\n      sum_weights = tf.constant_initializer(np.ones(flat.shape))\n      logits = tf.layers.dense(inputs = flat, units = 2,\n                               kernel_initializer = sum_weights,\n                               name = ""Logits"")\n      predictions = {""classes"": tf.argmax(input=logits, axis=1),\n                     ""probs"": tf.nn.softmax(logits, name=""softmax"")}\n\n      with tf.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        # Set up GradCam object\n        logits = graph.get_tensor_by_name(""Logits/BiasAdd:0"")\n        neuron_selector = tf.placeholder(tf.int32)\n        y = logits[0][neuron_selector]\n        conv_layer = graph.get_tensor_by_name(""Conv/BiasAdd:0"")\n\n        grad_cam = GradCam(graph, sess, y, images, conv_layer)\n\n        # Generate test input (centered matrix of 1s surrounded by 0s)\n        # and generate corresponding GradCAM mask\n        img = np.zeros([num_pix,num_pix])\n        img[1:-1,1:-1] = 1\n        img = img.reshape([num_pix,num_pix,1])\n        mask = grad_cam.GetMask(img,\n                                feed_dict={neuron_selector: 0},\n                                should_resize = True,\n                                three_dims = False)\n\n        #Compare generated mask to expected result\n        ref_mask = np.array([[0.  , 0.  , 0.  , 0.  , 0.  ],\n                             [0.33, 0.67, 1.  , 0.67, 0.33],\n                             [0.  , 0.  , 0.  , 0.  , 0.  ],\n                             [0.33, 0.67, 1.  , 0.67, 0.33],\n                             [0.  , 0.  , 0.  , 0.  , 0.  ]])\n        self.assertTrue(np.allclose(mask, ref_mask, atol=0.01),\n                        ""Generated mask did not match reference mask."")\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
saliency/guided_backprop.py,8,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilites to computed GuidedBackprop SaliencyMasks""""""\n\nfrom .base import SaliencyMask\nimport tensorflow as tf\n\nclass GuidedBackprop(SaliencyMask):\n  """"""A SaliencyMask class that computes saliency masks with GuidedBackProp.\n\n  This implementation copies the TensorFlow graph to a new graph with the ReLU\n  gradient overwritten as in the paper:\n  https://arxiv.org/abs/1412.6806\n\n  Thanks to Chris Olah for generously sharing his implementation of the ReLU\n  backprop.\n  """"""\n\n  GuidedReluRegistered = False\n\n  def __init__(self, graph, session, y, x):\n    """"""Constructs a GuidedBackprop SaliencyMask.""""""\n    super(GuidedBackprop, self).__init__(graph, session, y, x)\n\n    self.x = x\n\n    if GuidedBackprop.GuidedReluRegistered is False:\n      #### Acknowledgement to Chris Olah ####\n      @tf.RegisterGradient(""GuidedRelu"")\n      def _GuidedReluGrad(op, grad):\n        gate_g = tf.cast(grad > 0, ""float32"")\n        gate_y = tf.cast(op.outputs[0] > 0, ""float32"")\n        return gate_y * gate_g * grad\n    GuidedBackprop.GuidedReluRegistered = True\n\n    with graph.as_default():\n      saver = tf.train.Saver()\n      saver.save(session, \'/tmp/guided_backprop_ckpt\')\n\n    graph_def = graph.as_graph_def()\n\n    self.guided_graph = tf.Graph()\n    with self.guided_graph.as_default():\n      self.guided_sess = tf.Session(graph = self.guided_graph)\n      with self.guided_graph.gradient_override_map({\'Relu\': \'GuidedRelu\'}):\n        # Import the graph def, and all the variables.\n        tf.import_graph_def(graph_def, name=\'\')\n        saver.restore(self.guided_sess, \'/tmp/guided_backprop_ckpt\')\n\n        imported_y = self.guided_graph.get_tensor_by_name(y.name)\n        imported_x = self.guided_graph.get_tensor_by_name(x.name)\n\n        self.guided_grads_node = tf.gradients(imported_y, imported_x)[0]\n\n  def GetMask(self, x_value, feed_dict = {}):\n    """"""Returns a GuidedBackprop mask.""""""\n    with self.guided_graph.as_default():\n      # Move all the feed dict tensor keys to refer to the same tensor on the\n      # new graph.\n      guided_feed_dict = {}\n      for tensor in feed_dict:\n        guided_feed_dict[tensor.name] = feed_dict[tensor]\n      guided_feed_dict[self.x.name] = [x_value]\n\n    return self.guided_sess.run(\n        self.guided_grads_node, feed_dict = guided_feed_dict)[0]\n'"
saliency/integrated_gradients.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to compute an IntegratedGradients SaliencyMask.""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom .base import GradientSaliency\n\nclass IntegratedGradients(GradientSaliency):\n  """"""A SaliencyMask class that implements the integrated gradients method.\n\n  https://arxiv.org/abs/1703.01365\n  """"""\n\n  def GetMask(self, x_value, feed_dict={}, x_baseline=None, x_steps=25):\n    """"""Returns a integrated gradients mask.\n\n    Args:\n      x_value: input ndarray.\n      x_baseline: Baseline value used in integration. Defaults to 0.\n      x_steps: Number of integrated steps between baseline and x.\n    """"""\n    if x_baseline is None:\n      x_baseline = np.zeros_like(x_value)\n\n    assert x_baseline.shape == x_value.shape\n\n    x_diff = x_value - x_baseline\n\n    total_gradients = np.zeros_like(x_value)\n\n    for alpha in np.linspace(0, 1, x_steps):\n      x_step = x_baseline + alpha * x_diff\n\n      total_gradients += super(IntegratedGradients, self).GetMask(\n          x_step, feed_dict)\n\n    return total_gradients * x_diff / x_steps\n'"
saliency/integrated_gradients_test.py,4,"b'# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport numpy as np\nimport tensorflow as tf\nfrom . import integrated_gradients\nfrom tensorflow.python.platform import googletest\n\n\nclass IntegratedGradientsTest(googletest.TestCase):\n  """"""\n  To run:\n  ""python -m saliency.integrated_gradients_test"" from the PAIR-code/saliency\n  directory.\n  """"""\n\n  def testIntegratedGradientsGetMask(self):\n    with tf.Graph().as_default() as graph:\n      x = tf.placeholder(shape=[None, 3], dtype=tf.float32)\n      y = 5 * x[:, 0] + x[:, 0] * x[:, 1] + tf.sin(x[:, 2])\n      with tf.Session() as sess:\n        # Calculate the value of `y` at the baseline.\n        x_baseline_val = np.array([[0.5, 0.8, 1.0]], dtype=np.float)\n        y_baseline_val = sess.run(y, feed_dict={x: x_baseline_val})\n\n        # Calculate the value of `y` at the input.\n        x_input_val = np.array([[1.0, 2.0, 3.0]], dtype=np.float)\n        y_input_val = sess.run(y, feed_dict={x: x_input_val})\n\n        # Due to mathematical properties of the integrated gradients,\n        # the expected IG value is equal to the difference between\n        # the `y` value at the input and the `y` value at the baseline.\n        expected_val = y_input_val[0] - y_baseline_val[0]\n\n        # Calculate the integrated gradients attribution of the input.\n        ig = integrated_gradients.IntegratedGradients(graph, sess, y[0], x)\n        mask = ig.GetMask(x_value=x_input_val[0], feed_dict={},\n                          x_baseline=x_baseline_val[0], x_steps=1000)\n\n        # Verify the result.\n        self.assertAlmostEqual(expected_val, mask.sum(), places=3)\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
saliency/occlusion.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Utilities to compute an Occlusion SaliencyMask.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .base import SaliencyMask\n\nclass Occlusion(SaliencyMask):\n  """"""A SaliencyMask class that computes saliency masks by occluding the image.\n\n  This method slides a window over the image and computes how that occlusion\n  affects the class score. When the class score decreases, this is positive\n  evidence for the class, otherwise it is negative evidence.\n  """"""\n\n  def __init__(self, graph, session, y, x):\n    super(Occlusion, self).__init__(graph, session, y, x)\n\n  def GetMask(self, x_value, feed_dict = {}, size = 15, value = 0):\n    """"""Returns an occlusion mask.""""""\n    occlusion_window = np.array([size, size, x_value.shape[2]])\n    occlusion_window.fill(value)\n\n    occlusion_scores = np.zeros_like(x_value)\n\n    feed_dict[self.x] = [x_value]\n    original_y_value = self.session.run(self.y, feed_dict=feed_dict)\n\n    for row in range(x_value.shape[0] - size):\n      for col in range(x_value.shape[1] - size):\n        x_occluded = np.array(x_value)\n\n        x_occluded[row:row+size, col:col+size, :] = occlusion_window\n\n        feed_dict[self.x] = [x_occluded]\n        y_value = self.session.run(self.y, feed_dict=feed_dict)\n\n        score_diff = original_y_value - y_value\n        occlusion_scores[row:row+size, col:col+size, :] += score_diff\n    return occlusion_scores\n'"
saliency/visualization.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\ndef VisualizeImageGrayscale(image_3d, percentile=99):\n  r""""""Returns a 3D tensor as a grayscale 2D tensor.\n\n  This method sums a 3D tensor across the absolute value of axis=2, and then\n  clips values at a given percentile.\n  """"""\n  image_2d = np.sum(np.abs(image_3d), axis=2)\n\n  vmax = np.percentile(image_2d, percentile)\n  vmin = np.min(image_2d)\n\n  return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)\n\ndef VisualizeImageDiverging(image_3d, percentile=99):\n  r""""""Returns a 3D tensor as a 2D tensor with positive and negative values.\n  """"""\n  image_2d = np.sum(image_3d, axis=2)\n\n  span = abs(np.percentile(image_2d, percentile))\n  vmin = -span\n  vmax = span\n\n  return np.clip((image_2d - vmin) / (vmax - vmin), -1, 1)\n'"
saliency/xrai.py,0,"b'""""""Implementation of XRAI algorithm from the paper:\nhttps://arxiv.org/abs/1906.02825\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n_logger = logging.getLogger(__name__)\n\nimport numpy as np\nfrom skimage import segmentation\nfrom skimage.morphology import dilation\nfrom skimage.morphology import disk\nfrom skimage.transform import resize\n\nfrom .base import SaliencyMask\nfrom .integrated_gradients import IntegratedGradients\n\n_FELZENSZWALB_SCALE_VALUES = [50, 100, 150, 250, 500, 1200]\n_FELZENSZWALB_SIGMA_VALUES = [0.8]\n_FELZENSZWALB_IM_RESIZE = (224, 224)\n_FELZENSZWALB_IM_VALUE_RANGE = [-1.0, 1.0]\n_FELZENSZWALB_MIN_SEGMENT_SIZE = 150\n\n\ndef _normalize_image(im, value_range, resize_shape=None):\n  """"""Normalize an image by resizing it and rescaling its values\n\n  Args:\n      im: Input image.\n      value_range: [min_value, max_value]\n      resize_shape: New image shape. Defaults to None.\n\n  Returns:\n      Resized and rescaled image.\n  """"""\n  im_max = np.max(im)\n  im_min = np.min(im)\n  im = (im - im_min) / (im_max - im_min)\n  im = im * (value_range[1] - value_range[0]) + value_range[0]\n  if resize_shape is not None:\n    im = resize(im,\n                resize_shape,\n                order=3,\n                mode=\'constant\',\n                preserve_range=True,\n                anti_aliasing=True)\n  return im\n\n\ndef _get_segments_felzenszwalb(im,\n                               resize_image=True,\n                               scale_range=None,\n                               dilation_rad=5):\n  """"""Compute image segments based on Felzenszwalb\'s algorithm.\n\n  Efficient graph-based image segmentation, Felzenszwalb, P.F.\n  and Huttenlocher, D.P. International Journal of Computer Vision, 2004\n\n  Args:\n    im: Input image.\n    resize_image: If True, the image is resized to 224,224 for the segmentation\n                  purposes. The resulting segments are rescaled back to match\n                  the original image size. It is done for consistency w.r.t.\n                  segmentation parameter range. Defaults to True.\n    scale_range:  Range of image values to use for segmentation algorithm.\n                  Segmentation algorithm is sensitive to the input image\n                  values, therefore we need to be consistent with the range\n                  for all images. If None is passed, the range is scaled to\n                  [-1.0, 1.0]. Defaults to None.\n    dilation_rad: Sets how much each segment is dilated to include edges,\n                  larger values cause more blobby segments, smaller values\n                  get sharper areas. Defaults to 5.\n  Returns:\n      masks: A list of boolean masks as np.ndarrays if size HxW for im size of\n             HxWxC.\n  """"""\n\n  # TODO (tolgab) Set this to default float range of 0.0 - 1.0 and tune\n  # parameters for that\n  if scale_range is None:\n    scale_range = _FELZENSZWALB_IM_VALUE_RANGE\n  # Normalize image value range and size\n  original_shape = im.shape[:2]\n  # TODO (tolgab) This resize is unnecessary with more intelligent param range\n  # selection\n  if resize_image:\n    im = _normalize_image(im, scale_range, _FELZENSZWALB_IM_RESIZE)\n  else:\n    im = _normalize_image(im, scale_range)\n  segs = []\n  for scale in _FELZENSZWALB_SCALE_VALUES:\n    for sigma in _FELZENSZWALB_SIGMA_VALUES:\n      seg = segmentation.felzenszwalb(im,\n                                      scale=scale,\n                                      sigma=sigma,\n                                      min_size=_FELZENSZWALB_MIN_SEGMENT_SIZE)\n      if resize_image:\n        seg = resize(seg,\n                     original_shape,\n                     order=0,\n                     preserve_range=True,\n                     mode=\'constant\',\n                     anti_aliasing=False).astype(np.int)\n      segs.append(seg)\n  masks = _unpack_segs_to_masks(segs)\n  if dilation_rad:\n    selem = disk(dilation_rad)\n    masks = [dilation(mask, selem=selem) for mask in masks]\n  return masks\n\n\ndef _attr_aggregation_max(attr, axis=-1):\n  return attr.max(axis=axis)\n\n\ndef _gain_density(mask1, attr, mask2=None):\n  # Compute the attr density over mask1. If mask2 is specified, compute density\n  # for mask1 \\ mask2\n  if mask2 is None:\n    added_mask = mask1\n  else:\n    added_mask = _get_diff_mask(mask1, mask2)\n  if not np.any(added_mask):\n    return -np.inf\n  else:\n    return attr[added_mask].mean()\n\n\ndef _get_diff_mask(add_mask, base_mask):\n  return np.logical_and(add_mask, np.logical_not(base_mask))\n\n\ndef _get_diff_cnt(add_mask, base_mask):\n  return np.sum(_get_diff_mask(add_mask, base_mask))\n\n\ndef _unpack_segs_to_masks(segs):\n  masks = []\n  for seg in segs:\n    for l in range(seg.min(), seg.max() + 1):\n      masks.append(seg == l)\n  return masks\n\n\nclass XRAIParameters(object):\n\n  def __init__(self,\n               steps=100,\n               area_threshold=1.0,\n               return_baseline_predictions=False,\n               return_ig_attributions=False,\n               return_xrai_segments=False,\n               flatten_xrai_segments=True,\n               algorithm=\'full\'):\n    # TODO(tolgab) add return_ig_for_every_step functionality\n\n    # Number of steps to use for calculating the Integrated Gradients\n    # attribution. The higher the number of steps the higher is the precision\n    # but lower the performance. (see also XRAIOutput.error).\n    self.steps = steps\n    # The fraction of the image area that XRAI should calculate the segments\n    # for. All segments that exceed that threshold will be merged into a single\n    # segment. The parameter is used to accelerate the XRAI computation if the\n    # caller is only interested in the top fraction of segments, e.g. 20%. The\n    # value should be in the [0.0, 1.0] range, where 1.0 means that all segments\n    # should be returned (slowest). Fast algorithm ignores this setting.\n    self.area_threshold = area_threshold\n    # TODO(tolgab) Enable return_baseline_predictions\n    # If set to True returns predictions for the baselines as float32 [B] array,\n    # where B is the number of baselines. (see XraiOutput.baseline_predictions).\n    # self.return_baseline_predictions = return_baseline_predictions\n    # If set to True, the XRAI output returns Integrated Gradients attributions\n    # for every baseline. (see XraiOutput.ig_attribution)\n    self.return_ig_attributions = return_ig_attributions\n    # If set to True the XRAI output returns XRAI segments in the order of their\n    # importance. This parameter works in conjunction with the\n    # flatten_xrai_sements parameter. (see also XraiOutput.segments)\n    self.return_xrai_segments = return_xrai_segments\n    # If set to True, the XRAI segments are returned as an integer array with\n    # the same dimensions as the input (excluding color channels). The elements\n    # of the array are set to values from the [1,N] range, where 1 is the most\n    # important segment and N is the least important segment. If\n    # flatten_xrai_sements is set to False, the segments are returned as a\n    # boolean array, where the first dimension has size N. The [0, ...] mask is\n    # the most important and the [N-1, ...] mask is the least important. This\n    # parameter has an effect only if return_xrai_segments is set to True.\n    self.flatten_xrai_segments = flatten_xrai_segments\n    # Specifies a flavor of the XRAI algorithm. full - executes slower but more\n    # precise XRAI algorithm. fast - executes faster but less precise XRAI\n    # algorithm.\n    self.algorithm = algorithm\n    # EXPERIMENTAL - Contains experimental parameters that may change in future.\n    self.experimental_params = {\'min_pixel_diff\': 50}\n\n\nclass XRAIOutput(object):\n\n  def __init__(self, attribution_mask):\n    # The saliency mask of individual input features. For an [HxWx3] image, the\n    # returned attribution is [H,W,1] float32 array. Where HxW are the\n    # dimensions of the image.\n    self.attribution_mask = attribution_mask\n    # Baselines that were used for IG calculation. The shape is [B,H,W,C], where\n    # B is the number of baselines, HxWxC are the image dimensions.\n    self.baselines = None\n    # TODO(tolgab) add return IG error functionality from XRAI API\n    # The average error of the IG attributions as a percentage. The error can be\n    # decreased by increasing the number of steps (see XraiParameters.steps).\n    # self.error = None\n    # TODO(tolgab) add return baseline predictions functionality from XRAI API\n    # Predictions for the baselines that were used for the calculation of IG\n    # attributions. The value is set only when\n    # XraiParameters.return_baseline_predictions is set to True.\n    # self.baseline_predictions = None\n    # IG attributions for individual baselines. The value is set only when\n    # XraiParameters.ig_attributions is set to True. For the dimensions of the\n    # output see XraiParameters.return_ig_for_every _step.\n    self.ig_attribution = None\n    # The result of the XRAI segmentation. The value is set only when\n    # XraiParameters.return_xrai_segments is set to True. For the dimensions of\n    # the output see XraiParameters.flatten_xrai_segments.\n    self.segments = None\n\n\nclass XRAI(SaliencyMask):\n\n  def __init__(self, graph, session, y, x):\n    super(XRAI, self).__init__(graph, session, y, x)\n    # Initialize integrated gradients.\n    self._integrated_gradients = IntegratedGradients(graph, session, y, x)\n\n  def _get_integrated_gradients(self, im, feed_dict, baselines, steps):\n    """""" Takes mean of attributions from all baselines\n    """"""\n    grads = []\n    for baseline in baselines:\n      grads.append(\n          self._integrated_gradients.GetMask(im,\n                                             feed_dict=feed_dict,\n                                             x_baseline=baseline,\n                                             x_steps=steps))\n    return grads\n\n  def _make_baselines(self, x_value, x_baselines):\n    # If baseline is not provided default to im min and max values\n    if x_baselines is None:\n      x_baselines = []\n      x_baselines.append(np.min(x_value) * np.ones_like(x_value))\n      x_baselines.append(np.max(x_value) * np.ones_like(x_value))\n    else:\n      for baseline in x_baselines:\n        if baseline.shape != x_value.shape:\n          raise ValueError(\n              ""Baseline size {} does not match input size {}"".format(\n                  baseline.shape, x_value.shape))\n    return x_baselines\n\n  def _predict(self, x):\n    raise NotImplementedError\n\n  def GetMask(self,\n              x_value,\n              feed_dict={},\n              baselines=None,\n              segments=None,\n              base_attribution=None,\n              extra_parameters=None):\n    """""" Applies XRAI method on an input image and returns the result saliency\n    heatmap.\n\n\n    Args:\n        x_value: input value, not batched.\n        feed_dict: feed dictionary to pass to the TF session.run() call.\n                   Defaults to {}.\n        baselines: a list of baselines to use for calculating\n                   Integrated Gradients attribution. Every baseline in\n                   the list should have the same dimensions as the\n                   input. If the value is not set then the algorithm\n                   will make the best effort to select default\n                   baselines. Defaults to None.\n        segments: the list of precalculated image segments that should\n                  be passed to XRAI. Each element of the list is an\n                  [N,M] boolean array, where NxM are the image\n                  dimensions. Each elemeent on the list contains exactly the\n                  mask that corresponds to one segment. If the value is None,\n                  Felzenszwalb\'s segmentation algorithm will be applied.\n                  Defaults to None.\n        base_attribution: an optional pre-calculated base attribution that XRAI\n                          should use. The shape of the parameter should match\n                          the shape of `x_value`. If the value is None, the\n                          method calculates Integrated Gradients attribution and\n                          uses it.\n        extra_parameters: an XRAIParameters object that specifies\n                          additional parameters for the XRAI saliency\n                          method. If it is None, an XRAIParameters object\n                          will be created with default parameters. See\n                          XRAIParameters for more details.\n\n    Raises:\n        ValueError: If algorithm type is unknown (not full or fast).\n                    If the shape of `base_attribution` dosn\'t match the shape of `x_value`.\n\n    Returns:\n        np.ndarray: A numpy array that contains the saliency heatmap.\n\n\n    TODO(tolgab) Add output_selector functionality from XRAI API doc\n    """"""\n    results = self.GetMaskWithDetails(x_value,\n                                      feed_dict=feed_dict,\n                                      baselines=baselines,\n                                      segments=segments,\n                                      base_attribution=base_attribution,\n                                      extra_parameters=extra_parameters)\n    return results.attribution_mask\n\n  def GetMaskWithDetails(self,\n                         x_value,\n                         feed_dict={},\n                         baselines=None,\n                         segments=None,\n                         base_attribution=None,\n                         extra_parameters=None):\n    """"""Applies XRAI method on an input image and returns the result saliency\n    heatmap along with other detailed information.\n\n\n    Args:\n        x_value: input value, not batched.\n        feed_dict: feed dictionary to pass to the TF session.run() call.\n                   Defaults to {}.\n        baselines: a list of baselines to use for calculating\n                   Integrated Gradients attribution. Every baseline in\n                   the list should have the same dimensions as the\n                   input. If the value is not set then the algorithm\n                   will make the best effort to select default\n                   baselines. Defaults to None.\n        segments: the list of precalculated image segments that should\n                  be passed to XRAI. Each element of the list is an\n                  [N,M] boolean array, where NxM are the image\n                  dimensions. Each elemeent on the list contains exactly the\n                  mask that corresponds to one segment. If the value is None,\n                  Felzenszwalb\'s segmentation algorithm will be applied.\n                  Defaults to None.\n        base_attribution: an optional pre-calculated base attribution that XRAI\n                          should use. The shape of the parameter should match\n                          the shape of `x_value`. If the value is None, the\n                          method calculates Integrated Gradients attribution and\n                          uses it.\n        extra_parameters: an XRAIParameters object that specifies\n                          additional parameters for the XRAI saliency\n                          method. If it is None, an XRAIParameters object\n                          will be created with default parameters. See\n                          XRAIParameters for more details.\n\n    Raises:\n        ValueError: If algorithm type is unknown (not full or fast).\n                    If the shape of `base_attribution` dosn\'t match the shape of `x_value`.\n\n    Returns:\n        XRAIOutput: an object that contains the output of the XRAI algorithm.\n\n    TODO(tolgab) Add output_selector functionality from XRAI API doc\n    """"""\n    if extra_parameters is None:\n      extra_parameters = XRAIParameters()\n\n    # Check the shape of base_attribution.\n    if base_attribution is not None:\n      if not isinstance(base_attribution, np.ndarray):\n        base_attribution = np.array(base_attribution)\n      if base_attribution.shape != x_value.shape:\n        raise ValueError(\n          \'The base attribution shape should be the same as the shape of \'\n          \'`x_value`. Expected {}, got {}\'.format(\n            x_value.shape, base_attribution.shape))\n\n    # Calculate IG attribution if not provided by the caller.\n    if base_attribution is None:\n      _logger.info(""Computing IG..."")\n      x_baselines = self._make_baselines(x_value, baselines)\n\n      attrs = self._get_integrated_gradients(x_value,\n                                             feed_dict=feed_dict,\n                                             baselines=x_baselines,\n                                             steps=extra_parameters.steps)\n      # Merge attributions from different baselines.\n      attr = np.mean(attrs, axis=0)\n    else:\n      x_baselines = None\n      attrs = base_attribution\n      attr = base_attribution\n\n    # Merge attribution channels for XRAI input\n    attr = _attr_aggregation_max(attr)\n\n    _logger.info(""Done with IG. Computing XRAI..."")\n    if segments is not None:\n      segs = segments\n    else:\n      segs = _get_segments_felzenszwalb(x_value)\n\n    if extra_parameters.algorithm == \'full\':\n      attr_map, attr_data = self._xrai(\n          attr=attr,\n          segs=segs,\n          area_perc_th=extra_parameters.area_threshold,\n          min_pixel_diff=extra_parameters.experimental_params[\'min_pixel_diff\'],\n          gain_fun=_gain_density,\n          integer_segments=extra_parameters.flatten_xrai_segments)\n    elif extra_parameters.algorithm == \'fast\':\n      attr_map, attr_data = self._xrai_fast(\n          attr=attr,\n          segs=segs,\n          min_pixel_diff=extra_parameters.experimental_params[\'min_pixel_diff\'],\n          gain_fun=_gain_density,\n          integer_segments=extra_parameters.flatten_xrai_segments)\n    else:\n      raise ValueError(\'Unknown algorithm type: {}\'.format(\n          extra_parameters.algorithm))\n\n    results = XRAIOutput(attr_map)\n    results.baselines = x_baselines\n    if extra_parameters.return_xrai_segments:\n      results.segments = attr_data\n    # TODO(tolgab) Enable return_baseline_predictions\n    # if extra_parameters.return_baseline_predictions:\n    #   baseline_predictions = []\n    #   for baseline in x_baselines:\n    #     baseline_predictions.append(self._predict(baseline))\n    #   results.baseline_predictions = baseline_predictions\n    if extra_parameters.return_ig_attributions:\n      results.ig_attribution = attrs\n    return results\n\n  @staticmethod\n  def _xrai(attr,\n            segs,\n            gain_fun=_gain_density,\n            area_perc_th=1.0,\n            min_pixel_diff=50,\n            integer_segments=True):\n    """"""Run XRAI saliency given attributions and segments.\n\n    Args:\n        attr: Source attributions for XRAI. XRAI attributions will be same size\n              as the input attr.\n        segs: Input segments as a list of boolean masks. XRAI uses these to\n              compute attribution sums.\n        gain_fun: The function that computes XRAI area attribution from source\n                  attributions. Defaults to _gain_density, which calculates the\n                  density of attributions in a mask.\n        area_perc_th: The saliency map is computed to cover area_perc_th of\n                      the image. Lower values will run faster, but produce\n                      uncomputed areas in the image that will be filled to\n                      satisfy completeness. Defaults to 1.0.\n        min_pixel_diff: Do not consider masks that have difference less than\n                        this number compared to the current mask. Set it to 1\n                        to remove masks that completely overlap with the\n                        current mask.\n        integer_segments: See XRAIParameters. Defaults to True.\n\n    Returns:\n        tuple: saliency heatmap and list of masks or an integer image with\n               area ranks depending on the parameter integer_segments.\n    """"""\n    output_attr = -np.inf * np.ones(shape=attr.shape, dtype=np.float)\n\n    n_masks = len(segs)\n    current_area_perc = 0.0\n    current_mask = np.zeros(attr.shape, dtype=bool)\n\n    masks_trace = []\n    remaining_masks = {ind: mask for ind, mask in enumerate(segs)}\n\n    added_masks_cnt = 1\n    # While the mask area is less than area_th and remaining_masks is not empty\n    while current_area_perc <= area_perc_th:\n      best_gain = -np.inf\n      best_key = None\n      remove_key_queue = []\n      for mask_key in remaining_masks:\n        mask = remaining_masks[mask_key]\n        # If mask does not add more than min_pixel_diff to current mask, remove\n        mask_pixel_diff = _get_diff_cnt(mask, current_mask)\n        if mask_pixel_diff < min_pixel_diff:\n          remove_key_queue.append(mask_key)\n          if _logger.isEnabledFor(logging.DEBUG):\n            _logger.debug(""Skipping mask with pixel difference: {:.3g},"".format(\n                mask_pixel_diff))\n          continue\n        gain = gain_fun(mask, attr, mask2=current_mask)\n        if gain > best_gain:\n          best_gain = gain\n          best_key = mask_key\n      for key in remove_key_queue:\n        del remaining_masks[key]\n      if len(remaining_masks) == 0:\n        break\n      added_mask = remaining_masks[best_key]\n      mask_diff = _get_diff_mask(added_mask, current_mask)\n      masks_trace.append((mask_diff, best_gain))\n\n      current_mask = np.logical_or(current_mask, added_mask)\n      current_area_perc = np.mean(current_mask)\n      output_attr[mask_diff] = best_gain\n      del remaining_masks[best_key]  # delete used key\n      if _logger.isEnabledFor(logging.DEBUG):\n        current_attr_sum = np.sum(attr[current_mask])\n        _logger.debug(\n            ""{} of {} masks added,""\n            ""attr_sum: {}, area: {:.3g}/{:.3g}, {} remaining masks"".format(\n                added_masks_cnt, n_masks, current_attr_sum, current_area_perc,\n                area_perc_th, len(remaining_masks)))\n      added_masks_cnt += 1\n\n    uncomputed_mask = output_attr == -np.inf\n    # Assign the uncomputed areas a value such that sum is same as ig\n    output_attr[uncomputed_mask] = gain_fun(uncomputed_mask, attr)\n    masks_trace = [v[0] for v in sorted(masks_trace, key=lambda x: -x[1])]\n    if np.any(uncomputed_mask):\n      masks_trace.append(uncomputed_mask)\n    if integer_segments:\n      attr_ranks = np.zeros(shape=attr.shape, dtype=np.int)\n      for i, mask in enumerate(masks_trace):\n        attr_ranks[mask] = i + 1\n      return output_attr, attr_ranks\n    else:\n      return output_attr, masks_trace\n\n  @staticmethod\n  def _xrai_fast(attr,\n                 segs,\n                 gain_fun=_gain_density,\n                 area_perc_th=1.0,\n                 min_pixel_diff=50,\n                 integer_segments=True):\n    """"""Run approximate XRAI saliency given attributions and segments. This\n       version does not consider mask overlap during importance ranking,\n       significantly speeding up the algorithm for less accurate results.\n\n    Args:\n        attr: Source attributions for XRAI. XRAI attributions will be same size\n              as the input attr.\n        segs: Input segments as a list of boolean masks. XRAI uses these to\n              compute attribution sums.\n        gain_fun: The function that computes XRAI area attribution from source\n                  attributions. Defaults to _gain_density, which calculates the\n                  density of attributions in a mask.\n        area_perc_th: This parameter is ignored. Fast version always computes\n                      to 1.0. It is left here for API compatibility.\n        min_pixel_diff: Do not consider masks that have difference less than\n                        this number compared to the current mask. Set it to 1\n                        to remove masks that completely overlap with the\n                        current mask.\n        integer_segments: See XRAIParameters. Defaults to True.\n\n    Returns:\n        tuple: saliency heatmap and list of masks or an integer image with\n               area ranks depending on the parameter integer_segments.\n    """"""\n    output_attr = -np.inf * np.ones(shape=attr.shape, dtype=np.float)\n\n    n_masks = len(segs)\n    current_mask = np.zeros(attr.shape, dtype=bool)\n\n    masks_trace = []\n\n    # Sort all masks based on gain, ignore overlaps\n    seg_attrs = [gain_fun(seg_mask, attr) for seg_mask in segs]\n    segs, seg_attrs = list(\n        zip(*sorted(zip(segs, seg_attrs), key=lambda x: -x[1])))\n\n    for i, added_mask in enumerate(segs):\n      mask_diff = _get_diff_mask(added_mask, current_mask)\n      # If mask does not add more than min_pixel_diff to current mask, skip\n      mask_pixel_diff = _get_diff_cnt(added_mask, current_mask)\n      if mask_pixel_diff < min_pixel_diff:\n        if _logger.isEnabledFor(logging.DEBUG):\n          _logger.debug(""Skipping mask with pixel difference: {:.3g},"".format(\n              mask_pixel_diff))\n        continue\n      mask_gain = gain_fun(mask_diff, attr)\n      masks_trace.append((mask_diff, mask_gain))\n      output_attr[mask_diff] = mask_gain\n      current_mask = np.logical_or(current_mask, added_mask)\n      if _logger.isEnabledFor(logging.DEBUG):\n        current_attr_sum = np.sum(attr[current_mask])\n        current_area_perc = np.mean(current_mask)\n        _logger.debug(""{} of {} masks processed,""\n                     ""attr_sum: {}, area: {:.3g}/{:.3g}"".format(\n                         i + 1, n_masks, current_attr_sum, current_area_perc,\n                         area_perc_th))\n    uncomputed_mask = output_attr == -np.inf\n    # Assign the uncomputed areas a value such that sum is same as ig\n    output_attr[uncomputed_mask] = gain_fun(uncomputed_mask, attr)\n    masks_trace = [v[0] for v in sorted(masks_trace, key=lambda x: -x[1])]\n    if np.any(uncomputed_mask):\n      masks_trace.append(uncomputed_mask)\n    if integer_segments:\n      attr_ranks = np.zeros(shape=attr.shape, dtype=np.int)\n      for i, mask in enumerate(masks_trace):\n        attr_ranks[mask] = i + 1\n      return output_attr, attr_ranks\n    else:\n      return output_attr, masks_trace\n'"
saliency/xrai_test.py,1,"b'import mock\nimport numpy as np\nimport skimage.draw as sk_draw\nimport tensorflow as tf\nfrom tensorflow.python.platform import googletest\nfrom . import xrai\nfrom .xrai import XRAI, XRAIParameters\n\nIMAGE_SIZE = 299\n\n\nclass XraiTest(googletest.TestCase):\n  """"""\n  To run:\n  ""python -m saliency.xrai_test"" from the PAIR-code/saliency directory.\n  """"""\n\n  def setUp(self):\n    # Mock IntegratedGradients.\n    self.mock_ig = mock.patch(__name__ + \'.xrai.IntegratedGradients\').start()\n    self.input_image = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3) * 0.3 + 0.5\n    self.ig_bl_1_attr = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3) - 0.4\n    self.ig_bl_2_attr = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3) - 0.4\n    self.mock_ig_instance = mock.Mock()\n    self.mock_ig_instance.GetMask.side_effect = [self.ig_bl_1_attr,\n                                                 self.ig_bl_2_attr,\n                                                 self.ig_bl_1_attr,\n                                                 self.ig_bl_2_attr]\n    self.mock_ig.return_value = self.mock_ig_instance\n\n    # IG attribution that XRAI should calculate internally.\n    self.ig_mask = np.asarray([self.ig_bl_1_attr, self.ig_bl_2_attr]).mean(\n        axis=0)\n    # A mocked XRAI object that uses neither the graph, session, x or y values.\n    self.xrai = XRAI(tf.Graph(), tf.Session(), y=np.array([0]), x=np.array([0]))\n\n  def tearDown(self):\n    self.mock_ig.stop()\n\n  def testXraiGetMaskFullFlat(self):\n    # Calculate XRAI attribution using GetMaskWithDetails(...) method.\n    xrai_params = XRAIParameters(return_xrai_segments=True,\n                                 flatten_xrai_segments=True,\n                                 algorithm=\'full\',\n                                 area_threshold=1.0,\n                                 return_ig_attributions=True)\n    xrai_out = self.xrai.GetMaskWithDetails(x_value=self.input_image,\n                                            extra_parameters=xrai_params)\n\n    # Verify the result.\n    self._assert_xrai_correctness(xrai_out, is_flatten_segments=True)\n\n    # Calculate XRAI attribution using GetMask(...) method.\n    heatmap = self.xrai.GetMask(x_value=self.input_image,\n                                extra_parameters=xrai_params)\n\n    # Verify that the heatmaps returned by GetMaskWithDetails(...) and\n    # GetMaskWithDetails(...) are the same.\n    self.assertTrue(np.array_equal(xrai_out.attribution_mask, heatmap))\n\n  def testXraiGetMaskFastFlat(self):\n    # Calculate XRAI attribution using GetMaskWithDetails(...) method.\n    xrai_params = XRAIParameters(return_xrai_segments=True,\n                                 flatten_xrai_segments=True,\n                                 algorithm=\'fast\',\n                                 return_ig_attributions=True)\n    xrai_out = self.xrai.GetMaskWithDetails(x_value=self.input_image,\n                                            extra_parameters=xrai_params)\n\n    # Verify the result.\n    self._assert_xrai_correctness(xrai_out, is_flatten_segments=True)\n\n    # Calculate XRAI attribution using GetMask(...) method.\n    heatmap = self.xrai.GetMask(x_value=self.input_image,\n                                extra_parameters=xrai_params)\n\n    # Verify that the heatmaps returned by GetMaskWithDetails(...) and\n    # GetMaskWithDetails(...) are the same.\n    self.assertTrue(np.array_equal(xrai_out.attribution_mask, heatmap))\n\n  def testXraiGetMaskFullNonFlat(self):\n    # Calculate XRAI attribution using GetMaskWithDetails(...) method.\n    xrai_params = XRAIParameters(return_xrai_segments=True,\n                                 flatten_xrai_segments=False,\n                                 area_threshold=1.0,\n                                 algorithm=\'full\',\n                                 return_ig_attributions=True)\n    xrai_out = self.xrai.GetMaskWithDetails(x_value=self.input_image,\n                                            extra_parameters=xrai_params)\n\n    # Verify the result.\n    self._assert_xrai_correctness(xrai_out, is_flatten_segments=False)\n\n    # Calculate XRAI attribution using GetMask(...) method.\n    heatmap = self.xrai.GetMask(x_value=self.input_image,\n                                extra_parameters=xrai_params)\n\n    # Verify that the heatmaps returned by GetMaskWithDetails(...) and\n    # GetMaskWithDetails(...) are the same.\n    self.assertTrue(np.array_equal(xrai_out.attribution_mask, heatmap))\n\n  def testXraiGetMaskFastNonFlat(self):\n    # Calculate XRAI attribution using GetMaskWithDetails(...) method.\n    xrai_params = XRAIParameters(return_xrai_segments=True,\n                                 flatten_xrai_segments=False,\n                                 algorithm=\'fast\',\n                                 return_ig_attributions=True)\n    xrai_out = self.xrai.GetMaskWithDetails(x_value=self.input_image,\n                                            extra_parameters=xrai_params)\n    # Verify the result.\n    self._assert_xrai_correctness(xrai_out, is_flatten_segments=False)\n\n    # Calculate XRAI attribution using GetMask(...) method.\n    heatmap = self.xrai.GetMask(x_value=self.input_image,\n                                extra_parameters=xrai_params)\n\n    # Verify that the heatmaps returned by GetMaskWithDetails(...) and\n    # GetMaskWithDetails(...) are the same.\n    self.assertTrue(np.array_equal(xrai_out.attribution_mask, heatmap))\n\n  def testCustomSegments(self):\n    # Create the first segment.\n    rec_1 = sk_draw.rectangle(start=(10, 10), end=(30, 30),\n                              shape=(IMAGE_SIZE, IMAGE_SIZE))\n    seg_1 = np.zeros(shape=(IMAGE_SIZE, IMAGE_SIZE), dtype=np.bool)\n    seg_1[tuple(rec_1)] = True\n\n    # Create the second segment.\n    rec_2 = sk_draw.rectangle(start=(60, 60), end=(100, 100),\n                              shape=(IMAGE_SIZE, IMAGE_SIZE))\n    seg_2 = np.zeros(shape=(IMAGE_SIZE, IMAGE_SIZE), dtype=np.bool)\n    seg_2[tuple(rec_2)] = True\n\n    # Calculate the XRAI attribution.\n    xrai_params = XRAIParameters(return_xrai_segments=True,\n                                 flatten_xrai_segments=False,\n                                 area_threshold=1.0,\n                                 return_ig_attributions=True)\n    xrai_out = self.xrai.GetMaskWithDetails(x_value=self.input_image,\n                                            extra_parameters=xrai_params,\n                                            segments=[seg_1, seg_2])\n\n    # Verify correctness of the attribution.\n    self._assert_xrai_correctness(xrai_out, is_flatten_segments=False)\n\n    # Verify that the segments are ordered correctly, i.e. the segment with\n    # higher attribution comes first.\n    seg1_attr = self.ig_mask[seg_1].max(axis=1).mean()\n    seg2_attr = self.ig_mask[seg_2].max(axis=1).mean()\n    if seg1_attr >= seg2_attr:\n      self.assertTrue(np.array_equal(seg_1, xrai_out.segments[0]),\n                      msg=\'The segments might be ordered incorrectly.\')\n    else:\n      self.assertTrue(np.array_equal(seg_2, xrai_out.segments[0]),\n                      msg=\'The segments might be ordered incorrectly.\')\n\n    # Three segments are expected. The last segment should include all pixels\n    # that weren\'t included in the first two segments.\n    self.assertEqual(3, len(xrai_out.segments),\n                     \'Unexpected the number of returned segments.\')\n\n  def testBaselines(self):\n    """"""Tests that a client can pass an arbitrary baseline values; and that\n       these baselines are actually used for calculating the Integrated\n       Gradient masks.\n    """"""\n    # Create baselines and pass them to XRAI.GetMask(...).\n    baseline_1 = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3)\n    baseline_2 = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3)\n    self.xrai.GetMask(x_value=self.input_image,\n                      baselines=[baseline_1, baseline_2])\n\n    # Verify that the XRAI object called Integrated Gradients with the baselines\n    # that were passed to xrai.GetMask(...).\n    calls = self.mock_ig_instance.method_calls\n    self.assertEqual(2, len(calls),\n                     \'There should be only two calls to IG Getmask()\')\n    self.assertTrue(np.array_equal(baseline_1, calls[0][2][\'x_baseline\']),\n                    msg=\'IG was called with incorrect baseline.\')\n    self.assertTrue(np.array_equal(baseline_2, calls[1][2][\'x_baseline\']),\n                    msg=\'IG was called with incorrect baseline.\')\n\n  def testBaseAttribution(self):\n    base_attribution = np.random.rand(IMAGE_SIZE, IMAGE_SIZE, 3)\n\n    # Calculate XRAI attribution using GetMask(...) method.\n    heatmap = self.xrai.GetMask(x_value=self.input_image,\n                                base_attribution=base_attribution)\n    # Make sure that the GetMask() method doesn\'t return the same attribution\n    # that was passed to it.\n    self.assertFalse(np.array_equal(base_attribution.max(axis=2), heatmap))\n    # The sum of XRAI attribution should be equal to the sum of the underlying\n    # base attribution. Internally the attribution that is used by XRAI is\n    # the max over color channels.\n    self.assertAlmostEqual(base_attribution.max(axis=2).sum(), heatmap.sum())\n\n    # Verify that the XRAI object didn\'t called Integrated Gradients.\n    calls = self.mock_ig_instance.method_calls\n    self.assertEqual(0, len(calls),\n                     \'XRAI should not call Integrated Gradients.\')\n\n  def testBaseAttributionMismatchedShape(self):\n    # Create base_attribution that shape doesn\'t match the input.\n    base_attribution = np.random.rand(IMAGE_SIZE, IMAGE_SIZE + 1, 3)\n\n    # Verify that the exception was raised.\n    with self.assertRaisesRegexp(ValueError, \'The base attribution shape \'\n                                             \'should\'):\n      # Calling GetMask(...) should result in exception.\n      self.xrai.GetMask(x_value=self.input_image,\n                        base_attribution=base_attribution)\n\n  def _assert_xrai_correctness(self, xrai_out, is_flatten_segments):\n    """"""Performs general XRAIOutput verification that is applicable for all\n       XRAI results.\n    """"""\n    xrai_attribution_mask = xrai_out.attribution_mask\n    xrai_segments = xrai_out.segments\n\n    # Check completeness with respect to IG attribution.\n    self.assertAlmostEqual(self.ig_mask.max(axis=2).sum(),\n                           xrai_attribution_mask.sum(),\n                           msg=\'The sum of IG attribution (max along the color \'\n                               \'axis) should be equal to the sum of XRAI \'\n                               \'attribution.\')\n\n    # Check that the returned IG values are the same as returned by IG.\n    self.assertTrue(\n        np.array_equal(self.ig_bl_1_attr, xrai_out.ig_attribution[0]),\n        msg=\'IG values returned by IG and returned to the client do not match.\')\n    self.assertTrue(\n        np.array_equal(self.ig_bl_2_attr, xrai_out.ig_attribution[1]),\n        msg=\'IG values returned by IG and returned to the client do not match.\')\n\n    # If the result is flattened, verify that the first segment is assigned\n    # value 1. Convert the flattened integer segments to individual boolean\n    # segments.\n    segment_masks = []\n    if is_flatten_segments:\n      first_segment_id = xrai_segments.min()\n      last_segment_id = xrai_segments.max()\n      self.assertEqual(1, first_segment_id, msg=\'The first segment should\'\n                                                \' be assigned value ""1"".\')\n      for segment_id in range(first_segment_id, last_segment_id + 1):\n        segment_masks.append(xrai_segments == segment_id)\n    else:\n      segment_masks = xrai_segments\n\n    # Verify that 1) the segments are ordered according to their attribution;\n    # 2) that every segment preserves the completeness properties;\n    # 3) all pixels within a single segment have the same attribution.\n    prev_seg_attr = np.inf\n    for i, segment_mask in enumerate(segment_masks):\n      segment_id = i + 1\n      segment_attr = xrai_attribution_mask[segment_mask]\n      self.assertGreater(segment_mask.sum(), 0,\n                         msg=\'Segment {} of {} has zero area.\'.format(\n                             segment_id, len(segment_masks)))\n      self.assertEqual(segment_attr.min(), segment_attr.max(),\n                       \'All attribution values within a single segment should \'\n                       \'be equal.\')\n      segment_attr = segment_attr.max()\n      self.assertAlmostEqual(self.ig_mask.max(axis=2)[segment_mask].sum(),\n                             xrai_attribution_mask[segment_mask].sum(),\n                             msg=\'The sum of the XRAI attribution within a \'\n                                 \'segment should be equal to the sum of IG \'\n                                 \'attribution within the same segment.\')\n      # The last segment may have attribution higher than the previous one\n      # because it includes all pixels that weren\'t included by the previous\n      # segments.\n      if i < len(segment_masks) - 1:\n        self.assertLessEqual(segment_attr, prev_seg_attr,\n                             \'Pixel attributions of a segment with higher id \'\n                             \'should be lower than pixel attributions of a \'\n                             \'segment with a lower id. Segment {}\'.format(\n                                 segment_id))\n      prev_seg_attr = segment_attr\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
