file_path,api_count,code
convert_weights.py,12,"b""# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nimport yolo_v3\nimport yolo_v3_tiny\n\nfrom utils import load_coco_names, load_weights\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    'class_names', 'coco.names', 'File with class names')\ntf.app.flags.DEFINE_string(\n    'weights_file', 'yolov3.weights', 'Binary file with detector weights')\ntf.app.flags.DEFINE_string(\n    'data_format', 'NCHW', 'Data format: NCHW (gpu only) / NHWC')\ntf.app.flags.DEFINE_bool(\n    'tiny', False, 'Use tiny version of YOLOv3')\ntf.app.flags.DEFINE_string(\n    'ckpt_file', './saved_model/model.ckpt', 'Chceckpoint file')\n\n\ndef main(argv=None):\n    if FLAGS.tiny:\n        model = yolo_v3_tiny.yolo_v3_tiny\n    else:\n        model = yolo_v3.yolo_v3\n\n    classes = load_coco_names(FLAGS.class_names)\n\n    # placeholder for detector inputs\n    # any size > 320 will work here\n    inputs = tf.placeholder(tf.float32, [1, 416, 416, 3])\n\n    with tf.variable_scope('detector'):\n        detections = model(inputs, len(classes),\n                           data_format=FLAGS.data_format)\n        load_ops = load_weights(tf.global_variables(\n            scope='detector'), FLAGS.weights_file)\n\n    saver = tf.train.Saver(tf.global_variables(scope='detector'))\n\n    with tf.Session() as sess:\n        sess.run(load_ops)\n\n        save_path = saver.save(sess, save_path=FLAGS.ckpt_file)\n        print('Model saved in path: {}'.format(save_path))\n\n\nif __name__ == '__main__':\n    tf.app.run()\n"""
convert_weights_pb.py,12,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nimport yolo_v3\nimport yolo_v3_tiny\nfrom PIL import Image, ImageDraw\n\nfrom utils import load_weights, load_coco_names, detections_boxes, freeze_graph\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'class_names\', \'coco.names\', \'File with class names\')\ntf.app.flags.DEFINE_string(\n    \'weights_file\', \'yolov3.weights\', \'Binary file with detector weights\')\ntf.app.flags.DEFINE_string(\n    \'data_format\', \'NCHW\', \'Data format: NCHW (gpu only) / NHWC\')\ntf.app.flags.DEFINE_string(\n    \'output_graph\', \'frozen_darknet_yolov3_model.pb\', \'Frozen tensorflow protobuf model output path\')\n\ntf.app.flags.DEFINE_bool(\n    \'tiny\', False, \'Use tiny version of YOLOv3\')\ntf.app.flags.DEFINE_integer(\n    \'size\', 416, \'Image size\')\n\n\n\ndef main(argv=None):\n    if FLAGS.tiny:\n        model = yolo_v3_tiny.yolo_v3_tiny\n    else:\n        model = yolo_v3.yolo_v3\n\n    classes = load_coco_names(FLAGS.class_names)\n\n    # placeholder for detector inputs\n    inputs = tf.placeholder(tf.float32, [None, FLAGS.size, FLAGS.size, 3], ""inputs"")\n\n    with tf.variable_scope(\'detector\'):\n        detections = model(inputs, len(classes), data_format=FLAGS.data_format)\n        load_ops = load_weights(tf.global_variables(scope=\'detector\'), FLAGS.weights_file)\n\n    #detect_1.shape = (?, 507, 85)\n    #detect_2.shape = (?, 2028, 85)\n    #detect_3.shape = (?, 8112, 85)\n    #detections.shape = (?, 10647, 85)\n    #detections = Tensor(""detector/yolo-v3/detections:0"", shape=(?, 10647, 85), dtype=float32)\n    print(""detections.shape ="", detections.shape)\n    print(detections)\n    print(detections.name)\n    # Sets the output nodes in the current session\n    boxes = detections_boxes(detections)\n\n    with tf.Session() as sess:\n        sess.run(load_ops)\n        freeze_graph(sess, FLAGS.output_graph, FLAGS.tiny)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
demo.py,19,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nimport time\n\nimport yolo_v3\nimport yolo_v3_tiny\n\nfrom utils import load_coco_names, draw_boxes, get_boxes_and_inputs, get_boxes_and_inputs_pb, non_max_suppression, load_graph, letter_box_image\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'input_img\', \'\', \'Input image\')\ntf.app.flags.DEFINE_string(\'output_img\', \'\', \'Output image\')\ntf.app.flags.DEFINE_string(\'class_names\', \'coco.names\', \'File with class names\')\ntf.app.flags.DEFINE_string(\'weights_file\', \'yolov3.weights\', \'Binary file with detector weights\')\ntf.app.flags.DEFINE_string(\'data_format\', \'NCHW\', \'Data format: NCHW (gpu only) / NHWC\')\ntf.app.flags.DEFINE_string(\'ckpt_file\', \'./saved_model/model.ckpt\', \'Checkpoint file\')\ntf.app.flags.DEFINE_string(\'frozen_model\', \'\', \'Frozen tensorflow protobuf model\')\ntf.app.flags.DEFINE_bool(\'tiny\', False, \'Use tiny version of YOLOv3\')\n\ntf.app.flags.DEFINE_integer(\'size\', 416, \'Image size\')\n\ntf.app.flags.DEFINE_float(\'conf_threshold\', 0.5, \'Confidence threshold\')\ntf.app.flags.DEFINE_float(\'iou_threshold\', 0.4, \'IoU threshold\')\n\ntf.app.flags.DEFINE_float(\'gpu_memory_fraction\', 1.0, \'Gpu memory fraction to use\')\n\ndef main(argv=None):\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n\n    config = tf.ConfigProto(\n        gpu_options=gpu_options,\n        log_device_placement=False,\n    )\n\n    img = Image.open(FLAGS.input_img)\n    img_resized = letter_box_image(img, FLAGS.size, FLAGS.size, 128)\n    img_resized = img_resized.astype(np.float32)\n    classes = load_coco_names(FLAGS.class_names)\n\n    if FLAGS.frozen_model:\n\n        t0 = time.time()\n        frozenGraph = load_graph(FLAGS.frozen_model)\n        print(""Loaded graph in {:.2f}s"".format(time.time()-t0))\n\n        boxes, inputs = get_boxes_and_inputs_pb(frozenGraph)\n\n        with tf.Session(graph=frozenGraph, config=config) as sess:\n            t0 = time.time()\n            detected_boxes = sess.run(boxes, feed_dict={inputs: [img_resized]})\n\n    else:\n        if FLAGS.tiny:\n            model = yolo_v3_tiny.yolo_v3_tiny\n        else:\n            model = yolo_v3.yolo_v3\n\n        boxes, inputs = get_boxes_and_inputs(model, len(classes), FLAGS.size, FLAGS.data_format)\n\n        saver = tf.train.Saver(var_list=tf.global_variables(scope=\'detector\'))\n\n        with tf.Session(config=config) as sess:\n            t0 = time.time()\n            saver.restore(sess, FLAGS.ckpt_file)\n            print(\'Model restored in {:.2f}s\'.format(time.time()-t0))\n\n            t0 = time.time()\n            detected_boxes = sess.run(boxes, feed_dict={inputs: [img_resized]})\n\n    filtered_boxes = non_max_suppression(detected_boxes,\n                                         confidence_threshold=FLAGS.conf_threshold,\n                                         iou_threshold=FLAGS.iou_threshold)\n    print(""Predictions found in {:.2f}s"".format(time.time() - t0))\n\n    draw_boxes(filtered_boxes, img, classes, (FLAGS.size, FLAGS.size), True)\n\n    img.save(FLAGS.output_img)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
openvino_tiny-yolov3_MultiStick_test.py,0,"b'import sys, os, cv2, time, heapq, argparse\nimport numpy as np, math\ntry:\n    from armv7l.openvino.inference_engine import IENetwork, IEPlugin\nexcept:\n    from openvino.inference_engine import IENetwork, IEPlugin\nimport multiprocessing as mp\nfrom time import sleep\nimport threading\n\nyolo_scale_13 = 13\nyolo_scale_26 = 26\nyolo_scale_52 = 52\n\nclasses = 80\ncoords = 4\nnum = 3\nanchors = [10,14, 23,27, 37,58, 81,82, 135,169, 344,319]\n\nLABELS = (""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"",\n          ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"",\n          ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"",\n          ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"",\n          ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"",\n          ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"",\n          ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"",\n          ""baseball glove"", ""skateboard"", ""surfboard"",""tennis racket"", ""bottle"",\n          ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"",\n          ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n          ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"",\n          ""cake"", ""chair"", ""sofa"", ""pottedplant"", ""bed"",\n          ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n          ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"",\n          ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"",\n          ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush"")\n\nlabel_text_color = (255, 255, 255)\nlabel_background_color = (125, 175, 75)\nbox_color = (255, 128, 0)\nbox_thickness = 1\n\nprocesses = []\n\nfps = """"\ndetectfps = """"\nframecount = 0\ndetectframecount = 0\ntime1 = 0\ntime2 = 0\nlastresults = None\n\n\n\ndef EntryIndex(side, lcoords, lclasses, location, entry):\n    n = int(location / (side * side))\n    loc = location % (side * side)\n    return int(n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc)\n\n\nclass DetectionObject():\n    xmin = 0\n    ymin = 0\n    xmax = 0\n    ymax = 0\n    class_id = 0\n    confidence = 0.0\n\n    def __init__(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n        self.xmin = int((x - w / 2) * w_scale)\n        self.ymin = int((y - h / 2) * h_scale)\n        self.xmax = int(self.xmin + w * w_scale)\n        self.ymax = int(self.ymin + h * h_scale)\n        self.class_id = class_id\n        self.confidence = confidence\n\n\ndef IntersectionOverUnion(box_1, box_2):\n    width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)\n    height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)\n    area_of_overlap = 0.0\n    if (width_of_overlap_area < 0.0 or height_of_overlap_area < 0.0):\n        area_of_overlap = 0.0\n    else:\n        area_of_overlap = width_of_overlap_area * height_of_overlap_area\n    box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin)\n    box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin)\n    area_of_union = box_1_area + box_2_area - area_of_overlap\n    retval = 0.0\n    if area_of_union <= 0.0:\n        retval = 0.0\n    else:\n        retval = (area_of_overlap / area_of_union)\n    return retval\n\n\ndef ParseYOLOV3Output(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects):\n\n    out_blob_h = blob.shape[2]\n    out_blob_w = blob.shape[3]\n\n    side = out_blob_h\n    anchor_offset = 0\n\n    if side == yolo_scale_13:\n        anchor_offset = 2 * 3\n    elif side == yolo_scale_26:\n        anchor_offset = 2 * 0\n\n    side_square = side * side\n    output_blob = blob.flatten()\n\n    for i in range(side_square):\n        row = int(i / side)\n        col = int(i % side)\n        for n in range(num):\n            obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords)\n            box_index = EntryIndex(side, coords, classes, n * side * side + i, 0)\n            scale = output_blob[obj_index]\n            if (scale < threshold):\n                continue\n            x = (col + output_blob[box_index + 0 * side_square]) / side * resized_im_w\n            y = (row + output_blob[box_index + 1 * side_square]) / side * resized_im_h\n            height = math.exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1]\n            width = math.exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n]\n            for j in range(classes):\n                class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j)\n                prob = scale * output_blob[class_index]\n                if prob < threshold:\n                    continue\n                obj = DetectionObject(x, y, height, width, j, prob, (original_im_h / resized_im_h), (original_im_w / resized_im_w))\n                objects.append(obj)\n    return objects\n\n\ndef camThread(LABELS, results, frameBuffer, camera_width, camera_height, vidfps):\n    global fps\n    global detectfps\n    global lastresults\n    global framecount\n    global detectframecount\n    global time1\n    global time2\n    global cam\n    global window_name\n\n    cam = cv2.VideoCapture(0)\n    if cam.isOpened() != True:\n        print(""USB Camera Open Error!!!"")\n        sys.exit(0)\n    cam.set(cv2.CAP_PROP_FPS, vidfps)\n    cam.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n    cam.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n    window_name = ""USB Camera""\n    wait_key_time = 1\n\n    #cam = cv2.VideoCapture(""data/input/testvideo4.mp4"")\n    #camera_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n    #camera_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    #frame_count = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n    #window_name = ""Movie File""\n    #wait_key_time = int(1000 / vidfps)\n\n    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n\n    while True:\n        t1 = time.perf_counter()\n\n        # USB Camera Stream Read\n        s, color_image = cam.read()\n        if not s:\n            continue\n        if frameBuffer.full():\n            frameBuffer.get()\n\n        height = color_image.shape[0]\n        width = color_image.shape[1]\n        frameBuffer.put(color_image.copy())\n\n        if not results.empty():\n            objects = results.get(False)\n            detectframecount += 1\n\n            for obj in objects:\n                if obj.confidence < 0.2:\n                    continue\n                label = obj.class_id\n                confidence = obj.confidence\n                if confidence > 0.2:\n                    label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n                    cv2.rectangle(color_image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n                    cv2.putText(color_image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n            lastresults = objects\n        else:\n            if not isinstance(lastresults, type(None)):\n                for obj in lastresults:\n                    if obj.confidence < 0.2:\n                        continue\n                    label = obj.class_id\n                    confidence = obj.confidence\n                    if confidence > 0.2:\n                        label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n                        cv2.rectangle(color_image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n                        cv2.putText(color_image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n\n        cv2.putText(color_image, fps,       (width-170,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n        cv2.putText(color_image, detectfps, (width-170,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n        cv2.imshow(window_name, cv2.resize(color_image, (width, height)))\n\n        if cv2.waitKey(wait_key_time)&0xFF == ord(\'q\'):\n            sys.exit(0)\n\n        ## Print FPS\n        framecount += 1\n        if framecount >= 15:\n            fps       = ""(Playback) {:.1f} FPS"".format(time1/15)\n            detectfps = ""(Detection) {:.1f} FPS"".format(detectframecount/time2)\n            framecount = 0\n            detectframecount = 0\n            time1 = 0\n            time2 = 0\n        t2 = time.perf_counter()\n        elapsedTime = t2-t1\n        time1 += 1/elapsedTime\n        time2 += elapsedTime\n\n\n# l = Search list\n# x = Search target value\ndef searchlist(l, x, notfoundvalue=-1):\n    if x in l:\n        return l.index(x)\n    else:\n        return notfoundvalue\n\n\ndef async_infer(ncsworker):\n\n    #ncsworker.skip_frame_measurement()\n\n    while True:\n        ncsworker.predict_async()\n\n\nclass NcsWorker(object):\n\n    def __init__(self, devid, frameBuffer, results, camera_width, camera_height, number_of_ncs, vidfps):\n        self.devid = devid\n        self.frameBuffer = frameBuffer\n        self.model_xml = ""./lrmodels/tiny-YoloV3/FP16/frozen_tiny_yolo_v3.xml""\n        self.model_bin = ""./lrmodels/tiny-YoloV3/FP16/frozen_tiny_yolo_v3.bin""\n        self.camera_width = camera_width\n        self.camera_height = camera_height\n        self.m_input_size = 416\n        self.threshould = 0.4\n        self.num_requests = 4\n        self.inferred_request = [0] * self.num_requests\n        self.heap_request = []\n        self.inferred_cnt = 0\n        self.plugin = IEPlugin(device=""MYRIAD"")\n        self.net = IENetwork(model=self.model_xml, weights=self.model_bin)\n        self.input_blob = next(iter(self.net.inputs))\n        self.exec_net = self.plugin.load(network=self.net, num_requests=self.num_requests)\n        self.results = results\n        self.number_of_ncs = number_of_ncs\n        self.predict_async_time = 800\n        self.skip_frame = 0\n        self.roop_frame = 0\n        self.vidfps = vidfps\n        self.new_w = int(camera_width * self.m_input_size/camera_width)\n        self.new_h = int(camera_height * self.m_input_size/camera_height)\n\n    def image_preprocessing(self, color_image):\n        resized_image = cv2.resize(color_image, (self.new_w, self.new_h), interpolation = cv2.INTER_CUBIC)\n        canvas = np.full((self.m_input_size, self.m_input_size, 3), 128)\n        canvas[(self.m_input_size-self.new_h)//2:(self.m_input_size-self.new_h)//2 + self.new_h,(self.m_input_size-self.new_w)//2:(self.m_input_size-self.new_w)//2 + self.new_w,  :] = resized_image\n        prepimg = canvas\n        prepimg = prepimg[np.newaxis, :, :, :]     # Batch size axis add\n        prepimg = prepimg.transpose((0, 3, 1, 2))  # NHWC to NCHW\n        return prepimg\n\n\n    def skip_frame_measurement(self):\n            surplustime_per_second = (1000 - self.predict_async_time)\n            if surplustime_per_second > 0.0:\n                frame_per_millisecond = (1000 / self.vidfps)\n                total_skip_frame = surplustime_per_second / frame_per_millisecond\n                self.skip_frame = int(total_skip_frame / self.num_requests)\n            else:\n                self.skip_frame = 0\n\n\n    def predict_async(self):\n        try:\n\n            if self.frameBuffer.empty():\n                return\n\n            self.roop_frame += 1\n            if self.roop_frame <= self.skip_frame:\n               self.frameBuffer.get()\n               return\n            self.roop_frame = 0\n\n            prepimg = self.image_preprocessing(self.frameBuffer.get())\n            reqnum = searchlist(self.inferred_request, 0)\n\n            if reqnum > -1:\n                self.exec_net.start_async(request_id=reqnum, inputs={self.input_blob: prepimg})\n                self.inferred_request[reqnum] = 1\n                self.inferred_cnt += 1\n                if self.inferred_cnt == sys.maxsize:\n                    self.inferred_request = [0] * self.num_requests\n                    self.heap_request = []\n                    self.inferred_cnt = 0\n                heapq.heappush(self.heap_request, (self.inferred_cnt, reqnum))\n\n            cnt, dev = heapq.heappop(self.heap_request)\n\n            if self.exec_net.requests[dev].wait(0) == 0:\n                self.exec_net.requests[dev].wait(-1)\n\n                objects = []\n                outputs = self.exec_net.requests[dev].outputs\n                for output in outputs.values():\n                    objects = ParseYOLOV3Output(output, self.new_h, self.new_w, self.camera_height, self.camera_width, self.threshould, objects)\n\n                objlen = len(objects)\n                for i in range(objlen):\n                    if (objects[i].confidence == 0.0):\n                        continue\n                    for j in range(i + 1, objlen):\n                        if (IntersectionOverUnion(objects[i], objects[j]) >= 0.4):\n                            if objects[i].confidence < objects[j].confidence:\n                                objects[i], objects[j] = objects[j], objects[i]\n                            objects[j].confidence = 0.0\n\n                self.results.put(objects)\n                self.inferred_request[dev] = 0\n            else:\n                heapq.heappush(self.heap_request, (cnt, dev))\n        except:\n            import traceback\n            traceback.print_exc()\n\n\ndef inferencer(results, frameBuffer, number_of_ncs, camera_width, camera_height, vidfps):\n\n    # Init infer threads\n    threads = []\n    for devid in range(number_of_ncs):\n        thworker = threading.Thread(target=async_infer, args=(NcsWorker(devid, frameBuffer, results, camera_width, camera_height, number_of_ncs, vidfps),))\n        thworker.start()\n        threads.append(thworker)\n\n    for th in threads:\n        th.join()\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-numncs\',\'--numberofncs\',dest=\'number_of_ncs\',type=int,default=1,help=\'Number of NCS. (Default=1)\')\n    args = parser.parse_args()\n\n    number_of_ncs = args.number_of_ncs\n    camera_width = int(cv2.VideoCapture(0).get(cv2.CAP_PROP_FRAME_WIDTH))\n    camera_height = int(cv2.VideoCapture(0).get(cv2.CAP_PROP_FRAME_HEIGHT))\n    vidfps = 30\n\n    try:\n\n        mp.set_start_method(\'forkserver\')\n        frameBuffer = mp.Queue(10)\n        results = mp.Queue()\n\n        # Start detection MultiStick\n        # Activation of inferencer\n        p = mp.Process(target=inferencer, args=(results, frameBuffer, number_of_ncs, camera_width, camera_height, vidfps), daemon=True)\n        p.start()\n        processes.append(p)\n\n        sleep(number_of_ncs * 7)\n\n        # Start streaming\n        p = mp.Process(target=camThread, args=(LABELS, results, frameBuffer, camera_width, camera_height, vidfps), daemon=True)\n        p.start()\n        processes.append(p)\n\n        while True:\n            sleep(1)\n\n    except:\n        import traceback\n        traceback.print_exc()\n    finally:\n        for p in range(len(processes)):\n            processes[p].terminate()\n\n        print(""\\n\\nFinished\\n\\n"")\n'"
openvino_tiny-yolov3_test.py,0,"b'import sys, os, cv2, time\nimport numpy as np, math\nfrom argparse import ArgumentParser\ntry:\n    from armv7l.openvino.inference_engine import IENetwork, IEPlugin\nexcept:\n    from openvino.inference_engine import IENetwork, IEPlugin\n\nm_input_size = 416\n\nyolo_scale_13 = 13\nyolo_scale_26 = 26\nyolo_scale_52 = 52\n\nclasses = 80\ncoords = 4\nnum = 3\nanchors = [10,14, 23,27, 37,58, 81,82, 135,169, 344,319]\n\nLABELS = (""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"",\n          ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"",\n          ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"",\n          ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"",\n          ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"",\n          ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"",\n          ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"",\n          ""baseball glove"", ""skateboard"", ""surfboard"",""tennis racket"", ""bottle"",\n          ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"",\n          ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n          ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"",\n          ""cake"", ""chair"", ""sofa"", ""pottedplant"", ""bed"",\n          ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n          ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"",\n          ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"",\n          ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush"")\n\nlabel_text_color = (255, 255, 255)\nlabel_background_color = (125, 175, 75)\nbox_color = (255, 128, 0)\nbox_thickness = 1\n\ndef build_argparser():\n    parser = ArgumentParser()\n    parser.add_argument(""-d"", ""--device"", help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. \\\n                                                Sample will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"", type=str)\n    return parser\n\n\ndef EntryIndex(side, lcoords, lclasses, location, entry):\n    n = int(location / (side * side))\n    loc = location % (side * side)\n    return int(n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc)\n\n\nclass DetectionObject():\n    xmin = 0\n    ymin = 0\n    xmax = 0\n    ymax = 0\n    class_id = 0\n    confidence = 0.0\n\n    def __init__(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n        self.xmin = int((x - w / 2) * w_scale)\n        self.ymin = int((y - h / 2) * h_scale)\n        self.xmax = int(self.xmin + w * w_scale)\n        self.ymax = int(self.ymin + h * h_scale)\n        self.class_id = class_id\n        self.confidence = confidence\n\n\ndef IntersectionOverUnion(box_1, box_2):\n    width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)\n    height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)\n    area_of_overlap = 0.0\n    if (width_of_overlap_area < 0.0 or height_of_overlap_area < 0.0):\n        area_of_overlap = 0.0\n    else:\n        area_of_overlap = width_of_overlap_area * height_of_overlap_area\n    box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin)\n    box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin)\n    area_of_union = box_1_area + box_2_area - area_of_overlap\n    retval = 0.0\n    if area_of_union <= 0.0:\n        retval = 0.0\n    else:\n        retval = (area_of_overlap / area_of_union)\n    return retval\n\n\ndef ParseYOLOV3Output(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects):\n\n    out_blob_h = blob.shape[2]\n    out_blob_w = blob.shape[3]\n\n    side = out_blob_h\n    anchor_offset = 0\n\n    if len(anchors) == 18:   ## YoloV3\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 6\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_52:\n            anchor_offset = 2 * 0\n\n    elif len(anchors) == 12: ## tiny-YoloV3\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 0\n\n    else:                    ## ???\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 6\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_52:\n            anchor_offset = 2 * 0\n\n    side_square = side * side\n    output_blob = blob.flatten()\n\n    for i in range(side_square):\n        row = int(i / side)\n        col = int(i % side)\n        for n in range(num):\n            obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords)\n            box_index = EntryIndex(side, coords, classes, n * side * side + i, 0)\n            scale = output_blob[obj_index]\n            if (scale < threshold):\n                continue\n            x = (col + output_blob[box_index + 0 * side_square]) / side * resized_im_w\n            y = (row + output_blob[box_index + 1 * side_square]) / side * resized_im_h\n            height = math.exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1]\n            width = math.exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n]\n            for j in range(classes):\n                class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j)\n                prob = scale * output_blob[class_index]\n                if prob < threshold:\n                    continue\n                obj = DetectionObject(x, y, height, width, j, prob, (original_im_h / resized_im_h), (original_im_w / resized_im_w))\n                objects.append(obj)\n    return objects\n\n\ndef main_IE_infer():\n    camera_width = 320\n    camera_height = 240\n    fps = """"\n    framepos = 0\n    frame_count = 0\n    vidfps = 0\n    skip_frame = 0\n    elapsedTime = 0\n    new_w = int(camera_width * m_input_size/camera_width)\n    new_h = int(camera_height * m_input_size/camera_height)\n\n    args = build_argparser().parse_args()\n    #model_xml = ""lrmodels/tiny-YoloV3/FP32/frozen_tiny_yolo_v3.xml"" #<--- CPU\n    model_xml = ""lrmodels/tiny-YoloV3/FP16/frozen_tiny_yolo_v3.xml"" #<--- MYRIAD\n    model_bin = os.path.splitext(model_xml)[0] + "".bin""\n\n    cap = cv2.VideoCapture(0)\n    cap.set(cv2.CAP_PROP_FPS, 30)\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n\n    #cap = cv2.VideoCapture(""data/input/testvideo.mp4"")\n    #camera_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    #camera_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    #frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    #vidfps = int(cap.get(cv2.CAP_PROP_FPS))\n    #print(""videosFrameCount ="", str(frame_count))\n    #print(""videosFPS ="", str(vidfps))\n\n    time.sleep(1)\n\n    plugin = IEPlugin(device=args.device)\n    if ""CPU"" in args.device:\n        plugin.add_cpu_extension(""lib/libcpu_extension.so"")\n    net = IENetwork(model=model_xml, weights=model_bin)\n    input_blob = next(iter(net.inputs))\n    exec_net = plugin.load(network=net)\n\n    while cap.isOpened():\n        t1 = time.time()\n\n        ## Uncomment only when playing video files\n        #cap.set(cv2.CAP_PROP_POS_FRAMES, framepos)\n\n        ret, image = cap.read()\n        if not ret:\n            break\n\n        resized_image = cv2.resize(image, (new_w, new_h), interpolation = cv2.INTER_CUBIC)\n        canvas = np.full((m_input_size, m_input_size, 3), 128)\n        canvas[(m_input_size-new_h)//2:(m_input_size-new_h)//2 + new_h,(m_input_size-new_w)//2:(m_input_size-new_w)//2 + new_w,  :] = resized_image\n        prepimg = canvas\n        prepimg = prepimg[np.newaxis, :, :, :]     # Batch size axis add\n        prepimg = prepimg.transpose((0, 3, 1, 2))  # NHWC to NCHW\n        outputs = exec_net.infer(inputs={input_blob: prepimg})\n\n        #output_name = detector/yolo-v3-tiny/Conv_12/BiasAdd/YoloRegion\n        #output_name = detector/yolo-v3-tiny/Conv_9/BiasAdd/YoloRegion\n\n        objects = []\n\n        for output in outputs.values():\n            objects = ParseYOLOV3Output(output, new_h, new_w, camera_height, camera_width, 0.4, objects)\n\n        # Filtering overlapping boxes\n        objlen = len(objects)\n        for i in range(objlen):\n            if (objects[i].confidence == 0.0):\n                continue\n            for j in range(i + 1, objlen):\n                if (IntersectionOverUnion(objects[i], objects[j]) >= 0.4):\n                    if objects[i].confidence < objects[j].confidence:\n                        objects[i], objects[j] = objects[j], objects[i]\n                    objects[j].confidence = 0.0\n        \n        # Drawing boxes\n        for obj in objects:\n            if obj.confidence < 0.2:\n                continue\n            label = obj.class_id\n            confidence = obj.confidence\n            #if confidence >= 0.2:\n            label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n            cv2.rectangle(image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n            cv2.putText(image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n\n        cv2.putText(image, fps, (camera_width - 170, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38, 0, 255), 1, cv2.LINE_AA)\n        cv2.imshow(""Result"", image)\n\n        if cv2.waitKey(1)&0xFF == ord(\'q\'):\n            break\n        elapsedTime = time.time() - t1\n        fps = ""(Playback) {:.1f} FPS"".format(1/elapsedTime)\n\n        ## frame skip, video file only\n        #skip_frame = int((vidfps - int(1/elapsedTime)) / int(1/elapsedTime))\n        #framepos += skip_frame\n\n    cv2.destroyAllWindows()\n    del net\n    del exec_net\n    del plugin\n\n\nif __name__ == \'__main__\':\n    sys.exit(main_IE_infer() or 0)\n\n'"
openvino_yolov3_MultiStick_test.py,0,"b'import sys, os, cv2, time, heapq, argparse\nimport numpy as np, math\ntry:\n    from armv7l.openvino.inference_engine import IENetwork, IEPlugin\nexcept:\n    from openvino.inference_engine import IENetwork, IEPlugin\nimport multiprocessing as mp\nfrom time import sleep\nimport threading\n\nyolo_scale_13 = 13\nyolo_scale_26 = 26\nyolo_scale_52 = 52\n\nclasses = 80\ncoords = 4\nnum = 3\nanchors = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n\nLABELS = (""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"",\n          ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"",\n          ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"",\n          ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"",\n          ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"",\n          ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"",\n          ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"",\n          ""baseball glove"", ""skateboard"", ""surfboard"",""tennis racket"", ""bottle"",\n          ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"",\n          ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n          ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"",\n          ""cake"", ""chair"", ""sofa"", ""pottedplant"", ""bed"",\n          ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n          ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"",\n          ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"",\n          ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush"")\n\nlabel_text_color = (255, 255, 255)\nlabel_background_color = (125, 175, 75)\nbox_color = (255, 128, 0)\nbox_thickness = 1\n\nprocesses = []\n\nfps = """"\ndetectfps = """"\nframecount = 0\ndetectframecount = 0\ntime1 = 0\ntime2 = 0\nlastresults = None\n\ndef EntryIndex(side, lcoords, lclasses, location, entry):\n    n = int(location / (side * side))\n    loc = location % (side * side)\n    return int(n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc)\n\n\nclass DetectionObject():\n    xmin = 0\n    ymin = 0\n    xmax = 0\n    ymax = 0\n    class_id = 0\n    confidence = 0.0\n\n    def __init__(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n        self.xmin = int((x - w / 2) * w_scale)\n        self.ymin = int((y - h / 2) * h_scale)\n        self.xmax = int(self.xmin + w * w_scale)\n        self.ymax = int(self.ymin + h * h_scale)\n        self.class_id = class_id\n        self.confidence = confidence\n\n\ndef IntersectionOverUnion(box_1, box_2):\n    width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)\n    height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)\n    area_of_overlap = 0.0\n    if (width_of_overlap_area < 0.0 or height_of_overlap_area < 0.0):\n        area_of_overlap = 0.0\n    else:\n        area_of_overlap = width_of_overlap_area * height_of_overlap_area\n    box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin)\n    box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin)\n    area_of_union = box_1_area + box_2_area - area_of_overlap\n    retval = 0.0\n    if area_of_union <= 0.0:\n        retval = 0.0\n    else:\n        retval = (area_of_overlap / area_of_union)\n    return retval\n\n\ndef ParseYOLOV3Output(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects):\n\n    out_blob_h = blob.shape[2]\n    out_blob_w = blob.shape[3]\n\n    side = out_blob_h\n    anchor_offset = 0\n\n    if side == yolo_scale_13:\n        anchor_offset = 2 * 6\n    elif side == yolo_scale_26:\n        anchor_offset = 2 * 3\n    elif side == yolo_scale_52:\n        anchor_offset = 2 * 0\n\n    side_square = side * side\n    output_blob = blob.flatten()\n\n    for i in range(side_square):\n        row = int(i / side)\n        col = int(i % side)\n        for n in range(num):\n            obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords)\n            box_index = EntryIndex(side, coords, classes, n * side * side + i, 0)\n            scale = output_blob[obj_index]\n            if (scale < threshold):\n                continue\n            x = (col + output_blob[box_index + 0 * side_square]) / side * resized_im_w\n            y = (row + output_blob[box_index + 1 * side_square]) / side * resized_im_h\n            height = math.exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1]\n            width = math.exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n]\n            for j in range(classes):\n                class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j)\n                prob = scale * output_blob[class_index]\n                if prob < threshold:\n                    continue\n                obj = DetectionObject(x, y, height, width, j, prob, (original_im_h / resized_im_h), (original_im_w / resized_im_w))\n                objects.append(obj)\n    return objects\n\n\ndef camThread(LABELS, results, frameBuffer, camera_width, camera_height, vidfps):\n    global fps\n    global detectfps\n    global lastresults\n    global framecount\n    global detectframecount\n    global time1\n    global time2\n    global cam\n    global window_name\n\n    #cam = cv2.VideoCapture(0)\n    #if cam.isOpened() != True:\n    #    print(""USB Camera Open Error!!!"")\n    #    sys.exit(0)\n    #cam.set(cv2.CAP_PROP_FPS, vidfps)\n    #cam.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n    #cam.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n    #window_name = ""USB Camera""\n    #wait_key_time = 1\n\n    cam = cv2.VideoCapture(""data/input/testvideo4.mp4"")\n    camera_width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n    camera_height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame_count = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n    window_name = ""Movie File""\n    wait_key_time = int(1000 / vidfps)\n\n    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)\n\n    while True:\n        t1 = time.perf_counter()\n\n        # USB Camera Stream Read\n        s, color_image = cam.read()\n        if not s:\n            continue\n        if frameBuffer.full():\n            frameBuffer.get()\n\n        height = color_image.shape[0]\n        width = color_image.shape[1]\n        frameBuffer.put(color_image.copy())\n\n        if not results.empty():\n            objects = results.get(False)\n            detectframecount += 1\n\n            for obj in objects:\n                if obj.confidence < 0.2:\n                    continue\n                label = obj.class_id\n                confidence = obj.confidence\n                if confidence > 0.2:\n                    label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n                    cv2.rectangle(color_image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n                    cv2.putText(color_image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n            lastresults = objects\n        else:\n            if not isinstance(lastresults, type(None)):\n                for obj in lastresults:\n                    if obj.confidence < 0.2:\n                        continue\n                    label = obj.class_id\n                    confidence = obj.confidence\n                    if confidence > 0.2:\n                        label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n                        cv2.rectangle(color_image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n                        cv2.putText(color_image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n\n        cv2.putText(color_image, fps,       (width-170,15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n        cv2.putText(color_image, detectfps, (width-170,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38,0,255), 1, cv2.LINE_AA)\n        cv2.imshow(window_name, cv2.resize(color_image, (width, height)))\n\n        if cv2.waitKey(wait_key_time)&0xFF == ord(\'q\'):\n            sys.exit(0)\n\n        ## Print FPS\n        framecount += 1\n        if framecount >= 15:\n            fps       = ""(Playback) {:.1f} FPS"".format(time1/15)\n            detectfps = ""(Detection) {:.1f} FPS"".format(detectframecount/time2)\n            framecount = 0\n            detectframecount = 0\n            time1 = 0\n            time2 = 0\n        t2 = time.perf_counter()\n        elapsedTime = t2-t1\n        time1 += 1/elapsedTime\n        time2 += elapsedTime\n\n\n# l = Search list\n# x = Search target value\ndef searchlist(l, x, notfoundvalue=-1):\n    if x in l:\n        return l.index(x)\n    else:\n        return notfoundvalue\n\n\ndef async_infer(ncsworker):\n\n    ncsworker.skip_frame_measurement()\n\n    while True:\n        ncsworker.predict_async()\n\n\nclass NcsWorker(object):\n\n    def __init__(self, devid, frameBuffer, results, camera_width, camera_height, number_of_ncs, vidfps):\n        self.devid = devid\n        self.frameBuffer = frameBuffer\n        self.model_xml = ""./lrmodels/YoloV3/FP16/frozen_yolo_v3.xml""\n        self.model_bin = ""./lrmodels/YoloV3/FP16/frozen_yolo_v3.bin""\n        self.camera_width = camera_width\n        self.camera_height = camera_height\n        self.m_input_size = 416\n        self.threshould = 0.7\n        self.num_requests = 4\n        self.inferred_request = [0] * self.num_requests\n        self.heap_request = []\n        self.inferred_cnt = 0\n        self.plugin = IEPlugin(device=""MYRIAD"")\n        self.net = IENetwork(model=self.model_xml, weights=self.model_bin)\n        self.input_blob = next(iter(self.net.inputs))\n        self.exec_net = self.plugin.load(network=self.net, num_requests=self.num_requests)\n        self.results = results\n        self.number_of_ncs = number_of_ncs\n        self.predict_async_time = 800\n        self.skip_frame = 0\n        self.roop_frame = 0\n        self.vidfps = vidfps\n        self.new_w = int(camera_width * self.m_input_size/camera_width)\n        self.new_h = int(camera_height * self.m_input_size/camera_height)\n\n    def image_preprocessing(self, color_image):\n        resized_image = cv2.resize(color_image, (self.new_w, self.new_h), interpolation = cv2.INTER_CUBIC)\n        canvas = np.full((self.m_input_size, self.m_input_size, 3), 128)\n        canvas[(self.m_input_size-self.new_h)//2:(self.m_input_size-self.new_h)//2 + self.new_h,(self.m_input_size-self.new_w)//2:(self.m_input_size-self.new_w)//2 + self.new_w,  :] = resized_image\n        prepimg = canvas\n        prepimg = prepimg[np.newaxis, :, :, :]     # Batch size axis add\n        prepimg = prepimg.transpose((0, 3, 1, 2))  # NHWC to NCHW\n        return prepimg\n\n\n    def skip_frame_measurement(self):\n            surplustime_per_second = (1000 - self.predict_async_time)\n            if surplustime_per_second > 0.0:\n                frame_per_millisecond = (1000 / self.vidfps)\n                total_skip_frame = surplustime_per_second / frame_per_millisecond\n                self.skip_frame = int(total_skip_frame / self.num_requests)\n            else:\n                self.skip_frame = 0\n\n\n    def predict_async(self):\n        try:\n\n            if self.frameBuffer.empty():\n                return\n\n            self.roop_frame += 1\n            if self.roop_frame <= self.skip_frame:\n               self.frameBuffer.get()\n               return\n            self.roop_frame = 0\n\n            prepimg = self.image_preprocessing(self.frameBuffer.get())\n            reqnum = searchlist(self.inferred_request, 0)\n\n            if reqnum > -1:\n                self.exec_net.start_async(request_id=reqnum, inputs={self.input_blob: prepimg})\n                self.inferred_request[reqnum] = 1\n                self.inferred_cnt += 1\n                if self.inferred_cnt == sys.maxsize:\n                    self.inferred_request = [0] * self.num_requests\n                    self.heap_request = []\n                    self.inferred_cnt = 0\n                heapq.heappush(self.heap_request, (self.inferred_cnt, reqnum))\n\n            cnt, dev = heapq.heappop(self.heap_request)\n\n            if self.exec_net.requests[dev].wait(0) == 0:\n                self.exec_net.requests[dev].wait(-1)\n\n                objects = []\n                outputs = self.exec_net.requests[dev].outputs\n                for output in outputs.values():\n                    objects = ParseYOLOV3Output(output, self.new_h, self.new_w, self.camera_height, self.camera_width, self.threshould, objects)\n\n                objlen = len(objects)\n                for i in range(objlen):\n                    if (objects[i].confidence == 0.0):\n                        continue\n                    for j in range(i + 1, objlen):\n                        if (IntersectionOverUnion(objects[i], objects[j]) >= 0.4):\n                            objects[j].confidence = 0\n\n                self.results.put(objects)\n                self.inferred_request[dev] = 0\n            else:\n                heapq.heappush(self.heap_request, (cnt, dev))\n        except:\n            import traceback\n            traceback.print_exc()\n\n\ndef inferencer(results, frameBuffer, number_of_ncs, camera_width, camera_height, vidfps):\n\n    # Init infer threads\n    threads = []\n    for devid in range(number_of_ncs):\n        thworker = threading.Thread(target=async_infer, args=(NcsWorker(devid, frameBuffer, results, camera_width, camera_height, number_of_ncs, vidfps),))\n        thworker.start()\n        threads.append(thworker)\n\n    for th in threads:\n        th.join()\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-numncs\',\'--numberofncs\',dest=\'number_of_ncs\',type=int,default=1,help=\'Number of NCS. (Default=1)\')\n    args = parser.parse_args()\n\n    number_of_ncs = args.number_of_ncs\n    camera_width = 320\n    camera_height = 240\n    vidfps = 30\n\n    try:\n\n        mp.set_start_method(\'forkserver\')\n        frameBuffer = mp.Queue(10)\n        results = mp.Queue()\n\n        # Start detection MultiStick\n        # Activation of inferencer\n        p = mp.Process(target=inferencer, args=(results, frameBuffer, number_of_ncs, camera_width, camera_height, vidfps), daemon=True)\n        p.start()\n        processes.append(p)\n\n        sleep(number_of_ncs * 7)\n\n        # Start streaming\n        p = mp.Process(target=camThread, args=(LABELS, results, frameBuffer, camera_width, camera_height, vidfps), daemon=True)\n        p.start()\n        processes.append(p)\n\n        while True:\n            sleep(1)\n\n    except:\n        import traceback\n        traceback.print_exc()\n    finally:\n        for p in range(len(processes)):\n            processes[p].terminate()\n\n        print(""\\n\\nFinished\\n\\n"")\n'"
openvino_yolov3_test.py,0,"b'import sys, os, cv2, time\nimport numpy as np, math\nfrom argparse import ArgumentParser\ntry:\n    from armv7l.openvino.inference_engine import IENetwork, IEPlugin\nexcept:\n    from openvino.inference_engine import IENetwork, IEPlugin\n\nm_input_size = 416\n\nyolo_scale_13 = 13\nyolo_scale_26 = 26\nyolo_scale_52 = 52\n\nclasses = 80\ncoords = 4\nnum = 3\nanchors = [10,13,16,30,33,23,30,61,62,45,59,119,116,90,156,198,373,326]\n\nLABELS = (""person"", ""bicycle"", ""car"", ""motorbike"", ""aeroplane"",\n          ""bus"", ""train"", ""truck"", ""boat"", ""traffic light"",\n          ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"",\n          ""cat"", ""dog"", ""horse"", ""sheep"", ""cow"",\n          ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"",\n          ""umbrella"", ""handbag"", ""tie"", ""suitcase"", ""frisbee"",\n          ""skis"", ""snowboard"", ""sports ball"", ""kite"", ""baseball bat"",\n          ""baseball glove"", ""skateboard"", ""surfboard"",""tennis racket"", ""bottle"",\n          ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"",\n          ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n          ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"",\n          ""cake"", ""chair"", ""sofa"", ""pottedplant"", ""bed"",\n          ""diningtable"", ""toilet"", ""tvmonitor"", ""laptop"", ""mouse"",\n          ""remote"", ""keyboard"", ""cell phone"", ""microwave"", ""oven"",\n          ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"",\n          ""vase"", ""scissors"", ""teddy bear"", ""hair drier"", ""toothbrush"")\n\nlabel_text_color = (255, 255, 255)\nlabel_background_color = (125, 175, 75)\nbox_color = (255, 128, 0)\nbox_thickness = 1\n\ndef build_argparser():\n    parser = ArgumentParser()\n    parser.add_argument(""-d"", ""--device"", help=""Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. \\\n                                                Sample will look for a suitable plugin for device specified (CPU by default)"", default=""CPU"", type=str)\n    return parser\n\n\ndef EntryIndex(side, lcoords, lclasses, location, entry):\n    n = int(location / (side * side))\n    loc = location % (side * side)\n    return int(n * side * side * (lcoords + lclasses + 1) + entry * side * side + loc)\n\n\nclass DetectionObject():\n    xmin = 0\n    ymin = 0\n    xmax = 0\n    ymax = 0\n    class_id = 0\n    confidence = 0.0\n\n    def __init__(self, x, y, h, w, class_id, confidence, h_scale, w_scale):\n        self.xmin = int((x - w / 2) * w_scale)\n        self.ymin = int((y - h / 2) * h_scale)\n        self.xmax = int(self.xmin + w * w_scale)\n        self.ymax = int(self.ymin + h * h_scale)\n        self.class_id = class_id\n        self.confidence = confidence\n\n\ndef IntersectionOverUnion(box_1, box_2):\n    width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)\n    height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)\n    area_of_overlap = 0.0\n    if (width_of_overlap_area < 0.0 or height_of_overlap_area < 0.0):\n        area_of_overlap = 0.0\n    else:\n        area_of_overlap = width_of_overlap_area * height_of_overlap_area\n    box_1_area = (box_1.ymax - box_1.ymin)  * (box_1.xmax - box_1.xmin)\n    box_2_area = (box_2.ymax - box_2.ymin)  * (box_2.xmax - box_2.xmin)\n    area_of_union = box_1_area + box_2_area - area_of_overlap\n    retval = 0.0\n    if area_of_union <= 0.0:\n        retval = 0.0\n    else:\n        retval = (area_of_overlap / area_of_union)\n    return retval\n\n\ndef ParseYOLOV3Output(blob, resized_im_h, resized_im_w, original_im_h, original_im_w, threshold, objects):\n\n    out_blob_h = blob.shape[2]\n    out_blob_w = blob.shape[3]\n\n    side = out_blob_h\n    anchor_offset = 0\n\n    if len(anchors) == 18:   ## YoloV3\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 6\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_52:\n            anchor_offset = 2 * 0\n\n    elif len(anchors) == 12: ## tiny-YoloV3\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 0\n\n    else:                    ## ???\n        if side == yolo_scale_13:\n            anchor_offset = 2 * 6\n        elif side == yolo_scale_26:\n            anchor_offset = 2 * 3\n        elif side == yolo_scale_52:\n            anchor_offset = 2 * 0\n\n    side_square = side * side\n    output_blob = blob.flatten()\n\n    for i in range(side_square):\n        row = int(i / side)\n        col = int(i % side)\n        for n in range(num):\n            obj_index = EntryIndex(side, coords, classes, n * side * side + i, coords)\n            box_index = EntryIndex(side, coords, classes, n * side * side + i, 0)\n            scale = output_blob[obj_index]\n            if (scale < threshold):\n                continue\n            x = (col + output_blob[box_index + 0 * side_square]) / side * resized_im_w\n            y = (row + output_blob[box_index + 1 * side_square]) / side * resized_im_h\n            height = math.exp(output_blob[box_index + 3 * side_square]) * anchors[anchor_offset + 2 * n + 1]\n            width = math.exp(output_blob[box_index + 2 * side_square]) * anchors[anchor_offset + 2 * n]\n            for j in range(classes):\n                class_index = EntryIndex(side, coords, classes, n * side_square + i, coords + 1 + j)\n                prob = scale * output_blob[class_index]\n                if prob < threshold:\n                    continue\n                obj = DetectionObject(x, y, height, width, j, prob, (original_im_h / resized_im_h), (original_im_w / resized_im_w))\n                objects.append(obj)\n    return objects\n\n\ndef main_IE_infer():\n    camera_width = 320\n    camera_height = 240\n    fps = """"\n    framepos = 0\n    frame_count = 0\n    vidfps = 0\n    skip_frame = 0\n    elapsedTime = 0\n    new_w = int(camera_width * m_input_size/camera_width)\n    new_h = int(camera_height * m_input_size/camera_height)\n\n    args = build_argparser().parse_args()\n    model_xml = ""lrmodels/YoloV3/FP32/frozen_yolo_v3.xml"" #<--- CPU\n    #model_xml = ""lrmodels/YoloV3/FP16/frozen_yolo_v3.xml"" #<--- MYRIAD\n    model_bin = os.path.splitext(model_xml)[0] + "".bin""\n\n    cap = cv2.VideoCapture(0)\n    cap.set(cv2.CAP_PROP_FPS, 30)\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n\n    #cap = cv2.VideoCapture(""data/input/testvideo.mp4"")\n    #camera_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    #camera_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    #frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    #vidfps = int(cap.get(cv2.CAP_PROP_FPS))\n    #print(""videosFrameCount ="", str(frame_count))\n    #print(""videosFPS ="", str(vidfps))\n\n    time.sleep(1)\n\n    plugin = IEPlugin(device=args.device)\n    if ""CPU"" in args.device:\n        plugin.add_cpu_extension(""lib/libcpu_extension.so"")\n    net = IENetwork(model=model_xml, weights=model_bin)\n    input_blob = next(iter(net.inputs))\n    exec_net = plugin.load(network=net)\n\n    while cap.isOpened():\n        t1 = time.time()\n\n        ## Uncomment only when playing video files\n        #cap.set(cv2.CAP_PROP_POS_FRAMES, framepos)\n\n        ret, image = cap.read()\n        if not ret:\n            break\n\n        resized_image = cv2.resize(image, (new_w, new_h), interpolation = cv2.INTER_CUBIC)\n        canvas = np.full((m_input_size, m_input_size, 3), 128)\n        canvas[(m_input_size-new_h)//2:(m_input_size-new_h)//2 + new_h,(m_input_size-new_w)//2:(m_input_size-new_w)//2 + new_w,  :] = resized_image\n        prepimg = canvas\n        prepimg = prepimg[np.newaxis, :, :, :]     # Batch size axis add\n        prepimg = prepimg.transpose((0, 3, 1, 2))  # NHWC to NCHW\n        outputs = exec_net.infer(inputs={input_blob: prepimg})\n\n        objects = []\n\n        for output in outputs.values():\n            objects = ParseYOLOV3Output(output, new_h, new_w, camera_height, camera_width, 0.7, objects)\n\n        # Filtering overlapping boxes\n        objlen = len(objects)\n        for i in range(objlen):\n            if (objects[i].confidence == 0.0):\n                continue\n            for j in range(i + 1, objlen):\n                if (IntersectionOverUnion(objects[i], objects[j]) >= 0.4):\n                    objects[j].confidence = 0\n        \n        # Drawing boxes\n        for obj in objects:\n            if obj.confidence < 0.2:\n                continue\n            label = obj.class_id\n            confidence = obj.confidence\n            if confidence > 0.2:\n                label_text = LABELS[label] + "" ("" + ""{:.1f}"".format(confidence * 100) + ""%)""\n                cv2.rectangle(image, (obj.xmin, obj.ymin), (obj.xmax, obj.ymax), box_color, box_thickness)\n                cv2.putText(image, label_text, (obj.xmin, obj.ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 1)\n\n        cv2.putText(image, fps, (camera_width - 170, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38, 0, 255), 1, cv2.LINE_AA)\n        cv2.imshow(""Result"", image)\n\n        if cv2.waitKey(1)&0xFF == ord(\'q\'):\n            break\n        elapsedTime = time.time() - t1\n        fps = ""(Playback) {:.1f} FPS"".format(1/elapsedTime)\n\n        ## frame skip, video file only\n        #skip_frame = int((vidfps - int(1/elapsedTime)) / int(1/elapsedTime))\n        #framepos += skip_frame\n\n    cv2.destroyAllWindows()\n    del net\n    del exec_net\n    del plugin\n\n\nif __name__ == \'__main__\':\n    sys.exit(main_IE_infer() or 0)\n\n'"
tfconverter.py,6,"b""import tensorflow as tf\nfrom google.protobuf import text_format\nfrom tensorflow.python.platform import gfile\n\ndef pbtxt_to_graphdef(filename):\n  with open(filename, 'r') as f:\n    graph_def = tf.GraphDef()\n    file_content = f.read()\n    text_format.Merge(file_content, graph_def)\n    tf.import_graph_def(graph_def, name='')\n    tf.train.write_graph(graph_def, './', 'frozen_yolo_v3.pb', as_text=False)\n\ndef graphdef_to_pbtxt(filename): \n  with gfile.FastGFile(filename,'rb') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name='')\n    tf.train.write_graph(graph_def, './', filename+'txt', as_text=True)\n  return\n\n\ngraphdef_to_pbtxt('pbmodels/frozen_yolo_v3.pb')  # here you can write the name of the file to be converted\n"""
utils.py,17,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import ImageDraw, Image\n\n\ndef get_boxes_and_inputs_pb(frozen_graph):\n\n    with frozen_graph.as_default():\n        boxes = tf.get_default_graph().get_tensor_by_name(""output_boxes:0"")\n        inputs = tf.get_default_graph().get_tensor_by_name(""inputs:0"")\n\n    return boxes, inputs\n\n\ndef get_boxes_and_inputs(model, num_classes, size, data_format):\n\n    inputs = tf.placeholder(tf.float32, [1, size, size, 3])\n\n    with tf.variable_scope(\'detector\'):\n        detections = model(inputs, num_classes,\n                           data_format=data_format)\n\n    boxes = detections_boxes(detections)\n\n    return boxes, inputs\n\n\ndef load_graph(frozen_graph_filename):\n\n    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name="""")\n\n    return graph\n\n\ndef freeze_graph(sess, output_graph, tiny):\n\n    output_node_names=[]\n    if tiny:\n        output_node_names = [\n            ""output_boxes"",\n            ""inputs"",\n        ]\n    else:\n        output_node_names = [\n            ""output_boxes"",\n            ""inputs"",\n        ]\n    output_node_names = "","".join(output_node_names)\n\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,\n        tf.get_default_graph().as_graph_def(),\n        output_node_names.split("","")\n    )\n\n    with tf.gfile.GFile(output_graph, ""wb"") as f:\n        f.write(output_graph_def.SerializeToString())\n\n    print(""{} ops written to {}."".format(len(output_graph_def.node), output_graph))\n\n\ndef load_weights(var_list, weights_file):\n    """"""\n    Loads and converts pre-trained weights.\n    :param var_list: list of network variables.\n    :param weights_file: name of the binary file.\n    :return: list of assign ops\n    """"""\n    with open(weights_file, ""rb"") as fp:\n        _ = np.fromfile(fp, dtype=np.int32, count=5)\n\n        weights = np.fromfile(fp, dtype=np.float32)\n\n    ptr = 0\n    i = 0\n    assign_ops = []\n    while i < len(var_list) - 1:\n        var1 = var_list[i]\n        var2 = var_list[i + 1]\n        # do something only if we process conv layer\n        if \'Conv\' in var1.name.split(\'/\')[-2]:\n            # check type of next layer\n            if \'BatchNorm\' in var2.name.split(\'/\')[-2]:\n                # load batch norm params\n                gamma, beta, mean, var = var_list[i + 1:i + 5]\n                batch_norm_vars = [beta, gamma, mean, var]\n                for var in batch_norm_vars:\n                    shape = var.shape.as_list()\n                    num_params = np.prod(shape)\n                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n                    ptr += num_params\n                    assign_ops.append(\n                        tf.assign(var, var_weights, validate_shape=True))\n\n                # we move the pointer by 4, because we loaded 4 variables\n                i += 4\n            elif \'Conv\' in var2.name.split(\'/\')[-2]:\n                # load biases\n                bias = var2\n                bias_shape = bias.shape.as_list()\n                bias_params = np.prod(bias_shape)\n                bias_weights = weights[ptr:ptr +\n                                       bias_params].reshape(bias_shape)\n                ptr += bias_params\n                assign_ops.append(\n                    tf.assign(bias, bias_weights, validate_shape=True))\n\n                # we loaded 1 variable\n                i += 1\n            # we can load weights of conv layer\n            shape = var1.shape.as_list()\n            num_params = np.prod(shape)\n\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            # remember to transpose to column-major\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(\n                tf.assign(var1, var_weights, validate_shape=True))\n            i += 1\n\n    return assign_ops\n\n\ndef detections_boxes(detections):\n    """"""\n    Converts center x, center y, width and height values to coordinates of top left and bottom right points.\n\n    :param detections: outputs of YOLO v3 detector of shape (?, 10647, (num_classes + 5))\n    :return: converted detections of same shape as input\n    """"""\n    center_x, center_y, width, height, attrs = tf.split(\n        detections, [1, 1, 1, 1, -1], axis=-1)\n    w2 = width / 2\n    h2 = height / 2\n    x0 = center_x - w2\n    y0 = center_y - h2\n    x1 = center_x + w2\n    y1 = center_y + h2\n\n    boxes = tf.concat([x0, y0, x1, y1], axis=-1)\n    detections = tf.concat([boxes, attrs], axis=-1, name=""output_boxes"")\n    return detections\n\n\ndef _iou(box1, box2):\n    """"""\n    Computes Intersection over Union value for 2 bounding boxes\n\n    :param box1: array of 4 values (top left and bottom right coords): [x0, y0, x1, x2]\n    :param box2: same as box1\n    :return: IoU\n    """"""\n    b1_x0, b1_y0, b1_x1, b1_y1 = box1\n    b2_x0, b2_y0, b2_x1, b2_y1 = box2\n\n    int_x0 = max(b1_x0, b2_x0)\n    int_y0 = max(b1_y0, b2_y0)\n    int_x1 = min(b1_x1, b2_x1)\n    int_y1 = min(b1_y1, b2_y1)\n\n    int_area = (int_x1 - int_x0) * (int_y1 - int_y0)\n\n    b1_area = (b1_x1 - b1_x0) * (b1_y1 - b1_y0)\n    b2_area = (b2_x1 - b2_x0) * (b2_y1 - b2_y0)\n\n    # we add small epsilon of 1e-05 to avoid division by 0\n    iou = int_area / (b1_area + b2_area - int_area + 1e-05)\n    return iou\n\n\ndef non_max_suppression(predictions_with_boxes, confidence_threshold, iou_threshold=0.4):\n    """"""\n    Applies Non-max suppression to prediction boxes.\n\n    :param predictions_with_boxes: 3D numpy array, first 4 values in 3rd dimension are bbox attrs, 5th is confidence\n    :param confidence_threshold: the threshold for deciding if prediction is valid\n    :param iou_threshold: the threshold for deciding if two boxes overlap\n    :return: dict: class -> [(box, score)]\n    """"""\n    conf_mask = np.expand_dims(\n        (predictions_with_boxes[:, :, 4] > confidence_threshold), -1)\n    predictions = predictions_with_boxes * conf_mask\n\n    result = {}\n    for i, image_pred in enumerate(predictions):\n        shape = image_pred.shape\n        non_zero_idxs = np.nonzero(image_pred)\n        image_pred = image_pred[non_zero_idxs]\n        image_pred = image_pred.reshape(-1, shape[-1])\n\n        bbox_attrs = image_pred[:, :5]\n        classes = image_pred[:, 5:]\n        classes = np.argmax(classes, axis=-1)\n\n        unique_classes = list(set(classes.reshape(-1)))\n\n        for cls in unique_classes:\n            cls_mask = classes == cls\n            cls_boxes = bbox_attrs[np.nonzero(cls_mask)]\n            cls_boxes = cls_boxes[cls_boxes[:, -1].argsort()[::-1]]\n            cls_scores = cls_boxes[:, -1]\n            cls_boxes = cls_boxes[:, :-1]\n\n            while len(cls_boxes) > 0:\n                box = cls_boxes[0]\n                score = cls_scores[0]\n                if cls not in result:\n                    result[cls] = []\n                result[cls].append((box, score))\n                cls_boxes = cls_boxes[1:]\n                cls_scores = cls_scores[1:]\n                ious = np.array([_iou(box, x) for x in cls_boxes])\n                iou_mask = ious < iou_threshold\n                cls_boxes = cls_boxes[np.nonzero(iou_mask)]\n                cls_scores = cls_scores[np.nonzero(iou_mask)]\n\n    return result\n\n\ndef load_coco_names(file_name):\n    names = {}\n    with open(file_name) as f:\n        for id, name in enumerate(f):\n            names[id] = name\n    return names\n\n\ndef draw_boxes(boxes, img, cls_names, detection_size, is_letter_box_image):\n    draw = ImageDraw.Draw(img)\n\n    for cls, bboxs in boxes.items():\n        color = tuple(np.random.randint(0, 256, 3))\n        for box, score in bboxs:\n            box = convert_to_original_size(box, np.array(detection_size),\n                                           np.array(img.size),\n                                           is_letter_box_image)\n            draw.rectangle(box, outline=color)\n            draw.text(box[:2], \'{} {:.2f}%\'.format(\n                cls_names[cls], score * 100), fill=color)\n\n\ndef convert_to_original_size(box, size, original_size, is_letter_box_image):\n    if is_letter_box_image:\n        box = box.reshape(2, 2)\n        box[0, :] = letter_box_pos_to_original_pos(box[0, :], size, original_size)\n        box[1, :] = letter_box_pos_to_original_pos(box[1, :], size, original_size)\n    else:\n        ratio = original_size / size\n        box = box.reshape(2, 2) * ratio\n    return list(box.reshape(-1))\n\n\ndef letter_box_image(image: Image.Image, output_height: int, output_width: int, fill_value)-> np.ndarray:\n    """"""\n    Fit image with final image with output_width and output_height.\n    :param image: PILLOW Image object.\n    :param output_height: width of the final image.\n    :param output_width: height of the final image.\n    :param fill_value: fill value for empty area. Can be uint8 or np.ndarray\n    :return: numpy image fit within letterbox. dtype=uint8, shape=(output_height, output_width)\n    """"""\n\n    height_ratio = float(output_height)/image.size[1]\n    width_ratio = float(output_width)/image.size[0]\n    fit_ratio = min(width_ratio, height_ratio)\n    fit_height = int(image.size[1] * fit_ratio)\n    fit_width = int(image.size[0] * fit_ratio)\n    fit_image = np.asarray(image.resize((fit_width, fit_height), resample=Image.BILINEAR))\n\n    if isinstance(fill_value, int):\n        fill_value = np.full(fit_image.shape[2], fill_value, fit_image.dtype)\n\n    to_return = np.tile(fill_value, (output_height, output_width, 1))\n    pad_top = int(0.5 * (output_height - fit_height))\n    pad_left = int(0.5 * (output_width - fit_width))\n    to_return[pad_top:pad_top+fit_height, pad_left:pad_left+fit_width] = fit_image\n    return to_return\n\n\ndef letter_box_pos_to_original_pos(letter_pos, current_size, ori_image_size)-> np.ndarray:\n    """"""\n    Parameters should have same shape and dimension space. (Width, Height) or (Height, Width)\n    :param letter_pos: The current position within letterbox image including fill value area.\n    :param current_size: The size of whole image including fill value area.\n    :param ori_image_size: The size of image before being letter boxed.\n    :return:\n    """"""\n    letter_pos = np.asarray(letter_pos, dtype=np.float)\n    current_size = np.asarray(current_size, dtype=np.float)\n    ori_image_size = np.asarray(ori_image_size, dtype=np.float)\n    final_ratio = min(current_size[0]/ori_image_size[0], current_size[1]/ori_image_size[1])\n    pad = 0.5 * (current_size - final_ratio * ori_image_size)\n    pad = pad.astype(np.int32)\n    to_return_pos = (letter_pos - pad) / final_ratio\n    return to_return_pos\n'"
yolo_v3.py,41,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-05\n_LEAKY_RELU = 0.1\n\n_ANCHORS = [(10, 13), (16, 30), (33, 23),\n            (30, 61), (62, 45), (59, 119),\n            (116, 90), (156, 198), (373, 326)]\n\n\ndef darknet53(inputs):\n    """"""\n    Builds Darknet-53 model.\n    """"""\n    inputs = _conv2d_fixed_padding(inputs, 32, 3)\n    inputs = _conv2d_fixed_padding(inputs, 64, 3, strides=2)\n    inputs = _darknet53_block(inputs, 32)\n    inputs = _conv2d_fixed_padding(inputs, 128, 3, strides=2)\n\n    for i in range(2):\n        inputs = _darknet53_block(inputs, 64)\n\n    inputs = _conv2d_fixed_padding(inputs, 256, 3, strides=2)\n\n    for i in range(8):\n        inputs = _darknet53_block(inputs, 128)\n\n    route_1 = inputs\n    inputs = _conv2d_fixed_padding(inputs, 512, 3, strides=2)\n\n    for i in range(8):\n        inputs = _darknet53_block(inputs, 256)\n\n    route_2 = inputs\n    inputs = _conv2d_fixed_padding(inputs, 1024, 3, strides=2)\n\n    for i in range(4):\n        inputs = _darknet53_block(inputs, 512)\n\n    return route_1, route_2, inputs\n\n\ndef _conv2d_fixed_padding(inputs, filters, kernel_size, strides=1):\n    if strides > 1:\n        inputs = _fixed_padding(inputs, kernel_size)\n    inputs = slim.conv2d(inputs, filters, kernel_size, stride=strides,\n                         padding=(\'SAME\' if strides == 1 else \'VALID\'))\n    return inputs\n\n\ndef _darknet53_block(inputs, filters):\n    shortcut = inputs\n    inputs = _conv2d_fixed_padding(inputs, filters, 1)\n    inputs = _conv2d_fixed_padding(inputs, filters * 2, 3)\n\n    inputs = inputs + shortcut\n    return inputs\n\n\n@tf.contrib.framework.add_arg_scope\ndef _fixed_padding(inputs, kernel_size, *args, mode=\'CONSTANT\', **kwargs):\n    """"""\n    Pads the input along the spatial dimensions independently of input size.\n\n    Args:\n      inputs: A tensor of size [batch, channels, height_in, width_in] or\n        [batch, height_in, width_in, channels] depending on data_format.\n      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                   Should be a positive integer.\n      data_format: The input format (\'NHWC\' or \'NCHW\').\n      mode: The mode for tf.pad.\n\n    Returns:\n      A tensor with the same format as the input with the data either intact\n      (if kernel_size == 1) or padded (if kernel_size > 1).\n    """"""\n    pad_total = kernel_size - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n\n    if kwargs[\'data_format\'] == \'NCHW\':\n        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n                                        [pad_beg, pad_end],\n                                        [pad_beg, pad_end]],\n                               mode=mode)\n    else:\n        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                        [pad_beg, pad_end], [0, 0]], mode=mode)\n    return padded_inputs\n\n\ndef _yolo_block(inputs, filters):\n    inputs = _conv2d_fixed_padding(inputs, filters, 1)\n    inputs = _conv2d_fixed_padding(inputs, filters * 2, 3)\n    inputs = _conv2d_fixed_padding(inputs, filters, 1)\n    inputs = _conv2d_fixed_padding(inputs, filters * 2, 3)\n    inputs = _conv2d_fixed_padding(inputs, filters, 1)\n    route = inputs\n    inputs = _conv2d_fixed_padding(inputs, filters * 2, 3)\n    return route, inputs\n\n\ndef _get_size(shape, data_format):\n    if len(shape) == 4:\n        shape = shape[1:]\n    return shape[1:3] if data_format == \'NCHW\' else shape[0:2]\n\n\ndef _detection_layer(inputs, num_classes, anchors, img_size, data_format):\n    num_anchors = len(anchors)\n    predictions = slim.conv2d(inputs, num_anchors * (5 + num_classes), 1,\n                              stride=1, normalizer_fn=None,\n                              activation_fn=None,\n                              biases_initializer=tf.zeros_initializer())\n\n    shape = predictions.get_shape().as_list()\n    grid_size = _get_size(shape, data_format)\n    dim = grid_size[0] * grid_size[1]\n    bbox_attrs = 5 + num_classes\n\n    if data_format == \'NCHW\':\n        predictions = tf.reshape(\n            predictions, [-1, num_anchors * bbox_attrs, dim])\n        predictions = tf.transpose(predictions, [0, 2, 1])\n\n    predictions = tf.reshape(predictions, [-1, num_anchors * dim, bbox_attrs])\n\n    stride = (img_size[0] // grid_size[0], img_size[1] // grid_size[1])\n\n    anchors = [(a[0] / stride[0], a[1] / stride[1]) for a in anchors]\n\n    box_centers, box_sizes, confidence, classes = tf.split(\n        predictions, [2, 2, 1, num_classes], axis=-1)\n\n    box_centers = tf.nn.sigmoid(box_centers)\n    confidence = tf.nn.sigmoid(confidence)\n\n    grid_x = tf.range(grid_size[0], dtype=tf.float32)\n    grid_y = tf.range(grid_size[1], dtype=tf.float32)\n    a, b = tf.meshgrid(grid_x, grid_y)\n\n    x_offset = tf.reshape(a, (-1, 1))\n    y_offset = tf.reshape(b, (-1, 1))\n\n    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n    x_y_offset = tf.reshape(tf.tile(x_y_offset, [1, num_anchors]), [1, -1, 2])\n\n    box_centers = box_centers + x_y_offset\n    box_centers = box_centers * stride\n\n    anchors = tf.tile(anchors, [dim, 1])\n    box_sizes = tf.exp(box_sizes) * anchors\n    box_sizes = box_sizes * stride\n\n    detections = tf.concat([box_centers, box_sizes, confidence], axis=-1)\n\n    classes = tf.nn.sigmoid(classes)\n    predictions = tf.concat([detections, classes], axis=-1)\n    return predictions\n\n\ndef _upsample(inputs, out_shape, data_format=\'NCHW\'):\n    # tf.image.resize_nearest_neighbor accepts input in format NHWC\n    if data_format == \'NCHW\':\n        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n\n    if data_format == \'NCHW\':\n        new_height = out_shape[3]\n        new_width = out_shape[2]\n    else:\n        new_height = out_shape[2]\n        new_width = out_shape[1]\n\n    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n\n    # back to NCHW if needed\n    if data_format == \'NCHW\':\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    inputs = tf.identity(inputs, name=\'upsampled\')\n    return inputs\n\n\ndef yolo_v3(inputs, num_classes, is_training=False, data_format=\'NCHW\', reuse=False):\n    """"""\n    Creates YOLO v3 model.\n\n    :param inputs: a 4-D tensor of size [batch_size, height, width, channels].\n        Dimension batch_size may be undefined. The channel order is RGB.\n    :param num_classes: number of predicted classes.\n    :param is_training: whether is training or not.\n    :param data_format: data format NCHW or NHWC.\n    :param reuse: whether or not the network and its variables should be reused.\n    :return:\n    """"""\n    # it will be needed later on\n    img_size = inputs.get_shape().as_list()[1:3]\n\n    # transpose the inputs to NCHW\n    if data_format == \'NCHW\':\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    # normalize values to range [0..1]\n    inputs = inputs / 255\n\n    # set batch norm params\n    batch_norm_params = {\n        \'decay\': _BATCH_NORM_DECAY,\n        \'epsilon\': _BATCH_NORM_EPSILON,\n        \'scale\': True,\n        \'is_training\': is_training,\n        \'fused\': None,  # Use fused batch norm if possible.\n    }\n\n    # Set activation_fn and parameters for conv2d, batch_norm.\n    with slim.arg_scope([slim.conv2d, slim.batch_norm, _fixed_padding], data_format=data_format, reuse=reuse):\n        with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm,\n                            normalizer_params=batch_norm_params,\n                            biases_initializer=None,\n                            activation_fn=lambda x: tf.nn.leaky_relu(x, alpha=_LEAKY_RELU)):\n            with tf.variable_scope(\'darknet-53\'):\n                route_1, route_2, inputs = darknet53(inputs)\n\n            with tf.variable_scope(\'yolo-v3\'):\n                route, inputs = _yolo_block(inputs, 512)\n                detect_1 = _detection_layer(inputs, num_classes, _ANCHORS[6:9], img_size, data_format)\n                detect_1 = tf.identity(detect_1, name=\'detect_1\')\n                print(""detect_1.shape ="", detect_1.shape)\n\n                inputs = _conv2d_fixed_padding(route, 256, 1)\n                upsample_size = route_2.get_shape().as_list()\n                inputs = _upsample(inputs, upsample_size, data_format)\n                inputs = tf.concat([inputs, route_2], axis=1 if data_format == \'NCHW\' else 3)\n\n                route, inputs = _yolo_block(inputs, 256)\n\n                detect_2 = _detection_layer(inputs, num_classes, _ANCHORS[3:6], img_size, data_format)\n                detect_2 = tf.identity(detect_2, name=\'detect_2\')\n                print(""detect_2.shape ="", detect_2.shape)\n\n                inputs = _conv2d_fixed_padding(route, 128, 1)\n                upsample_size = route_1.get_shape().as_list()\n                inputs = _upsample(inputs, upsample_size, data_format)\n                inputs = tf.concat([inputs, route_1], axis=1 if data_format == \'NCHW\' else 3)\n\n                _, inputs = _yolo_block(inputs, 128)\n\n                detect_3 = _detection_layer(inputs, num_classes, _ANCHORS[0:3], img_size, data_format)\n                detect_3 = tf.identity(detect_3, name=\'detect_3\')\n                print(""detect_3.shape ="", detect_3.shape)\n\n                detections = tf.concat([detect_1, detect_2, detect_3], axis=1)\n                #detections = tf.reshape(detections, [1, detect_1.shape[1]+detect_2.shape[1]+detect_3.shape[1], num_classes+5])\n                detections = tf.identity(detections, name=\'detections\')\n                return detections\n'"
yolo_v3_tiny.py,9,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nfrom yolo_v3 import _conv2d_fixed_padding, _fixed_padding, _get_size, \\\n    _detection_layer, _upsample\n\nslim = tf.contrib.slim\n\n_BATCH_NORM_DECAY = 0.9\n_BATCH_NORM_EPSILON = 1e-05\n_LEAKY_RELU = 0.1\n\n_ANCHORS = [(10, 14),  (23, 27),  (37, 58),\n            (81, 82),  (135, 169),  (344, 319)]\n\n\ndef yolo_v3_tiny(inputs, num_classes, is_training=False, data_format=\'NCHW\', reuse=False):\n    """"""\n    Creates YOLO v3 tiny model.\n\n    :param inputs: a 4-D tensor of size [batch_size, height, width, channels].\n        Dimension batch_size may be undefined. The channel order is RGB.\n    :param num_classes: number of predicted classes.\n    :param is_training: whether is training or not.\n    :param data_format: data format NCHW or NHWC.\n    :param reuse: whether or not the network and its variables should be reused.\n    :return:\n    """"""\n    # it will be needed later on\n    img_size = inputs.get_shape().as_list()[1:3]\n\n    # transpose the inputs to NCHW\n    if data_format == \'NCHW\':\n        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n\n    # normalize values to range [0..1]\n    inputs = inputs / 255\n\n    # set batch norm params\n    batch_norm_params = {\n        \'decay\': _BATCH_NORM_DECAY,\n        \'epsilon\': _BATCH_NORM_EPSILON,\n        \'scale\': True,\n        \'is_training\': is_training,\n        \'fused\': None,  # Use fused batch norm if possible.\n    }\n\n    # Set activation_fn and parameters for conv2d, batch_norm.\n    with slim.arg_scope([slim.conv2d, slim.batch_norm, _fixed_padding, slim.max_pool2d], data_format=data_format):\n        with slim.arg_scope([slim.conv2d, slim.batch_norm, _fixed_padding], reuse=reuse):\n            with slim.arg_scope([slim.conv2d],\n                                normalizer_fn=slim.batch_norm,\n                                normalizer_params=batch_norm_params,\n                                biases_initializer=None,\n                                activation_fn=lambda x: tf.nn.leaky_relu(x, alpha=_LEAKY_RELU)):\n\n                with tf.variable_scope(\'yolo-v3-tiny\'):\n                    for i in range(6):\n                        inputs = _conv2d_fixed_padding(\n                            inputs, 16 * pow(2, i), 3)\n\n                        if i == 4:\n                            route_1 = inputs\n\n                        if i == 5:\n                            inputs = slim.max_pool2d(\n                                inputs, [2, 2], stride=1, padding=""SAME"", scope=\'pool2\')\n                        else:\n                            inputs = slim.max_pool2d(\n                                inputs, [2, 2], scope=\'pool2\')\n\n                    inputs = _conv2d_fixed_padding(inputs, 1024, 3)\n                    inputs = _conv2d_fixed_padding(inputs, 256, 1)\n                    route_2 = inputs\n\n                    inputs = _conv2d_fixed_padding(inputs, 512, 3)\n                    # inputs = _conv2d_fixed_padding(inputs, 255, 1)\n\n                    detect_1 = _detection_layer(\n                        inputs, num_classes, _ANCHORS[3:6], img_size, data_format)\n                    detect_1 = tf.identity(detect_1, name=\'detect_1\')\n\n                    inputs = _conv2d_fixed_padding(route_2, 128, 1)\n                    upsample_size = route_1.get_shape().as_list()\n                    inputs = _upsample(inputs, upsample_size, data_format)\n\n                    inputs = tf.concat([inputs, route_1],\n                                       axis=1 if data_format == \'NCHW\' else 3)\n\n                    inputs = _conv2d_fixed_padding(inputs, 256, 3)\n                    # inputs = _conv2d_fixed_padding(inputs, 255, 1)\n\n                    detect_2 = _detection_layer(\n                        inputs, num_classes, _ANCHORS[0:3], img_size, data_format)\n                    detect_2 = tf.identity(detect_2, name=\'detect_2\')\n\n                    detections = tf.concat([detect_1, detect_2], axis=1)\n                    detections = tf.identity(detections, name=\'detections\')\n                    return detections\n'"
pbmodels/tensorboard_log_output_tiny-yolov3.py,4,"b'import tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nwith tf.Session() as sess:\n    model_filename =""frozen_tiny_yolo_v3.pb""\n    with gfile.FastGFile(model_filename, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        g_in = tf.import_graph_def(graph_def)\n\n    LOGDIR=""logs/tiny-YoloV3""\n    train_writer = tf.summary.FileWriter(LOGDIR)\n    train_writer.add_graph(sess.graph)\n\n'"
pbmodels/tensorboard_log_output_yolov3.py,4,"b'import tensorflow as tf\nfrom tensorflow.python.platform import gfile\n\nwith tf.Session() as sess:\n    model_filename =""frozen_yolo_v3.pb""\n    with gfile.FastGFile(model_filename, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        g_in = tf.import_graph_def(graph_def)\n\n    LOGDIR=""logs/YoloV3""\n    train_writer = tf.summary.FileWriter(LOGDIR)\n    train_writer.add_graph(sess.graph)\n\n'"
