file_path,api_count,code
train_py/cnn_dataset_performance.py,5,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the network on an input image, input video, or entire dataset to analyze\n  performance.\n\'\'\'\nimport os\nimport argparse\nimport imp\nimport yaml\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_dataset_performance.py"")\n  parser.add_argument(\n      \'--dataset\',\n      type=str,\n      required=True,\n      help=\'Image to infer. No Default\',\n  )\n  parser.add_argument(\n      \'--batchsize\', \'-b\',\n      type=int,\n      required=False,\n      default=1,\n      help=\'Image to infer. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/net_predict_log\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'acc\', \'iou\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'iou\',\n      help=\'Type of model (best acc or best iou). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--data\', \'-d\',\n      type=str,\n      help=\'Dataset yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--net\', \'-n\',\n      type=str,\n      help=\'Network yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--train\', \'-t\',\n      type=str,\n      help=\'Training hyperparameters yaml cfg file. Defaults to the one in log dir\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Dataset: "", FLAGS.dataset)\n  print(""Batchsize: "", FLAGS.batchsize)\n  print(""Log dir: "", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""data yaml: "", FLAGS.data)\n  print(""net yaml: "", FLAGS.net)\n  print(""train yaml: "", FLAGS.train)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    if(FLAGS.data):\n      print(""Opening desired data file %s"" % FLAGS.data)\n      f = open(FLAGS.data, \'r\')\n    else:\n      print(""Opening default data file data.yaml from log folder"")\n      f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    if(FLAGS.net):\n      print(""Opening desired net file %s"" % FLAGS.net)\n      f = open(FLAGS.net, \'r\')\n    else:\n      print(""Opening default net file net.yaml from log folder"")\n      f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    if(FLAGS.train):\n      print(""Opening desired train file %s"" % FLAGS.train)\n      f = open(FLAGS.train, \'r\')\n    else:\n      print(""Opening default train file train.yaml from log folder"")\n      f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # try to get model\n  if tf.gfile.Exists(FLAGS.path + \'/\' + FLAGS.model):\n    print(""Model folder exists! Using model from %s"" %\n          (FLAGS.path + \'/\' + FLAGS.model))\n  else:\n    print(""Model does not exist"")\n    quit()\n\n  # try to get dataset\n  if tf.gfile.Exists(FLAGS.dataset):\n    print(""Dataset folder exists!"")\n  else:\n    print(""Model does not exist. Gimme data. Exiting..."")\n    quit()\n\n  # get architecture\n  architecture = imp.load_source(""architecture"",\n                                 os.getcwd() + \'/arch/\' +\n                                 NET[""name""] + \'.py\')\n\n  # build the network\n  net = architecture.Network(DATA, NET, TRAIN, FLAGS.log)\n\n  # handle ctrl-c for threads\n  signal.signal(signal.SIGINT, net.cleanup)\n  signal.signal(signal.SIGTERM, net.cleanup)\n  # signal.pause()\n\n  # create log dir\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # predict\n  ignore_crap = TRAIN[""ignore_crap""]\n  net.predict_dataset(FLAGS.dataset, path=FLAGS.path +\n                      \'/\' + FLAGS.model, batchsize=FLAGS.batchsize,\n                      ignore_last = ignore_crap)\n\n  # clean up\n  net.cleanup(None, None)\n'"
train_py/cnn_freeze.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Takes a trained model in the training format and turns it into a frozen pb.\n\'\'\'\nimport os\nimport argparse\nimport imp\nimport yaml\nimport signal\nfrom shutil import copyfile\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_freeze.py"")\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'acc\', \'iou\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'iou\',\n      help=\'Type of model (best acc or best iou). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/frozen_model\',\n      help=\'Directory to save the frozen graph. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--data\', \'-d\',\n      type=str,\n      help=\'Dataset yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--net\', \'-n\',\n      type=str,\n      help=\'Network yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--train\', \'-t\',\n      type=str,\n      help=\'Training hyperparameters yaml cfg file. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--verbose\', \'-v\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""output dir"", FLAGS.log)\n  print(""data yaml: "", FLAGS.data)\n  print(""net yaml: "", FLAGS.net)\n  print(""train yaml: "", FLAGS.train)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    if(FLAGS.data):\n      print(""Opening desired data file %s"" % FLAGS.data)\n      f = open(FLAGS.data, \'r\')\n      datafile = FLAGS.data\n    else:\n      print(""Opening default data file data.yaml from log folder"")\n      f = open(FLAGS.path + \'/data.yaml\', \'r\')\n      datafile = FLAGS.path + \'/data.yaml\'\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    if(FLAGS.net):\n      print(""Opening desired net file %s"" % FLAGS.net)\n      f = open(FLAGS.net, \'r\')\n      netfile = FLAGS.net\n    else:\n      print(""Opening default net file net.yaml from log folder"")\n      f = open(FLAGS.path + \'/net.yaml\', \'r\')\n      netfile = FLAGS.path + \'/net.yaml\'\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    if(FLAGS.train):\n      print(""Opening desired train file %s"" % FLAGS.train)\n      f = open(FLAGS.train, \'r\')\n      trainfile = FLAGS.train\n    else:\n      print(""Opening default train file train.yaml from log folder"")\n      f = open(FLAGS.path + \'/train.yaml\', \'r\')\n      trainfile = FLAGS.path + \'/train.yaml\'\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  if tf.gfile.Exists(FLAGS.path + \'/\' + FLAGS.model):\n    print(""model folder exists! Using model from %s"" %\n          (FLAGS.path + \'/\' + FLAGS.model))\n  else:\n    print(""model folder does not exist. Gimme dat model yo!"")\n    quit()\n\n  try:\n    print(""Creating log dir in"", FLAGS.log)\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # copy all files to log folder Also, standardize name to be able to open it later\n  try:\n    print(""Copying files to %s for further reference."" % FLAGS.log)\n    copyfile(datafile, FLAGS.log + ""/data.yaml"")\n    copyfile(netfile, FLAGS.log + ""/net.yaml"")\n    copyfile(trainfile, FLAGS.log + ""/train.yaml"")\n  except:\n    print(""Error copying files, check permissions. Exiting..."")\n    quit()\n\n  # get architecture\n  architecture = imp.load_source(\n      ""architecture"", os.getcwd() + \'/arch/\' + NET[""name""] + \'.py\')\n\n  # build the network\n  net = architecture.Network(DATA, NET, TRAIN, FLAGS.log)\n\n  # handle ctrl-c for threads\n  signal.signal(signal.SIGINT, net.cleanup)\n  signal.signal(signal.SIGTERM, net.cleanup)\n\n  # freeze the graph\n  path = os.path.join(FLAGS.path, FLAGS.model)\n  net.freeze_graph(path=path, verbose=FLAGS.verbose)\n\n  # clean up\n  net.cleanup(None, None)\n'"
train_py/cnn_graph_log.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Extracts a log file from a graph, so as to be able to be read by\n  tensorboard.\n\'\'\'\n\nimport tensorflow as tf\nimport argparse\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_graph_log.py"")\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      required=True,\n      help=\'Directory to log output of predictions.\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Path to the graph. No default!\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Model Path: "", FLAGS.path)\n  print(""Log dir: "", FLAGS.log)\n  print(""----------\\n"")\n\n  # define a graph\n  g = tf.Graph()\n\n  # fill it with the metagraph\n  with g.as_default() as g:\n    tf.train.import_meta_graph(FLAGS.path)\n\n  # save the log from that graph\n  with tf.Session(graph=g) as sess:\n    file_writer = tf.summary.FileWriter(logdir=FLAGS.log, graph=g)\n'"
train_py/cnn_plant_features.py,0,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\nimport cv2\nimport argparse\nimport dataset.aux_scripts.util as util\nimport dataset.plant_features as pf\nimport dataset.augment_data as ad\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser(""Feature Calculator"")\n  parser.add_argument(\n      \'--rgb\',\n      nargs=\'+\',\n      type=str,\n      help=\'path of rgb image\'\n  )\n  parser.add_argument(\n      \'--t\',\n      type=float,\n      help=\'Threshold to generate mask\',\n      default=0\n  )\n  filter_options = [""exgr"", ""exr"", ""cive"", ""ndi"", ""hsv"",\n                    ""gradients"", ""laplace"", ""edges"", ""water"", ""mask"", ""norm""]\n  parser.add_argument(\n      \'--filters\',\n      type=str,\n      nargs=\'+\',\n      help=\'Filters to apply\',\n      default=filter_options,\n      choices=filter_options\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # check that I have the inputs\n  if FLAGS.rgb is None:\n    print(""No input image. Gimme dat image"")\n    quit()\n\n  print(""---------------------------------------------------------------------"")\n  print(""ARGS:"")\n  print(""input rgb: "", FLAGS.rgb)\n  print(""thresh: "", FLAGS.t)\n  print(""filters: "", FLAGS.filters)\n  print(""---------------------------------------------------------------------"")\n\n  # opening rgb\n  for img in FLAGS.rgb:\n    print(""Opening RGB image"")\n    rgb_img = cv2.imread(img, cv2.IMREAD_UNCHANGED)  # open image\n    proper_h = 384\n    proper_w = 512\n    # resize to see how it works with kernels\n    rgb_img = ad.resize(rgb_img, [proper_h, proper_w])\n    if rgb_img is None:\n      print(""Rgb image doesn\'t exist"")\n      quit()\n\n    # get exgreen\n    exgr = None\n    exgr_mask = None\n    if ""exgr"" in FLAGS.filters:\n      exgr = pf.exgreen(rgb_img)\n      print(""Exgr shape: "", exgr.shape)\n      util.im_gray_plt(exgr, ""exgreen (%s)"" % img)\n      exgr_mask = pf.thresh(exgr, FLAGS.t)\n      util.im_gray_plt(exgr_mask, ""exgreen mask (%s)"" % img)\n      # util.hist_plot(exgr,""exgreen histogram (%s)""%img)\n\n    # get cive\n    if ""cive"" in FLAGS.filters:\n      c = pf.cive(rgb_img)\n      print(""cive shape: "", c.shape)\n      util.im_gray_plt(c, ""cive (%s)"" % img)\n      c_mask = pf.thresh(c, FLAGS.t)\n      util.im_gray_plt(c_mask, ""cive mask (%s)"" % img)\n      # util.hist_plot(c,""cive histogram (%s)""%img)\n\n    # get exred\n    if ""exr"" in FLAGS.filters:\n      exr = pf.exred(rgb_img)\n      print(""Exred shape: "", exr.shape)\n      util.im_gray_plt(exr, ""exred (%s)"" % img)\n      exr_mask = pf.thresh(exr, FLAGS.t)\n      util.im_gray_plt(exr_mask, ""exred mask (%s)"" % img)\n\n    # get ndi\n    if ""ndi"" in FLAGS.filters:\n      n = pf.ndi(rgb_img)\n      print(""NDI shape: "", n.shape)\n      util.im_gray_plt(n, ""ndi (%s)"" % img)\n      ndi_mask = pf.thresh(n, FLAGS.t)\n      util.im_gray_plt(ndi_mask, ""ndi mask (%s)"" % img)\n\n    # get hsv\n    if ""hsv"" in FLAGS.filters:\n      h = pf.hsv(rgb_img)\n      print(""HSV shape: "", h.shape)\n      # threshold hue\n      util.im_gray_plt(h[:, :, 0], ""hsv hue (%s)"" % img)\n      h_mask = pf.thresh(h[:, :, 0], FLAGS.t)\n      util.im_gray_plt(h_mask, ""hsv hue mask (%s)"" % img)\n\n    if ""gradients"" in FLAGS.filters:\n      # x gradient\n      if exgr is None:\n        exgr = pf.exgreen(rgb_img)\n      g = pf.gradients(exgr, \'x\')\n      print(""Gradient x shape: "", g.shape)\n      util.im_gray_plt(g, ""Gradient mask x (%s)"" % img)\n      # y gradient\n      g = pf.gradients(exgr, \'y\')\n      print(""Gradient y shape: "", g.shape)\n      util.im_gray_plt(g, ""Gradient mask y (%s)"" % img)\n\n    if ""laplace"" in FLAGS.filters:\n      if exgr is None:\n        exgr = pf.exgreen(rgb_img)\n      lplc = pf.laplacian(exgr)\n      print(""Laplacian shape: "", lplc.shape)\n      util.im_gray_plt(lplc, ""laplacian mask (%s)"" % img)\n\n    if ""edges"" in FLAGS.filters:\n      if exgr is None:\n        exgr = pf.exgreen(rgb_img)\n      e = pf.edges(exgr)\n      print(""Edge shape: "", e.shape)\n      util.im_gray_plt(e, ""edges mask (%s)"" % img)\n\n    if ""water"" in FLAGS.filters:\n      if exgr_mask is None:\n        exgr = pf.exgreen(rgb_img)\n        exgr_mask = pf.thresh(exgr, FLAGS.t)\n      w = pf.watershed(rgb_img, exgr, exgr_mask)\n      print(""Watershed shape: "", w.shape)\n      util.im_gray_plt(w, ""watershed (%s)"" % img)\n\n    if ""mask"" in FLAGS.filters:\n      if exgr_mask is None:\n        exgr = pf.exgreen(rgb_img)\n        exgr_mask = pf.thresh(exgr, FLAGS.t)\n      m = pf.mask_multidim(rgb_img, exgr_mask)\n      print(""m shape: "", m.shape)\n      util.im_plt(m, ""watershed (%s)"" % img)\n      mgray = pf.mask_multidim(exgr, exgr_mask)\n      print(""mgray shape: "", mgray.shape)\n      util.im_gray_plt(mgray, ""watershed (%s)"" % img)\n\n    if ""norm"" in FLAGS.filters:\n      # print(""rgb_img maxes: "",rgb_img[:,:,0].max(),rgb_img[:,:,1].max(),rgb_img[:,:,2].max())\n      # print(""rgb_img mins: "",rgb_img[:,:,0].min(),rgb_img[:,:,1].min(),rgb_img[:,:,2].min())\n      n = pf.chanelwise_norm(rgb_img)\n      print(""n shape: "", n.shape)\n      util.im_plt(n, ""normalized per channel (%s)"" % img)\n\n  # block thread until images are done\n  util.im_block()\n'"
train_py/cnn_train.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  App to train the desired architecture with the desired parameters, and on the\n  desired dataset.\n\'\'\'\n# os and file stuff\nimport os\nimport argparse\nimport datetime\nimport imp\nimport yaml\nfrom shutil import copyfile\nimport subprocess\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_train.py"")\n  parser.add_argument(\n      \'--data\', \'-d\',\n      type=str,\n      required=False,\n      help=\'Dataset yaml cfg file. See /cfg for sample. No default!\',\n  )\n  parser.add_argument(\n      \'--net\', \'-n\',\n      type=str,\n      required=False,\n      help=\'Network yaml cfg file. See /cfg for sample. No default!\',\n  )\n  parser.add_argument(\n      \'--train\', \'-t\',\n      type=str,\n      required=False,\n      help=\'Training hyperparameters yaml cfg file. See /cfg for sample. No default!\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=os.path.expanduser(""~"") + \'/logs/\' +\n      datetime.datetime.now().strftime(""%Y-%-m-%d-%H:%M"") + \'/\',\n      help=\'Directory to put the log data. Default: ~/logs/date+time\'\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=False,\n      default=None,\n      help=\'Directory to get the model. If not passed, do not retrain!\'\n  )\n  model_choices = [\'acc\', \'iou\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'iou\',\n      help=\'Type of model (best acc or best iou). Default to %(default)s\',\n      choices=model_choices\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""data yaml: "", FLAGS.data)\n  print(""net yaml: "", FLAGS.net)\n  print(""train yaml: "", FLAGS.train)\n  print(""log dir"", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""----------\\n"")\n  print(""Commit hash (training version): "", str(\n      subprocess.check_output([\'git\', \'rev-parse\', \'--short\', \'HEAD\']).strip()))\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    if(FLAGS.data):\n      print(""Opening desired data file %s"" % FLAGS.data)\n      f = open(FLAGS.data, \'r\')\n      datafile = FLAGS.data\n    else:\n      print(""Opening default data file data.yaml from log folder"")\n      f = open(FLAGS.path + \'/data.yaml\', \'r\')\n      datafile = FLAGS.path + \'/data.yaml\'\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file. Check! Exiting..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    if(FLAGS.net):\n      print(""Opening desired net file %s"" % FLAGS.net)\n      f = open(FLAGS.net, \'r\')\n      netfile = FLAGS.net\n    else:\n      print(""Opening default net file net.yaml from log folder"")\n      f = open(FLAGS.path + \'/net.yaml\', \'r\')\n      netfile = FLAGS.path + \'/net.yaml\'\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file. Check! Exiting..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    if(FLAGS.train):\n      print(""Opening desired train file %s"" % FLAGS.train)\n      f = open(FLAGS.train, \'r\')\n      trainfile = FLAGS.train\n    else:\n      print(""Opening default train file train.yaml from log folder"")\n      f = open(FLAGS.path + \'/train.yaml\', \'r\')\n      trainfile = FLAGS.path + \'/train.yaml\'\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file. Check! Exiting..."")\n    quit()\n\n  # create log folder\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # does model folder exist?\n  if FLAGS.path is not None:\n    if tf.gfile.Exists(FLAGS.path + \'/\' + FLAGS.model):\n      print(""model folder exists! Using model from %s"" %\n            (FLAGS.path + \'/\' + FLAGS.model))\n    else:\n      print(""model folder doesnt exist! Exiting..."")\n      quit()\n\n  # copy all files to log folder (to remember what we did, and make inference\n  # easier). Also, standardize name to be able to open it later\n  try:\n    print(""Copying files to %s for further reference."" % FLAGS.log)\n    copyfile(datafile, FLAGS.log + ""/data.yaml"")\n    copyfile(netfile, FLAGS.log + ""/net.yaml"")\n    copyfile(trainfile, FLAGS.log + ""/train.yaml"")\n  except:\n    print(""Error copying files, check permissions. Exiting..."")\n    quit()\n\n  # get architecture\n  architecture = imp.load_source(""architecture"",\n                                 os.getcwd() + \'/arch/\' +\n                                 NET[""name""] + \'.py\')\n\n  # build the network\n  net = architecture.Network(DATA, NET, TRAIN, FLAGS.log)\n\n  # handle ctrl-c for threads\n  signal.signal(signal.SIGINT, net.cleanup)\n  signal.signal(signal.SIGTERM, net.cleanup)\n  # signal.pause()\n\n  # train\n  if FLAGS.path is None:\n    print(""Training from scratch"")\n    net.train()\n  else:\n    print(""Training from model in "", str(FLAGS.path + \'/\' + FLAGS.model))\n    net.train(path=str(FLAGS.path + \'/\' + FLAGS.model))\n\n  # clean up\n  net.cleanup(None, None)\n'"
train_py/cnn_use.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained network on an input image.\n\'\'\'\nimport os\nimport argparse\nimport imp\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport numpy as np\nimport dataset.aux_scripts.util as util\nimport scipy.io as sio\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\n\ndef predict_mask(img, net, FLAGS, DATA):\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n  # predict mask from image\n  start = time.time()\n  mask = net.predict(cvim, path=FLAGS.path + \'/\' +\n                     FLAGS.model, verbose=FLAGS.verbose)\n  print(""Prediction for img "", img, "". Elapsed: "", time.time() - start, ""s"")\n  # change to color\n  color_mask = util.prediction_to_color(\n      mask, DATA[""label_remap""], DATA[""color_map""])\n\n  # assess accuracy (if wanted)\n  if FLAGS.label is not None:\n    label = cv2.imread(FLAGS.label, 0)\n    if label is None:\n      print(""No label to open"")\n      quit()\n    net.individual_accuracy(mask, label)\n\n  cv2.imwrite(FLAGS.log + ""/"" + os.path.basename(img), color_mask)\n\n  if FLAGS.verbose:\n    # show me the image\n    # first, mix with image\n    im, transparent_mask = util.transparency(cvim, color_mask)\n    all_img = np.concatenate((im, transparent_mask, color_mask), axis=1)\n    util.im_tight_plt(all_img)\n    util.im_block()\n\n  return\n\n\ndef predict_probs(img, net, FLAGS, DATA):\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n  # predict mask from image\n  start = time.time()\n  probs = net.predict(cvim, path=FLAGS.path + \'/\' +\n                      FLAGS.model, verbose=FLAGS.verbose, as_probs=True)\n  print(""Prediction for img "", img, "". Elapsed: "", time.time() - start, ""s"")\n\n  # save to matlab matrix\n  matname = FLAGS.log + ""/"" + \\\n      os.path.splitext(os.path.basename(img))[0] + "".mat""\n  sio.savemat(matname, {\'p\': probs})\n\n  return\n\n\ndef predict_code(img, net, FLAGS):\n  # predict feature map from image\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n  # predict mask from image\n  start = time.time()\n  code = net.predict_code(cvim, path=FLAGS.path + \'/\' +\n                          FLAGS.model, verbose=FLAGS.verbose)\n  print(""Prediction for img "", img, "". Elapsed: "", time.time() - start, ""s"")\n\n  # reshape code to single dimension\n  reshaped_code = np.reshape(code, (1, -1))\n  # print(""Shape"", reshaped_code.shape)\n\n  # save code to text file\n  filename = FLAGS.log + ""/"" + \\\n      os.path.splitext(os.path.basename(img))[0] + "".txt""\n  print(""Saving feature map to: "", filename)\n  np.savetxt(filename, reshaped_code, fmt=""%.8f"", delimiter="" "")\n\n  return\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_use.py"")\n  parser.add_argument(\n      \'--image\', \'-i\',\n      nargs=\'+\',\n      type=str,\n      required=True,\n      help=\'Image to infer. No Default\',\n  )\n  parser.add_argument(\n      \'--label\', \'--lbl\',\n      type=str,\n      required=False,\n      default=None,\n      help=\'Label to assess accuracy, if wanted. Only works for the first image\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/net_predict_log\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'acc\', \'iou\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'iou\',\n      help=\'Type of model (best acc or best iou). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--data\', \'-d\',\n      type=str,\n      help=\'Dataset yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--net\', \'-n\',\n      type=str,\n      help=\'Network yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--train\', \'-t\',\n      type=str,\n      help=\'Training hyperparameters yaml cfg file. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--verbose\', \'-v\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--code\', \'-c\',\n      dest=\'code\',\n      default=False,\n      action=\'store_true\',\n      help=\'Code mode. Calculates feature map instead of mask. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--probs\',\n      dest=\'probs\',\n      default=False,\n      action=\'store_true\',\n      help=\'Probability mode. Calculates probability map instead of mask. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Image to infer: "", FLAGS.image)\n  print(""Label: "", FLAGS.label)\n  print(""Log dir: "", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""data yaml: "", FLAGS.data)\n  print(""net yaml: "", FLAGS.net)\n  print(""train yaml: "", FLAGS.train)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""Features?: "", FLAGS.code)\n  print(""Probabilities?: "", FLAGS.probs)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    if(FLAGS.data):\n      print(""Opening desired data file %s"" % FLAGS.data)\n      f = open(FLAGS.data, \'r\')\n    else:\n      print(""Opening default data file data.yaml from log folder"")\n      f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    if(FLAGS.net):\n      print(""Opening desired net file %s"" % FLAGS.net)\n      f = open(FLAGS.net, \'r\')\n    else:\n      print(""Opening default net file net.yaml from log folder"")\n      f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    if(FLAGS.train):\n      print(""Opening desired train file %s"" % FLAGS.train)\n      f = open(FLAGS.train, \'r\')\n    else:\n      print(""Opening default train file train.yaml from log folder"")\n      f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # create log folder\n  if tf.gfile.Exists(FLAGS.path + \'/\' + FLAGS.model):\n    print(""Model folder exists! Using model from %s"" %\n          (FLAGS.path + \'/\' + FLAGS.model))\n\n  # get architecture\n  architecture = imp.load_source(""architecture"",\n                                 os.getcwd() + \'/arch/\' +\n                                 NET[""name""] + \'.py\')\n\n  # build the network\n  net = architecture.Network(DATA, NET, TRAIN, FLAGS.log)\n\n  # handle ctrl-c for threads\n  signal.signal(signal.SIGINT, net.cleanup)\n  signal.signal(signal.SIGTERM, net.cleanup)\n  # signal.pause()\n\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  if type(FLAGS.image) is not list:\n    images = [FLAGS.image]\n  else:\n    images = FLAGS.image\n\n  for img in images:\n    # predict\n    if FLAGS.code:\n      predict_code(img, net, FLAGS)\n    elif FLAGS.probs:\n      predict_probs(img, net, FLAGS, DATA)\n    else:\n      predict_mask(img, net, FLAGS, DATA)\n\n  # clean up\n  net.cleanup(None, None)\n'"
train_py/cnn_use_pb.py,12,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained frozen pb on an input image.\n\'\'\'\nimport os\nimport argparse\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport numpy as np\n\nimport dataset.aux_scripts.util as util\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\n\ndef predict_mask(img, sess, input, output, FLAGS, DATA):\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n  # predict mask from image\n  start = time.time()\n  mask = sess.run(output, feed_dict={input: [cvim]})\n  print(""Prediction for img "", img, "". Elapsed: "", time.time() - start, ""s"")\n  # change to color\n  color_mask = util.prediction_to_color(\n      mask[0, :, :], DATA[""label_remap""], DATA[""color_map""])\n\n  cv2.imwrite(FLAGS.log + ""/"" + os.path.basename(img), color_mask)\n\n  if FLAGS.verbose:\n    # show me the image\n    # first, mix with image\n    im, transparent_mask = util.transparency(cvim, color_mask)\n    all_img = np.concatenate((im, transparent_mask, color_mask), axis=1)\n    util.im_tight_plt(all_img)\n    util.im_block()\n\n  return\n\n\ndef predict_code(img, sess, input, output, FLAGS):\n  # predict feature map from image\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n\n  # predict code from image\n  print(""Prediction for img "", img)\n  code = sess.run(output, feed_dict={input: [cvim]})\n\n  # reshape code to single dimension\n  reshaped_code = np.reshape(code, (1, -1))\n  print(""Shape"", reshaped_code.shape)\n\n  # save code to text file\n  filename = FLAGS.log + ""/"" + \\\n      os.path.splitext(os.path.basename(img))[0] + "".txt""\n  print(""Saving feature map to: "", filename)\n  np.savetxt(filename, reshaped_code, fmt=""%.8f"", delimiter="" "")\n\n  return\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_use_pb.py"")\n  parser.add_argument(\n      \'--image\', \'-i\',\n      nargs=\'+\',\n      type=str,\n      required=True,\n      help=\'Image to infer. No Default\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/pb_predictions/\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'frozen_nchw\', \'frozen_nhwc\', \'optimized\', \'quantized\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'frozen_nchw\',\n      help=\'Type of model (frozen or optimized). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--verbose\', \'-v\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--code\', \'-c\',\n      dest=\'code\',\n      default=False,\n      action=\'store_true\',\n      help=\'Code mode. Calculates feature map instead of mask. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Image to infer: "", FLAGS.image)\n  print(""Log dir: "", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""Features?: "", FLAGS.code)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    print(""Opening default data file data.yaml from log folder"")\n    f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    print(""Opening default net file net.yaml from log folder"")\n    f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    print(""Opening default train file train.yaml from log folder"")\n    f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # try to open nodes yaml\n  try:\n    print(""Opening default nodes file nodes.yaml from log folder"")\n    f = open(FLAGS.path + \'/nodes.yaml\', \'r\')\n    NODES = yaml.load(f)\n  except:\n    print(""Error opening nodes yaml file..."")\n    quit()\n\n  frozen_name = os.path.join(FLAGS.path, FLAGS.model + "".pb"")\n  if tf.gfile.Exists(frozen_name):\n    print(""Model file exists! Using model from %s"" % (frozen_name))\n  else:\n    print(""Model not found. Exiting..."")\n    quit()\n\n  # create log folder\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # node names\n  input_node = NODES[""input_node""] + \':0\'\n  code_node = NODES[""code_node""] + \':0\'\n  mask_node = NODES[""mask_node""] + \':0\'\n\n  with tf.Graph().as_default() as graph:\n    # open graph def from frozen model\n    try:\n      with tf.gfile.GFile(frozen_name, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    except:\n      print(""Failed to extract grapfdef. Exiting..."")\n      quit()\n\n    # import the graph\n    pl, code, mask = tf.import_graph_def(graph_def, return_elements=[\n                                         input_node, code_node, mask_node])\n\n    # infer from pb\n    gpu_options = tf.GPUOptions(allow_growth=True, force_gpu_compatible=True)\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=False, gpu_options=gpu_options)\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_2\n\n    # start a session\n    sess = tf.Session(config=config)\n\n    # process images\n    if type(FLAGS.image) is not list:\n      images = [FLAGS.image]\n    else:\n      images = FLAGS.image\n\n    # use model for prediction\n    for img in images:\n      # predict\n      if FLAGS.code:\n        predict_code(img, sess, pl, code, FLAGS)\n      else:\n        predict_mask(img, sess, pl, mask, FLAGS, DATA)\n'"
train_py/cnn_use_pb_tensorRT.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained frozen pb on an input image.\n\'\'\'\nimport os\nimport argparse\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport numpy as np\n\nimport dataset.aux_scripts.util as util\n\n# tensorRT stuff\nimport pycuda.driver as cuda\nimport tensorrt as trt\nfrom tensorrt.parsers import uffparser\nimport uff\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\n\n\ndef predict_mask(img, stream, d_input, d_output, context, FLAGS, DATA):\n  # open image\n  cvim = cv2.imread(img, cv2.IMREAD_UNCHANGED).astype(np.float32)\n  if cvim is None:\n    print(""No image to open for "", img)\n    return\n\n  cvim = cv2.resize(cvim, (DATA[\'img_prop\'][\'width\'],\n                           DATA[\'img_prop\'][\'height\']), interpolation=cv2.INTER_LINEAR)\n  tcvim = np.transpose(cvim, axes=(2, 0, 1))\n  tcvim = tcvim.copy(order=\'C\')\n  tcvim = (tcvim - 128.0) / 128.0\n\n  # Bindings provided as pointers to the GPU memory.\n  # PyCUDA lets us do this for memory allocations by\n  # casting those allocations to ints\n  bindings = [int(d_input), int(d_output)]\n\n  # allocate memory on the CPU to hold results after inference\n  output = np.empty((len(DATA[\'label_map\']), DATA[\'img_prop\'][\'height\'],\n                     DATA[\'img_prop\'][\'width\']), dtype=np.float32, order=\'C\')\n\n  # predict mask from image\n  start = time.time()\n  cuda.memcpy_htod_async(d_input, tcvim, stream)\n  # execute model\n  context.enqueue(1, bindings, stream.handle, None)\n  # transfer predictions back\n  cuda.memcpy_dtoh_async(output, d_output, stream)\n  # syncronize threads\n  stream.synchronize()\n  print(""Prediction for img "", img, "". Elapsed: "", time.time() - start, ""s"")\n\n  # mask from logits\n  mask = np.argmax(output, axis=0)\n\n  # change to color\n  color_mask = util.prediction_to_color(mask, DATA[""label_remap""],\n                                        DATA[""color_map""])\n\n  # save to log folder\n  cv2.imwrite(FLAGS.log + ""/"" + os.path.basename(img), color_mask)\n\n  if FLAGS.verbose:\n    # show me the image\n    # first, mix with image\n    im, transparent_mask = util.transparency(cvim, color_mask)\n    all_img = np.concatenate((im, transparent_mask, color_mask), axis=1)\n    util.im_tight_plt(all_img)\n    util.im_block()\n\n  return\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_use_pb_tensorRT.py"")\n  parser.add_argument(\n      \'--image\', \'-i\',\n      nargs=\'+\',\n      type=str,\n      required=True,\n      help=\'Image to infer. No Default\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/pb_tRT_predictions/\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  model_choices = [\'FP32\', \'FP16\']\n  parser.add_argument(\n      \'--precision\',\n      type=str,\n      default=\'FP32\',\n      help=\'Precision for calculations (FP32, FP16). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  parser.add_argument(\n      \'--verbose\', \'-v\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Image to infer: "", FLAGS.image)\n  print(""Log dir: "", FLAGS.log)\n  print(""Precision: "", FLAGS.precision)\n  print(""model path"", FLAGS.path)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    print(""Opening default data file data.yaml from log folder"")\n    f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    print(""Opening default net file net.yaml from log folder"")\n    f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    print(""Opening default train file train.yaml from log folder"")\n    f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # try to open nodes yaml\n  try:\n    print(""Opening default nodes file nodes.yaml from log folder"")\n    f = open(FLAGS.path + \'/nodes.yaml\', \'r\')\n    NODES = yaml.load(f)\n  except:\n    print(""Error opening nodes yaml file..."")\n    quit()\n\n  frozen_name = os.path.join(FLAGS.path, ""optimized_tRT.pb"")\n  if tf.gfile.Exists(frozen_name):\n    print(""Model file exists! Using model from %s"" % (frozen_name))\n  else:\n    print(""Model not found. Exiting..."")\n    quit()\n\n  # create log folder\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # node names\n  input_node = NODES[""input_norm_and_resized_node""]\n  mask_node = NODES[""logits_node""]\n  output_nodes = [mask_node]\n  input_nodes = [input_node]\n\n  # import uff from tensorflow frozen\n  uff_model = uff.from_tensorflow_frozen_model(frozen_name,\n                                               output_nodes,\n                                               input_nodes=input_nodes)\n\n  # creating a logger for TensorRT\n  G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)\n\n  # create a uff parser\n  parser = uffparser.create_uff_parser()\n  parser.register_input(input_node, (DATA[\'img_prop\'][\'depth\'],\n                                     DATA[\'img_prop\'][\'height\'],\n                                     DATA[\'img_prop\'][\'width\']), 0)\n  parser.register_output(mask_node)\n\n  # pass the logger, parser and the uff model stream and some settings to create the engine\n  MAX_ALLOWED_BATCH_SIZE = 1\n  MAX_ALLOWED_WS_SIZE = 1 << 20\n  if FLAGS.precision == ""FP32"":\n    DATA_TYPE = trt.infer.DataType.FLOAT\n  elif FLAGS.precision == ""FP16"":\n    DATA_TYPE = trt.infer.DataType.HALF\n\n  engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser,\n                                       MAX_ALLOWED_BATCH_SIZE,\n                                       MAX_ALLOWED_WS_SIZE,\n                                       DATA_TYPE)  # .HALF for fp16 in jetson!\n\n  # get rid of the parser\n  parser.destroy()\n\n  # create a runtime and an execution context for the engine\n  runtime = trt.infer.create_infer_runtime(G_LOGGER)\n  context = engine.create_execution_context()\n\n  # alocate device memory\n  input_size = DATA[\'img_prop\'][\'depth\'] * DATA[\'img_prop\'][\'height\'] * \\\n      DATA[\'img_prop\'][\'width\']\n  output_size = len(DATA[\'label_map\']) * DATA[\'img_prop\'][\'height\'] * \\\n      DATA[\'img_prop\'][\'width\']\n  d_input = cuda.mem_alloc(1 * input_size * 4)\n  d_output = cuda.mem_alloc(1 * output_size * 4)\n\n  # cuda stream to run inference in.\n  stream = cuda.Stream()\n\n  # process images\n  if type(FLAGS.image) is not list:\n    images = [FLAGS.image]\n  else:\n    images = FLAGS.image\n\n  # use model for prediction\n  for img in images:\n    predict_mask(img, stream, d_input, d_output, context, FLAGS, DATA)\n\n  # Save the engine to a file to use later. Use this engine by using tensorrt.utils.load_engine\n  trt.utils.write_engine_to_file(\n      FLAGS.log + ""pb-to-tRT.engine"", engine.serialize())\n\n  # Example use engine\n  # new_engine = trt.utils.load_engine(G_LOGGER, ""./tf_mnist.engine"")\n\n  # Clean up context, engine and runtime\n  context.destroy()\n  engine.destroy()\n  runtime.destroy()\n'"
train_py/cnn_video.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained network on an input video.\n\'\'\'\nimport os\nimport argparse\nimport imp\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport skvideo.io as skio\nimport numpy as np\nimport dataset.aux_scripts.util as util\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\n\ndef predict_mask(cvim, frame, net, FLAGS, DATA):\n  # predict mask from image\n  cvim = cv2.cvtColor(cvim, cv2.COLOR_RGB2BGR)\n  start = time.time()\n  mask = net.predict(cvim, path=FLAGS.path + \'/\' +\n                     FLAGS.model, verbose=FLAGS.verbose)\n  elapsed = time.time() - start\n  print(""Prediction for frame "", frame, "". Elapsed: "", elapsed, ""s"")\n\n  # change to color\n  color_mask = util.prediction_to_color(\n      mask, DATA[""label_remap""], DATA[""color_map""])\n  im, transparent_mask = util.transparency(cvim, color_mask)\n  all_img = np.concatenate((im, transparent_mask), axis=1)\n  w, h, _ = all_img.shape\n  watermark = ""Time: {:.3f}s, FPS: {:.3f}img/s."".format(elapsed, 1 / elapsed)\n  cv2.putText(all_img, watermark,\n              org=(10, w - 10),\n              fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n              fontScale=0.75,\n              color=(255, 255, 255),\n              thickness=2,\n              lineType=cv2.LINE_AA)\n\n  # write to disk\n  cv2.imwrite(FLAGS.log + ""/mask_"" + frame + "".jpg"", color_mask)\n  cv2.imwrite(FLAGS.log + ""/full_"" + frame + "".jpg"", all_img)\n\n  # show me the image\n  cv2.imshow(""video"", all_img.astype(np.uint8))\n  ch = cv2.waitKey(1)\n\n  return ch\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_video.py"")\n  parser.add_argument(\n      \'--video\', \'-v\',\n      type=str,\n      required=False,\n      default="""",\n      help=\'Video to infer.\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/net_predict_log\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'acc\', \'iou\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'iou\',\n      help=\'Type of model (best acc or best iou). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--data\', \'-d\',\n      type=str,\n      help=\'Dataset yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--net\', \'-n\',\n      type=str,\n      help=\'Network yaml cfg file. See /cfg for sample. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--train\', \'-t\',\n      type=str,\n      help=\'Training hyperparameters yaml cfg file. Defaults to the one in log dir\',\n  )\n  parser.add_argument(\n      \'--verbose\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Video to infer: "", FLAGS.video)\n  print(""Log dir: "", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""data yaml: "", FLAGS.data)\n  print(""net yaml: "", FLAGS.net)\n  print(""train yaml: "", FLAGS.train)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    if(FLAGS.data):\n      print(""Opening desired data file %s"" % FLAGS.data)\n      f = open(FLAGS.data, \'r\')\n    else:\n      print(""Opening default data file data.yaml from log folder"")\n      f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    if(FLAGS.net):\n      print(""Opening desired net file %s"" % FLAGS.net)\n      f = open(FLAGS.net, \'r\')\n    else:\n      print(""Opening default net file net.yaml from log folder"")\n      f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    if(FLAGS.train):\n      print(""Opening desired train file %s"" % FLAGS.train)\n      f = open(FLAGS.train, \'r\')\n    else:\n      print(""Opening default train file train.yaml from log folder"")\n      f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # create log folder\n  if tf.gfile.Exists(FLAGS.path + \'/\' + FLAGS.model):\n    print(""model folder exists! Using model from %s"" %\n          (FLAGS.path + \'/\' + FLAGS.model))\n\n  # get architecture\n  architecture = imp.load_source(""architecture"",\n                                 os.getcwd() + \'/arch/\' +\n                                 NET[""name""] + \'.py\')\n\n  # build the network\n  net = architecture.Network(DATA, NET, TRAIN, FLAGS.log)\n\n  # handle ctrl-c for threads\n  signal.signal(signal.SIGINT, net.cleanup)\n  signal.signal(signal.SIGTERM, net.cleanup)\n  # signal.pause()\n\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # create resizeable window\n  cv2.namedWindow(""video"", cv2.WINDOW_NORMAL)\n\n  # open video capture\n  if FLAGS.video is """":\n    print(""Webcam reading not implemented. Exiting"")\n    quit()\n  else:\n    inputparameters = {}\n    outputparameters = {}\n    reader = skio.FFmpegReader(FLAGS.video,\n                               inputdict=inputparameters,\n                               outputdict=outputparameters)\n\n    i = 0\n    for frame in reader.nextFrame():\n      # predict\n      ch = predict_mask(frame, str(i), net, FLAGS, DATA)\n      if ch == 27:\n        break\n      # add to frame nr.\n      i += 1\n    # clean up\n    cv2.destroyAllWindows()\n  net.cleanup(None, None)\n'"
train_py/cnn_video_pb.py,12,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained frozen pb on an input video.\n\'\'\'\nimport os\nimport argparse\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport skvideo.io as skio\nimport numpy as np\n\nimport dataset.aux_scripts.util as util\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\nimport signal\n\n\ndef predict_mask(cvim, frame, sess, input, output, FLAGS, DATA):\n  # predict mask from image\n  cvim = cv2.cvtColor(cvim, cv2.COLOR_RGB2BGR)\n  start = time.time()\n  mask = sess.run(output, feed_dict={input: [cvim]})\n  elapsed = time.time() - start\n  print(""Prediction for frame "", frame, "". Elapsed: "", elapsed, ""s"")\n\n  # change to color\n  color_mask = util.prediction_to_color(\n      mask[0, :, :], DATA[""label_remap""], DATA[""color_map""])\n  im, transparent_mask = util.transparency(cvim, color_mask)\n  all_img = np.concatenate((im, transparent_mask), axis=1)\n  w, h, _ = all_img.shape\n  watermark = ""Time: {:.3f}s, FPS: {:.3f}img/s."".format(elapsed, 1 / elapsed)\n  cv2.putText(all_img, watermark,\n              org=(10, w - 10),\n              fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n              fontScale=0.75,\n              color=(255, 255, 255),\n              thickness=2,\n              lineType=cv2.LINE_AA)\n\n  # write to disk\n  cv2.imwrite(FLAGS.log + ""/mask_"" + frame + "".jpg"", color_mask)\n  cv2.imwrite(FLAGS.log + ""/full_"" + frame + "".jpg"", all_img)\n\n  # show me the image\n  cv2.imshow(""video"", all_img.astype(np.uint8))\n  ch = cv2.waitKey(1)\n\n  return ch\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_video_pb.py"")\n  parser.add_argument(\n      \'--video\', \'-v\',\n      type=str,\n      required=False,\n      default="""",\n      help=\'Video to infer.\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/pb_predictions/\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  model_choices = [\'frozen_nchw\', \'frozen_nhwc\', \'optimized\', \'quantized\']\n  parser.add_argument(\n      \'--model\', \'-m\',\n      type=str,\n      default=\'frozen_nchw\',\n      help=\'Type of model (frozen or optimized). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--verbose\',\n      dest=\'verbose\',\n      default=False,\n      action=\'store_true\',\n      help=\'Verbose mode. Calculates profile. Defaults to %(default)s\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Video to infer: "", FLAGS.video)\n  print(""Log dir: "", FLAGS.log)\n  print(""model path"", FLAGS.path)\n  print(""model type"", FLAGS.model)\n  print(""Verbose?: "", FLAGS.verbose)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    print(""Opening default data file data.yaml from log folder"")\n    f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    print(""Opening default net file net.yaml from log folder"")\n    f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    print(""Opening default train file train.yaml from log folder"")\n    f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # try to open nodes yaml\n  try:\n    print(""Opening default nodes file nodes.yaml from log folder"")\n    f = open(FLAGS.path + \'/nodes.yaml\', \'r\')\n    NODES = yaml.load(f)\n  except:\n    print(""Error opening nodes yaml file..."")\n    quit()\n\n  frozen_name = os.path.join(FLAGS.path, FLAGS.model + "".pb"")\n  if tf.gfile.Exists(frozen_name):\n    print(""Model file exists! Using model from %s"" % (frozen_name))\n  else:\n    print(""Model not found. Exiting..."")\n    quit()\n\n  # create log folder\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # node names\n  input_node = NODES[""input_node""] + \':0\'\n  code_node = NODES[""code_node""] + \':0\'\n  mask_node = NODES[""mask_node""] + \':0\'\n\n  with tf.Graph().as_default() as graph:\n    # open graph def from frozen model\n    try:\n      with tf.gfile.GFile(frozen_name, ""rb"") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n    except:\n      print(""Failed to extract grapfdef. Exiting..."")\n      quit()\n\n    # import the graph\n    pl, code, mask = tf.import_graph_def(graph_def, return_elements=[\n                                         input_node, code_node, mask_node])\n\n    # infer from pb\n    gpu_options = tf.GPUOptions(allow_growth=True, force_gpu_compatible=True)\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=False, gpu_options=gpu_options)\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_2\n\n    # start a session\n    sess = tf.Session(config=config)\n\n    # create resizeable window\n    cv2.namedWindow(""video"", cv2.WINDOW_NORMAL)\n\n    # open video capture\n    if FLAGS.video is """":\n      print(""Webcam reading not implemented. Exiting"")\n      quit()\n    else:\n      inputparameters = {}\n      outputparameters = {}\n      reader = skio.FFmpegReader(FLAGS.video,\n                                 inputdict=inputparameters,\n                                 outputdict=outputparameters)\n\n      i = 0\n      for frame in reader.nextFrame():\n        # predict\n        ch = predict_mask(frame, str(i), sess, pl, mask, FLAGS, DATA)\n        if ch == 27:\n          break\n        # add to frame nr.\n        i += 1\n      # clean up\n      cv2.destroyAllWindows()\n'"
train_py/cnn_video_pb_tensorRT.py,4,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the trained frozen pb on an input image.\n\'\'\'\nimport os\nimport argparse\nimport yaml\nimport time\n\n# image plot stuff\nimport cv2\nimport skvideo.io as skio\nimport numpy as np\n\nimport dataset.aux_scripts.util as util\n\n# tensorRT stuff\nimport pycuda.driver as cuda\nimport tensorrt as trt\nfrom tensorrt.parsers import uffparser\nimport uff\n\n# tensorflow stuff\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'  # shut up TF!\nimport tensorflow as tf\n\n\ndef predict_mask(cvim, frame, stream, d_input, d_output, context, FLAGS, DATA):\n  # do all required transpositions\n  cvim = cv2.cvtColor(cvim, cv2.COLOR_RGB2BGR)\n  cvim = cv2.resize(cvim.astype(np.float32), (DATA[\'img_prop\'][\'width\'],\n                                              DATA[\'img_prop\'][\'height\']),\n                    interpolation=cv2.INTER_LINEAR)\n  tcvim = np.transpose(cvim, axes=(2, 0, 1))\n  tcvim = tcvim.copy(order=\'C\')\n  tcvim = (tcvim - 128.0) / 128.0\n\n  # Bindings provided as pointers to the GPU memory.\n  # PyCUDA lets us do this for memory allocations by\n  # casting those allocations to ints\n  bindings = [int(d_input), int(d_output)]\n\n  # allocate memory on the CPU to hold results after inference\n  output = np.empty((len(DATA[\'label_map\']), DATA[\'img_prop\'][\'height\'],\n                     DATA[\'img_prop\'][\'width\']), dtype=np.float32, order=\'C\')\n\n  # predict mask from image\n  start = time.time()\n  cuda.memcpy_htod_async(d_input, tcvim, stream)\n  # execute model\n  context.enqueue(1, bindings, stream.handle, None)\n  # transfer predictions back\n  cuda.memcpy_dtoh_async(output, d_output, stream)\n  # syncronize threads\n  stream.synchronize()\n  elapsed = time.time() - start\n  print(""Prediction for frame "", frame, "". Elapsed: "", elapsed, ""s"")\n\n  # mask from logits\n  mask = np.argmax(output, axis=0)\n\n  # change to color\n  color_mask = util.prediction_to_color(mask, DATA[""label_remap""],\n                                        DATA[""color_map""])\n\n  # transparent\n  im, transparent_mask = util.transparency(cvim, color_mask)\n  all_img = np.concatenate((cvim, transparent_mask), axis=1)\n  w, h, _ = all_img.shape\n  watermark = ""Time: {:.3f}s, FPS: {:.3f}img/s."".format(elapsed, 1 / elapsed)\n  cv2.putText(all_img, watermark,\n              org=(10, w - 10),\n              fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n              fontScale=0.75,\n              color=(255, 255, 255),\n              thickness=2,\n              lineType=cv2.LINE_AA)\n\n  # write to disk\n  cv2.imwrite(FLAGS.log + ""/mask_"" + frame + "".jpg"", color_mask)\n  cv2.imwrite(FLAGS.log + ""/full_"" + frame + "".jpg"", all_img)\n\n  # show me the image\n  cv2.imshow(""video"", all_img.astype(np.uint8))\n  ch = cv2.waitKey(1)\n\n  return ch\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cnn_video_pb_tensorRT.py"")\n  parser.add_argument(\n      \'--video\', \'-v\',\n      type=str,\n      required=False,\n      default="""",\n      help=\'Video to infer.\',\n  )\n  parser.add_argument(\n      \'--log\', \'-l\',\n      type=str,\n      default=\'/tmp/pb_tRT_predictions/\',\n      help=\'Directory to log output of predictions. Defaults to %(default)s\',\n  )\n  model_choices = [\'FP32\', \'FP16\']\n  parser.add_argument(\n      \'--precision\',\n      type=str,\n      default=\'FP32\',\n      help=\'Precision for calculations (FP32, FP16). Default to %(default)s\',\n      choices=model_choices\n  )\n  parser.add_argument(\n      \'--path\', \'-p\',\n      type=str,\n      required=True,\n      help=\'Directory to get the model. No default!\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""Video to infer: "", FLAGS.video)\n  print(""Log dir: "", FLAGS.log)\n  print(""Precision: "", FLAGS.precision)\n  print(""model path"", FLAGS.path)\n  print(""----------\\n"")\n\n  # try to open data yaml\n  try:\n    print(""Opening default data file data.yaml from log folder"")\n    f = open(FLAGS.path + \'/data.yaml\', \'r\')\n    DATA = yaml.load(f)\n  except:\n    print(""Error opening data yaml file..."")\n    quit()\n\n  # try to open net yaml\n  try:\n    print(""Opening default net file net.yaml from log folder"")\n    f = open(FLAGS.path + \'/net.yaml\', \'r\')\n    NET = yaml.load(f)\n  except:\n    print(""Error opening net yaml file..."")\n    quit()\n\n  # try to open train yaml\n  try:\n    print(""Opening default train file train.yaml from log folder"")\n    f = open(FLAGS.path + \'/train.yaml\', \'r\')\n    TRAIN = yaml.load(f)\n  except:\n    print(""Error opening train yaml file..."")\n    quit()\n\n  # try to open nodes yaml\n  try:\n    print(""Opening default nodes file nodes.yaml from log folder"")\n    f = open(FLAGS.path + \'/nodes.yaml\', \'r\')\n    NODES = yaml.load(f)\n  except:\n    print(""Error opening nodes yaml file..."")\n    quit()\n\n  frozen_name = os.path.join(FLAGS.path, ""optimized_tRT.pb"")\n  if tf.gfile.Exists(frozen_name):\n    print(""Model file exists! Using model from %s"" % (frozen_name))\n  else:\n    print(""Model not found. Exiting..."")\n    quit()\n\n  # create log folder\n  try:\n    if tf.gfile.Exists(FLAGS.log):\n      tf.gfile.DeleteRecursively(FLAGS.log)\n    tf.gfile.MakeDirs(FLAGS.log)\n  except:\n    print(""Error creating log directory. Check permissions! Exiting..."")\n    quit()\n\n  # node names\n  input_node = NODES[""input_norm_and_resized_node""]\n  mask_node = NODES[""logits_node""]\n  output_nodes = [mask_node]\n  input_nodes = [input_node]\n\n  # import uff from tensorflow frozen\n  uff_model = uff.from_tensorflow_frozen_model(frozen_name,\n                                               output_nodes,\n                                               input_nodes=input_nodes)\n\n  # creating a logger for TensorRT\n  G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)\n\n  # create a uff parser\n  parser = uffparser.create_uff_parser()\n  parser.register_input(input_node, (DATA[\'img_prop\'][\'depth\'],\n                                     DATA[\'img_prop\'][\'height\'],\n                                     DATA[\'img_prop\'][\'width\']), 0)\n  parser.register_output(mask_node)\n\n  # pass the logger, parser and the uff model stream and some settings to create the engine\n  MAX_ALLOWED_BATCH_SIZE = 1\n  MAX_ALLOWED_WS_SIZE = 1 << 20\n  if FLAGS.precision == ""FP32"":\n    DATA_TYPE = trt.infer.DataType.FLOAT\n  elif FLAGS.precision == ""FP16"":\n    DATA_TYPE = trt.infer.DataType.HALF\n\n  engine = trt.utils.uff_to_trt_engine(G_LOGGER, uff_model, parser,\n                                       MAX_ALLOWED_BATCH_SIZE,\n                                       MAX_ALLOWED_WS_SIZE,\n                                       DATA_TYPE)  # .HALF for fp16 in jetson!\n\n  # get rid of the parser\n  parser.destroy()\n\n  # create a runtime and an execution context for the engine\n  runtime = trt.infer.create_infer_runtime(G_LOGGER)\n  context = engine.create_execution_context()\n\n  # alocate device memory\n  input_size = DATA[\'img_prop\'][\'depth\'] * DATA[\'img_prop\'][\'height\'] * \\\n      DATA[\'img_prop\'][\'width\']\n  output_size = len(DATA[\'label_map\']) * DATA[\'img_prop\'][\'height\'] * \\\n      DATA[\'img_prop\'][\'width\']\n  d_input = cuda.mem_alloc(1 * input_size * 4)\n  d_output = cuda.mem_alloc(1 * output_size * 4)\n\n  # cuda stream to run inference in.\n  stream = cuda.Stream()\n\n  # create resizeable window\n  cv2.namedWindow(""video"", cv2.WINDOW_NORMAL)\n\n  # open video capture\n  if FLAGS.video is """":\n    print(""Webcam reading not implemented. Exiting"")\n    quit()\n  else:\n    inputparameters = {}\n    outputparameters = {}\n    reader = skio.FFmpegReader(FLAGS.video,\n                               inputdict=inputparameters,\n                               outputdict=outputparameters)\n\n    i = 0\n    for frame in reader.nextFrame():\n      # predict\n      ch = predict_mask(frame, str(i), stream, d_input,\n                        d_output, context, FLAGS, DATA)\n      if ch == 27:\n        break\n      # add to frame nr.\n      i += 1\n    # clean up\n    cv2.destroyAllWindows()\n\n  # Save the engine to a file to use later. Use this engine by using tensorrt.utils.load_engine\n  trt.utils.write_engine_to_file(\n      FLAGS.log + ""pb-to-tRT.engine"", engine.serialize())\n\n  # Example use engine\n  # new_engine = trt.utils.load_engine(G_LOGGER, ""./tf_mnist.engine"")\n\n  # Clean up context, engine and runtime\n  context.destroy()\n  engine.destroy()\n  runtime.destroy()\n'"
train_py/arch/abstract_net.py,145,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Network class, containing:\n    - Training steps and training procedure\n    - Checkpoint saver and restorer\n    - Function to predict mask from image\n    - etc :)\n\n  API Style should be the same for all nets\n\'\'\'\n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.tools import freeze_graph\nfrom tensorflow.tools.graph_transforms import TransformGraph\nimport numpy as np\nimport cv2\nimport imp\nimport os\nimport time\nimport sys\nimport yaml\nimport dataset.augment_data as ad\nimport dataset.aux_scripts.util as util\nimport arch.msg as msg\n\n\nclass AbstractNetwork:\n  def __init__(self, DATA, NET, TRAIN, logdir):\n    # init\n    self.DATA = DATA      # dictionary with dataset parameters\n    self.NET = NET        # dictionary with network parameters\n    self.TRAIN = TRAIN    # dictionary with training hyperparams\n    self.log = logdir     # where to put the log for training\n    self.sess = None      # session (no session until needed)\n    self.code_valid = None  # if this is not defined in the graph, we need to complain\n\n  def build_graph(self, train_stage, data_format=""NCHW""):\n    # some graph info depending on what I will do with it\n    print(""This needs to be re-implemented in each arch. Exiting..."")\n    quit()\n    return\n\n  def resize_label(self, lbls_pl):\n    """""" Resize the y pl to fit the image for loss and confusion matrix\n    """"""\n    # reshape label\n    lbls_pl_exp = tf.expand_dims(lbls_pl, -1)\n    lbls_resized = tf.image.resize_images(lbls_pl_exp,\n                                          [self.DATA[""img_prop""][""height""],\n                                           self.DATA[""img_prop""][""width""]],\n                                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    lbls_resized = tf.reshape(lbls_resized, [self.batch_size_gpu,\n                                             self.DATA[""img_prop""][""height""],\n                                             self.DATA[""img_prop""][""width""]])\n    return lbls_resized\n\n  def loss_f(self, lbls_pl, logits_train, gamma_focal=2, w_t=""log"", w_d=1e-4):\n    """"""Calculates the loss from the logits and the labels.\n    """"""\n    print(""Defining loss function"")\n    with tf.variable_scope(""loss""):\n      lbls_resized = self.resize_label(lbls_pl)\n\n      # Apply median freq balancing (median frec / freq(class))\n      w = np.empty(len(self.dataset.train.content))\n\n      if w_t == ""log"":\n        # get the frequencies and weights\n        for key in self.dataset.train.content:\n          e = 1.02  # max weight = 50\n          f_c = self.dataset.train.content[key]\n          w[self.DATA[""label_remap""][key]] = 1 / np.log(f_c + e)\n        print(""\\nWeights for loss function (1/log(frec(c)+e)):\\n"", w)\n\n      elif w_t == ""median_freq"":\n        # get the frequencies\n        f = np.empty(len(self.dataset.train.content))\n        for key in self.dataset.train.content:\n          e = 0.001\n          f_c = self.dataset.train.content[key]\n          f[self.DATA[""label_remap""][key]] = f_c\n          w[self.DATA[""label_remap""][key]] = 1 / (f_c + e)\n\n        # calculate the median frequencies and normalize\n        median_freq = np.median(f)\n        print(""\\nFrequencies of classes:\\n"", f)\n        print(""\\nMedian freq:\\n"", median_freq)\n        print(""\\nWeights for loss function (1/frec(c)):\\n"", w)\n        w = median_freq * w\n        print(""\\nWeights for loss function (median frec/frec(c)):\\n"", w)\n      else:\n        print(""Using natural weights, since no valid loss option was given."")\n        w.fill(1.0)\n        for key in self.dataset.train.content:\n          if self.dataset.train.content[key] == float(""inf""):\n            w[self.DATA[""label_remap""][key]] = 0\n        print(""weights: "", w)\n\n      # use class weights as tf constant\n      w_tf = tf.constant(w, dtype=tf.float32, name=\'class_weights\')\n      w_mask = w.astype(np.bool).astype(np.float32)\n      w_mask_tf = tf.constant(w_mask, dtype=tf.float32,\n                              name=\'class_weights_mask\')\n\n      # make logits softmax matrixes for loss\n      loss_epsilon = tf.constant(value=1e-10)\n      softmax = tf.nn.softmax(logits_train)\n      softmax_mat = tf.reshape(softmax, (-1, self.num_classes))\n      zerohot_softmax_mat = 1 - softmax_mat\n\n      # make the labels one-hot for the cross-entropy\n      onehot_mat = tf.reshape(tf.one_hot(lbls_resized, self.num_classes),\n                              (-1, self.num_classes))\n\n      # make the zero hot to punish the false negatives, but ignore the\n      # zero-weight classes\n      masked_sum = tf.reduce_sum(onehot_mat * w_mask_tf, axis=1)\n      zeros = onehot_mat * 0.0\n      zerohot_mat = tf.where(tf.less(masked_sum, 1e-5),\n                             x=zeros,\n                             y=1 - onehot_mat)\n\n      # focal loss p and gamma\n      gamma = np.full(onehot_mat.get_shape().as_list(), fill_value=gamma_focal)\n      gamma_tf = tf.constant(gamma, dtype=tf.float32)\n      focal_softmax = tf.pow(1 - softmax_mat, gamma_tf) * \\\n          tf.log(softmax_mat + loss_epsilon)\n      zerohot_focal_softmax = tf.pow(1 - zerohot_softmax_mat, gamma_tf) * \\\n          tf.log(zerohot_softmax_mat + loss_epsilon)\n\n      # calculate xentropy\n      cross_entropy = - tf.reduce_sum(tf.multiply(focal_softmax * onehot_mat +\n                                                  zerohot_focal_softmax * zerohot_mat, w_tf),\n                                      axis=[1])\n\n      loss = tf.reduce_mean(cross_entropy, name=\'xentropy_mean\')\n\n      # weight decay\n      print(""Weight decay: "", w_d)\n      w_d_tf = tf.constant(w_d, dtype=tf.float32, name=\'weight_decay\')\n      variables = tf.trainable_variables(scope=""model"")\n      for var in variables:\n        if ""weights"" in var.name:\n          loss += w_d_tf * tf.nn.l2_loss(var)\n      return loss\n\n  def average_gradients(self, tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    This function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers. Notice that this function already averages the gradients,\n       it doesn\'t sum them. This is important when scaling the hyper-params for\n       multi-gpu training.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n      # Each grad_and_vars looks like the following:\n      #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n      grads = []\n      for g, _ in grad_and_vars:\n        # Add 0 dimension to the gradients to represent the tower.\n        expanded_g = tf.expand_dims(g, 0)\n\n        # Append on a \'tower\' dimension which we average over below.\n        grads.append(expanded_g)\n\n      # Average over the \'tower\' dimension.\n      grad = tf.concat(axis=0, values=grads)\n      grad = tf.reduce_mean(grad, 0)\n\n      # the variables are redundant because they are shared across towers.\n      # So we just return the first tower\'s pointer to the Variable.\n      v = grad_and_vars[0][1]\n      grad_and_var = (grad, v)\n      average_grads.append(grad_and_var)\n\n    return average_grads\n\n  def restore_session(self, path):\n    # restore from checkpoint (to continue training, or to infer at test time)\n    print(""Restoring checkpoint"")\n\n    # Restore the graph\n    print(""Looking for model in %s"" % path)\n    self.ckpt = tf.train.get_checkpoint_state(path)\n\n    # only try if I have a checkpoint\n    if self.ckpt and self.ckpt.model_checkpoint_path:\n      print(""Retrieving model from: "", self.ckpt.model_checkpoint_path)\n\n      # try to get the full model including classifier, but with no crap from\n      # previous training such as learning rate, moments, etc.\n      try:\n        restore = []\n        not_restore = []\n        restore.extend(tf.global_variables(scope=\'model\'))\n        restore_var = [v for v in restore if v not in not_restore]\n        restore_saver = tf.train.Saver(var_list=restore_var)\n        # restore all variables\n        restore_saver.restore(self.sess, self.ckpt.model_checkpoint_path)\n\n      except:\n        # if it fails to load, reload only the feat extractor, and not the linear\n        # classifier. This is useful when retraining for a different number of classes\n        print(\' WARNING \'.center(80, \'*\'))\n        print(""Failed to restore model"".center(80, \'!\'))\n        print(\'*\' * 80)\n        print(""Keeping classifier random, to see if this helps (also keeping all the training stuff the same)"")\n        restore = []\n        not_restore = []\n        restore.extend(tf.global_variables(scope=\'model\'))\n        not_restore.extend(tf.global_variables(scope=\'model/logits\'))\n        restore_var = [v for v in restore if v not in not_restore]\n        restore_saver = tf.train.Saver(var_list=restore_var)\n        # restore all variables\n        restore_saver.restore(self.sess, self.ckpt.model_checkpoint_path)\n\n        try:\n          # try again without the linear part\n          restore_saver.restore(self.sess, self.ckpt.model_checkpoint_path)\n\n        except:\n          # if all fails, I need to be doing something wrong, like using\n          # a wrong arch checkpoint. Report and exit\n          print(""Restore failed again. Something else is wrong. Exiting"")\n          quit()\n\n      # hooray! Everything great\n      print(""Successfully restored model weights! :D"")\n      return True\n\n    else:\n      # no model :(\n      print(""No model to restore in path"")\n      return False\n\n  def predict_kickstart(self, path, batchsize=1, data_format=""NCHW""):\n    # bake placeholders\n    self.img_pl, self.lbls_pl = self.placeholders(\n        self.DATA[""img_prop""][""depth""], batchsize)\n\n    # make list\n    self.n_gpus = 1\n    self.img_pl_list = [self.img_pl]\n    self.lbls_pl_list = [self.lbls_pl]\n\n    # inititialize inference graph\n    print(""Initializing network"")\n    with tf.name_scope(""test_model""):\n      with tf.variable_scope(""model"", reuse=None):\n        self.logits_valid, self.code_valid, self.n_img_valid = self.build_graph(\n            self.img_pl, False, data_format=data_format)  # not training\n\n    # lists of outputs\n    self.logits_valid_list = [self.logits_valid]\n    self.logits_code_list = [self.code_valid]\n\n    # get model size and report it (so that I can report in paper)\n    n_parameters = 0\n    for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'model\'):\n      # print(var.name , var.get_shape().as_list(), np.prod(var.get_shape().as_list()))\n      var_params = np.prod(var.get_shape().as_list())\n      n_parameters += var_params\n    print(""*"" * 80)\n    print(""Total number of parameters in network: "",\n          ""{:,}"".format(n_parameters))\n    print(""*"" * 80)\n\n    # build graph and predict value (if graph is not built)\n    print(""Predicting mask"")\n\n    # set up evaluation head in the graph\n    with tf.variable_scope(""output""):\n      self.output_p = tf.nn.softmax(self.logits_valid)\n      self.mask = tf.argmax(self.output_p, axis=3, output_type=tf.int32)\n\n    # report the mask shape as a sort of sanity check\n    mask_shape = self.mask.get_shape().as_list()\n    print(""mask shape"", mask_shape)\n\n    # metadata collector for verbose mode (spits out layer-wise profile)\n    self.run_metadata = tf.RunMetadata()\n\n    # Add the variable initializer Op.\n    self.init = tf.global_variables_initializer()\n\n    # Create a saver for restoring and saving checkpoints.\n    self.saver = tf.train.Saver(save_relative_paths=True)\n\n    # xla stuff for faster inference (and soft placement for low ram device)\n    gpu_options = tf.GPUOptions(allow_growth=True, force_gpu_compatible=True)\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=False, gpu_options=gpu_options)\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_2\n\n    # start a session\n    self.sess = tf.Session(config=config)\n\n    # init variables\n    self.sess.run(self.init)\n\n    # if path to model is give, try to restore:\n    self.restore_session(path)\n\n    print(""Saving this graph in %s"" % self.log)\n    self.summary_writer = tf.summary.FileWriter(self.log, self.sess.graph)\n    self.summary_writer.flush()\n\n    # save this graph\n    self.chkpt_graph = os.path.join(self.log, \'model.ckpt\')\n    self.saver.save(self.sess, self.chkpt_graph)\n    tf.train.write_graph(self.sess.graph_def, self.log, \'model.pbtxt\')\n\n  def freeze_graph(self, path=None, verbose=False):\n    """""" Extract the sub graph defined by the output nodes and convert\n        all its variables into constant\n    """"""\n    # kickstart the model. If session is initialized everything may be dirty,\n    # so please use this function from a clean tf environment :)\n    if self.sess is None:\n      self.predict_kickstart(path, data_format=""NHWC"")\n    else:\n      print(""existing session. This is unintended behavior. Check!"")\n      quit()\n\n    # outputs\n    in_node_names = [str(self.img_pl.op.name)]\n    print(""in_node_names"", in_node_names)\n    in_trt_node_names = [str(self.n_img_valid.op.name)]\n    print(""in_tensorRT_node_names"", in_trt_node_names)\n    out_node_names = [str(self.mask.op.name), str(self.code_valid.op.name)]\n    print(""out_node_names"", out_node_names)\n    input_graph_path = os.path.join(self.log, \'model.pbtxt\')\n    checkpoint_path = os.path.join(self.log, \'model.ckpt\')\n    input_saver_def_path = """"\n    input_binary = False\n    restore_op_name = ""save/restore_all""\n    filename_tensor_name = ""save/Const:0""\n    out_frozen_graph_name_nchw = os.path.join(self.log, \'frozen_nchw.pb\')\n    out_frozen_graph_name_nhwc = os.path.join(self.log, \'frozen_nhwc.pb\')\n    out_opt_graph_name = os.path.join(self.log, \'optimized.pb\')\n    out_opt_tensorRT_graph_name = os.path.join(self.log, \'optimized_tRT.pb\')\n    uff_opt_tensorRT_graph_name = os.path.join(self.log, \'optimized_tRT.uff\')\n    output_quantized_graph_name = os.path.join(self.log, \'quantized.pb\')\n    clear_devices = True\n\n    # freeze\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n                              input_binary, checkpoint_path, "","".join(\n                                  out_node_names),\n                              restore_op_name, filename_tensor_name,\n                              out_frozen_graph_name_nhwc, clear_devices, """")\n\n    # Optimize for inference\n    input_graph_def = tf.GraphDef()\n    with tf.gfile.Open(out_frozen_graph_name_nhwc, ""rb"") as f:\n      data = f.read()\n      input_graph_def.ParseFromString(data)\n\n    # transforms for optimization\n    transforms = [\'add_default_attributes\',\n                  \'remove_nodes(op=Identity, op=CheckNumerics)\',\n                  \'fold_constants(ignore_errors=true)\', \'fold_batch_norms\',\n                  \'fold_old_batch_norms\',\n                  \'strip_unused_nodes\', \'sort_by_execution_order\']\n\n    # optimize and save\n    output_graph_def = TransformGraph(input_graph_def,\n                                      in_node_names,\n                                      out_node_names,\n                                      transforms)\n    f = tf.gfile.FastGFile(out_opt_graph_name, ""w"")\n    f.write(output_graph_def.SerializeToString())\n\n    # quantize and optimize, and save\n    transforms += [\'quantize_weights\', \'quantize_nodes\']\n    output_graph_def = TransformGraph(input_graph_def,\n                                      in_node_names,\n                                      out_node_names,\n                                      transforms)\n    f = tf.gfile.FastGFile(output_quantized_graph_name, ""w"")\n    f.write(output_graph_def.SerializeToString())\n\n    # save the names of the input and output nodes\n    input_node = str(self.img_pl.op.name)\n    input_norm_and_resized_node = str(self.n_img_valid.op.name)\n    code_node = str(self.code_valid.op.name)\n    logits_node = str(self.logits_valid.op.name)\n    out_probs_node = str(self.output_p.op.name)\n    mask_node = str(self.mask.op.name)\n    node_dict = {""input_node"": input_node,\n                 ""input_norm_and_resized_node"": input_norm_and_resized_node,\n                 ""code_node"": code_node,\n                 ""logits_node"": logits_node,\n                 ""out_probs_node"": out_probs_node,\n                 ""mask_node"": mask_node}\n    node_file = os.path.join(self.log, ""nodes.yaml"")\n    with open(node_file, \'w\') as f:\n      yaml.dump(node_dict, f, default_flow_style=False)\n\n    # do the same for NCHW but don\'t save any quantized models,\n    # since quantization doesn\'t work in NCHW (only save optimized for tensort)\n    self.sess.close()\n    tf.reset_default_graph()\n    self.predict_kickstart(path, data_format=""NCHW"")\n\n    # freeze\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n                              input_binary, checkpoint_path,\n                              "","".join(out_node_names),\n                              restore_op_name, filename_tensor_name,\n                              out_frozen_graph_name_nchw, clear_devices, """")\n\n    # Optimize for inference on tensorRT\n    input_graph_def = tf.GraphDef()\n    with tf.gfile.Open(out_frozen_graph_name_nchw, ""rb"") as f:\n      data = f.read()\n      input_graph_def.ParseFromString(data)\n\n    # transforms for optimization\n    transforms = [\'add_default_attributes\',\n                  \'remove_nodes(op=Identity, op=CheckNumerics)\',\n                  \'fold_batch_norms\', \'fold_old_batch_norms\',\n                  \'strip_unused_nodes\', \'sort_by_execution_order\']\n\n    # optimize and save\n    output_graph_def = TransformGraph(input_graph_def,\n                                      in_trt_node_names,\n                                      out_node_names,\n                                      transforms)\n    f = tf.gfile.FastGFile(out_opt_tensorRT_graph_name, ""w"")\n    f.write(output_graph_def.SerializeToString())\n    f.close()\n\n    # last but not least, try to convert the NCHW model to UFF for TensorRT\n    # inference\n    print(""Saving uff model for TensorRT inference"")\n    try:\n      # import tensorRT stuff\n      import uff\n      # import uff from tensorflow frozen and save as uff file\n      uff.from_tensorflow_frozen_model(out_opt_tensorRT_graph_name,\n                                       [logits_node],\n                                       input_nodes=[\n                                           input_norm_and_resized_node],\n                                       output_filename=uff_opt_tensorRT_graph_name)\n    except:\n      print(""Error saving TensorRT UFF model"")\n\n    return\n\n  def gpu_available(self):\n    # can I use a gpu? Return number of GPUs available.\n    # tensorflow is very greedy with the GPUs, and it always tries to use\n    # everything available. So make sure you restrict its vision with\n    # the CUDA_VISIBLE_DEVICES environment variable.\n    n_gpus_avail = 0\n    devices = device_lib.list_local_devices()\n    for dev in devices:\n      print(""DEVICE AVAIL: "", dev.name)\n      if \'/device:GPU\' in dev.name:\n        n_gpus_avail += 1\n    return n_gpus_avail\n\n  def predict(self, img, path=None, verbose=False, as_probs=False):\n    \'\'\' Predict an opencv image labels with a trained model. Kickstarts the\n        session if it is the first call\n    \'\'\'\n\n    # if there is no session, kick it!\n    if self.sess is None:\n      # get dataset reader\n      print(""Fetching dataset"")\n      self.parser = imp.load_source(""parser"",\n                                    os.getcwd() + \'/dataset/\' +\n                                    self.DATA[""name""] + \'.py\')\n\n      # kickstart in NCHW or NHWC depending on availability or not of GPUs\n      n_gpus_avail = self.gpu_available()\n      if n_gpus_avail:\n        self.predict_kickstart(path, data_format=""NCHW"")\n      else:\n        self.predict_kickstart(path, data_format=""NHWC"")\n\n    # choose op to run according to choice of mask or feature map:\n    if as_probs:\n      node_to_run = self.output_p\n    else:\n      node_to_run = self.mask\n\n    # run the classifier and report according to desired verbosity\n    if verbose:\n      # run the classifier in verbose mode (get profile and report it)\n      start_time = time.time()\n      predicted_mask = self.sess.run(node_to_run, {self.img_pl: [img]},\n                                     options=tf.RunOptions(\n          trace_level=tf.RunOptions.FULL_TRACE),\n          run_metadata=self.run_metadata)\n      time_to_run = time.time() - start_time\n      print(""Time to evaluate: %f"" % time_to_run)\n\n      # profile amount of flops\n      opts = tf.profiler.ProfileOptionBuilder.float_operation()\n      flops = tf.profiler.profile(tf.get_default_graph(\n      ), run_meta=self.run_metadata, cmd=\'op\', options=opts)\n      if flops is not None:\n        print(""*"" * 80)\n        print(""Amount of floating point ops (FLOPs): "",\n              ""{:,}"".format(flops.total_float_ops))\n        print(""*"" * 80)\n\n      # Builder to create options to profile the time and memory information.\n      builder = tf.profiler.ProfileOptionBuilder\n\n      # profile with stdout\n      opts = (builder(builder.time_and_memory()).with_stdout_output().build())\n      tf.profiler.profile(tf.get_default_graph(),\n                          run_meta=self.run_metadata, cmd=\'op\', options=opts)\n\n      # profile with log file\n      tracename = os.path.join(self.log, \'timeline.ctf.json\')\n      opts = (builder(builder.time_and_memory()\n                      ).with_timeline_output(tracename).build())\n      tf.profiler.profile(tf.get_default_graph(),\n                          run_meta=self.run_metadata, cmd=\'graph\', options=opts)\n\n    else:\n      # run the classifier and report nothing back!\n      predicted_mask = self.sess.run(node_to_run, {self.img_pl: [img]})\n\n    # return the single prediction\n    return predicted_mask[0]\n\n  def predict_code(self, img, path=None, verbose=False):\n    \'\'\' Extract CNN features from an opencv image with a trained model.\n        Kickstarts the session if it is the first call.\n    \'\'\'\n\n    if self.sess is None:\n      # get dataset reader\n      print(""Fetching dataset"")\n      self.parser = imp.load_source(""parser"",\n                                    os.getcwd() + \'/dataset/\' +\n                                    self.DATA[""name""] + \'.py\')\n      # kickstart in NCHW or NHWC depending on availability or not of GPUs\n      n_gpus_avail = self.gpu_available()\n      if n_gpus_avail:\n        self.predict_kickstart(path, data_format=""NCHW"")\n      else:\n        self.predict_kickstart(path, data_format=""NHWC"")\n\n    # check if arch gave me the code in the kickstarting\n    if self.code_valid is None:\n      print(""Code is not defined in architecture. Can\'t be inferred."")\n      quit()\n\n    # run the feature extractor and report back according to verbosity\n    if verbose:\n      start_time = time.time()\n      infered_code = self.sess.run(self.code_valid, {self.img_pl: [img]},\n                                   options=tf.RunOptions(\n          trace_level=tf.RunOptions.FULL_TRACE),\n          run_metadata=self.run_metadata)\n      time_to_run = time.time() - start_time\n      print(""Time to evaluate: %f"" % time_to_run)\n\n      # profile amount of flops\n      opts = tf.profiler.ProfileOptionBuilder.float_operation()\n      flops = tf.profiler.profile(tf.get_default_graph(\n      ), run_meta=self.run_metadata, cmd=\'op\', options=opts)\n      if flops is not None:\n        print(""*"" * 80)\n        print(""Amount of floating point ops (FLOPs): "",\n              ""{:,}"".format(flops.total_float_ops))\n        print(""*"" * 80)\n\n      # Builder to create options to profile the time and memory information.\n      builder = tf.profiler.ProfileOptionBuilder\n\n      # profile with stdout\n      opts = (builder(builder.time_and_memory()).with_stdout_output().build())\n      tf.profiler.profile(tf.get_default_graph(),\n                          run_meta=self.run_metadata, cmd=\'op\', options=opts)\n\n      # profile with log file\n      tracename = os.path.join(self.log, \'timeline.ctf.json\')\n      opts = (builder(builder.time_and_memory()\n                      ).with_timeline_output(tracename).build())\n      tf.profiler.profile(tf.get_default_graph(),\n                          run_meta=self.run_metadata, cmd=\'graph\', options=opts)\n\n    else:\n      infered_code = self.sess.run(self.code_valid, {self.img_pl: [img]})\n\n    # return the single feature map as 3D numpy array\n    return infered_code[0]\n\n  def predict_dataset(self, datadir, path, batchsize=1, ignore_last=False):\n    \'\'\' Test accuracy in an entire dataset. Also kickstarts the session if needed\n    \'\'\'\n    if self.sess is None:\n      # get dataset reader\n      print(""Fetching dataset"")\n      self.parser = imp.load_source(""parser"",\n                                    os.getcwd() + \'/dataset/\' +\n                                    self.DATA[""name""] + \'.py\')\n      # import dataset\n      self.DATA[""data_dir""] = datadir\n      self.dataset = self.parser.read_data_sets(self.DATA)\n\n      # define mode of model according to gpu availability\n      n_gpus_avail = self.gpu_available()\n      if n_gpus_avail:\n        self.predict_kickstart(path, data_format=""NCHW"")\n      else:\n        self.predict_kickstart(path, data_format=""NHWC"")\n\n    # run the classifier in each split of dataset\n    print(""Train data"")\n    self.dataset_accuracy(self.dataset.train, batchsize, ignore_last)\n    print(""//////////\\n\\n"")\n    print(""Validation data"")\n    self.dataset_accuracy(self.dataset.validation, batchsize, ignore_last)\n    print(""//////////\\n\\n"")\n    print(""Test data"")\n    self.dataset_accuracy(self.dataset.test, batchsize, ignore_last)\n\n    return\n\n  def pix_histogram(self, mask, lbl):\n    \'\'\'\n      get individual mask and label and create 2d hist\n    \'\'\'\n    # flatten mask and cast\n    flat_mask = mask.flatten().astype(np.uint32)\n    # flatten label and cast\n    flat_label = lbl.flatten().astype(np.uint32)\n    # get the histogram\n    histrange = np.array([[-0.5, self.num_classes - 0.5],\n                          [-0.5, self.num_classes - 0.5]], dtype=\'float64\')\n    h_now, _, _ = np.histogram2d(np.array(flat_mask),\n                                 np.array(flat_label),\n                                 bins=self.num_classes,\n                                 range=histrange)\n    return h_now\n\n  def pix_acc_from_histogram(self, hist):\n    \'\'\'\n      get complete 2d hist and return:\n        mean accuracy\n        per class iou\n        mean iou\n        per class precision\n        per class recall\n    \'\'\'\n    # calculate accuracy from histogram\n    if hist.sum():\n      mean_acc = np.diag(hist).sum() / hist.sum()\n    else:\n      mean_acc = 0\n\n    # calculate IoU\n    per_class_iou = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    mean_iou = np.nanmean(per_class_iou)\n\n    # calculate precision and recall\n    per_class_prec = np.diag(hist) / hist.sum(1)\n    per_class_rec = np.diag(hist) / hist.sum(0)\n\n    return mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec\n\n  def obj_histogram(self, mask, label):\n    # holders for predicted object and right object (easily calculate histogram)\n    predicted = []\n    labeled = []\n\n    # get connected components in label for each class\n    for i in range(self.num_classes):\n      # get binary image for this class\n      bin_lbl = np.zeros(label.shape)\n      bin_lbl[label == i] = 1\n      bin_lbl[label != i] = 0\n\n      # util.im_gray_plt(bin_lbl,\'class \'+str(i))\n      connectivity = 4\n      output = cv2.connectedComponentsWithStats(\n          bin_lbl.astype(np.uint8), connectivity, cv2.CV_32S)\n      num_components = output[0]\n      components = output[1]\n      stats = output[2]\n      centroids = output[3]\n\n      for j in range(1, num_components):  # 0 is background (useless)\n        # only process if it has more than 50pix\n        if stats[j][cv2.CC_STAT_AREA] > 50:\n          # for each component in each class, see the class with the highest percentage of pixels\n          # make mask with just this component of this class\n          comp_mask = np.zeros(label.shape)\n          comp_mask[components == j] = 0\n          comp_mask[components != j] = 1\n          # mask the prediction\n          masked_prediction = np.ma.masked_array(mask, mask=comp_mask)\n          # get histogram and get the argmax that is not zero\n          class_hist, _ = np.histogram(masked_prediction.compressed(),\n                                       bins=self.num_classes, range=[0, self.num_classes])\n          max_class = np.argmax(class_hist)\n          # print(""\\nMax class: "",max_class,""  real: "",i)\n          # util.im_gray_plt(comp_mask)\n          # util.im_block()\n          # sum an entry to the containers depending on right or wrong\n          predicted.append(max_class)\n          labeled.append(i)\n    # for idx in range(len(predicted)):\n    #   print(predicted[idx],labeled[idx])\n\n    # histogram to count right and wrong objects\n    histrange = np.array([[-0.5, self.num_classes - 0.5],\n                          [-0.5, self.num_classes - 0.5]], dtype=\'float64\')\n    h_now, _, _ = np.histogram2d(np.array(predicted),\n                                 np.array(labeled),\n                                 bins=self.num_classes,\n                                 range=histrange)\n\n    return h_now\n\n  def obj_acc_from_histogram(self, hist):\n    # calculate accuracy, precision and recall\n    if hist.sum():\n      obj_acc = np.diag(hist).sum() / hist.sum()\n    else:\n      obj_acc = 0\n\n    # calculate precision and recall\n    obj_prec = np.diag(hist) / hist.sum(1)\n    obj_rec = np.diag(hist) / hist.sum(0)\n\n    return obj_acc, obj_prec, obj_rec\n\n  def individual_accuracy(self, mask, label):\n    # individual image prediction accuracy with label\n\n    # check size of label\n    proper_w = self.DATA[""img_prop""][""width""]\n    proper_h = self.DATA[""img_prop""][""height""]\n    h, w = label.shape\n    if proper_w != w or proper_h != h:\n      label = ad.resize(label, [proper_h, proper_w], neighbor=True)\n\n    # calculate pixelwise accuracy from histogram\n    hist = self.pix_histogram(mask, label)\n    mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec = self.pix_acc_from_histogram(\n        hist)\n    print("" Pixelwise Performance: "")\n    print(\'   Mean Accuracy: %0.04f, Mean IoU: %0.04f\' % (mean_acc, mean_iou))\n    print(\'   Intersection over union:\')\n    for idx in range(0, len(per_class_iou)):\n      print(\'     class %d IoU: %f\' % (idx, per_class_iou[idx]))\n    print(\'   Precision:\')\n    for idx in range(0, len(per_class_prec)):\n      print(\'     class %d Precision: %f\' % (idx, per_class_prec[idx]))\n    print(\'   Recall:\')\n    for idx in range(0, len(per_class_rec)):\n      print(\'     class %d Recall: %f\' % (idx, per_class_rec[idx]))\n\n    # report objectwise accuracy\n    hist = self.obj_histogram(mask, label)\n    obj_acc, obj_prec, obj_rec = self.obj_acc_from_histogram(hist)\n    print("" Objectwise Performance: "")\n    print(\'   Accuracy: %0.04f\' % (obj_acc))\n    print(\'   Precision:\')\n    for idx in range(0, len(obj_prec)):\n      print(\'     class %d Precision: %f\' % (idx, obj_prec[idx]))\n    print(\'   Recall:\')\n    for idx in range(0, len(obj_rec)):\n      print(\'     class %d Recall: %f\' % (idx, obj_rec[idx]))\n\n    return mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec, obj_acc, obj_prec, obj_rec\n\n  def dataset_accuracy(self, dataset, batch_size, ignore_last=False):\n    \'\'\' Slower metrics using numpy confusion matrix, and reporting estimate\n        objectwise metrics, for testing\n    \'\'\'\n\n    # define accuracy metric for this model\n    start_time_overall = time.time()  # save curr time to report duration\n    inference_time = 0.0\n    steps_per_epoch = dataset.num_examples // batch_size\n    assert(steps_per_epoch > 0 and ""Dataset length should be more than batchsize"")\n    num_examples = steps_per_epoch * batch_size\n    pix_hist = np.zeros((self.num_classes, self.num_classes), dtype=np.float64)\n    obj_hist = np.zeros((self.num_classes, self.num_classes), dtype=np.float64)\n    for step in range(steps_per_epoch):\n      feed_dict, names = self.fill_feed_dict(\n          dataset, self.img_pl_list, self.lbls_pl_list, batch_size)\n      for g in range(0, self.n_gpus):\n        inference_start = time.time()\n        pred = self.sess.run(self.logits_valid_list[g], feed_dict=feed_dict)\n        inference_time += time.time() - inference_start\n        # calculate 2d histogram of size (n_classes,n_classes)\n        # one axis is the true label, the other one the predicted value, so\n        # the diagonal contains the right detections\n        for idx in range(0, batch_size):\n          # get mask and labels\n          mask = pred[idx].argmax(2)\n          img = feed_dict[self.img_pl_list[g]][idx]\n          label = feed_dict[self.lbls_pl_list[g]][idx]\n          name = names[g][idx]\n          if "".png"" in name:\n            name = name.replace("".png"", "".jpg"")\n          # check size of label\n          proper_w = self.DATA[""img_prop""][""width""]\n          proper_h = self.DATA[""img_prop""][""height""]\n          h, w = label.shape\n          if proper_w != w or proper_h != h:\n            label = ad.resize(label, [proper_h, proper_w], neighbor=True)\n            img = ad.resize(img, [proper_h, proper_w])\n          # get histograms\n          pix_h_now = self.pix_histogram(mask, label)\n          obj_h_now = self.obj_histogram(mask, label)\n          # sum to history\n          pix_hist += pix_h_now\n          obj_hist += obj_h_now\n          if self.TRAIN[""save_imgs""]:\n            color_mask = util.prediction_to_color(\n                mask, self.DATA[""label_remap""], self.DATA[""color_map""])\n            color_label = util.prediction_to_color(\n                label, self.DATA[""label_remap""], self.DATA[""color_map""])\n            path_to_save = self.log + \'/predictions/\'\n            if not tf.gfile.Exists(path_to_save):\n              tf.gfile.MakeDirs(path_to_save)\n            cv2.imwrite(path_to_save + dataset.name + \'_\' + str(name),\n                        np.concatenate((img, color_mask, color_label), axis=1))\n\n    # calculate pixelwise metrics  histogram\n    if ignore_last:\n      pix_hist = pix_hist[:-1, :-1]\n    mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec = self.pix_acc_from_histogram(\n        pix_hist)\n\n    # calculate objectwise metrics from histogram\n    if ignore_last:\n      obj_hist = obj_hist[:-1, :-1]\n    obj_acc, obj_prec, obj_rec = self.obj_acc_from_histogram(obj_hist)\n\n    overall_duration = time.time() - start_time_overall  # calculate time elapsed\n    print(\'   Num samples: %d, Time to run %.3f sec (only inference: %.3f sec)\' %\n          (num_examples, overall_duration, inference_time))\n    fps = (num_examples / inference_time)\n    print(\'   Network FPS: %.3f\' % fps)\n    print(\'   Time per image: %.3f s\' % (1 / fps))\n    print("" Pixelwise Performance: "")\n    print(\'   Mean Accuracy: %0.04f, Mean IoU: %0.04f\' % (mean_acc, mean_iou))\n    print(\'   Intersection over union:\')\n    for idx in range(0, len(per_class_iou)):\n      print(\'     class %d IoU: %f\' % (idx, per_class_iou[idx]))\n    print(\'   Precision:\')\n    for idx in range(0, len(per_class_prec)):\n      print(\'     class %d Precision: %f\' % (idx, per_class_prec[idx]))\n    print(\'   Recall:\')\n    for idx in range(0, len(per_class_rec)):\n      print(\'     class %d Recall: %f\' % (idx, per_class_rec[idx]))\n\n    print("" Objectwise Performance: "")\n    print(\'   Accuracy: %0.04f\' % (obj_acc))\n    print(\'   Precision:\')\n    for idx in range(0, len(obj_prec)):\n      print(\'     class %d Precision: %f\' % (idx, obj_prec[idx]))\n    print(\'   Recall:\')\n    for idx in range(0, len(obj_rec)):\n      print(\'     class %d Recall: %f\' % (idx, obj_rec[idx]))\n\n    return mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec\n\n  def training_dataset_accuracy(self, dataset, batch_size, batch_size_gpu,\n                                ignore_last=False):\n    \'\'\' Faster tensorflow metrics using tensorflow confusion matrix,\n        for training\n    \'\'\'\n\n    # define accuracy metric for this model\n    start_time_overall = time.time()  # save curr time to report duration\n    inference_time = 0.0\n    steps_per_epoch = dataset.num_examples // batch_size\n    assert(steps_per_epoch > 0 and ""Dataset length should be more than batchsize"")\n    num_examples = steps_per_epoch * batch_size\n    pix_hist = np.zeros((self.num_classes, self.num_classes), dtype=np.float32)\n    for step in range(steps_per_epoch):\n      feed_dict, names = self.fill_feed_dict(\n          dataset, self.img_pl_list, self.lbls_pl_list, batch_size_gpu)\n      inference_start = time.time()\n      pix_h_now, pred = self.sess.run(\n          [self.confusion_matrix, self.logits_valid], feed_dict=feed_dict)\n      inference_time += time.time() - inference_start\n      # masks from logits\n      masks = pred.argmax(3)\n      # sum to history\n      pix_hist += pix_h_now\n      # save to disk\n      for g in range(0, self.n_gpus):\n        for idx in range(0, batch_size_gpu):\n          # get mask and labels\n          img = feed_dict[self.img_pl_list[g]][idx]\n          label = feed_dict[self.lbls_pl_list[g]][idx]\n          name = names[g][idx]\n          mask = masks[idx + g * batch_size_gpu]\n          if "".png"" in name:\n            name = name.replace("".png"", "".jpg"")\n          # check size of label\n          proper_w = self.DATA[""img_prop""][""width""]\n          proper_h = self.DATA[""img_prop""][""height""]\n          h, w = label.shape\n\n          # save predictions\n          if self.TRAIN[""save_imgs""]:\n            # resize if proper\n            if proper_w != w or proper_h != h:\n              label = ad.resize(label, [proper_h, proper_w], neighbor=True)\n              img = ad.resize(img, [proper_h, proper_w])\n            # convert to color\n            color_mask = util.prediction_to_color(\n                mask, self.DATA[""label_remap""], self.DATA[""color_map""])\n            color_label = util.prediction_to_color(\n                label, self.DATA[""label_remap""], self.DATA[""color_map""])\n            path_to_save = self.log + \'/predictions/\'\n            if not tf.gfile.Exists(path_to_save):\n              tf.gfile.MakeDirs(path_to_save)\n            cv2.imwrite(path_to_save + dataset.name + \'_\' + str(name),\n                        np.concatenate((img, color_mask, color_label), axis=1))\n\n    # calculate pixelwise metrics histogram\n    if ignore_last:\n      pix_hist = pix_hist[:-1, :-1]\n    mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec = self.pix_acc_from_histogram(\n        pix_hist)\n\n    overall_duration = time.time() - start_time_overall  # calculate time elapsed\n    print(\'   Num samples: %d, Time to run %.3f sec (only inference: %.3f sec)\' %\n          (num_examples, overall_duration, inference_time))\n    fps = (num_examples / inference_time)\n    print(\'   Network FPS: %.3f\' % fps)\n    print(\'   Time per image: %.3f s\' % (1 / fps))\n    print("" Pixelwise Performance: "")\n    print(\'   Mean Accuracy: %0.04f, Mean IoU: %0.04f\' % (mean_acc, mean_iou))\n    print(\'   Intersection over union:\')\n    for idx in range(0, len(per_class_iou)):\n      print(\'     class %d IoU: %f\' % (idx, per_class_iou[idx]))\n    print(\'   Precision:\')\n    for idx in range(0, len(per_class_prec)):\n      print(\'     class %d Precision: %f\' % (idx, per_class_prec[idx]))\n    print(\'   Recall:\')\n    for idx in range(0, len(per_class_rec)):\n      print(\'     class %d Recall: %f\' % (idx, per_class_rec[idx]))\n\n    return mean_acc, mean_iou, per_class_iou, per_class_prec, per_class_rec\n\n  def assign_to_device(self, op_dev, var_dev=\'/cpu:0\'):\n    """"""Returns a function to place variables on the var_dev, and the ops in the\n    op_dev.\n\n    Args:\n      op_dev: Device for ops\n      var_dev: Device for variables\n    """"""\n    VAR_OPS = [\'Variable\', \'VariableV2\', \'AutoReloadVariable\',\n               \'MutableHashTable\', \'MutableHashTableOfTensors\',\n               \'MutableDenseHashTable\']\n\n    def _assign(op):\n      node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n      if node_def.op in VAR_OPS:\n        return ""/"" + var_dev\n      else:\n        return op_dev\n    return _assign\n\n  def train(self, path=None):\n    \'\'\' Main function to train a network from scratch or from checkpoint\n    \'\'\'\n\n    # get dataset reader\n    print(""Fetching dataset"")\n    self.parser = imp.load_source(""parser"",\n                                  os.getcwd() + \'/dataset/\' +\n                                  self.DATA[""name""] + \'.py\')\n\n    # report batch size and gpus to use\n    self.batch_size = int(self.TRAIN[""batch_size""])\n    self.n_gpus = int(self.TRAIN[\'gpus\'])\n    print(""Training with %d GPU\'s"" % self.n_gpus)\n    print(""Training with batch size %d"" % self.batch_size)\n\n    # gpus available\n    self.n_gpus_avail = self.gpu_available()\n    print(""Number of GPU\'s available is %d"" % self.n_gpus_avail)\n    assert(self.n_gpus == self.n_gpus_avail)\n\n    # calculate batch size per gpu\n    self.batch_size_gpu = int(self.batch_size / self.n_gpus_avail)\n    assert(self.batch_size % self.n_gpus == 0)\n    assert(self.batch_size_gpu > 0)\n    print(""This means %d images per GPU"" % self.batch_size_gpu)\n\n    # import dataset\n    self.dataset = self.parser.read_data_sets(self.DATA)\n\n    # get learn rate from config file\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n      self.lrate = self.TRAIN[\'lr\']\n      with tf.variable_scope(""learn_rate""):\n        lr_init = tf.constant(self.lrate)\n        self.learn_rate_var = tf.get_variable(name=""learn_rate"",\n                                              initializer=lr_init,\n                                              trainable=False)\n        # report the current learn rate to tf log\n        tf.summary.scalar(\'learn_rate\', self.learn_rate_var)\n\n      with tf.variable_scope(""trainstep""):\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learn_rate_var,\n                                                beta1=self.TRAIN[""decay1""],\n                                                beta2=self.TRAIN[""decay2""],\n                                                epsilon=self.TRAIN[""epsilon""])\n\n      # inititialize inference graph\n      self.logits_train_list = []\n      self.logits_valid_list = []\n      self.losses = []\n      self.tower_grads = []\n      self.img_pl_list = []\n      self.lbls_pl_list = []\n      self.confusion_matrixes = []\n\n      print(""Initializing network"")\n      with tf.name_scope(""train_model""):\n        with tf.variable_scope(""model""):\n          for i in range(self.n_gpus):\n            with tf.device(self.assign_to_device(\'/gpu:%d\' % i)):\n              print(\' TRAINING GRAPH \'.center(80, \'*\'))\n              print(\' GRAPH GPU:%d \' % i)\n              # get placeholders\n              img_pl, lbls_pl = self.placeholders(\n                  self.DATA[""img_prop""][""depth""], self.batch_size_gpu)\n              self.img_pl_list.append(img_pl)\n              self.lbls_pl_list.append(lbls_pl)\n\n              # graph\n              logits_train, _, _ = self.build_graph(img_pl, True)  # train\n              self.logits_train_list.append(logits_train)\n\n              # define the loss function, and calculate the gradients\n              with tf.name_scope(""loss_%d"" % i):\n                loss = self.loss_f(lbls_pl, logits_train, self.TRAIN[""gamma""],\n                                   self.TRAIN[""loss""], self.TRAIN[""w_decay""])\n                if self.TRAIN[""grads""] == ""speed"":\n                  # calculate tower grads by using OpenAI\'s implementation\n                  # of checkpointed gradients (better for memory)\n                  grads = msg.gradients_speed(\n                      loss, tf.trainable_variables(), gate_gradients=True)\n                elif self.TRAIN[""grads""] == ""mem"":\n                  # calculate tower grads by using OpenAI\'s implementation\n                  # of checkpointed gradients (better for speed)\n                  grads = msg.gradients_memory(\n                      loss, tf.trainable_variables(), gate_gradients=True)\n                elif self.TRAIN[""grads""] == ""tf"":\n                  # calculate tower grads by using TF implementation\n                  print(""Using tensorflow gradients"")\n                  grads = tf.gradients(\n                      loss, tf.trainable_variables())\n                else:\n                  print(""Gradient option not supported. Check config"")\n                grads_and_vars = list(zip(grads, tf.trainable_variables()))\n\n              # append to the list of gradients and losses\n              self.losses.append(loss)\n              self.tower_grads.append(grads_and_vars)\n\n              # Reuse variables for the next tower.\n              tf.get_variable_scope().reuse_variables()\n\n              print(\'*\' * 80)\n      with tf.name_scope(""test_model""):\n        with tf.variable_scope(""model"", reuse=True):\n          for i in range(self.n_gpus):\n            with tf.device(\'/gpu:%d\' % i):\n              print(\' TESTING GRAPH \'.center(80, \'*\'))\n              print(\' GRAPH GPU:%d \' % i)\n              img_pl = self.img_pl_list[i]\n              lbls_pl = self.lbls_pl_list[i]\n              logits_valid, _, _ = self.build_graph(img_pl, False)  # test\n              self.logits_valid_list.append(logits_valid)\n\n              # create a confusion matrix to run with every training step\n              with tf.variable_scope(""confusion""):\n                lbls_resized = self.resize_label(lbls_pl)\n                lbls_flattened = tf.reshape(lbls_resized, [-1])\n                argmax_flattened = tf.reshape(\n                    tf.argmax(logits_valid, axis=3), [-1])\n                conf_mat = tf.confusion_matrix(argmax_flattened,\n                                               lbls_flattened,\n                                               num_classes=self.num_classes,\n                                               dtype=tf.float32,\n                                               name=""confusion_matrix"")\n                self.confusion_matrixes.append(conf_mat)\n\n              print(\'*\' * 80)\n\n      # print number of parameters (just a check)\n      n_parameters = 0\n      for var in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'model\'):\n        var_params = np.prod(var.get_shape().as_list())\n        n_parameters += var_params\n      print(""Total number of parameters in network: "", n_parameters)\n\n      with tf.variable_scope(""optimizer"", reuse=tf.AUTO_REUSE):\n        # We must calculate the mean of each gradient. Note that this is the\n        # synchronization point across all towers.\n        self.grads = self.average_gradients(self.tower_grads)\n\n        # total loss\n        self.loss = tf.add_n(self.losses)\n\n        # confusion matrix and total logits\n        self.confusion_matrix = tf.add_n(self.confusion_matrixes)\n        self.logits_valid = tf.concat(self.logits_valid_list, 0)\n\n        # Add histograms for gradients.\n        if self.TRAIN[\'summary\']:\n          for grad, var in self.grads:\n            if grad is not None:\n              tf.summary.histogram(var.op.name + \'/gradients\', grad)\n\n        # Apply the gradients to adjust the shared variables.\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n          self.train_op = self.optimizer.apply_gradients(self.grads)\n\n      # define the best performance so far in the validation set\n      self.best_acc_validation = 0\n      self.best_iou_validation = 0\n\n      # periodically report accuracy and IoU\n      # accuracy\n      print(""Reporting accuracy every "",\n            self.TRAIN[""acc_report_epochs""], "" epochs"")\n      with tf.variable_scope(""accuracies""):\n        self.train_accuracy = tf.Variable(0.0, name=""train"", trainable=False)\n        self.validation_accuracy = tf.Variable(\n            0.0, name=""validation"", trainable=False)\n        self.test_accuracy = tf.Variable(0.0, name=""test"", trainable=False)\n        # summaries for the accuracies (to be evaluated later on)\n        tf.summary.scalar(\'train_accuracy\', self.train_accuracy)\n        tf.summary.scalar(\'validation_accuracy\', self.validation_accuracy)\n        tf.summary.scalar(\'test_accuracy\', self.test_accuracy)\n      # IoU\n      with tf.variable_scope(""IoU""):\n        self.train_IoU = tf.Variable(0.0, name=""train"", trainable=False)\n        self.validation_IoU = tf.Variable(\n            0.0, name=""validation"", trainable=False)\n        self.test_IoU = tf.Variable(0.0, name=""test"", trainable=False)\n        # summaries for the accuracies (to be evaluated later on)\n        tf.summary.scalar(\'train_IoU\', self.train_IoU)\n        tf.summary.scalar(\'validation_IoU\', self.validation_IoU)\n        tf.summary.scalar(\'test_IoU\', self.test_IoU)\n      with tf.variable_scope(""loss_value""):\n        # Add a scalar summary for the snapshot loss.\n        self.train_loss = tf.Variable(0.0, name=""train_loss"", trainable=False)\n        tf.summary.scalar(\'train\', self.train_loss)\n        self.train_batch_iou = tf.Variable(\n            0.0, name=""train_batch_iou"", trainable=False)\n        tf.summary.scalar(\'batch_iou\', self.train_batch_iou)\n        self.train_batch_acc = tf.Variable(\n            0.0, name=""train_batch_acc"", trainable=False)\n        tf.summary.scalar(\'batch_acc\', self.train_batch_acc)\n\n      # Build the summary Tensor based on the TF collection of Summaries.\n      self.summary = tf.summary.merge_all()\n\n      # Add the variable initializer Op.\n      self.init = tf.global_variables_initializer()\n\n      # Create a saver for writing training checkpoints.\n      self.saver = tf.train.Saver(save_relative_paths=True)\n\n      # xla stuff for faster inference (and soft placement for low ram device)\n      gpu_options = tf.GPUOptions(allow_growth=True, force_gpu_compatible=True)\n      config = tf.ConfigProto(allow_soft_placement=True,\n                              log_device_placement=False, gpu_options=gpu_options)\n      config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.OFF\n\n      # start a session\n      self.sess = tf.Session(config=config)\n\n      # Instantiate a SummaryWriter to output summaries and the Graph.\n      self.log_dir = self.log + \'/lr_\' + str(self.lrate)\n      print(""Saving this iteration of training in %s"" % self.log_dir)\n      self.summary_writer = tf.summary.FileWriter(\n          self.log_dir, self.sess.graph)\n\n      # Run the Op to initialize the variables.\n      self.sess.run(self.init)\n\n      # if path to model is give, try to restore:\n      if path is not None:\n        self.restore_session(path)\n\n      # do the training\n      print(""Training model"")\n\n      # Start the training loop\n      steps_per_epoch = self.dataset.train.num_examples / \\\n          float(self.batch_size)\n      acc_report_steps = int(self.TRAIN[""acc_report_epochs""] * steps_per_epoch)\n      max_steps = int(self.TRAIN[""max_epochs""] * steps_per_epoch)\n      print(""Training network %d epochs (%d iterations at batch size %d)"" %\n            (self.TRAIN[""max_epochs""], max_steps, self.TRAIN[""batch_size""]))\n\n      # calculate the decay steps with the batch size and num examples\n      self.decay_steps = int(self.TRAIN[""lr_decay""] * steps_per_epoch)\n      self.decay_rate = float(self.TRAIN[""lr_rate""])\n      print(""Decaying learn rate by %f every %d epochs (%d steps)"" %\n            (self.decay_rate, self.TRAIN[""lr_decay""], self.decay_steps))\n      for self.step in range(max_steps):\n        # do learn rate decay\n        if self.step % self.decay_steps == 0 and self.step > 0:\n          assign_lr = self.learn_rate_var.assign(\n              self.sess.run(self.learn_rate_var) / self.decay_rate)\n          self.sess.run(assign_lr)  # assign the value to the node\n          print(""Decreased learning rate to: "",\n                self.sess.run(self.learn_rate_var))\n\n        # fill in the dictionaries\n        start_time = time.time()\n        feed_dict, _ = self.fill_feed_dict(self.dataset.train,\n                                           self.img_pl_list,\n                                           self.lbls_pl_list,\n                                           self.batch_size_gpu)\n        duration_get_batch = time.time() - start_time\n\n        # Run one step of the model in all gpus\n        start_time = time.time()\n        _, self.loss_value = self.sess.run(\n            [self.train_op, self.loss], feed_dict=feed_dict)\n        duration_train_step = time.time() - start_time\n\n        # Print status to stdout.\n        print(\'Epoch: %d. Step %d: loss = %.5f (train step: %.3f sec, get_batch: %.3f)\'\n              % (self.step / steps_per_epoch, self.step, self.loss_value,\n                 duration_train_step, duration_get_batch))\n        # Write the summaries\n        if self.step % self.TRAIN[""summary_freq""] == 0:\n          # write loss summary\n          train_loss_op = self.train_loss.assign(self.loss_value)\n          self.sess.run(train_loss_op)\n\n          # write batch iou summary\n          pix_h = self.sess.run(self.confusion_matrix, feed_dict=feed_dict)\n          if self.TRAIN[""ignore_crap""]:\n            pix_h = pix_h[:-1, :-1]\n          mean_acc, mean_iou, _, _, _ = self.pix_acc_from_histogram(pix_h)\n          self.sess.run(self.train_batch_iou.assign(mean_iou))\n          self.sess.run(self.train_batch_acc.assign(mean_acc))\n\n          # Update the events file only if I wont do that later on\n          print(""Saving summaries"")\n          self.summary_str = self.sess.run(self.summary, feed_dict=feed_dict)\n          # add_summary takes ints, so x axis in log will be epoch * 1000\n          fake_epoch = int(self.step / float(steps_per_epoch) * 1000)\n          self.summary_writer.add_summary(self.summary_str, fake_epoch)\n          self.summary_writer.flush()\n\n        # Save a checkpoint and evaluate the model periodically.\n        if self.step % acc_report_steps == 0 or (self.step + 1) == max_steps:\n          # Evaluate against the training set.\n          print(\'Training Data Eval:\')\n          ignore_last = self.TRAIN[""ignore_crap""]\n          m_acc, m_iou, _, _, _ = self.training_dataset_accuracy(\n              self.dataset.train, self.batch_size, self.batch_size_gpu, ignore_last)\n          acc_op = self.train_accuracy.assign(m_acc)\n          iou_op = self.train_IoU.assign(m_iou)\n          self.sess.run([acc_op, iou_op])  # assign the value to the nodes\n\n          # Evaluate against the validation set.\n          print(\'Validation Data Eval:\')\n          m_acc, m_iou, _, _, _ = self.training_dataset_accuracy(\n              self.dataset.validation, self.batch_size, self.batch_size_gpu, ignore_last)\n          acc_op = self.validation_accuracy.assign(m_acc)\n          iou_op = self.validation_IoU.assign(m_iou)\n          self.sess.run([acc_op, iou_op])  # assign the value to the nodes\n\n          # if the validation performance is the best yet, replace saved model\n          if m_acc > self.best_acc_validation:\n            acc_log_folder = self.log + ""/acc/""\n            if not tf.gfile.Exists(acc_log_folder):\n              tf.gfile.MakeDirs(acc_log_folder)\n            # save a checkpoint\n            self.best_acc_validation = m_acc\n            self.acc_checkpoint_file = os.path.join(\n                acc_log_folder, \'model-best-acc.ckpt\')\n            self.saver.save(self.sess, self.acc_checkpoint_file)\n            # report to user\n            print(""Best validation accuracy yet, saving network checkpoint"")\n\n          if m_iou > self.best_iou_validation:\n            iou_log_folder = self.log + ""/iou/""\n            if not tf.gfile.Exists(iou_log_folder):\n              tf.gfile.MakeDirs(iou_log_folder)\n            # save a checkpoint\n            self.best_iou_validation = m_iou\n            self.iou_checkpoint_file = os.path.join(\n                iou_log_folder, \'model-best-iou.ckpt\')\n            self.saver.save(self.sess, self.iou_checkpoint_file)\n            # report to user\n            print(""Best validation mean IoU yet, saving network checkpoint"")\n\n          # Evaluate against the test set.\n          print(\'Test Data Eval:\')\n          m_acc, m_iou, _, _, _ = self.training_dataset_accuracy(\n              self.dataset.test, self.batch_size, self.batch_size_gpu, ignore_last)\n          acc_op = self.test_accuracy.assign(m_acc)\n          iou_op = self.test_IoU.assign(m_iou)\n          self.sess.run([acc_op, iou_op])  # assign the value to the nodes\n\n          # summarize\n          self.summary_str = self.sess.run(self.summary, feed_dict=feed_dict)\n          # add_summary takes ints, so x axis in log will be epoch * 1000\n          fake_epoch = int(self.step / float(steps_per_epoch) * 1000)\n          self.summary_writer.add_summary(self.summary_str, fake_epoch)\n          self.summary_writer.flush()\n\n  def placeholders(self, depth, batch_size):\n    """"""Generate placeholder variables to represent the input tensors\n    Args:\n      batch_size: The batch size will be baked into both placeholders.\n    Return:\n      img_pl: placeholder for inputs\n      lbls_pl: placeholder for labels\n    """"""\n    img_pl = tf.placeholder(tf.float32, shape=(\n        batch_size, None, None, depth), name=""x_pl"")\n    lbls_pl = tf.placeholder(tf.int32, shape=(\n        batch_size, None, None), name=""y_pl"")\n    return img_pl, lbls_pl\n\n  def fill_feed_dict(self, data_set, img_pl_list, lbls_pl_list, batch_size):\n    """"""Fills the feed_dict for training the given step.\n    Args:\n      data_set: The set of images and labels, from input_data.read_data_sets()\n      img_pl: Placeholder list for images (one item per gpu)\n      lbls_pl: Placeholder list for labels (one item per gpu)\n      batch_size: Batch size for getting dataset batch\n    Returns:\n      feed_dict: to be fed to training op (or cnn forward pass)\n      name_list:  names of files in batch\n    """"""\n    # Create the feed_dict for the placeholders filled with the next\n    # `batch size` examples.\n    name_list = []\n    feed_dict = {}\n\n    for i in range(0, len(img_pl_list)):\n      images_feed, labels_feed, names = data_set.next_batch(batch_size)\n      feed_dict[img_pl_list[i]] = images_feed\n      feed_dict[lbls_pl_list[i]] = labels_feed\n      name_list.append(names)\n    return feed_dict, name_list\n\n  def cleanup(self, signum, frame):\n    print(\'Killing all threads and exiting!\')\n    if hasattr(self, \'dataset\') and self.dataset is not None:\n      self.dataset.cleanup()\n    sys.exit(0)\n'"
train_py/arch/bonnet.py,39,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Network class, containing definition of the graph\n  API Style should be the same for all nets (Same class name and member functions)\n\'\'\'\n# tf\nimport tensorflow as tf\n\n# common layers\nfrom arch.abstract_net import AbstractNetwork\nimport arch.layer as lyr\n\n\nclass Network(AbstractNetwork):\n  def __init__(self, DATA, NET, TRAIN, logdir):\n    # init parent\n    super().__init__(DATA, NET, TRAIN, logdir)\n\n  def build_graph(self, img_pl, train_stage, data_format=""NCHW""):\n    # some graph info depending on what I will do with it\n    summary = self.TRAIN[\'summary\']\n    train_lyr = self.NET[\'train_lyr\']\n    n_k_lyr = self.NET[\'n_k_lyr\']\n    if len(train_lyr) != 9:\n      print(""Wrong length in train list for network. Exiting..."")\n      quit()\n    self.num_classes = len(self.DATA[\'label_map\'])\n\n    # build the graph\n    print(""Building graph"")\n\n    with tf.variable_scope(\'images\'):\n      # resize input to desired size\n      img_resized = tf.image.resize_images(img_pl,\n                                           [self.DATA[""img_prop""][""height""],\n                                            self.DATA[""img_prop""][""width""]])\n      # if on GPU. transpose to NCHW\n      if data_format == ""NCHW"":\n        # convert from NHWC to NCHW (faster on GPU)\n        img_transposed = tf.transpose(img_resized, [0, 3, 1, 2])\n      else:\n        img_transposed = img_resized\n      # normalization of input\n      n_img = (img_transposed - 128) / 128\n\n    with tf.variable_scope(""encoder""):\n      print(""encoder"")\n      with tf.variable_scope(""downsample1""):\n        print(""downsample1"")\n        # input image 1024*512 - 960*720\n        down_lyr1 = lyr.uERF_downsample(n_img, n_k_lyr[0], 5,\n                                        train_stage and train_lyr[0],\n                                        summary,\n                                        data_format=data_format)\n\n        with tf.variable_scope(""non-bt-1""):\n          non_bt_1_lyr1 = lyr.uERF_non_bt(down_lyr1, 3,\n                                          train_stage and train_lyr[0],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-2""):\n          non_bt_2_lyr1 = lyr.uERF_non_bt(non_bt_1_lyr1, 3,\n                                          train_stage and train_lyr[0],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n      with tf.variable_scope(""downsample2""):\n        print(""downsample2"")\n        # input image 512*256 - 480*360\n        down_lyr2 = lyr.uERF_downsample(non_bt_2_lyr1, n_k_lyr[1], 5,\n                                        train_stage and train_lyr[1],\n                                        summary,\n                                        data_format=data_format)\n\n        with tf.variable_scope(""non-bt-1""):\n          print(""non-bt-1"")\n          non_bt_1_lyr2 = lyr.uERF_non_bt(down_lyr2, 5,\n                                          train_stage and train_lyr[1],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-2""):\n          print(""non-bt-2"")\n          non_bt_2_lyr2 = lyr.uERF_non_bt(non_bt_1_lyr2, 5,\n                                          train_stage and train_lyr[1],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-3""):\n          print(""non-bt-3"")\n          non_bt_3_lyr2 = lyr.uERF_non_bt(non_bt_2_lyr2, 5,\n                                          train_stage and train_lyr[1],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-4""):\n          print(""non-bt-4"")\n          non_bt_4_lyr2 = lyr.uERF_non_bt(non_bt_3_lyr2, 5,\n                                          train_stage and train_lyr[1],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        non_bt_lyr2 = non_bt_4_lyr2\n\n      with tf.variable_scope(""downsample3""):\n        print(""downsample3"")\n        # input image 256*128 - 240*180\n        down_lyr3 = lyr.uERF_downsample(non_bt_lyr2, n_k_lyr[2], 5,\n                                        train_stage and train_lyr[2],\n                                        summary,\n                                        data_format=data_format)\n\n        with tf.variable_scope(""non-bt-1""):\n          print(""non-bt-1"")\n          non_bt_1_lyr3 = lyr.uERF_non_bt(down_lyr3, 7,\n                                          train_stage and train_lyr[2],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-2""):\n          print(""non-bt-2"")\n          non_bt_2_lyr3 = lyr.uERF_non_bt(non_bt_1_lyr3, 7,\n                                          train_stage and train_lyr[2],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-3""):\n          print(""non-bt-3"")\n          non_bt_3_lyr3 = lyr.uERF_non_bt(non_bt_2_lyr3, 7,\n                                          train_stage and train_lyr[2],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-4""):\n          print(""non-bt-4"")\n          non_bt_4_lyr3 = lyr.uERF_non_bt(non_bt_3_lyr3, 7,\n                                          train_stage and train_lyr[2],\n                                          summary,\n                                          data_format=data_format,\n                                          dropout=self.NET[""dropout""],\n                                          bn_decay=self.NET[""bn_decay""])\n\n        downsampled = non_bt_4_lyr3\n\n      with tf.variable_scope(""godeep""):\n        print(""godeep"")\n        # input image 64*32 - 60*45\n        with tf.variable_scope(""non-bt-1""):\n          print(""non-bt-1"")\n          godeep_ly1 = lyr.uERF_non_bt(downsampled, 7,\n                                       train_stage and train_lyr[3],\n                                       summary,\n                                       data_format=data_format,\n                                       dropout=self.NET[""dropout""],\n                                       bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-2""):\n          print(""non-bt-2"")\n          godeep_ly2 = lyr.uERF_non_bt(godeep_ly1, 7,\n                                       train_stage and train_lyr[3],\n                                       summary,\n                                       data_format=data_format,\n                                       dropout=self.NET[""dropout""],\n                                       bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-3""):\n          print(""non-bt-3"")\n          godeep_ly3 = lyr.uERF_non_bt(godeep_ly2, 7,\n                                       train_stage and train_lyr[3],\n                                       summary,\n                                       data_format=data_format,\n                                       dropout=self.NET[""dropout""],\n                                       bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""non-bt-4""):\n          print(""non-bt-4"")\n          godeep_ly4 = lyr.uERF_non_bt(godeep_ly3, 7,\n                                       train_stage and train_lyr[3],\n                                       summary,\n                                       data_format=data_format,\n                                       dropout=self.NET[""dropout""],\n                                       bn_decay=self.NET[""bn_decay""])\n\n          godeep = godeep_ly4\n\n        code = godeep\n\n    # end encoder, start decoder\n    print(""============= End of encoder ==============="")\n    print(""size of code: "", code.get_shape().as_list())\n    print(""=========== Beginning of decoder============"")\n\n    with tf.variable_scope(""decoder""):\n      print(""decoder"")\n\n      with tf.variable_scope(""upsample""):\n        print(""upsample"")\n        with tf.variable_scope(""unpool1""):\n          print(""unpool1"")\n          # input image 64*32 - 60*45\n          unpool_lyr1 = lyr.upsample_layer(code,\n                                           train_stage and train_lyr[5],\n                                           kernels=n_k_lyr[3],\n                                           data_format=data_format)\n\n          with tf.variable_scope(""non-bt-1""):\n            un_non_bt_1_lyr1 = lyr.uERF_non_bt(unpool_lyr1, 3,\n                                               train_stage and train_lyr[5],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-2""):\n            un_non_bt_2_lyr1 = lyr.uERF_non_bt(un_non_bt_1_lyr1, 3,\n                                               train_stage and train_lyr[5],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-3""):\n            un_non_bt_3_lyr1 = lyr.uERF_non_bt(un_non_bt_2_lyr1, 3,\n                                               train_stage and train_lyr[5],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-4""):\n            un_non_bt_4_lyr1 = lyr.uERF_non_bt(un_non_bt_3_lyr1, 3,\n                                               train_stage and train_lyr[5],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""unpool2""):\n          print(""unpool2"")\n          # input image 128*64 - 120*90\n          unpool_lyr2 = lyr.upsample_layer(un_non_bt_4_lyr1,\n                                           train_stage and train_lyr[6],\n                                           kernels=n_k_lyr[4],\n                                           data_format=data_format)\n\n          with tf.variable_scope(""non-bt-1""):\n            un_non_bt_1_lyr2 = lyr.uERF_non_bt(unpool_lyr2, 3,\n                                               train_stage and train_lyr[6],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-2""):\n            un_non_bt_2_lyr2 = lyr.uERF_non_bt(un_non_bt_1_lyr2, 3,\n                                               train_stage and train_lyr[6],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-3""):\n            un_non_bt_3_lyr2 = lyr.uERF_non_bt(un_non_bt_2_lyr2, 3,\n                                               train_stage and train_lyr[6],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-4""):\n            un_non_bt_4_lyr2 = lyr.uERF_non_bt(un_non_bt_3_lyr2, 3,\n                                               train_stage and train_lyr[6],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""unpool3""):\n          print(""unpool3"")\n          # input image 256*128 - 240*180\n          unpool_lyr3 = lyr.upsample_layer(un_non_bt_4_lyr2,\n                                           train_stage and train_lyr[7],\n                                           kernels=n_k_lyr[5],\n                                           data_format=data_format)\n          with tf.variable_scope(""non-bt-1""):\n            un_non_bt_1_lyr3 = lyr.uERF_non_bt(unpool_lyr3, 3,\n                                               train_stage and train_lyr[7],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n          with tf.variable_scope(""non-bt-2""):\n            un_non_bt_2_lyr3 = lyr.uERF_non_bt(un_non_bt_1_lyr3, 3,\n                                               train_stage and train_lyr[7],\n                                               summary,\n                                               data_format=data_format,\n                                               dropout=self.NET[""dropout""],\n                                               bn_decay=self.NET[""bn_decay""])\n\n      unpooled = un_non_bt_2_lyr3\n\n    with tf.variable_scope(""logits""):\n      # convert to logits with a linear layer\n      logits_linear = lyr.linear_layer(unpooled, self.num_classes,\n                                       train_stage and train_lyr[8],\n                                       summary=summary,\n                                       data_format=data_format)\n\n    # transpose logits back to NHWC\n    if data_format == ""NCHW"":\n      logits = tf.transpose(logits_linear, [0, 2, 3, 1])\n    else:\n      logits = logits_linear\n\n    return logits, code, n_img\n'"
train_py/arch/bonnet_inception.py,24,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Network class, containing definition of the graph\n  API Style should be the same for all nets (Same class name and member functions)\n\'\'\'\n# tf\nimport tensorflow as tf\n\n# common layers\nfrom arch.abstract_net import AbstractNetwork\nimport arch.layer as lyr\n\n\nclass Network(AbstractNetwork):\n  def __init__(self, DATA, NET, TRAIN, logdir):\n    # init parent\n    super().__init__(DATA, NET, TRAIN, logdir)\n\n  def build_graph(self, img_pl, train_stage, data_format=""NCHW""):\n    # some graph info depending on what I will do with it\n    summary = self.TRAIN[\'summary\']\n    train_lyr = self.NET[\'train_lyr\']\n    n_k_lyr = self.NET[\'n_k_lyr\']\n    n_b_lyr = self.NET[\'n_b_lyr\']\n    if len(train_lyr) != 10:\n      print(""Wrong length in train list for network. Exiting..."")\n      quit()\n    self.num_classes = len(self.DATA[\'label_map\'])\n\n    # build the graph\n    print(""Building graph"")\n\n    with tf.variable_scope(\'images\'):\n      # resize input to desired size\n      img_resized = tf.image.resize_images(img_pl,\n                                           [self.DATA[""img_prop""][""height""],\n                                            self.DATA[""img_prop""][""width""]])\n      # if on GPU. transpose to NCHW\n      if data_format == ""NCHW"":\n        # convert from NHWC to NCHW (faster on GPU)\n        img_transposed = tf.transpose(img_resized, [0, 3, 1, 2])\n      else:\n        img_transposed = img_resized\n      # normalization of input\n      n_img = (img_transposed - 128) / 128\n\n    with tf.variable_scope(""encoder""):\n      print(""encoder"")\n      with tf.variable_scope(""downsample1""):\n        print(""downsample1"")\n        # input image 1024*512 - 960*720\n        down_lyr1 = lyr.uERF_downsample(n_img, n_k_lyr[0], 3,\n                                        train_stage and train_lyr[0],\n                                        summary,\n                                        data_format=data_format)\n\n        with tf.variable_scope(""incept-1""):\n          incept_1_lyr1 = lyr.dense_inception(down_lyr1, n_b_lyr[0],\n                                              train_stage and train_lyr[0],\n                                              summary,\n                                              data_format=data_format,\n                                              dropout=self.NET[""dropout""],\n                                              bn_decay=self.NET[""bn_decay""])\n\n          downsample1 = incept_1_lyr1\n\n      with tf.variable_scope(""downsample2""):\n        print(""downsample2"")\n        # input image 512*256 - 480*360\n        down_lyr2 = lyr.uERF_downsample(downsample1, n_k_lyr[1], 3,\n                                        train_stage and train_lyr[1],\n                                        summary,\n                                        data_format=data_format)\n\n        with tf.variable_scope(""incept-1""):\n          print(""incept-1"")\n          incept_1_lyr2 = lyr.dense_inception(down_lyr2, n_b_lyr[1],\n                                              train_stage and train_lyr[1],\n                                              summary,\n                                              data_format=data_format,\n                                              dropout=self.NET[""dropout""],\n                                              bn_decay=self.NET[""bn_decay""])\n\n        downsample2 = incept_1_lyr2\n\n      with tf.variable_scope(""downsample3""):\n        print(""downsample3"")\n        # input image 256*128 - 240*180\n        down_lyr3 = lyr.uERF_downsample(downsample2, n_k_lyr[2], 3,\n                                        train_stage and train_lyr[2],\n                                        summary,\n                                        data_format=data_format)\n\n        downsample3 = down_lyr3\n\n      with tf.variable_scope(""godeep""):\n        print(""godeep"")\n        # input image 64*32 - 60*45\n        with tf.variable_scope(""dense-inception-1""):\n          print(""dense-inception-1"")\n          godeep_ly1 = lyr.dense_inception(downsample3, n_b_lyr[2],\n                                           train_stage and train_lyr[3],\n                                           summary,\n                                           data_format=data_format,\n                                           dropout=self.NET[""dropout""],\n                                           bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""dense-inception-2""):\n          print(""dense-inception-2"")\n          godeep_ly2 = lyr.dense_inception(godeep_ly1, n_b_lyr[2],\n                                           train_stage and train_lyr[3],\n                                           summary,\n                                           data_format=data_format,\n                                           dropout=self.NET[""dropout""],\n                                           bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""dense-inception-3""):\n          print(""dense-inception-3"")\n          godeep_ly3 = lyr.dense_inception(godeep_ly2, n_b_lyr[2],\n                                           train_stage and train_lyr[3],\n                                           summary,\n                                           data_format=data_format,\n                                           dropout=self.NET[""dropout""],\n                                           bn_decay=self.NET[""bn_decay""])\n\n        with tf.variable_scope(""dense-inception-4""):\n          print(""dense-inception-4"")\n          godeep_ly4 = lyr.dense_inception(godeep_ly3, n_b_lyr[2],\n                                           train_stage and train_lyr[3],\n                                           summary,\n                                           data_format=data_format,\n                                           dropout=self.NET[""dropout""],\n                                           bn_decay=self.NET[""bn_decay""])\n\n          godeep = godeep_ly4\n\n        code = godeep\n\n    # end encoder, start decoder\n    print(""============= End of encoder ==============="")\n    print(""size of code: "", code.get_shape().as_list())\n    print(""=========== Beginning of decoder============"")\n\n    with tf.variable_scope(""decoder""):\n      print(""decoder"")\n\n      with tf.variable_scope(""upsample""):\n        print(""upsample"")\n        with tf.variable_scope(""unpool1""):\n          print(""unpool1"")\n          # input image 64*32 - 60*45\n          unpool_lyr1 = lyr.upsample_layer(code,\n                                           train_stage and train_lyr[5],\n                                           kernels=n_k_lyr[3],\n                                           data_format=data_format)\n\n          with tf.variable_scope(""incept-1""):\n            un_incept_1_lyr1 = lyr.dense_inception(unpool_lyr1, n_b_lyr[3],\n                                                   train_stage and train_lyr[5],\n                                                   summary,\n                                                   data_format=data_format,\n                                                   dropout=self.NET[""dropout""],\n                                                   bn_decay=self.NET[""bn_decay""])\n\n          unpool1 = un_incept_1_lyr1\n\n        with tf.variable_scope(""unpool2""):\n          print(""unpool2"")\n          # input image 128*64 - 120*90\n          unpool_lyr2 = lyr.upsample_layer(unpool1,\n                                           train_stage and train_lyr[6],\n                                           kernels=n_k_lyr[4],\n                                           data_format=data_format)\n\n          with tf.variable_scope(""incept-1""):\n            un_incept_1_lyr2 = lyr.dense_inception(unpool_lyr2, n_b_lyr[4],\n                                                   train_stage and train_lyr[6],\n                                                   summary,\n                                                   data_format=data_format,\n                                                   dropout=self.NET[""dropout""],\n                                                   bn_decay=self.NET[""bn_decay""])\n\n            unpool2 = un_incept_1_lyr2\n\n        with tf.variable_scope(""unpool3""):\n          print(""unpool3"")\n          # input image 256*128 - 240*180\n          unpool_lyr3 = lyr.upsample_layer(unpool2,\n                                           train_stage and train_lyr[7],\n                                           kernels=n_k_lyr[5],\n                                           data_format=data_format)\n          with tf.variable_scope(""incept-1""):\n            un_incept_1_lyr3 = lyr.dense_inception(unpool_lyr3, n_b_lyr[5],\n                                                   train_stage and train_lyr[7],\n                                                   summary,\n                                                   data_format=data_format,\n                                                   dropout=self.NET[""dropout""],\n                                                   bn_decay=self.NET[""bn_decay""])\n\n          unpool3 = un_incept_1_lyr3\n\n      unpooled = unpool3\n\n    with tf.variable_scope(""logits""):\n      # convert to logits with a linear layer\n      logits_linear = lyr.linear_layer(unpooled, self.num_classes,\n                                       train_stage and train_lyr[9],\n                                       summary=summary,\n                                       data_format=data_format)\n\n    # transpose logits back to NHWC\n    if data_format == ""NCHW"":\n      logits = tf.transpose(logits_linear, [0, 2, 3, 1])\n    else:\n      logits = logits_linear\n\n    return logits, code, n_img\n'"
train_py/arch/bonnet_mobilenets.py,20,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Network class, containing definition of the graph\n  API Style should be the same for all nets (Same class name and member functions)\n\'\'\'\n# tf\nimport tensorflow as tf\n\n# common layers\nfrom arch.abstract_net import AbstractNetwork\nimport arch.layer as lyr\n\n\nclass Network(AbstractNetwork):\n  def __init__(self, DATA, NET, TRAIN, logdir):\n    # init parent\n    super().__init__(DATA, NET, TRAIN, logdir)\n\n  def build_graph(self, img_pl, train_stage, data_format=""NCHW""):\n    # some graph info depending on what I will do with it\n    summary = self.TRAIN[\'summary\']\n    train_lyr = self.NET[\'train_lyr\']\n    n_k_lyr = self.NET[\'n_k_lyr\']\n    n_lyr = self.NET[\'n_lyr\']\n    if len(train_lyr) != 7:\n      print(""Wrong length in train list for network. Exiting..."")\n      quit()\n    self.num_classes = len(self.DATA[\'label_map\'])\n\n    # build the graph\n    print(""Building graph"")\n\n    with tf.variable_scope(\'images\'):\n      # resize input to desired size\n      img_resized = tf.image.resize_images(img_pl,\n                                           [self.DATA[""img_prop""][""height""],\n                                            self.DATA[""img_prop""][""width""]])\n      # if on GPU. transpose to NCHW\n      if data_format == ""NCHW"":\n        # convert from NHWC to NCHW (faster on GPU)\n        img_transposed = tf.transpose(img_resized, [0, 3, 1, 2])\n      else:\n        img_transposed = img_resized\n      # normalization of input\n      n_img = (img_transposed - 128) / 128\n\n    with tf.variable_scope(""encoder""):\n      print(""encoder"")\n      with tf.variable_scope(""downsample1""):\n        print(""downsample1"")\n        # input image 1024*512 - 960*720\n        down_lyr1 = lyr.uERF_downsample(n_img, n_k_lyr[0], 3,\n                                        train_stage and train_lyr[0],\n                                        summary,\n                                        data_format=data_format)\n\n        inv_residual_lyr1 = [down_lyr1]\n        for n in range(n_lyr[0]):\n          with tf.variable_scope(""inv-res-"" + str(n)):\n            inv_residual_lyr1.append(lyr.inv_residual(inv_residual_lyr1[-1], n_k_lyr[1],\n                                                      train_stage and train_lyr[0],\n                                                      summary,\n                                                      data_format=data_format,\n                                                      dropout=self.NET[""dropout""],\n                                                      bn_decay=self.NET[""bn_decay""]))\n\n          downsample1 = inv_residual_lyr1[-1]\n\n      with tf.variable_scope(""downsample2""):\n        print(""downsample2"")\n        # input image 512*256 - 480*360\n        down_lyr2 = lyr.uERF_downsample(downsample1, n_k_lyr[2], 3,\n                                        train_stage and train_lyr[1],\n                                        summary,\n                                        data_format=data_format)\n\n        inv_residual_lyr2 = [down_lyr2]\n        for n in range(n_lyr[1]):\n          with tf.variable_scope(""inv-res-"" + str(n)):\n            inv_residual_lyr2.append(lyr.inv_residual(inv_residual_lyr2[-1], n_k_lyr[3],\n                                                      train_stage and train_lyr[1],\n                                                      summary,\n                                                      data_format=data_format,\n                                                      dropout=self.NET[""dropout""],\n                                                      bn_decay=self.NET[""bn_decay""]))\n\n        downsample2 = inv_residual_lyr2[-1]\n\n      with tf.variable_scope(""downsample3""):\n        print(""downsample3"")\n        # input image 512*256 - 480*360\n        down_lyr3 = lyr.uERF_downsample(downsample2, n_k_lyr[4], 3,\n                                        train_stage and train_lyr[2],\n                                        summary,\n                                        data_format=data_format)\n\n        inv_residual_lyr3 = [down_lyr3]\n        for n in range(n_lyr[2]):\n          with tf.variable_scope(""inv-res-"" + str(n)):\n            inv_residual_lyr3.append(lyr.inv_residual(inv_residual_lyr3[-1], n_k_lyr[5],\n                                                      train_stage and train_lyr[2],\n                                                      summary,\n                                                      data_format=data_format,\n                                                      dropout=self.NET[""dropout""],\n                                                      bn_decay=self.NET[""bn_decay""]))\n\n        downsample3 = inv_residual_lyr3[-1]\n\n    code = downsample3\n\n    # end encoder, start decoder\n    print(""============= End of encoder ==============="")\n    print(""size of code: "", code.get_shape().as_list())\n    print(""=========== Beginning of decoder============"")\n\n    with tf.variable_scope(""decoder""):\n      print(""decoder"")\n\n      with tf.variable_scope(""upsample""):\n        print(""upsample"")\n        with tf.variable_scope(""unpool1""):\n          print(""unpool1"")\n          unpool_lyr1 = lyr.upsample_layer(code,\n                                           train_stage and train_lyr[3],\n                                           kernels=n_k_lyr[6],\n                                           data_format=data_format) + downsample2\n\n          inv_residual_unpool_1 = [unpool_lyr1]\n          for n in range(n_lyr[4]):\n            with tf.variable_scope(""inv-res-"" + str(n)):\n              inv_residual_unpool_1.append(lyr.inv_residual(inv_residual_unpool_1[-1], n_k_lyr[7],\n                                                            train_stage and train_lyr[3],\n                                                            summary,\n                                                            data_format=data_format,\n                                                            dropout=self.NET[""dropout""],\n                                                            bn_decay=self.NET[""bn_decay""]))\n\n          unpool1 = inv_residual_unpool_1[-1]\n\n        with tf.variable_scope(""unpool2""):\n          print(""unpool2"")\n          unpool_lyr2 = lyr.upsample_layer(unpool1,\n                                           train_stage and train_lyr[4],\n                                           kernels=n_k_lyr[8],\n                                           data_format=data_format) + downsample1\n\n          inv_residual_unpool_2 = [unpool_lyr2]\n          for n in range(n_lyr[4]):\n            with tf.variable_scope(""inv-res-"" + str(n)):\n              inv_residual_unpool_2.append(lyr.inv_residual(inv_residual_unpool_2[-1], n_k_lyr[9],\n                                                            train_stage and train_lyr[4],\n                                                            summary,\n                                                            data_format=data_format,\n                                                            dropout=self.NET[""dropout""],\n                                                            bn_decay=self.NET[""bn_decay""]))\n\n          unpool2 = inv_residual_unpool_2[-1]\n\n        with tf.variable_scope(""unpool3""):\n          print(""unpool3"")\n          # input image 64*32 - 60*45\n          unpool_lyr3 = lyr.upsample_layer(unpool2 + inv_residual_lyr1[-1],\n                                           train_stage and train_lyr[5],\n                                           kernels=n_k_lyr[10],\n                                           data_format=data_format)\n\n          inv_residual_unpool_3 = [unpool_lyr3]\n          for n in range(n_lyr[4]):\n            with tf.variable_scope(""inv-res-"" + str(n)):\n              inv_residual_unpool_3.append(lyr.inv_residual(inv_residual_unpool_3[-1], n_k_lyr[11],\n                                                            train_stage and train_lyr[5],\n                                                            summary,\n                                                            data_format=data_format,\n                                                            dropout=self.NET[""dropout""],\n                                                            bn_decay=self.NET[""bn_decay""]))\n\n          unpool3 = inv_residual_unpool_3[-1]\n\n      unpooled = unpool3\n\n    with tf.variable_scope(""logits""):\n      # convert to logits with a linear layer\n      logits_linear = lyr.linear_layer(unpooled, self.num_classes,\n                                       train_stage and train_lyr[6],\n                                       summary=summary,\n                                       data_format=data_format)\n\n    # transpose logits back to NHWC\n    if data_format == ""NCHW"":\n      logits = tf.transpose(logits_linear, [0, 2, 3, 1])\n    else:\n      logits = logits_linear\n\n    return logits, code, n_img\n'"
train_py/arch/layer.py,95,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Layer class, containing:\n    - Definition of important layers\n\'\'\'\nimport tensorflow as tf\nimport numpy as np\n\n\ndef weight_variable(shape, train):\n  print(""W: "", shape, ""Train:"", train)\n  return tf.get_variable(""w"", shape=shape,\n                         initializer=tf.variance_scaling_initializer,\n                         trainable=train)\n\n\ndef bias_variable(shape, train):\n  print(""b: "", shape, ""Train:"", train)\n  init = tf.constant(np.full(shape, fill_value=0.1), dtype=tf.float32)\n  return tf.get_variable(""b"",\n                         initializer=init,\n                         trainable=train)\n\n\ndef conv2d(x, W, stride=1, data_format=""NCHW""):\n  # default to a stride of 1 because it is the one we use the most\n  if data_format == ""NCHW"":\n    output = tf.nn.conv2d(x, W,\n                          strides=[1, 1, stride, stride],\n                          padding=\'SAME\', data_format=""NCHW"")\n  else:\n    output = tf.nn.conv2d(x, W,\n                          strides=[1, stride, stride, 1],\n                          padding=\'SAME\', data_format=""NHWC"")\n  return output\n\n\ndef max_pool(x, k_size=2, stride=2, data_format=""NCHW"", pad=\'SAME\'):\n  # default to a stride of 2 because it is the one we use the most\n  if data_format == ""NCHW"":\n    output = tf.nn.max_pool(x,\n                            ksize=[1, 1, k_size, k_size],\n                            strides=[1, 1, stride, stride],\n                            padding=pad, data_format=""NCHW"")\n  else:\n    output = tf.nn.max_pool(x,\n                            ksize=[1, k_size, k_size, 1],\n                            strides=[1, stride, stride, 1],\n                            padding=pad, data_format=""NHWC"")\n  return output\n\n\ndef variable_summaries(var):\n  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n  with tf.variable_scope(\'summaries\'):\n    mean = tf.reduce_mean(var)\n    tf.summary.scalar(\'mean\', mean)\n    with tf.variable_scope(\'stddev\'):\n      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n    tf.summary.scalar(\'stddev\', stddev)\n    tf.summary.scalar(\'max\', tf.reduce_max(var))\n    tf.summary.scalar(\'min\', tf.reduce_min(var))\n    tf.summary.histogram(\'histogram\', var)\n\n\ndef spatial_dropout(x, keep_prob, training, data_format=""NCHW""):\n  """"""\n    Drop random channels, using tf.nn.dropout\n    (Partially from https://stats.stackexchange.com/questions/282282/how-is-spatial-dropout-in-2d-implemented)\n  """"""\n  if training:\n    with tf.variable_scope(""spatial_dropout""):\n      batch_size = x.get_shape().as_list()[0]\n      if data_format == ""NCHW"":\n        # depth of previous layer feature map\n        prev_depth = x.get_shape().as_list()[1]\n        num_feature_maps = [batch_size, prev_depth]\n      else:\n        # depth of previous layer feature map\n        prev_depth = x.get_shape().as_list()[3]\n        num_feature_maps = [batch_size, prev_depth]\n\n      # get some uniform noise between keep_prob and 1 + keep_prob\n      random_tensor = keep_prob\n      random_tensor += tf.random_uniform(num_feature_maps,\n                                         dtype=x.dtype)\n\n      # if we take the floor of this, we get a binary matrix where\n      # (1-keep_prob)% of the values are 0 and the rest are 1\n      binary_tensor = tf.floor(random_tensor)\n\n      # Reshape to multiply our feature maps by this tensor correctly\n      if data_format == ""NCHW"":\n        binary_tensor = tf.reshape(binary_tensor,\n                                   [batch_size, prev_depth, 1, 1])\n      else:\n        binary_tensor = tf.reshape(binary_tensor,\n                                   [batch_size, 1, 1, prev_depth])\n\n      # Zero out feature maps where appropriate; scale up to compensate\n      ret = tf.div(x, keep_prob) * binary_tensor\n  else:\n    ret = x\n\n  return ret\n\n\ndef conv_layer(input_tensor, kernel_nr, kernel_size, stride,\n               train, summary=False, bnorm=True, relu=True,\n               data_format=""NCHW"", bn_decay=0.95):\n  """"""Builds a full conv layer, with variables and relu\n  Args:\n    input_tensor: input tensor\n    kernel_nr: This layer\'s number of filters\n    kernel_size: Size of the kernel [h, w]\n    train: If we want to train this layer or not\n    bnorm: Use batchnorm?\n    relu: Use relu?\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  # get previous depth from input tensor\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  with tf.variable_scope(\'weights\'):\n    W = weight_variable([kernel_size[0], kernel_size[1],\n                         prev_depth, kernel_nr], train)\n    if summary:\n      variable_summaries(W)\n\n  with tf.variable_scope(\'convolution\'):\n    preactivations = conv2d(input_tensor, W, stride,\n                            data_format=data_format)\n    if summary:\n      variable_summaries(preactivations)\n\n  if bnorm:\n    with tf.variable_scope(\'batchnorm\'):\n      normalized = tf.contrib.layers.batch_norm(preactivations,\n                                                center=True, scale=True,\n                                                is_training=train,\n                                                data_format=data_format,\n                                                fused=True,\n                                                decay=bn_decay)\n      if summary:\n        variable_summaries(normalized)\n  else:\n    normalized = preactivations\n\n  if relu:\n    with tf.variable_scope(\'relu\'):\n      output = relu = tf.nn.leaky_relu(normalized)\n    if summary:\n      variable_summaries(relu)\n  else:\n    output = normalized\n\n  return output\n\n\ndef upsample_layer(input_tensor, train, upsample_factor=2, kernels=-1, data_format=""NCHW""):\n  """"""Builds a full conv layer, with variables and relu\n  Args:\n    input_tensor: input tensor\n    upsample_factor: how much to upsample\n    kernels: -1 = same as input, otherwise number of kernels to upsample\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the upsampling\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  if kernels < 0:\n    kernel_nr = prev_depth\n  else:\n    kernel_nr = kernels\n\n  with tf.variable_scope(\'upconv\'):\n    output = tf.contrib.layers.conv2d_transpose(input_tensor,\n                                                kernel_nr,\n                                                (2, 2),\n                                                stride=2,\n                                                padding=\'VALID\',\n                                                data_format=data_format,\n                                                activation_fn=tf.nn.relu,\n                                                weights_initializer=tf.variance_scaling_initializer,\n                                                weights_regularizer=None,\n                                                trainable=train)\n  print(""W: "", [2, 2, prev_depth, kernel_nr], ""Train:"", train)\n\n  return output\n\n\ndef asym_conv_layer(input_tensor, kernel_nr, kernel_size, train, summary=False, bnorm=True, relu=True, data_format=""NCHW""):\n  """"""Builds a full asymetric conv layer, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    kernel_nr: This layer\'s number of filters\n    kernel_size: Size of the kernel [symetric]\n    train: ;If we want to train this layer or not\n    bnorm: Use batchnorm?\n    relu: Use relu?\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  with tf.variable_scope(\'horiz\'):\n    conv = conv_layer(input_tensor, kernel_nr, [kernel_size, 1], 1, train,\n                      summary=summary, bnorm=bnorm, relu=False, data_format=data_format)\n  with tf.variable_scope(\'vert\'):\n    output = conv_layer(conv, kernel_nr, [1, kernel_size], 1, train,\n                        summary=summary, bnorm=bnorm, relu=relu, data_format=data_format)\n\n  return output\n\n\ndef inception(input_tensor, train, summary=False, data_format=""NCHW"", dropout=0.3, bn_decay=0.95):\n  """"""Builds a NEW full asymmetric conv layer non-bt, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  if prev_depth % 4:\n    print(""Warning! Depth cannot be divided by 4 in inception module"")\n\n  # kernel number to match input\n  kernel_nr = int(prev_depth / 4)\n\n  with tf.variable_scope(\'inception\'):\n    with tf.variable_scope(\'bottleneck\'):\n      bt = conv_layer(input_tensor, kernel_nr, [1, 1],\n                      1, train, summary=summary, bnorm=False,\n                      relu=False, data_format=data_format)\n\n    with tf.variable_scope(\'3x3\'):\n      asym3 = asym_conv_layer(bt, kernel_nr * 2, 3,\n                              train, summary=summary, bnorm=False,\n                              relu=False, data_format=data_format)\n\n    with tf.variable_scope(\'5x5\'):\n      asym5 = asym_conv_layer(bt, kernel_nr, 5,\n                              train, summary=summary, bnorm=False,\n                              relu=False, data_format=data_format)\n\n    with tf.variable_scope(\'7x7\'):\n      asym7 = asym_conv_layer(bt, kernel_nr, 7,\n                              train, summary=summary, bnorm=False,\n                              relu=False, data_format=data_format)\n\n    with tf.variable_scope(\'concat\'):\n      if data_format == ""NCHW"":\n        concat = tf.concat([asym3, asym5, asym7], 1)\n      else:\n        concat = tf.concat([asym3, asym5, asym7], 3)\n\n    with tf.variable_scope(\'batchnorm\'):\n      # use batch renorm for small minibatches\n      normalized = tf.contrib.layers.batch_norm(concat,\n                                                center=True, scale=True,\n                                                is_training=train,\n                                                data_format=data_format,\n                                                fused=True,\n                                                decay=bn_decay,\n                                                renorm=True)\n      if summary:\n        variable_summaries(normalized)\n\n    # add the residual\n    with tf.variable_scope(\'out\'):\n      drop = spatial_dropout(normalized, keep_prob=1 - dropout,\n                             training=train, data_format=data_format)\n      output = tf.nn.leaky_relu(drop)\n\n  return output\n\n\ndef dense_inception(input_tensor, n_blocks, train, summary=False, data_format=""NCHW"", dropout=0.3, bn_decay=0.95):\n  """"""Builds a NEW full asymmetric conv layer non-bt, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    n_blocks: number of layers inside block\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  with tf.variable_scope(\'dense_inception\'):\n    if data_format == ""NCHW"":\n      # depth of previous layer feature map\n      prev_depth = input_tensor.get_shape().as_list()[1]\n    else:\n      # depth of previous layer feature map\n      prev_depth = input_tensor.get_shape().as_list()[3]\n\n    blocks = input_tensor\n    for b in range(n_blocks):\n      with tf.variable_scope(\'skip_conv_\' + str(b)):\n        out_block = inception(blocks, train, summary=summary,\n                              data_format=data_format, dropout=dropout,\n                              bn_decay=bn_decay)\n        if data_format == ""NCHW"":\n          blocks = tf.concat([blocks, out_block], 1)\n        else:\n          blocks = tf.concat([blocks, out_block], 3)\n\n    # linear squash\n    with tf.variable_scope(\'squash\'):\n      squash = conv_layer(blocks, prev_depth, [1, 1], 1, train,\n                          summary=summary, bnorm=False, relu=False,\n                          data_format=data_format)\n\n    with tf.variable_scope(\'res\'):\n      output = squash + input_tensor\n\n  return output\n\n\ndef inv_residual(input_tensor, channel_mult, train, summary=False,\n                 data_format=""NCHW"", dropout=0.3, bn_decay=0.95):\n  """"""Builds a NEW full asymmetric conv layer non-bt, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    channel_mult: number of filters in each inverted residual (ratio with input filters)\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  # number filters\n  n_filters = channel_mult * prev_depth\n\n  with tf.variable_scope(""inverted_residual""):\n    with tf.variable_scope(""conv""):\n      with tf.variable_scope(""bnorm""):\n        conv_norm = tf.contrib.layers.batch_norm(input_tensor,\n                                                 center=True, scale=True,\n                                                 is_training=train,\n                                                 data_format=data_format,\n                                                 fused=True,\n                                                 decay=bn_decay)\n\n      with tf.variable_scope(""conv""):\n        with tf.variable_scope(""depthwise_filter""):\n          depthwise_filter = weight_variable([3, 3, prev_depth, channel_mult], train)\n          if summary:\n            variable_summaries(depthwise_filter)\n        with tf.variable_scope(""pointwise_filter""):\n          pointwise_filter = weight_variable(\n              [1, 1, n_filters, prev_depth], train)\n          if summary:\n            variable_summaries(pointwise_filter)\n        with tf.variable_scope(""conv""):\n          if data_format == ""NCHW"":\n            conv = tf.nn.separable_conv2d(conv_norm,\n                                          depthwise_filter,\n                                          pointwise_filter,\n                                          strides=[1, 1, 1, 1],\n                                          padding=""SAME"",\n                                          data_format=""NCHW"")\n          else:\n            conv = tf.nn.separable_conv2d(conv_norm,\n                                          depthwise_filter,\n                                          pointwise_filter,\n                                          strides=[1, 1, 1, 1],\n                                          padding=""SAME"",\n                                          data_format=""NHWC"")\n\n      with tf.variable_scope(""residual""):\n        dropout = spatial_dropout(conv, keep_prob=1 - dropout,\n                                  training=train, data_format=data_format)\n\n        residual = dropout + input_tensor\n\n      with tf.variable_scope(""out""):\n        output = tf.nn.leaky_relu(residual)\n\n  return output\n\n\ndef uERF_non_bt(input_tensor, kernel_size, train, summary=False, data_format=""NCHW"", dropout=0.3, bn_decay=0.95):\n  """"""Builds a NEW full asymmetric conv layer non-bt, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    kernel_size: Size of the kernel [symetric]\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n  kernel_nr = prev_depth\n\n  # batchnorm once\n  with tf.variable_scope(\'non_bt\'):\n    # normal assym bottleneck with relus, no batchnorm\n    with tf.variable_scope(\'asym1\'):\n      asym1 = asym_conv_layer(input_tensor, kernel_nr, kernel_size,\n                              train, summary=summary, bnorm=False,\n                              relu=True, data_format=data_format)\n\n    with tf.variable_scope(\'asym2\'):\n      asym2 = asym_conv_layer(asym1, kernel_nr, kernel_size,\n                              train, summary=summary, bnorm=False,\n                              relu=False, data_format=data_format)\n\n    with tf.variable_scope(\'batchnorm\'):\n      normalized = tf.contrib.layers.batch_norm(asym2,\n                                                center=True, scale=True,\n                                                is_training=train,\n                                                data_format=data_format,\n                                                fused=True,\n                                                decay=bn_decay)\n      if summary:\n        variable_summaries(normalized)\n\n    # add the residual\n    with tf.variable_scope(\'res\'):\n      drop = spatial_dropout(normalized, keep_prob=1 - dropout,\n                             training=train, data_format=data_format)\n      with tf.variable_scope(\'relu\'):\n        relu = tf.nn.leaky_relu(input_tensor + drop)\n      output = relu\n\n  return output\n\n\ndef uERF_downsample(input_tensor, kernel_nr, kernel_size, train,\n                    summary=False, data_format=""NCHW""):\n  """"""Builds a NEW downsample module, with variables and relu.\n  Args:\n    input_tensor: input tensor\n    kernel_nr: This layer\'s number of real filters\n    kernel_size: Size of the kernel [symmetric]\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  conv_kernels = kernel_nr - prev_depth\n  if conv_kernels <= 0:\n    print(""Wrong number of kernels. Exiting..."")\n    quit()\n\n  with tf.variable_scope(\'downsample\'):\n    with tf.variable_scope(\'conv\'):\n      conv = conv_layer(input_tensor, conv_kernels, [kernel_size, kernel_size],\n                        2, train, summary=summary, bnorm=True,\n                        relu=True, data_format=data_format)\n\n    with tf.variable_scope(\'pool\'):\n      pool = max_pool(input_tensor, data_format=data_format)\n\n    with tf.variable_scope(\'concat\'):\n      if data_format == ""NCHW"":\n        output = tf.concat([conv, pool], 1)\n      else:\n        output = tf.concat([conv, pool], 3)\n\n  return output\n\n\ndef psp_layer(input_tensor, piramids, biggest_piramid, train, summary=False, data_format=""NCHW""):\n  """"""Builds a psp layer to get context info.\n  Args:\n    input_tensor: input tensor\n    piramids: number of pooling layers to use\n    biggest_piramid: Size of biggest pooling kernel\n    train: If we want to train this layer or not\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the convolution\n  """"""\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n    prev_h = input_tensor.get_shape().as_list()[2]\n    prev_w = input_tensor.get_shape().as_list()[3]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    prev_h = input_tensor.get_shape().as_list()[1]\n    prev_w = input_tensor.get_shape().as_list()[2]\n  batch_size = input_tensor.get_shape().as_list()[0]\n\n  # calculate pooling kernel sizes\n  min_h_w = min(prev_h, prev_w)\n  if biggest_piramid > min_h_w:\n    print(""WARNING! Biggest piramid is bigger than the shortest code dimension"")\n  kernel_sizes = []\n  for i in range(piramids):\n    k_size = int(biggest_piramid / (2 ** i))\n    kernel_sizes.append(k_size)\n    print(""PSP KSIZE: "", k_size)\n\n  # number of convolutions for each level\n  nodes = input_tensor\n  n_convs = int(prev_depth / float(len(kernel_sizes)))\n  print(""NCONVS: "", n_convs)\n\n  with tf.variable_scope(\'psp\'):\n    p = piramids\n    for k_size in kernel_sizes:\n      print(\'psp-\' + str(p))\n      with tf.variable_scope(\'psp-\' + str(p)):\n        p -= 1  # if I call the module psp-ksize it will break when retraining\n        # pool\n        pool = max_pool(input_tensor, k_size=k_size,\n                        stride=1, data_format=data_format, pad=\'VALID\')\n        # conv\n        conv = conv_layer(pool, n_convs, (5, 5), 1, train, summary=summary,\n                          bnorm=False, relu=True, data_format=data_format)\n\n        # upsample kernel and deconv\n        # make kernel with 1\'s only where I want to upsample etc,etc\n        k_shape = (k_size, k_size, n_convs, n_convs)\n        k = np.zeros(k_shape)\n        ones = np.ones((k_size, k_size))\n        for i in range(n_convs):\n          k[:, :, i, i] = ones\n        k_tf = tf.constant(k, dtype=tf.float32)\n        print(""Upsample k: "", k_shape)\n        if data_format == ""NCHW"":\n          upsample = tf.nn.conv2d_transpose(conv,\n                                            k_tf,\n                                            output_shape=(batch_size, n_convs,\n                                                          prev_h, prev_w),\n                                            strides=(1, 1, 1, 1),\n                                            padding=\'VALID\',\n                                            data_format=data_format)\n          nodes = tf.concat([nodes, upsample], 1)\n        else:\n          upsample = tf.nn.conv2d_transpose(conv,\n                                            k_tf,\n                                            output_shape=(batch_size, prev_h,\n                                                          prev_w, n_convs),\n                                            strides=(1, 1, 1, 1),\n                                            padding=\'VALID\',\n                                            data_format=data_format)\n          nodes = tf.concat([nodes, upsample], 3)\n\n  return nodes\n\n\ndef reduce(input_tensor, skip, n_kernels, train, summary=False, data_format=""NCHW""):\n  """""" Concatenate input tensor and skip connection and apply a 1x1 conv\n  to reduce to n_kernels depth.\n  Args:\n    input_tensor: input tensor\n    skip: skip connection from encoder\n    n_kernels: number of new kernels\n    train: If we want to train this layer or not\n    summary: Save summaries\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the op\n  """"""\n  with tf.variable_scope(""reduce""):\n    if data_format == ""NCHW"":\n      concat = tf.concat([input_tensor, skip], 1)\n    else:\n      concat = tf.concat([input_tensor, skip], 3)\n\n    output = conv_layer(concat, n_kernels, [1, 1], 1, train,\n                        summary=summary, bnorm=True, relu=True, data_format=data_format)\n\n  return output\n\n\n# definition of pre-softmax layer + its variables (softmax is done by the cost\n# function, so to use this model, softmax needs to be applied afterwards)\n\n\ndef linear_layer(input_tensor, classes, train,\n                 summary=False, rf=1, data_format=""NCHW""):\n  """"""Builds a logit layer that we apply the softmax to, with variables\n  Args:\n    input_tensor: input tensor\n    classes: Number of classes to classify in the output\n    train: If we want to train this layer or not\n    rf = receptive field of output neuron (for eliminating wrong borders)\n    data_format: Self explanatory\n  Returns:\n    output: Output tensor from the linear layer (end of inference)\n  """"""\n  # get previous depth from input tensor\n  if data_format == ""NCHW"":\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[1]\n  else:\n    # depth of previous layer feature map\n    prev_depth = input_tensor.get_shape().as_list()[3]\n\n  with tf.variable_scope(\'weights\'):\n    W = weight_variable([rf, rf, prev_depth, classes], train)\n    if summary:\n      variable_summaries(W)\n\n  with tf.variable_scope(\'biases\'):\n    b = bias_variable([classes], train)\n    if summary:\n      variable_summaries(b)\n\n  with tf.variable_scope(\'linear\'):\n    output = tf.nn.bias_add(\n        conv2d(input_tensor, W, data_format=data_format), b, data_format=data_format)\n    if summary:\n      variable_summaries(output)\n\n  return output\n'"
train_py/arch/msg.py,10,"b'# This file is NOT a part of Bonnet. It was included from OpenAI\'s code on\n# this GitHub repo: https://github.com/openai/gradient-checkpointing\n# Give them a star, they deserve it.\n\nfrom toposort import toposort\nimport contextlib\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.graph_editor as ge\nimport time\nimport sys\nsys.setrecursionlimit(10000)\n# refers back to current module if we decide to split helpers out\nutil = sys.modules[__name__]\n\n# getting rid of ""WARNING:tensorflow:VARIABLES collection name is deprecated""\nsetattr(tf.GraphKeys, ""VARIABLES"", ""variables"")\n\n# save original gradients since tf.gradient could be monkey-patched to point\n# to our version\nfrom tensorflow.python.ops import gradients as tf_gradients_lib\ntf_gradients = tf_gradients_lib.gradients\n\nMIN_CHECKPOINT_NODE_SIZE = 1024    # use lower value during testing\n\n# specific versions we can use to do process-wide replacement of tf.gradients\n\n\ndef gradients_speed(ys, xs, grad_ys=None, **kwargs):\n    print(""Using OpenAI\'s checkpoint\'d grads. Speed optimization."")\n    return gradients(ys, xs, grad_ys, checkpoints=\'speed\', **kwargs)\n\n\ndef gradients_memory(ys, xs, grad_ys=None, **kwargs):\n    print(""Using OpenAI\'s checkpoint\'d grads. Memory optimization."")\n    return gradients(ys, xs, grad_ys, checkpoints=\'memory\', **kwargs)\n\n\ndef gradients_collection(ys, xs, grad_ys=None, **kwargs):\n    print(""Using OpenAI\'s checkpoint\'d grads. Specific collection."")\n    return gradients(ys, xs, grad_ys, checkpoints=\'collection\', **kwargs)\n\n\ndef gradients(ys, xs, grad_ys=None, checkpoints=\'collection\', **kwargs):\n    \'\'\'\n    Authors: Tim Salimans & Yaroslav Bulatov\n\n    memory efficient gradient implementation inspired by ""Training Deep Nets with Sublinear Memory Cost""\n    by Chen et al. 2016 (https://arxiv.org/abs/1604.06174)\n\n    ys,xs,grad_ys,kwargs are the arguments to standard tensorflow tf.gradients\n    (https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#gradients)\n\n    \'checkpoints\' can either be\n        - a list consisting of tensors from the forward pass of the neural net\n          that we should re-use when calculating the gradients in the backward pass\n          all other tensors that do not appear in this list will be re-computed\n        - a string specifying how this list should be determined. currently we support\n            - \'speed\':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,\n                        so checkpointing them maximizes the running speed\n                        (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n            - \'memory\': try to minimize the memory usage\n                        (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n            - \'collection\': look for a tensorflow collection named \'checkpoints\', which holds the tensors to checkpoint\n    \'\'\'\n\n    #    print(""Calling memsaving gradients with"", checkpoints)\n    if not isinstance(ys, list):\n        ys = [ys]\n    if not isinstance(xs, list):\n        xs = [xs]\n\n    bwd_ops = ge.get_backward_walk_ops([y.op for y in ys],\n                                       inclusive=True)\n\n    debug_print(""bwd_ops: %s"", bwd_ops)\n\n    # forward ops are all ops that are candidates for recomputation\n    fwd_ops = ge.get_forward_walk_ops([x.op for x in xs],\n                                      inclusive=True,\n                                      within_ops=bwd_ops)\n    debug_print(""fwd_ops: %s"", fwd_ops)\n\n    # exclude ops with no inputs\n    fwd_ops = [op for op in fwd_ops if op._inputs]\n\n    # don\'t recompute xs, remove variables\n    xs_ops = _to_ops(xs)\n    fwd_ops = [op for op in fwd_ops if not op in xs_ops]\n    fwd_ops = [op for op in fwd_ops if not \'/assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/Assign\' in op.name]\n    fwd_ops = [op for op in fwd_ops if not \'/read\' in op.name]\n    ts_all = ge.filter_ts(fwd_ops, True)  # get the tensors\n    ts_all = [t for t in ts_all if \'/read\' not in t.name]\n    ts_all = set(ts_all) - set(xs) - set(ys)\n\n    # construct list of tensors to checkpoint during forward pass, if not\n    # given as input\n    if type(checkpoints) is not list:\n        if checkpoints == \'collection\':\n            checkpoints = tf.get_collection(\'checkpoints\')\n\n        elif checkpoints == \'speed\':\n            # checkpoint all expensive ops to maximize running speed\n            checkpoints = ge.filter_ts_from_regex(\n                fwd_ops, \'conv2d|Conv|MatMul\')\n\n        elif checkpoints == \'memory\':\n\n            # remove very small tensors and some weird ops\n            def fixdims(t):  # tf.Dimension values are not compatible with int, convert manually\n                try:\n                    return [int(e if e.value is not None else 64) for e in t]\n                except:\n                    return [0]  # unknown shape\n            ts_all = [t for t in ts_all if np.prod(\n                fixdims(t.shape)) > MIN_CHECKPOINT_NODE_SIZE]\n            ts_all = [t for t in ts_all if \'L2Loss\' not in t.name]\n            ts_all = [t for t in ts_all if \'entropy\' not in t.name]\n            ts_all = [t for t in ts_all if \'FusedBatchNorm\' not in t.name]\n            ts_all = [t for t in ts_all if \'Switch\' not in t.name]\n            ts_all = [t for t in ts_all if \'dropout\' not in t.name]\n\n            # filter out all tensors that are inputs of the backward graph\n            with util.capture_ops() as bwd_ops:\n                tf_gradients(ys, xs, grad_ys, **kwargs)\n\n            bwd_inputs = [t for op in bwd_ops for t in op.inputs]\n            # list of tensors in forward graph that is in input to bwd graph\n            ts_filtered = list(set(bwd_inputs).intersection(ts_all))\n            debug_print(""Using tensors %s"", ts_filtered)\n\n            # try two slightly different ways of getting bottlenecks tensors\n            # to checkpoint\n            for ts in [ts_filtered, ts_all]:\n\n                # get all bottlenecks in the graph\n                bottleneck_ts = []\n                for t in ts:\n                    b = set(ge.get_backward_walk_ops(\n                        t.op, inclusive=True, within_ops=fwd_ops))\n                    f = set(ge.get_forward_walk_ops(\n                        t.op, inclusive=False, within_ops=fwd_ops))\n                    # check that there are not shortcuts\n                    b_inp = set(\n                        [inp for op in b for inp in op.inputs]).intersection(ts_all)\n                    f_inp = set(\n                        [inp for op in f for inp in op.inputs]).intersection(ts_all)\n                    if not set(b_inp).intersection(f_inp) and len(b_inp) + len(f_inp) >= len(ts_all):\n                        bottleneck_ts.append(t)  # we have a bottleneck!\n                    else:\n                        debug_print(""Rejected bottleneck candidate and ops %s"", [\n                                    t] + list(set(ts_all) - set(b_inp) - set(f_inp)))\n\n                # success? or try again without filtering?\n                if len(bottleneck_ts) >= np.sqrt(len(ts_filtered)):  # yes, enough bottlenecks found!\n                    break\n\n            if not bottleneck_ts:\n                raise Exception(\n                    \'unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=""speed"".\')\n\n            # sort the bottlenecks\n            bottlenecks_sorted_lists = tf_toposort(\n                bottleneck_ts, within_ops=fwd_ops)\n            sorted_bottlenecks = [\n                t for ts in bottlenecks_sorted_lists for t in ts]\n\n            # save an approximately optimal number ~ sqrt(N)\n            N = len(ts_filtered)\n            if len(bottleneck_ts) <= np.ceil(np.sqrt(N)):\n                checkpoints = sorted_bottlenecks\n            else:\n                step = int(np.ceil(len(bottleneck_ts) / np.sqrt(N)))\n                checkpoints = sorted_bottlenecks[step::step]\n\n        else:\n            raise Exception(\n                \'%s is unsupported input for ""checkpoints""\' % (checkpoints,))\n\n    checkpoints = list(set(checkpoints).intersection(ts_all))\n\n    # at this point automatic selection happened and checkpoints is list of nodes\n    assert isinstance(checkpoints, list)\n\n    debug_print(""Checkpoint nodes used: %s"", checkpoints)\n    # better error handling of special cases\n    # xs are already handled as checkpoint nodes, so no need to include them\n    xs_intersect_checkpoints = set(xs).intersection(set(checkpoints))\n    if xs_intersect_checkpoints:\n        debug_print(""Warning, some input nodes are also checkpoint nodes: %s"",\n                    xs_intersect_checkpoints)\n    ys_intersect_checkpoints = set(ys).intersection(set(checkpoints))\n    debug_print(""ys: %s, checkpoints: %s, intersect: %s"", ys, checkpoints,\n                ys_intersect_checkpoints)\n    # saving an output node (ys) gives no benefit in memory while creating\n    # new edge cases, exclude them\n    if ys_intersect_checkpoints:\n        debug_print(""Warning, some output nodes are also checkpoints nodes: %s"",\n                    format_ops(ys_intersect_checkpoints))\n\n    # remove initial and terminal nodes from checkpoints list if present\n    checkpoints = list(set(checkpoints) - set(ys) - set(xs))\n\n    # check that we have some nodes to checkpoint\n    if not checkpoints:\n        raise Exception(\'no checkpoints nodes found or given as input! \')\n\n    # disconnect dependencies between checkpointed tensors\n    checkpoints_disconnected = {}\n    for x in checkpoints:\n        if x.op and x.op.name is not None:\n            grad_node = tf.stop_gradient(x, name=x.op.name + ""_sg"")\n        else:\n            grad_node = tf.stop_gradient(x)\n        checkpoints_disconnected[x] = grad_node\n\n    # partial derivatives to the checkpointed tensors and xs\n    ops_to_copy = fast_backward_ops(seed_ops=[y.op for y in ys],\n                                    stop_at_ts=checkpoints, within_ops=fwd_ops)\n    debug_print(""Found %s ops to copy within fwd_ops %s, seed %s, stop_at %s"",\n                len(ops_to_copy), fwd_ops, [r.op for r in ys], checkpoints)\n    debug_print(""ops_to_copy = %s"", ops_to_copy)\n    debug_print(""Processing list %s"", ys)\n    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n    copied_ops = info._transformed_ops.values()\n    debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n    ge.reroute_ts(checkpoints_disconnected.values(),\n                  checkpoints_disconnected.keys(), can_modify=copied_ops)\n    debug_print(""Rewired %s in place of %s restricted to %s"",\n                checkpoints_disconnected.values(), checkpoints_disconnected.keys(), copied_ops)\n\n    # get gradients with respect to current boundary + original x\'s\n    copied_ys = [info._transformed_ops[y.op]._outputs[0] for y in ys]\n    boundary = list(checkpoints_disconnected.values())\n    dv = tf_gradients(ys=copied_ys, xs=boundary +\n                      xs, grad_ys=grad_ys, **kwargs)\n    debug_print(""Got gradients %s"", dv)\n    debug_print(""for %s"", copied_ys)\n    debug_print(""with respect to %s"", boundary + xs)\n\n    inputs_to_do_before = [y.op for y in ys]\n    if grad_ys is not None:\n        inputs_to_do_before += grad_ys\n    wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n    my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n    # partial derivatives to the checkpointed nodes\n    # dictionary of ""node: backprop"" for nodes in the boundary\n    d_checkpoints = {r: dr for r, dr in zip(checkpoints_disconnected.keys(),\n                                            dv[:len(checkpoints_disconnected)])}\n    # partial derivatives to xs (usually the params of the neural net)\n    d_xs = dv[len(checkpoints_disconnected):]\n\n    # incorporate derivatives flowing through the checkpointed nodes\n    checkpoints_sorted_lists = tf_toposort(checkpoints, within_ops=fwd_ops)\n    for ts in checkpoints_sorted_lists[::-1]:\n        debug_print(""Processing list %s"", ts)\n        checkpoints_other = [r for r in checkpoints if r not in ts]\n        checkpoints_disconnected_other = [\n            checkpoints_disconnected[r] for r in checkpoints_other]\n\n        # copy part of the graph below current checkpoint node, stopping at\n        # other checkpoints nodes\n        ops_to_copy = fast_backward_ops(within_ops=fwd_ops, seed_ops=[\n                                        r.op for r in ts], stop_at_ts=checkpoints_other)\n        debug_print(""Found %s ops to copy within %s, seed %s, stop_at %s"",\n                    len(ops_to_copy), fwd_ops, [r.op for r in ts],\n                    checkpoints_other)\n        debug_print(""ops_to_copy = %s"", ops_to_copy)\n        if not ops_to_copy:  # we\'re done!\n            break\n        copied_sgv, info = ge.copy_with_input_replacements(\n            ge.sgv(ops_to_copy), {})\n        copied_ops = info._transformed_ops.values()\n        debug_print(""Copied %s to %s"", ops_to_copy, copied_ops)\n        ge.reroute_ts(checkpoints_disconnected_other,\n                      checkpoints_other, can_modify=copied_ops)\n        debug_print(""Rewired %s in place of %s restricted to %s"",\n                    checkpoints_disconnected_other, checkpoints_other, copied_ops)\n\n        # gradient flowing through the checkpointed node\n        boundary = [info._transformed_ops[r.op]._outputs[0] for r in ts]\n        substitute_backprops = [d_checkpoints[r] for r in ts]\n        dv = tf_gradients(boundary,\n                          checkpoints_disconnected_other + xs,\n                          grad_ys=substitute_backprops, **kwargs)\n        debug_print(""Got gradients %s"", dv)\n        debug_print(""for %s"", boundary)\n        debug_print(""with respect to %s"", checkpoints_disconnected_other + xs)\n        debug_print(""with boundary backprop substitutions %s"",\n                    substitute_backprops)\n\n        inputs_to_do_before = [d_checkpoints[r].op for r in ts]\n        wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n        my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n\n        # partial derivatives to the checkpointed nodes\n        for r, dr in zip(checkpoints_other, dv[:len(checkpoints_other)]):\n            if dr is not None:\n                if d_checkpoints[r] is None:\n                    d_checkpoints[r] = dr\n                else:\n                    d_checkpoints[r] += dr\n\n        # partial derivatives to xs (usually the params of the neural net)\n        d_xs_new = dv[len(checkpoints_other):]\n        for j in range(len(xs)):\n            if d_xs_new[j] is not None:\n                if d_xs[j] is None:\n                    d_xs[j] = d_xs_new[j]\n                else:\n                    d_xs[j] += d_xs_new[j]\n\n    return d_xs\n\n\ndef tf_toposort(ts, within_ops=None):\n    all_ops = ge.get_forward_walk_ops(\n        [x.op for x in ts], within_ops=within_ops)\n\n    deps = {}\n    for op in all_ops:\n        for o in op.outputs:\n            deps[o] = set(op.inputs)\n    sorted_ts = toposort(deps)\n\n    # only keep the tensors from our original list\n    ts_sorted_lists = []\n    for l in sorted_ts:\n        keep = list(set(l).intersection(ts))\n        if keep:\n            ts_sorted_lists.append(keep)\n\n    return ts_sorted_lists\n\n\ndef fast_backward_ops(within_ops, seed_ops, stop_at_ts):\n    bwd_ops = set(ge.get_backward_walk_ops(seed_ops, stop_at_ts=stop_at_ts))\n    ops = bwd_ops.intersection(within_ops).difference(\n        [t.op for t in stop_at_ts])\n    return list(ops)\n\n\n@contextlib.contextmanager\ndef capture_ops():\n    """"""Decorator to capture ops created in the block.\n    with capture_ops() as ops:\n      # create some ops\n    print(ops) # => prints ops created.\n    """"""\n\n    micros = int(time.time() * 10**6)\n    scope_name = str(micros)\n    op_list = []\n    with tf.name_scope(scope_name):\n        yield op_list\n\n    g = tf.get_default_graph()\n    op_list.extend(ge.select_ops(scope_name + ""/.*"", graph=g))\n\n\ndef _to_op(tensor_or_op):\n    if hasattr(tensor_or_op, ""op""):\n        return tensor_or_op.op\n    return tensor_or_op\n\n\ndef _to_ops(iterable):\n    if not _is_iterable(iterable):\n        return iterable\n    return [_to_op(i) for i in iterable]\n\n\ndef _is_iterable(o):\n    try:\n        _ = iter(o)\n    except Exception:\n        return False\n    return True\n\n\nDEBUG_LOGGING = False\n\n\ndef debug_print(s, *args):\n    """"""Like logger.log, but also replaces all TensorFlow ops/tensors with their\n    names. Sensitive to value of DEBUG_LOGGING, see enable_debug/disable_debug\n\n    Usage:\n      debug_print(""see tensors %s for %s"", tensorlist, [1,2,3])\n    """"""\n\n    if DEBUG_LOGGING:\n        formatted_args = [format_ops(arg) for arg in args]\n        print(""DEBUG "" + s % tuple(formatted_args))\n\n\ndef format_ops(ops, sort_outputs=True):\n    """"""Helper method for printing ops. Converts Tensor/Operation op to op.name,\n    rest to str(op).""""""\n\n    if hasattr(ops, \'__iter__\') and not isinstance(ops, str):\n        l = [(op.name if hasattr(op, ""name"") else str(op)) for op in ops]\n        if sort_outputs:\n            return sorted(l)\n        return l\n    else:\n        return ops.name if hasattr(ops, ""name"") else str(ops)\n\n\ndef my_add_control_inputs(wait_to_do_ops, inputs_to_do_before):\n    for op in wait_to_do_ops:\n        ci = [i for i in inputs_to_do_before if op.control_inputs is None or i not in op.control_inputs]\n        ge.add_control_inputs(op, ci)\n'"
train_py/dataset/abstract_dataset.py,0,"b'# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n"""""" Abstraction for every dataset. All of the datasets we use should have\n    this overall structure\n""""""\n\n# queue for the pre-fetching\nimport queue\nimport threading\nimport cv2\nimport numpy as np\nimport os\nimport random\n\n\nclass ImgFetcher(threading.Thread):\n  def __init__(self, name, dataset):\n    threading.Thread.__init__(self)\n    self.name = name\n    self.dataset = dataset\n    self.die = False\n    if self.name == ""ImgBufftrain"":\n      # randomize data at the beginning\n      combined = list(zip(self.dataset.images, self.dataset.labels))\n      random.shuffle(combined)\n      self.dataset.images[:], self.dataset.labels[:] = zip(*combined)\n\n  def augment(self, img, lbl):\n    # define the augmentations\n\n    # flip horizontally?\n    flip = bool(random.getrandbits(1))\n    if flip:\n      img = cv2.flip(img, 1)\n      lbl = cv2.flip(lbl, 1)\n\n    # gamma shift\n    gamma = bool(random.getrandbits(1))\n    if gamma:\n      # build a lookup table mapping the pixel values [0, 255] to\n      # their adjusted gamma values\n      randomGamma = np.random.uniform(low=0.8, high=1.2)\n      invGamma = 1.0 / randomGamma\n      table = np.array([((i / 255.0) ** invGamma) * 255\n                        for i in np.arange(0, 256)]).astype(""uint8"")\n      img = cv2.LUT(img, table)\n\n    # blur\n    blur = bool(random.getrandbits(1))\n    if blur:\n      ksize = random.randint(3, 7)\n      img = cv2.blur(img,(ksize,ksize))\n\n    return img, lbl\n\n  def run(self):\n    # loop infinitely, the queue will block\n    while not self.die:\n      # Fetch images\n      # print self.dataset.images[self.dataset.idx]\n      img = cv2.imread(\n          self.dataset.images[self.dataset.idx], cv2.IMREAD_COLOR)\n      lbl = cv2.imread(self.dataset.labels[self.dataset.idx], 0)\n      name = os.path.basename(self.dataset.images[self.dataset.idx])\n\n      # augment\n      if self.name == ""ImgBufftrain"":\n        img, lbl = self.augment(img, lbl)\n\n      # queue if there is still room, otherwise block\n      self.dataset.img_q.put(img)  # blocking\n      self.dataset.lbl_q.put(lbl)  # blocking\n      self.dataset.name_q.put(name)  # blocking\n      self.dataset.idx += 1\n      if self.dataset.idx == self.dataset.num_examples:\n        self.dataset.idx = 0  # begin again\n        if self.name == ""ImgBufftrain"":\n          # randomize data after each epoch\n          combined = list(zip(self.dataset.images, self.dataset.labels))\n          random.shuffle(combined)\n          self.dataset.images[:], self.dataset.labels[:] = zip(*combined)\n\n    print(""Exiting Thread %s"" % self.name)\n\n  def cleanup(self):\n    self.die = True\n    if not self.dataset.img_q.empty():\n      self.dataset.img_q.get()\n    if not self.dataset.lbl_q.empty():\n      self.dataset.lbl_q.get()\n\n\nclass Dataset:\n  """""" Dataset class, for abstraction\n      Contains all images and labels for each set (train, validation or test)\n  """"""\n\n  def __init__(self, images, labels, num_examples, content, name, DATA):\n    self.images = images  # name of files, not data!\n    self.labels = labels  # name of files, not data!\n    self.num_examples = num_examples\n    self.idx = 0\n    self.img_width = DATA[""img_prop""][""width""]\n    self.img_height = DATA[""img_prop""][""height""]\n    self.img_depth = DATA[""img_prop""][""depth""]\n    self.content = content  # matches label_map keys, but contains class\n    # percentage as ratio of pixels in entire dataset.\n    self.name = name\n    # if true, we spawn a thread that prefetches batches for training\n    self.buff = DATA[""buff""]\n    # init prefetch thread\n    self.init(DATA[""buff_nr""])\n\n  def init(self, img_nr):  # img_nr should be bigger than 1 batch for it to make sense\n    # only start the thread if buffering was enabled\n    if self.buff:\n      # create the fifo\n      self.img_q = queue.Queue(maxsize=img_nr)\n      self.lbl_q = queue.Queue(maxsize=img_nr)\n      self.name_q = queue.Queue(maxsize=img_nr)\n      # start a thread pre-fetching images to get them fast from ram in a batch\n      self.imgfetcher = ImgFetcher(""ImgBuff"" + self.name, self)\n      self.imgfetcher.setDaemon(True)\n      self.imgfetcher.start()\n    else:\n      print(""Batch buff has been disabled, so images will be opened on the fly"")\n    return\n\n  def next_batch(self, size):\n    \'\'\'\n      Return size items (wraps around if the last elements are less than a\n      batch size. Be careful with this for evaluation)\n    \'\'\'\n    # different if images are being buffered or not\n    images = []\n    labels = []\n    names = []\n    if self.buff:\n      for i in range(0, size):\n        images.append(self.img_q.get())  # blocking\n        labels.append(self.lbl_q.get())  # blocking\n        names.append(self.name_q.get())  # blocking\n    else:\n      for i in range(0, size):\n        img = cv2.imread(self.images[self.idx], cv2.IMREAD_UNCHANGED)\n        lbl = cv2.imread(self.labels[self.idx], 0)\n        images.append(img)\n        labels.append(lbl)\n        names.append(os.path.basename(self.images[self.idx]))\n        self.idx += 1\n        if self.idx == self.num_examples:\n          self.idx = 0\n    return images, labels, names\n\n  def cleanup(self):\n    if self.buff:\n      self.imgfetcher.cleanup()\n\n\nclass FullDataset:\n  """""" FullDataset class, for abstraction\n      Contains all training, validation and test data, along with image size,\n      depth, and number of classes\n  """"""\n\n  def __init__(self, train, validation, test, DATA):\n    self.train = train\n    self.validation = validation\n    self.test = test\n    self.label_map = DATA[""label_map""]  # maps value to string class\n    self.num_classes = len(self.label_map)  # number of classes to classify\n\n  def cleanup(self):\n    self.train.cleanup()\n    self.validation.cleanup()\n    self.test.cleanup()\n'"
train_py/dataset/augment_data.py,0,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nThis code uses opencv to apply different augmentations to a single image or a\ntraining dataset, which is a common practice in machine learning. Functions are\npretty self-explanatory.\n\nGeneral comment for all functions:\n  We don\'t do anything with labels in this functions, they take images as\n  inputs, and return images as output, but in the same order, so label\n  management can be done easily in a higher level.\n\n  e.g: images: [...,img1,img2,...]\n       labels: [...,lbl1,lbl2,...]\n       index:  [...,i,   i+1 ,...]\n\n       n_transformations: 2\n       returns: [...,img1_1,img1_2,img1_3,img2_1,img2_2,img2_3,...]\n       labels:  [...,lbl1,  lbl1,  lbl1,  lbl2,  lbl2,  lbl2,  ...]\n       index:   [...,i,     i+1,    ...   i+3,   i+4     ...      ]\n                                           ^---> (i+n_transformations+1)\n\n  Indexes for labels should therefore travel in increments of\n  (n_transformations+1) in the higher level to recover the labeled dataset\n  structure. Randomize before feeding this to the training, even if the dataset\n  has been shuffled before, since all the transformed images will be highly\n  correlated with each other, and they will be of the same class!\n\n  Unless otherwise noted, the above structure is used in all functions, where\n  the original image is returned and the transformations requested are appended\n  after it.\n""""""\n\n# opencv is needed for the image transformations\nimport cv2\nimport numpy as np\n\n\ndef resize(img, new_size, neighbor=False):\n  """"""\n  SINGLE IMAGE FUNCTION - Takes one image, returns one image, not list!\n  returns the resized img to size new size, where new_size=(rows,cols)\n\n  When resizing labels we want to grab the nearest neighbor, not interpolate,\n  so we should set neighbor to True\n  """"""\n  # get rows and cols of original image\n  if len(img.shape) == 3:\n    rows, cols, depth = img.shape\n  else:\n    rows, cols = img.shape\n\n  # get rows and cols of new image\n  new_rows = new_size[0]\n  new_cols = new_size[1]\n\n  # resizing should be done differently if we upsize or downsize, due to interpol\n  if neighbor:\n    interpol = cv2.INTER_NEAREST\n  elif new_rows > rows:\n    interpol = cv2.INTER_CUBIC\n  else:\n    interpol = cv2.INTER_LINEAR\n  resized_img = cv2.resize(img, (cols, new_rows), interpolation=interpol)\n\n  if neighbor:\n    interpol = cv2.INTER_NEAREST\n  elif new_cols > cols:\n    interpol = cv2.INTER_CUBIC\n  else:\n    interpol = cv2.INTER_LINEAR\n  resized_img = cv2.resize(\n      resized_img, (new_cols, new_rows), interpolation=interpol)\n\n  return resized_img\n\n\ndef extract_patch(img, corner1, corner2, resize=False, shape=None):\n  """"""\n  SINGLE IMAGE FUNCTION - Takes one image, returns one image, not list!\n\n  Extracts patch from image img starting in corner1 and finishing in corner2,\n  where both corners are given as a list of [x,y] coordinates in the original\n  image. x and y start in the upper left corner of the image (opencv images)\n\n  If shape is given, then ignore the second corner and use the first corner and\n  the shape to extract the patch\n\n  If any of the coordinates are off boundaries, the patch extracted will be\n  cropped to the boundaries of the image. The boundaries can be expressed in\n  any order, the function takes care of the rearrangement necessary for the\n  crop.\n\n  By default we return the patch, but if the flag resize is set to True,\n  we resize the patch to the size of the original image (useful for CNN)\n\n  Returns the extracted patch, if everything is correct, otherwise it returns\n  None, for error checking\n  """"""\n  # sanity checks\n  if (type(corner1) is not list or len(corner1) != 2 or\n      type(corner2) is not list or len(corner2) != 2 or\n          (shape and (type(shape) is not list or len(shape) != 2))):\n    print(""Wrong usage of the corner parameters"")\n    return None\n\n  # get rows and cols of original image\n  rows, cols, depth = img.shape\n\n  # copy the corners internally to work with them\n  pt1 = corner1[:]\n  pt2 = corner2[:]\n\n  # if shape is given, use instead of second corner\n  if shape:\n    pt2 = [pt1[0] + shape[0], pt1[1] + shape[1]]\n\n  # limits\n  minim_pt = [0, 0]\n  maxim_pt = [cols, rows]\n\n  # clip the values to the min (0,0) and max (rows,cols)\n  for pt in [pt1, pt2]:\n    for i in [0, 1]:\n      pt[i] = minim_pt[i] if pt[i] < minim_pt[i] else pt[i]\n      pt[i] = maxim_pt[i] if pt[i] > maxim_pt[i] else pt[i]\n\n  # swap points if improperly arranged for cropping\n  if pt2[0] < pt1[0]:\n    pt2, pt1 = pt1, pt2\n  if pt2[1] < pt1[1]:\n    pt2[1], pt1[1] = pt1[1], pt2[1]\n\n  # now crop\n  patch = img[pt1[1]:pt2[1] + 1, pt1[0]:pt2[0] + 1]\n\n  # resize?\n  if resize:\n    patch = cv2.resize(patch, (cols, rows), interpolation=cv2.INTER_CUBIC)\n\n  return patch\n\n\ndef extract_patch_n(images, indexes, shape):\n  """"""\n  Extracts patches from the images with the indicated shape [x,y], from the\n  indexes asked in the indexes list, where:\n  indexes:\n    1: top left\n    2: top right\n    3: center\n    4: bottom left\n    5: bottom right\n\n  It returns a list with the original image and all the patches, unless\n  something is wrong, in which case we return None for error checking\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # if we only have 1 index, transform into a list to work with same script\n  if type(indexes) is not list:\n    indexes = [indexes]\n\n  if not all((idx > 0 and idx < 6) for idx in indexes):\n    print(""Wrong usage of indexes -> Off boundaries"")\n    return None\n\n  if type(shape) is not list or len(shape) != 2:\n    print(""Wrong usage of shape parameter"")\n    return None\n\n  # container for patches\n  patches = []\n\n  # extract desired patches from each image\n  for img in images:\n    # get rows and cols to rotate\n    rows, cols, depth = img.shape\n\n    # append original at the beginning\n    patches.append(img)\n\n    # extract one patch per index\n    for idx in indexes:\n      patch = {\n          1: lambda x: extract_patch(x, [0, 0], [None, None], shape=shape),\n          2: lambda x: extract_patch(x, [cols - shape[0], 0], [None, None], shape=shape),\n          3: lambda x: extract_patch(x, [cols / 2 - shape[0] / 2, rows / 2 - shape[1] / 2], [cols / 2 + shape[0] / 2, rows / 2 + shape[1] / 2]),\n          4: lambda x: extract_patch(x, [0, rows - shape[1]], [None, None], shape=shape),\n          5: lambda x: extract_patch(x, [cols - shape[0], rows - shape[1]], [None, None], shape=shape)\n      }[idx](img)\n\n      patches.append(patch)\n\n  return patches\n\n\ndef rotations(images, n_rot, ccw_limit, cw_limit):\n  """"""\n  Rotates every image in the list ""images"" n_rot times, between 0 and cw_limit\n  (clockwise limit) n_rot times and between 0 and ccw_limit (counterclockwise\n  limit) n_rot times more. The limits are there to make sense of the data\n  augmentation. E.g: Rotating an mnist digit 180 degrees turns a 6 into a 9,\n  which makes no sense at all.\n\n  cw_limit and ccw_limit are in degrees!\n\n  Returns a list with all the rotated samples. Size will be 2*n_rot+1, because\n  we also want the original sample to be included\n\n  Example: images=[img],n_rot=3,ccw_limit=90,cw_limit=90\n  Returns: [img1: original,\n            img2: 90 degrees rot ccw,\n            img3: 60 degrees rot ccw,\n            img4: 30 degrees rot ccw,\n            img5: 30 degrees rot cw,\n            img5: 60 degrees rot cw\n            img5: 90 degrees rot cw]\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # calculate the initial angle and the step\n  cw_step_angle = float(cw_limit) / float(n_rot)\n  ccw_step_angle = float(ccw_limit) / float(n_rot)\n\n  # container for rotated images\n  rotated_images = []\n\n  # get every image and apply the number of desired rotations\n  for img in images:\n    # get rows and cols to rotate\n    rows, cols, depth = img.shape\n\n    # append the original one too\n    rotated_images.append(img)\n\n    # rotate the amount of times we want them rotated\n    for i in range(1, n_rot + 1):\n      # create rotation matrix with center in the center of the image,\n      # scale 1, and the desired angle (we travel counter clockwise first, and\n      # then clockwise\n      M_ccw = cv2.getRotationMatrix2D(\n          (cols / 2, rows / 2), i * ccw_step_angle, 1)\n      # rotate using the matrix (using bicubic interpolation)\n      rot_img = cv2.warpAffine(img, M_ccw, (cols, rows), flags=cv2.INTER_CUBIC)\n      # append to rotated images container\n      rotated_images.append(rot_img)\n\n      M_cw = cv2.getRotationMatrix2D(\n          (cols / 2, rows / 2), -i * cw_step_angle, 1)\n      # rotate using the matrix (using bicubic interpolation)\n      rot_img = cv2.warpAffine(img, M_cw, (cols, rows), flags=cv2.INTER_CUBIC)\n      # append to rotated images container\n      rotated_images.append(rot_img)\n\n  return rotated_images\n\n\ndef horiz_stretch(images, n_stretch, max_stretch, crop_center=True):\n  """"""\n  Applies a horizontal stretch transform to every image in the list ""images""\n  n_stretch times, with the max stretch passed in the max_stretch argument.\n\n  Stretch > 1 expands the image, and < 1 compresses the image.\n\n  By default, we crop the center of the images so that all images have the same\n  shape, but this can be changed with the arg crop_center set to False.\n\n  Returns a list with all the stretch samples. Size will be n_stretch+1, because\n  we also want the original sample to be included.\n\n  Example: images=[img],n_stretch=2, max_stretch=1.5\n  Returns: [img1=img,\n           img2=img with stretch of 1.25 in x,\n           img3=img with stretch of 1.5 in x]\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # calculate the stretch steps\n  step_rel = (max_stretch - 1) / float(n_stretch)  # relative to image size\n\n  # container for stretched images\n  stretched_images = []\n\n  # get every image and apply the number of desired stretches\n  for img in images:\n    # get rows and cols to stretch\n    rows, cols, depth = img.shape\n\n    stretched_images.append(img)\n\n    # stretch the amount of times we want\n    for i in range(1, n_stretch + 1):\n      # create stretch matrix by mapping points\n      # abs increase in size (or decrease if <1)\n      new_size = int((step_rel * i + 1) * cols)\n\n      # compress or stretch? (neg vs pos)\n      pts1 = np.float32([[0, 0], [cols, 0], [cols, rows]])\n      pts2 = np.float32([[0, 0], [new_size, 0], [new_size, rows]])\n      M = cv2.getAffineTransform(pts1, pts2)\n      stretch_img = cv2.warpAffine(\n          img, M, (new_size, rows), flags=cv2.INTER_CUBIC)\n\n      if crop_center:\n        # different if I stretch or compress.\n        if(max_stretch >= 1):\n          # if stretch, then cut out center\n          row_start = (stretch_img.shape[0] / 2) - (rows / 2)\n          col_start = (stretch_img.shape[1] / 2) - (cols / 2)\n          stretch_img = stretch_img[row_start:row_start +\n                                    rows, col_start:col_start + cols]\n        else:\n          # if compress image will be smaller than original\n          # fill image with zeros and copy the compressed one in the center\n          fix_size_stretch_img = np.zeros(img.shape).astype(np.uint8)\n          row_start = (rows - stretch_img.shape[0]) / 2\n          col_start = (cols - stretch_img.shape[1]) / 2\n          fix_size_stretch_img[row_start:row_start + stretch_img.shape[0],\n                               col_start:col_start + stretch_img.shape[1]] = stretch_img\n          stretch_img = fix_size_stretch_img\n      # append to stretched images container\n      stretched_images.append(stretch_img)\n\n  return stretched_images\n\n\ndef vert_stretch(images, n_stretch, max_stretch, crop_center=True):\n  """"""\n  Applies a vertical stretch transform to every image in the list ""images""\n  n_stretch times, with the max stretch passed in the max_stretch argument.\n\n  Stretch > 1 expands the image, and < 1 compresses the image.\n\n  By default, we crop the center of the images so that all images have the same\n  shape, but this can be changed with the arg crop_center set to False.\n\n  Returns a list with all the stretch samples. Size will be n_stretch+1, because\n  we also want the original sample to be included.\n\n  Example: images=[img],n_stretch=2, max_stretch=1.5\n  Returns: [img1=img,\n           img2=img with stretch of 1.25 in y,\n           img3=img with stretch of 1.5 in y]\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # calculate the stretch steps\n  step_rel = (max_stretch - 1) / float(n_stretch)  # relative to image size\n\n  # container for stretched images\n  stretched_images = []\n\n  # get every image and apply the number of desired stretches\n  for img in images:\n    # get rows and cols to stretch\n    rows, cols, depth = img.shape\n\n    stretched_images.append(img)\n\n    # stretch the amount of times we want\n    for i in range(1, n_stretch + 1):\n      # create stretch matrix by mapping points\n      # abs increase in size (or decrease if <1)\n      new_size = int((step_rel * i + 1) * rows)\n\n      # compress or stretch? (neg vs pos)\n      pts1 = np.float32([[0, 0], [0, rows], [cols, rows]])\n      pts2 = np.float32([[0, 0], [0, new_size], [cols, new_size]])\n      M = cv2.getAffineTransform(pts1, pts2)\n      stretch_img = cv2.warpAffine(\n          img, M, (cols, new_size), flags=cv2.INTER_CUBIC)\n\n      if crop_center:\n        # different if I stretch or compress.\n        if(max_stretch >= 1):\n          # if stretch, then cut out center\n          row_start = (stretch_img.shape[0] / 2) - (rows / 2)\n          col_start = (stretch_img.shape[1] / 2) - (cols / 2)\n          stretch_img = stretch_img[row_start:row_start +\n                                    rows, col_start:col_start + cols]\n        else:\n          # if compress image will be smaller than original\n          # fill image with zeros and copy the compressed one in the center\n          fix_size_stretch_img = np.zeros(img.shape).astype(np.uint8)\n          row_start = (rows - stretch_img.shape[0]) / 2\n          col_start = (cols - stretch_img.shape[1]) / 2\n          fix_size_stretch_img[row_start:row_start + stretch_img.shape[0],\n                               col_start:col_start + stretch_img.shape[1]] = stretch_img\n          stretch_img = fix_size_stretch_img\n      # append to stretched images container\n      stretched_images.append(stretch_img)\n\n  return stretched_images\n\n\ndef horiz_shear(images, n_shear, max_shear, crop_center=True):\n  """"""\n  Applies a horizontal shear transform to every image in the list ""images""\n  n_shear times, with the max shear passed in the max_shear argument. By\n  default, we crop the center of the images so that all images have the same\n  shape, but this can be changed with the arg crop_center set to False.\n\n  Returns a list with all the shear samples. Size will be n_shear+1, because\n  we also want the original sample to be included.\n\n  Example: images=[img],n_shear=2, max_shear=0.5\n  Returns: [img1=img,\n           img2=img with shear of 0.25 in x,\n           img3=img with shear of 0.5 in x]\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # calculate the shear steps\n  step_rel = max_shear / float(n_shear)  # relative to image size\n\n  # container for sheared images\n  sheared_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # get rows and cols to shear\n    rows, cols, depth = img.shape\n    step_abs = step_rel * cols  # absolute to image size\n\n    # shear the amount of times we want\n    for i in range(0, n_shear + 1):\n      # create shear matrix by mapping points\n      # increases in size in each dim of the img\n      size_inc = abs(int(step_abs * i))\n\n      # shear is different if is positive or negative\n      if int(step_abs * i) > 0:\n        pts1 = np.float32([[0, 0], [0, rows], [cols, rows]])\n        pts2 = np.float32([[0, 0], [size_inc, rows], [cols + size_inc, rows]])\n        M = cv2.getAffineTransform(pts1, pts2)\n        shear_img = cv2.warpAffine(\n            img, M, (cols + size_inc, rows), flags=cv2.INTER_CUBIC)\n      elif int(step_abs * i) < 0:\n        pts1 = np.float32([[0, rows], [0, 0], [cols, rows]])\n        pts2 = np.float32([[0, rows], [size_inc, 0], [cols, rows]])\n        M = cv2.getAffineTransform(pts1, pts2)\n        shear_img = cv2.warpAffine(\n            img, M, (cols + size_inc, rows), flags=cv2.INTER_CUBIC)\n      else:\n        shear_img = img\n      # shear using the matrix (and bicubic interpolation)\n\n      if crop_center:\n        row_start = (shear_img.shape[0] / 2) - (rows / 2)\n        col_start = (shear_img.shape[1] / 2) - (cols / 2)\n        shear_img = shear_img[row_start:row_start +\n                              rows, col_start:col_start + cols]\n\n      # append to sheared images container\n      sheared_images.append(shear_img)\n\n  return sheared_images\n\n\ndef vert_shear(images, n_shear, max_shear, crop_center=True):\n  """"""\n  Applies a vertical shear transform to every image in the list ""images""\n  n_shear times, with the max shear passed in the max_shear argument. By\n  default, we crop the center of the images so that all images have the same\n  shape, but this can be changed with the arg crop_center set to False.\n\n  Returns a list with all the shear samples. Size will be n_shear+1, because\n  we also want the original sample to be included.\n\n  Example: images=[img],n_shear=2, max_shear=0.5\n  Returns: [img1=img,\n           img2=img with shear of 0.25 in y,\n           img3=img with shear of 0.5 in y]\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # calculate the shear steps\n  step_rel = max_shear / float(n_shear)  # relative to image size\n\n  # container for sheared images\n  sheared_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # get rows and cols to shear\n    rows, cols, depth = img.shape\n    step_abs = step_rel * rows  # absolute to image size\n\n    # shear the amount of times we want\n    for i in range(0, n_shear + 1):\n      # create shear matrix by mapping points\n      # increases in size in each dim of the img\n      size_inc = abs(int(step_abs * i))\n\n      # shear is different if is positive or negative\n      if int(step_abs * i) > 0:\n        pts1 = np.float32([[0, 0], [cols, 0], [cols, rows]])\n        pts2 = np.float32([[0, 0], [cols, size_inc], [cols, rows + size_inc]])\n        M = cv2.getAffineTransform(pts1, pts2)\n        shear_img = cv2.warpAffine(\n            img, M, (cols, rows + size_inc), flags=cv2.INTER_CUBIC)\n      elif int(step_abs * i) < 0:\n        pts1 = np.float32([[0, 0], [cols, 0], [0, rows]])\n        pts2 = np.float32([[0, size_inc], [cols, 0], [0, rows + size_inc]])\n        M = cv2.getAffineTransform(pts1, pts2)\n        shear_img = cv2.warpAffine(\n            img, M, (cols, rows + size_inc), flags=cv2.INTER_CUBIC)\n      else:\n        shear_img = img\n      # shear using the matrix (and bicubic interpolation)\n\n      if crop_center:\n        row_start = (shear_img.shape[0] / 2) - (rows / 2)\n        col_start = (shear_img.shape[1] / 2) - (cols / 2)\n        shear_img = shear_img[row_start:row_start +\n                              rows, col_start:col_start + cols]\n\n      # append to sheared images container\n      sheared_images.append(shear_img)\n\n  return sheared_images\n\n\ndef horiz_flip(images):\n  """"""\n  Applies a horizontal flip to every image in the list ""images""\n  Returns a list with all the original and flipped samples.\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # container for sheared images\n  flipped_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # append original and flipped images to container\n    flipped_images.append(img)\n    flipped_images.append(cv2.flip(img, 1))\n\n  return flipped_images\n\n\ndef vert_flip(images):\n  """"""\n  Applies a vertical flip to every image in the list ""images""\n  Returns a list with all the original and flipped samples.\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # container for sheared images\n  flipped_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # append original and flipped images to container\n    flipped_images.append(img)\n    flipped_images.append(cv2.flip(img, 0))\n\n  return flipped_images\n\n\ndef gaussian_noise(images, mean, std):\n  """"""\n  Applies gaussian noise to every image in the list ""images"" with the desired\n\n  Returns a list with all the original and noisy images.\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # container for sheared images\n  noisy_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # get rows and cols apply noise to\n    rows, cols, depth = img.shape\n\n    # append original image\n    noisy_images.append(img)\n\n    # fill in the per-channel mean and std\n    m = np.full((1, depth), mean)\n    s = np.full((1, depth), std)\n\n    # add noise to image\n    # noisy_img = img.copy()\n    noisy_img = np.zeros((rows, cols, depth), dtype=np.uint8)\n    noisy_img = cv2.randn(noisy_img, m, s)\n    noisy_img = img + noisy_img\n\n    # append noisy image to container\n    noisy_images.append(noisy_img)\n\n  return noisy_images\n\n\ndef occlusions(images, grid_x, grid_y, selection):\n  """"""\n  Applies a grid to each image and removes a block from selection (zeroing it).\n  Returns a list with all the original and occluded images.\n\n  Example: grid_x=3,grid_y=3,selection=[1,3].\n\n  This divides the image in 9, and returns the original image with the 1st\n  and 3rd quadrant occluded (in separate images).\n\n  Grid for this case:\n  -------------------------\n  |   0   |   1-< |   2   |\n  -------------------------\n  |   3<- |   4   |   5   |\n  -------------------------\n  |   6   |   7   |   8   |\n  -------------------------\n\n  """"""\n  # if we only have 1 image, transform into a list to work with same script\n  if type(images) is not list:\n    images = [images]\n\n  # container for sheared images\n  occluded_images = []\n\n  # get every image and apply the number of desired shears\n  for img in images:\n    # append original image\n    occluded_images.append(img)\n\n    # get rows and cols\n    rows, cols, depth = img.shape\n\n    # number of rows and cols in subsections\n    x_subsec = cols / grid_x\n    y_subsec = rows / grid_y\n\n    for idx in selection:\n      # select x_box and y_box\n      x_box = idx % grid_x\n      y_box = idx / grid_x\n\n      # generate the mask\n      mask = np.full((rows, cols), 255).astype(np.uint8)\n      mask[y_box * y_subsec:(y_box + 1) * y_subsec,\n           x_box * x_subsec:(x_box + 1) * x_subsec] = 0\n\n      # occlude image\n      occ_img = cv2.bitwise_and(img, img, mask=mask)\n\n      # append occluded image to container\n      occluded_images.append(occ_img)\n\n  return occluded_images\n\n\nif __name__ == ""__main__"":\n  print(""Config as app not done!"")\n'"
train_py/dataset/cityscapes.py,0,"b'# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n"""""" Abstraction for getting cityscapes dataset and putting it in an abstract\n    class that can handle any segmentation problem :)\n""""""\n\n# import the abstract dataset classes\nimport dataset.abstract_dataset as abs_data\n\n# opencv stuff for images and numpy for matrix representation\nimport cv2\n\n# file and folder handling\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nfrom colorama import Fore, Back, Style\nimport shutil\n\n\ndef dir_to_data(directory, label_map, label_remap, new_shape=None, force_remap=False):\n  """"""Get the dataset in the format that we need it\n     and give it to dataset generator\n  Args:\n    directory: Folder where the dataset is stored\n    label_map: all the classes we know\n    label_remap: remap to classes we need for the crossentropy loss\n    new_shape: New shape for the images to use\n    force_remap: Delete remap folders, so that they are populated again\n  Returns:\n    images: images file names\n    labels: remaped label filenames\n    n_data = amount of data in the dataset\n    content_perc = dict of percentage content for each class in the entire set\n  """"""\n  print(""---------------------------------------------------------------------"")\n  print(""Parsing directory %s"" % directory)\n\n  # make lists strings\n\n  # create the content dictionary and other statistics in data\n  content_perc = {}\n  for key in label_map:\n    content_perc[key] = 0.0\n  total_pix = 0.0\n  n_data = 0\n\n  img_dir = directory + \'/img/\'\n  img_remap_dir = img_dir + \'/remap/\'\n  label_dir = directory + \'/lbl/\'\n  label_remap_dir = label_dir + \'/remap/\'\n\n  # get the file list in the folder\n  images = [f for f in listdir(img_dir)\n            if isfile(join(img_dir, f))]\n  labels = [f for f in listdir(label_dir)\n            if isfile(join(label_dir, f))]\n\n  # check if image has a corresponding label, otherwise warn and continue\n  for f in images[:]:\n    if f not in labels[:]:\n      # warn\n      print(""Image file %s has no label, GIMME DAT LABEL YO! Ignoring..."" % f)\n      # ignore image\n      images.remove(f)\n    else:\n      # success!\n      n_data += 1\n      # calculate class content in the image\n      # print(""Calculating label content of label %s""%f)\n      l = cv2.imread(label_dir + f, 0)  # open label as grayscale\n      h, w = l.shape\n      total_pix += h * w  # add to the total count of pixels in images\n      # print(""Number of pixels in image %s""%(h*w))\n      # create histogram\n      hist = cv2.calcHist([l], [0], None, [256], [0, 256])\n      for key in content_perc:\n        # look known class\n        content_perc[key] += hist[key]\n        hist[key] = 0\n        # print(""Content of class %s: %f""%(label_map[key],content_perc[key]))\n      # report unknowkn class\n      flag = 0\n      for i in range(0, 256):\n        if hist[i] > 0:\n          if not flag:\n            print(Back.RED + ""Achtung! Img %s"" % f + Style.RESET_ALL)\n            flag = 1\n          print(Fore.RED + ""   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80 labels contains %d unmapped class pixels with value %d"" %\n                (hist[i], i) + Style.RESET_ALL)\n      if flag:\n        # there were pixels that don\'t belong to our classes, so drop before\n        # breaking our crossentropy\n        print(""Dropping image %s"" % f)\n        labels.remove(f)\n        images.remove(f)\n\n  # loop labels checking rogue labels with no images (magic label from ether)\n  for f in labels[:]:\n    if f not in images[:]:\n      # warn\n      print(""Label file %s has no image, IS THIS MAGIC?! Ignoring..."" % f)\n      # ignore image\n      labels.remove(f)\n\n  # remove remap folders to create them again\n  if force_remap and os.path.exists(label_remap_dir):\n    shutil.rmtree(label_remap_dir)\n  if force_remap and os.path.exists(img_remap_dir):\n    shutil.rmtree(img_remap_dir)\n\n  # remap all labels to [0,num_classes], otherwise it breaks the crossentropy\n  if not os.path.exists(label_remap_dir):\n    print(""Cross Entropy remap non existent, creating..."")\n    os.makedirs(label_remap_dir)\n    for f in labels:\n      lbl = cv2.imread(label_dir + f, 0)\n      if(new_shape is not None):\n        lbl = cv2.resize(lbl, new_shape, interpolation=cv2.INTER_NEAREST)\n      for key in label_remap:\n        lbl[lbl == key] = label_remap[key]\n      cv2.imwrite(directory + \'/lbl/remap/\' + f, lbl)\n\n  # remap all images to jpg and resized to proper size, so that we open faster\n  new_images = []\n  if not os.path.exists(img_remap_dir):\n    print(""Jpeg remap non existent, creating..."")\n    os.makedirs(img_remap_dir)\n    for f in images:\n      img = cv2.imread(img_dir + f, cv2.IMREAD_UNCHANGED)\n      f = os.path.splitext(f)[0] + \'.jpg\'\n      new_images.append(f)\n      if(new_shape is not None):\n        img = cv2.resize(img, new_shape, interpolation=cv2.INTER_LINEAR)\n      cv2.imwrite(img_remap_dir + f, img)\n  else:\n    for f in images:\n      f = os.path.splitext(f)[0] + \'.jpg\'\n      new_images.append(f)\n\n  # final percentage calculation\n  print(""Total number of pixels: %d"" % (total_pix))\n  for key in content_perc:\n    print(""Total number of pixels of class %s: %d"" %\n          (label_map[key], content_perc[key]))\n    content_perc[key] /= total_pix\n  # report number of class labels\n  for key in label_map:\n    print(""Content percentage of class %s in dataset: %f"" %\n          (label_map[key], content_perc[key]))\n  print(""Total amount of images: %d"" % n_data)\n  print(""---------------------------------------------------------------------"")\n\n  print(\' SPECIFIC TO CITYSCAPES \'.center(80, \'*\'))\n  print(""Don\'t weigh the \'crap\' class (key 255)"")\n  # this is a hack, and needs to be done more elegantly\n  content_perc[255] = float(""inf"")\n  print(""Content percentage of class %s in dataset: %f"" %\n        (label_map[255], content_perc[255]))\n  print(\' SPECIFIC TO CITYSCAPES \'.center(80, \'*\'))\n\n  # prepend the folder to each file name\n  new_images = [directory + \'/img/remap/\' + name for name in new_images]\n  labels = [directory + \'/lbl/remap/\' + name for name in labels]\n\n  # order to ensure matching (necessary?)\n  new_images.sort()\n  labels.sort()\n\n  return new_images, labels, n_data, content_perc\n\n\ndef read_data_sets(DATA):\n  """"""Get the dataset in the format that we need it\n     and give it to the main app\n  -*- coding: utf-8 -*-\n  Args:\n    DATA: Dictionary with dataset information.\n          Structure for the input dir:\n            Dataset\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 test\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic3.jpg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic3.jpg\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic1.jpg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic1.jpg\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 valid\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic2.jpg\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic2.jpg\n  Returns:\n    data_sets: Object of class dataset where all training, validation and\n               test data is stored, along with other information to train\n               the desired net.\n  """"""\n\n  # get the datasets from the folders\n  data_dir = DATA[\'data_dir\']\n\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n\n  # check that all folders exist:\n  for d in directories:\n    for t in types:\n      if not os.path.exists(data_dir + d + t):\n        print(""%s dir does not exist. Check dataset folder"" %\n              (data_dir + d + t))\n        quit()\n\n  print(""Data depth: "", DATA[""img_prop""][""depth""])\n\n  # if force resize, do it properly\n  if ""force_resize"" in DATA and DATA[""force_resize""]:\n    new_rows = DATA[""img_prop""][""height""]\n    new_cols = DATA[""img_prop""][""width""]\n    new_shape = (new_cols, new_rows)\n  else:\n    new_shape = None\n\n  # for backward compatibility\n  if ""force_remap"" in DATA:\n    force_remap = DATA[""force_remap""]\n  else:\n    force_remap = False\n\n  # train data\n  train_img, train_lbl, train_n, train_cont = dir_to_data(join(data_dir, ""train""),\n                                                          DATA[""label_map""],\n                                                          DATA[""label_remap""],\n                                                          new_shape=new_shape,\n                                                          force_remap=force_remap)\n  train_data = abs_data.Dataset(train_img, train_lbl, train_n, train_cont,\n                                ""train"", DATA)\n\n  # validation data\n  valid_img, valid_lbl, valid_n, valid_cont = dir_to_data(join(data_dir, ""valid""),\n                                                          DATA[""label_map""],\n                                                          DATA[""label_remap""],\n                                                          new_shape=new_shape,\n                                                          force_remap=force_remap)\n  valid_data = abs_data.Dataset(valid_img, valid_lbl, valid_n, valid_cont,\n                                ""valid"", DATA)\n\n  # test data\n  test_img, test_lbl, test_n, test_cont = dir_to_data(join(data_dir, ""test""),\n                                                      DATA[""label_map""],\n                                                      DATA[""label_remap""],\n                                                      new_shape=new_shape,\n                                                      force_remap=force_remap)\n  test_data = abs_data.Dataset(test_img, test_lbl, test_n, test_cont,\n                               ""test"", DATA)\n\n  data_sets = abs_data.FullDataset(train_data, valid_data, test_data, DATA)\n\n  print(""Successfully imported datasets"")\n  print(""Train data samples: "", train_n)\n  print(""Validation data samples: "", valid_n)\n  print(""Test data samples: "", test_n)\n\n  return data_sets\n'"
train_py/dataset/general.py,0,"b'# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n"""""" Abstraction for getting general dataset and putting it in an abstract class\n    that can handle any segmentation problem :) You can use this file when\n    you don\'t need to preprocess your data in any special way\n""""""\n\n# import the abstract dataset classes\nimport dataset.abstract_dataset as abs_data\n\n# opencv stuff for images and numpy for matrix representation\nimport cv2\n\n# file and folder handling\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nfrom colorama import Fore, Back, Style\nimport shutil\n\n\ndef dir_to_data(directory, label_map, label_remap, new_shape=None, force_remap=False):\n  """"""Get the dataset in the format that we need it\n     and give it to dataset generator\n  Args:\n    directory: Folder where the dataset is stored\n    label_map: all the classes we know\n    label_remap: remap to classes we need for the crossentropy loss\n    new_shape: New shape for the images to use\n    force_remap: Delete remap folders, so that they are populated again\n  Returns:\n    images: images file names\n    labels: remaped label filenames\n    n_data = amount of data in the dataset\n    content_perc = dict of percentage content for each class in the entire set\n  """"""\n  print(""---------------------------------------------------------------------"")\n  print(""Parsing directory %s"" % directory)\n\n  # make lists strings\n\n  # create the content dictionary and other statistics in data\n  content_perc = {}\n  for key in label_map:\n    content_perc[key] = 0.0\n  total_pix = 0.0\n  n_data = 0\n\n  img_dir = directory + \'/img/\'\n  img_remap_dir = img_dir + \'/remap/\'\n  label_dir = directory + \'/lbl/\'\n  label_remap_dir = label_dir + \'/remap/\'\n\n  # get the file list in the folder\n  images = [f for f in listdir(img_dir)\n            if isfile(join(img_dir, f))]\n  labels = [f for f in listdir(label_dir)\n            if isfile(join(label_dir, f))]\n\n  # check if image has a corresponding label, otherwise warn and continue\n  for f in images[:]:\n    if os.path.splitext(f)[0] not in [os.path.splitext(l)[0] for l in labels[:]]:\n      # warn\n      print(""Image file %s has no label, GIMME DAT LABEL YO! Ignoring..."" % f)\n      # ignore image\n      images.remove(f)\n    else:\n      # success!\n      n_data += 1\n      # calculate class content in the image\n      # print(""Calculating label content of label %s""%f)\n      l = cv2.imread(label_dir + os.path.splitext(f)[0] + "".png"", 0)  # open label as grayscale\n      h, w = l.shape\n      total_pix += h * w  # add to the total count of pixels in images\n      # print(""Number of pixels in image %s""%(h*w))\n      # create histogram\n      hist = cv2.calcHist([l], [0], None, [256], [0, 256])\n      for key in content_perc:\n        # look known class\n        content_perc[key] += hist[key]\n        hist[key] = 0\n        # print(""Content of class %s: %f""%(label_map[key],content_perc[key]))\n      # report unknowkn class\n      flag = 0\n      for i in range(0, 256):\n        if hist[i] > 0:\n          if not flag:\n            print(Back.RED + ""Achtung! Img %s"" % f + Style.RESET_ALL)\n            flag = 1\n          print(Fore.RED + ""   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80 labels contains %d unmapped class pixels with value %d"" %\n                (hist[i], i) + Style.RESET_ALL)\n      if flag:\n        # there were pixels that don\'t belong to our classes, so drop before\n        # breaking our crossentropy\n        print(""Dropping image %s"" % f)\n        labels.remove(os.path.splitext(f)[0] + "".png"")\n        images.remove(f)\n\n  # loop labels checking rogue labels with no images (magic label from ether)\n  for f in labels[:]:\n    if os.path.splitext(f)[0] not in [os.path.splitext(i)[0] for i in images[:]]:\n      # warn\n      print(""Label file %s has no image, IS THIS MAGIC?! Ignoring..."" % f)\n      # ignore image\n      labels.remove(f)\n\n  # remove remap folders to create them again\n  if force_remap and os.path.exists(label_remap_dir):\n    shutil.rmtree(label_remap_dir)\n  if force_remap and os.path.exists(img_remap_dir):\n    shutil.rmtree(img_remap_dir)\n\n  # remap all labels to [0,num_classes], otherwise it breaks the crossentropy\n  if not os.path.exists(label_remap_dir):\n    print(""Cross Entropy remap non existent, creating..."")\n    os.makedirs(label_remap_dir)\n    for f in labels:\n      lbl = cv2.imread(label_dir + f, 0)\n      if(new_shape is not None):\n        lbl = cv2.resize(lbl, new_shape, interpolation=cv2.INTER_NEAREST)\n      for key in label_remap:\n        lbl[lbl == key] = label_remap[key]\n      cv2.imwrite(directory + \'/lbl/remap/\' + f, lbl)\n\n  # remap all images to jpg and resized to proper size, so that we open faster\n  new_images = []\n  if not os.path.exists(img_remap_dir):\n    print(""Jpeg remap non existent, creating..."")\n    os.makedirs(img_remap_dir)\n    for f in images:\n      img = cv2.imread(img_dir + f, cv2.IMREAD_UNCHANGED)\n      f = os.path.splitext(f)[0] + \'.jpg\'\n      new_images.append(f)\n      if(new_shape is not None):\n        img = cv2.resize(img, new_shape, interpolation=cv2.INTER_LINEAR)\n      cv2.imwrite(img_remap_dir + f, img)\n  else:\n    for f in images:\n      f = os.path.splitext(f)[0] + \'.jpg\'\n      new_images.append(f)\n\n  # final percentage calculation\n  print(""Total number of pixels: %d"" % (total_pix))\n  for key in content_perc:\n    print(""Total number of pixels of class %s: %d"" %\n          (label_map[key], content_perc[key]))\n    content_perc[key] /= total_pix\n  # report number of class labels\n  for key in label_map:\n    print(""Content percentage of class %s in dataset: %f"" %\n          (label_map[key], content_perc[key]))\n  print(""Total amount of images: %d"" % n_data)\n  print(""---------------------------------------------------------------------"")\n\n  # prepend the folder to each file name\n  new_images = [directory + \'/img/remap/\' + name for name in new_images]\n  labels = [directory + \'/lbl/remap/\' + name for name in labels]\n\n  # order to ensure matching (necessary?)\n  assert(len(new_images) == len(labels))\n  new_images.sort()\n  labels.sort()\n\n  return new_images, labels, n_data, content_perc\n\n\ndef read_data_sets(DATA):\n  """"""Get the dataset in the format that we need it\n     and give it to the main app\n  -*- coding: utf-8 -*-\n  Args:\n    DATA: Dictionary with dataset information.\n          Structure for the input dir:\n            Dataset\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 test\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic3.jpg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic3.jpg\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic1.jpg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic1.jpg\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 valid\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic2.jpg\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pic2.jpg\n  Returns:\n    data_sets: Object of class dataset where all training, validation and\n               test data is stored, along with other information to train\n               the desired net.\n  """"""\n\n  # get the datasets from the folders\n  data_dir = DATA[\'data_dir\']\n\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n\n  # check that all folders exist:\n  for d in directories:\n    for t in types:\n      if not os.path.exists(data_dir + d + t):\n        print(""%s dir does not exist. Check dataset folder"" %\n              (data_dir + d + t))\n        quit()\n\n  print(""Data depth: "", DATA[""img_prop""][""depth""])\n\n  # if force resize, do it properly\n  if ""force_resize"" in DATA and DATA[""force_resize""]:\n    new_rows = DATA[""img_prop""][""height""]\n    new_cols = DATA[""img_prop""][""width""]\n    new_shape = (new_cols, new_rows)\n  else:\n    new_shape = None\n\n  # for backward compatibility\n  if ""force_remap"" in DATA:\n    force_remap = DATA[""force_remap""]\n  else:\n    force_remap = False\n\n  # train data\n  train_img, train_lbl, train_n, train_cont = dir_to_data(join(data_dir, ""train""),\n                                                          DATA[""label_map""],\n                                                          DATA[""label_remap""],\n                                                          new_shape=new_shape,\n                                                          force_remap=force_remap)\n  train_data = abs_data.Dataset(train_img, train_lbl, train_n, train_cont,\n                                ""train"", DATA)\n\n  # validation data\n  valid_img, valid_lbl, valid_n, valid_cont = dir_to_data(join(data_dir, ""valid""),\n                                                          DATA[""label_map""],\n                                                          DATA[""label_remap""],\n                                                          new_shape=new_shape,\n                                                          force_remap=force_remap)\n  valid_data = abs_data.Dataset(valid_img, valid_lbl, valid_n, valid_cont,\n                                ""valid"", DATA)\n\n  # test data\n  test_img, test_lbl, test_n, test_cont = dir_to_data(join(data_dir, ""test""),\n                                                      DATA[""label_map""],\n                                                      DATA[""label_remap""],\n                                                      new_shape=new_shape,\n                                                      force_remap=force_remap)\n  test_data = abs_data.Dataset(test_img, test_lbl, test_n, test_cont,\n                               ""test"", DATA)\n\n  data_sets = abs_data.FullDataset(train_data, valid_data, test_data, DATA)\n\n  print(""Successfully imported datasets"")\n  print(""Train data samples: "", train_n)\n  print(""Validation data samples: "", valid_n)\n  print(""Test data samples: "", test_n)\n\n  return data_sets\n'"
train_py/dataset/plant_features.py,0,"b'#!/usr/bin/python3\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Given an RGB plant image, the functions in this file\n  calculate the extra features for addition to CNN.\n\'\'\'\n\nimport cv2\nimport numpy as np\n\n\ndef contrast_stretch(img):\n  """"""\n  Performs a simple contrast stretch of the given image, in order to remove\n  extreme outliers.\n  """"""\n  in_min = np.percentile(img, 0.05)\n  in_max = np.percentile(img, 99.95)\n\n  out_min = 0.0\n  out_max = 255.0\n\n  out = img - in_min\n  out *= out_max / (in_max - in_min)\n\n  out[out < out_min] = 0.0\n  out[out > out_max] = 255.0\n\n  return out\n\n\ndef contrast_stretch_const(img, in_min, in_max):\n  """"""\n  Performs a simple contrast stretch of the given image, in order to remove\n  extreme outliers.\n  """"""\n  out_min = 0.0\n  out_max = 255.0\n\n  out = img - in_min\n  out *= out_max / (in_max - in_min)\n\n  out[out < out_min] = 0.0\n  out[out > out_max] = 255.0\n\n  return out\n\n\ndef thresh(img, conservative=0, min_blob_size=50):\n  \'\'\'\n    Get threshold to make mask using the otsus method, and apply a correction\n    passed in conservative (-100;100) as a percentage of th.\n  \'\'\'\n\n  # blur and get level using otsus\n  blur = cv2.GaussianBlur(img, (13, 13), 0)\n  level, _ = cv2.threshold(\n      blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_TRIANGLE)\n\n  # print(""Otsus Level: "",level)\n\n  # change with conservative\n  level += conservative / 100.0 * level\n\n  # check boundaries\n  level = 255 if level > 255 else level\n  level = 0 if level < 0 else level\n\n  # mask image\n  _, mask = cv2.threshold(blur, level, 255, cv2.THRESH_BINARY)\n\n  # morph operators\n  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n  mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n  # remove small connected blobs\n  # find connected components\n  n_components, output, stats, centroids = cv2.connectedComponentsWithStats(\n      mask, connectivity=8)\n  # remove background class\n  sizes = stats[1:, -1]\n  n_components = n_components - 1\n\n  # remove blobs\n  mask_clean = np.zeros((output.shape))\n  # for every component in the image, keep it only if it\'s above min_blob_size\n  for i in range(0, n_components):\n    if sizes[i] >= min_blob_size:\n      mask_clean[output == i + 1] = 255\n\n  return mask_clean\n\n\ndef exgreen(img):\n  \'\'\'\n    Returns the excess green of the image as:\n      exgreen = 2 * G - R - B\n  \'\'\'\n\n  # get channels\n  B, G, R = cv2.split(img)\n\n  # normalize\n  B_ = B.astype(float) / np.median(B.astype(float))\n  G_ = G.astype(float) / np.median(G.astype(float))\n  R_ = R.astype(float) / np.median(R.astype(float))\n\n  E = B_ + G_ + R_ + 0.001\n  b = B_ / E\n  g = G_ / E\n  r = R_ / E\n\n  # calculate exgreen\n  exgr = 2.8 * g - r - b\n\n  # expand contrast\n  exgr = contrast_stretch(exgr)\n\n  # convert to saveable image\n  exgr = exgr.astype(np.uint8)\n\n  return exgr\n\n\ndef cive(img):\n  \'\'\'\n    Returns the inverse color index of vegetation extraction of the image as:\n      cive = 0.881 * g - 0.441 * r - 0.385 * b - 18.78745\n  \'\'\'\n\n  # get channels\n  B, G, R = cv2.split(img)\n\n  # normalize\n  B_ = B.astype(float) / np.median(B.astype(float))\n  G_ = G.astype(float) / np.median(G.astype(float))\n  R_ = R.astype(float) / np.median(R.astype(float))\n\n  E = B_ + G_ + R_ + 0.001\n  b = B_ / E\n  g = G_ / E\n  r = R_ / E\n\n  # calculate cive\n  c = 0.881 * g - 0.441 * r - 0.385 * b - 18.78745\n\n  # expand contrast\n  c = contrast_stretch(c)\n\n  # convert to saveable image\n  c = c.astype(np.uint8)\n\n  return c\n\n\ndef exred(img):\n  \'\'\'\n    Returns the excess green (inverted, to comply with other masks) of the image as:\n      exred = 1.4 * R - G\n  \'\'\'\n\n  # get channels\n  B, G, R = cv2.split(img)\n\n  # normalize\n  B_ = B.astype(float) / np.median(B.astype(float))\n  G_ = G.astype(float) / np.median(G.astype(float))\n  R_ = R.astype(float) / np.median(R.astype(float))\n\n  E = B_ + G_ + R_ + 0.001\n  b = B_ / E\n  g = G_ / E\n  r = R_ / E\n\n  # calculate exgreen\n  exr = 1.4 * r - g\n\n  # expand contrast\n  exr = contrast_stretch(exr)\n\n  # convert to saveable image\n  exr = exr.astype(np.uint8)\n\n  return exr\n\n\ndef ndi(img):\n  \'\'\'\n    Get the normalized diference index\n  \'\'\'\n  # get channels\n  B, G, R = cv2.split(img)\n\n  # normalize\n  B_ = B.astype(float) / np.median(B.astype(float))\n  G_ = G.astype(float) / np.median(G.astype(float))\n  R_ = R.astype(float) / np.median(R.astype(float))\n\n  E = B_ + G_ + R_ + 0.001\n  b = B_ / E\n  g = G_ / E\n  r = R_ / E\n\n  # calculate ndi\n  idx = (g - r) / (g + r)\n\n  # expand contrast\n  idx = contrast_stretch(idx)\n\n  # convert to saveable image\n  idx = idx.astype(np.uint8)\n\n  return idx\n\n\ndef hsv(img):\n  \'\'\'\n    Convert image to hsv\n  \'\'\'\n\n  ret = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n  return ret\n\n\ndef edges(mask):\n  \'\'\'\n    Get edges with canny detector\n  \'\'\'\n  # blur\n  mask = cv2.GaussianBlur(mask, (5, 5), 0)\n\n  edges = cv2.Canny(mask, 100, 200)\n\n  # stretch\n  edges = contrast_stretch(edges)\n\n  # cast\n  edges = np.uint8(edges)\n\n  return edges\n\n\ndef laplacian(mask):\n  \'\'\'\n    Get 2nd order gradients using the Laplacian\n  \'\'\'\n\n  # blur\n  mask = cv2.GaussianBlur(mask, (5, 5), 0)\n\n  # edges with laplacian\n  laplacian = cv2.Laplacian(mask, cv2.CV_64F, 5)\n\n  # stretch\n  laplacian = contrast_stretch(laplacian)\n\n  # cast\n  laplacian = np.uint8(laplacian)\n\n  return laplacian\n\n\ndef gradients(mask, direction=\'x\'):\n  \'\'\'\n    Get gradients using sobel operator\n  \'\'\'\n  mask = cv2.GaussianBlur(mask, (5, 5), 0)\n\n  if direction == \'x\':\n    # grad x\n    sobel = cv2.Sobel(mask, cv2.CV_64F, 1, 0, ksize=7)\n  elif direction == \'y\':\n    # grad y\n    sobel = cv2.Sobel(mask, cv2.CV_64F, 0, 1, ksize=7)\n  else:\n    print(""Invalid gradient direction. Must be x or y"")\n    quit()\n\n  # sobel = np.absolute(sobel)\n  sobel = contrast_stretch(sobel)   # expand contrast\n  sobel = np.uint8(sobel)\n\n  return sobel\n\n\ndef watershed(rgb, idx, mask):\n  \'\'\'\n    Get watershed transform from image\n  \'\'\'\n\n  # kernel definition\n  kernel = np.ones((3, 3), np.uint8)\n\n  # sure background area\n  sure_bg = cv2.dilate(mask, kernel)\n  sure_bg = np.uint8(sure_bg)\n  # util.im_gray_plt(sure_bg,""sure back"")\n\n  # Finding sure foreground area\n  dist_transform = cv2.distanceTransform(np.uint8(mask), cv2.DIST_L2, 3)\n  # util.im_gray_plt(dist_transform,""dist transform"")\n  ret, sure_fg = cv2.threshold(\n      dist_transform, 0.5 * dist_transform.max(), 255, 0)\n\n  # Finding unknown region\n  sure_fg = np.uint8(sure_fg)\n  # util.im_gray_plt(sure_fg,""sure fore"")\n\n  unknown = cv2.subtract(sure_bg, sure_fg)\n  # util.im_gray_plt(unknown,""unknown"")\n\n  # marker labelling\n  ret, markers = cv2.connectedComponents(sure_fg)\n\n  # add one to all labels so that sure background is not 0, but 1\n  markers = markers + 1\n\n  # mark the region of unknown with zero\n  markers[unknown == 255] = 0\n\n  # util.im_gray_plt(np.uint8(markers),""markers"")\n\n  # apply watershed\n  markers = cv2.watershed(rgb, markers)\n\n  # create limit mask\n  mask = np.zeros(mask.shape, np.uint8)\n  mask[markers == -1] = 255\n\n  return mask\n\n\ndef mask_multidim(img, mask):\n  \'\'\'\n    mask an image with a mask (0,255)\n  \'\'\'\n  ret = np.array(img)\n  if len(img.shape) == 3:\n    ret[mask == 0, :] = 0\n  elif len(img.shape) == 2:\n    ret[mask == 0] = 0\n  else:\n    # unknown shape\n    ret = img\n\n  return ret\n\n\ndef chanelwise_norm(img):\n  \'\'\'\n    Returns the normalized image:\n  \'\'\'\n  ret = np.array(img)\n\n  # expand contrast\n  for i in range(img.shape[2]):\n    ret[:, :, i] = contrast_stretch(ret[:, :, i].astype(float))\n\n  # convert to saveable image\n  ret = ret.astype(np.uint8)\n\n  return ret\n'"
train_py/dataset/aux_scripts/cityscapes_preprocess.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n\n  Inputs are already split in train,val,test, and put into a format we can read, so all we need to do is join them.\n\n  Input:\n\n     - InputDirectory (with the remaped images already there)\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gtFine_trainvaltest\n              \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 gtFine\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 test\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 berlin\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bielefeld\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonn\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leverkusen\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 mainz\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 munich\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 aachen\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bochum\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bremen\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 cologne\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 darmstadt\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 dusseldorf\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 erfurt\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 hamburg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 hanover\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 jena\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 krefeld\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 monchengladbach\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 strasbourg\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 stuttgart\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 tubingen\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ulm\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 weimar\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 zurich\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 zurich_000000_000019_gtFine_labelTrainIds.png\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 zurich_000001_000019_gtFine_labelTrainIds.png\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0 \xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 zurich_000002_000019_gtFine_labelTrainIds.png\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0     \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 zurich_000003_000019_gtFine_labelTrainIds.png\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0\xc2\xa0      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 zurich_000004_000019_gtFine_labelTrainIds.png\n              \xe2\x94\x82    \xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\n              \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\n              \xe2\x94\x82\xc2\xa0\xc2\xa0         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 frankfurt\n              \xe2\x94\x82\xc2\xa0\xc2\xa0         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 lindau\n              \xe2\x94\x82\xc2\xa0\xc2\xa0         \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 munster\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 leftImg8bit_trainvaltest\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 leftImg8bit\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 test\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 berlin\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bielefeld\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonn\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 leverkusen\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 mainz\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 munich\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 aachen\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bochum\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bremen\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 cologne\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 darmstadt\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 dusseldorf\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 erfurt\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 hamburg\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 hanover\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 jena\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 krefeld\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 monchengladbach\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 strasbourg\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 stuttgart\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 tubingen\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ulm\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 weimar\n                      \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 zurich\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 val\n                          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 frankfurt\n                          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 lindau\n                          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 munster\n                                \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 munster_000169_000019_leftImg8bit.png\n                     \xc2\xa0\xc2\xa0         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 munster_000170_000019_leftImg8bit.png\n                      \xc2\xa0         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 munster_000171_000019_leftImg8bit.png\n                      \xc2\xa0         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 munster_000172_000019_leftImg8bit.png\n                      \xc2\xa0         \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 munster_000173_000019_leftImg8bit.png\n\n      - OutputDirectory: Where to put the images\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./synthia_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n\n  # rgb image stuff\n  rgb_img_path = ""leftImg8bit_trainvaltest/leftImg8bit/""\n  rgb_img_tail = ""_leftImg8bit.png""\n\n  # annotation stuff\n  annotations_path = ""gtFine_trainvaltest/gtFine/""\n  annotation_tail = ""_gtFine_labelTrainIds.png""\n\n  # check directories\n  directories = [rgb_img_path, annotations_path]\n  for d in directories:\n    if not os.path.exists(os.path.join(FLAGS.dataset, d)):\n      print(""%s dir does not exist. Check dataset folder"" %\n            (os.path.join(FLAGS.dataset, d)))\n      quit()\n    else:\n      print(""Found dir dataset->%s"" % (d))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # create train, valid and test dirs\n  directories = [""train"", ""valid"", ""test""]\n  types = [""img"", ""lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (os.path.join(FLAGS.out, d)))\n    if not os.path.exists(os.path.join(FLAGS.out, d)):\n      os.makedirs(os.path.join(FLAGS.out, d))\n    for t in types:\n      print(""Creating %s dir."" % (os.path.join(FLAGS.out, d, t)))\n      if not os.path.exists(os.path.join(FLAGS.out, d, t)):\n        os.makedirs(os.path.join(FLAGS.out, d, t))\n\n  # # train\n  train_cities = [""aachen"", ""bochum"", ""bremen"", ""cologne"", ""darmstadt"", ""dusseldorf"", ""erfurt"", ""hamburg"",\n                  ""hanover"", ""jena"", ""krefeld"", ""monchengladbach"", ""strasbourg"", ""stuttgart"", ""tubingen"", ""ulm"", ""weimar"", ""zurich""]\n  for city in train_cities:\n    images = [f for f in os.listdir(os.path.join(FLAGS.dataset, rgb_img_path, ""train"", city)) if (\n        rgb_img_tail in f and os.path.isfile(os.path.join(FLAGS.dataset, rgb_img_path, ""train"", city, f)))]\n    labels = [f for f in os.listdir(os.path.join(FLAGS.dataset, annotations_path, ""train"", city)) if (\n        annotation_tail in f and os.path.isfile(os.path.join(FLAGS.dataset, annotations_path, ""train"", city, f)))]\n    for img in images:\n      src = os.path.join(FLAGS.dataset, rgb_img_path, ""train"", city, img)\n      dst = os.path.join(FLAGS.out, ""train/img"",\n                         img.replace(rgb_img_tail, "".png""))\n      print(""copying "", src, "" to "", dst)\n      copyfile(src, dst)  # copy image\n    for lbl in labels:\n      src = os.path.join(FLAGS.dataset, annotations_path, ""train"", city, lbl)\n      dst = os.path.join(FLAGS.out, ""train/lbl"",\n                         lbl.replace(annotation_tail, "".png""))\n      print(""copying "", src, "" to "", dst)\n      copyfile(src, dst)  # copy image\n\n  # # valid\n  val_cities = [""frankfurt"", ""lindau"", ""munster""]\n  for city in val_cities:\n    images = [f for f in os.listdir(os.path.join(FLAGS.dataset, rgb_img_path, ""val"", city)) if (\n        rgb_img_tail in f and os.path.isfile(os.path.join(FLAGS.dataset, rgb_img_path, ""val"", city, f)))]\n    labels = [f for f in os.listdir(os.path.join(FLAGS.dataset, annotations_path, ""val"", city)) if (\n        annotation_tail in f and os.path.isfile(os.path.join(FLAGS.dataset, annotations_path, ""val"", city, f)))]\n    for img in images:\n      src = os.path.join(FLAGS.dataset, rgb_img_path, ""val"", city, img)\n      dst = os.path.join(FLAGS.out, ""valid/img"",\n                         img.replace(rgb_img_tail, "".png""))\n      print(""copying "", src, "" to "", dst)\n      copyfile(src, dst)  # copy image\n    for lbl in labels:\n      src = os.path.join(FLAGS.dataset, annotations_path, ""val"", city, lbl)\n      dst = os.path.join(FLAGS.out, ""valid/lbl"",\n                         lbl.replace(annotation_tail, "".png""))\n      print(""copying "", src, "" to "", dst)\n      copyfile(src, dst)  # copy image\n\n  # test\n  test_cities = [""berlin"", ""bielefeld"",\n                 ""bonn"", ""leverkusen"", ""mainz"", ""munich""]\n  # create black labels, because we have nothing here.\n  black_label = np.full((100, 100), 255, dtype=np.uint8)\n  for city in test_cities:\n    images = [f for f in os.listdir(os.path.join(FLAGS.dataset, rgb_img_path, ""test"", city)) if (\n        rgb_img_tail in f and os.path.isfile(os.path.join(FLAGS.dataset, rgb_img_path, ""test"", city, f)))]\n    for img in images:\n      src = os.path.join(FLAGS.dataset, rgb_img_path, ""test"", city, img)\n      dst = os.path.join(FLAGS.out, ""test/img"",\n                         img.replace(rgb_img_tail, "".png""))\n      print(""copying "", src, "" to "", dst)\n      copyfile(src, dst)  # copy image\n      lbl_dst = os.path.join(FLAGS.out, ""test/lbl"",\n                             img.replace(rgb_img_tail, "".png""))\n      cv2.imwrite(lbl_dst, black_label)\n'"
train_py/dataset/aux_scripts/cwc_preprocess.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the output format extracted from the BAG that uses images, labels created\n  by Philipp\'s label creator and the YAML to create a consistent 1 to 1 mapping\n  dataset of image->label\n\n  Input:\n\n     - InputDirectory\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 images\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png\n          \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc...\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 labels\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1462872558_85679863_GroundTruth_color.png\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1462872558_85759015_GroundTruth_color.png\n          \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc...\n          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 timestamp\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.yaml\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.yaml\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc\n\n                  bonirob_2016-05-10-11-29-18_13_frame0.yaml contains:\n\n                   time stamp sec: 1462872558\n                   time stamp nsec: 85679863\n\n                   Which maps the name of the image to the name of the label\n\n      - OutputDirectory: Where to put the images\n\n      - Yaml config: - Mapping from colors in GroundTruth_color images to class\n                       numbers in grayscale image we output\n                     - Split for the randomization of the dataset\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 4.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 4.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 2.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 6.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 2.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 6.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 5.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 3.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 5.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 3.png\n\n      - The directory is randomized, split according to desired percentages, and\n        all the labels are mapped according to the mapping we want.\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cwc_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--cfg\', \'-c\',\n      type=str,\n      required=True,\n      help=\'Config yaml location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""cfg: %s"" % FLAGS.cfg)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to open dataset yaml\n  try:\n    print(""Opening cfg file %s"" % FLAGS.cfg)\n    f = open(FLAGS.cfg, \'r\')\n    CFG = yaml.load(f)\n  except:\n    print(""Error opening cfg yaml file. Check! Exiting..."")\n    quit()\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n  directories = [""images"", ""labels"", ""timestamp""]\n  for d in directories:\n    if not os.path.exists(FLAGS.dataset + d):\n      print(""%s dir does not exist. Check dataset folder"" %\n            (FLAGS.dataset + d))\n      quit()\n    else:\n      print(""Found dir dataset->%s"" % (d))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # create temporary output directory to put all the images and labels\n  tmpdir = FLAGS.out + ""/tmp/""\n  try:\n    print(""Creating tmp/ dir within output dir"")\n    os.makedirs(tmpdir)\n    os.makedirs(tmpdir + ""/images/"")\n    os.makedirs(tmpdir + ""/labels/"")\n  except:\n    print(""Cannot create tmp dirs"")\n\n  # check which files have an image and a corresponding label, and put both in\n  # a list\n  images = [f for f in listdir(\n      FLAGS.dataset + \'/images/\') if isfile(join(FLAGS.dataset + \'/images/\', f))]\n  labels = [f for f in listdir(\n      FLAGS.dataset + \'/labels/\') if isfile(join(FLAGS.dataset + \'/labels/\', f))]\n  ord_images = []\n  ord_labels = []\n\n  print(""Matching images to labels when possible"")\n  for img_name in images:\n    # get img stem for finding yaml\n    stem = os.path.splitext(img_name)[0]\n    # name of corresponding yaml file\n    yamlfile = FLAGS.dataset + \'/timestamp/\' + stem + "".yaml""\n    try:\n      # try to open the yaml file\n      f = open(yamlfile, \'r\')\n      # turn it into dictionary\n      f.readline()  # eat a line first, libyaml cannot handle ""%YAML:1.0""\n      times = yaml.load(f)\n    except:\n      print(""No timestamp for img %s in %s "" % (img_name, yamlfile))\n    finally:\n      # convert into label name\n      label_name = str(times[""time stamp sec""]) + ""_"" + \\\n          str(times[""time stamp nsec""]) + ""_GroundTruth_color.png""\n      # is it in list?\n      if label_name in labels:\n        # put in ordered list\n        # print(img_name,label_name)\n        ord_images.append(img_name)\n        ord_labels.append(label_name)\n      else:\n        print(""No label for img %s in %s "" % (img_name, label_name))\n  # zip them to access both at the same time\n  duples = zip(ord_images, ord_labels)\n\n  # Map each RGB label to a monochrome label and save it and its image to tmp dir\n  # naming them more nicely\n  idx = 0\n  for t in duples:\n    copyfile(FLAGS.dataset + ""/images/"" +\n             t[0], tmpdir + ""/images/"" + str(idx) + "".png"")  # copy image\n    copyfile(FLAGS.dataset + ""/labels/"" +\n             t[1], tmpdir + ""/labels/"" + str(idx) + "".png"")  # copy image\n    # map labels to monochrome\n    lbl_name = tmpdir + ""/labels/"" + str(idx) + "".png""\n    print(lbl_name)\n    lbl = cv2.imread(lbl_name)\n    h, w, d = lbl.shape\n    graylbl = np.zeros((h, w), np.uint8)\n    for key in CFG[""color_map""]:\n      graylbl[np.where((lbl == CFG[""color_map""][key]).all(2))] = key\n    cv2.imwrite(lbl_name, graylbl)\n    idx += 1\n\n  # shuffle temp dir files (only images, we know now that labels have the same name)\n  images = [f for f in listdir(tmpdir + ""/images/"")\n            if isfile(join(tmpdir + ""/images/"", f))]\n  files_num = len(images)\n  random.shuffle(images)\n\n  # split according to desired split\n  # create train, valid and test dirs\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (FLAGS.out + d))\n    if not os.path.exists(FLAGS.out + d):\n      os.makedirs(FLAGS.out + d)\n    for t in types:\n      print(""Creating %s dir."" % (FLAGS.out + d + t))\n      if not os.path.exists(FLAGS.out + d + t):\n        os.makedirs(FLAGS.out + d + t)\n\n  # move into output structure\n  train_split = CFG[""split""][0]\n  valid_split = CFG[""split""][1]\n  test_split = CFG[""split""][2]\n  if train_split + valid_split + valid_split > 100:\n    print(""Watch out, split is inconsistent. Doing my best."")\n    # normalize\n    n = (train_split + valid_split + test_split) / 100.00\n    print(""modified train split %f->%f"" % (train_split, train_split / n))\n    train_split /= n\n    print(""modified valid split %f->%f"" % (valid_split, valid_split / n))\n    valid_split /= n\n    print(""modified test split %f->%f"" % (test_split, test_split / n))\n    test_split /= n\n  files_num = len(images)\n\n  # train\n  train_num = int(math.floor(files_num * float(train_split) / 100.0))\n  train_set = images[0:train_num]\n  print(""Copying train files to %s"" % (FLAGS.out + ""/train/""))\n  for f in train_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/train/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/train/lbl/"" + f)  # copy images\n\n  # valid\n  valid_num = int(math.floor(files_num * float(valid_split) / 100.0))\n  valid_set = images[train_num:train_num + valid_num]\n  for f in valid_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/valid/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/valid/lbl/"" + f)  # copy images\n\n  # test\n  test_num = int(math.floor(files_num * float(test_split) / 100.0))\n  test_set = images[train_num + valid_num:train_num + valid_num + test_num]\n  for f in test_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/test/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/test/lbl/"" + f)  # copy images\n\n  # delete tmp dir\n  shutil.rmtree(tmpdir)\n'"
train_py/dataset/aux_scripts/cwc_preprocess_newstruct.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the output format extracted from the BAG that uses images and color\n  labels created by Philipp\'s label creator.\n\n  Input:\n\n     - InputDirectory\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 images\n          \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 rgb\n          \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n          \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png\n          \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc...\n          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 annotations\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 dlp\n                   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80colorCleaned\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png OR\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-05-13-20-54_20_frame0_GroundTruth_color\n                        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80etc...\n\n      - OutputDirectory: Where to put the images\n\n      - Yaml config: - Mapping from colors in GroundTruth_color images to class\n                       numbers in grayscale image we output\n                     - Split for the randomization of the dataset\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n\n      - The directory is randomized, split according to desired percentages, and\n        all the labels are mapped according to the mapping we want.\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cwc_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--cfg\', \'-c\',\n      type=str,\n      required=True,\n      help=\'Config yaml location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""cfg: %s"" % FLAGS.cfg)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to open dataset yaml\n  try:\n    print(""Opening cfg file %s"" % FLAGS.cfg)\n    f = open(FLAGS.cfg, \'r\')\n    CFG = yaml.load(f)\n  except:\n    print(""Error opening cfg yaml file. Check! Exiting..."")\n    quit()\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n  rgb_img_path = ""/images/rgb/""\n  annotations_path = ""/annotations/dlp/colorCleaned/""\n  directories = [rgb_img_path, annotations_path]\n  for d in directories:\n    if not os.path.exists(FLAGS.dataset + d):\n      print(""%s dir does not exist. Check dataset folder"" %\n            (FLAGS.dataset + d))\n      quit()\n    else:\n      print(""Found dir dataset->%s"" % (d))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # create temporary output directory to put all the images and labels\n  tmpdir = ""/tmp/cwctmp""\n  # try to remove\n  try:\n    shutil.rmtree(tmpdir)\n  except:\n    print(""no tmp dir to remove, passing"")\n    pass\n  # try to create\n  try:\n    print(""Creating /tmp/cwctmp dir"")\n    os.makedirs(tmpdir)\n    os.makedirs(tmpdir + ""/images/"")\n    os.makedirs(tmpdir + ""/labels/"")\n  except:\n    print(""Cannot create tmp dirs"")\n    quit()\n\n  # check which files have an image and a corresponding label, and put both in\n  # a list\n  images = [f for f in listdir(FLAGS.dataset + rgb_img_path) if isfile(join(FLAGS.dataset + rgb_img_path, f))]\n  labels = [f for f in listdir(FLAGS.dataset + annotations_path)\n            if isfile(join(FLAGS.dataset + annotations_path, f))]\n  ord_images = []\n  ord_labels = []\n\n  print(""Matching images to labels when possible"")\n  for img_name in images:\n    # get img stem for finding alt label\n    stem = os.path.splitext(img_name)[0]\n    # convert into label name\n    alt_label_name = stem + ""_GroundTruth_color.png""\n    alt_label_name_2 = stem + ""_GroundTruth_iMap.png""\n    # is it in list?\n    if (img_name in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(img_name)\n    elif (alt_label_name in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(alt_label_name)\n    elif (alt_label_name_2 in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(alt_label_name_2)\n    else:\n      print(""No label for img %s"" % (img_name))\n  # zip them to access both at the same time\n  duples = zip(ord_images, ord_labels)\n\n  # Map each RGB label to a monochrome label and save it and its image to tmp dir\n  # naming them more nicely (to match at least)\n  for t in duples:\n    copyfile(FLAGS.dataset + rgb_img_path +\n             t[0], tmpdir + ""/images/"" + t[0])  # copy image\n    copyfile(FLAGS.dataset + annotations_path +\n             t[1], tmpdir + ""/labels/"" + t[0])  # copy image\n    # map labels to monochrome\n    lbl_name = tmpdir + ""/labels/"" + t[0]\n    print(lbl_name)\n    lbl = cv2.imread(lbl_name)\n    h, w, d = lbl.shape\n    graylbl = np.zeros((h, w), np.uint8)\n    for key in CFG[""color_map""]:\n      graylbl[np.where((lbl == CFG[""color_map""][key]).all(2))] = key\n    cv2.imwrite(lbl_name, graylbl)\n\n  # shuffle temp dir files (only images, we know now that labels have the same name)\n  images = [f for f in listdir(tmpdir + ""/images/"")\n            if isfile(join(tmpdir + ""/images/"", f))]\n  files_num = len(images)\n  random.shuffle(images)\n\n  # split according to desired split\n  # create train, valid and test dirs\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (FLAGS.out + d))\n    if not os.path.exists(FLAGS.out + d):\n      os.makedirs(FLAGS.out + d)\n    for t in types:\n      print(""Creating %s dir."" % (FLAGS.out + d + t))\n      if not os.path.exists(FLAGS.out + d + t):\n        os.makedirs(FLAGS.out + d + t)\n\n  # move into output structure\n  train_split = CFG[""split""][0]\n  valid_split = CFG[""split""][1]\n  test_split = CFG[""split""][2]\n  if train_split + valid_split + valid_split > 100:\n    print(""Watch out, split is inconsistent. Doing my best."")\n    # normalize\n    n = (train_split + valid_split + test_split) / 100.00\n    print(""modified train split %f->%f"" % (train_split, train_split / n))\n    train_split /= n\n    print(""modified valid split %f->%f"" % (valid_split, valid_split / n))\n    valid_split /= n\n    print(""modified test split %f->%f"" % (test_split, test_split / n))\n    test_split /= n\n  files_num = len(images)\n\n  # train\n  train_num = int(math.floor(files_num * float(train_split) / 100.0))\n  train_set = images[0:train_num]\n  print(""Copying train files to %s"" % (FLAGS.out + ""/train/""))\n  for f in train_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/train/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/train/lbl/"" + f)  # copy images\n\n  # valid\n  valid_num = int(math.floor(files_num * float(valid_split) / 100.0))\n  valid_set = images[train_num:train_num + valid_num]\n  for f in valid_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/valid/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/valid/lbl/"" + f)  # copy images\n\n  # test\n  test_num = int(math.floor(files_num * float(test_split) / 100.0))\n  test_set = images[train_num + valid_num:train_num + valid_num + test_num]\n  for f in test_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/test/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/test/lbl/"" + f)  # copy images\n\n  # delete tmp dir\n  shutil.rmtree(tmpdir)\n'"
train_py/dataset/aux_scripts/cwc_preprocess_newstruct_nir.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto, Cyrill Stachniss. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the output format extracted from the BAG that uses images and color\n  labels created by Philipp\'s label creator.\n\n  Input:\n\n     - InputDirectory\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 images\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 rgb\n          \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n          \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png\n          \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc...\n          \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80nir\n          \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n          \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png\n          \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc...\n          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 annotations\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 dlp\n                   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80colorCleaned\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame0.png\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-10-11-29-18_13_frame1.png OR\n                        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 bonirob_2016-05-05-13-20-54_20_frame0_GroundTruth_color\n                        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80etc...\n\n      - OutputDirectory: Where to put the images\n\n      - Yaml config: - Mapping from colors in GroundTruth_color images to class\n                       numbers in grayscale image we output\n                     - Split for the randomization of the dataset\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n\n      - The directory is randomized, split according to desired percentages, and\n        all the labels are mapped according to the mapping we want.\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cwc_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--cfg\', \'-c\',\n      type=str,\n      required=True,\n      help=\'Config yaml location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""cfg: %s"" % FLAGS.cfg)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to open dataset yaml\n  try:\n    print(""Opening cfg file %s"" % FLAGS.cfg)\n    f = open(FLAGS.cfg, \'r\')\n    CFG = yaml.load(f)\n  except:\n    print(""Error opening cfg yaml file. Check! Exiting..."")\n    quit()\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n  rgb_img_path = ""/images/rgb/""\n  nir_img_path = ""/images/nir/""\n  annotations_path = ""/annotations/dlp/colorCleaned/""\n  # annotations_path = ""/annotations/PNG/pixelwise/color/""\n  directories = [rgb_img_path, nir_img_path, annotations_path]\n  for d in directories:\n    if not os.path.exists(FLAGS.dataset + d):\n      print(""%s dir does not exist. Check dataset folder"" %\n            (FLAGS.dataset + d))\n      quit()\n    else:\n      print(""Found dir dataset->%s"" % (d))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # create temporary output directory to put all the images and labels\n  tmpdir = ""/tmp/cwctmp""\n  # try to remove\n  try:\n    shutil.rmtree(tmpdir)\n  except:\n    print(""no tmp dir to remove, passing"")\n    pass\n  # try to create\n  try:\n    print(""Creating /tmp/cwctmp dir"")\n    os.makedirs(tmpdir)\n    os.makedirs(tmpdir + ""/images/"")\n    os.makedirs(tmpdir + ""/labels/"")\n  except:\n    print(""Cannot create tmp dirs"")\n    quit()\n\n  # check which files have an image and a corresponding label, and put both in\n  # a list\n  images = [f for f in listdir(FLAGS.dataset + rgb_img_path) if isfile(join(FLAGS.dataset + rgb_img_path, f))]\n  nir = [f for f in listdir(FLAGS.dataset + nir_img_path)\n         if isfile(join(FLAGS.dataset + nir_img_path, f))]\n  labels = [f for f in listdir(FLAGS.dataset + annotations_path)\n            if isfile(join(FLAGS.dataset + annotations_path, f))]\n  rgb_nir_images = []\n  ord_images = []\n  ord_labels = []\n\n  print(""MAtching rgb to nir when possible"")\n  for img_name in images:\n    if (img_name in nir):\n      print(""Found nir for img %s"" % (img_name))\n      rgb_nir_images.append(img_name)\n    else:\n      print(""No nir for img %s"" % (img_name))\n\n  print(""Matching images to labels when possible"")\n  for img_name in rgb_nir_images:\n    # get img stem for finding alt label\n    stem = os.path.splitext(img_name)[0]\n    # convert into label name\n    alt_label_name = stem + ""_GroundTruth_color.png""\n    alt_label_name_2 = stem + ""_GroundTruth_iMap.png""\n    # is it in list?\n    if (img_name in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(img_name)\n    elif (alt_label_name in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(alt_label_name)\n    elif (alt_label_name_2 in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(alt_label_name_2)\n    else:\n      print(""No label for img %s"" % (img_name))\n  # zip them to access both at the same time\n  duples = zip(ord_images, ord_labels)\n\n  # Map each RGB label to a monochrome label and save it and its image to tmp dir\n  # naming them more nicely (to match at least)\n  for t in duples:\n    # copyfile(FLAGS.dataset+rgb_img_path+t[0],tmpdir+""/images/""+t[0]) # copy image\n\n    # instead of copying, open both rgb and nir, mix in one 4dimensional tensor and save\n    rgb = cv2.imread(FLAGS.dataset + rgb_img_path + t[0])  # get rgb\n    nir = cv2.imread(FLAGS.dataset + nir_img_path +\n                     t[0], 0)  # get nir grayscale\n    stacked = np.dstack((rgb, nir))  # stack\n    print(""saving image to name "", tmpdir + ""/images/"" + t[0])\n    # save to temp folder as 4D png\n    cv2.imwrite(tmpdir + ""/images/"" + t[0], stacked)\n\n    # map labels to monochrome\n    lbl_name = tmpdir + ""/labels/"" + t[0]\n    copyfile(FLAGS.dataset + annotations_path + t[1], lbl_name)  # copy label\n    print(lbl_name)\n    lbl = cv2.imread(lbl_name)\n    h, w, d = lbl.shape\n    graylbl = np.zeros((h, w), np.uint8)\n    for key in CFG[""color_map""]:\n      graylbl[np.where((lbl == CFG[""color_map""][key]).all(2))] = key\n    cv2.imwrite(lbl_name, graylbl)\n\n  # shuffle temp dir files (only images, we know now that labels have the same name)\n  images = [f for f in listdir(tmpdir + ""/images/"")\n            if isfile(join(tmpdir + ""/images/"", f))]\n  files_num = len(images)\n  random.shuffle(images)\n\n  # split according to desired split\n  # create train, valid and test dirs\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (FLAGS.out + d))\n    if not os.path.exists(FLAGS.out + d):\n      os.makedirs(FLAGS.out + d)\n    for t in types:\n      print(""Creating %s dir."" % (FLAGS.out + d + t))\n      if not os.path.exists(FLAGS.out + d + t):\n        os.makedirs(FLAGS.out + d + t)\n\n  # move into output structure\n  train_split = CFG[""split""][0]\n  valid_split = CFG[""split""][1]\n  test_split = CFG[""split""][2]\n  if train_split + valid_split + valid_split > 100:\n    print(""Watch out, split is inconsistent. Doing my best."")\n    # normalize\n    n = (train_split + valid_split + test_split) / 100.00\n    print(""modified train split %f->%f"" % (train_split, train_split / n))\n    train_split /= n\n    print(""modified valid split %f->%f"" % (valid_split, valid_split / n))\n    valid_split /= n\n    print(""modified test split %f->%f"" % (test_split, test_split / n))\n    test_split /= n\n  files_num = len(images)\n\n  # train\n  train_num = int(math.floor(files_num * float(train_split) / 100.0))\n  train_set = images[0:train_num]\n  print(""Copying train files to %s"" % (FLAGS.out + ""/train/""))\n  for f in train_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/train/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/train/lbl/"" + f)  # copy images\n\n  # valid\n  valid_num = int(math.floor(files_num * float(valid_split) / 100.0))\n  valid_set = images[train_num:train_num + valid_num]\n  for f in valid_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/valid/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/valid/lbl/"" + f)  # copy images\n\n  # test\n  test_num = int(math.floor(files_num * float(test_split) / 100.0))\n  test_set = images[train_num + valid_num:train_num + valid_num + test_num]\n  for f in test_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/test/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/test/lbl/"" + f)  # copy images\n\n  # delete tmp dir\n  shutil.rmtree(tmpdir)\n'"
train_py/dataset/aux_scripts/general_preprocess.py,0,"b'# this script is helping to create dataset from image and labeled image folders. By create a new folder for Training,\n# Validation, and Testing. AS default the image of validation and Testing dataset will be 10% for each, however, it\n# could be change that by but the value they want for the arguments --v and --t.\n\nimport argparse\nimport os\nimport numpy as np\nfrom shutil import copyfile\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--img\', action=\'store\', dest=\'images_path\', help=\'the path of the images folder.\')\nparser.add_argument(\'--lbl\', action=\'store\', dest=\'labels_path\', help=\'the path of the labels folder.\')\nparser.add_argument(\'--dis\', action=\'store\', dest=\'des_path\', help=\'the path of the save folder.\'\n                    , default=False)\nparser.add_argument(\'--v\', action=\'store\', dest=\'val_per\', help=\'the percent of the validation set\'\n                    , default=False, type=int)\nparser.add_argument(\'--t\', action=\'store\', dest=\'test_per\', help=\'the percent of the test set\'\n                    , default=False, type=float)\n\nargs = parser.parse_args()\nif not args.des_path:\n    args.des_path = "".""\n\nif not args.val_per:\n    args.val_per = 10\n\nif not args.test_per:\n    args.test_per = 10\n\nimg_list = np.array(os.listdir(args.images_path))\nlbl_list = np.array(os.listdir(args.labels_path))\n\n# create random array\nrandom_array = np.arange(len(img_list))\nnp.random.shuffle(random_array)\n\n# find the test_set\nnum_of_test_set = int(len(img_list) * args.test_per / 100)\ntest_data_set = img_list[random_array[:num_of_test_set]]\n# get index of test dataset\ntest_data_set_idx = list()\nfor f in test_data_set:\n    test_data_set_idx.append(list(img_list).index(f))\n\n# delete the test data set from img_list\nimg_list = np.delete(img_list, np.array(test_data_set_idx))\nlbl_list = np.delete(lbl_list, np.array(test_data_set_idx))\n\n# create random array\nrandom_array = np.arange(len(img_list))\nnp.random.shuffle(random_array)\n# find the valid set\nnum_of_valid_set = int(len(img_list) * args.val_per / 100)\nvalid_data_set = img_list[random_array[:num_of_valid_set]]\n\n# create folders\n# create Bonnet_dataset\nargs.des_path = os.path.join(args.des_path, ""Bonnet_dataset"")\nif not os.path.exists(args.des_path):\n    os.makedirs(args.des_path)\n\n# create train\ntrain_path = os.path.join(args.des_path, ""train"")\nif not os.path.exists(train_path):\n    os.makedirs(train_path)\n\ntrain_img_path = os.path.join(train_path, ""img"")\nif not os.path.exists(train_img_path):\n    os.makedirs(train_img_path)\n\ntrain_lbl_path = os.path.join(train_path, ""lbl"")\nif not os.path.exists(train_lbl_path):\n    os.makedirs(train_lbl_path)\n\n# copy train files\nfor f in img_list:\n    copyfile(os.path.join(args.images_path, f), os.path.join(train_img_path, f))\n    copyfile(os.path.join(args.labels_path, f), os.path.join(train_lbl_path, f))\n\n# create validation\nvalid_path = os.path.join(args.des_path, ""valid"")\nif not os.path.exists(valid_path):\n    os.makedirs(valid_path)\n\nvalid_img_path = os.path.join(valid_path, ""img"")\nif not os.path.exists(valid_img_path):\n    os.makedirs(valid_img_path)\n\nvalid_lbl_path = os.path.join(valid_path, ""lbl"")\nif not os.path.exists(valid_lbl_path):\n    os.makedirs(valid_lbl_path)\n\n# copy valid files\nfor f in valid_data_set:\n    copyfile(os.path.join(args.images_path, f), os.path.join(valid_img_path, f))\n    copyfile(os.path.join(args.labels_path, f), os.path.join(valid_lbl_path, f))\n\n# create test\ntest_path = os.path.join(args.des_path, ""test"")\nif not os.path.exists(test_path):\n    os.makedirs(test_path)\n\ntest_img_path = os.path.join(test_path, ""img"")\nif not os.path.exists(test_img_path):\n    os.makedirs(test_img_path)\n\ntest_lbl_path = os.path.join(test_path, ""lbl"")\nif not os.path.exists(test_lbl_path):\n    os.makedirs(test_lbl_path)\n\n# copy valid files\nfor f in test_data_set:\n    copyfile(os.path.join(args.images_path, f), os.path.join(test_img_path, f))\n    copyfile(os.path.join(args.labels_path, f), os.path.join(test_lbl_path, f))\n'"
train_py/dataset/aux_scripts/persons_preprocess.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Use the output format extracted from the BAG that uses images, labels created\n  by Philipp\'s label creator and the YAML to create a consistent 1 to 1 mapping\n  dataset of image->label\n\n  Input:\n\n     - InputDirectory\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds1/\n          \xe2\x94\x82        \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n          \xe2\x94\x82        \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 masks_machine\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds10/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds11/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds12/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds13/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds2/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds3/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds4/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds5/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds6/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds7/\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 persons__ds8/\n          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 persons__ds9/\n\n      - OutputDirectory: Where to put the images\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 4.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 1.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 4.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 2.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 6.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 2.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 6.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 5.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 3.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 5.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 3.png\n\n      - The directory is randomized, split according to desired percentages, and\n        all the labels are mapped according to the mapping we want.\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./cwc_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n  datasets = [""/persons__ds10/"",\n              ""/persons__ds11/"",\n              ""/persons__ds12/"",\n              ""/persons__ds13/"",\n              ""/persons__ds2/"",\n              ""/persons__ds3/"",\n              ""/persons__ds4/"",\n              ""/persons__ds5/"",\n              ""/persons__ds6/"",\n              ""/persons__ds7/"",\n              ""/persons__ds8/"",\n              ""/persons__ds9/"",\n              ""/coco_val2017/"",\n              ""/coco_train2017/""]\n  directories = [""/img"", ""/masks_machine""]\n\n  for dat in datasets:\n    for dire in directories:\n      if not os.path.exists(FLAGS.dataset + dat + dire):\n        print(""%s dir does not exist. Check dataset folder"" %\n              (FLAGS.dataset + dat + dire))\n        quit()\n      else:\n        print(""Found dir dataset->%s"" % (FLAGS.dataset + dat + dire))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # check which files have an image and a corresponding label, and put both in\n  # a list\n  images = []\n  labels = []\n  for dataset in datasets:\n    # append new images (avoiding duplicates)\n    new_images = [FLAGS.dataset + dataset + \'/img/\' + f\n                  for f in listdir(FLAGS.dataset + dataset + \'/img/\')\n                  if isfile(join(FLAGS.dataset + dataset + \'/img/\', f))]\n    for image in new_images:\n      if os.path.splitext(os.path.basename(image))[0] not in [os.path.splitext(os.path.basename(i))[0] for i in images]:\n        images.append(image)\n      else:\n        print(""Image"", image, ""is duplicated. Removing..."")\n\n    # append new labels (avoiding duplicates)\n    new_labels = [FLAGS.dataset + dataset + \'/masks_machine/\' + f\n                  for f in listdir(FLAGS.dataset + dataset + \'/masks_machine/\')\n                  if isfile(join(FLAGS.dataset + dataset + \'/masks_machine/\', f))]\n    for label in new_labels:\n      if os.path.splitext(os.path.basename(label))[0] not in [os.path.splitext(os.path.basename(l))[0] for l in labels]:\n        labels.append(label)\n      else:\n        print(""label"", label, ""is duplicated. Removing..."")\n\n  # order to match\n  images.sort()\n  labels.sort()\n  assert(len(images) == len(labels))\n  duples = list(zip(images, labels))\n\n  # shuffle temp dir files (only images, we know now that labels have the same name)\n  files_num = len(duples)\n  random.shuffle(duples)\n\n  # split according to desired split\n  # create train, valid and test dirs\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (FLAGS.out + d))\n    if not os.path.exists(FLAGS.out + d):\n      os.makedirs(FLAGS.out + d)\n    for t in types:\n      print(""Creating %s dir."" % (FLAGS.out + d + t))\n      if not os.path.exists(FLAGS.out + d + t):\n        os.makedirs(FLAGS.out + d + t)\n\n  # move into output structure\n  train_split = 80\n  valid_split = 10\n  test_split = 10\n  if train_split + valid_split + valid_split > 100:\n    print(""Watch out, split is inconsistent. Doing my best."")\n    # normalize\n    n = (train_split + valid_split + test_split) / 100.00\n    print(""modified train split %f->%f"" % (train_split, train_split / n))\n    train_split /= n\n    print(""modified valid split %f->%f"" % (valid_split, valid_split / n))\n    valid_split /= n\n    print(""modified test split %f->%f"" % (test_split, test_split / n))\n    test_split /= n\n\n  # train\n  print(""Copying train files to %s"" % (FLAGS.out + ""/train/""))\n  train_num = int(math.floor(files_num * float(train_split) / 100.0))\n  train_set = duples[0:train_num]\n  for d in train_set:\n    copyfile(d[0], FLAGS.out + ""/train/img/"" +\n             os.path.basename(d[0]))  # copy images\n    copyfile(d[1], FLAGS.out + ""/train/lbl/"" +\n             os.path.basename(d[1]))  # copy labels\n\n  # valid\n  print(""Copying valid files to %s"" % (FLAGS.out + ""/valid/""))\n  valid_num = int(math.floor(files_num * float(valid_split) / 100.0))\n  valid_set = duples[train_num:train_num + valid_num]\n  for d in valid_set:\n    copyfile(d[0], FLAGS.out + ""/valid/img/"" +\n             os.path.basename(d[0]))  # copy images\n    copyfile(d[1], FLAGS.out + ""/valid/lbl/"" +\n             os.path.basename(d[1]))  # copy labels\n\n  # test\n  print(""Copying test files to %s"" % (FLAGS.out + ""/test/""))\n  test_num = int(math.floor(files_num * float(test_split) / 100.0))\n  test_set = duples[train_num + valid_num:train_num + valid_num + test_num]\n  for d in test_set:\n    copyfile(d[0], FLAGS.out + ""/test/img/"" +\n             os.path.basename(d[0]))  # copy images\n    copyfile(d[1], FLAGS.out + ""/test/lbl/"" +\n             os.path.basename(d[1]))  # copy labels\n'"
train_py/dataset/aux_scripts/synthia_preprocess.py,0,"b'#!/usr/bin/python3\n# coding: utf-8\n\n# Copyright 2017 Andres Milioto. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Input:\n\n     - InputDirectory\n          \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 RGB\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ap_000_01-11-2015_19-20-57_000000_0_Rand_0.png\n          \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ap_000_01-11-2015_19-20-57_000000_0_Rand_1.png\n          \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc\n          \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 GT\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ap_000_01-11-2015_19-20-57_000000_0_Rand_0.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 ap_000_01-11-2015_19-20-57_000000_0_Rand_1.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 etc\n\n      - OutputDirectory: Where to put the images\n\n      - Yaml config: - Mapping from colors in GroundTruth_color images to class\n                       numbers in grayscale image we output\n                     - Split for the randomization of the dataset\n\n  Output:\n\n      - OutputDirectory\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 train\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 valid\n              \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n              \xe2\x94\x82       \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n              \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80test\n                  \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 img\n                  \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                  \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 lbl\n                      \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n                      \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 xxxxxxxxxx.png\n\n      - The directory is randomized, split according to desired percentages, and\n        all the labels are mapped according to the mapping we want.\n\'\'\'\n\nimport os\nimport yaml\nfrom shutil import copyfile\nimport argparse\nimport shutil\nfrom os import listdir\nfrom os.path import isfile, join\nimport cv2\nimport numpy as np\nimport random\nimport math\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser(""./synthia_preprocess.py"")\n  parser.add_argument(\n      \'--dataset\', \'-d\',\n      type=str,\n      required=True,\n      help=\'Dataset location. No default!\',\n  )\n  parser.add_argument(\n      \'--cfg\', \'-c\',\n      type=str,\n      required=True,\n      help=\'Config yaml location. No default!\',\n  )\n  parser.add_argument(\n      \'--out\', \'-o\',\n      type=str,\n      required=True,\n      help=\'Organized dataset location. No default!\',\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n\n  # print summary of what we will do\n  print(""----------"")\n  print(""INTERFACE:"")\n  print(""dataset: %s"" % FLAGS.dataset)\n  print(""cfg: %s"" % FLAGS.cfg)\n  print(""out: %s"" % FLAGS.out)\n  print(""----------"")\n\n  # try to open dataset yaml\n  try:\n    print(""Opening cfg file %s"" % FLAGS.cfg)\n    f = open(FLAGS.cfg, \'r\')\n    CFG = yaml.load(f)\n  except:\n    print(""Error opening cfg yaml file. Check! Exiting..."")\n    quit()\n\n  # try to get the dataset:\n  print(""Parsing the dataset"")\n  rgb_img_path = ""/RGB/""\n  annotations_path = ""/GT/""\n  directories = [rgb_img_path, annotations_path]\n  for d in directories:\n    if not os.path.exists(FLAGS.dataset + d):\n      print(""%s dir does not exist. Check dataset folder"" %\n            (FLAGS.dataset + d))\n      quit()\n    else:\n      print(""Found dir dataset->%s"" % (d))\n\n  # create the output directory structure\n  try:\n    if os.path.exists(FLAGS.out):\n      shutil.rmtree(FLAGS.out)\n    print(""Creating output dir"")\n    os.makedirs(FLAGS.out)\n  except:\n    print(""Cannot create output dir"")\n\n  # create temporary output directory to put all the images and labels\n  tmpdir = ""/tmp/synthiatmp""\n  # try to remove\n  try:\n    shutil.rmtree(tmpdir)\n  except:\n    print(""no tmp dir to remove, passing"")\n    pass\n  # try to create\n  try:\n    print(""Creating /tmp/synthiatmp dir"")\n    os.makedirs(tmpdir)\n    os.makedirs(tmpdir + ""/images/"")\n    os.makedirs(tmpdir + ""/labels/"")\n  except:\n    print(""Cannot create tmp dirs"")\n    quit()\n\n  # check which files have an image and a corresponding label, and put both in\n  # a list\n  images = [f for f in listdir(FLAGS.dataset + rgb_img_path) if isfile(join(FLAGS.dataset + rgb_img_path, f))]\n  labels = [f for f in listdir(FLAGS.dataset + annotations_path)\n            if isfile(join(FLAGS.dataset + annotations_path, f))]\n  ord_images = []\n  ord_labels = []\n\n  print(""Matching images to labels when possible"")\n  for img_name in images:\n    if (img_name in labels):\n      # put in ordered list\n      # print(img_name,label_name)\n      ord_images.append(img_name)\n      ord_labels.append(img_name)\n    else:\n      print(""No label for img %s"" % (img_name))\n  # zip them to access both at the same time\n  duples = zip(ord_images, ord_labels)\n\n  # Map each RGB label to a monochrome label and save it and its image to tmp dir\n  # naming them more nicely (to match at least)\n  for t in duples:\n    copyfile(FLAGS.dataset + rgb_img_path +\n             t[0], tmpdir + ""/images/"" + t[0])  # copy image\n    copyfile(FLAGS.dataset + annotations_path +\n             t[1], tmpdir + ""/labels/"" + t[0])  # copy image\n    # map labels to monochrome\n    lbl_name = tmpdir + ""/labels/"" + t[0]\n    print(lbl_name)\n    lbl = cv2.imread(lbl_name)\n    h, w, d = lbl.shape\n    graylbl = np.zeros((h, w), np.uint8)\n    for key in CFG[""color_map""]:\n      graylbl[np.where((lbl == CFG[""color_map""][key]).all(2))] = key\n    cv2.imwrite(lbl_name, graylbl)\n\n  # shuffle temp dir files (only images, we know now that labels have the same name)\n  images = [f for f in listdir(tmpdir + ""/images/"")\n            if isfile(join(tmpdir + ""/images/"", f))]\n  files_num = len(images)\n  random.shuffle(images)\n\n  # split according to desired split\n  # create train, valid and test dirs\n  directories = [""/train"", ""/valid"", ""/test""]\n  types = [""/img"", ""/lbl""]\n  for d in directories:\n    print(""Creating %s dir."" % (FLAGS.out + d))\n    if not os.path.exists(FLAGS.out + d):\n      os.makedirs(FLAGS.out + d)\n    for t in types:\n      print(""Creating %s dir."" % (FLAGS.out + d + t))\n      if not os.path.exists(FLAGS.out + d + t):\n        os.makedirs(FLAGS.out + d + t)\n\n  # move into output structure\n  train_split = CFG[""split""][0]\n  valid_split = CFG[""split""][1]\n  test_split = CFG[""split""][2]\n  if train_split + valid_split + valid_split > 100:\n    print(""Watch out, split is inconsistent. Doing my best."")\n    # normalize\n    n = (train_split + valid_split + test_split) / 100.00\n    print(""modified train split %f->%f"" % (train_split, train_split / n))\n    train_split /= n\n    print(""modified valid split %f->%f"" % (valid_split, valid_split / n))\n    valid_split /= n\n    print(""modified test split %f->%f"" % (test_split, test_split / n))\n    test_split /= n\n  files_num = len(images)\n\n  # train\n  train_num = int(math.floor(files_num * float(train_split) / 100.0))\n  train_set = images[0:train_num]\n  print(""Copying train files to %s"" % (FLAGS.out + ""/train/""))\n  for f in train_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/train/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/train/lbl/"" + f)  # copy images\n\n  # valid\n  valid_num = int(math.floor(files_num * float(valid_split) / 100.0))\n  valid_set = images[train_num:train_num + valid_num]\n  for f in valid_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/valid/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/valid/lbl/"" + f)  # copy images\n\n  # test\n  test_num = int(math.floor(files_num * float(test_split) / 100.0))\n  test_set = images[train_num + valid_num:train_num + valid_num + test_num]\n  for f in test_set:\n    copyfile(tmpdir + ""/images/"" + f, FLAGS.out +\n             ""/test/img/"" + f)  # copy images\n    copyfile(tmpdir + ""/labels/"" + f, FLAGS.out +\n             ""/test/lbl/"" + f)  # copy images\n\n  # delete tmp dir\n  shutil.rmtree(tmpdir)\n'"
train_py/dataset/aux_scripts/util.py,0,"b'# Copyright 2017 Andres Milioto. All Rights Reserved.\n#\n#  This file is part of Bonnet.\n#\n#  Bonnet is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Bonnet is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Bonnet. If not, see <http://www.gnu.org/licenses/>.\n\n\'\'\'\n  Some auxiliary functions to do some opencv stuff\n\'\'\'\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n\ndef im_plt(img, title=None):\n  """"""\n    Open image and print it on screen\n  """"""\n  plt.ion()\n  plt.figure()\n  plt.imshow(cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB))\n  if title is not None:\n    plt.title(str(title))\n\n\ndef im_tight_plt(img):\n  """"""\n    Open image and print it without borders on screen\n  """"""\n  plt.ion()\n  fig, ax = plt.subplots()\n  fig.subplots_adjust(0, 0, 1, 1)\n  ax.axis(""off"")\n  ax.imshow(cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB))\n\n\ndef im_gray_plt(img, title=None):\n  """"""\n    Open grayscale image and print it on screen\n  """"""\n  plt.ion()\n  plt.figure()\n  plt.imshow(img.astype(np.uint8), cmap=plt.get_cmap(\'gist_gray\'))\n  if title is not None:\n    plt.title(str(title))\n\n\ndef hist_plot(img, title=None):\n  """"""\n    Calculate histogram and plot it\n  """"""\n  plt.ion()\n  plt.figure()\n  plt.hist(img.ravel(), 256)\n  if title is not None:\n    plt.title(str(title))\n\n\ndef im_block():\n  """"""\n  Blocks thread until windows are closed\n  """"""\n  plt.show(block=True)\n\n\ndef transparency(img, mask):\n  alpha = 1\n  beta = 0.5\n  gamma = 0\n  rows, cols, depth = mask.shape\n  img = cv2.resize(img, (cols, rows)).astype(np.uint8)\n  mask = mask.astype(np.uint8)\n  transparent_mask = cv2.addWeighted(img, alpha, mask, beta, gamma, dtype=-1)\n  return img, transparent_mask\n\n\ndef prediction_to_color(predicted_mask, label_remap, color_map):\n  # get prediction and make it color\n  # map to color\n  color_mask = np.zeros([predicted_mask.shape[0], predicted_mask.shape[1], 3])\n  for key in label_remap:\n    color_mask[np.where((predicted_mask == label_remap[key]))] = color_map[key]\n  return color_mask\n'"
