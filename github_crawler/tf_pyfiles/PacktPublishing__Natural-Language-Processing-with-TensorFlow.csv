file_path,api_count,code
ch10/word2vec.py,25,"b'import tensorflow as tf\nimport numpy as np\nimport math\nsentence_cursors = None\ntot_sentences = None\nsrc_max_sent_length, tgt_max_sent_length = 0, 0\nsrc_dictionary, tgt_dictionary = {}, {}\nsrc_reverse_dictionary, tgt_reverse_dictionary = {},{}\ntrain_inputs, train_outputs = None, None\nembedding_size = None # Dimension of the embedding vector.\nvocabulary_size = None\ndef define_data_and_hyperparameters(\n        _tot_sentences, _src_max, _tgt_max, _src_dict, _tgt_dict,\n        _src_rev_dict, _tgt_rev_dict, _tr_inp, _tr_out, _emb_size, _vocab_size):\n    global tot_sentences, sentence_cursors\n    global src_max_sent_length, tgt_max_sent_length\n    global src_dictionary, tgt_dictionary\n    global src_reverse_dictionary, tgt_reverse_dictionary\n    global train_inputs, train_outputs\n    global embedding_size, vocabulary_size\n\n    embedding_size = _emb_size\n    vocabulary_size = _vocab_size\n    src_max_sent_length, tgt_max_sent_length = _src_max, _tgt_max\n\n    src_dictionary = _src_dict\n    tgt_dictionary = _tgt_dict\n\n    src_reverse_dictionary = _src_rev_dict\n    tgt_reverse_dictionary = _tgt_rev_dict\n\n    train_inputs = _tr_inp\n    train_outputs = _tr_out\n\n    tot_sentences = _tot_sentences\n    sentence_cursors = [0 for _ in range(tot_sentences)]\n\n\ndef generate_batch_for_word2vec(batch_size, window_size, is_source):\n    # window_size is the amount of words we\'re looking at from each side of a given word\n    # creates a single batch\n    global sentence_cursors\n    global src_dictionary, tgt_dictionary\n    global train_inputs, train_outputs\n    span = 2 * window_size + 1  # [ skip_window target skip_window ]\n\n    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    # e.g if skip_window = 2 then span = 5\n    # span is the length of the whole frame we are considering for a single word (left + word + right)\n    # skip_window is the length of one side\n\n    sentence_ids_for_batch = np.random.randint(0, tot_sentences, batch_size)\n\n    for b_i in range(batch_size):\n        sent_id = sentence_ids_for_batch[b_i]\n\n        if is_source:\n            buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n        else:\n            buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n        assert buffer.size == span, \'Buffer length (%d), Current data index (%d), Span(%d)\' % (\n        buffer.size, sentence_cursors[sent_id], span)\n        # If we only have EOS tokesn in the sampled text, we sample a new one\n        if is_source:\n            while np.all(buffer == src_dictionary[\'</s>\']):\n                # reset the sentence_cursors for that cap_id\n                sentence_cursors[sent_id] = 0\n                # sample a new cap_id\n                sent_id = np.random.randint(0, tot_sentences)\n                buffer = train_inputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n        else:\n            while np.all(buffer == tgt_dictionary[\'</s>\']):\n                # reset the sentence_cursors for that cap_id\n                sentence_cursors[sent_id] = 0\n                # sample a new cap_id\n                sent_id = np.random.randint(0, tot_sentences)\n                buffer = train_outputs[sent_id, sentence_cursors[sent_id]:sentence_cursors[sent_id] + span]\n\n        # fill left and right sides of batch\n        batch[b_i, :window_size] = buffer[:window_size]\n        batch[b_i, window_size:] = buffer[window_size + 1:]\n\n        labels[b_i, 0] = buffer[window_size]\n\n        # increase the corresponding index\n        if is_source:\n            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (src_max_sent_length - span)\n        else:\n            sentence_cursors[sent_id] = (sentence_cursors[sent_id] + 1) % (tgt_max_sent_length - span)\n\n    assert batch.shape[0] == batch_size and batch.shape[1] == span - 1\n    return batch, labels\n\n\ndef print_some_batches():\n    global sentence_cursors, tot_sentences\n    global src_reverse_dictionary\n\n    for window_size in [1, 2]:\n        sentence_cursors = [0 for _ in range(tot_sentences)]\n        batch, labels = generate_batch_for_word2vec(batch_size=8, window_size=window_size, is_source=True)\n        print(\'\\nwith window_size = %d:\' % (window_size))\n        print(\'    batch:\', [[src_reverse_dictionary[bii] for bii in bi] for bi in batch])\n        print(\'    labels:\', [src_reverse_dictionary[li] for li in labels.reshape(8)])\n\n    sentence_cursors = [0 for _ in range(tot_sentences)]\n\nbatch_size, window_size = None, None\nvalid_size, valid_window, valid_examples = None, None, None\nnum_sampled = None\n\ntrain_dataset, train_labels = None, None\nvalid_dataset = None\n\nsoftmax_weights, softmax_biases = None, None\n\nloss, optimizer, similarity, normalized_embeddings = None, None, None, None\n\ndef define_word2vec_tensorflow(batch_size):\n\n    global embedding_size, window_size\n    global\tvalid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases\n    global loss, optimizer, similarity\n    global vocabulary_size, embedding_size\n    global normalized_embeddings\n\n\n    window_size = 2  # How many words to consider left and right.\n    # We pick a random validation set to sample nearest neighbors. here we limit the\n    # validation samples to the words that have a low numeric ID, which by\n    # construction are also the most frequent.\n    valid_size = 20  # Random set of words to evaluate similarity on.\n    valid_window = 100  # Only pick dev samples in the head of the distribution.\n    # pick 16 samples from 100\n    valid_examples = np.array(np.random.randint(0, valid_window, valid_size // 2))\n    valid_examples = np.append(valid_examples, np.random.randint(1000, 1000 + valid_window, valid_size // 2))\n    num_sampled = 32  # Number of negative examples to sample.\n\n    tf.reset_default_graph()\n\n    # Input data.\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * window_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Variables.\n    # embedding, vector for each word in the vocabulary\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float32))\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                                      stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float32))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size], dtype=tf.float32))\n\n    # Model.\n    # Look up embeddings for inputs.\n    # this might efficiently find the embeddings for given ids (traind dataset)\n    # manually doing this might not be efficient given there are 50000 entries in embeddings\n    stacked_embedings = None\n    print(\'Defining %d embedding lookups representing each word in the context\' % (2 * window_size))\n    for i in range(2 * window_size):\n        embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n        x_size, y_size = embedding_i.get_shape().as_list()\n        if stacked_embedings is None:\n            stacked_embedings = tf.reshape(embedding_i, [x_size, y_size, 1])\n        else:\n            stacked_embedings = tf.concat(axis=2,\n                                          values=[stacked_embedings, tf.reshape(embedding_i, [x_size, y_size, 1])])\n\n    assert stacked_embedings.get_shape().as_list()[2] == 2 * window_size\n    print(""Stacked embedding size: %s"" % stacked_embedings.get_shape().as_list())\n    mean_embeddings = tf.reduce_mean(stacked_embedings, 2, keepdims=False)\n    print(""Reduced mean embedding size: %s"" % mean_embeddings.get_shape().as_list())\n\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    # inputs are embeddings of the train words\n    # with this loss we optimize weights, biases, embeddings\n\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n    # Optimizer.\n    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n\n\ndef run_word2vec_source(batch_size):\n    global embedding_size, window_size\n    global valid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases\n    global loss, optimizer, similarity, normalized_embeddings\n    global src_reverse_dictionary\n    global vocabulary_size, embedding_size\n\n    num_steps = 100001\n\n    config=tf.ConfigProto(allow_soft_placement=True) \n    config.gpu_options.allow_growth = True\t\n    \t\n    with tf.Session(config=config) as session:\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        average_loss = 0\n        for step in range(num_steps):\n\n            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=True)\n            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n            average_loss += l\n            if (step + 1) % 2000 == 0:\n                if step > 0:\n                    average_loss = average_loss / 2000\n                    # The average loss is an estimate of the loss over the last 2000 batches.\n                print(\'Average loss at step %d: %f\' % (step + 1, average_loss))\n                average_loss = 0\n            # note that this is expensive (~20% slowdown if computed every 500 steps)\n            if (step + 1) % 10000 == 0:\n                sim = similarity.eval()\n                for i in range(valid_size):\n                    valid_word = src_reverse_dictionary[valid_examples[i]]\n                    top_k = 8  # number of nearest neighbors\n                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                    log = \'Nearest to %s:\' % valid_word\n                    for k in range(top_k):\n                        close_word = src_reverse_dictionary[nearest[k]]\n                        log = \'%s %s,\' % (log, close_word)\n                    print(log)\n        cbow_final_embeddings = normalized_embeddings.eval()\n\n    np.save(\'de-embeddings.npy\', cbow_final_embeddings)\n\ndef run_word2vec_target(batch_size):\n    global embedding_size, window_size\n    global valid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases\n    global loss, optimizer, similarity, normalized_embeddings\n    global tgt_reverse_dictionary\n    global vocabulary_size, embedding_size\n\n    num_steps = 100001\n    \n    config=tf.ConfigProto(allow_soft_placement=True) \n    config.gpu_options.allow_growth = True\t\n    with tf.Session(config=config) as session:\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        average_loss = 0\n        for step in range(num_steps):\n\n            batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size, is_source=False)\n            feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n            average_loss += l\n            if (step + 1) % 2000 == 0:\n                if step > 0:\n                    average_loss = average_loss / 2000\n                    # The average loss is an estimate of the loss over the last 2000 batches.\n                print(\'Average loss at step %d: %f\' % (step + 1, average_loss))\n                average_loss = 0\n            # note that this is expensive (~20% slowdown if computed every 500 steps)\n            if (step + 1) % 10000 == 0:\n                sim = similarity.eval()\n                for i in range(valid_size):\n                    valid_word = tgt_reverse_dictionary[valid_examples[i]]\n                    top_k = 8  # number of nearest neighbors\n                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                    log = \'Nearest to %s:\' % valid_word\n                    for k in range(top_k):\n                        close_word = tgt_reverse_dictionary[nearest[k]]\n                        log = \'%s %s,\' % (log, close_word)\n                    print(log)\n        cbow_final_embeddings = normalized_embeddings.eval()\n\n    np.save(\'en-embeddings.npy\', cbow_final_embeddings)'"
ch8/word2vec.py,19,"b'import numpy as np\nimport tensorflow as tf\nimport collections\nimport random\nimport math\n\ndata_indices = None\ndata_list = None\nreverse_dictionary = None\nembedding_size = None\nvocabulary_size = None\nnum_files = None\ndef define_data_and_hyperparameters(_num_files,_data_list, _reverse_dictionary, _emb_size, _vocab_size):\n    global num_files, data_indices, data_list, reverse_dictionary\n    global embedding_size, vocabulary_size\n    \n    num_files = _num_files\n    data_indices = [0 for _ in range(num_files)]\n    data_list = _data_list\n    reverse_dictionary = _reverse_dictionary\n    embedding_size = _emb_size\n    vocabulary_size = _vocab_size\n\t\n\ndef generate_batch_for_word2vec(data_list, doc_id, batch_size, window_size):\n    # window_size is the amount of words we\'re looking at from each side of a given word\n    # creates a single batch\n    # doc_id is the ID of the story we want to extract a batch from\n    \n    # data_indices[doc_id] is updated by 1 everytime we read a set of data point\n    # from the document identified by doc_id\n    global data_indices\n\n    # span defines the total window size, where\n    # data we consider at an instance looks as follows. \n    # [ skip_window target skip_window ]\n    # e.g if skip_window = 2 then span = 5\n    span = 2 * window_size + 1 \n\n    # two numpy arras to hold target words (batch)\n    # and context words (labels)\n    # Note that batch has span-1=2*window_size columns\n    batch = np.ndarray(shape=(batch_size,span-1), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    \n    # The buffer holds the data contained within the span\n    buffer = collections.deque(maxlen=span)\n\n    # Fill the buffer and update the data_index\n    for _ in range(span):\n        buffer.append(data_list[doc_id][data_indices[doc_id]])\n        data_indices[doc_id] = (data_indices[doc_id] + 1) % len(data_list[doc_id])\n\n    # Here we do the batch reading\n    # We iterate through each batch index\n    # For each batch index, we iterate through span elements\n    # to fill in the columns of batch array\n    for i in range(batch_size):\n        target = window_size  # target label at the center of the buffer\n        target_to_avoid = [ window_size ] # we only need to know the words around a given word, not the word itself\n\n        # add selected target to avoid_list for next time\n        col_idx = 0\n        for j in range(span):\n            # ignore the target word when creating the batch\n            if j==span//2:\n                continue\n            batch[i,col_idx] = buffer[j] \n            col_idx += 1\n        labels[i, 0] = buffer[target]\n\n        # Everytime we read a data point,\n        # we need to move the span by 1\n        # to update the span\n        buffer.append(data_list[doc_id][data_indices[doc_id]])\n        data_indices[doc_id] = (data_indices[doc_id] + 1) % len(data_list[doc_id])\n\n    assert batch.shape[0]==batch_size and batch.shape[1]== span-1\n    return batch, labels\n\ndef print_some_batches():\n    global num_files, data_list, reverse_dictionary\n    \n    for window_size in [1,2]:\n\t    data_indices = [0 for _ in range(num_files)]\n\t    batch, labels = generate_batch_for_word2vec(data_list, doc_id=0, batch_size=8, window_size=window_size)\n\t    print(\'\\nwith window_size = %d:\' % (window_size))\n\t    print(\'    batch:\', [[reverse_dictionary[bii] for bii in bi] for bi in batch])\n\t    print(\'    labels:\', [reverse_dictionary[li] for li in labels.reshape(8)])\n\t\t\nbatch_size, embedding_size, window_size = None, None, None\nvalid_size, valid_window, valid_examples = None, None, None\nnum_sampled = None\n\ntrain_dataset, train_labels = None, None\nvalid_dataset = None\n\nsoftmax_weights, softmax_biases = None, None\n\nloss, optimizer, similarity, normalized_embeddings = None, None, None, None\n\t\ndef define_word2vec_tensorflow():\n    global batch_size, embedding_size, window_size\n    global\tvalid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases \n    global loss, optimizer, similarity\n    global vocabulary_size, embedding_size\n    global normalized_embeddings\n\t\n    batch_size = 128 # Data points in a single batch\n    \n    # How many words to consider left and right.\n    # Skip gram by design does not require to have all the context words in a given step\n    # However, for CBOW that\'s a requirement, so we limit the window size\n    window_size = 3 \n    \n    # We pick a random validation set to sample nearest neighbors\n    valid_size = 16 # Random set of words to evaluate similarity on.\n    # We sample valid datapoints randomly from a large window without always being deterministic\n    valid_window = 50\n    \n    # When selecting valid examples, we select some of the most frequent words as well as\n    # some moderately rare words as well\n    valid_examples = np.array(random.sample(range(valid_window), valid_size))\n    valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n\n    num_sampled = 32 # Number of negative examples to sample.\n\n    tf.reset_default_graph()\n\n    # Training input data (target word IDs). Note that it has 2*window_size columns\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size,2*window_size])\n    # Training input label data (context word IDs)\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    # Validation input data, we don\'t need a placeholder\n    # as we have already defined the IDs of the words selected\n    # as validation data\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Variables.\n\n    # Embedding layer, contains the word embeddings\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0,dtype=tf.float32))\n\n    # Softmax Weights and Biases\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                 stddev=0.5 / math.sqrt(embedding_size),dtype=tf.float32))\n    softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n    \n    # Model.\n    # Look up embeddings for a batch of inputs.\n    # Here we do embedding lookups for each column in the input placeholder\n    # and then average them to produce an embedding_size word vector\n    stacked_embedings = None\n    print(\'Defining %d embedding lookups representing each word in the context\'%(2*window_size))\n    for i in range(2*window_size):\n        embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:,i])        \n        x_size,y_size = embedding_i.get_shape().as_list()\n        if stacked_embedings is None:\n            stacked_embedings = tf.reshape(embedding_i,[x_size,y_size,1])\n        else:\n            stacked_embedings = tf.concat(axis=2,values=[stacked_embedings,tf.reshape(embedding_i,[x_size,y_size,1])])\n\n    assert stacked_embedings.get_shape().as_list()[2]==2*window_size\n    print(""Stacked embedding size: %s""%stacked_embedings.get_shape().as_list())\n    mean_embeddings =  tf.reduce_mean(stacked_embedings,2,keepdims=False)\n    print(""Reduced mean embedding size: %s""%mean_embeddings.get_shape().as_list())\n    \n    \t\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    # inputs are embeddings of the train words\n    # with this loss we optimize weights, biases, embeddings\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    # AdamOptimizer.\n    optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n\t\n\ndef run_word2vec():\n    global batch_size, embedding_size, window_size\n    global\tvalid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases \n    global loss, optimizer, similarity, normalized_embeddings\n    global data_list, num_files, reverse_dictionary\n    global vocabulary_size, embedding_size\n\t\n    num_steps = 10\n    steps_per_doc = 100\n    \n    session = tf.InteractiveSession()\n    \n    # Initialize the variables in the graph\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    \n    average_loss = 0\n    \n    for step in range(num_steps):\n    \n        # Iterate through the documents in a random order\n        for doc_id in np.random.permutation(num_files):\n            for doc_step in range(steps_per_doc):\n                \n                # Generate a single batch of data from a document\n                batch_data, batch_labels = generate_batch_for_word2vec(data_list, doc_id, batch_size, window_size)\n                \n                # Populate the feed_dict and run the optimizer (minimize loss)\n                # and compute the loss\n                feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n                _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n                \n                average_loss += l\n            \n        if (step+1) % 1 == 0:\n            if step > 0:\n                # compute average loss\n                average_loss = average_loss / (doc_id*steps_per_doc)\n            \n            print(\'Average loss at step %d: %f\' % (step+1, average_loss))\n            average_loss = 0 # reset average loss\n\n        # Evaluating validation set word similarities\n        if (step+1) % 5 == 0:\n            sim = similarity.eval()\n        \n            # Here we compute the top_k closest words for a given validation word\n            # in terms of the cosine distance\n            # We do this for all the words in the validation set\n            # Note: This is an expensive step\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 4 # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n                log = \'Nearest to %s:\' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = \'%s %s,\' % (log, close_word)\n                print(log)\n    cbow_final_embeddings = normalized_embeddings.eval()\n\n    # We save the embeddings as embeddings.npy \n    np.save(\'embeddings\',cbow_final_embeddings)'"
ch9/correct_spellings.py,0,"b""from difflib import SequenceMatcher\n\ndef string_similarity(a, b):\n    return SequenceMatcher(None, a, b).ratio()\n\ndef correct_wrong_word(cw,gw,cap):\n    '''\n    Spelling correction logic\n    This is a very simple logic that replaces\n    words with incorrect spelling with the word that highest\n    similarity. Some words are manually corrected as the words\n    found to be most similar semantically did not match.\n    '''\n    correct_word = None\n    found_similar_word = False\n    sim = string_similarity(gw,cw)\n    if sim>0.9:\n        if cw != 'stting' and cw != 'sittign' and cw != 'smilling' and \\\n            cw!='skiies' and cw!='childi' and cw!='sittion' and cw!='peacefuly' and cw!='stainding' and\\\n            cw != 'staning' and cw!='lating' and cw!='sking' and cw!='trolly' and cw!='umping' and cw!='earing' and \\\n            cw !='baters' and cw !='talkes' and cw !='trowing' and cw !='convered' and cw !='onsie' and cw !='slying':\n            print(gw,' ',cw,' ',sim,' (',cap,')')\n            correct_word = gw\n            found_similar_word = True\n        elif cw == 'stting' or cw == 'sittign' or cw == 'sittion':\n            correct_word = 'sitting'\n            found_similar_word = True\n        elif cw == 'smilling':\n            correct_word = 'smiling'\n            found_similar_word = True\n        elif cw == 'skiies':\n            correct_word = 'skis'\n            found_similar_word = True\n        elif cw == 'childi':\n            correct_word = 'child'\n            found_similar_word = True\n        elif cw == 'peacefuly':\n            correct_word = 'peacefully'\n            found_similar_word = True\n        elif cw == 'stainding' or cw == 'staning':\n            correct_word = 'standing'\n            found_similar_word = True\n        elif cw == 'lating':\n            correct_word = 'laying'\n            found_similar_word = True\n        elif cw == 'sking':\n            correct_word = 'skiing'\n            found_similar_word = True\n        elif cw == 'trolly':\n            correct_word = 'trolley'\n            found_similar_word = True\n        elif cw == 'umping':\n            correct_word = 'jumping'\n            found_similar_word = True\n        elif cw == 'earing':\n            correct_word = 'eating'\n            found_similar_word = True\n        elif cw == 'baters':\n            correct_word = 'batters'\n            found_similar_word = True\n        elif cw == 'talkes':\n            correct_word = 'talks'\n            found_similar_word = True\n        elif cw == 'trowing':\n            correct_word = 'throwing'\n            found_similar_word = True\n        elif cw =='convered':\n            correct_word = 'covered'\n            found_similar_word = True\n        elif cw == 'onsie':\n            correct_word = cw\n            found_similar_word = True\n        elif cw =='slying':\n            correct_word = 'flying'\n            found_similar_word = True\n        else:\n            raise NotImplementedError\n    else:\n        correct_word = cw\n        found_similar_word = False\n        \n    return correct_word, found_similar_word"""
ch9/word2vec.py,19,"b'import numpy as np\nimport tensorflow as tf\nimport collections\nimport random\nimport math\nimport os\n\ntot_captions, only_captions = None, None\n\ndata_indices = None\nreverse_dictionary = None\nembedding_size = None\nvocabulary_size = None\nmax_caption_length = None\n\ndef define_data_and_hyperparameters(\n        _tot_captions, _only_captions, _reverse_dictionary,\n        _emb_size, _vocab_size, _max_cap_length):\n    global data_indices, tot_captions, only_captions, reverse_dictionary\n    global embedding_size, vocabulary_size, max_caption_length\n\n    tot_captions = _tot_captions\n    only_captions = _only_captions\n\n    data_indices = [0 for _ in range(tot_captions)]\n    reverse_dictionary = _reverse_dictionary\n    embedding_size = _emb_size\n    vocabulary_size = _vocab_size\n    max_caption_length = _max_cap_length\n\n\ndef generate_batch_for_word2vec(batch_size, window_size):\n    # window_size is the amount of words we\'re looking at from each side of a given word\n    # creates a single batch\n    global data_indices\n\n    span = 2 * window_size + 1  # [ skip_window target skip_window ]\n\n    batch = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    # e.g if skip_window = 2 then span = 5\n    # span is the length of the whole frame we are considering for a single word (left + word + right)\n    # skip_window is the length of one side\n\n    caption_ids_for_batch = np.random.randint(0, tot_captions, batch_size)\n\n    for b_i in range(batch_size):\n        cap_id = caption_ids_for_batch[b_i]\n\n        buffer = only_captions[cap_id, data_indices[cap_id]:data_indices[cap_id] + span]\n        assert buffer.size == span, \'Buffer length (%d), Current data index (%d), Span(%d)\' % (\n        buffer.size, data_indices[cap_id], span)\n        # If we only have EOS tokesn in the sampled text, we sample a new one\n        while np.all(buffer == 1):\n            # reset the data_indices for that cap_id\n            data_indices[cap_id] = 0\n            # sample a new cap_id\n            cap_id = np.random.randint(0, tot_captions)\n            buffer = only_captions[cap_id, data_indices[cap_id]:data_indices[cap_id] + span]\n\n        # fill left and right sides of batch\n        batch[b_i, :window_size] = buffer[:window_size]\n        batch[b_i, window_size:] = buffer[window_size + 1:]\n\n        labels[b_i, 0] = buffer[window_size]\n\n        # increase the corresponding index\n        data_indices[cap_id] = (data_indices[cap_id] + 1) % (max_caption_length - span)\n\n    assert batch.shape[0] == batch_size and batch.shape[1] == span - 1\n    return batch, labels\n\ndef print_some_batches():\n    global data_indices, reverse_dictionary\n\n    for w_size in [1, 2]:\n        data_indices = [0 for _ in range(tot_captions)]\n        batch, labels = generate_batch_for_word2vec(batch_size=8, window_size=w_size)\n        print(\'\\nwith window_size = %d:\' %w_size)\n        print(\'    batch:\', [[reverse_dictionary[bii] for bii in bi] for bi in batch])\n        print(\'    labels:\', [reverse_dictionary[li] for li in labels.reshape(8)])\n\t\t\nbatch_size, embedding_size, window_size = None, None, None\nvalid_size, valid_window, valid_examples = None, None, None\nnum_sampled = None\n\ntrain_dataset, train_labels = None, None\nvalid_dataset = None\n\nsoftmax_weights, softmax_biases = None, None\n\nloss, optimizer, similarity, normalized_embeddings = None, None, None, None\n\t\ndef define_word2vec_tensorflow(batch_size):\n    global embedding_size, window_size\n    global\tvalid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases \n    global loss, optimizer, similarity\n    global vocabulary_size, embedding_size\n    global normalized_embeddings\n    \n    # How many words to consider left and right.\n    # Skip gram by design does not require to have all the context words in a given step\n    # However, for CBOW that\'s a requirement, so we limit the window size\n    window_size = 3 \n    \n    # We pick a random validation set to sample nearest neighbors\n    valid_size = 16 # Random set of words to evaluate similarity on.\n    # We sample valid datapoints randomly from a large window without always being deterministic\n    valid_window = 50\n    \n    # When selecting valid examples, we select some of the most frequent words as well as\n    # some moderately rare words as well\n    valid_examples = np.array(random.sample(range(valid_window), valid_size))\n    valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n\n    num_sampled = 32 # Number of negative examples to sample.\n\n    tf.reset_default_graph()\n\n    # Training input data (target word IDs). Note that it has 2*window_size columns\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size,2*window_size])\n    # Training input label data (context word IDs)\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    # Validation input data, we don\'t need a placeholder\n    # as we have already defined the IDs of the words selected\n    # as validation data\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Variables.\n\n    # Embedding layer, contains the word embeddings\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0,dtype=tf.float32))\n\n    # Softmax Weights and Biases\n    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                 stddev=0.5 / math.sqrt(embedding_size),dtype=tf.float32))\n    softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],0.0,0.01))\n    \n    # Model.\n    # Look up embeddings for a batch of inputs.\n    # Here we do embedding lookups for each column in the input placeholder\n    # and then average them to produce an embedding_size word vector\n    stacked_embedings = None\n    print(\'Defining %d embedding lookups representing each word in the context\'%(2*window_size))\n    for i in range(2*window_size):\n        embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:,i])        \n        x_size,y_size = embedding_i.get_shape().as_list()\n        if stacked_embedings is None:\n            stacked_embedings = tf.reshape(embedding_i,[x_size,y_size,1])\n        else:\n            stacked_embedings = tf.concat(axis=2,values=[stacked_embedings,tf.reshape(embedding_i,[x_size,y_size,1])])\n\n    assert stacked_embedings.get_shape().as_list()[2]==2*window_size\n    print(""Stacked embedding size: %s""%stacked_embedings.get_shape().as_list())\n    mean_embeddings =  tf.reduce_mean(stacked_embedings,2,keepdims=False)\n    print(""Reduced mean embedding size: %s""%mean_embeddings.get_shape().as_list())\n    \n    \t\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    # inputs are embeddings of the train words\n    # with this loss we optimize weights, biases, embeddings\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=mean_embeddings,\n                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n    # AdamOptimizer.\n    optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n\t\n\ndef run_word2vec(batch_size):\n    global embedding_size, window_size\n    global valid_size, valid_window, valid_examples\n    global num_sampled\n    global train_dataset, train_labels\n    global valid_dataset\n    global softmax_weights, softmax_biases \n    global loss, optimizer, similarity, normalized_embeddings\n    global data_list, num_files, reverse_dictionary\n    global vocabulary_size, embedding_size\n\n    work_dir = \'image_caption_data\'\n    num_steps = 100001\n\n    session = tf.InteractiveSession()\n\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    average_loss = 0\n    for step in range(num_steps):\n\n        # Load a batch of data\n        batch_data, batch_labels = generate_batch_for_word2vec(batch_size, window_size)\n\n        # Populate the feed_dict and run the optimizer and get the loss out\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n\n        average_loss += l\n\n        if (step + 1) % 2000 == 0:\n            if step > 0:\n                # The average loss is an estimate of the loss over the last 2000 batches.\n                average_loss = average_loss / 2000\n\n            print(\'Average loss at step %d: %f\' % (step + 1, average_loss))\n            average_loss = 0 # Reset average loss\n\n        if (step + 1) % 10000 == 0:\n            sim = similarity.eval()\n            # Calculate the most similar (top_k) words\n            # to the previosly selected set of valid words\n            # Note that this is an expensive step\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 3  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = \'Nearest to %s:\' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = \'%s %s,\' % (log, close_word)\n                print(log)\n\n    # Get the normalized embeddings we learnt\n    cbow_final_embeddings = normalized_embeddings.eval()\n\n    # Save the embeddings to the disk as \'caption_embeddings-tmp.npy\'\n    # If you want to use this embeddings in the next steps\n    # please change the filename to \'caption-embeddings.npy\'\n    np.save(os.path.join(work_dir,\'caption-embeddings-tmp\'), cbow_final_embeddings)'"
