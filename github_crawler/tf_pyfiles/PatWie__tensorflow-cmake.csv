file_path,api_count,code
custom_op/example.py,4,"b'#!/usr/bin/env python\n# 2018, Patrick Wieschollek <mail@patwie.com>\n\nimport numpy as np\nimport tensorflow as tf\nfrom user_ops import matrix_add\n\nnp.random.seed(42)\ntf.set_random_seed(42)\n\nmatA = np.random.randn(1, 2, 3, 4).astype(np.float32) * 10\nmatB = np.random.randn(1, 2, 3, 4).astype(np.float32) * 10\n\n\nA = tf.placeholder(tf.float32, shape=[None, 2, 3, 4])\nB = tf.placeholder(tf.float32, shape=[None, 2, 3, 4])\n\nbias = 42.\n\nactual_op = matrix_add(A, B, bias)\n\n\nwith tf.Session() as sess:\n  print sess.run(actual_op, {A: matA, B: matB})\n'"
inference/example.py,11,"b'import tensorflow as tf\nimport numpy as np\n\nx = tf.placeholder(tf.float32, shape=[1, 2], name=\'input\')\noutput = tf.identity(tf.layers.dense(x, 1), name=\'output\')\n\nval = np.array([[1, 1]], dtype=np.float32)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # save graph\n    saver = tf.train.Saver(tf.global_variables())\n    saver.save(sess, \'./exported/my_model\')\n\n    tf.train.write_graph(sess.graph, \'.\', ""./exported/graph.pb"", as_text=False)\n    tf.train.write_graph(sess.graph, \'.\', ""./exported/graph.pb_txt"", as_text=True)\n\n    t1 = tf.get_default_graph().get_tensor_by_name(\'output:0\')\n    t2 = tf.get_default_graph().get_tensor_by_name(\'dense/kernel:0\')\n    t3 = tf.get_default_graph().get_tensor_by_name(\'dense/bias:0\')\n\n    t1, t2, t3, x = sess.run([t1, t2, t3, x], {x: val})\n\n    print(tf.global_variables())\n    print(""input           "", x)\n    print(""output          "", t1)\n    print(""dense/kernel:0  "", t2)\n    print(""dense/bias:0    "", t3)\n'"
custom_op/user_ops/__init__.py,1,"b'# 2018, Patrick Wieschollek <mail@patwie.com>\n\n# manually generated file\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.framework import ops\n\n__all__ = []\n\ndef load_op(name, has_grad=False):\n  """"""Load operation and add it to __all__ for imports.\n\n  Args:\n      name (str): name of operation without ""_op"" suffix\n      has_grad (bool, optional): gradient (if exists) should be loaded as well\n\n  Returns:\n      functions\n  """"""\n  global __all__\n  path = os.path.join(os.path.dirname(__file__), \'%s_op.so\' % name)\n  _module = tf.load_op_library(path)\n  if has_grad:\n    __all__.append(\'%s\' % name)\n    __all__.append(\'%s_grad\' % name)\n    return getattr(_module, \'%s\' % name), getattr(_module, \'%s_grad\' % name)\n  else:\n    __all__.append(\'%s\' % name)\n    return getattr(_module, \'%s\' % name)\n\n\nmatrix_add, matrix_add_grad = load_op(\'matrix_add\', has_grad=True)\n\n\n@ops.RegisterGradient(""MatrixAdd"")\ndef _MatrixAddGrad(op, *grads):\n  bias = op.get_attr(\'bias\')\n  matA = op.inputs[0]\n  matB = op.inputs[1]\n  # top = op.outputs[0]\n  topdiff = grads[0]\n  return matrix_add_grad(matA, matB, topdiff, bias=bias)\n'"
custom_op/user_ops/test_matrix_add.py,8,"b""#!/usr/bin/env python\n# 2018, Patrick Wieschollek <mail@patwie.com>\n\nimport numpy as np\nimport tensorflow as tf\nfrom __init__ import matrix_add\n\nnp.random.seed(42)\ntf.set_random_seed(42)\n\n\nclass MatrixAddtest(tf.test.TestCase):\n\n  def _forward(self, use_gpu=False, dtype=np.float32):\n    matA = np.random.randn(1, 2, 3, 4).astype(dtype) * 10\n    matB = np.random.randn(1, 2, 3, 4).astype(dtype) * 10\n    bias = dtype(42.)\n\n    expected = matA + matB + bias\n\n    matA_op = tf.convert_to_tensor(matA)\n    matB_op = tf.convert_to_tensor(matB)\n\n    with self.test_session(use_gpu=use_gpu, force_gpu=use_gpu) as sess:\n      actual_op = matrix_add(matA_op, matB_op, bias)\n      actual = sess.run(actual_op)\n\n    self.assertShapeEqual(expected, actual_op)\n    self.assertAllClose(expected, actual)\n\n  def test_forward_int32(self):\n    self._forward(use_gpu=False, dtype=np.int32)\n    self._forward(use_gpu=True, dtype=np.int32)\n\n  def test_forward_uint32(self):\n    self._forward(use_gpu=False, dtype=np.uint32)\n    self._forward(use_gpu=True, dtype=np.uint32)\n\n  def test_forward_float(self):\n    self._forward(use_gpu=False, dtype=np.float32)\n    self._forward(use_gpu=True, dtype=np.float32)\n\n  def test_forward_double(self):\n    self._forward(use_gpu=False, dtype=np.float64)\n    self._forward(use_gpu=True, dtype=np.float64)\n\n  def _backward(self, use_gpu=False, dtype=np.float32):\n    matA = np.random.randn(1, 2, 3, 4).astype(dtype) * 10\n    matB = np.random.randn(1, 2, 3, 4).astype(dtype) * 10\n    bias = 42.\n\n    expected = (matA + matB + bias).astype(np.float32)\n\n    matA_op = tf.convert_to_tensor(matA)\n    matB_op = tf.convert_to_tensor(matB)\n\n    with self.test_session(use_gpu=use_gpu, force_gpu=use_gpu):\n      actual_op = matrix_add(matA_op, matB_op, bias)\n      err = tf.test.compute_gradient_error(\n          [matA_op, matB_op], [matA.shape, matB.shape],\n          actual_op, expected.shape)\n\n    self.assertLess(err, 1e-2)\n\n  def test_backward_float(self):\n    self._backward(use_gpu=False, dtype=np.float32)\n    self._backward(use_gpu=True, dtype=np.float32)\n\n  def test_backward_double(self):\n    self._backward(use_gpu=False, dtype=np.float64)\n    self._backward(use_gpu=True, dtype=np.float64)\n\n\nif __name__ == '__main__':\n  tf.test.main()\n"""
examples/keras/keras_graph.py,8,"b'import tensorflow as tf\nimport numpy as np\nsess = tf.Session()\n\nfrom keras import backend as K  # noqa\nK.set_session(sess)\n\n\nimg = tf.placeholder(tf.float32, shape=(None, 2), name=\'input_plhdr\')\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation=\'relu\', name=\'Intermediate\'),\n    tf.keras.layers.Dense(2, activation=\'softmax\', name=\'Output\'),\n])\n\nM = model(img)\nprint(\'input\', img.name)\nprint(\'output\', M.name)\nsess.run(tf.global_variables_initializer())\nprint(\'result\', sess.run(M, {img: np.array([[42, 43.]], dtype=np.float32)}))\n\nsaver = tf.train.Saver(tf.global_variables())\nsaver.save(sess, \'./exported/my_model\')\ntf.train.write_graph(sess.graph, \'.\', ""./exported/graph.pb"", as_text=False)\n'"
inference/python/inference.py,8,"b'import tensorflow as tf\nimport numpy as np\n\nval = np.array([[1, 1]], dtype=np.float32)\n\nwith tf.Session() as sess:\n\n    # load the computation graph\n    loader = tf.train.import_meta_graph(\'./exported/my_model.meta\')\n    sess.run(tf.global_variables_initializer())\n    loader = loader.restore(sess, \'./exported/my_model\')\n\n    x = tf.get_default_graph().get_tensor_by_name(\'input:0\')\n    t1 = tf.get_default_graph().get_tensor_by_name(\'output:0\')\n    t2 = tf.get_default_graph().get_tensor_by_name(\'dense/kernel:0\')\n    t3 = tf.get_default_graph().get_tensor_by_name(\'dense/bias:0\')\n\n    t1, t2, t3, x = sess.run([t1, t2, t3, x], {x: val})\n\n    print tf.global_variables()\n    print ""input           "", x\n    print ""output          "", t1\n    print ""dense/kernel:0  "", t2\n    print ""dense/bias:0    "", t3\n'"
inference/python/inference2.py,8,"b'import tensorflow as tf\nimport numpy as np\n\nval = np.array([[1, 1]], dtype=np.float32)\n\nwith tf.Session() as sess:\n\n    metaGraph = tf.train.import_meta_graph(\'./exported/my_model.meta\')\n    restore_op_name = metaGraph.as_saver_def().restore_op_name\n    restore_op = tf.get_default_graph().get_operation_by_name(restore_op_name)\n    filename_tensor_name = metaGraph.as_saver_def().filename_tensor_name\n    sess.run(restore_op, {filename_tensor_name: \'./exported/my_model\'})\n\n    x = tf.get_default_graph().get_tensor_by_name(\'input:0\')\n    t1 = tf.get_default_graph().get_tensor_by_name(\'output:0\')\n    t2 = tf.get_default_graph().get_tensor_by_name(\'dense/kernel:0\')\n    t3 = tf.get_default_graph().get_tensor_by_name(\'dense/bias:0\')\n\n    t1, t2, t3, x = sess.run([t1, t2, t3, x], {x: val})\n\n    print tf.global_variables()\n    print ""input           "", x\n    print ""output          "", t1\n    print ""dense/kernel:0  "", t2\n    print ""dense/bias:0    "", t3\n'"
serving/basic/client/client_grpc.py,1,"b'import tensorflow as tf\nimport grpc\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\n\n""""""\npip install tensorflow-serving-api --user\n""""""\n\ndata = 1.0\ntimeout = 5.0  # seconds\n\nchannel = grpc.insecure_channel(""localhost:8500"")\n\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\nrequest = predict_pb2.PredictRequest()\n\nrequest = predict_pb2.PredictRequest()\nrequest.model_spec.name = \'simple_model\'\nrequest.model_spec.signature_name = \'serving_default\'\n\nrequest.inputs[\'input\'].CopyFrom(\n    tf.contrib.util.make_tensor_proto([data], shape=[1]))\n\nprint(stub.Predict(request, timeout))\n'"
serving/basic/client/client_rest.py,0,"b'import requests\n\nurl = ""http://localhost:8501/v1/models/simple_model:predict""\n\npayload = {\'instances\': [1.0]}\nresponse = requests.post(url, json=payload)\n\nprint(response.text)\nprint(response.status_code, response.reason)\n'"
serving/basic/training/create.py,13,"b""import tensorflow as tf\n\nx = tf.placeholder(tf.float32)\nw = tf.get_variable('w', initializer=1.)\ny = tf.add(x, w, name='y')\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  print(sess.run(y, {x: 5}))\n\n  builder = tf.saved_model.builder.SavedModelBuilder('/tmp/simple_model/1')\n\n  tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n  tensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n\n  builder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map={\n          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n          tf.saved_model.signature_def_utils.build_signature_def(\n              inputs={'input': tensor_info_x},\n              outputs={'output': tensor_info_y},\n              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n      },\n      legacy_init_op=tf.group(tf.tables_initializer(), name='legacy_init_op'))\n\n  builder.save()\n"""
serving/image/client/client_grpc.py,1,"b'import tensorflow as tf\nimport grpc\nimport base64\nimport argparse\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\n\n\n""""""\npip install tensorflow-serving-api --user\n""""""\n\n\ndef main(fn):\n  channel = grpc.insecure_channel(""localhost:8500"")\n\n  stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n  request = predict_pb2.PredictRequest()\n\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = \'simple_img_model\'\n  request.model_spec.signature_name = \'serving_default\'\n\n  image_buffer = open(fn, ""rb"").read()\n\n  request.inputs[\'input_bytes:0\'].CopyFrom(\n      tf.contrib.util.make_tensor_proto([image_buffer], shape=[1]))\n\n  response = stub.Predict(request, timeout=5.0)\n  response_b64 = response.outputs[\'output_bytes:0\'].string_val[0]\n  response_buffer = base64.urlsafe_b64decode(response_b64)\n  open(\'prediction_grpc.png\', \'wb\').write(response_buffer)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'fn\', help=\'path to img\', type=str)\n  args = parser.parse_args()\n  main(args.fn)\n'"
serving/image/client/client_rest.py,0,"b'\nimport requests\nimport base64\nimport json\nimport argparse\n\n\ndef main(fn):\n  endpoint = ""http://localhost:8501/v1/models/simple_img_model:predict""\n\n  image_buffer = open(fn, ""rb"").read()\n  image_b64 = base64.b64encode(image_buffer).decode(""utf-8"")\n\n  headers = {""content-type"": ""application/json""}\n  data = json.dumps({\n      ""inputs"": [\n          {""b64"": image_b64}\n      ]\n  })\n\n  result = requests.post(endpoint, data=data, headers=headers)\n  response = json.loads(result.text)\n  response_b64 = response[\'outputs\'][0]\n  response_buffer = base64.urlsafe_b64decode(response_b64.encode(""utf-8""))\n  open(\'prediction_rest.png\', \'wb\').write(response_buffer)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'fn\', help=\'path to img\', type=str)\n  args = parser.parse_args()\n  main(args.fn)\n'"
serving/image/training/create.py,20,"b'import tensorflow as tf\nimport tensorflow.saved_model as sm\n\n\ndef pre_process_input(x):\n  """"""Prepare received inputs.\n\n  Remarks:\n    TensorFlow-Serving expects encoded image data, but the model\n    itself relies on decoded data. This function maps between both.\n\n  Args:\n      x (tf.tensor): A received unprocessed tensor.\n\n  Returns:\n      tf.tensor: A tensor, which is expected by the network itself.\n  """"""\n\n  # this is ugly, but fixe a few headaches\n  x = tf.reshape(x, [])\n  # decode the buffer as an image\n  x = tf.image.decode_image(x, channels=3, dtype=tf.float32)\n  # add the batch dimension and scale to [0, 1]\n  x = tf.expand_dims(x, axis=0) / 255.\n  return x\n\n\ndef post_process_output(y):\n  """"""Prepare outputs before sending them back.\n\n  Remarks:\n    The model might produce an image, but we need to deliver the image encoded,\n    such that a REST user can handle the response.\n\n  Todo:\n    In contrast to the official docs, the outputs has *not* been encoded to\n    base64 similar to the input. Therefore, we explicitly encode it here.\n\n  Args:\n      y (tf.tensor): A tensor, which is produced by our model.\n\n  Returns:\n      tf.tensor: A tensor, which is ready to be delivered as a response.\n  """"""\n\n  # remove batch dimension\n  y = tf.squeeze(y, axis=[0])\n  # clip to correct range\n  y = tf.clip_by_value(y, 0, 1) * 255.\n  # Convert image to dtype, scaling its values if needed (is needed!).\n  y = tf.image.convert_image_dtype(y, tf.uint8)\n  # output is png file\n  y = tf.image.encode_png(y)\n  # add batch-dimension back\n  y = tf.expand_dims(y, axis=0)\n\n  # NOTE: While the doc clearly states the output format of TF-Serving will\n  # match the input format, this is not the case. Hence, we explicitly encode\n  # the output. (`encode_base64` is like `urlsafe_b64encode`)\n  y = tf.io.encode_base64(y, pad=True)\n  return y\n\n\ndef model(x):\n  # so far only the identity\n  return x\n\n\n# the placeholder has to have the postfix ""_bytes"".\ninput_bytes = tf.placeholder(tf.string, shape=[], name=""input_bytes"")\ninput_img = pre_process_input(input_bytes)\nprediction = model(input_img)\noutput = post_process_output(prediction)\noutput_bytes = tf.identity(output, name=\'output_bytes\')\n\n\ndef build_signatures(*tensor_list):\n  signatures = {t.name: tf.saved_model.utils.build_tensor_info(\n      t) for t in tensor_list}\n  return signatures\n\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  builder = tf.saved_model.builder.SavedModelBuilder(\'/tmp/simple_img_model/1\')\n\n  builder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map={\n          sm.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n          sm.signature_def_utils.build_signature_def(\n              inputs=build_signatures(input_bytes),\n              outputs=build_signatures(output_bytes),\n              method_name=sm.signature_constants.PREDICT_METHOD_NAME)\n      })\n\n  builder.save()\n'"
