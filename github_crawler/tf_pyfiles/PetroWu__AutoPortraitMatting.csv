file_path,api_count,code
FCN.py,48,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\n\nimport TensorflowUtils as utils\n#import read_MITSceneParsingData as scene_parsing\nimport datetime\n#import BatchDatsetReader as dataset\nfrom portrait import BatchDatset, TestDataset\nfrom six.moves import xrange\nfrom PIL import Image\nfrom scipy import misc\n\nFLAGS = tf.flags.FLAGS\ntf.flags.DEFINE_integer(""batch_size"", ""2"", ""batch size for training"")\ntf.flags.DEFINE_string(""logs_dir"", ""logs/"", ""path to logs directory"")\ntf.flags.DEFINE_string(""data_dir"", ""Data_zoo/MIT_SceneParsing/"", ""path to dataset"")\ntf.flags.DEFINE_float(""learning_rate"", ""1e-4"", ""Learning rate for Adam Optimizer"")\ntf.flags.DEFINE_string(""model_dir"", ""Model_zoo/"", ""Path to vgg model mat"")\ntf.flags.DEFINE_bool(\'debug\', ""False"", ""Debug mode: True/ False"")\ntf.flags.DEFINE_string(\'mode\', ""train"", ""Mode train/ test/ visualize"")\n\nMODEL_URL = \'http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\'\n\nMAX_ITERATION = int(1e5 + 1)\nNUM_OF_CLASSESS = 2\nIMAGE_WIDTH = 600\nIMAGE_HEIGHT = 800\n\n\ndef vgg_net(weights, image):\n    layers = (\n        \'conv1_1\', \'relu1_1\', \'conv1_2\', \'relu1_2\', \'pool1\',\n\n        \'conv2_1\', \'relu2_1\', \'conv2_2\', \'relu2_2\', \'pool2\',\n\n        \'conv3_1\', \'relu3_1\', \'conv3_2\', \'relu3_2\', \'conv3_3\',\n        \'relu3_3\', \'conv3_4\', \'relu3_4\', \'pool3\',\n\n        \'conv4_1\', \'relu4_1\', \'conv4_2\', \'relu4_2\', \'conv4_3\',\n        \'relu4_3\', \'conv4_4\', \'relu4_4\', \'pool4\',\n\n        \'conv5_1\', \'relu5_1\', \'conv5_2\', \'relu5_2\', \'conv5_3\',\n        \'relu5_3\', \'conv5_4\', \'relu5_4\'\n    )\n\n    net = {}\n    current = image\n    for i, name in enumerate(layers):\n        if name in [\'conv3_4\', \'relu3_4\', \'conv4_4\', \'relu4_4\', \'conv5_4\', \'relu5_4\']:\n            continue\n        kind = name[:4]\n        if kind == \'conv\':\n            kernels, bias = weights[i][0][0][0][0]\n            # matconvnet: weights are [width, height, in_channels, out_channels]\n            # tensorflow: weights are [height, width, in_channels, out_channels]\n            kernels = utils.get_variable(np.transpose(kernels, (1, 0, 2, 3)), name=name + ""_w"")\n            bias = utils.get_variable(bias.reshape(-1), name=name + ""_b"")\n            current = utils.conv2d_basic(current, kernels, bias)\n        elif kind == \'relu\':\n            current = tf.nn.relu(current, name=name)\n            if FLAGS.debug:\n                utils.add_activation_summary(current)\n        elif kind == \'pool\':\n            current = utils.avg_pool_2x2(current)\n        net[name] = current\n\n    return net\n\n\ndef inference(image, keep_prob):\n    """"""\n    Semantic segmentation network definition\n    :param image: input image. Should have values in range 0-255\n    :param keep_prob:\n    :return:\n    """"""\n    print(""setting up vgg initialized conv layers ..."")\n    model_data = utils.get_model_data(FLAGS.model_dir, MODEL_URL)\n\n    mean = model_data[\'normalization\'][0][0][0]\n    mean_pixel = np.mean(mean, axis=(0, 1))\n\n    weights = np.squeeze(model_data[\'layers\'])\n\n    #processed_image = utils.process_image(image, mean_pixel)\n\n    with tf.variable_scope(""inference""):\n        image_net = vgg_net(weights, image)\n        conv_final_layer = image_net[""conv5_3""]\n\n        pool5 = utils.max_pool_2x2(conv_final_layer)\n\n        W6 = utils.weight_variable([7, 7, 512, 4096], name=""W6"")\n        b6 = utils.bias_variable([4096], name=""b6"")\n        conv6 = utils.conv2d_basic(pool5, W6, b6)\n        relu6 = tf.nn.relu(conv6, name=""relu6"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu6)\n        relu_dropout6 = tf.nn.dropout(relu6, keep_prob=keep_prob)\n\n        W7 = utils.weight_variable([1, 1, 4096, 4096], name=""W7"")\n        b7 = utils.bias_variable([4096], name=""b7"")\n        conv7 = utils.conv2d_basic(relu_dropout6, W7, b7)\n        relu7 = tf.nn.relu(conv7, name=""relu7"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu7)\n        relu_dropout7 = tf.nn.dropout(relu7, keep_prob=keep_prob)\n\n        W8 = utils.weight_variable([1, 1, 4096, NUM_OF_CLASSESS], name=""W8"")\n        b8 = utils.bias_variable([NUM_OF_CLASSESS], name=""b8"")\n        conv8 = utils.conv2d_basic(relu_dropout7, W8, b8)\n        # annotation_pred1 = tf.argmax(conv8, dimension=3, name=""prediction1"")\n\n        # now to upscale to actual image size\n        deconv_shape1 = image_net[""pool4""].get_shape()\n        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, NUM_OF_CLASSESS], name=""W_t1"")\n        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=""b_t1"")\n        conv_t1 = utils.conv2d_transpose_strided(conv8, W_t1, b_t1, output_shape=tf.shape(image_net[""pool4""]))\n        fuse_1 = tf.add(conv_t1, image_net[""pool4""], name=""fuse_1"")\n\n        deconv_shape2 = image_net[""pool3""].get_shape()\n        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=""W_t2"")\n        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=""b_t2"")\n        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(image_net[""pool3""]))\n        fuse_2 = tf.add(conv_t2, image_net[""pool3""], name=""fuse_2"")\n\n        shape = tf.shape(image)\n        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])\n        W_t3 = utils.weight_variable([16, 16, NUM_OF_CLASSESS, deconv_shape2[3].value], name=""W_t3"")\n        b_t3 = utils.bias_variable([NUM_OF_CLASSESS], name=""b_t3"")\n        conv_t3 = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=8)\n\n        annotation_pred = tf.argmax(conv_t3, dimension=3, name=""prediction"")\n\n    return tf.expand_dims(annotation_pred, dim=3), conv_t3\n\n\ndef train(loss_val, var_list):\n    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n    if FLAGS.debug:\n        # print(len(var_list))\n        for grad, var in grads:\n            utils.add_gradient_summary(grad, var)\n    return optimizer.apply_gradients(grads)\n\n\ndef main(argv=None):\n    keep_probability = tf.placeholder(tf.float32, name=""keep_probabilty"")\n    image = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 3], name=""input_image"")\n    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 1], name=""annotation"")\n\n    pred_annotation, logits = inference(image, keep_probability)\n    tf.summary.image(""input_image"", image, max_outputs=2)\n    tf.summary.image(""ground_truth"", tf.cast(annotation, tf.uint8), max_outputs=2)\n    tf.summary.image(""pred_annotation"", tf.cast(pred_annotation, tf.uint8), max_outputs=2)\n    loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n                                                                          labels=tf.squeeze(annotation, squeeze_dims=[3]),\n                                                                          name=""entropy"")))\n    tf.summary.scalar(""entropy"", loss)\n\n    trainable_var = tf.trainable_variables()\n    if FLAGS.debug:\n        for var in trainable_var:\n            utils.add_to_regularization_and_summary(var)\n    train_op = train(loss, trainable_var)\n\n    print(""Setting up summary op..."")\n    summary_op = tf.summary.merge_all()\n\n    \'\'\'\n    print(""Setting up image reader..."")\n    train_records, valid_records = scene_parsing.read_dataset(FLAGS.data_dir)\n    print(len(train_records))\n    print(len(valid_records))\n\n    print(""Setting up dataset reader"")\n    image_options = {\'resize\': True, \'resize_size\': IMAGE_SIZE}\n    if FLAGS.mode == \'train\':\n        train_dataset_reader = dataset.BatchDatset(train_records, image_options)\n    validation_dataset_reader = dataset.BatchDatset(valid_records, image_options)\n    \'\'\'\n    train_dataset_reader = BatchDatset(\'data/trainlist.mat\')\n\n    sess = tf.Session()\n\n    print(""Setting up Saver..."")\n    saver = tf.train.Saver()\n    summary_writer = tf.summary.FileWriter(FLAGS.logs_dir, sess.graph)\n\n    sess.run(tf.global_variables_initializer())\n    ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        saver.restore(sess, ckpt.model_checkpoint_path)\n        print(""Model restored..."")\n\n    #if FLAGS.mode == ""train"":\n    itr = 0\n    train_images, train_annotations = train_dataset_reader.next_batch()\n    while len(train_annotations) > 0:\n        #train_images, train_annotations = train_dataset_reader.next_batch(FLAGS.batch_size)\n        #print(\'==> batch data: \', train_images[0][100][100], \'===\', train_annotations[0][100][100])\n        feed_dict = {image: train_images, annotation: train_annotations, keep_probability: 0.5}\n\n        sess.run(train_op, feed_dict=feed_dict)\n\n        if itr % 100 == 0:\n            train_loss, summary_str, rpred = sess.run([loss, summary_op, pred_annotation], feed_dict=feed_dict)\n            print(""Step: %d, Train_loss:%g"" % (itr, train_loss))\n            summary_writer.add_summary(summary_str, itr)\n            print(np.sum(rpred))\n            print(\'=============\')\n            print(np.sum(train_annotations))\n            print(\'------------>>>\')\n\n        #if itr % 10000 == 0 and itr > 0:\n        \'\'\'\n        valid_images, valid_annotations = validation_dataset_reader.next_batch(FLAGS.batch_size)\n        valid_loss = sess.run(loss, feed_dict={image: valid_images, annotation: valid_annotations,\n                                                       keep_probability: 1.0})\n        print(""%s ---> Validation_loss: %g"" % (datetime.datetime.now(), valid_loss))\'\'\'\n\n        itr += 1\n        train_images, train_annotations = train_dataset_reader.next_batch()\n\n    saver.save(sess, FLAGS.logs_dir + ""model.ckpt"", itr)\n\n    \'\'\'elif FLAGS.mode == ""visualize"":\n        valid_images, valid_annotations = validation_dataset_reader.get_random_batch(FLAGS.batch_size)\n        pred = sess.run(pred_annotation, feed_dict={image: valid_images, annotation: valid_annotations,\n                                                    keep_probability: 1.0})\n        valid_annotations = np.squeeze(valid_annotations, axis=3)\n        pred = np.squeeze(pred, axis=3)\n\n        for itr in range(FLAGS.batch_size):\n            utils.save_image(valid_images[itr].astype(np.uint8), FLAGS.logs_dir, name=""inp_"" + str(5+itr))\n            utils.save_image(valid_annotations[itr].astype(np.uint8), FLAGS.logs_dir, name=""gt_"" + str(5+itr))\n            utils.save_image(pred[itr].astype(np.uint8), FLAGS.logs_dir, name=""pred_"" + str(5+itr))\n            print(""Saved image: %d"" % itr)\'\'\'\n\n\ndef pred():\n    keep_probability = tf.placeholder(tf.float32, name=""keep_probabilty"")\n    image = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 3], name=""input_image"")\n    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 1], name=""annotation"")\n\n    pred_annotation, logits = inference(image, keep_probability)\n    test_dataset_reader = TestDataset(\'data/testlist.mat\')\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n        saver = tf.train.Saver()\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            print(""Model restored..."")\n        itr = 0\n        test_images, test_annotations, test_orgs = test_dataset_reader.next_batch()\n        #print(\'getting\', test_annotations[0, 200:210, 200:210])\n        if len(test_annotations) > 0:\n            feed_dict = {image: test_images, annotation: test_annotations, keep_probability: 0.5}\n            preds = sess.run(pred_annotation, feed_dict=feed_dict)\n            org0_im = Image.fromarray(np.uint8(test_orgs[0]))\n            org0_im.save(\'res/org0.jpg\')\n            org1_im = Image.fromarray(np.uint8(test_orgs[1]))\n            org1_im.save(\'res/org1.jpg\')\n            save_alpha_img(test_orgs[0], test_annotations[0], \'res/ann0\')\n            save_alpha_img(test_orgs[1], test_annotations[1], \'res/ann1\')\n            save_alpha_img(test_orgs[0], preds[0], \'res/pre0\')\n            save_alpha_img(test_orgs[1], preds[1], \'res/pre1\')\n\ndef save_alpha_img(org, mat, name):\n    w, h, _ = mat.shape\n    #print(mat[200:210, 200:210])\n    rmat = np.reshape(mat, (w, h))\n    amat = np.zeros((w, h, 4), dtype=np.int)\n    amat[:, :, 3] = rmat * 1000\n    amat[:, :, 0:3] = org\n    print(amat[200:205, 200:205])\n    #im = Image.fromarray(np.uint8(amat))\n    #im.save(name + \'.png\')\n    misc.imsave(name + \'.png\', amat)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n    #pred()\n'"
FCN_plus.py,49,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\n\nimport TensorflowUtils_plus as utils\n#import read_MITSceneParsingData as scene_parsing\nimport datetime\n#import BatchDatsetReader as dataset\nfrom portrait_plus import BatchDatset, TestDataset\nfrom PIL import Image\nfrom six.moves import xrange\nfrom scipy import misc\n\nFLAGS = tf.flags.FLAGS\ntf.flags.DEFINE_integer(""batch_size"", ""5"", ""batch size for training"")\ntf.flags.DEFINE_string(""logs_dir"", ""logs/"", ""path to logs directory"")\ntf.flags.DEFINE_string(""data_dir"", ""Data_zoo/MIT_SceneParsing/"", ""path to dataset"")\ntf.flags.DEFINE_float(""learning_rate"", ""1e-4"", ""Learning rate for Adam Optimizer"")\ntf.flags.DEFINE_string(""model_dir"", ""Model_zoo/"", ""Path to vgg model mat"")\ntf.flags.DEFINE_bool(\'debug\', ""False"", ""Debug mode: True/ False"")\ntf.flags.DEFINE_string(\'mode\', ""train"", ""Mode train/ test/ visualize"")\n\nMODEL_URL = \'http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\'\n\nMAX_ITERATION = int(1e5 + 1)\nNUM_OF_CLASSESS = 2\nIMAGE_WIDTH = 600\nIMAGE_HEIGHT = 800\n\n\ndef vgg_net(weights, image):\n    layers = (\n        \'conv1_1\', \'relu1_1\', \'conv1_2\', \'relu1_2\', \'pool1\',\n\n        \'conv2_1\', \'relu2_1\', \'conv2_2\', \'relu2_2\', \'pool2\',\n\n        \'conv3_1\', \'relu3_1\', \'conv3_2\', \'relu3_2\', \'conv3_3\',\n        \'relu3_3\', \'conv3_4\', \'relu3_4\', \'pool3\',\n\n        \'conv4_1\', \'relu4_1\', \'conv4_2\', \'relu4_2\', \'conv4_3\',\n        \'relu4_3\', \'conv4_4\', \'relu4_4\', \'pool4\',\n\n        \'conv5_1\', \'relu5_1\', \'conv5_2\', \'relu5_2\', \'conv5_3\',\n        \'relu5_3\', \'conv5_4\', \'relu5_4\'\n    )\n\n    net = {}\n    current = image\n    for i, name in enumerate(layers):\n        if name in [\'conv3_4\', \'relu3_4\', \'conv4_4\', \'relu4_4\', \'conv5_4\', \'relu5_4\']:\n            continue\n        kind = name[:4]\n        if kind == \'conv\':\n            kernels, bias = weights[i][0][0][0][0]\n            # matconvnet: weights are [width, height, in_channels, out_channels]\n            # tensorflow: weights are [height, width, in_channels, out_channels]\n            kernels = utils.get_variable(np.transpose(kernels, (1, 0, 2, 3)), name=name + ""_w"")\n            bias = utils.get_variable(bias.reshape(-1), name=name + ""_b"")\n            current = utils.conv2d_basic(current, kernels, bias)\n        elif kind == \'relu\':\n            current = tf.nn.relu(current, name=name)\n            if FLAGS.debug:\n                utils.add_activation_summary(current)\n        elif kind == \'pool\':\n            current = utils.avg_pool_2x2(current)\n        net[name] = current\n\n    return net\n\n\ndef inference(image, keep_prob):\n    """"""\n    Semantic segmentation network definition\n    :param image: input image. Should have values in range 0-255\n    :param keep_prob:\n    :return:\n    """"""\n    print(""setting up vgg initialized conv layers ..."")\n    model_data = utils.get_model_data(FLAGS.model_dir, MODEL_URL)\n\n    mean = model_data[\'normalization\'][0][0][0]\n    mean_pixel = np.mean(mean, axis=(0, 1))\n\n    weights = np.squeeze(model_data[\'layers\'])\n\n    #processed_image = utils.process_image(image, mean_pixel)\n\n    with tf.variable_scope(""inference""):\n        image_net = vgg_net(weights, image)\n        conv_final_layer = image_net[""conv5_3""]\n\n        pool5 = utils.max_pool_2x2(conv_final_layer)\n\n        W6 = utils.weight_variable([7, 7, 512, 4096], name=""W6"")\n        b6 = utils.bias_variable([4096], name=""b6"")\n        conv6 = utils.conv2d_basic(pool5, W6, b6)\n        relu6 = tf.nn.relu(conv6, name=""relu6"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu6)\n        relu_dropout6 = tf.nn.dropout(relu6, keep_prob=keep_prob)\n\n        W7 = utils.weight_variable([1, 1, 4096, 4096], name=""W7"")\n        b7 = utils.bias_variable([4096], name=""b7"")\n        conv7 = utils.conv2d_basic(relu_dropout6, W7, b7)\n        relu7 = tf.nn.relu(conv7, name=""relu7"")\n        if FLAGS.debug:\n            utils.add_activation_summary(relu7)\n        relu_dropout7 = tf.nn.dropout(relu7, keep_prob=keep_prob)\n\n        W8 = utils.weight_variable([1, 1, 4096, NUM_OF_CLASSESS], name=""W8"")\n        b8 = utils.bias_variable([NUM_OF_CLASSESS], name=""b8"")\n        conv8 = utils.conv2d_basic(relu_dropout7, W8, b8)\n        # annotation_pred1 = tf.argmax(conv8, dimension=3, name=""prediction1"")\n\n        # now to upscale to actual image size\n        deconv_shape1 = image_net[""pool4""].get_shape()\n        W_t1 = utils.weight_variable([4, 4, deconv_shape1[3].value, NUM_OF_CLASSESS], name=""W_t1"")\n        b_t1 = utils.bias_variable([deconv_shape1[3].value], name=""b_t1"")\n        conv_t1 = utils.conv2d_transpose_strided(conv8, W_t1, b_t1, output_shape=tf.shape(image_net[""pool4""]))\n        fuse_1 = tf.add(conv_t1, image_net[""pool4""], name=""fuse_1"")\n\n        deconv_shape2 = image_net[""pool3""].get_shape()\n        W_t2 = utils.weight_variable([4, 4, deconv_shape2[3].value, deconv_shape1[3].value], name=""W_t2"")\n        b_t2 = utils.bias_variable([deconv_shape2[3].value], name=""b_t2"")\n        conv_t2 = utils.conv2d_transpose_strided(fuse_1, W_t2, b_t2, output_shape=tf.shape(image_net[""pool3""]))\n        fuse_2 = tf.add(conv_t2, image_net[""pool3""], name=""fuse_2"")\n\n        shape = tf.shape(image)\n        deconv_shape3 = tf.stack([shape[0], shape[1], shape[2], NUM_OF_CLASSESS])\n        W_t3 = utils.weight_variable([16, 16, NUM_OF_CLASSESS, deconv_shape2[3].value], name=""W_t3"")\n        b_t3 = utils.bias_variable([NUM_OF_CLASSESS], name=""b_t3"")\n        conv_t3 = utils.conv2d_transpose_strided(fuse_2, W_t3, b_t3, output_shape=deconv_shape3, stride=8)\n\n        annotation_pred = tf.argmax(conv_t3, dimension=3, name=""prediction"")\n\n    return tf.expand_dims(annotation_pred, dim=3), conv_t3\n\n\ndef train(loss_val, var_list):\n    optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n    if FLAGS.debug:\n        # print(len(var_list))\n        for grad, var in grads:\n            utils.add_gradient_summary(grad, var)\n    return optimizer.apply_gradients(grads)\n\n\ndef main(argv=None):\n    keep_probability = tf.placeholder(tf.float32, name=""keep_probabilty"")\n    image = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 6], name=""input_image"")\n    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 1], name=""annotation"")\n\n    pred_annotation, logits = inference(image, keep_probability)\n    #tf.image_summary(""input_image"", image, max_images=2)\n    #tf.image_summary(""ground_truth"", tf.cast(annotation, tf.uint8), max_images=2)\n    #tf.image_summary(""pred_annotation"", tf.cast(pred_annotation, tf.uint8), max_images=2)\n    loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                          tf.squeeze(annotation, squeeze_dims=[3]),\n                                                                          name=""entropy"")))\n    #tf.scalar_summary(""entropy"", loss)\n\n    trainable_var = tf.trainable_variables()\n    train_op = train(loss, trainable_var)\n\n    #print(""Setting up summary op..."")\n    #summary_op = tf.merge_all_summaries()\n\n    \'\'\'\n    print(""Setting up image reader..."")\n    train_records, valid_records = scene_parsing.read_dataset(FLAGS.data_dir)\n    print(len(train_records))\n    print(len(valid_records))\n\n    print(""Setting up dataset reader"")\n    image_options = {\'resize\': True, \'resize_size\': IMAGE_SIZE}\n    if FLAGS.mode == \'train\':\n        train_dataset_reader = dataset.BatchDatset(train_records, image_options)\n    validation_dataset_reader = dataset.BatchDatset(valid_records, image_options)\n    \'\'\'\n    train_dataset_reader = BatchDatset(\'data/trainlist.mat\')\n\n    sess = tf.Session()\n\n    print(""Setting up Saver..."")\n    saver = tf.train.Saver()\n    #summary_writer = tf.train.SummaryWriter(FLAGS.logs_dir, sess.graph)\n\n    sess.run(tf.initialize_all_variables())\n    ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n        saver.restore(sess, ckpt.model_checkpoint_path)\n        print(""Model restored..."")\n\n    #if FLAGS.mode == ""train"":\n    itr = 0\n    train_images, train_annotations = train_dataset_reader.next_batch()\n    trloss = 0.0\n    while len(train_annotations) > 0:\n        #train_images, train_annotations = train_dataset_reader.next_batch(FLAGS.batch_size)\n        #print(\'==> batch data: \', train_images[0][100][100], \'===\', train_annotations[0][100][100])\n        feed_dict = {image: train_images, annotation: train_annotations, keep_probability: 0.5}\n        _, rloss =  sess.run([train_op, loss], feed_dict=feed_dict)\n        trloss += rloss\n\n        if itr % 100 == 0:\n            #train_loss, rpred = sess.run([loss, pred_annotation], feed_dict=feed_dict)\n            print(""Step: %d, Train_loss:%f"" % (itr, trloss / 100))\n            trloss = 0.0\n            #summary_writer.add_summary(summary_str, itr)\n\n        #if itr % 10000 == 0 and itr > 0:\n        \'\'\'\n        valid_images, valid_annotations = validation_dataset_reader.next_batch(FLAGS.batch_size)\n        valid_loss = sess.run(loss, feed_dict={image: valid_images, annotation: valid_annotations,\n                                                       keep_probability: 1.0})\n        print(""%s ---> Validation_loss: %g"" % (datetime.datetime.now(), valid_loss))\'\'\'\n        itr += 1\n\n        train_images, train_annotations = train_dataset_reader.next_batch()\n    saver.save(sess, FLAGS.logs_dir + ""plus_model.ckpt"", itr)\n\n    \'\'\'elif FLAGS.mode == ""visualize"":\n        valid_images, valid_annotations = validation_dataset_reader.get_random_batch(FLAGS.batch_size)\n        pred = sess.run(pred_annotation, feed_dict={image: valid_images, annotation: valid_annotations,\n                                                    keep_probability: 1.0})\n        valid_annotations = np.squeeze(valid_annotations, axis=3)\n        pred = np.squeeze(pred, axis=3)\n\n        for itr in range(FLAGS.batch_size):\n            utils.save_image(valid_images[itr].astype(np.uint8), FLAGS.logs_dir, name=""inp_"" + str(5+itr))\n            utils.save_image(valid_annotations[itr].astype(np.uint8), FLAGS.logs_dir, name=""gt_"" + str(5+itr))\n            utils.save_image(pred[itr].astype(np.uint8), FLAGS.logs_dir, name=""pred_"" + str(5+itr))\n            print(""Saved image: %d"" % itr)\'\'\'\n\ndef pred():\n    keep_probability = tf.placeholder(tf.float32, name=""keep_probabilty"")\n    image = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 6], name=""input_image"")\n    annotation = tf.placeholder(tf.int32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, 1], name=""annotation"")\n\n    pred_annotation, logits = inference(image, keep_probability)\n    sft = tf.nn.softmax(logits)\n    test_dataset_reader = TestDataset(\'data/testlist.mat\')\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        ckpt = tf.train.get_checkpoint_state(FLAGS.logs_dir)\n        saver = tf.train.Saver()\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n            print(""Model restored..."")\n        itr = 0\n        test_images, test_annotations, test_orgs = test_dataset_reader.next_batch()\n        #print(\'getting\', test_annotations[0, 200:210, 200:210])\n        while len(test_annotations) > 0:\n            if itr < 22:\n                test_images, test_annotations, test_orgs = test_dataset_reader.next_batch()\n                itr += 1\n                continue\n            elif itr > 22:\n                break\n            feed_dict = {image: test_images, annotation: test_annotations, keep_probability: 0.5}\n            rsft, pred_ann = sess.run([sft, pred_annotation], feed_dict=feed_dict)\n            print(rsft.shape)\n            _, h, w, _ = rsft.shape\n            preds = np.zeros((h, w, 1), dtype=np.float)\n            for i in range(h):\n                for j in range(w):\n                    if rsft[0][i][j][0] < 0.1:\n                        preds[i][j][0] = 1.0\n                    elif rsft[0][i][j][0] < 0.9:\n                        preds[i][j][0] = 0.5\n                    else:\n                        preds[i][j]  = 0.0\n            org0_im = Image.fromarray(np.uint8(test_orgs[0]))\n            org0_im.save(\'res/org\' + str(itr) + \'.jpg\')\n            save_alpha_img(test_orgs[0], test_annotations[0], \'res/ann\' + str(itr))\n            save_alpha_img(test_orgs[0], preds, \'res/trimap\' + str(itr))\n            save_alpha_img(test_orgs[0], pred_ann[0], \'res/pre\' + str(itr))\n            test_images, test_annotations, test_orgs = test_dataset_reader.next_batch()\n            itr += 1\n\ndef save_alpha_img(org, mat, name):\n    w, h = mat.shape[0], mat.shape[1]\n    #print(mat[200:210, 200:210])\n    rmat = np.reshape(mat, (w, h))\n    amat = np.zeros((w, h, 4), dtype=np.int)\n    amat[:, :, 3] = np.round(rmat * 1000)\n    amat[:, :, 0:3] = org\n    #print(amat[200:205, 200:205])\n    #im = Image.fromarray(np.uint8(amat))\n    #im.save(name + \'.png\')\n    misc.imsave(name + \'.png\', amat)\n\nif __name__ == ""__main__"":\n    #tf.app.run()\n    pred()\n'"
TensorflowUtils.py,43,"b'__author__ = \'Charlie\'\n# Utils used with tensorflow implemetation\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc as misc\nimport os, sys\nfrom six.moves import urllib\nimport tarfile\nimport zipfile\nimport scipy.io\n\n\ndef get_model_data(dir_path, model_url):\n    maybe_download_and_extract(dir_path, model_url)\n    filename = model_url.split(""/"")[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        raise IOError(""VGG Model not found!"")\n    data = scipy.io.loadmat(filepath)\n    return data\n\n\ndef maybe_download_and_extract(dir_path, url_name, is_tarfile=False, is_zipfile=False):\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    filename = url_name.split(\'/\')[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\n                \'\\r>> Downloading %s %.1f%%\' % (filename, float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n\n        filepath, _ = urllib.request.urlretrieve(url_name, filepath, reporthook=_progress)\n        print()\n        statinfo = os.stat(filepath)\n        print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n        if is_tarfile:\n            tarfile.open(filepath, \'r:gz\').extractall(dir_path)\n        elif is_zipfile:\n            with zipfile.ZipFile(filepath) as zf:\n                zip_dir = zf.namelist()[0]\n                zf.extractall(dir_path)\n\n\ndef save_image(image, save_dir, name, mean=None):\n    """"""\n    Save image by unprocessing if mean given else just save\n    :param mean:\n    :param image:\n    :param save_dir:\n    :param name:\n    :return:\n    """"""\n    if mean:\n        image = unprocess_image(image, mean)\n    misc.imsave(os.path.join(save_dir, name + "".png""), image)\n\n\ndef get_variable(weights, name):\n    init = tf.constant_initializer(weights, dtype=tf.float32)\n    var = tf.get_variable(name=name, initializer=init,  shape=weights.shape)\n    return var\n\n\ndef weight_variable(shape, stddev=0.02, name=None):\n    # print(shape)\n    initial = tf.truncated_normal(shape, stddev=stddev)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef bias_variable(shape, name=None):\n    initial = tf.constant(0.0, shape=shape)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef get_tensor_size(tensor):\n    from operator import mul\n    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n\n\ndef conv2d_basic(x, W, bias):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, bias)\n\n\ndef conv2d_strided(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef conv2d_transpose_strided(x, W, b, output_shape=None, stride = 2):\n    # print x.get_shape()\n    # print W.get_shape()\n    if output_shape is None:\n        output_shape = x.get_shape().as_list()\n        output_shape[1] *= 2\n        output_shape[2] *= 2\n        output_shape[3] = W.get_shape().as_list()[2]\n    # print output_shape\n    conv = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef leaky_relu(x, alpha=0.0, name=""""):\n    return tf.maximum(alpha * x, x, name)\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef avg_pool_2x2(x):\n    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef local_response_norm(x):\n    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)\n\n\ndef batch_norm(x, n_out, phase_train, scope=\'bn\', decay=0.9, eps=1e-5):\n    """"""\n    Code taken from http://stackoverflow.com/a/34634291/2267819\n    """"""\n    with tf.variable_scope(scope):\n        beta = tf.get_variable(name=\'beta\', shape=[n_out], initializer=tf.constant_initializer(0.0)\n                               , trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[n_out], initializer=tf.random_normal_initializer(1.0, 0.02),\n                                trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n    return normed\n\n\ndef process_image(image, mean_pixel):\n    return image - mean_pixel\n\n\ndef unprocess_image(image, mean_pixel):\n    return image + mean_pixel\n\n\ndef bottleneck_unit(x, out_chan1, out_chan2, down_stride=False, up_stride=False, name=None):\n    """"""\n    Modified implementation from github ry?!\n    """"""\n\n    def conv_transpose(tensor, out_channel, shape, strides, name=None):\n        out_shape = tensor.get_shape().as_list()\n        in_channel = out_shape[-1]\n        kernel = weight_variable([shape, shape, out_channel, in_channel], name=name)\n        shape[-1] = out_channel\n        return tf.nn.conv2d_transpose(x, kernel, output_shape=out_shape, strides=[1, strides, strides, 1],\n                                      padding=\'SAME\', name=\'conv_transpose\')\n\n    def conv(tensor, out_chans, shape, strides, name=None):\n        in_channel = tensor.get_shape().as_list()[-1]\n        kernel = weight_variable([shape, shape, in_channel, out_chans], name=name)\n        return tf.nn.conv2d(x, kernel, strides=[1, strides, strides, 1], padding=\'SAME\', name=\'conv\')\n\n    def bn(tensor, name=None):\n        """"""\n        :param tensor: 4D tensor input\n        :param name: name of the operation\n        :return: local response normalized tensor - not using batch normalization :(\n        """"""\n        return tf.nn.lrn(tensor, depth_radius=5, bias=2, alpha=1e-4, beta=0.75, name=name)\n\n    in_chans = x.get_shape().as_list()[3]\n\n    if down_stride or up_stride:\n        first_stride = 2\n    else:\n        first_stride = 1\n\n    with tf.variable_scope(\'res%s\' % name):\n        if in_chans == out_chan2:\n            b1 = x\n        else:\n            with tf.variable_scope(\'branch1\'):\n                if up_stride:\n                    b1 = conv_transpose(x, out_chans=out_chan2, shape=1, strides=first_stride,\n                                        name=\'res%s_branch1\' % name)\n                else:\n                    b1 = conv(x, out_chans=out_chan2, shape=1, strides=first_stride, name=\'res%s_branch1\' % name)\n                b1 = bn(b1, \'bn%s_branch1\' % name, \'scale%s_branch1\' % name)\n\n        with tf.variable_scope(\'branch2a\'):\n            if up_stride:\n                b2 = conv_transpose(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            else:\n                b2 = conv(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            b2 = bn(b2, \'bn%s_branch2a\' % name, \'scale%s_branch2a\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2b\'):\n            b2 = conv(b2, out_chans=out_chan1, shape=3, strides=1, name=\'res%s_branch2b\' % name)\n            b2 = bn(b2, \'bn%s_branch2b\' % name, \'scale%s_branch2b\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2c\'):\n            b2 = conv(b2, out_chans=out_chan2, shape=1, strides=1, name=\'res%s_branch2c\' % name)\n            b2 = bn(b2, \'bn%s_branch2c\' % name, \'scale%s_branch2c\' % name)\n\n        x = b1 + b2\n        return tf.nn.relu(x, name=\'relu\')\n\n\ndef add_to_regularization_and_summary(var):\n    if var is not None:\n        tf.histogram_summary(var.op.name, var)\n        tf.add_to_collection(""reg_loss"", tf.nn.l2_loss(var))\n\n\ndef add_activation_summary(var):\n    if var is not None:\n        tf.histogram_summary(var.op.name + ""/activation"", var)\n        tf.scalar_summary(var.op.name + ""/sparsity"", tf.nn.zero_fraction(var))\n\n\ndef add_gradient_summary(grad, var):\n    if grad is not None:\n        tf.histogram_summary(var.op.name + ""/gradient"", grad)\n'"
TensorflowUtils_plus.py,45,"b'__author__ = \'Charlie\'\n# Utils used with tensorflow implemetation\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc as misc\nimport os, sys\nfrom six.moves import urllib\nimport tarfile\nimport zipfile\nimport scipy.io\n\n\ndef get_model_data(dir_path, model_url):\n    maybe_download_and_extract(dir_path, model_url)\n    filename = model_url.split(""/"")[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        raise IOError(""VGG Model not found!"")\n    data = scipy.io.loadmat(filepath)\n    return data\n\n\ndef maybe_download_and_extract(dir_path, url_name, is_tarfile=False, is_zipfile=False):\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n    filename = url_name.split(\'/\')[-1]\n    filepath = os.path.join(dir_path, filename)\n    if not os.path.exists(filepath):\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\n                \'\\r>> Downloading %s %.1f%%\' % (filename, float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n\n        filepath, _ = urllib.request.urlretrieve(url_name, filepath, reporthook=_progress)\n        print()\n        statinfo = os.stat(filepath)\n        print(\'Succesfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n        if is_tarfile:\n            tarfile.open(filepath, \'r:gz\').extractall(dir_path)\n        elif is_zipfile:\n            with zipfile.ZipFile(filepath) as zf:\n                zip_dir = zf.namelist()[0]\n                zf.extractall(dir_path)\n\n\ndef save_image(image, save_dir, name, mean=None):\n    """"""\n    Save image by unprocessing if mean given else just save\n    :param mean:\n    :param image:\n    :param save_dir:\n    :param name:\n    :return:\n    """"""\n    if mean:\n        image = unprocess_image(image, mean)\n    misc.imsave(os.path.join(save_dir, name + "".png""), image)\n\n# as describe at Sec.4.2\ndef get_variable(weights, name):\n    if name == \'conv1_1_w\':\n        k1, k2, ic, oc = weights.shape\n        concat_weights =  np.random.normal(0.0, 1.0, size=(k1, k2, 2 * ic, oc))\n        concat_weights[:, :, 0:ic, :] = weights\n        init = tf.constant_initializer(concat_weights, dtype=tf.float32)\n        var = tf.get_variable(name=name, initializer=init,  shape=concat_weights.shape)\n        return var\n    init = tf.constant_initializer(weights, dtype=tf.float32)\n    var = tf.get_variable(name=name, initializer=init,  shape=weights.shape)\n    return var\n\n\ndef weight_variable(shape, stddev=0.02, name=None):\n    # print(shape)\n    initial = tf.truncated_normal(shape, stddev=stddev)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef bias_variable(shape, name=None):\n    initial = tf.constant(0.0, shape=shape)\n    if name is None:\n        return tf.Variable(initial)\n    else:\n        return tf.get_variable(name, initializer=initial)\n\n\ndef get_tensor_size(tensor):\n    from operator import mul\n    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n\n\ndef conv2d_basic(x, W, bias):\n    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, bias)\n\n\ndef conv2d_strided(x, W, b):\n    conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef conv2d_transpose_strided(x, W, b, output_shape=None, stride = 2):\n    # print x.get_shape()\n    # print W.get_shape()\n    if output_shape is None:\n        output_shape = x.get_shape().as_list()\n        output_shape[1] *= 2\n        output_shape[2] *= 2\n        output_shape[3] = W.get_shape().as_list()[2]\n    # print output_shape\n    conv = tf.nn.conv2d_transpose(x, W, output_shape, strides=[1, stride, stride, 1], padding=""SAME"")\n    return tf.nn.bias_add(conv, b)\n\n\ndef leaky_relu(x, alpha=0.0, name=""""):\n    return tf.maximum(alpha * x, x, name)\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef avg_pool_2x2(x):\n    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef local_response_norm(x):\n    return tf.nn.lrn(x, depth_radius=5, bias=2, alpha=1e-4, beta=0.75)\n\n\ndef batch_norm(x, n_out, phase_train, scope=\'bn\', decay=0.9, eps=1e-5):\n    """"""\n    Code taken from http://stackoverflow.com/a/34634291/2267819\n    """"""\n    with tf.variable_scope(scope):\n        beta = tf.get_variable(name=\'beta\', shape=[n_out], initializer=tf.constant_initializer(0.0)\n                               , trainable=True)\n        gamma = tf.get_variable(name=\'gamma\', shape=[n_out], initializer=tf.random_normal_initializer(1.0, 0.02),\n                                trainable=True)\n        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n        ema = tf.train.ExponentialMovingAverage(decay=decay)\n\n        def mean_var_with_update():\n            ema_apply_op = ema.apply([batch_mean, batch_var])\n            with tf.control_dependencies([ema_apply_op]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n\n        mean, var = tf.cond(phase_train,\n                            mean_var_with_update,\n                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, eps)\n    return normed\n\n\ndef process_image(image, mean_pixel):\n    return image - mean_pixel\n\n\ndef unprocess_image(image, mean_pixel):\n    return image + mean_pixel\n\n\ndef bottleneck_unit(x, out_chan1, out_chan2, down_stride=False, up_stride=False, name=None):\n    """"""\n    Modified implementation from github ry?!\n    """"""\n\n    def conv_transpose(tensor, out_channel, shape, strides, name=None):\n        out_shape = tensor.get_shape().as_list()\n        in_channel = out_shape[-1]\n        kernel = weight_variable([shape, shape, out_channel, in_channel], name=name)\n        shape[-1] = out_channel\n        return tf.nn.conv2d_transpose(x, kernel, output_shape=out_shape, strides=[1, strides, strides, 1],\n                                      padding=\'SAME\', name=\'conv_transpose\')\n\n    def conv(tensor, out_chans, shape, strides, name=None):\n        in_channel = tensor.get_shape().as_list()[-1]\n        kernel = weight_variable([shape, shape, in_channel, out_chans], name=name)\n        return tf.nn.conv2d(x, kernel, strides=[1, strides, strides, 1], padding=\'SAME\', name=\'conv\')\n\n    def bn(tensor, name=None):\n        """"""\n        :param tensor: 4D tensor input\n        :param name: name of the operation\n        :return: local response normalized tensor - not using batch normalization :(\n        """"""\n        return tf.nn.lrn(tensor, depth_radius=5, bias=2, alpha=1e-4, beta=0.75, name=name)\n\n    in_chans = x.get_shape().as_list()[3]\n\n    if down_stride or up_stride:\n        first_stride = 2\n    else:\n        first_stride = 1\n\n    with tf.variable_scope(\'res%s\' % name):\n        if in_chans == out_chan2:\n            b1 = x\n        else:\n            with tf.variable_scope(\'branch1\'):\n                if up_stride:\n                    b1 = conv_transpose(x, out_chans=out_chan2, shape=1, strides=first_stride,\n                                        name=\'res%s_branch1\' % name)\n                else:\n                    b1 = conv(x, out_chans=out_chan2, shape=1, strides=first_stride, name=\'res%s_branch1\' % name)\n                b1 = bn(b1, \'bn%s_branch1\' % name, \'scale%s_branch1\' % name)\n\n        with tf.variable_scope(\'branch2a\'):\n            if up_stride:\n                b2 = conv_transpose(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            else:\n                b2 = conv(x, out_chans=out_chan1, shape=1, strides=first_stride, name=\'res%s_branch2a\' % name)\n            b2 = bn(b2, \'bn%s_branch2a\' % name, \'scale%s_branch2a\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2b\'):\n            b2 = conv(b2, out_chans=out_chan1, shape=3, strides=1, name=\'res%s_branch2b\' % name)\n            b2 = bn(b2, \'bn%s_branch2b\' % name, \'scale%s_branch2b\' % name)\n            b2 = tf.nn.relu(b2, name=\'relu\')\n\n        with tf.variable_scope(\'branch2c\'):\n            b2 = conv(b2, out_chans=out_chan2, shape=1, strides=1, name=\'res%s_branch2c\' % name)\n            b2 = bn(b2, \'bn%s_branch2c\' % name, \'scale%s_branch2c\' % name)\n\n        x = b1 + b2\n        return tf.nn.relu(x, name=\'relu\')\n\n\ndef add_to_regularization_and_summary(var):\n    if var is not None:\n        tf.histogram_summary(var.op.name, var)\n        tf.add_to_collection(""reg_loss"", tf.nn.l2_loss(var))\n\n\ndef add_activation_summary(var):\n    if var is not None:\n        tf.histogram_summary(var.op.name + ""/activation"", var)\n        tf.scalar_summary(var.op.name + ""/sparsity"", tf.nn.zero_fraction(var))\n\n\ndef add_gradient_summary(grad, var):\n    if grad is not None:\n        tf.histogram_summary(var.op.name + ""/gradient"", grad)\n'"
portrait.py,0,"b""import numpy as np\nimport scipy.io as sio\nimport os\nfrom PIL import Image\n\nclass BatchDatset:\n    imgs = []\n    max_batch = 0\n    batch_size = 0\n    cur_imgs = []\n    cur_labels = []\n    cur_batch = 0 # index of batch generated\n    cur_ind = 0 # index of current image in imgs\n    img_width = 600\n    img_height = 800\n\n    def __init__(self, imgs_path, batch_size=2):\n        self.imgs = sio.loadmat(imgs_path)['trainlist'][0]\n        #self.labels = sio.loadmat(labels_path)['test_list'][0]\n        self.batch_size = batch_size\n        #self.max_batch = len(self.imgs) * 9 / batch_size\n        self.cur_imgs, self.cur_labels = self.get_variations(self.imgs[0])\n\n    def next_batch(self):\n        while len(self.cur_imgs) < self.batch_size: # if not enough, get the next image\n            self.cur_ind += 1\n            #print('appending', self.cur_ind)\n            if self.cur_ind >= len(self.imgs):\n                #print('leaving', self.cur_ind)\n                break\n            cur_name = self.imgs[self.cur_ind]\n            tmp_imgs, tmp_labels = self.get_variations(cur_name)\n            self.cur_imgs += tmp_imgs\n            self.cur_labels += tmp_labels\n        if len(self.cur_imgs) >= self.batch_size:\n            #print('getting', self.cur_ind)\n            rimat = np.zeros((self.batch_size, self.img_height, self.img_width, 3), dtype=np.float)\n            ramat = np.zeros((self.batch_size, self.img_height, self.img_width, 1), dtype=np.int)\n            self.cur_batch += 1 # output a new batch\n            for i in range(self.batch_size):\n                rimat[i] = self.cur_imgs.pop(0)\n                ramat[i, :, :, 0] = self.cur_labels.pop(0)\n            #print('batch:', self.cur_batch, 'at img:', self.imgs[self.cur_ind], 'generate image shape', rimat.shape, 'and label shape', ramat.shape)\n            return rimat, ramat\n        return [], []\n\n    def get_variations(self, img_name):\n        imgs = []\n        labels = []\n        stp = str(img_name)\n        if img_name < 10:\n            stp = '0000' + stp\n        elif img_name < 100:\n            stp = '000' + stp\n        elif img_name < 1000:\n            stp = '00' + stp\n        else:\n            stp = '0' + stp\n        img_path = 'data/portraitFCN_data/' + stp + '.mat'\n        alpha_path = 'data/images_mask/' + stp + '_mask.mat'\n        if os.path.exists(img_path) and os.path.exists(alpha_path):\n            imat = sio.loadmat(img_path)['img']\n            amat = sio.loadmat(alpha_path)['mask']\n            nimat = np.array(imat, dtype=np.float)\n            namat = np.array(amat, dtype=np.int)\n            imgs.append(nimat)\n            labels.append(namat)\n\n            angs = [-45, -22, 22, 45]\n            gammas = [0.8, 0.9, 1.1, 1.2]\n            org_mat = np.zeros(nimat.shape, dtype=np.int)\n            h, w, _ = nimat.shape\n            for i in range(h):\n                for j in range(w):\n                    org_mat[i][j][0] = round(nimat[i][j][2] * 255 + 122.675)\n                    org_mat[i][j][1] = round(nimat[i][j][1] * 255 + 116.669)\n                    org_mat[i][j][2] = round(nimat[i][j][0] * 255 + 104.008)\n            i_img = Image.fromarray(np.uint8(org_mat))\n            a_img = Image.fromarray(np.uint8(amat))\n            for i in range(4):\n                tmpi_img = i_img.rotate(angs[i])\n                tmpa_img = a_img.rotate(angs[i])\n                tmpri_img = np.array(tmpi_img, dtype=np.int)\n                rimat = np.zeros(tmpri_img.shape, dtype=np.float)\n                for k in range(h):\n                    for j in range(w):\n                        rimat[k][j][0] = (tmpri_img[k][j][2] * 1.0 - 104.008) / 255\n                        rimat[k][j][1] = (tmpri_img[k][j][1] * 1.0 - 116.669) / 255\n                        rimat[k][j][2] = (tmpri_img[k][j][0] * 1.0 - 122.675) / 255\n                imgs.append(rimat)\n                labels.append(np.array(tmpa_img, dtype=np.int))\n                tmp_nimat = np.array(imat, dtype=np.float)\n                tmp_nimat[:, :, 0] = tmp_nimat[:, :, 0] * 255 + 104.01\n                tmp_nimat[:, :, 0] = (pow(tmp_nimat[:, :, 0], gammas[i]) - pow(104.01, gammas[i])) / pow(255, gammas[i])\n                tmp_nimat[:, :, 1] = tmp_nimat[:, :, 1] * 255 + 116.67\n                tmp_nimat[:, :, 1] = (pow(tmp_nimat[:, :, 1], gammas[i]) - pow(116.67, gammas[i])) / pow(255, gammas[i])\n                tmp_nimat[:, :, 2] = tmp_nimat[:, :, 2] * 255 + 122.68\n                tmp_nimat[:, :, 2] = (pow(tmp_nimat[:, :, 2], gammas[i]) - pow(122.68, gammas[i])) / pow(255, gammas[i])\n                imgs.append(tmp_nimat)\n                labels.append(namat)\n        return imgs, labels\n\n\nclass TestDataset:\n    imgs = []\n    max_batch = 0\n    batch_size = 0\n    cur_batch = 0 # index of batch generated\n    cur_ind = -1 # index of current image in imgs\n    img_width = 600\n    img_height = 800\n\n    def __init__(self, imgs_path, batch_size=2):\n        self.imgs = sio.loadmat(imgs_path)['testlist'][0]\n        #self.labels = sio.loadmat(labels_path)['test_list'][0]\n        self.batch_size = batch_size\n        #self.max_batch = len(self.imgs) * 9 / batch_size\n        #self.cur_imgs, self.cur_labels = self.get_images(self.imgs[0])\n\n    def next_batch(self):\n        cur_imgs = []\n        cur_labels = []\n        cur_orgs = []\n        while len(cur_imgs) < self.batch_size: # if not enough, get the next image\n            self.cur_ind += 1\n            #print('appending', self.cur_ind)\n            if self.cur_ind >= len(self.imgs):\n                #print('leaving', self.cur_ind)\n                break\n            cur_name = self.imgs[self.cur_ind]\n            tmp_img, tmp_label, tmp_org = self.get_images(cur_name)\n            if tmp_img is not None:\n                cur_imgs.append(tmp_img)\n                cur_labels.append(tmp_label)\n                cur_orgs.append(tmp_org)\n        if len(cur_imgs) == self.batch_size:\n            #print('getting', self.cur_ind)\n            rimat = np.zeros((self.batch_size, self.img_height, self.img_width, 3), dtype=np.float)\n            org_mat = np.zeros((self.batch_size, self.img_height, self.img_width, 3), dtype=np.int)\n            ramat = np.zeros((self.batch_size, self.img_height, self.img_width, 1), dtype=np.int)\n            self.cur_batch += 1 # output a new batch\n            for i in range(self.batch_size):\n                rimat[i] = cur_imgs.pop(0)\n                org_mat[i] = cur_orgs.pop(0)\n                ramat[i, :, :, 0] = cur_labels.pop(0)\n            #print('getting', ramat[0, 200:210, 200:220])\n            #print('batch:', self.cur_batch, 'at img:', self.imgs[self.cur_ind], 'generate image shape', rimat.shape, 'and label shape', ramat.shape)\n            return rimat, ramat, org_mat\n        return [], [], []\n\n    def get_images(self, img_name):\n        stp = str(img_name)\n        if img_name < 10:\n            stp = '0000' + stp\n        elif img_name < 100:\n            stp = '000' + stp\n        elif img_name < 1000:\n            stp = '00' + stp\n        else:\n            stp = '0' + stp\n        img_path = 'data/portraitFCN_data/' + stp + '.mat'\n        alpha_path = 'data/images_mask/' + stp + '_mask.mat'\n        if os.path.exists(img_path) and os.path.exists(alpha_path):\n            imat = sio.loadmat(img_path)['img']\n            amat = sio.loadmat(alpha_path)['mask']\n            nimat = np.array(imat, dtype=np.float)\n            namat = np.array(amat, dtype=np.int)\n            org_mat = np.zeros(nimat.shape, dtype=np.int)\n            h, w, _ = nimat.shape\n            for i in range(h):\n                for j in range(w):\n                    org_mat[i][j][0] = round(nimat[i][j][2] * 255 + 122.675)\n                    org_mat[i][j][1] = round(nimat[i][j][1] * 255 + 116.669)\n                    org_mat[i][j][2] = round(nimat[i][j][0] * 255 + 104.008)\n            return nimat, namat, org_mat\n        return None, None, None\n\nif __name__ == '__main__':\n    data = BatchDatset('data/trainlist.mat')\n    '''ri, ra = data.next_batch()\n    while len(ri) != 0:\n        ri, ra = data.next_batch()\n        print(np.sum(ra))'''\n    imgs, labels = data.get_variations(47)\n    cnt = 0\n    for img in imgs:\n        mat = np.zeros(img.shape, dtype=np.int)\n        h, w, _ = img.shape\n        for i in range(h):\n            for j in range(w):\n                mat[i][j][0] = round(img[i][j][2] * 255 + 122.675)\n                mat[i][j][1] = round(img[i][j][1] * 255 + 116.669)\n                mat[i][j][2] = round(img[i][j][0] * 255 + 104.008)\n        im = Image.fromarray(np.uint8(mat))\n        im.save('img-'+str(cnt)+'.jpg')\n        cnt += 1\n"""
portrait_plus.py,0,"b""import numpy as np\nimport scipy.io as sio\nimport os\nfrom PIL import Image\nimport math\nfrom scipy import misc\n\nclass BatchDatset:\n    imgs = []\n    max_batch = 0\n    batch_size = 0\n    cur_imgs = []\n    cur_labels = []\n    cur_batch = 0 # index of batch generated\n    cur_ind = 0 # index of current image in imgs\n    img_width = 600\n    img_height = 800\n\n    def __init__(self, imgs_path, batch_size=1):\n        self.imgs = sio.loadmat(imgs_path)['trainlist'][0]\n        #self.labels = sio.loadmat(labels_path)['test_list'][0]\n        self.batch_size = batch_size\n        #self.max_batch = len(self.imgs) * 9 / batch_size\n        self.cur_imgs, self.cur_labels = self.get_variations(self.imgs[0])\n\n    def next_batch(self):\n        while len(self.cur_imgs) < self.batch_size: # if not enough, get the next image\n            self.cur_ind += 1\n            #print('appending', self.cur_ind)\n            if self.cur_ind >= len(self.imgs):\n                #print('leaving', self.cur_ind)\n                break\n            cur_name = self.imgs[self.cur_ind]\n            tmp_imgs, tmp_labels = self.get_variations(cur_name)\n            self.cur_imgs += tmp_imgs\n            self.cur_labels += tmp_labels\n        if len(self.cur_imgs) >= self.batch_size:\n            #print('getting', self.cur_ind)\n            rimat = np.zeros((self.batch_size, self.img_height, self.img_width, 6), dtype=np.float)\n            ramat = np.zeros((self.batch_size, self.img_height, self.img_width, 1), dtype=np.int)\n            self.cur_batch += 1 # output a new batch\n            for i in range(self.batch_size):\n                rimat[i] = self.cur_imgs.pop(0)\n                ramat[i, :, :, 0] = self.cur_labels.pop(0)\n            #print('batch:', self.cur_batch, 'at img:', self.imgs[self.cur_ind], 'generate image shape', rimat.shape, 'and label shape', ramat.shape)\n            return rimat, ramat\n        return [], []\n\n    def rotateNormalizedCord(self, matx, maty, angle):\n        h, w = matx.shape\n        x_avg = np.mean(matx)\n        x_min = np.min(matx)\n        y_avg = np.mean(maty)\n        y_min = np.min(maty)\n        xmat = np.zeros((h, w), dtype=np.float)\n        ymat = np.zeros((h, w), dtype=np.float)\n        for k in range(h):\n            for j in range(w):\n                cor_y = k - h / 2\n                cor_x = j - w / 2\n                if cor_x == 0 and cor_y == 0:\n                    xmat[k][j] = x_avg\n                    ymat[k][j] = y_avg\n                else:\n                    x_dis = math.cos(math.pi / 2 - angle) * (-math.tan(math.pi / 2 - angle) * cor_x + cor_y)\n                    xmat[k][j] = x_avg - (x_avg - x_min) * x_dis * 2 / w\n                    y_dis = math.cos(angle) * (math.tan(angle) * cor_x + cor_y)\n                    ymat[k][j] = y_avg + (y_avg - y_min) * y_dis * 2 / h\n        return xmat, ymat\n\n    def scaleNormalizedCord(self, matx, maty, scale):\n        h, w = matx.shape\n        x_avg = np.mean(matx)\n        x_max = np.max(matx)\n        y_avg = np.mean(maty)\n        y_max = np.max(maty)\n        xmat = np.zeros((h, w), dtype=np.float)\n        ymat = np.zeros((h, w), dtype=np.float)\n        for k in range(h):\n            for j in range(w):\n                cor_y = k - h / 2\n                cor_x = j - w / 2\n                xmat[k][j] = x_avg + (x_max - x_avg) * cor_x / scale\n                ymat[k][j] = y_avg + (y_max - y_avg) * cor_y / scale\n        return xmat, ymat\n\n    def get_variations(self, img_name):\n        imgs = []\n        labels = []\n        stp = str(img_name)\n        if img_name < 10:\n            stp = '0000' + stp\n        elif img_name < 100:\n            stp = '000' + stp\n        elif img_name < 1000:\n            stp = '00' + stp\n        else:\n            stp = '0' + stp\n        img_path = 'data/portraitFCN+_data/' + stp + '.mat'\n        alpha_path = 'data/images_mask/' + stp + '_mask.mat'\n        if os.path.exists(img_path) and os.path.exists(alpha_path):\n            imat = sio.loadmat(img_path)['img']\n            amat = sio.loadmat(alpha_path)['mask']\n            nimat = np.array(imat, dtype=np.float)\n            namat = np.array(amat, dtype=np.int)\n            imgs.append(nimat)\n            labels.append(namat)\n\n            angs = [-45, -22, 22, 45]\n            gammas = [0.5, 0.8, 1.2, 1.5]\n            scales = [0.6, 0.8, 1.2, 1.5]\n            h, w, _ = nimat.shape\n            org_mat = np.zeros((h, w, 3), dtype=np.int)\n            app_mat = np.zeros((h, w, 3), dtype=np.int)\n            min3 = np.min(nimat[:, :, 3])\n            min4 = np.min(nimat[:, :, 4])\n            min5 = np.min(nimat[:, :, 5])\n            ran3 = np.max(nimat[:, :, 3]) - min3\n            ran4 = np.max(nimat[:, :, 4]) - min4\n            ran5 = np.max(nimat[:, :, 5]) - min5\n            org_mat[:, :, 0] = np.round(nimat[:, :, 2] * 255 + 122.675)\n            org_mat[:, :, 1] = np.round(nimat[:, :, 1] * 255 + 116.669)\n            org_mat[:, :, 2] = np.round(nimat[:, :, 0] * 255 + 104.008)\n            if ran3 != 0:\n                app_mat[:, :, 0] = np.round((nimat[:, :, 3] - min3) * 255 / ran3)\n            else:\n                app_mat[:, :, 0] = min3\n            if ran4 != 0:\n                app_mat[:, :, 1] = np.round((nimat[:, :, 4] - min4) * 255 / ran4)\n            else:\n                app_mat[:, :, 0] = min4\n            if ran5 != 0:\n                app_mat[:, :, 2] = np.round((nimat[:, :, 5] - min5) * 255 / ran5)\n            else:\n                app_mat[:, :, 0] = min5\n            i_img = Image.fromarray(np.uint8(org_mat))\n            p_img = Image.fromarray(np.uint8(app_mat))\n            a_img = Image.fromarray(np.uint8(amat))\n            for i in range(4):\n                # rotation\n                tmpi_img = i_img.rotate(angs[i])\n                tmpp_img = p_img.rotate(angs[i])\n                tmpa_img = a_img.rotate(angs[i])\n                tmpri_img = np.array(tmpi_img, dtype=np.int)\n                tmprp_img = np.array(tmpp_img, dtype=np.int)\n                rot_p1, rot_p2 = self.rotateNormalizedCord(nimat[:, :, 3], nimat[:, :, 4], angs[i] * math.pi / 180)\n                rimat = np.zeros((h, w, 6), dtype=np.float)\n                rimat[:, :, 0] = (tmpri_img[:, :, 2] - 104.008) / 255\n                rimat[:, :, 1] = (tmpri_img[:, :, 1] - 116.669) / 255\n                rimat[:, :, 2] = (tmpri_img[:, :, 0] - 122.675) / 255\n                rimat[:, :, 3] = rot_p1\n                rimat[:, :, 4] = rot_p2\n                rimat[:, :, 5] = tmprp_img[:, :, 2] * ran5 / 255 + min5\n                imgs.append(rimat)\n                labels.append(np.array(tmpa_img, dtype=np.int))\n                # gamma transformation\n                tmp_nimat = np.array(imat, dtype=np.float)\n                tmp_nimat[:, :, 0] = tmp_nimat[:, :, 0] + 104.008 / 255\n                tmp_nimat[:, :, 0] = (pow(tmp_nimat[:, :, 0], gammas[i]) - pow(104.008 / 255, gammas[i]))\n                tmp_nimat[:, :, 1] = tmp_nimat[:, :, 1] + 116.669 / 255\n                tmp_nimat[:, :, 1] = (pow(tmp_nimat[:, :, 1], gammas[i]) - pow(116.669 / 255, gammas[i]))\n                tmp_nimat[:, :, 2] = tmp_nimat[:, :, 2] + 122.675 / 255\n                tmp_nimat[:, :, 2] = (pow(tmp_nimat[:, :, 2], gammas[i]) - pow(122.675 / 255, gammas[i]))\n                imgs.append(tmp_nimat)\n                labels.append(namat)\n                # scale transformation\n                if scales[i] > 1.0:\n                    resize_box = (round(scales[i] * w), round(scales[i] * h))\n                    si_img = i_img.resize(resize_box, Image.ANTIALIAS)\n                    sp_img = p_img.resize(resize_box, Image.ANTIALIAS)\n                    sa_img = a_img.resize(resize_box, Image.ANTIALIAS)\n                    crop_up, crop_down = (scales[i] - 1) / 2, (scales[i] + 1) / 2\n                    crop_box = (round(crop_up * w), round(crop_up * h), round(crop_down * w), round(crop_down * h))\n                    ci_img = si_img.crop(crop_box)\n                    cp_img = sp_img.crop(crop_box)\n                    ca_img = sa_img.crop(crop_box)\n                    tmpsi_img = np.array(ci_img, dtype=np.int)\n                    tmpsp_img = np.array(cp_img, dtype=np.int)\n                    tmpsa_img = np.array(ca_img, dtype=np.int)\n                    simat = np.zeros(imat.shape, dtype=np.float)\n                    simat[:, :, 0] = (tmpsi_img[:, :, 2] - 104.008) / 255\n                    simat[:, :, 1] = (tmpsi_img[:, :, 1] - 116.669) / 255\n                    simat[:, :, 2] = (tmpsi_img[:, :, 0] - 122.675) / 255\n                    xmat, ymat = self.scaleNormalizedCord(nimat[:, :, 3], nimat[:, :, 4], scales[i] * 300)\n                    simat[:, :, 3] = xmat\n                    simat[:, :, 4] = ymat\n                    simat[:, :, 5] = tmpsp_img[:, :, 2] * ran5 / 255 + min5\n                    imgs.append(simat)\n                    labels.append(tmpsa_img)\n                else:\n                    resize_box = (round(scales[i] * w), round(scales[i] * h))\n                    si_img = i_img.resize(resize_box, Image.ANTIALIAS)\n                    sp_img = p_img.resize(resize_box, Image.ANTIALIAS)\n                    sa_img = a_img.resize(resize_box, Image.ANTIALIAS)\n                    tmpsi_img = np.array(si_img, dtype=np.int)\n                    tmpsp_img = np.array(sp_img, dtype=np.int)\n                    tmpsa_img = np.array(sa_img, dtype=np.int)\n                    simat = np.zeros(imat.shape, dtype=np.float)\n                    samat = np.zeros(amat.shape, dtype=np.int)\n                    crop_up, crop_down = (1 - scales[i]) / 2, (1 + scales[i]) / 2\n                    simat[round(crop_up * h):round(crop_down * h), round(crop_up * w):round(crop_down * w), 0] = (tmpsi_img[:, :, 2] - 104.008) / 255\n                    simat[round(crop_up * h):round(crop_down * h), round(crop_up * w):round(crop_down * w), 1] = (tmpsi_img[:, :, 1] - 116.669) / 255\n                    simat[round(crop_up * h):round(crop_down * h), round(crop_up * w):round(crop_down * w), 2] = (tmpsi_img[:, :, 0] - 122.675) / 255\n                    simat[round(crop_up * h):round(crop_down * h), round(crop_up * w):round(crop_down * w), 5] = tmpsp_img[:, :, 2] * ran5 / 255 + min5\n                    samat[round(crop_up * h):round(crop_down * h), round(crop_up * w):round(crop_down * w)] = tmpsa_img\n                    xmat, ymat = self.scaleNormalizedCord(nimat[:, :, 3], nimat[:, :, 4], scales[i] * 300)\n                    simat[:, :, 3] = xmat\n                    simat[:, :, 4] = ymat\n                    imgs.append(simat)\n                    labels.append(samat)\n        return imgs, labels\n\n\nclass TestDataset:\n    imgs = []\n    max_batch = 0\n    batch_size = 0\n    cur_batch = 0 # index of batch generated\n    cur_ind = -1 # index of current image in imgs\n    img_width = 600\n    img_height = 800\n\n    def __init__(self, imgs_path, batch_size=1):\n        self.imgs = sio.loadmat(imgs_path)['testlist'][0]\n        #self.labels = sio.loadmat(labels_path)['test_list'][0]\n        self.batch_size = batch_size\n        #self.max_batch = len(self.imgs) * 9 / batch_size\n        #self.cur_imgs, self.cur_labels = self.get_images(self.imgs[0])\n\n    def next_batch(self):\n        cur_imgs = []\n        cur_labels = []\n        cur_orgs = []\n        while len(cur_imgs) < self.batch_size: # if not enough, get the next image\n            self.cur_ind += 1\n            #print('appending', self.cur_ind)\n            if self.cur_ind >= len(self.imgs):\n                #print('leaving', self.cur_ind)\n                break\n            cur_name = self.imgs[self.cur_ind]\n            tmp_img, tmp_label, tmp_org = self.get_images(cur_name)\n            if tmp_img is not None:\n                cur_imgs.append(tmp_img)\n                cur_labels.append(tmp_label)\n                cur_orgs.append(tmp_org)\n        if len(cur_imgs) == self.batch_size:\n            #print('getting', self.cur_ind)\n            rimat = np.zeros((self.batch_size, self.img_height, self.img_width, 6), dtype=np.float)\n            org_mat = np.zeros((self.batch_size, self.img_height, self.img_width, 3), dtype=np.int)\n            ramat = np.zeros((self.batch_size, self.img_height, self.img_width, 1), dtype=np.int)\n            self.cur_batch += 1 # output a new batch\n            for i in range(self.batch_size):\n                rimat[i] = cur_imgs.pop(0)\n                org_mat[i] = cur_orgs.pop(0)\n                ramat[i, :, :, 0] = cur_labels.pop(0)\n            #print('getting', ramat[0, 200:210, 200:220])\n            #print('batch:', self.cur_batch, 'at img:', self.imgs[self.cur_ind], 'generate image shape', rimat.shape, 'and label shape', ramat.shape)\n            return rimat, ramat, org_mat\n        return [], [], []\n\n    def get_images(self, img_name):\n        stp = str(img_name)\n        if img_name < 10:\n            stp = '0000' + stp\n        elif img_name < 100:\n            stp = '000' + stp\n        elif img_name < 1000:\n            stp = '00' + stp\n        else:\n            stp = '0' + stp\n        img_path = 'data/portraitFCN+_data/' + stp + '.mat'\n        alpha_path = 'data/images_mask/' + stp + '_mask.mat'\n        if os.path.exists(img_path) and os.path.exists(alpha_path):\n            imat = sio.loadmat(img_path)['img']\n            amat = sio.loadmat(alpha_path)['mask']\n            nimat = np.array(imat, dtype=np.float)\n            namat = np.array(amat, dtype=np.int)\n            h, w, _ = nimat.shape\n            org_mat = np.zeros((h, w, 3), dtype=np.int)\n            for i in range(h):\n                for j in range(w):\n                    org_mat[i][j][0] = round(nimat[i][j][2] * 255 + 122.675)\n                    org_mat[i][j][1] = round(nimat[i][j][1] * 255 + 116.669)\n                    org_mat[i][j][2] = round(nimat[i][j][0] * 255 + 104.008)\n            return nimat, namat, org_mat\n        return None, None, None\n\n\nif __name__ == '__main__':\n    dat = BatchDatset('data/trainlist.mat', batch_size=13)\n    rimat, ramat = dat.next_batch()\n    for i in range(13):\n        imat = rimat[i]\n        amat = ramat[i]\n        rgb = np.zeros((imat.shape[0], imat.shape[1], 3), dtype=np.int)\n        rgb[:, :, 0] = np.round(imat[:, :, 2] * 255 + 122.675)\n        rgb[:, :, 1] = np.round(imat[:, :, 1] * 255 + 116.669)\n        rgb[:, :, 2] = np.round(imat[:, :, 0] * 255 + 104.008)\n        misc.imsave('res/org' + str(i) + '.jpg', rgb)\n        xxc = imat[:, :, 3]\n        xxc = np.round((xxc - np.min(xxc) / (np.max(xxc) - np.min(xxc))) * 255)\n        misc.imsave('res/xxc' + str(i) + '.jpg', xxc)\n        yyc = imat[:, :, 4]\n        yyc = np.round((yyc - np.min(yyc) / (np.max(yyc) - np.min(yyc))) * 255)\n        misc.imsave('res/yyc' + str(i) + '.jpg', yyc)\n        mean = imat[:, :, 5] * 255\n        misc.imsave('res/mean' + str(i) + '.jpg', mean)\n        alpha = amat * 255\n        alpha = alpha.reshape((alpha.shape[0], alpha.shape[1]))\n        misc.imsave('res/alpha' + str(i) + '.jpg', alpha)\n"""
preprocess.py,0,"b""import scipy.io as scio\nimport skimage\nimport numpy as np\nimport math\nfrom PIL import Image\nimport os\nfrom skimage import transform, io as skio\n\nmean_rgb = [122.675, 116.669, 104.008]\nscales = [0.6, 0.8, 1.2, 1.5]\nrorations = [-45, -22, 22, 45]\ngammas = [.05, 0.8, 1.2, 1.5]\n\ndef gen_data(name):\n    reftracker = scio.loadmat('data/images_tracker.00047.mat')['tracker']\n    desttracker = scio.loadmat('data/images_tracker/'+name+'.mat')['tracker']\n    refpos = np.floor(np.mean(reftracker, 0))\n    xxc, yyc = np.meshgrid(np.arange(1, 1801, dtype=np.int), np.arange(1, 2001, dtype=np.int))\n    #normalize x and y channels\n    xxc = (xxc - 600 - refpos[0]) * 1.0 / 600\n    yyc = (yyc - 600 - refpos[1]) * 1.0 / 600\n    maskimg = Image.open('data/meanmask.png')\n    maskc = np.array(maskimg, dtype=np.float)\n    maskc = np.pad(maskc, (600, 600), 'minimum')\n    # warp is an inverse transform, and so src and dst must be reversed here\n    tform = transform.estimate_transform('affine', desttracker + 600, reftracker + 600)\n    \n    img_data = skio.imread('data/images_data/'+name+'.jpg')\n    # save org mat\n    warpedxx = transform.warp(xxc, tform, output_shape=xxc.shape)\n    warpedyy = transform.warp(yyc, tform, output_shape=xxc.shape)\n    warpedmask = transform.warp(maskc, tform, output_shape=xxc.shape)\n    warpedxx = warpedxx[600:1400, 600:1200, :]\n    warpedyy = warpedyy[600:1400, 600:1200, :]\n    warpedmask = warpedmask[600:1400, 600:1200, :]\n    img_h, img_w, _ = img_data.shape\n    mat = np.zeros((img_h, img_w, 6), dtype=np.float)\n    mat[:, :, 0] = (img_data[2] * 1.0 - 104.008) / 255\n    mat[:, :, 1] = (img_data[1] * 1.0 - 116.669) / 255\n    mat[:, :, 2] = (img_data[0] * 1.0 - 122.675) / 255\n    scio.savemat('portraitFCN_data/' + name + '.mat', {'img':mat})\n    mat_plus = np.zeros((img_h, img_w, 6), dtype=np.float)\n    mat_plus[:, :, 0:3] = mat\n    mat_plus[:, :, 3] = warpedxx\n    mat_plus[:, :, 4] = warpedyy\n    mat_plus[:, :, 5] = warpedmask\n\ndef gamma_trans(mat, gamma):\n    gamma_mean = np.pow(mean_rgb / 255, gamma)\n    tmp_mat = np.pow(mat / 255, gamma)\n    gamma_mat = np.zeros(mat.shape, dtype=np.float)\n    gamma_mat[:, :, 0] = tmp_mat[:, :, 2] - gamma_mean[:, :, 2]\n    gamma_mat[:, :, 1] = tmp_mat[:, :, 1] - gamma_mean[:, :, 1]\n    gamma_mat[:, :, 2] = tmp_mat[:, :, 0] - gamma_mean[:, :, 0]\n    return gamma_mat\n\ndef crop_all():\n    files = os.listdir('data/images_data_crop')\n    if not os.path.exists('data/portraitFCN_data'):\n        os.mkdir('data/portraitFCN_data')\n    if not os.path.exists('data/portraitFCN+_data'):\n        os.mkdir('data/portraitFCN+_data')\n"""
