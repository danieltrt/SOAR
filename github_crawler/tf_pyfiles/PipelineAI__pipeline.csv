file_path,api_count,code
spark/scripts/hdfs_test.py,7,"b'import tensorflow as tf\n\nfilename_queue = tf.train.string_input_producer([\n  ""hdfs://hdfs-namenode:9000/hdfs/file1.csv"",\n  ""hdfs://hdfs-namenode:9000/hdfs/file2.csv"",\n])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n    print(example, label)\n\n  coord.request_stop()\n  coord.join(threads)\n'"
train/scripts/hdfs_test.py,7,"b'import tensorflow as tf\n\nfilename_queue = tf.train.string_input_producer([\n  ""hdfs://hdfs-namenode:9000/hdfs/file1.csv"",\n  ""hdfs://hdfs-namenode:9000/hdfs/file2.csv"",\n])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n    print(example, label)\n\n  coord.request_stop()\n  coord.join(threads)\n'"
kubeflow/pach-pipelines/code/seq2seq_utils.py,5,"b'import logging\nimport dill as dpickle\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom IPython.display import SVG, display\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.utils.vis_utils import model_to_dot\nfrom annoy import AnnoyIndex\nfrom tqdm import tqdm, tqdm_notebook\nfrom nltk.translate.bleu_score import corpus_bleu\n\n\ndef load_text_processor(fname=\'title_pp.dpkl\'):\n  """"""\n  Load preprocessors from disk.\n\n  Parameters\n  ----------\n  fname: str\n    file name of ktext.proccessor object\n\n  Returns\n  -------\n  num_tokens : int\n    size of vocabulary loaded into ktext.processor\n  pp : ktext.processor\n    the processor you are trying to load\n\n  Typical Usage:\n  -------------\n\n  num_decoder_tokens, title_pp = load_text_processor(fname=\'title_pp.dpkl\')\n  num_encoder_tokens, body_pp = load_text_processor(fname=\'body_pp.dpkl\')\n\n  """"""\n  # Load files from disk\n  with open(fname, \'rb\') as f:\n    pp = dpickle.load(f)\n\n  num_tokens = max(pp.id2token.keys()) + 1\n  print(\'Size of vocabulary for {}: {}\'.format(fname, num_tokens))\n  return num_tokens, pp\n\n\ndef load_decoder_inputs(decoder_np_vecs=\'train_title_vecs.npy\'):\n  """"""\n  Load decoder inputs.\n\n  Parameters\n  ----------\n  decoder_np_vecs : str\n    filename of serialized numpy.array of decoder input (issue title)\n\n  Returns\n  -------\n  decoder_input_data : numpy.array\n    The data fed to the decoder as input during training for teacher forcing.\n    This is the same as `decoder_np_vecs` except the last position.\n  decoder_target_data : numpy.array\n    The data that the decoder data is trained to generate (issue title).\n    Calculated by sliding `decoder_np_vecs` one position forward.\n\n  """"""\n  vectorized_title = np.load(decoder_np_vecs)\n  # For Decoder Input, you don\'t need the last word as that is only for prediction\n  # when we are training using Teacher Forcing.\n  decoder_input_data = vectorized_title[:, :-1]\n\n  # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n  decoder_target_data = vectorized_title[:, 1:]\n\n  print(\'Shape of decoder input: {}\'.format(decoder_input_data.shape))\n  print(\'Shape of decoder target: {}\'.format(decoder_target_data.shape))\n  return decoder_input_data, decoder_target_data\n\n\ndef load_encoder_inputs(encoder_np_vecs=\'train_body_vecs.npy\'):\n  """"""\n  Load variables & data that are inputs to encoder.\n\n  Parameters\n  ----------\n  encoder_np_vecs : str\n    filename of serialized numpy.array of encoder input (issue title)\n\n  Returns\n  -------\n  encoder_input_data : numpy.array\n    The issue body\n  doc_length : int\n    The standard document length of the input for the encoder after padding\n    the shape of this array will be (num_examples, doc_length)\n\n  """"""\n  vectorized_body = np.load(encoder_np_vecs)\n  # Encoder input is simply the body of the issue text\n  encoder_input_data = vectorized_body\n  doc_length = encoder_input_data.shape[1]\n  print(\'Shape of encoder input: {}\'.format(encoder_input_data.shape))\n  return encoder_input_data, doc_length\n\n\ndef viz_model_architecture(model):\n  """"""Visualize model architecture in Jupyter notebook.""""""\n  display(SVG(model_to_dot(model).create(prog=\'dot\', format=\'svg\')))\n\n\ndef free_gpu_mem():\n  """"""Attempt to free gpu memory.""""""\n  K.get_session().close()\n  cfg = K.tf.ConfigProto()\n  cfg.gpu_options.allow_growth = True\n  K.set_session(K.tf.Session(config=cfg))\n\n\ndef test_gpu():\n  """"""Run a toy computation task in tensorflow to test GPU.""""""\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  session = tf.Session(config=config)\n  hello = tf.constant(\'Hello, TensorFlow!\')\n  print(session.run(hello))\n\n\ndef plot_model_training_history(history_object):\n  """"""Plots model train vs. validation loss.""""""\n  plt.title(\'model accuracy\')\n  plt.ylabel(\'accuracy\')\n  plt.xlabel(\'epoch\')\n  plt.plot(history_object.history[\'loss\'])\n  plt.plot(history_object.history[\'val_loss\'])\n  plt.legend([\'train\', \'test\'], loc=\'upper left\')\n  plt.show()\n\n\ndef extract_encoder_model(model):\n  """"""\n  Extract the encoder from the original Sequence to Sequence Model.\n\n  Returns a keras model object that has one input (body of issue) and one\n  output (encoding of issue, which is the last hidden state).\n\n  Input:\n  -----\n  model: keras model object\n\n  Returns:\n  -----\n  keras model object\n\n  """"""\n  encoder_model = model.get_layer(\'Encoder-Model\')\n  return encoder_model\n\n\ndef extract_decoder_model(model):\n  """"""\n  Extract the decoder from the original model.\n\n  Inputs:\n  ------\n  model: keras model object\n\n  Returns:\n  -------\n  A Keras model object with the following inputs and outputs:\n\n  Inputs of Keras Model That Is Returned:\n  1: the embedding index for the last predicted word or the <Start> indicator\n  2: the last hidden state, or in the case of the first word the hidden state from the encoder\n\n  Outputs of Keras Model That Is Returned:\n  1.  Prediction (class probabilities) for the next word\n  2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n\n  Implementation Notes:\n  ----------------------\n  Must extract relevant layers and reconstruct part of the computation graph\n  to allow for different inputs as we are not going to use teacher forcing at\n  inference time.\n\n  """"""\n  # the latent dimension is the same throughout the architecture so we are going to\n  # cheat and grab the latent dimension of the embedding because that is the same as what is\n  # output from the decoder\n  latent_dim = model.get_layer(\'Decoder-Word-Embedding\').output_shape[-1]\n\n  # Reconstruct the input into the decoder\n  decoder_inputs = model.get_layer(\'Decoder-Input\').input\n  dec_emb = model.get_layer(\'Decoder-Word-Embedding\')(decoder_inputs)\n  dec_bn = model.get_layer(\'Decoder-Batchnorm-1\')(dec_emb)\n\n  # Instead of setting the intial state from the encoder and forgetting about it, during inference\n  # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back\n  # into the GRU, thus we define this input layer for the state so we can add this capability\n  gru_inference_state_input = Input(shape=(latent_dim,), name=\'hidden_state_input\')\n\n  # we need to reuse the weights that is why we are getting this\n  # If you inspect the decoder GRU that we created for training, it will take as input\n  # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n  #                  (which will now be the last step\'s prediction, and will be _start_ on the\n  #                  first time step)\n  #              (2) is the state, which we will initialize with the encoder on the first time step\n  #              but then grab the state after the first prediction and feed that back in again.\n  gru_out, gru_state_out = model.get_layer(\'Decoder-GRU\')([dec_bn, gru_inference_state_input])\n\n  # Reconstruct dense layers\n  dec_bn2 = model.get_layer(\'Decoder-Batchnorm-2\')(gru_out)\n  dense_out = model.get_layer(\'Final-Output-Dense\')(dec_bn2)\n  decoder_model = Model([decoder_inputs, gru_inference_state_input],\n                        [dense_out, gru_state_out])\n  return decoder_model\n\n\nclass Seq2Seq_Inference(object):\n\n  # pylint: disable=too-many-instance-attributes\n\n  def __init__(self,\n               encoder_preprocessor,\n               decoder_preprocessor,\n               seq2seq_model):\n\n    self.pp_body = encoder_preprocessor\n    self.pp_title = decoder_preprocessor\n    self.seq2seq_model = seq2seq_model\n    self.encoder_model = extract_encoder_model(seq2seq_model)\n    self.decoder_model = extract_decoder_model(seq2seq_model)\n    self.default_max_len_title = self.pp_title.padding_maxlen\n    self.nn = None\n    self.rec_df = None\n\n  def generate_issue_title(self,\n                           raw_input_text,\n                           max_len_title=None):\n    """"""\n    Use the seq2seq model to generate a title given the body of an issue.\n\n    Inputs\n    ------\n    raw_input: str\n        The body of the issue text as an input string\n\n    max_len_title: int (optional)\n        The maximum length of the title the model will generate\n\n    """"""\n    if max_len_title is None:\n      max_len_title = self.default_max_len_title\n    # get the encoder\'s features for the decoder\n    raw_tokenized = self.pp_body.transform([raw_input_text])\n    body_encoding = self.encoder_model.predict(raw_tokenized)\n    # we want to save the encoder\'s embedding before its updated by decoder\n    #   because we can use that as an embedding for other tasks.\n    original_body_encoding = body_encoding\n    state_value = np.array(self.pp_title.token2id[\'_start_\']).reshape(1, 1)\n\n    decoded_sentence = []\n    stop_condition = False\n    while not stop_condition:\n      preds, st = self.decoder_model.predict([state_value, body_encoding])\n\n      # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n      # Argmax will return the integer index corresponding to the\n      #  prediction + 2 b/c we chopped off first two\n      pred_idx = np.argmax(preds[:, :, 2:]) + 2\n\n      # retrieve word from index prediction\n      pred_word_str = self.pp_title.id2token[pred_idx]\n\n      if pred_word_str == \'_end_\' or len(decoded_sentence) >= max_len_title:\n        stop_condition = True\n        break\n      decoded_sentence.append(pred_word_str)\n\n      # update the decoder for the next word\n      body_encoding = st\n      state_value = np.array(pred_idx).reshape(1, 1)\n\n    return original_body_encoding, \' \'.join(decoded_sentence)\n\n\n  def print_example(self,\n                    i,\n                    body_text,\n                    title_text,\n                    url,\n                    threshold):\n    """"""\n    Prints an example of the model\'s prediction for manual inspection.\n    """"""\n    if i:\n      print(\'\\n\\n==============================================\')\n      print(\'============== Example # {} =================\\n\'.format(i))\n\n    if url:\n      print(url)\n\n    print(""Issue Body:\\n {} \\n"".format(body_text))\n\n    if title_text:\n      print(""Original Title:\\n {}"".format(title_text))\n\n    emb, gen_title = self.generate_issue_title(body_text)\n    print(""\\n****** Machine Generated Title (Prediction) ******:\\n {}"".format(gen_title))\n\n    if self.nn:\n      # return neighbors and distances\n      n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n                                       include_distances=True)\n      neighbors = n[1:]\n      dist = d[1:]\n\n      if min(dist) <= threshold:\n        cols = [\'issue_url\', \'issue_title\', \'body\']\n        dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n        dfcopy[\'dist\'] = dist\n        similar_issues_df = dfcopy.query(\'dist <= {}\'.format(threshold))\n\n        print(""\\n**** Similar Issues (using encoder embedding) ****:\\n"")\n        display(similar_issues_df)\n\n\n  def demo_model_predictions(self,\n                             n,\n                             issue_df,\n                             threshold=1):\n    """"""\n    Pick n random Issues and display predictions.\n\n    Input:\n    ------\n    n : int\n      Number of issues to display from issue_df\n    issue_df : pandas DataFrame\n      DataFrame that contains two columns: `body` and `issue_title`.\n    threshold : float\n      distance threshold for recommendation of similar issues.\n\n    Returns:\n    --------\n    None\n      Prints the original issue body and the model\'s prediction.\n    """"""\n    # Extract body and title from DF\n    body_text = issue_df.body.tolist()\n    title_text = issue_df.issue_title.tolist()\n    url = issue_df.issue_url.tolist()\n\n    demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n    for i in demo_list:\n      self.print_example(i,\n                         body_text=body_text[i],\n                         title_text=title_text[i],\n                         url=url[i],\n                         threshold=threshold)\n\n  def prepare_recommender(self, vectorized_array, original_df):\n    """"""\n    Use the annoy library to build recommender\n\n    Parameters\n    ----------\n    vectorized_array : List[List[int]]\n      This is the list of list of integers that represents your corpus\n      that is fed into the seq2seq model for training.\n    original_df : pandas.DataFrame\n      This is the original dataframe that has the columns\n      [\'issue_url\', \'issue_title\', \'body\']\n\n    Returns\n    -------\n    annoy.AnnoyIndex  object (see https://github.com/spotify/annoy)\n    """"""\n    self.rec_df = original_df\n    emb = self.encoder_model.predict(x=vectorized_array,\n                                     batch_size=vectorized_array.shape[0]//200)\n\n    f = emb.shape[1]\n    self.nn = AnnoyIndex(f)\n    logging.warning(\'Adding embeddings\')\n    for i in tqdm(range(len(emb))):\n      self.nn.add_item(i, emb[i])\n    logging.warning(\'Building trees for similarity lookup.\')\n    self.nn.build(50)\n    return self.nn\n\n  def set_recsys_data(self, original_df):\n    self.rec_df = original_df\n\n  def set_recsys_annoyobj(self, annoyobj):\n    self.nn = annoyobj\n\n  def evaluate_model(self, holdout_bodies, holdout_titles):\n    """"""\n    Method for calculating BLEU Score.\n\n    Parameters\n    ----------\n    holdout_bodies : List[str]\n      These are the issue bodies that we want to summarize\n    holdout_titles : List[str]\n      This is the ground truth we are trying to predict --> issue titles\n\n    Returns\n    -------\n    bleu : float\n      The BLEU Score\n\n    """"""\n    actual, predicted = list(), list()\n    assert len(holdout_bodies) == len(holdout_titles)\n    num_examples = len(holdout_bodies)\n\n    logging.warning(\'Generating predictions.\')\n    # step over the whole set TODO: parallelize this\n    for i in tqdm_notebook(range(num_examples)):\n      _, yhat = self.generate_issue_title(holdout_bodies[i])\n\n      actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n      predicted.append(self.pp_title.process_text([yhat])[0])\n\n    # calculate BLEU score\n    logging.warning(\'Calculating BLEU.\')\n    #must be careful with nltk api for corpus_bleu!,\n    # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n    # erroneous results.\n    bleu = corpus_bleu([[a] for a in actual], predicted)\n    return bleu\n'"
libs/pipeline_model/pipeline_model/__init__.py,1,"b'from grpc.beta import implementations\n#import tensorflow as tf\n# These are generated from the TF serving source \n#   or copied from various places such as the following:\n#     https://github.com/Vetal1977/tf_serving_example\n#     https://github.com/tobegit3hub/tensorflow_template_application\nfrom tensorflow_serving.apis import predict_pb2, prediction_service_pb2\nfrom tensorflow.core.framework import tensor_pb2, tensor_shape_pb2, types_pb2\n\n__version__ = ""1.0.8""\n\n# TODO:  Add convenience methods for the following techniques:\n#   https://towardsdatascience.com/tensorflow-serving-client-make-it-slimmer-and-faster-b3e5f71208fb\n#   https://medium.com/@stianlindpetlund/tensorflow-serving-101-pt-2-682eaf7469e7\n#   https://github.com/davyzhang/dict-to-protobuf\n#  \n# TODO:  Add mock tensorflow serving server described here: \n#    https://medium.com/@stianlindpetlund/tensorflow-serving-101-pt-2-682eaf7469e7\n\nclass TensorFlowServingModel():\n   \n# TODO:  Don\'t expose any of these...\n#        (They\'re all internal to the model server runtime.) \n    def __init__(self,\n                 host: str,\n                 port: int,\n                 model_name: str,\n                 model_signature_name=None,\n                 timeout_seconds=5.0): # 5 second timeout\n\n        self._host = host\n        self._port = port\n        self._model_name = model_name\n        self._model_signature_name = model_signature_name\n        self._timeout_seconds = timeout_seconds\n\n\n    def predict(self,\n                input_string_to_tensor_dict):\n\n        channel = implementations.insecure_channel(self._host,\n                                                   self._port)\n\n        stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n       \n        # Transform input str::nparray dict into TensorFlow PredictRequest/tensors\n        tf_request = predict_pb2.PredictRequest()\n        tf_request.model_spec.name = self._model_name\n        if self._model_signature_name:\n            tf_request.model_spec.signature_name = self._model_signature_name\n        # We assume only a single version per model is running in this model server.\n        # tf_request.model_spec.version.value = ...\n\n        for input_str_key, input_tensor in input_string_to_tensor_dict.items():\n            tf_request.inputs[input_str_key].CopyFrom(input_tensor)\n\n        # Call TensorFlow Serving Predict\n        response = stub.Predict(tf_request, self._timeout_seconds)\n\n        # Return tensor dict\n        output_string_to_tensor_dict = {}\n        for output_str_key, _ in response.outputs.items():\n            output_string_to_tensor_dict[output_str_key] = repsonse.outputs[output_str_key]\n               #tf.make_ndarray(response.outputs[output_str_key])\n\n        return output_string_to_tensor_dict\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/base.py,31,"b'""""""\nThis file was scratch work. Check out main.py for refactored version.\n\nWeird phenomenon: This file runs about three times slower than the main.py\nversion, but not sure why. The main parts of the code are similar.\nNeeds more investigation.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nfrom scipy import ndimage\nfrom scipy.misc import imresize\nimport time\nimport os\n\n# GET DATA\nnum_labels = 2\npixel_depth = 255.0\nresized_height = 64\nresized_width = 64\nfolder = \'train/\'\nfiles = [folder + f for f in os.listdir(folder)]\n\n# QUEUE GRAPH\nbatch_size = 32\ncapacity = 2000\nnum_channels = 3\nqueue_graph = tf.Graph()\nwith queue_graph.as_default():\n    with queue_graph.device(\'/cpu:0\'):\n        filename_queue = tf.train.string_input_producer(files)\n        reader = tf.WholeFileReader()\n        key, value = reader.read(filename_queue)\n        image = tf.image.decode_jpeg(value, channels=num_channels)\n        image = (tf.cast(image, tf.float32) - 128.0) / 255.0\n        resized = tf.image.resize_images([image], resized_height, resized_width)\n        image_batch, label_batch = tf.train.batch(\n                                        [resized, key],\n                                        batch_size=batch_size,\n                                        capacity=capacity)\n        image_squeezed = tf.squeeze(image_batch)\n        batch_data = tf.identity(image_squeezed, name=\'batch_data\')\n        batch_labels = tf.identity(label_batch)\n\n# TODO: Replace with reading in a GraphDef or from another file\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    input_p = tf.placeholder(tf.float32, [None, resized_height, resized_width,\n                             num_channels], name=\'input\')\n    label_p = tf.placeholder(tf.int32, [None], name=\'label\')\n    one_hot_labels = tf.one_hot(label_p, num_labels)\n    shape = input_p.get_shape().as_list()\n    reshaped = tf.reshape(input_p, [-1, shape[1] * shape[2] * shape[3]])\n    weights = tf.Variable(tf.truncated_normal([shape[1] * shape[2] * shape[3],\n                                              num_labels], stddev=0.1))\n    bias = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n    logits = tf.matmul(reshaped, weights) + bias\n    loss = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_labels,\n                                                name=\'loss\'))\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n    init = tf.initialize_all_variables()\n\n    model_prediction = tf.nn.softmax(logits)\n    label_prediction = tf.argmax(model_prediction, 1, name=\'predicted_label\')\n    correct_prediction = tf.equal(label_prediction, tf.argmax(one_hot_labels,\n                                                              1))\n    model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nqueue_sess = tf.Session(graph=queue_graph)\ntrain_sess = tf.Session(graph=train_graph)\ntrain_sess.run(init)\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=queue_sess, coord=coord)\ntry:\n    time_list = []\n    while not coord.should_stop():\n        t_start = time.clock()\n        data_input, labels = queue_sess.run([image_squeezed, batch_labels])\n        labels = [\n            1\n            if b\'dog\' in s\n            else 0\n            for s in labels\n        ]\n        feed_dict = {input_p: data_input, label_p: labels}\n        loss_val, _, acc = train_sess.run([loss, train, model_accuracy],\n                                          feed_dict=feed_dict)\n        t_end = time.clock()\n        time_list.append(t_end - t_start)\n        if len(time_list) >= 30:\n            print(sum(time_list) / len(time_list))\n            time_list = []\nexcept tf.errors.OutOfRangeError:\n    print(\'Done training -- epoch limit reached\')\nfinally:\n    # When done, ask the threads to stop\n    coord.request_stop()\n\ncoord.join(threads)\nqueue_sess.close()\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/models.py,23,"b'import tensorflow as tf\n\n\ndef create_model_graph(height, width, channels, num_labels):\n    """"""\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        input_p = tf.placeholder(tf.float32, [None, height, width, channels],\n                                 name=\'input\')\n        label_p = tf.placeholder(tf.int32, [None], name=\'label\')\n        one_hot_labels = tf.one_hot(label_p, num_labels)\n        shape = input_p.get_shape().as_list()\n        num_pixels = shape[1] * shape[2] * shape[3]\n        reshaped = tf.reshape(input_p, [-1, num_pixels])\n        weights = tf.Variable(tf.truncated_normal([num_pixels, num_labels],\n                                                  stddev=0.1))\n        bias = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n        logits = tf.matmul(reshaped, weights) + bias\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_labels,\n                                                    name=\'loss\'))\n        train = tf.train.GradientDescentOptimizer(0.01).minimize(loss,\n                                                                 name=\'train\')\n        init = tf.initialize_all_variables()\n\n        model_prediction = tf.nn.softmax(logits)\n        label_prediction = tf.argmax(model_prediction, 1,\n                                     name=\'predicted_label\')\n        correct_prediction = tf.equal(label_prediction,\n                                      tf.argmax(one_hot_labels, 1))\n        model_accuracy = tf.reduce_mean(tf.cast(correct_prediction,\n                                                tf.float32), name=\'accuracy\')\n    return graph\n\n\ndef generic_supervised_graph(input_type, input_shape, label_type, label_shape):\n    """"""\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        input_p = tf.placeholder(input_type, input_shape, name=\'input\')\n        label_p = tf.placeholder(label_type, label_shape, name=\'label\')\n\n        loss = tf.identity(input_p, name=\'loss\')\n        train = tf.train.GradientDescentOptimizer(0.01).minimize(loss,\n                                                                 name=\'train\')\n    return graph\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/queues.py,15,"b'import tensorflow as tf\nimport numpy as np\nimport imghdr\n\n\ndef create_queue_reader(files):\n    """"""\n    Given a string Tensor of filenames, creates a filename queue and\n    `WholeFileReader`. Returns the key, value tensors returned from the\n    `Reader.read()` method\n\n    Args:\n        files: String Tensor of file paths\n\n    Returns:\n        key, value tensors from the `tf.Reader.read()` method\n    """"""\n    filename_queue = tf.train.string_input_producer(files)\n    reader = tf.WholeFileReader()\n    return reader.read(filename_queue)\n\n\ndef clean_image_data(image, pixel_depth, height, width):\n    """"""\n    Cleans data from an image `Tensor`, and returns a new `Tensor`\n\n    Args\n    """"""\n    norm = (tf.cast(image, tf.float32) - (pixel_depth / 2)) / pixel_depth\n    resized = tf.image.resize_images([norm], height, width)\n    return resized\n\n\ndef is_jpeg(filename, data):\n    return imghdr.what(filename, data) == \'jpeg\'\n\n\ndef is_png(filename, data):\n    return imghdr.what(filename, data) == \'png\'\n\n\ndef has_jpeg_ext(filename):\n    return np.array(filename.endswith(b\'.jpeg\') or filename.endswith(b\'.jpg\'))\n\n\ndef has_png_ext(filename):\n    return np.array(filename.endswith(b\'.png\'))\n\n\ndef create_image_queue_graph(files, pixel_depth, height, width, channels,\n                             batch_size, capacity):\n    """"""\n    Creates and returns a TensorFlow graph containing a configured queue to\n    pull in data\n    """"""\n    graph = tf.Graph()\n    with graph.as_default():\n        with graph.device(\'/cpu:0\'):\n            key, value = create_queue_reader(files)\n            # image = tf.image.decode_jpeg(value, channels=channels)\n            is_jpeg = tf.py_func(has_jpeg_ext, [key], [tf.bool])\n            image = tf.cond(is_jpeg[0],\n                            lambda: tf.image.decode_jpeg(value,\n                                                         channels=channels),\n                            lambda: tf.image.decode_png(value,\n                                                        channels=channels)\n                            )\n            cleaned = clean_image_data(image, pixel_depth, height, width)\n            image_batch, label_batch = tf.train.batch(\n                [cleaned, key],\n                batch_size=batch_size,\n                capacity=capacity)\n            image_squeezed = tf.squeeze(image_batch)\n            batch_data = tf.identity(image_squeezed, name=\'batch_data\')\n            batch_labels = tf.identity(label_batch, name=\'batch_labels\')\n    return graph\n'"
stream/jvm/src/main/java/io/pipeline/tensorflow/consumer/src/train.py,13,"b'import tensorflow as tf\nimport time\n\n\ndef get_op(graph, name):\n    """"""\n    Returns the operation(s) in `graph` with `name`\n    """"""\n    return graph.get_operation_by_name(name)\n\n\ndef get_tensor(graph, name, output_idx=0):\n    """"""\n    Returns the output of the operation in `graph` with `name`\n    """"""\n    return get_op(graph, name).outputs[output_idx]\n\n\ndef op_in_types(op, types):\n    """"""\n\n    """"""\n    if type(op) == tf.Tensor:\n        op_type = op.op.type\n    elif type(op) == tf.Operation:\n        op_type = op.type\n    else:\n        raise TypeError(\'Object being checked must be of type `tf.Tensor` or \'\n                        \'`tf.Operation`\')\n    return op in types, op_type\n\n\ndef check_op_type(op, types):\n    is_valid, actual_type = op_in_types(op, types)\n    if is_valid:\n        return\n    else:\n        raise TypeError(\'Expected node \\\'{}\\\' to be in types {}\'\n                        \', got type {}\'.format([op.name, types, actual_type]))\n\n\ndef create_tensor_dict(graph, names, types=None):\n    tensor_dict = {}\n    for name in names:\n        tensor = get_tensor(graph, name)\n        if types is not None:\n            check_op_type(tensor, types)\n        tensor_dict[name] = tensor\n    return tensor_dict\n\n\ndef create_op_dict(graph, names, types=None):\n    op_dict = {}\n    for name in names:\n        op = get_op(graph, name)\n        if types is not None:\n            check_op_type(op, types)\n        op_dict[name] = op\n    return op_dict\n\n\ndef create_placeholder_dict(graph, placeholder_pairs):\n    """"""\n    Creates a dictionary mapping string names to placeholder operations.\n\n    Args:\n        graph: `tf.Graph` object containing operations\n        placeholder_pairs: list of tuples\n\n    Returns:\n        Dictionary mapping string:`tf.placeholder`\n    """"""\n    names = [\n        p[1]\n        for\n        p\n        in\n        placeholder_pairs\n        ]\n    return create_tensor_dict(graph, names, types=[\'Placeholder\'])\n\n\ndef get_init_op(graph):\n    return get_op(graph, \'init\')\n\n\ndef add_variable_summaries(name, op):\n    """"""\n    TODO: Add mean, standard deviation, min, max, histogram\n    """"""\n    pass\n\n\ndef add_summary_ops(graph, log_ops):\n    """"""\n    """"""\n    with graph.as_default():\n        for name in log_ops:\n            tf.scalar_summary(name, log_ops[name])\n\n\ndef add_merged_summaries(graph):\n    """"""\n    """"""\n    with graph.as_default():\n        return tf.merge_all_summaries()\n\n\ndef train_model(queue_graph, model_graph, placeholder_list, train_list,\n                log_list):\n    """"""\n    TODO: Documentation\n    TODO: Clean up the all of the `get_operation_by_name()` lines\n    TODO: TensorBoard\n    """"""\n    batch_data = get_tensor(queue_graph, \'batch_data\')\n    batch_labels = get_tensor(queue_graph, \'batch_labels\')\n    init = get_init_op(model_graph)\n    placeholder_ops = create_tensor_dict(model_graph, placeholder_list)\n    train_ops = create_op_dict(model_graph, train_list)\n    train = train_ops[\'train\']\n    log_ops = create_tensor_dict(model_graph, log_list)\n    add_summary_ops(model_graph, log_ops)\n    summaries = add_merged_summaries(model_graph)\n\n    queue_sess = tf.Session(graph=queue_graph)\n    model_sess = tf.Session(graph=model_graph)\n    model_sess.run(init)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=queue_sess, coord=coord)\n    try:\n        time_list = []\n        while not coord.should_stop():\n            t_start = time.clock()\n            data_input, labels = queue_sess.run([batch_data, batch_labels])\n            labels = [\n                1\n                if b\'dog\' in s\n                else 0\n                for s in labels\n                ]\n            feed_dict = {placeholder_ops[placeholder_list[0]]: data_input,\n                         placeholder_ops[placeholder_list[1]]: labels}\n            summ, _ = model_sess.run([summaries, train],\n                                     feed_dict=feed_dict)\n            t_end = time.clock()\n            time_list.append(t_end - t_start)\n            if len(time_list) >= 30:\n                print(sum(time_list) / len(time_list))\n                time_list = []\n    except tf.errors.OutOfRangeError:\n        print(\'Done training -- epoch limit reached\')\n    finally:\n        # When done, ask the threads to stop\n        coord.request_stop()\n    coord.join(threads)\n    queue_sess.close()\n    model_sess.close()'"
