file_path,api_count,code
BERT_NER.py,78,"b'#! usr/bin/env python3\r\n# -*- coding:utf-8 -*-\r\n""""""\r\nCopyright 2018 The Google AI Language Team Authors.\r\nBASED ON Google_BERT.\r\n@Author:zhoukaiyin\r\nAdjust code for chinese ner\r\n""""""\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\nimport os\r\nfrom bert import modeling\r\nfrom bert import optimization\r\nfrom bert import tokenization\r\nimport tensorflow as tf\r\nfrom sklearn.metrics import f1_score,precision_score,recall_score\r\nfrom tensorflow.python.ops import math_ops\r\nimport tf_metrics\r\nimport pickle\r\nflags = tf.flags\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string(\r\n    ""data_dir"", None,\r\n    ""The input datadir."",\r\n)\r\n\r\nflags.DEFINE_string(\r\n    ""bert_config_file"", None,\r\n    ""The config json file corresponding to the pre-trained BERT model.""\r\n)\r\n\r\nflags.DEFINE_string(\r\n    ""task_name"", ""NER"", ""The name of the task to train.""\r\n)\r\n\r\nflags.DEFINE_string(\r\n    ""output_dir"", None,\r\n    ""The output directory where the model checkpoints will be written.""\r\n)\r\n\r\n## Other parameters\r\nflags.DEFINE_string(\r\n    ""init_checkpoint"", None,\r\n    ""Initial checkpoint (usually from a pre-trained BERT model).""\r\n)\r\n\r\nflags.DEFINE_bool(\r\n    ""do_lower_case"", True,\r\n    ""Whether to lower case the input text.""\r\n)\r\n\r\nflags.DEFINE_integer(\r\n    ""max_seq_length"", 128,\r\n    ""The maximum total input sequence length after WordPiece tokenization.""\r\n)\r\n\r\nflags.DEFINE_bool(\r\n    ""do_train"", True,\r\n    ""Whether to run training.""\r\n)\r\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\r\n\r\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\r\n\r\nflags.DEFINE_bool(""do_predict"", False,""Whether to run the model in inference mode on the test set."")\r\n\r\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\r\n\r\nflags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")\r\n\r\nflags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")\r\n\r\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")\r\n\r\nflags.DEFINE_float(""num_train_epochs"", 3.0, ""Total number of training epochs to perform."")\r\n\r\n\r\n\r\nflags.DEFINE_float(\r\n    ""warmup_proportion"", 0.1,\r\n    ""Proportion of training to perform linear learning rate warmup for. ""\r\n    ""E.g., 0.1 = 10% of training."")\r\n\r\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\r\n                     ""How often to save the model checkpoint."")\r\n\r\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\r\n                     ""How many steps to make in each estimator call."")\r\n\r\nflags.DEFINE_string(""vocab_file"", None,\r\n                    ""The vocabulary file that the BERT model was trained on."")\r\ntf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\r\nflags.DEFINE_integer(\r\n    ""num_tpu_cores"", 8,\r\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\r\n\r\nclass InputExample(object):\r\n    """"""A single training/test example for simple sequence classification.""""""\r\n\r\n    def __init__(self, guid, text, label=None):\r\n        """"""Constructs a InputExample.\r\n\r\n        Args:\r\n          guid: Unique id for the example.\r\n          text_a: string. The untokenized text of the first sequence. For single\r\n            sequence tasks, only this sequence must be specified.\r\n          label: (Optional) string. The label of the example. This should be\r\n            specified for train and dev examples, but not for test examples.\r\n        """"""\r\n        self.guid = guid\r\n        self.text = text\r\n        self.label = label\r\n\r\n\r\nclass InputFeatures(object):\r\n    """"""A single set of features of data.""""""\r\n\r\n    def __init__(self, input_ids, input_mask, segment_ids, label_ids,):\r\n        self.input_ids = input_ids\r\n        self.input_mask = input_mask\r\n        self.segment_ids = segment_ids\r\n        self.label_ids = label_ids\r\n        #self.label_mask = label_mask\r\n\r\n\r\nclass DataProcessor(object):\r\n    """"""Base class for data converters for sequence classification data sets.""""""\r\n\r\n    def get_train_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the train set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_labels(self):\r\n        """"""Gets the list of labels for this data set.""""""\r\n        raise NotImplementedError()\r\n\r\n    @classmethod\r\n    def _read_data(cls, input_file):\r\n        """"""Reads a BIO data.""""""\r\n        with open(input_file) as f:\r\n            lines = []\r\n            words = []\r\n            labels = []\r\n            for line in f:\r\n                contends = line.strip()\r\n                word = line.strip().split(\' \')[0]\r\n                label = line.strip().split(\' \')[-1]\r\n                if contends.startswith(""-DOCSTART-""):\r\n                    words.append(\'\')\r\n                    continue\r\n                # if len(contends) == 0 and words[-1] == \'\xe3\x80\x82\':\r\n                if len(contends) == 0:\r\n                    l = \' \'.join([label for label in labels if len(label) > 0])\r\n                    w = \' \'.join([word for word in words if len(word) > 0])\r\n                    lines.append([l, w])\r\n                    words = []\r\n                    labels = []\r\n                    continue\r\n                words.append(word)\r\n                labels.append(label)\r\n            return lines\r\n\r\n\r\nclass NerProcessor(DataProcessor):\r\n    def get_train_examples(self, data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""train.txt"")), ""train""\r\n        )\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""dev.txt"")), ""dev""\r\n        )\r\n\r\n    def get_test_examples(self,data_dir):\r\n        return self._create_example(\r\n            self._read_data(os.path.join(data_dir, ""test.txt"")), ""test"")\r\n\r\n\r\n    def get_labels(self):\r\n        # prevent potential bug for chinese text mixed with english text\r\n        # return [""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""[CLS]"",""[SEP]""]\r\n        return [""O"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""X"",""[CLS]"",""[SEP]""]\r\n\r\n    def _create_example(self, lines, set_type):\r\n        examples = []\r\n        for (i, line) in enumerate(lines):\r\n            guid = ""%s-%s"" % (set_type, i)\r\n            text = tokenization.convert_to_unicode(line[1])\r\n            label = tokenization.convert_to_unicode(line[0])\r\n            examples.append(InputExample(guid=guid, text=text, label=label))\r\n        return examples\r\n\r\n\r\ndef write_tokens(tokens,mode):\r\n    if mode==""test"":\r\n        path = os.path.join(FLAGS.output_dir, ""token_""+mode+"".txt"")\r\n        wf = open(path,\'a\')\r\n        for token in tokens:\r\n            if token!=""**NULL**"":\r\n                wf.write(token+\'\\n\')\r\n        wf.close()\r\n\r\ndef convert_single_example(ex_index, example, label_map, max_seq_length, tokenizer,mode):\r\n    textlist = example.text.split(\' \')\r\n    labellist = example.label.split(\' \')\r\n    tokens = []\r\n    labels = []\r\n    # print(textlist)\r\n    for i, word in enumerate(textlist):\r\n        token = tokenizer.tokenize(word)\r\n        # print(token)\r\n        tokens.extend(token)\r\n        label_1 = labellist[i]\r\n        # print(label_1)\r\n        for m in range(len(token)):\r\n            if m == 0:\r\n                labels.append(label_1)\r\n            else:\r\n                labels.append(""X"")\r\n        # print(tokens, labels)\r\n    # tokens = tokenizer.tokenize(example.text)\r\n    if len(tokens) >= max_seq_length - 1:\r\n        tokens = tokens[0:(max_seq_length - 2)]\r\n        labels = labels[0:(max_seq_length - 2)]\r\n    ntokens = []\r\n    segment_ids = []\r\n    label_ids = []\r\n    ntokens.append(""[CLS]"")\r\n    segment_ids.append(0)\r\n    # append(""O"") or append(""[CLS]"") not sure!\r\n    label_ids.append(label_map[""[CLS]""])\r\n    for i, token in enumerate(tokens):\r\n        ntokens.append(token)\r\n        segment_ids.append(0)\r\n        label_ids.append(label_map[labels[i]])\r\n    ntokens.append(""[SEP]"")\r\n    segment_ids.append(0)\r\n    # append(""O"") or append(""[SEP]"") not sure!\r\n    label_ids.append(label_map[""[SEP]""])\r\n    input_ids = tokenizer.convert_tokens_to_ids(ntokens)\r\n    input_mask = [1] * len(input_ids)\r\n    #label_mask = [1] * len(input_ids)\r\n    while len(input_ids) < max_seq_length:\r\n        input_ids.append(0)\r\n        input_mask.append(0)\r\n        segment_ids.append(0)\r\n        # we don\'t concerned about it!\r\n        label_ids.append(0)\r\n        ntokens.append(""**NULL**"")\r\n        #label_mask.append(0)\r\n    # print(len(input_ids))\r\n    assert len(input_ids) == max_seq_length\r\n    assert len(input_mask) == max_seq_length\r\n    assert len(segment_ids) == max_seq_length\r\n    assert len(label_ids) == max_seq_length\r\n    #assert len(label_mask) == max_seq_length\r\n\r\n    if ex_index < 5:\r\n        tf.logging.info(""*** Example ***"")\r\n        tf.logging.info(""guid: %s"" % (example.guid))\r\n        tf.logging.info(""tokens: %s"" % "" "".join(\r\n            [tokenization.printable_text(x) for x in tokens]))\r\n        tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\r\n        tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\r\n        tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\r\n        tf.logging.info(""label_ids: %s"" % "" "".join([str(x) for x in label_ids]))\r\n        #tf.logging.info(""label_mask: %s"" % "" "".join([str(x) for x in label_mask]))\r\n\r\n    feature = InputFeatures(\r\n        input_ids=input_ids,\r\n        input_mask=input_mask,\r\n        segment_ids=segment_ids,\r\n        label_ids=label_ids,\r\n        #label_mask = label_mask\r\n    )\r\n    write_tokens(ntokens,mode)\r\n    return feature\r\n\r\n\r\ndef filed_based_convert_examples_to_features(\r\n        examples, label_list, max_seq_length, tokenizer, output_file,mode=None\r\n):\r\n    label_map = {}\r\n    for (i, label) in enumerate(label_list,1):\r\n        label_map[label] = i\r\n    with open(\'./output/label2id.pkl\',\'wb\') as w:\r\n        pickle.dump(label_map,w)\r\n\r\n    writer = tf.python_io.TFRecordWriter(output_file)\r\n    for (ex_index, example) in enumerate(examples):\r\n        if ex_index % 5000 == 0:\r\n            tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\r\n        feature = convert_single_example(ex_index, example, label_map, max_seq_length, tokenizer,mode)\r\n        \r\n        def create_int_feature(values):\r\n            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\r\n            return f\r\n\r\n        features = collections.OrderedDict()\r\n        features[""input_ids""] = create_int_feature(feature.input_ids)\r\n        features[""input_mask""] = create_int_feature(feature.input_mask)\r\n        features[""segment_ids""] = create_int_feature(feature.segment_ids)\r\n        features[""label_ids""] = create_int_feature(feature.label_ids)\r\n        #features[""label_mask""] = create_int_feature(feature.label_mask)\r\n        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\r\n        writer.write(tf_example.SerializeToString())\r\n\r\n\r\ndef file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\r\n    name_to_features = {\r\n        ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        ""label_ids"": tf.FixedLenFeature([seq_length], tf.int64),\r\n        # ""label_ids"":tf.VarLenFeature(tf.int64),\r\n        #""label_mask"": tf.FixedLenFeature([seq_length], tf.int64),\r\n    }\r\n\r\n    def _decode_record(record, name_to_features):\r\n        example = tf.parse_single_example(record, name_to_features)\r\n        for name in list(example.keys()):\r\n            t = example[name]\r\n            if t.dtype == tf.int64:\r\n                t = tf.to_int32(t)\r\n            example[name] = t\r\n        return example\r\n\r\n    def input_fn(params):\r\n        batch_size = params[""batch_size""]\r\n        d = tf.data.TFRecordDataset(input_file)\r\n        if is_training:\r\n            d = d.repeat()\r\n            d = d.shuffle(buffer_size=100)\r\n        d = d.apply(tf.contrib.data.map_and_batch(\r\n            lambda record: _decode_record(record, name_to_features),\r\n            batch_size=batch_size,\r\n            drop_remainder=drop_remainder\r\n        ))\r\n        return d\r\n    return input_fn\r\n\r\n\r\ndef create_model(bert_config, is_training, input_ids, input_mask,\r\n                 segment_ids, labels, num_labels, use_one_hot_embeddings):\r\n    model = modeling.BertModel(\r\n        config=bert_config,\r\n        is_training=is_training,\r\n        input_ids=input_ids,\r\n        input_mask=input_mask,\r\n        token_type_ids=segment_ids,\r\n        use_one_hot_embeddings=use_one_hot_embeddings\r\n    )\r\n\r\n    output_layer = model.get_sequence_output()\r\n\r\n    hidden_size = output_layer.shape[-1].value\r\n\r\n    output_weight = tf.get_variable(\r\n        ""output_weights"", [num_labels, hidden_size],\r\n        initializer=tf.truncated_normal_initializer(stddev=0.02)\r\n    )\r\n    output_bias = tf.get_variable(\r\n        ""output_bias"", [num_labels], initializer=tf.zeros_initializer()\r\n    )\r\n    with tf.variable_scope(""loss""):\r\n        if is_training:\r\n            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\r\n        output_layer = tf.reshape(output_layer, [-1, hidden_size])\r\n        logits = tf.matmul(output_layer, output_weight, transpose_b=True)\r\n        logits = tf.nn.bias_add(logits, output_bias)\r\n        logits = tf.reshape(logits, [-1, FLAGS.max_seq_length, 11])\r\n        # mask = tf.cast(input_mask,tf.float32)\r\n        # loss = tf.contrib.seq2seq.sequence_loss(logits,labels,mask)\r\n        # return (loss, logits, predict)\r\n        ##########################################################################\r\n        log_probs = tf.nn.log_softmax(logits, axis=-1)\r\n        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\r\n        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\r\n        loss = tf.reduce_sum(per_example_loss)\r\n        probabilities = tf.nn.softmax(logits, axis=-1)\r\n        predict = tf.argmax(probabilities,axis=-1)\r\n        return (loss, per_example_loss, logits,predict)\r\n        ##########################################################################\r\n        \r\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\r\n                     num_train_steps, num_warmup_steps, use_tpu,\r\n                     use_one_hot_embeddings):\r\n    def model_fn(features, labels, mode, params):\r\n        tf.logging.info(""*** Features ***"")\r\n        for name in sorted(features.keys()):\r\n            tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\r\n        input_ids = features[""input_ids""]\r\n        input_mask = features[""input_mask""]\r\n        segment_ids = features[""segment_ids""]\r\n        label_ids = features[""label_ids""]\r\n        #label_mask = features[""label_mask""]\r\n        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n\r\n        (total_loss,  per_example_loss,logits,predicts) = create_model(\r\n            bert_config, is_training, input_ids, input_mask,segment_ids, label_ids,\r\n            num_labels, use_one_hot_embeddings)\r\n        tvars = tf.trainable_variables()\r\n        scaffold_fn = None\r\n        if init_checkpoint:\r\n            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,init_checkpoint)\r\n            tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n            if use_tpu:\r\n                def tpu_scaffold():\r\n                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n                    return tf.train.Scaffold()\r\n                scaffold_fn = tpu_scaffold\r\n            else:\r\n                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n        tf.logging.info(""**** Trainable Variables ****"")\r\n\r\n        for var in tvars:\r\n            init_string = """"\r\n            if var.name in initialized_variable_names:\r\n                init_string = "", *INIT_FROM_CKPT*""\r\n            tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\r\n                            init_string)\r\n        output_spec = None\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            train_op = optimization.create_optimizer(\r\n                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=total_loss,\r\n                train_op=train_op,\r\n                scaffold_fn=scaffold_fn)\r\n        elif mode == tf.estimator.ModeKeys.EVAL:\r\n            \r\n            def metric_fn(per_example_loss, label_ids, logits):\r\n            # def metric_fn(label_ids, logits):\r\n                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\r\n                precision = tf_metrics.precision(label_ids,predictions,11,[2,3,4,5,6,7],average=""macro"")\r\n                recall = tf_metrics.recall(label_ids,predictions,11,[2,3,4,5,6,7],average=""macro"")\r\n                f = tf_metrics.f1(label_ids,predictions,11,[2,3,4,5,6,7],average=""macro"")\r\n                #\r\n                return {\r\n                    ""eval_precision"":precision,\r\n                    ""eval_recall"":recall,\r\n                    ""eval_f"": f,\r\n                    #""eval_loss"": loss,\r\n                }\r\n            eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\r\n            # eval_metrics = (metric_fn, [label_ids, logits])\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode=mode,\r\n                loss=total_loss,\r\n                eval_metrics=eval_metrics,\r\n                scaffold_fn=scaffold_fn)\r\n        else:\r\n            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n                mode = mode,predictions= predicts,scaffold_fn=scaffold_fn\r\n            )\r\n        return output_spec\r\n    return model_fn\r\n\r\n\r\ndef main(_):\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    processors = {\r\n        ""ner"": NerProcessor\r\n    }\r\n    if not FLAGS.do_train and not FLAGS.do_eval:\r\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\r\n\r\n    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\r\n\r\n    if FLAGS.max_seq_length > bert_config.max_position_embeddings:\r\n        raise ValueError(\r\n            ""Cannot use sequence length %d because the BERT model ""\r\n            ""was only trained up to sequence length %d"" %\r\n            (FLAGS.max_seq_length, bert_config.max_position_embeddings))\r\n\r\n    task_name = FLAGS.task_name.lower()\r\n    if task_name not in processors:\r\n        raise ValueError(""Task not found: %s"" % (task_name))\r\n    processor = processors[task_name]()\r\n\r\n    label_list = processor.get_labels()\r\n\r\n    tokenizer = tokenization.FullTokenizer(\r\n        vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\r\n    tpu_cluster_resolver = None\r\n    if FLAGS.use_tpu and FLAGS.tpu_name:\r\n        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\r\n\r\n    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\n\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        cluster=tpu_cluster_resolver,\r\n        master=FLAGS.master,\r\n        model_dir=FLAGS.output_dir,\r\n        save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n        tpu_config=tf.contrib.tpu.TPUConfig(\r\n            iterations_per_loop=FLAGS.iterations_per_loop,\r\n            num_shards=FLAGS.num_tpu_cores,\r\n            per_host_input_for_training=is_per_host))\r\n\r\n    train_examples = None\r\n    num_train_steps = None\r\n    num_warmup_steps = None\r\n\r\n    if FLAGS.do_train:\r\n        train_examples = processor.get_train_examples(FLAGS.data_dir)\r\n        num_train_steps = int(\r\n            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\r\n        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\r\n\r\n    model_fn = model_fn_builder(\r\n        bert_config=bert_config,\r\n        num_labels=len(label_list)+1,\r\n        init_checkpoint=FLAGS.init_checkpoint,\r\n        learning_rate=FLAGS.learning_rate,\r\n        num_train_steps=num_train_steps,\r\n        num_warmup_steps=num_warmup_steps,\r\n        use_tpu=FLAGS.use_tpu,\r\n        use_one_hot_embeddings=FLAGS.use_tpu)\r\n\r\n    estimator = tf.contrib.tpu.TPUEstimator(\r\n        use_tpu=FLAGS.use_tpu,\r\n        model_fn=model_fn,\r\n        config=run_config,\r\n        train_batch_size=FLAGS.train_batch_size,\r\n        eval_batch_size=FLAGS.eval_batch_size,\r\n        predict_batch_size=FLAGS.predict_batch_size)\r\n\r\n    if FLAGS.do_train:\r\n        train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")\r\n        filed_based_convert_examples_to_features(\r\n            train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\r\n        tf.logging.info(""***** Running training *****"")\r\n        tf.logging.info(""  Num examples = %d"", len(train_examples))\r\n        tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\r\n        tf.logging.info(""  Num steps = %d"", num_train_steps)\r\n        train_input_fn = file_based_input_fn_builder(\r\n            input_file=train_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=True,\r\n            drop_remainder=True)\r\n        estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n    if FLAGS.do_eval:\r\n        eval_examples = processor.get_dev_examples(FLAGS.data_dir)\r\n        eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")\r\n        filed_based_convert_examples_to_features(\r\n            eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\r\n\r\n        tf.logging.info(""***** Running evaluation *****"")\r\n        tf.logging.info(""  Num examples = %d"", len(eval_examples))\r\n        tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)\r\n        eval_steps = None\r\n        if FLAGS.use_tpu:\r\n            eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\r\n        eval_drop_remainder = True if FLAGS.use_tpu else False\r\n        eval_input_fn = file_based_input_fn_builder(\r\n            input_file=eval_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=False,\r\n            drop_remainder=eval_drop_remainder)\r\n        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\r\n        output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")\r\n        with open(output_eval_file, ""w"") as writer:\r\n            tf.logging.info(""***** Eval results *****"")\r\n            for key in sorted(result.keys()):\r\n                tf.logging.info(""  %s = %s"", key, str(result[key]))\r\n                writer.write(""%s = %s\\n"" % (key, str(result[key])))\r\n    if FLAGS.do_predict:\r\n        token_path = os.path.join(FLAGS.output_dir, ""token_test.txt"")\r\n        with open(\'./output/label2id.pkl\',\'rb\') as rf:\r\n            label2id = pickle.load(rf)\r\n            id2label = {value:key for key,value in label2id.items()}\r\n        if os.path.exists(token_path):\r\n            os.remove(token_path)\r\n        predict_examples = processor.get_test_examples(FLAGS.data_dir)\r\n\r\n        predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")\r\n        filed_based_convert_examples_to_features(predict_examples, label_list,\r\n                                                FLAGS.max_seq_length, tokenizer,\r\n                                                predict_file,mode=""test"")\r\n                            \r\n        tf.logging.info(""***** Running prediction*****"")\r\n        tf.logging.info(""  Num examples = %d"", len(predict_examples))\r\n        tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)\r\n        if FLAGS.use_tpu:\r\n            # Warning: According to tpu_estimator.py Prediction on TPU is an\r\n            # experimental feature and hence not supported here\r\n            raise ValueError(""Prediction in TPU not supported"")\r\n        predict_drop_remainder = True if FLAGS.use_tpu else False\r\n        predict_input_fn = file_based_input_fn_builder(\r\n            input_file=predict_file,\r\n            seq_length=FLAGS.max_seq_length,\r\n            is_training=False,\r\n            drop_remainder=predict_drop_remainder)\r\n\r\n        result = estimator.predict(input_fn=predict_input_fn)\r\n        output_predict_file = os.path.join(FLAGS.output_dir, ""label_test.txt"")\r\n        with open(output_predict_file,\'w\') as writer:\r\n            for prediction in result:\r\n                output_line = ""\\n"".join(id2label[id] for id in prediction if id!=0) + ""\\n""\r\n                writer.write(output_line)\r\n\r\nif __name__ == ""__main__"":\r\n    flags.mark_flag_as_required(""data_dir"")\r\n    flags.mark_flag_as_required(""task_name"")\r\n    flags.mark_flag_as_required(""vocab_file"")\r\n    flags.mark_flag_as_required(""bert_config_file"")\r\n    flags.mark_flag_as_required(""output_dir"")\r\n    tf.app.run()\r\n\r\n\r\n'"
tf_metrics.py,22,"b'""""""\r\nMulticlass\r\nfrom: \r\nhttps://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\r\n\r\n""""""\r\n\r\n__author__ = ""Guillaume Genthial""\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.metrics_impl import _streaming_confusion_matrix\r\n\r\n\r\ndef precision(labels, predictions, num_classes, pos_indices=None,\r\n              weights=None, average=\'micro\'):\r\n    """"""Multi-class precision metric for Tensorflow\r\n    Parameters\r\n    ----------\r\n    labels : Tensor of tf.int32 or tf.int64\r\n        The true labels\r\n    predictions : Tensor of tf.int32 or tf.int64\r\n        The predictions, same shape as labels\r\n    num_classes : int\r\n        The number of classes\r\n    pos_indices : list of int, optional\r\n        The indices of the positive classes, default is all\r\n    weights : Tensor of tf.int32, optional\r\n        Mask, must be of compatible shape with labels\r\n    average : str, optional\r\n        \'micro\': counts the total number of true positives, false\r\n            positives, and false negatives for the classes in\r\n            `pos_indices` and infer the metric from it.\r\n        \'macro\': will compute the metric separately for each class in\r\n            `pos_indices` and average. Will not account for class\r\n            imbalance.\r\n        \'weighted\': will compute the metric separately for each class in\r\n            `pos_indices` and perform a weighted average by the total\r\n            number of true labels for each class.\r\n    Returns\r\n    -------\r\n    tuple of (scalar float Tensor, update_op)\r\n    """"""\r\n    cm, op = _streaming_confusion_matrix(\r\n        labels, predictions, num_classes, weights)\r\n    pr, _, _ = metrics_from_confusion_matrix(\r\n        cm, pos_indices, average=average)\r\n    op, _, _ = metrics_from_confusion_matrix(\r\n        op, pos_indices, average=average)\r\n    return (pr, op)\r\n\r\n\r\ndef recall(labels, predictions, num_classes, pos_indices=None, weights=None,\r\n           average=\'micro\'):\r\n    """"""Multi-class recall metric for Tensorflow\r\n    Parameters\r\n    ----------\r\n    labels : Tensor of tf.int32 or tf.int64\r\n        The true labels\r\n    predictions : Tensor of tf.int32 or tf.int64\r\n        The predictions, same shape as labels\r\n    num_classes : int\r\n        The number of classes\r\n    pos_indices : list of int, optional\r\n        The indices of the positive classes, default is all\r\n    weights : Tensor of tf.int32, optional\r\n        Mask, must be of compatible shape with labels\r\n    average : str, optional\r\n        \'micro\': counts the total number of true positives, false\r\n            positives, and false negatives for the classes in\r\n            `pos_indices` and infer the metric from it.\r\n        \'macro\': will compute the metric separately for each class in\r\n            `pos_indices` and average. Will not account for class\r\n            imbalance.\r\n        \'weighted\': will compute the metric separately for each class in\r\n            `pos_indices` and perform a weighted average by the total\r\n            number of true labels for each class.\r\n    Returns\r\n    -------\r\n    tuple of (scalar float Tensor, update_op)\r\n    """"""\r\n    cm, op = _streaming_confusion_matrix(\r\n        labels, predictions, num_classes, weights)\r\n    _, re, _ = metrics_from_confusion_matrix(\r\n        cm, pos_indices, average=average)\r\n    _, op, _ = metrics_from_confusion_matrix(\r\n        op, pos_indices, average=average)\r\n    return (re, op)\r\n\r\n\r\ndef f1(labels, predictions, num_classes, pos_indices=None, weights=None,\r\n       average=\'micro\'):\r\n    return fbeta(labels, predictions, num_classes, pos_indices, weights,\r\n                 average)\r\n\r\n\r\ndef fbeta(labels, predictions, num_classes, pos_indices=None, weights=None,\r\n          average=\'micro\', beta=1):\r\n    """"""Multi-class fbeta metric for Tensorflow\r\n    Parameters\r\n    ----------\r\n    labels : Tensor of tf.int32 or tf.int64\r\n        The true labels\r\n    predictions : Tensor of tf.int32 or tf.int64\r\n        The predictions, same shape as labels\r\n    num_classes : int\r\n        The number of classes\r\n    pos_indices : list of int, optional\r\n        The indices of the positive classes, default is all\r\n    weights : Tensor of tf.int32, optional\r\n        Mask, must be of compatible shape with labels\r\n    average : str, optional\r\n        \'micro\': counts the total number of true positives, false\r\n            positives, and false negatives for the classes in\r\n            `pos_indices` and infer the metric from it.\r\n        \'macro\': will compute the metric separately for each class in\r\n            `pos_indices` and average. Will not account for class\r\n            imbalance.\r\n        \'weighted\': will compute the metric separately for each class in\r\n            `pos_indices` and perform a weighted average by the total\r\n            number of true labels for each class.\r\n    beta : int, optional\r\n        Weight of precision in harmonic mean\r\n    Returns\r\n    -------\r\n    tuple of (scalar float Tensor, update_op)\r\n    """"""\r\n    cm, op = _streaming_confusion_matrix(\r\n        labels, predictions, num_classes, weights)\r\n    _, _, fbeta = metrics_from_confusion_matrix(\r\n        cm, pos_indices, average=average, beta=beta)\r\n    _, _, op = metrics_from_confusion_matrix(\r\n        op, pos_indices, average=average, beta=beta)\r\n    return (fbeta, op)\r\n\r\n\r\ndef safe_div(numerator, denominator):\r\n    """"""Safe division, return 0 if denominator is 0""""""\r\n    numerator, denominator = tf.to_float(numerator), tf.to_float(denominator)\r\n    zeros = tf.zeros_like(numerator, dtype=numerator.dtype)\r\n    denominator_is_zero = tf.equal(denominator, zeros)\r\n    return tf.where(denominator_is_zero, zeros, numerator / denominator)\r\n\r\n\r\ndef pr_re_fbeta(cm, pos_indices, beta=1):\r\n    """"""Uses a confusion matrix to compute precision, recall and fbeta""""""\r\n    num_classes = cm.shape[0]\r\n    neg_indices = [i for i in range(num_classes) if i not in pos_indices]\r\n    cm_mask = np.ones([num_classes, num_classes])\r\n    cm_mask[neg_indices, neg_indices] = 0\r\n    diag_sum = tf.reduce_sum(tf.diag_part(cm * cm_mask))\r\n\r\n    cm_mask = np.ones([num_classes, num_classes])\r\n    cm_mask[:, neg_indices] = 0\r\n    tot_pred = tf.reduce_sum(cm * cm_mask)\r\n\r\n    cm_mask = np.ones([num_classes, num_classes])\r\n    cm_mask[neg_indices, :] = 0\r\n    tot_gold = tf.reduce_sum(cm * cm_mask)\r\n\r\n    pr = safe_div(diag_sum, tot_pred)\r\n    re = safe_div(diag_sum, tot_gold)\r\n    fbeta = safe_div((1. + beta**2) * pr * re, beta**2 * pr + re)\r\n\r\n    return pr, re, fbeta\r\n\r\n\r\ndef metrics_from_confusion_matrix(cm, pos_indices=None, average=\'micro\',\r\n                                  beta=1):\r\n    """"""Precision, Recall and F1 from the confusion matrix\r\n    Parameters\r\n    ----------\r\n    cm : tf.Tensor of type tf.int32, of shape (num_classes, num_classes)\r\n        The streaming confusion matrix.\r\n    pos_indices : list of int, optional\r\n        The indices of the positive classes\r\n    beta : int, optional\r\n        Weight of precision in harmonic mean\r\n    average : str, optional\r\n        \'micro\', \'macro\' or \'weighted\'\r\n    """"""\r\n    num_classes = cm.shape[0]\r\n    if pos_indices is None:\r\n        pos_indices = [i for i in range(num_classes)]\r\n\r\n    if average == \'micro\':\r\n        return pr_re_fbeta(cm, pos_indices, beta)\r\n    elif average in {\'macro\', \'weighted\'}:\r\n        precisions, recalls, fbetas, n_golds = [], [], [], []\r\n        for idx in pos_indices:\r\n            pr, re, fbeta = pr_re_fbeta(cm, [idx], beta)\r\n            precisions.append(pr)\r\n            recalls.append(re)\r\n            fbetas.append(fbeta)\r\n            cm_mask = np.zeros([num_classes, num_classes])\r\n            cm_mask[idx, :] = 1\r\n            n_golds.append(tf.to_float(tf.reduce_sum(cm * cm_mask)))\r\n\r\n        if average == \'macro\':\r\n            pr = tf.reduce_mean(precisions)\r\n            re = tf.reduce_mean(recalls)\r\n            fbeta = tf.reduce_mean(fbetas)\r\n            return pr, re, fbeta\r\n        if average == \'weighted\':\r\n            n_gold = tf.reduce_sum(n_golds)\r\n            pr_sum = sum(p * n for p, n in zip(precisions, n_golds))\r\n            pr = safe_div(pr_sum, n_gold)\r\n            re_sum = sum(r * n for r, n in zip(recalls, n_golds))\r\n            re = safe_div(re_sum, n_gold)\r\n            fbeta_sum = sum(f * n for f, n in zip(fbetas, n_golds))\r\n            fbeta = safe_div(fbeta_sum, n_gold)\r\n            return pr, re, fbeta\r\n\r\n    else:\r\n        raise NotImplementedError()'"
