file_path,api_count,code
examples/mxnetVisualDL/demo.py,0,"b'import numpy as np\nimport logging\n\n# Here we import LogWriter so that we can write log data while MXNet is training\nfrom visualdl import LogWriter\n\nimport mxnet as mx\n\nimport os\nimport gzip, struct\n\n# load mnist data\ndef read_data(label_url, image_url):\n    with gzip.open(os.path.join(\'data\',label_url)) as flbl:\n        struct.unpack("">II"", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(os.path.join(\'data\',image_url), \'rb\') as fimg:\n        _, _, rows, cols = struct.unpack("">IIII"", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n        image = image.reshape(image.shape[0], 1, 28, 28).astype(np.float32)/255\n    return (label, image)\n\n(train_lbl, train_img) = read_data(\n        \'train-labels-idx1-ubyte.gz\', \'train-images-idx3-ubyte.gz\')\n(test_lbl, test_img) = read_data(\n        \'t10k-labels-idx1-ubyte.gz\', \'t10k-images-idx3-ubyte.gz\')\n\nmnist = {\'train_data\':train_img, \'train_label\':train_lbl,\n            \'test_data\':test_img, \'test_label\':test_lbl}\n\n\nbatch_size = 100\n\n\n# Provide a folder to store data for log, model, image, etc. VisualDL\'s visualization will be\n# based on this folder.\nlogdir = ""./log""\n\n# Initialize a logger instance. Parameter \'sync_cycle\' means write a log every 10 operations on\n# memory.\nlogger = LogWriter(logdir, sync_cycle=10)\n\n# mark the components with \'train\' label.\nwith logger.mode(""train""):\n    # scalar0 is used to record scalar metrics while MXNet is training. We will record accuracy.\n    # In the visualization, we can see the accuracy is increasing as more training steps happen.\n    scalar0 = logger.scalar(""scalars/scalar0"")\n    image0 = logger.image(""images/image0"", 1)\n    histogram0 = logger.histogram(""histogram/histogram0"", num_buckets=100)\n\n# Record training steps\ncnt_step = 0\n\n\n# MXNet provides many callback interface. Here we define our own callback method and it is called\n# after every batch.\n# https://mxnet.incubator.apache.org/api/python/callback/callback.html\ndef add_scalar():\n    def _callback(param):\n        with logger.mode(""train""):\n            global cnt_step\n            # Here the value is the accuracy we want to record\n            # https://mxnet.incubator.apache.org/_modules/mxnet/callback.html\n            name_value = param.eval_metric.get_name_value()\n            for name, value in name_value:\n                scalar0.add_record(cnt_step, value)\n                cnt_step += 1\n    return _callback\n\ndef add_image_histogram():\n    def _callback(iter_no, sym, arg, aux):\n        image0.start_sampling()\n        weight = arg[\'fullyconnected1_weight\'].asnumpy()\n        shape = [100, 50]\n        data = weight.flatten()\n\n        image0.add_sample(shape, list(data))\n        histogram0.add_record(iter_no, list(data))\n\n        image0.finish_sampling()\n    return _callback\n\n\n# Start to build CNN in MXNet, train MNIST dataset. For more info, check MXNet\'s official website:\n# https://mxnet.incubator.apache.org/tutorials/python/mnist.html\n\nlogging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\n\ntrain_iter = mx.io.NDArrayIter(mnist[\'train_data\'], mnist[\'train_label\'], batch_size, shuffle=True)\nval_iter = mx.io.NDArrayIter(mnist[\'test_data\'], mnist[\'test_label\'], batch_size)\n\ndata = mx.sym.var(\'data\')\n# first conv layer\nconv1 = mx.sym.Convolution(data=data, kernel=(5, 5), num_filter=20)\ntanh1 = mx.sym.Activation(data=conv1, act_type=""tanh"")\npool1 = mx.sym.Pooling(data=tanh1, pool_type=""max"", kernel=(2, 2), stride=(2, 2))\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\ntanh2 = mx.sym.Activation(data=conv2, act_type=""tanh"")\npool2 = mx.sym.Pooling(data=tanh2, pool_type=""max"", kernel=(2, 2), stride=(2, 2))\n# first fullc layer\nflatten = mx.sym.flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\ntanh3 = mx.sym.Activation(data=fc1, act_type=""tanh"")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n# softmax loss\nlenet = mx.sym.SoftmaxOutput(data=fc2, name=\'softmax\')\n\n# create a trainable module on CPU\nlenet_model = mx.mod.Module(symbol=lenet, context=mx.cpu())\n\nmodel_prefix = \'output/mx_mlp\'\ncheckpoint = mx.callback.do_checkpoint(model_prefix)\n\n# train with the same\nlenet_model.fit(train_iter,\n                eval_data=val_iter,\n                optimizer=\'sgd\',\n                optimizer_params={\'learning_rate\': 0.1},\n                eval_metric=\'acc\',\n                # integrate our customized callback method\n                batch_end_callback=[add_scalar()],\n                epoch_end_callback=[add_image_histogram(), checkpoint],\n                num_epoch=2)\n\ntest_iter = mx.io.NDArrayIter(mnist[\'test_data\'], None, batch_size)\nprob = lenet_model.predict(test_iter)\ntest_iter = mx.io.NDArrayIter(mnist[\'test_data\'], mnist[\'test_label\'], batch_size)\n\n# predict accuracy for lenet\nacc = mx.metric.Accuracy()\nlenet_model.score(test_iter, acc)\nprint(acc)\n'"
examples/tensorflow/dataDeal.py,0,"b'import numpy as np\nimport os\n\nclass trainData:\n  def __init__(self, fileStr, batch_size):\n    self.fileStr = os.listdir(fileStr)\n    self.batch_size = batch_size\n    self.batch = []\n    self.cache = []\n    self.count_batch = 0\n    self.flag = 0\n    self.inc = -1\n    self.iter_num = 0\n\n    for s in self.fileStr:\n      f = open(fileStr+""/""+s, ""r"")\n      for line in f:\n        iter = (line.split("" "")[0], (np.asarray(line.split("" "")[1].split("",""), dtype=np.float32)))\n        if self.iter_num < self.batch_size:\n          self.batch.append(iter)\n          self.iter_num += 1\n        else:\n          self.cache.append(self.batch)\n          self.count_batch += 1\n          self.batch = [iter]\n          self.iter_num = 1 \n  \n    if self.batch:\n      supplement_batch = self.cache[0]\n      supplement_count = self.batch_size-len(self.batch)\n      self.batch.extend(supplement_batch[:supplement_count])\n      self.cache.append(self.batch)\n      self.count_batch += 1\n\n  def nextBatch(self):\n    if(self.inc + 1 == self.count_batch):\n      self.inc = -1\n    self.inc += 1\n    return self.cache[self.inc]\n  \n  def batchCount(self):\n    return self.count_batch\n\ndef oneHot(sLabel):\n    arr = np.zeros(2,np.float32)\n    arr[int(\'\'.join(sLabel))] = 1.0\n    return arr\n'"
examples/tensorflow/demo.py,34,"b'import argparse\nimport sys\nimport os\nimport json\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nsys.path.append(os.getcwd())\nfrom dataDeal import oneHot\nfrom dataDeal import trainData\n\nFLAGS = None\n\ndef main(_):\n  # cluster specification\n  FLAGS.task_index = int(os.environ[""TF_INDEX""])\n  FLAGS.job_name = os.environ[""TF_ROLE""]\n  cluster_def = json.loads(os.environ[""TF_CLUSTER_DEF""])\n  cluster = tf.train.ClusterSpec(cluster_def)\n\n  print(""ClusterSpec:"", cluster_def)\n  print(""current task id:"", FLAGS.task_index, "" role:"", FLAGS.job_name)\n  \n  gpu_options = tf.GPUOptions(allow_growth = True)\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name,task_index=FLAGS.task_index,config = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement = True))\n  \n  if FLAGS.job_name == ""ps"":\n    server.join()\n  elif FLAGS.job_name == ""worker"":\n    # set the train parameters\n    learning_rate = FLAGS.learning_rate\n    training_epochs = FLAGS.training_epochs\n    batch_size = FLAGS.batch_size\n    iterData = trainData(FLAGS.data_path, batch_size)\n    \n    with tf.device(tf.train.replica_device_setter(worker_device=(""/job:worker/task:%d""%(FLAGS.task_index)),cluster=cluster)):\n      # count the number of updates\n      global_step = tf.get_variable(\'global_step\', [],initializer = tf.constant_initializer(0), trainable = False)\n      # input \n      with tf.name_scope(\'input\'):\n        x = tf.placeholder(tf.float32, shape=[None, 100], name=""x-input"")\n        y_ = tf.placeholder(tf.float32, shape=[None, 2], name=""y-input"")\n      # model parameters\n      tf.set_random_seed(1)\n      with tf.name_scope(""weights""):\n        W1 = tf.Variable(tf.random_normal([100, 50]))\n        W2 = tf.Variable(tf.random_normal([50, 2]))\n      # bias\n      with tf.name_scope(""biases""):\n        b1 = tf.Variable(tf.zeros([50]))\n        b2 = tf.Variable(tf.zeros([2]))\n      # implement model\n      with tf.name_scope(""softmax""):\n        # y is our prediction\n        z1 = tf.add(tf.matmul(x,W1),b1)\n        a1 = tf.nn.softmax(z1)\n        z2 = tf.add(tf.matmul(a1,W2),b2)\n        y = tf.nn.softmax(z2)\n      # specify cost function\n      with tf.name_scope(\'cross_entropy\'):\n        # this is our cost\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n      # specify optimizer\n      with tf.name_scope(\'train\'):\n        # optimizer is an ""operation"" which we can execute in a session\n        grad_op = tf.train.GradientDescentOptimizer(learning_rate)\n        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\n      # init_op\n      tf.summary.scalar(\'cross_entropy\', cross_entropy )\n      merged = tf.summary.merge_all()\n      init_op = tf.global_variables_initializer()\n      saver = tf.train.Saver()\n      print(""Variables initialized ..."")\n    sv = tf.train.Supervisor(is_chief = (FLAGS.task_index == 0), global_step = global_step, init_op = init_op)\n\n    # Filter all connections except that between ps and this worker to avoid hanging issues when\n    # one worker finishes. We are using asynchronous training so there is no need for the workers to communicate.\n    config_proto = tf.ConfigProto(device_filters = [\'/job:ps\', \'/job:worker/task:%d\' % FLAGS.task_index],\n        gpu_options = gpu_options,\n        allow_soft_placement = True,\n        log_device_placement = True)\n\n    with sv.prepare_or_wait_for_session(server.target, config = config_proto) as sess:\n      # perform training cycles\n      start_time = time.time()\n      if(FLAGS.task_index == 0):\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n      for epoch in range(training_epochs):\n        # number of batches in one epoch                \n        sys.stderr.write(""reporter progress:%0.4f\\n""%(float(epoch)/(training_epochs)))\n        totalStep = iterData.batchCount()\n        for step in range(totalStep):\n          iterator_curr = iterData.nextBatch()\n          flag = 0\n          for iter in iterator_curr:\n            if 0 == flag:\n                train_x = iter[1].reshape(1,100)\n                train_y = oneHot(iter[0]).reshape(1,2)\n            else:\n                train_x = np.concatenate((train_x, iter[1].reshape(1,100)))\n                train_y = np.concatenate((train_y, oneHot(iter[0]).reshape(1,2)))\n            flag = 1\n          _, summary, cost, gstep = sess.run(\n                  [train_op, merged, cross_entropy, global_step],\n                  feed_dict={x: train_x, y_: train_y})\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          if(FLAGS.task_index == 0):\n            train_writer.add_summary(summary, gstep)\n          print(""Step: %d,"" % (gstep),\n                "" Epoch: %2d,"" % (epoch),                            \n                "" Cost: %.4f,"" % cost,\n                "" Time: %3.2fms"" % float(elapsed_time*1000))\n        sys.stderr.write(""reporter progress:%0.4f\\n""%(float(epoch+1)/(training_epochs)))\n  \n      print(""Train Completed."")\n      if(FLAGS.task_index == 0):\n        train_writer.close()\n        print(""saving model..."")\n        saver.save(sess, FLAGS.save_path+""/model.ckpt"")\n    print(""done"")       \n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  # Flags for defining the tf.train.ClusterSpec\n  parser.add_argument(\n    ""--job_name"",\n    type=str,\n    default="""",\n    help=""One of \'ps\', \'worker\'""\n  )\n  # Flags for defining the tf.train.Server\n  parser.add_argument(\n    ""--task_index"",\n    type=int,\n    default=0,\n    help=""Index of task within the job""\n  )\n  # Flags for defining the parameter of data path\n  parser.add_argument(\n    ""--data_path"",\n    type=str,\n    default="""",\n    help=""The path for train file""\n  )\n  parser.add_argument(\n    ""--save_path"",\n    type=str,\n    default="""",\n    help=""The save path for model""\n  )\n  parser.add_argument(\n    ""--log_dir"",\n    type=str,\n    default="""",\n    help=""The log path for model""\n  )\n  # Flags for defining the parameter of train\n  parser.add_argument(\n    ""--learning_rate"",\n    type=float,\n    default=0.001,\n    help=""learning rate of the train""\n  )\n  parser.add_argument(\n    ""--training_epochs"",\n    type=int,\n    default=5,\n    help=""the epoch of the train""\n  )\n  parser.add_argument(\n    ""--batch_size"",\n    type=int,\n    default=1200,\n    help=""The size of one batch""\n  )\n\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main)\n'"
examples/tfEstimator/demo.py,12,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport numpy as np\nimport tensorflow as tf\n\nFLAGS = None\n\n# Data sets\nIRIS_TRAINING = ""iris_training.csv""\nIRIS_TEST = ""iris_test.csv""\n\nimport os\nimport json\n\ncluster = json.loads(os.environ[""TF_CLUSTER_DEF""])\ntask_index = int(os.environ[""TF_INDEX""])\ntask_type = os.environ[""TF_ROLE""]\n\ntf_config = dict()\nworker_num = len(cluster[""worker""])\nif task_type == ""ps"":\n    tf_config[""task""] = {""index"":task_index, ""type"":task_type}\nelif task_type == ""worker"":\n    if task_index == 0:\n        tf_config[""task""] = {""index"":0, ""type"":""chief""}\n    else:\n        tf_config[""task""] = {""index"":task_index-1, ""type"":task_type}\nelif task_type == ""evaluator"":\n    tf_config[""task""] = {""index"":task_index, ""type"":task_type}\n\nif worker_num == 1:\n    cluster[""chief""] = cluster[""worker""]\n    del cluster[""worker""]\nelse:\n    cluster[""chief""] = [cluster[""worker""][0]]\n    del cluster[""worker""][0]\n\ntf_config[""cluster""] = cluster\nos.environ[""TF_CONFIG""] = json.dumps(tf_config)\nprint(json.loads(os.environ[""TF_CONFIG""]))\n\n\ndef main(_):\n\n    # Load datasets.\n    training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n        filename=FLAGS.data_path + ""/"" + IRIS_TRAINING,\n        target_dtype=np.int,\n        features_dtype=np.float32)\n\n    test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n        filename=FLAGS.data_path + ""/"" + IRIS_TEST,\n        target_dtype=np.int,\n        features_dtype=np.float32)\n\n    # Define the training inputs\n    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={""x"": np.array(training_set.data)},\n        y=np.array(training_set.target),\n        num_epochs=None,\n        shuffle=True)\n\n    # Define the test inputs\n    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={""x"": np.array(test_set.data)},\n        y=np.array(test_set.target),\n        num_epochs=1,\n        shuffle=False)\n\n    # Specify that all features have real-value data\n    feature_columns = [tf.feature_column.numeric_column(""x"", shape=[4])]\n\n    # Build 3 layer DNN with 10, 20, 10 units respectively.\n    classifier = tf.estimator.DNNClassifier(\n        config=tf.estimator.RunConfig(\n           model_dir=FLAGS.model_path,\n            save_checkpoints_steps = 50,\n            save_summary_steps = 50,\n            keep_checkpoint_max=5,\n            log_step_count_steps=50\n        ),\n        feature_columns=feature_columns,\n        hidden_units=[10, 20, 10],\n        n_classes=3)\n\n    tf.logging.set_verbosity(tf.logging.INFO)\n\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=FLAGS.max_steps)\n    eval_spec = tf.estimator.EvalSpec(input_fn=test_input_fn, steps=10, start_delay_secs=150, throttle_secs=200)\n\n    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n\n    # Evaluate accuracy.\n    accuracy_score = classifier.evaluate(input_fn=test_input_fn)[""accuracy""]\n    print(""\\nTest Accuracy: {0:f}\\n"".format(accuracy_score))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n\n    # Flags for defining the parameter of data path\n    parser.add_argument(\n        ""--data_path"",\n        type=str,\n        default=""./"",\n        help=""The path for data""\n    )\n    parser.add_argument(\n        ""--model_path"",\n        type=str,\n        default=""./my_model"",\n        help=""The save path for model""\n    )\n\n    # Flags for defining the parameter of train\n    parser.add_argument(\n        ""--learning_rate"",\n        type=float,\n        default=0.001,\n        help=""learning rate of the train""\n    )\n    parser.add_argument(\n        ""--max_steps"",\n        type=int,\n        default=1000,\n        help=""the max_steps of the train""\n    )\n\n    FLAGS, unparsed = parser.parse_known_args()\n    tf.app.run(main=main)\n'"
