file_path,api_count,code
ANN/test_ann.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_ann():\n    """"""Test ANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load ann model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-ann-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_ann()\n'"
ANN/text_ann.py,40,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextANN(object):\n    """"""A ANN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, fc_hidden_size,\n            num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Average Vectors\n        self.embedded_sentence_average = tf.reduce_mean(self.embedded_sentence, axis=1)  # [batch_size, embedding_size]\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[embedding_size, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.embedded_sentence_average, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
ANN/train_ann.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorboard.plugins import projector\nfrom text_ann import TextANN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_ann():\n    """"""Training ANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and ann object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            ann = TextANN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=ann.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(ann.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=ann.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", ann.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load ann model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(ann.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    ann.input_x: x_batch,\n                    ann.input_y: y_batch,\n                    ann.dropout_keep_prob: args.dropout_rate,\n                    ann.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, ann.global_step, train_summary_op, ann.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        ann.input_x: x_batch_val,\n                        ann.input_y: y_batch_val,\n                        ann.dropout_keep_prob: 1.0,\n                        ann.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [ann.global_step, validation_summary_op, ann.scores, ann.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, ann.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_ann()'"
CNN/test_cnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_cnn():\n    """"""Test CNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load cnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-cnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_cnn()\n'"
CNN/text_cnn.py,51,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextCNN(object):\n    """"""A CNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, filter_sizes,\n            num_filters, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n            self.embedded_sentence_expanded = tf.expand_dims(self.embedded_sentence, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.embedded_sentence_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n\n                conv = tf.nn.bias_add(conv, b)\n\n                # Batch Normalization Layer\n                conv_bn = tf.layers.batch_normalization(conv, training=self.is_training)\n\n                # Apply nonlinearity\n                conv_out = tf.nn.relu(conv_bn, name=""relu"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    conv_out,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n            pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.pool = tf.concat(pooled_outputs, axis=3)\n        self.pool_flat = tf.reshape(self.pool, shape=[-1, num_filters_total])\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.pool_flat, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
CNN/train_cnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_cnn import TextCNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_cnn():\n    """"""Training CNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and cnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            cnn = TextCNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=cnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(cnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=cnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load cnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(cnn.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    cnn.input_x: x_batch,\n                    cnn.input_y: y_batch,\n                    cnn.dropout_keep_prob: args.dropout_rate,\n                    cnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, cnn.global_step, train_summary_op, cnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        cnn.input_x: x_batch_val,\n                        cnn.input_y: y_batch_val,\n                        cnn.dropout_keep_prob: 1.0,\n                        cnn.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [cnn.global_step, validation_summary_op, cnn.scores, cnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, cnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_cnn()'"
CRNN/test_crnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_crnn():\n    """"""Test CRNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load crnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-crnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_crnn()\n'"
CRNN/text_crnn.py,60,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextCRNN(object):\n    """"""A CRNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, filter_sizes, num_filters,\n            lstm_hidden_size, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n            self.embedded_sentence_expanded = tf.expand_dims(self.embedded_sentence, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.embedded_sentence_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n\n                conv = tf.nn.bias_add(conv, b)\n\n                # Batch Normalization Layer\n                conv_bn = tf.layers.batch_normalization(conv, training=self.is_training)\n\n                # Apply nonlinearity\n                conv_out = tf.nn.relu(conv_bn, name=""relu"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    conv_out,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n            pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        pool_flat_outputs = []\n        for i in pooled_outputs:\n            pool_flat = tf.reshape(i, shape=[-1, 1, num_filters])\n            pool_flat = tf.nn.dropout(pool_flat, self.dropout_keep_prob)\n            pool_flat_outputs.append(pool_flat)\n\n        # Bi-LSTM Layer\n        lstm_outputs = []\n\n        for index, pool_flat in enumerate(pool_flat_outputs):\n            with tf.variable_scope(""Bi-lstm-{0}"".format(index)):\n                lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n                lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n                if self.dropout_keep_prob is not None:\n                    lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                    lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n                # Creates a dynamic bidirectional recurrent neural network\n                # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n                # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n                # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n                # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n\n                outputs, state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, pool_flat, dtype=tf.float32)\n                # Concat output\n                lstm_concat = tf.concat(outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n                lstm_out = tf.reduce_mean(lstm_concat, axis=1)  # [batch_size, lstm_hidden_size * 2]\n\n                # shape of `lstm_outputs`: list -> len(filter_sizes) * [batch_size, lstm_hidden_size * 2]\n                lstm_outputs.append(lstm_out)\n\n        self.lstm_out = tf.concat(lstm_outputs, axis=1)  # [batch_size, lstm_hidden_size * 2 * len(filter_sizes)]\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[lstm_hidden_size * 2 * len(filter_sizes), fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.lstm_out, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
CRNN/train_crnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_crnn import TextCRNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_crnn():\n    """"""Training CRNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and crnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            crnn = TextCRNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=crnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(crnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=crnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", crnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load crnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(crnn.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    crnn.input_x: x_batch,\n                    crnn.input_y: y_batch,\n                    crnn.dropout_keep_prob: args.dropout_rate,\n                    crnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, crnn.global_step, train_summary_op, crnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        crnn.input_x: x_batch_val,\n                        crnn.input_y: y_batch_val,\n                        crnn.dropout_keep_prob: 1.0,\n                        crnn.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [crnn.global_step, validation_summary_op, crnn.scores, crnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, crnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_crnn()'"
FastText/test_fast.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_fasttext():\n    """"""Test FASTTEXT model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load fasttext model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-fasttext-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_fasttext()\n'"
FastText/text_fast.py,33,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextFAST(object):\n    """"""A FASTTEXT for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size,\n            num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Average Vectors\n        self.embedded_sentence_average = tf.reduce_mean(self.embedded_sentence, axis=1)  # [batch_size, embedding_size]\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.embedded_sentence_average,\n                                          self.embedded_sentence_average.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[embedding_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
FastText/train_fast.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_fast import TextFAST\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_fasttext():\n    """"""Training FASTTEXT model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and fasttext object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            fasttext = TextFAST(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=fasttext.global_step,\n                                                           decay_steps=args.decay_steps, decay_rate=args.decay_rate,\n                                                           staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(fasttext.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=fasttext.global_step,\n                                                     name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", fasttext.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load fasttext model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(fasttext.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    fasttext.input_x: x_batch,\n                    fasttext.input_y: y_batch,\n                    fasttext.dropout_keep_prob: args.dropout_rate,\n                    fasttext.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, fasttext.global_step, train_summary_op, fasttext.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        fasttext.input_x: x_batch_val,\n                        fasttext.input_y: y_batch_val,\n                        fasttext.dropout_keep_prob: 1.0,\n                        fasttext.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [fasttext.global_step, validation_summary_op, fasttext.scores, fasttext.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, fasttext.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_fasttext()'"
HAN/test_han.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_han():\n    """"""Test HAN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load han model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-han-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_han()\n'"
HAN/text_han.py,55,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextHAN(object):\n    """"""A HAN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size,\n            fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs, state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell,\n                                                             self.embedded_sentence, dtype=tf.float32)\n\n        # Concat output\n        self.lstm_concat = tf.concat(outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n\n        # Attention Layer\n        with tf.name_scope(""attention""):\n            num_units = self.lstm_concat.get_shape().as_list()[-1]  # Get last dimension [lstm_hidden_size * 2]\n            u_attention = tf.Variable(tf.truncated_normal(shape=[num_units], stddev=0.1, dtype=tf.float32),\n                                      name=""u_attention"")\n            # 1. One-Layer MLP\n            # W = tf.Variable(tf.truncated_normal(shape=[lstm_hidden_size * 2, num_units],\n            #                                     stddev=0.1, dtype=tf.float32), name=""W"")\n            # b = tf.Variable(tf.constant(0.1, shape=[num_units], dtype=tf.float32), name=""b"")\n            # shape of `u`: [batch_size, sequence_length, num_units]\n            u = tf.layers.dense(self.lstm_concat, units=num_units, activation=tf.nn.tanh, use_bias=True)\n\n            # 2. Compute weight by computing similarity of u and attention vector u_attention\n            score = tf.multiply(u, u_attention)  # [batch_size, sequence_length, num_units]\n            weight = tf.reduce_mean(score, axis=2, keepdims=True)  # [batch_size, sequence_length, 1]\n\n            # 3. Weight sum\n            self.attention = tf.reduce_sum(tf.multiply(u, weight), axis=1)  # [batch_size, num_units]\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_units, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.attention, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
HAN/train_han.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_han import TextHAN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_han():\n    """"""Training HAN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and han object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            han = TextHAN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=han.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(han.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=han.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", han.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load han model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(han.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    han.input_x: x_batch,\n                    han.input_y: y_batch,\n                    han.dropout_keep_prob: args.dropout_rate,\n                    han.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, han.global_step, train_summary_op, han.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        han.input_x: x_batch_val,\n                        han.input_y: y_batch_val,\n                        han.dropout_keep_prob: 1.0,\n                        han.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [han.global_step, validation_summary_op, han.scores, han.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, han.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_han()'"
RCNN/test_rcnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_rcnn():\n    """"""Test RCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load rcnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-rcnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_rcnn()\n'"
RCNN/text_rcnn.py,61,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextRCNN(object):\n    """"""A RCNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size, filter_sizes,\n            num_filters, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Add dropout\n        with tf.name_scope(""dropout-input""):\n            self.embedded_sentence_drop = tf.nn.dropout(self.embedded_sentence, self.dropout_keep_prob)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs, state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell,\n                                                             self.embedded_sentence_drop, dtype=tf.float32)\n            # Concat output\n            # shape of `lstm_concat`: [batch_size, sequence_length, lstm_hidden_size * 2]\n            self.lstm_concat = tf.concat(outputs, axis=2)\n\n            # shape of `lstm_out`: [batch_size, sequence_length, lstm_hidden_size * 2, 1]\n            self.lstm_out = tf.expand_dims(self.lstm_concat, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, lstm_hidden_size * 2, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.lstm_out,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n\n                conv = tf.nn.bias_add(conv, b)\n\n                # Batch Normalization Layer\n                conv_bn = batch_norm(conv, is_training=self.is_training, trainable=True, updates_collections=None)\n\n                # Apply nonlinearity\n                conv_out = tf.nn.relu(conv_bn, name=""relu"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                avg_pooled = tf.nn.avg_pool(\n                    conv_out,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                max_pooled = tf.nn.max_pool(\n                    conv_out,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                # shape of `pooled_combine`: [batch_size, 1, 1, num_filters * 2]\n                pooled_combine = tf.concat([avg_pooled, max_pooled], axis=3)\n\n            pooled_outputs.append(pooled_combine)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n\n        # shape of `pool`: [batch_size, 1, 1, num_filters * 2 * len(filter_sizes)]\n        self.pool = tf.concat(pooled_outputs, axis=3)\n        self.pool_flat = tf.reshape(self.pool, shape=[-1, num_filters_total * 2])\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.pool_flat, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
RCNN/train_rcnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_rcnn import TextRCNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_rcnn():\n    """"""Training RCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and rcnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            rcnn = TextRCNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=rcnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(rcnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=rcnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", rcnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load rcnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(rcnn.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    rcnn.input_x: x_batch,\n                    rcnn.input_y: y_batch,\n                    rcnn.dropout_keep_prob: args.dropout_rate,\n                    rcnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, rcnn.global_step, train_summary_op, rcnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        rcnn.input_x: x_batch_val,\n                        rcnn.input_y: y_batch_val,\n                        rcnn.dropout_keep_prob: 1.0,\n                        rcnn.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [rcnn.global_step, validation_summary_op, rcnn.scores, rcnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, rcnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_rcnn()'"
RNN/test_rnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_rnn():\n    """"""Test RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load rcnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-rnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_rnn()\n'"
RNN/text_rnn.py,57,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import tanh\nfrom tensorflow import sigmoid\nfrom tensorflow.contrib import rnn\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass BatchNormLSTMCell(rnn.RNNCell):\n    """"""Batch normalized LSTM (cf. http://arxiv.org/abs/1603.09025)""""""\n\n    def __init__(self, num_units, is_training=False, forget_bias=1.0,\n                 activation=tanh, reuse=None):\n        """"""Initialize the BNLSTM cell.\n\n        Args:\n          num_units: int, The number of units in the BNLSTM cell.\n          forget_bias: float, The bias added to forget gates (see above).\n            Must set to `0.0` manually when restoring from CudnnLSTM-trained\n            checkpoints.\n          activation: Activation function of the inner states.  Default: `tanh`.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        self._num_units = num_units\n        self._is_training = is_training\n        self._forget_bias = forget_bias\n        self._activation = activation\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return rnn.LSTMStateTuple(self._num_units, self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__, reuse=self._reuse):\n            c, h = state\n            input_size = inputs.get_shape().as_list()[1]\n            W_xh = tf.get_variable(\'W_xh\',\n                                   [input_size, 4 * self._num_units],\n                                   initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\',\n                                   [self._num_units, 4 * self._num_units],\n                                   initializer=bn_lstm_identity_initializer(0.95))\n            bias = tf.get_variable(\'bias\', [4 * self._num_units])\n\n            xh = tf.matmul(inputs, W_xh)\n            hh = tf.matmul(h, W_hh)\n\n            bn_xh = batch_norm(xh, self._is_training)\n            bn_hh = batch_norm(hh, self._is_training)\n\n            hidden = bn_xh + bn_hh + bias\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            i, j, f, o = array_ops.split(value=hidden, num_or_size_splits=4, axis=1)\n\n            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n            bn_new_c = batch_norm(new_c, \'c\', self._is_training)\n            new_h = self._activation(bn_new_c) * sigmoid(o)\n            new_state = rnn.LSTMStateTuple(new_c, new_h)\n\n            return new_h, new_state\n\n\ndef orthogonal(shape):\n    flat_shape = (shape[0], np.prod(shape[1:]))\n    a = np.random.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    return q.reshape(shape)\n\n\ndef bn_lstm_identity_initializer(scale):\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        """"""\n        Ugly cause LSTM params calculated in one matrix multiply\n        """"""\n\n        size = shape[0]\n        # gate (j) is identity\n        t = np.zeros(shape)\n        t[:, size:size * 2] = np.identity(size) * scale\n        t[:, :size] = orthogonal([size, size])\n        t[:, size * 2:size * 3] = orthogonal([size, size])\n        t[:, size * 3:] = orthogonal([size, size])\n        return tf.constant(t, dtype=dtype)\n\n    return _initializer\n\n\ndef orthogonal_initializer():\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        return tf.constant(orthogonal(shape), dtype)\n    return _initializer\n\n\nclass TextRNN(object):\n    """"""A RNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size,\n            fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs, state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell,\n                                                             self.embedded_sentence, dtype=tf.float32)\n\n        # Concat output\n        self.lstm_concat = tf.concat(outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n        self.lstm_out = tf.reduce_mean(self.lstm_concat, axis=1)  # [batch_size, lstm_hidden_size * 2]\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[lstm_hidden_size * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.lstm_out, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
RNN/train_rnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_rnn import TextRNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_rnn():\n    """"""Training RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and rnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            rnn = TextRNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=rnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(rnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=rnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", rnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load rnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(rnn.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    rnn.input_x: x_batch,\n                    rnn.input_y: y_batch,\n                    rnn.dropout_keep_prob: args.dropout_rate,\n                    rnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, rnn.global_step, train_summary_op, rnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        rnn.input_x: x_batch_val,\n                        rnn.input_y: y_batch_val,\n                        rnn.dropout_keep_prob: 1.0,\n                        rnn.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [rnn.global_step, validation_summary_op, rnn.scores, rnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, rnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_rnn()'"
SANN/test_sann.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_sann():\n    """"""Test SANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_test, y_test = dh.pad_data(test_data, args.pad_seq_len)\n    y_test_labels = test_data.labels\n\n    # Load sann model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            scores = graph.get_operation_by_name(""output/scores"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/scores""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-sann-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches = dh.batch_iter(list(zip(x_test, y_test, y_test_labels)), args.batch_size, 1, shuffle=False)\n\n            test_counter, test_loss = 0, 0.0\n\n            test_pre_tk = [0.0] * args.topK\n            test_rec_tk = [0.0] * args.topK\n            test_F1_tk = [0.0] * args.topK\n\n            # Collect the predictions here\n            true_labels = []\n            predicted_labels = []\n            predicted_scores = []\n\n            # Collect for calculating metrics\n            true_onehot_labels = []\n            predicted_onehot_scores = []\n            predicted_onehot_labels_ts = []\n            predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n            for batch_test in batches:\n                x_batch_test, y_batch_test, y_batch_test_labels = zip(*batch_test)\n                feed_dict = {\n                    input_x: x_batch_test,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n                batch_scores, cur_loss = sess.run([scores, loss], feed_dict)\n\n                # Prepare for calculating metrics\n                for i in y_batch_test:\n                    true_onehot_labels.append(i)\n                for j in batch_scores:\n                    predicted_onehot_scores.append(j)\n\n                # Get the predicted labels by threshold\n                batch_predicted_labels_ts, batch_predicted_scores_ts = \\\n                    dh.get_label_threshold(scores=batch_scores, threshold=args.threshold)\n\n                # Add results to collection\n                for i in y_batch_test_labels:\n                    true_labels.append(i)\n                for j in batch_predicted_labels_ts:\n                    predicted_labels.append(j)\n                for k in batch_predicted_scores_ts:\n                    predicted_scores.append(k)\n\n                # Get onehot predictions by threshold\n                batch_predicted_onehot_labels_ts = \\\n                    dh.get_onehot_label_threshold(scores=batch_scores, threshold=args.threshold)\n                for i in batch_predicted_onehot_labels_ts:\n                    predicted_onehot_labels_ts.append(i)\n\n                # Get onehot predictions by topK\n                for top_num in range(args.topK):\n                    batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=batch_scores, top_num=top_num+1)\n\n                    for i in batch_predicted_onehot_labels_tk:\n                        predicted_onehot_labels_tk[top_num].append(i)\n\n                test_loss = test_loss + cur_loss\n                test_counter = test_counter + 1\n\n            # Calculate Precision & Recall & F1\n            test_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                          y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                       y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n            test_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                  y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n            for top_num in range(args.topK):\n                test_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                       y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                       average=\'micro\')\n                test_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                    y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                    average=\'micro\')\n                test_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                               y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                               average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                     y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n            # Calculate the average PR\n            test_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                               y_score=np.array(predicted_onehot_scores), average=""micro"")\n            test_loss = float(test_loss / test_counter)\n\n            logger.info(""All Test Dataset: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                        .format(test_loss, test_auc, test_prc))\n\n            # Predict by threshold\n            logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                        .format(test_pre_ts, test_rec_ts, test_F1_ts))\n\n            # Predict by topK\n            logger.info(""Predict by topK:"")\n            for top_num in range(args.topK):\n                logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                            .format(top_num + 1, test_pre_tk[top_num], test_rec_tk[top_num], test_F1_tk[top_num]))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", data_id=test_data.testid,\n                                      all_labels=true_labels, all_predict_labels=predicted_labels,\n                                      all_predict_scores=predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_sann()\n'"
SANN/text_sann.py,70,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport numpy as np\nimport tensorflow as tf\nimport tflearn\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow import sigmoid\nfrom tensorflow import tanh\nfrom tensorflow.contrib import rnn\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass BatchNormLSTMCell(rnn.RNNCell):\n    """"""Batch normalized LSTM (cf. http://arxiv.org/abs/1603.09025)""""""\n\n    def __init__(self, num_units, is_training=False, forget_bias=1.0,\n                 activation=tanh, reuse=None):\n        """"""Initialize the BNLSTM cell.\n\n        Args:\n          num_units: int, The number of units in the BNLSTM cell.\n          forget_bias: float, The bias added to forget gates (see above).\n            Must set to `0.0` manually when restoring from CudnnLSTM-trained\n            checkpoints.\n          activation: Activation function of the inner states.  Default: `tanh`.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        self._num_units = num_units\n        self._is_training = is_training\n        self._forget_bias = forget_bias\n        self._activation = activation\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return rnn.LSTMStateTuple(self._num_units, self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__, reuse=self._reuse):\n            c, h = state\n            input_size = inputs.get_shape().as_list()[1]\n            W_xh = tf.get_variable(\'W_xh\',\n                                   [input_size, 4 * self._num_units],\n                                   initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\',\n                                   [self._num_units, 4 * self._num_units],\n                                   initializer=bn_lstm_identity_initializer(0.95))\n            bias = tf.get_variable(\'bias\', [4 * self._num_units])\n\n            xh = tf.matmul(inputs, W_xh)\n            hh = tf.matmul(h, W_hh)\n\n            bn_xh = batch_norm(xh, self._is_training)\n            bn_hh = batch_norm(hh, self._is_training)\n\n            hidden = bn_xh + bn_hh + bias\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            i, j, f, o = array_ops.split(value=hidden, num_or_size_splits=4, axis=1)\n\n            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n            bn_new_c = batch_norm(new_c, \'c\', self._is_training)\n            new_h = self._activation(bn_new_c) * sigmoid(o)\n            new_state = rnn.LSTMStateTuple(new_c, new_h)\n\n            return new_h, new_state\n\n\ndef orthogonal(shape):\n    flat_shape = (shape[0], np.prod(shape[1:]))\n    a = np.random.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    return q.reshape(shape)\n\n\ndef bn_lstm_identity_initializer(scale):\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        """"""\n        Ugly cause LSTM params calculated in one matrix multiply\n        """"""\n\n        size = shape[0]\n        # gate (j) is identity\n        t = np.zeros(shape)\n        t[:, size:size * 2] = np.identity(size) * scale\n        t[:, :size] = orthogonal([size, size])\n        t[:, size * 2:size * 3] = orthogonal([size, size])\n        t[:, size * 3:] = orthogonal([size, size])\n        return tf.constant(t, dtype=dtype)\n\n    return _initializer\n\n\ndef orthogonal_initializer():\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        return tf.constant(orthogonal(shape), dtype)\n    return _initializer\n\n\nclass TextSANN(object):\n    """"""A SANN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size, attention_unit_size,\n            attention_hops_size, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence = tf.nn.embedding_lookup(self.embedding, self.input_x)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs, state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell,\n                                                             self.embedded_sentence, dtype=tf.float32)\n\n        # Concat output\n        self.lstm_out = tf.concat(outputs, axis=2)  # [batch_size, sequence_length, lstm_hidden_size * 2]\n\n        # Add attention\n        with tf.name_scope(""attention""):\n            W_s1 = tf.Variable(tf.truncated_normal(shape=[attention_unit_size, lstm_hidden_size * 2],\n                                                   stddev=0.1, dtype=tf.float32), name=""W_s1"")\n            W_s2 = tf.Variable(tf.truncated_normal(shape=[attention_hops_size, attention_unit_size],\n                                                   stddev=0.1, dtype=tf.float32), name=""W_s2"")\n            self.attention = tf.map_fn(\n                fn=lambda x: tf.matmul(W_s2, x),\n                elems=tf.tanh(\n                    tf.map_fn(\n                        fn=lambda x: tf.matmul(W_s1, tf.transpose(x)),\n                        elems=self.lstm_out,\n                        dtype=tf.float32\n                    )\n                )\n            )\n            self.attention_out = tf.nn.softmax(self.attention)  # [batch_size, attention_hops_size, sequence_length]\n\n        self.M = tf.matmul(self.attention_out, self.lstm_out)  # [batch_size, attention_hops_size, lstm_hidden_size * 2]\n        # shape of `M_flat`: [batch_size, attention_hops_size * lstm_hidden_size * 2]\n        self.M_flat = tf.reshape(self.M, shape=[-1, attention_hops_size * lstm_hidden_size * 2])\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[attention_hops_size * lstm_hidden_size * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.M_flat, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.scores = tf.sigmoid(self.logits, name=""scores"")\n\n        # Calculate mean cross-entropy loss, L2 loss and attention penalization loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(tf.reduce_sum(losses, axis=1), name=""sigmoid_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n'"
SANN/train_sann.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_sann import TextSANN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_sann():\n    """"""Training RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n    val_data = dh.load_data_and_labels(args.validation_file, args.num_classes, args.word2vec_file, data_aug_flag=False)\n\n    logger.info(""Data padding..."")\n    x_train, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_val, y_val = dh.pad_data(val_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and sann object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            sann = TextSANN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                attention_unit_size=args.attention_dim,\n                attention_hops_size=args.attention_hops_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=args.num_classes,\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=sann.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(sann.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=sann.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", sann.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load sann model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(sann.global_step)\n\n            def train_step(x_batch, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    sann.input_x: x_batch,\n                    sann.input_y: y_batch,\n                    sann.dropout_keep_prob: args.dropout_rate,\n                    sann.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, sann.global_step, train_summary_op, sann.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_val, y_val, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_val, y_val)), args.batch_size, 1)\n\n                # Predict classes by threshold or topk (\'ts\': threshold; \'tk\': topk)\n                eval_counter, eval_loss = 0, 0.0\n\n                eval_pre_tk = [0.0] * args.topK\n                eval_rec_tk = [0.0] * args.topK\n                eval_F1_tk = [0.0] * args.topK\n\n                true_onehot_labels = []\n                predicted_onehot_scores = []\n                predicted_onehot_labels_ts = []\n                predicted_onehot_labels_tk = [[] for _ in range(args.topK)]\n\n                for batch_validation in batches_validation:\n                    x_batch_val, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        sann.input_x: x_batch_val,\n                        sann.input_y: y_batch_val,\n                        sann.dropout_keep_prob: 1.0,\n                        sann.is_training: False\n                    }\n                    step, summaries, scores, cur_loss = sess.run(\n                        [sann.global_step, validation_summary_op, sann.scores, sann.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_onehot_labels.append(i)\n                    for j in scores:\n                        predicted_onehot_scores.append(j)\n\n                    # Predict by threshold\n                    batch_predicted_onehot_labels_ts = \\\n                        dh.get_onehot_label_threshold(scores=scores, threshold=args.threshold)\n\n                    for k in batch_predicted_onehot_labels_ts:\n                        predicted_onehot_labels_ts.append(k)\n\n                    # Predict by topK\n                    for top_num in range(args.topK):\n                        batch_predicted_onehot_labels_tk = dh.get_onehot_label_topk(scores=scores, top_num=top_num+1)\n\n                        for i in batch_predicted_onehot_labels_tk:\n                            predicted_onehot_labels_tk[top_num].append(i)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_pre_ts = precision_score(y_true=np.array(true_onehot_labels),\n                                              y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_rec_ts = recall_score(y_true=np.array(true_onehot_labels),\n                                           y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n                eval_F1_ts = f1_score(y_true=np.array(true_onehot_labels),\n                                      y_pred=np.array(predicted_onehot_labels_ts), average=\'micro\')\n\n                for top_num in range(args.topK):\n                    eval_pre_tk[top_num] = precision_score(y_true=np.array(true_onehot_labels),\n                                                           y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                           average=\'micro\')\n                    eval_rec_tk[top_num] = recall_score(y_true=np.array(true_onehot_labels),\n                                                        y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                        average=\'micro\')\n                    eval_F1_tk[top_num] = f1_score(y_true=np.array(true_onehot_labels),\n                                                   y_pred=np.array(predicted_onehot_labels_tk[top_num]),\n                                                   average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_onehot_labels),\n                                         y_score=np.array(predicted_onehot_scores), average=\'micro\')\n                # Calculate the average PR\n                eval_prc = average_precision_score(y_true=np.array(true_onehot_labels),\n                                                   y_score=np.array(predicted_onehot_scores), average=\'micro\')\n\n                return eval_loss, eval_auc, eval_prc, eval_pre_ts, eval_rec_ts, eval_F1_ts, \\\n                       eval_pre_tk, eval_rec_tk, eval_F1_tk\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_train, y_batch_train = zip(*batch_train)\n                train_step(x_batch_train, y_batch_train)\n                current_step = tf.train.global_step(sess, sann.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_auc, eval_prc, \\\n                    eval_pre_ts, eval_rec_ts, eval_F1_ts, eval_pre_tk, eval_rec_tk, eval_F1_tk = \\\n                        validation_step(x_val, y_val, writer=validation_summary_writer)\n\n                    logger.info(""All Validation set: Loss {0:g} | AUC {1:g} | AUPRC {2:g}""\n                                .format(eval_loss, eval_auc, eval_prc))\n\n                    # Predict by threshold\n                    logger.info(""Predict by threshold: Precision {0:g}, Recall {1:g}, F1 {2:g}""\n                                .format(eval_pre_ts, eval_rec_ts, eval_F1_ts))\n\n                    # Predict by topK\n                    logger.info(""Predict by topK:"")\n                    for top_num in range(args.topK):\n                        logger.info(""Top{0}: Precision {1:g}, Recall {2:g}, F1 {3:g}""\n                                    .format(top_num+1, eval_pre_tk[top_num], eval_rec_tk[top_num], eval_F1_tk[top_num]))\n                    best_saver.handle(eval_prc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_sann()'"
utils/checkmate.py,6,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport glob\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BestCheckpointSaver(object):\n    """"""Maintains a directory containing only the best n checkpoints\n    Inside the directory is a best_checkpoints JSON file containing a dictionary\n    mapping of the best checkpoint filepaths to the values by which the checkpoints\n    are compared.  Only the best n checkpoints are contained in the directory and JSON file.\n    This is a light-weight wrapper class only intended to work in simple,\n    non-distributed settings.  It is not intended to work with the tf.Estimator\n    framework.\n    """"""\n    def __init__(self, save_dir, num_to_keep=1, maximize=True, saver=None):\n        """"""Creates a `BestCheckpointSaver`\n        `BestCheckpointSaver` acts as a wrapper class around a `tf.train.Saver`\n        Args:\n            save_dir: The directory in which the checkpoint files will be saved\n            num_to_keep: The number of best checkpoint files to retain\n            maximize: Define \'best\' values to be the highest values.  For example,\n              set this to True if selecting for the checkpoints with the highest\n              given accuracy.  Or set to False to select for checkpoints with the\n              lowest given error rate.\n            saver: A `tf.train.Saver` to use for saving checkpoints.  A default\n              `tf.train.Saver` will be created if none is provided.\n        """"""\n        self._num_to_keep = num_to_keep\n        self._save_dir = save_dir\n        self._save_path = os.path.join(save_dir, \'model\')\n        self._maximize = maximize\n        self._saver = saver if saver else tf.train.Saver(\n            max_to_keep=None,\n            save_relative_paths=True\n        )\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        self.best_checkpoints_file = os.path.join(save_dir, \'best_checkpoints\')\n\n    def handle(self, value, sess, global_step):\n        """"""Updates the set of best checkpoints based on the given result.\n        Args:\n            value: The value by which to rank the checkpoint.\n            sess: A tf.Session to use to save the checkpoint\n            global_step: The global step\n        """"""\n        current_ckpt = \'model-{}\'.format(global_step)\n        value = float(value)\n        if not os.path.exists(self.best_checkpoints_file):\n            self._save_best_checkpoints_file({current_ckpt: value})\n            self._saver.save(sess, self._save_path, global_step)\n            return\n\n        best_checkpoints = self._load_best_checkpoints_file()\n\n        if len(best_checkpoints) < self._num_to_keep:\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n            self._saver.save(sess, self._save_path, global_step)\n            return\n\n        if self._maximize:\n            should_save = not all(current_best >= value\n                                  for current_best in best_checkpoints.values())\n        else:\n            should_save = not all(current_best <= value\n                                  for current_best in best_checkpoints.values())\n        if should_save:\n            best_checkpoint_list = self._sort(best_checkpoints)\n\n            worst_checkpoint = os.path.join(self._save_dir,\n                                            best_checkpoint_list.pop(-1)[0])\n            self._remove_outdated_checkpoint_files(worst_checkpoint)\n            self._update_internal_saver_state(best_checkpoint_list)\n\n            best_checkpoints = dict(best_checkpoint_list)\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n\n            self._saver.save(sess, self._save_path, global_step)\n\n    def _save_best_checkpoints_file(self, updated_best_checkpoints):\n        with open(self.best_checkpoints_file, \'w\') as f:\n            json.dump(updated_best_checkpoints, f, indent=3)\n\n    def _remove_outdated_checkpoint_files(self, worst_checkpoint):\n        os.remove(os.path.join(self._save_dir, \'checkpoint\'))\n        for ckpt_file in glob.glob(worst_checkpoint + \'.*\'):\n            os.remove(ckpt_file)\n\n    def _update_internal_saver_state(self, best_checkpoint_list):\n        best_checkpoint_files = [\n            (ckpt[0], np.inf)  # TODO: Try to use actual file timestamp\n            for ckpt in best_checkpoint_list\n        ]\n        self._saver.set_last_checkpoints_with_time(best_checkpoint_files)\n\n    def _load_best_checkpoints_file(self):\n        with open(self.best_checkpoints_file, \'r\') as f:\n            best_checkpoints = json.load(f)\n        return best_checkpoints\n\n    def _sort(self, best_checkpoints):\n        best_checkpoints = [\n            (ckpt, best_checkpoints[ckpt])\n            for ckpt in sorted(best_checkpoints,\n                               key=best_checkpoints.get,\n                               reverse=self._maximize)\n        ]\n        return best_checkpoints\n\n\ndef get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True):\n    """"""\n    Returns filepath to the best checkpoint\n    Reads the best_checkpoints file in the best_checkpoint_dir directory.\n    Returns the filepath in the best_checkpoints file associated with\n    the highest value if select_maximum_value is True, or the filepath\n    associated with the lowest value if select_maximum_value is False.\n    Args:\n        best_checkpoint_dir: Directory containing best_checkpoints JSON file\n        select_maximum_value: If True, select the filepath associated\n          with the highest value.  Otherwise, select the filepath associated\n          with the lowest value.\n    Returns:\n        The full path to the best checkpoint file\n    """"""\n    best_checkpoints_file = os.path.join(best_checkpoint_dir, \'best_checkpoints\')\n    assert os.path.exists(best_checkpoints_file)\n    with open(best_checkpoints_file, \'r\') as f:\n        best_checkpoints = json.load(f)\n    best_checkpoints = [\n        ckpt for ckpt in sorted(best_checkpoints,\n                                key=best_checkpoints.get,\n                                reverse=select_maximum_value)\n    ]\n    return os.path.join(os.path.abspath(best_checkpoint_dir),  best_checkpoints[0])\n'"
utils/data_helpers.py,0,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport time\nimport heapq\nimport multiprocessing\nimport gensim\nimport logging\nimport json\n\nfrom collections import OrderedDict\nfrom pylab import *\nfrom texttable import Texttable\nfrom gensim.models import word2vec\nfrom tflearn.data_utils import pad_sequences\n\nANALYSIS_DIR = \'../data/data_analysis/\'\n\n\ndef _option(pattern):\n    """"""\n    Get the option according to the pattern.\n    (pattern 0: Choose training or restore; pattern 1: Choose best or latest checkpoint.)\n\n    Args:\n        pattern: 0 for training step. 1 for testing step.\n    Returns:\n        The OPTION\n    """"""\n    if pattern == 0:\n        OPTION = input(""[Input] Train or Restore? (T/R): "")\n        while not (OPTION.upper() in [\'T\', \'R\']):\n            OPTION = input(""[Warning] The format of your input is illegal, please re-input: "")\n    if pattern == 1:\n        OPTION = input(""Load Best or Latest Model? (B/L): "")\n        while not (OPTION.isalpha() and OPTION.upper() in [\'B\', \'L\']):\n            OPTION = input(""[Warning] The format of your input is illegal, please re-input: "")\n    return OPTION.upper()\n\n\ndef logger_fn(name, input_file, level=logging.INFO):\n    """"""\n    The Logger.\n\n    Args:\n        name: The name of the logger\n        input_file: The logger file path\n        level: The logger level\n    Returns:\n        The logger\n    """"""\n    tf_logger = logging.getLogger(name)\n    tf_logger.setLevel(level)\n    log_dir = os.path.dirname(input_file)\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    fh = logging.FileHandler(input_file, mode=\'w\')\n    formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\n    fh.setFormatter(formatter)\n    tf_logger.addHandler(fh)\n    return tf_logger\n\n\ndef tab_printer(args, logger):\n    """"""\n    Function to print the logs in a nice tabular format.\n\n    Args:\n        args: Parameters used for the model.\n        logger: The logger\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    t.add_rows([[""Parameter"", ""Value""]])\n    logger.info(\'\\n\' + t.draw())\n\n\ndef get_out_dir(option, logger):\n    """"""\n    Get the out dir.\n\n    Args:\n        option: Train or Restore\n        logger: The logger\n    Returns:\n        The output dir\n    """"""\n    if option == \'T\':\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n        logger.info(""Writing to {0}\\n"".format(out_dir))\n    if option == \'R\':\n        MODEL = input(""[Input] Please input the checkpoints model you want to restore, ""\n                      ""it should be like (1490175368): "")  # The model you want to restore\n\n        while not (MODEL.isdigit() and len(MODEL) == 10):\n            MODEL = input(""[Warning] The format of your input is illegal, please re-input: "")\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", MODEL))\n        logger.info(""Writing to {0}\\n"".format(out_dir))\n    return out_dir\n\n\ndef get_model_name():\n    """"""\n    Get the model name used for test.\n\n    Returns:\n        The model name\n    """"""\n    MODEL = input(""[Input] Please input the model file you want to test, it should be like (1490175368): "")\n\n    while not (MODEL.isdigit() and len(MODEL) == 10):\n        MODEL = input(""[Warning] The format of your input is illegal, ""\n                      ""it should be like (1490175368), please re-input: "")\n    return MODEL\n\n\ndef create_prediction_file(output_file, data_id, all_labels, all_predict_labels, all_predict_scores):\n    """"""\n    Create the prediction file.\n\n    Args:\n        output_file: The all classes predicted results provided by network\n        data_id: The data record id info provided by class Data\n        all_labels: The all origin labels\n        all_predict_labels: The all predict labels by threshold\n        all_predict_scores: The all predict scores by threshold\n    Raises:\n        IOError: If the prediction file is not a .json file\n    """"""\n    if not output_file.endswith(\'.json\'):\n        raise IOError(""[Error] The prediction file is not a json file.""\n                      ""Please make sure the prediction data is a json file."")\n    with open(output_file, \'w\') as fout:\n        data_size = len(all_predict_labels)\n        for i in range(data_size):\n            predict_labels = [int(i) for i in all_predict_labels[i]]\n            predict_scores = [round(i, 4) for i in all_predict_scores[i]]\n            labels = [int(i) for i in all_labels[i]]\n            data_record = OrderedDict([\n                (\'id\', data_id[i]),\n                (\'labels\', labels),\n                (\'predict_labels\', predict_labels),\n                (\'predict_scores\', predict_scores)\n            ])\n            fout.write(json.dumps(data_record, ensure_ascii=False) + \'\\n\')\n\n\ndef get_onehot_label_threshold(scores, threshold=0.5):\n    """"""\n    Get the predicted onehot labels based on the threshold.\n    If there is no predict score greater than threshold, then choose the label which has the max predict score.\n\n    Args:\n        scores: The all classes predicted scores provided by network\n        threshold: The threshold (default: 0.5)\n    Returns:\n        predicted_onehot_labels: The predicted labels (onehot)\n    """"""\n    predicted_onehot_labels = []\n    scores = np.ndarray.tolist(scores)\n    for score in scores:\n        count = 0\n        onehot_labels_list = [0] * len(score)\n        for index, predict_score in enumerate(score):\n            if predict_score >= threshold:\n                onehot_labels_list[index] = 1\n                count += 1\n        if count == 0:\n            max_score_index = score.index(max(score))\n            onehot_labels_list[max_score_index] = 1\n        predicted_onehot_labels.append(onehot_labels_list)\n    return predicted_onehot_labels\n\n\ndef get_onehot_label_topk(scores, top_num=1):\n    """"""\n    Get the predicted onehot labels based on the topK number.\n\n    Args:\n        scores: The all classes predicted scores provided by network\n        top_num: The max topK number (default: 5)\n    Returns:\n        predicted_onehot_labels: The predicted labels (onehot)\n    """"""\n    predicted_onehot_labels = []\n    scores = np.ndarray.tolist(scores)\n    for score in scores:\n        onehot_labels_list = [0] * len(score)\n        max_num_index_list = list(map(score.index, heapq.nlargest(top_num, score)))\n        for i in max_num_index_list:\n            onehot_labels_list[i] = 1\n        predicted_onehot_labels.append(onehot_labels_list)\n    return predicted_onehot_labels\n\n\ndef get_label_threshold(scores, threshold=0.5):\n    """"""\n    Get the predicted labels based on the threshold.\n    If there is no predict score greater than threshold, then choose the label which has the max predict score.\n    Note: Only Used in `test_model.py`\n\n    Args:\n        scores: The all classes predicted scores provided by network\n        threshold: The threshold (default: 0.5)\n    Returns:\n        predicted_labels: The predicted labels\n        predicted_scores: The predicted scores\n    """"""\n    predicted_labels = []\n    predicted_scores = []\n    scores = np.ndarray.tolist(scores)\n    for score in scores:\n        count = 0\n        index_list = []\n        score_list = []\n        for index, predict_score in enumerate(score):\n            if predict_score >= threshold:\n                index_list.append(index)\n                score_list.append(predict_score)\n                count += 1\n        if count == 0:\n            index_list.append(score.index(max(score)))\n            score_list.append(max(score))\n        predicted_labels.append(index_list)\n        predicted_scores.append(score_list)\n    return predicted_labels, predicted_scores\n\n\ndef get_label_topk(scores, top_num=1):\n    """"""\n    Get the predicted labels based on the topK number.\n    Note: Only Used in `test_model.py`\n\n    Args:\n        scores: The all classes predicted scores provided by network\n        top_num: The max topK number (default: 5)\n    Returns:\n        The predicted labels\n    """"""\n    predicted_labels = []\n    predicted_scores = []\n    scores = np.ndarray.tolist(scores)\n    for score in scores:\n        score_list = []\n        index_list = np.argsort(score)[-top_num:]\n        index_list = index_list[::-1]\n        for index in index_list:\n            score_list.append(score[index])\n        predicted_labels.append(np.ndarray.tolist(index_list))\n        predicted_scores.append(score_list)\n    return predicted_labels, predicted_scores\n\n\ndef create_metadata_file(word2vec_file, output_file):\n    """"""\n    Create the metadata file based on the corpus file (Used for the Embedding Visualization later).\n\n    Args:\n        word2vec_file: The word2vec file\n        output_file: The metadata file path\n    Raises:\n        IOError: If word2vec model file doesn\'t exist\n    """"""\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist."")\n\n    model = gensim.models.Word2Vec.load(word2vec_file)\n    word2idx = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n    word2idx_sorted = [(k, word2idx[k]) for k in sorted(word2idx, key=word2idx.get, reverse=False)]\n\n    with open(output_file, \'w+\') as fout:\n        for word in word2idx_sorted:\n            if word[0] is None:\n                print(""[Warning] Empty Line, should replaced by any thing else, or will cause a bug of tensorboard"")\n                fout.write(\'<Empty Line>\' + \'\\n\')\n            else:\n                fout.write(word[0] + \'\\n\')\n\n\ndef load_word2vec_matrix(word2vec_file):\n    """"""\n    Return the word2vec model matrix.\n\n    Args:\n        word2vec_file: The word2vec file\n    Returns:\n        The word2vec model matrix\n    Raises:\n        IOError: If word2vec model file doesn\'t exist\n    """"""\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist. "")\n\n    model = gensim.models.Word2Vec.load(word2vec_file)\n    vocab_size = model.wv.vectors.shape[0]\n    embedding_size = model.vector_size\n    vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n    embedding_matrix = np.zeros([vocab_size, embedding_size])\n    for key, value in vocab.items():\n        if key is not None:\n            embedding_matrix[value] = model[key]\n    return vocab_size, embedding_size, embedding_matrix\n\n\ndef data_word2vec(input_file, num_labels, word2vec_model):\n    """"""\n    Create the research data tokenindex based on the word2vec model file.\n    Return the class _Data() (includes the data tokenindex and data labels).\n\n    Args:\n        input_file: The research data\n        num_labels: The number of classes\n        word2vec_model: The word2vec model file\n    Returns:\n        The Class _Data() (includes the data tokenindex and data labels)\n    Raises:\n        IOError: If the input file is not the .json file\n    """"""\n    vocab = dict([(k, v.index) for (k, v) in word2vec_model.wv.vocab.items()])\n\n    def _token_to_index(content):\n        result = []\n        for item in content:\n            word2id = vocab.get(item)\n            if word2id is None:\n                word2id = 0\n            result.append(word2id)\n        return result\n\n    def _create_onehot_labels(labels_index):\n        label = [0] * num_labels\n        for item in labels_index:\n            label[int(item)] = 1\n        return label\n\n    if not input_file.endswith(\'.json\'):\n        raise IOError(""[Error] The research data is not a json file. ""\n                      ""Please preprocess the research data into the json file."")\n    with open(input_file) as fin:\n        testid_list = []\n        content_index_list = []\n        labels_list = []\n        onehot_labels_list = []\n        labels_num_list = []\n        total_line = 0\n\n        for eachline in fin:\n            data = json.loads(eachline)\n            testid = data[\'testid\']\n            features_content = data[\'features_content\']\n            labels_index = data[\'labels_index\']\n            labels_num = data[\'labels_num\']\n\n            testid_list.append(testid)\n            content_index_list.append(_token_to_index(features_content))\n            labels_list.append(labels_index)\n            onehot_labels_list.append(_create_onehot_labels(labels_index))\n            labels_num_list.append(labels_num)\n            total_line += 1\n\n    class _Data:\n        def __init__(self):\n            pass\n\n        @property\n        def number(self):\n            return total_line\n\n        @property\n        def testid(self):\n            return testid_list\n\n        @property\n        def tokenindex(self):\n            return content_index_list\n\n        @property\n        def labels(self):\n            return labels_list\n\n        @property\n        def onehot_labels(self):\n            return onehot_labels_list\n\n        @property\n        def labels_num(self):\n            return labels_num_list\n\n    return _Data()\n\n\ndef data_augmented(data, drop_rate=1.0):\n    """"""\n    Data augment.\n\n    Args:\n        data: The Class _Data()\n        drop_rate: The drop rate\n    Returns:\n        The Class _AugData()\n    """"""\n    aug_num = data.number\n    aug_testid = data.testid\n    aug_tokenindex = data.tokenindex\n    aug_labels = data.labels\n    aug_onehot_labels = data.onehot_labels\n    aug_labels_num = data.labels_num\n\n    for i in range(len(data.tokenindex)):\n        data_record = data.tokenindex[i]\n        if len(data_record) == 1:  # \xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba 1\xef\xbc\x8c\xe5\x88\x99\xe4\xb8\x8d\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa2\x9e\xe5\xb9\xbf\n            continue\n        elif len(data_record) == 2:  # \xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba 2\xef\xbc\x8c\xe5\x88\x99\xe4\xba\xa4\xe6\x8d\xa2\xe4\xb8\xa4\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\n            data_record[0], data_record[1] = data_record[1], data_record[0]\n            aug_testid.append(data.testid[i])\n            aug_tokenindex.append(data_record)\n            aug_labels.append(data.labels[i])\n            aug_onehot_labels.append(data.onehot_labels[i])\n            aug_labels_num.append(data.labels_num[i])\n            aug_num += 1\n        else:\n            data_record = np.array(data_record)\n            for num in range(len(data_record) // 10):  # \xe6\x89\x93\xe4\xb9\xb1\xe8\xaf\x8d\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\xef\xbc\x8c\xe6\xac\xa1\xe6\x95\xb0\xe5\x8d\xb3\xe7\x94\x9f\xe6\x88\x90\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x9b\xe6\xac\xa1\xe6\x95\xb0\xe6\xa0\xb9\xe6\x8d\xae\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\xe8\x80\x8c\xe5\xae\x9a\n                # random shuffle & random drop\n                data_shuffled = np.random.permutation(np.arange(int(len(data_record) * drop_rate)))\n                new_data_record = data_record[data_shuffled]\n\n                aug_testid.append(data.testid[i])\n                aug_tokenindex.append(list(new_data_record))\n                aug_labels.append(data.labels[i])\n                aug_onehot_labels.append(data.onehot_labels[i])\n                aug_labels_num.append(data.labels_num[i])\n                aug_num += 1\n\n    class _AugData:\n        def __init__(self):\n            pass\n\n        @property\n        def number(self):\n            return aug_num\n\n        @property\n        def testid(self):\n            return aug_testid\n\n        @property\n        def tokenindex(self):\n            return aug_tokenindex\n\n        @property\n        def labels(self):\n            return aug_labels\n\n        @property\n        def onehot_labels(self):\n            return aug_onehot_labels\n\n        @property\n        def labels_num(self):\n            return aug_labels_num\n\n    return _AugData()\n\n\ndef load_data_and_labels(data_file, num_labels, word2vec_file, data_aug_flag):\n    """"""\n    Load research data from files, splits the data into words and generates labels.\n    Return split sentences, labels and the max sentence length of the research data.\n\n    Args:\n        data_file: The research data\n        num_labels: The number of classes\n        word2vec_file: The word2vec model file\n        data_aug_flag: The flag of data augmented\n    Returns:\n        The class _Data()\n    Raises:\n        IOError: If word2vec model file doesn\'t exist\n    """"""\n    # Load word2vec file\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist. "")\n\n    model = word2vec.Word2Vec.load(word2vec_file)\n\n    # Load data from files and split by words\n    data = data_word2vec(input_file=data_file, num_labels=num_labels, word2vec_model=model)\n    if data_aug_flag:\n        data = data_augmented(data)\n\n    # plot_seq_len(data_file, data)\n\n    return data\n\n\ndef pad_data(data, pad_seq_len):\n    """"""\n    Padding each sentence of research data according to the max sentence length.\n    Return the padded data and data labels.\n\n    Args:\n        data: The research data\n        pad_seq_len: The max sentence length of research data\n    Returns:\n        pad_seq: The padded data\n        labels: The data labels\n    """"""\n    pad_seq = pad_sequences(data.tokenindex, maxlen=pad_seq_len, value=0.)\n    onehot_labels = data.onehot_labels\n    return pad_seq, onehot_labels\n\n\ndef plot_seq_len(data_file, data, percentage=0.98):\n    """"""\n    Visualizing the sentence length of each data sentence.\n\n    Args:\n        data_file: The data_file\n        data: The class Data (includes the data tokenindex and data labels)\n        percentage: The percentage of the total data you want to show\n    """"""\n    if \'train\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Train Sequence Length Distribution Histogram.png\'\n    if \'validation\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Validation Sequence Length Distribution Histogram.png\'\n    if \'test\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Test Sequence Length Distribution Histogram.png\'\n    result = dict()\n    for x in data.tokenindex:\n        if len(x) not in result.keys():\n            result[len(x)] = 1\n        else:\n            result[len(x)] += 1\n    freq_seq = [(key, result[key]) for key in sorted(result.keys())]\n    x = []\n    y = []\n    avg = 0\n    count = 0\n    border_index = []\n    for item in freq_seq:\n        x.append(item[0])\n        y.append(item[1])\n        avg += item[0] * item[1]\n        count += item[1]\n        if count > data.number * percentage:\n            border_index.append(item[0])\n    avg = avg / data.number\n    print(\'The average of the data sequence length is {0}\'.format(avg))\n    print(\'The recommend of padding sequence length should more than {0}\'.format(border_index[0]))\n    xlim(0, 400)\n    plt.bar(x, y)\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n    \xe5\x90\xab\xe6\x9c\x89 yield \xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x99\xae\xe9\x80\x9a\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa Generator.\n    \xe5\x87\xbd\xe6\x95\xb0\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x9a\xe5\xaf\xb9 data\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe5\x88\x86\xe6\x88\x90 num_epochs \xe4\xb8\xaa\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x88epoch\xef\xbc\x89\xef\xbc\x8c\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa epoch \xe5\x86\x85\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c shuffle=True\xef\xbc\x8c\xe5\xb0\xb1\xe5\xb0\x86 data \xe9\x87\x8d\xe6\x96\xb0\xe6\xb4\x97\xe7\x89\x8c\xef\xbc\x8c\n    \xe6\x89\xb9\xe9\x87\x8f\xe7\x94\x9f\xe6\x88\x90 (yield) \xe4\xb8\x80\xe6\x89\xb9\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe9\x87\x8d\xe6\xb4\x97\xe8\xbf\x87\xe7\x9a\x84 data\xef\xbc\x8c\xe6\xaf\x8f\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf batch_size\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe7\x94\x9f\xe6\x88\x90 int(len(data)/batch_size)+1 \xe6\x89\xb9\xe3\x80\x82\n\n    Args:\n        data: The data\n        batch_size: The size of the data batch\n        num_epochs: The number of epochs\n        shuffle: Shuffle or not (default: True)\n    Returns:\n        A batch iterator for data set\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n'"
utils/param_parser.py,0,"b'import argparse\n\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    The default hyperparameters give good results without cross-validation.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run MTC Task."")\n\n    # Data Parameters\n    parser.add_argument(""--train-file"",\n                        nargs=""?"",\n                        default=""../data/Train_sample.json"",\n                        help=""Training data."")\n\n    parser.add_argument(""--validation-file"",\n                        nargs=""?"",\n                        default=""../data/Validation_sample.json"",\n                        help=""Validation data."")\n\n    parser.add_argument(""--test-file"",\n                        nargs=""?"",\n                        default=""../data/Test_sample.json"",\n                        help=""Testing data."")\n\n    parser.add_argument(""--metadata-file"",\n                        nargs=""?"",\n                        default=""../data/metadata.tsv"",\n                        help=""Metadata file for embedding visualization."")\n\n    parser.add_argument(""--word2vec-file"",\n                        nargs=""?"",\n                        default=""../data/word2vec_100.model"",\n                        help=""Word2vec file for embedding characters (the dim need to be the same as embedding dim)."")\n\n    # Model Hyperparameters\n    parser.add_argument(""--pad-seq-len"",\n                        type=int,\n                        default=150,\n                        help=""Padding sequence length of data. (depends on the data)"")\n\n    parser.add_argument(""--embedding-type"",\n                        type=int,\n                        default=1,\n                        help=""The embedding type. (default: 1)"")\n\n    parser.add_argument(""--embedding-dim"",\n                        type=int,\n                        default=100,\n                        help=""Dimensionality of character embedding. (default: 100)"")\n\n    parser.add_argument(""--filter-sizes"",\n                        type=list,\n                        default=[3, 4, 5],\n                        help=""Filter sizes. (default: [3, 4, 5])"")\n\n    parser.add_argument(""--num-filters"",\n                        type=int,\n                        default=128,\n                        help=""Number of filters per filter size. (default: 128)"")\n\n    parser.add_argument(""--pooling-size"",\n                        type=int,\n                        default=3,\n                        help=""Pooling sizes. (default: 3)"")\n\n    parser.add_argument(""--lstm-dim"",\n                        type=int,\n                        default=256,\n                        help=""Dimensionality of LSTM neurons. (default: 256)"")\n\n    parser.add_argument(""--lstm-layers"",\n                        type=int,\n                        default=1,\n                        help=""Number of LSTM layers. (default: 1)"")\n\n    parser.add_argument(""--attention-dim"",\n                        type=int,\n                        default=200,\n                        help=""Dimensionality of Attention neurons. (default: 200)"")\n\n    parser.add_argument(""--attention-hops-dim"",\n                        type=int,\n                        default=30,\n                        help=""Dimensionality of Attention hops. (default: 30)"")\n\n    parser.add_argument(""--fc-dim"",\n                        type=int,\n                        default=512,\n                        help=""Dimensionality for FC neurons. (default: 512)"")\n\n    parser.add_argument(""--dropout-rate"",\n                        type=float,\n                        default=0.5,\n                        help=""Dropout keep probability. (default: 0.5)"")\n\n    parser.add_argument(""--num-classes"",\n                        type=int,\n                        default=661,\n                        help=""Total number of labels. (depends on the task)"")\n\n    parser.add_argument(""--topK"",\n                        type=int,\n                        default=5,\n                        help=""Number of top K prediction classes. (default: 5)"")\n\n    parser.add_argument(""--threshold"",\n                        type=float,\n                        default=0.5,\n                        help=""Threshold for prediction classes. (default: 0.5)"")\n\n    # Training Parameters\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=100,\n                        help=""Number of training epochs. (default: 100)."")\n\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        default=64,\n                        help=""Batch size. (default: 64)"")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.001,\n                        help=""Learning rate. (default: 0.001)"")\n\n    parser.add_argument(""--decay-rate"",\n                        type=float,\n                        default=0.95,\n                        help=""Rate of decay for learning rate. (default: 0.95)"")\n\n    parser.add_argument(""--decay-steps"",\n                        type=int,\n                        default=500,\n                        help=""How many steps before decay learning rate. (default: 500)"")\n\n    parser.add_argument(""--evaluate-steps"",\n                        type=int,\n                        default=50,\n                        help=""Evaluate model on val set after how many steps. (default: 50)"")\n\n    parser.add_argument(""--norm-ratio"",\n                        type=float,\n                        default=1.25,\n                        help=""The ratio of the sum of gradients norms of trainable variable. (default: 1.25)"")\n\n    parser.add_argument(""--l2-lambda"",\n                        type=float,\n                        default=0.0,\n                        help=""L2 regularization lambda. (default: 0.0)"")\n\n    parser.add_argument(""--checkpoint-steps"",\n                        type=int,\n                        default=50,\n                        help=""Save model after how many steps. (default: 50)"")\n\n    parser.add_argument(""--num-checkpoints"",\n                        type=int,\n                        default=10,\n                        help=""Number of checkpoints to store. (default: 10)"")\n\n    # Misc Parameters\n    parser.add_argument(""--allow-soft-placement"",\n                        type=bool,\n                        default=True,\n                        help=""Allow device soft device placement. (default: True)"")\n\n    parser.add_argument(""--log-device-placement"",\n                        type=bool,\n                        default=False,\n                        help=""Log placement of ops on devices. (default: False)"")\n\n    parser.add_argument(""--gpu-options-allow-growth"",\n                        type=bool,\n                        default=True,\n                        help=""Allow gpu options growth. (default: True)"")\n\n    return parser.parse_args()'"
