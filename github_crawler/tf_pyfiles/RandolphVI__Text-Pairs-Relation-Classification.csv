file_path,api_count,code
ABCNN/test_abcnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_abcnn():\n    """"""Test ABCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load abcnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-abcnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_abcnn()\n'"
ABCNN/text_abcnn.py,90,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nimport numpy as np\n\n\nclass TextABCNN(object):\n    """"""A ABCNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, filter_sizes,\n            num_filters, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _cos_sim(input_x1, input_x2):\n            norm1 = tf.square(tf.reduce_sum(tf.square(input_x1), axis=1))\n            norm2 = tf.square(tf.reduce_sum(tf.square(input_x2), axis=1))\n            dot_products = tf.reduce_sum(input_x1 * input_x2, axis=1, name=""cos_sim"")\n            return dot_products / (norm1 * norm2)\n\n        def _make_attention_mat(input_x1, input_x2):\n            # shape of `input_x1` and `input_x2`: [batch_size, embedding_size, sequence_length, 1]\n            # input_x2 need to transpose to the [batch_size, embedding_size, 1, sequence_length]\n            # shape of output: [batch_size, sequence_length, sequence_length]\n            dist = tf.reduce_sum(tf.square(input_x1 - tf.matrix_transpose(input_x2)), axis=1)\n            euclidean = tf.sqrt(tf.maximum(dist, 1e-10))\n            return 1.0 / (1.0 + euclidean)\n\n        def _w_pool(input_x, attention, filter_size, scope):\n            # input_x: [batch_size, num_filters, sequence_length + filter_size - 1, 1]\n            # attention: [batch_size, sequence_length + filter_size - 1]\n            pools = []\n\n            # [batch_size, 1, sequence_length + filter_size - 1, 1]\n            attention = tf.transpose(tf.expand_dims(tf.expand_dims(attention, axis=-1), axis=-1), perm=[0, 2, 1, 3])\n\n            for i in range(sequence_length):\n                # [batch_size, num_filters, filter_size, 1]\n                # reduce_sum => [batch_size, num_filters, 1, 1]\n                pools.append(\n                    tf.reduce_sum(input_x[:, :, i:i + filter_size, :] * attention[:, :, i:i + filter_size, :],\n                                  axis=2, keepdims=True))\n            # [batch_size, num_filters, sequence_length, 1]\n            w_ap = tf.concat(pools, axis=2, name=""w_ap_"" + scope)\n            return w_ap\n\n        def _all_pool(input_x, filter_size, scope):\n            # input_x: [batch_size, num_filters, sequence_length + filter_size -1, 1]\n            all_ap = tf.nn.avg_pool(\n                input_x,\n                ksize=[1, 1, sequence_length + filter_size - 1, 1],\n                strides=[1, 1, 1, 1],\n                padding=""VALID"",\n                name=""all_pool_"" + scope\n            )\n            all_ap_reshaped = tf.reshape(all_ap, shape=[-1, num_filters])\n            return all_ap_reshaped\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n            # transpose the embedding sentence: [batch_size, embedding_size, sequence_length]\n            embedded_sentence_front_trans = tf.transpose(embedded_sentence_front, perm=[0, 2, 1])\n            embedded_sentence_behind_trans = tf.transpose(embedded_sentence_behind, perm=[0, 2, 1])\n\n            # [batch_size, embedding_size, sequence_length, 1]\n            embedded_sentence_expanded_front_trans = tf.expand_dims(embedded_sentence_front_trans, axis=-1)\n            embedded_sentence_expanded_behind_trans = tf.expand_dims(embedded_sentence_behind_trans, axis=-1)\n\n            # shape of `L0_0` and `R0_0`: [batch_size, embedding_size]\n            self.F0_0 = tf.reshape(tf.reduce_mean(embedded_sentence_front, axis=1), shape=[-1, embedding_size])\n            self.B0_0 = tf.reshape(tf.reduce_mean(embedded_sentence_behind, axis=1), shape=[-1, embedding_size])\n\n        # Attention Layer\n        with tf.name_scope(""attention_matrix""):\n            W_a = tf.Variable(tf.truncated_normal(shape=[sequence_length, embedding_size],\n                                                  stddev=0.1, dtype=tf.float32), name=""W_a"")\n            # shape of `attention_matrix`: [batch_size, sequence_length, sequence_length]\n            attention_matrix = _make_attention_mat(embedded_sentence_expanded_front_trans,\n                                                   embedded_sentence_expanded_behind_trans)\n\n            # [batch_size, sequence_length, sequence_length] * [sequence_length, embedding_size]\n            # einsum => [batch_size, sequence_length, embedding_size]\n            # matrix transpose => [batch_size, embedding_size, sequence_length]\n            # expand dims => [batch_size, embedding_size, sequence_length, 1]\n            front_attention = tf.expand_dims(tf.matrix_transpose(\n                tf.einsum(""ijk,kl->ijl"", attention_matrix, W_a)), axis=-1)\n            behind_attention = tf.expand_dims(tf.matrix_transpose(\n                tf.einsum(""ijk,kl->ijl"", tf.matrix_transpose(attention_matrix), W_a)), axis=-1)\n\n            # shape of new `embedded_sentence_expanded_trans`: [batch_size, embedding_size, sequence_length, 2]\n            embedded_sentence_expanded_front_trans = tf.concat([embedded_sentence_expanded_front_trans,\n                                                                front_attention], axis=3)\n            embedded_sentence_expanded_behind_trans = tf.concat([embedded_sentence_expanded_behind_trans,\n                                                                 behind_attention], axis=3)\n\n        # Convolution layer\n        pooled_outputs_wp_front = []\n        pooled_outputs_wp_behind = []\n\n        pooled_outputs_ap_front = []\n        pooled_outputs_ap_behind = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                in_channels = 2  # The in_channels of filter_shape is 2 (two channels, origin + attention)\n\n                # shape of new `embedded_sentence_expanded`\n                # [batch_size, embedding_size, sequence_length + filter_size - 1, 2]\n                input_x1 = tf.pad(embedded_sentence_expanded_front_trans, np.array(\n                    [[0, 0], [0, 0], [filter_size - 1, filter_size - 1], [0, 0]]), mode=""CONSTANT"")\n                input_x2 = tf.pad(embedded_sentence_expanded_behind_trans, np.array(\n                    [[0, 0], [0, 0], [filter_size - 1, filter_size - 1], [0, 0]]), mode=""CONSTANT"")\n\n                filter_shape = [embedding_size, filter_size, in_channels, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv_front = tf.nn.conv2d(\n                    input_x1,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_front"")\n\n                conv_behind = tf.nn.conv2d(\n                    input_x2,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_behind"")\n\n                # Apply nonlinearity\n                # [batch_size, 1, sequence_length + filter_size - 1, num_filters]\n                conv_out_front = tf.nn.relu(tf.nn.bias_add(conv_front, b), name=""relu_front"")\n                conv_out_behind = tf.nn.relu(tf.nn.bias_add(conv_behind, b), name=""relu_behind"")\n\n                # [batch_size, num_filters, sequence_length + filter_size - 1, 1]\n                conv_out_front_trans = tf.transpose(conv_out_front, perm=[0, 3, 2, 1])\n                conv_out_behind_trans = tf.transpose(conv_out_behind, perm=[0, 3, 2, 1])\n\n            with tf.name_scope(""attention-filter{0}"".format(filter_size)):\n                # [batch_size, sequence_length + filter_size - 1, sequence_length + filter_size - 1]\n                attention_matrix_v2 = _make_attention_mat(conv_out_front_trans, conv_out_behind_trans)\n\n                # [batch_size, sequence_length + filter_size - 1]\n                front_attention_v2 = tf.reduce_sum(attention_matrix_v2, axis=2)\n                behind_attention_v2 = tf.reduce_sum(attention_matrix_v2, axis=1)\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # shape of `front_wp`: [batch_size, num_filters, sequence_length, 1]\n                front_wp = _w_pool(input_x=conv_out_front_trans, attention=front_attention_v2,\n                                   filter_size=filter_size, scope=""front"")\n                behind_wp = _w_pool(input_x=conv_out_behind_trans, attention=behind_attention_v2,\n                                    filter_size=filter_size, scope=""behind"")\n\n                # shape of `front_ap`: [batch_size, num_filters]\n                front_ap = _all_pool(input_x=conv_out_front_trans, filter_size=filter_size, scope=""front"")\n                behind_ap = _all_pool(input_x=conv_out_behind_trans, filter_size=filter_size, scope=""behind"")\n\n                pooled_outputs_wp_front.append(front_wp)\n                pooled_outputs_wp_behind.append(behind_wp)\n\n                pooled_outputs_ap_front.append(front_ap)\n                pooled_outputs_ap_behind.append(behind_ap)\n\n        # shape of `FI_1` & `BI_1`: [batch_size, num_filters_total, sequence_length, 1]\n        self.FI_1 = tf.concat(pooled_outputs_wp_front, axis=1)\n        self.BI_1 = tf.concat(pooled_outputs_wp_behind, axis=1)\n\n        # shape of `F0_1` & `B0_1`: [batch_size, num_filters_total]\n        self.F0_1 = tf.concat(pooled_outputs_ap_front, axis=1)\n        self.B0_1 = tf.concat(pooled_outputs_ap_behind, axis=1)\n\n        # Concat Layer\n        num_filters_total = num_filters * len(filter_sizes)\n\n        # shape of `conv_front` & `conv_behind`: [batch_size, embedding_size + num_filters_total]\n        self.conv_front = tf.concat([self.F0_0, self.F0_1], axis=1)\n        self.conv_behind = tf.concat([self.B0_0, self.B0_1], axis=1)\n\n        self.sims = tf.stack([_cos_sim(self.F0_0, self.B0_0), _cos_sim(self.F0_1, self.B0_1)], axis=1)\n        # shape of `conv_combine`: [batch_size, 2 * (embedding_size + num_filters_total)]\n        self.conv_combine = tf.concat([self.conv_front, self.conv_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[2 * (embedding_size + num_filters_total), fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.conv_combine, W, b)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
ABCNN/text_abcnn的副本.py,117,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.contrib.layers import fully_connected, xavier_initializer, l2_regularizer, softmax\n\n\ndef linear(input_, output_size, scope=None):\n    """"""\n    Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n    Args:\n        input_: a tensor or a list of 2D, batch x n, Tensors.\n        output_size: int, second dimension of W[i].\n        scope: VariableScope for the created subgraph; defaults to ""Linear"".\n    Returns:\n        A 2D Tensor with shape [batch x output_size] equal to\n        sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n    Raises:\n        ValueError: if some of the arguments has unspecified or wrong shape.\n    """"""\n\n    shape = input_.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n    if not shape[1]:\n        raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n    input_size = shape[1]\n\n    # Now the computation.\n    with tf.variable_scope(scope or ""SimpleLinear""):\n        W = tf.get_variable(""W"", [output_size, input_size], dtype=input_.dtype)\n        b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n    return tf.nn.xw_plus_b(input_, tf.transpose(W), b)\n\n\ndef highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope=\'Highway\'):\n    """"""\n    Highway Network (cf. http://arxiv.org/abs/1505.00387).\n    t = sigmoid(Wy + b)\n    z = t * g(Wy + b) + (1 - t) * y\n    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n    """"""\n\n    with tf.variable_scope(scope):\n        for idx in range(num_layers):\n            g = f(linear(input_, size, scope=(\'highway_lin_{0}\'.format(idx))))\n            t = tf.sigmoid(linear(input_, size, scope=(\'highway_gate_{0}\'.format(idx))) + bias)\n            output = t * g + (1. - t) * input_\n            input_ = output\n\n    return output\n\n\nclass TextABCNN(object):\n    """"""A ABCNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, num_classes, model_type, vocab_size, fc_hidden_size, embedding_size,\n            embedding_type, filter_sizes, num_filters, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def cos_sim(input_x1, input_x2):\n            norm1 = tf.square(tf.reduce_sum(tf.square(input_x1), axis=1))\n            norm2 = tf.square(tf.reduce_sum(tf.square(input_x2), axis=1))\n            dot_products = tf.reduce_sum(input_x1 * input_x2, axis=1, name=""cos_sim"")\n            return dot_products / (norm1 * norm2)\n\n        def make_attention_mat(input_x1, input_x2):\n            # shape of `input_x1` and `input_x2`: [batch_size, embedding_size, sequence_length, 1]\n            # input_x2 need to transpose to the [batch_size, embedding_size, 1, sequence_length]\n            # shape of output: [batch_size, sequence_length, sequence_length]\n            euclidean = tf.sqrt(tf.reduce_sum(tf.square(input_x1 - tf.matrix_transpose(input_x2)), axis=1))\n            return 1 / (1 + euclidean)\n\n        def w_pool(input_x, attention, filter_size, scope):\n            # input_x: [batch_size, num_filters, sequence_length + filter_size - 1, 1]\n            # attention: [batch_size, sequence_length + filter_size - 1]\n            if model_type in [\'ABCNN2\', \'ABCNN3\']:\n                pools = []\n\n                # [batch_size, 1, sequence_length + filter_size - 1, 1]\n                attention = tf.transpose(tf.expand_dims(tf.expand_dims(attention, axis=-1), axis=-1), perm=[0, 2, 1, 3])\n\n                for i in range(sequence_length):\n                    # [batch_size, num_filters, filter_size, 1]\n                    # reduce_sum => [batch_size, num_filters, 1, 1]\n                    pools.append(\n                        tf.reduce_sum(input_x[:, :, i:i + filter_size, :] * attention[:, :, i:i + filter_size, :],\n                                      axis=2, keepdims=True))\n                # [batch_size, num_filters, sequence_length, 1]\n                w_ap = tf.concat(pools, axis=2, name=""w_ap_"" + scope)\n            else:\n                # [batch_size, num_filters, sequence_length, 1]\n                w_ap = tf.nn.avg_pool(\n                    input_x,\n                    ksize=[1, 1, filter_size, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""w_ap_"" + scope\n                )\n            return w_ap\n\n        def all_pool(input_x, filter_size, scope):\n            # input_x: [batch_size, num_filters, sequence_length + filter_size -1, 1]\n            all_ap = tf.nn.avg_pool(\n                input_x,\n                ksize=[1, 1, sequence_length + filter_size - 1, 1],\n                strides=[1, 1, 1, 1],\n                padding=""VALID"",\n                name=""all_pool_"" + scope\n            )\n            all_ap_reshaped = tf.reshape(all_ap, shape=[-1, num_filters])\n            return all_ap_reshaped\n\n        def cnn_layer(variable_scope, input_x1, input_x2, dims_size):\n            """"""\n            Args:\n                variable_scope: `cnn-1` or `cnn-2`\n                input_x1: [batch_size, dims_size, sequence_length, 1]\n                input_x2: [batch_size, dims_size, sequence_length, 1]\n                dims_size: embedding_size in `cnn-1`, num_filters_total in `cnn-2`\n            """"""\n            pooled_outputs_wp_front = []\n            pooled_outputs_wp_behind = []\n\n            pooled_outputs_ap_front = []\n            pooled_outputs_ap_behind = []\n\n            with tf.name_scope(variable_scope):\n                if model_type in [\'ABCNN1\', \'ABCNN3\']:\n                    # Attention\n                    with tf.name_scope(""attention_matrix""):\n                        W_a = tf.Variable(tf.truncated_normal(shape=[sequence_length, dims_size],\n                                                              stddev=0.1, dtype=tf.float32), name=""W_a"")\n                        # shape of `attention_matrix`: [batch_size, sequence_length, sequence_length]\n                        attention_matrix = make_attention_mat(input_x1, input_x2)\n\n                        # [batch_size, sequence_length, sequence_length] * [sequence_length, dims_size]\n                        # einsum => [batch_size, sequence_length, dims_size]\n                        # matrix transpose => [batch_size, dims_size, sequence_length]\n                        # expand dims => [batch_size, dims_size, sequence_length, 1]\n                        front_attention = tf.expand_dims(tf.matrix_transpose(\n                            tf.einsum(""ijk,kl->ijl"", attention_matrix, W_a)), axis=-1)\n                        behind_attention = tf.expand_dims(tf.matrix_transpose(\n                            tf.einsum(""ijk,kl->ijl"", tf.matrix_transpose(attention_matrix), W_a)), axis=-1)\n\n                        # shape of new `input_x1`: [batch_size, dims_size, sequence_length, 2]\n                        input_x1 = tf.concat([input_x1, front_attention], axis=3)\n                        input_x2 = tf.concat([input_x2, behind_attention], axis=3)\n\n                for filter_size in filter_sizes:\n                    with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                        # Convolution Layer\n                        if model_type in [\'ABCNN1\', \'ABCNN3\']:\n                            # The in_channels of filter_shape is 2 (two channels, origin + attention)\n                            in_channels = 2\n                        else:\n                            in_channels = 1\n\n                        # shape of new `embedded_sentence_expanded_front`\n                        # [batch_size, dims_size, sequence_length + filter_size - 1, 2]\n                        input_x1 = tf.pad(input_x1, np.array(\n                            [[0, 0], [0, 0], [filter_size - 1, filter_size - 1], [0, 0]]), mode=""CONSTANT"")\n                        input_x2 = tf.pad(input_x2, np.array(\n                            [[0, 0], [0, 0], [filter_size - 1, filter_size - 1], [0, 0]]), mode=""CONSTANT"")\n\n                        filter_shape = [dims_size, filter_size, in_channels, num_filters]\n                        W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                        b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                        conv_front = tf.nn.conv2d(\n                            input_x1,\n                            W,\n                            strides=[1, 1, 1, 1],\n                            padding=""VALID"",\n                            name=""conv_front"")\n\n                        conv_behind = tf.nn.conv2d(\n                            input_x2,\n                            W,\n                            strides=[1, 1, 1, 1],\n                            padding=""VALID"",\n                            name=""conv_behind"")\n\n                        # Batch Normalization Layer\n                        conv_bn_front = tf.layers.batch_normalization(\n                            tf.nn.bias_add(conv_front, b), training=self.is_training)\n                        conv_bn_behind = tf.layers.batch_normalization(\n                            tf.nn.bias_add(conv_behind, b), training=self.is_training)\n\n                        # Apply nonlinearity\n                        # [batch_size, 1, sequence_length + filter_size - 1, num_filters]\n                        conv_out_front = tf.nn.relu(conv_bn_front, name=""relu_front"")\n                        conv_out_behind = tf.nn.relu(conv_bn_behind, name=""relu_behind"")\n\n                        # [batch_size, num_filters, sequence_length + filter_size - 1, 1]\n                        conv_out_front_trans = tf.transpose(conv_out_front, perm=[0, 3, 2, 1])\n                        conv_out_behind_trans = tf.transpose(conv_out_behind, perm=[0, 3, 2, 1])\n\n                    front_attention_v2, behind_attention_v2 = None, None\n\n                    if model_type in [\'ABCNN2\', \'ABCNN3\']:\n                        # [batch_size, sequence_length + filter_size - 1, sequence_length + filter_size - 1]\n                        attention_matrix_v2 = make_attention_mat(conv_out_front_trans, conv_out_behind_trans)\n\n                        # [batch_size, sequence_length + filter_size - 1]\n                        front_attention_v2 = tf.reduce_sum(attention_matrix_v2, axis=2)\n                        behind_attention_v2 = tf.reduce_sum(attention_matrix_v2, axis=1)\n\n                    with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                        # shape of `front_wp`: [batch_size, num_filters, sequence_length, 1]\n                        front_wp = w_pool(input_x=conv_out_front_trans, attention=front_attention_v2,\n                                          filter_size=filter_size, scope=""front"")\n                        behind_wp = w_pool(input_x=conv_out_behind_trans, attention=behind_attention_v2,\n                                           filter_size=filter_size, scope=""behind"")\n\n                        # shape of `front_ap`: [batch_size, num_filters]\n                        front_ap = all_pool(input_x=conv_out_front_trans, filter_size=filter_size, scope=""front"")\n                        behind_ap = all_pool(input_x=conv_out_behind_trans, filter_size=filter_size, scope=""behind"")\n\n                        pooled_outputs_wp_front.append(front_wp)\n                        pooled_outputs_wp_behind.append(behind_wp)\n\n                        pooled_outputs_ap_front.append(front_ap)\n                        pooled_outputs_ap_behind.append(behind_ap)\n\n                # shape of `FI_1` & `BI_1`: [batch_size, num_filters * len(filter_sizes), sequence_length, 1]\n                FI_1 = tf.concat(pooled_outputs_wp_front, axis=1)\n                BI_1 = tf.concat(pooled_outputs_wp_behind, axis=1)\n\n                # shape of `F0_1` & `B0_1`: [batch_size, num_filters * len(filter_sizes)]\n                F0_1 = tf.concat(pooled_outputs_ap_front, axis=1)\n                B0_1 = tf.concat(pooled_outputs_ap_behind, axis=1)\n\n                print(F0_1)\n\n                return FI_1, F0_1, BI_1, B0_1\n\n        # Embedding Layer\n        with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n            self.embedded_sentence_expanded_front = tf.expand_dims(self.embedded_sentence_front, axis=-1)\n            self.embedded_sentence_expanded_behind = tf.expand_dims(self.embedded_sentence_behind, axis=-1)\n\n        self.embedded_sentence_front_trans = tf.transpose(self.embedded_sentence_front, perm=[0, 2, 1])\n        self.embedded_sentence_behind_trans = tf.transpose(self.embedded_sentence_behind, perm=[0, 2, 1])\n\n        # [batch_size, embedding_size, sequence_length, 1]\n        self.embedded_sentence_expanded_front_trans = tf.expand_dims(self.embedded_sentence_front_trans, axis=-1)\n        self.embedded_sentence_expanded_behind_trans = tf.expand_dims(self.embedded_sentence_behind_trans, axis=-1)\n\n        # Average-pooling Layer\n        with tf.name_scope(""input-all-avg_pool""):\n            self.embedded_sentence_front_avg_pool = tf.nn.avg_pool(\n                self.embedded_sentence_expanded_front,\n                ksize=[1, sequence_length, 1, 1],\n                strides=[1, 1, 1, 1],\n                padding=""VALID"",\n                name=""all_avg_pool_front""\n            )\n\n            self.embedded_sentence_behind_avg_pool = tf.nn.avg_pool(\n                self.embedded_sentence_expanded_behind,\n                ksize=[1, sequence_length, 1, 1],\n                strides=[1, 1, 1, 1],\n                padding=""VALID"",\n                name=""all_avg_pool_behind""\n            )\n            # shape of `L0_0` and `R0_0`: [batch_size, embedding_size]\n            self.F0_0 = tf.reshape(self.embedded_sentence_front_avg_pool, shape=[-1, embedding_size])\n            self.B0_0 = tf.reshape(self.embedded_sentence_behind_avg_pool, shape=[-1, embedding_size])\n\n        sims = []\n        conv_out_front_list = []\n        conv_out_behind_list = []\n\n        # Convolution Layer\n        num_filters_total = num_filters * len(filter_sizes)\n        # shape of `FI_1` & `BI_1`: [batch_size, num_filters_total, sequence_length, 1]\n        # shape of `F0_1` & `B0_1`: [batch_size, num_filters_total]\n        self.FI_1, self.F0_1, self.BI_1, self.B0_1 = cnn_layer(\n            variable_scope=""CNN-1"", input_x1=self.embedded_sentence_expanded_front_trans,\n            input_x2=self.embedded_sentence_expanded_behind_trans, dims_size=embedding_size)\n\n        sims.append(cos_sim(self.F0_0, self.B0_0))\n        sims.append(cos_sim(self.F0_1, self.B0_1))\n\n        conv_out_front_list.append(self.F0_0)\n        conv_out_behind_list.append(self.B0_0)\n\n        conv_out_front_list.append(self.F0_1)\n        conv_out_behind_list.append(self.B0_1)\n\n        # shape of `F0_2` & `B0_2`: [batch_size, num_filters_total]\n        _, self.F0_2, _, self.B0_2 = cnn_layer(\n            variable_scope=""CNN-2"", input_x1=self.FI_1, input_x2=self.BI_1, dims_size=num_filters_total)\n\n        sims.append(cos_sim(self.F0_2, self.B0_2))\n\n        conv_out_front_list.append(self.F0_2)\n        conv_out_behind_list.append(self.B0_2)\n\n        # shape of `conv_front` & `conv_behind`: [batch_size, embedding_size + num_filters_total * 2]\n        self.conv_front = tf.concat(conv_out_front_list, axis=1)\n        self.conv_behind = tf.concat(conv_out_behind_list, axis=1)\n\n        self.sims = tf.transpose(tf.stack(sims, axis=1))\n        # shape of `conv_combine`: [batch_size, 2 * (embedding_size + num_filters_total * 2)]\n        self.conv_combine = tf.concat([self.conv_front, self.conv_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[2 * (embedding_size + num_filters_total * 2), fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.conv_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        self.highway = highway(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0, scope=""Highway"")\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")\n\n        # Accuracy\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n\n        # Number of correct predictions\n        with tf.name_scope(""num_correct""):\n            correct = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.num_correct = tf.reduce_sum(tf.cast(correct, ""float""), name=""num_correct"")\n\n        # Calculate Fp\n        with tf.name_scope(""fp""):\n            fp = tf.metrics.false_positives(labels=tf.argmax(self.input_y, 1), predictions=self.predictions)\n            self.fp = tf.reduce_sum(tf.cast(fp, ""float""), name=""fp"")\n\n        # Calculate Fn\n        with tf.name_scope(""fn""):\n            fn = tf.metrics.false_negatives(labels=tf.argmax(self.input_y, 1), predictions=self.predictions)\n            self.fn = tf.reduce_sum(tf.cast(fn, ""float""), name=""fn"")\n\n        # Calculate Recall\n        with tf.name_scope(""recall""):\n            self.recall = self.num_correct / (self.num_correct + self.fn)\n\n        # Calculate Precision\n        with tf.name_scope(""precision""):\n            self.precision = self.num_correct / (self.num_correct + self.fp)\n\n        # Calculate F1\n        with tf.name_scope(""F1""):\n            self.F1 = (2 * self.precision * self.recall) / (self.precision + self.recall)\n\n        # Calculate AUC\n        with tf.name_scope(""AUC""):\n            self.AUC = tf.metrics.auc(self.softmax_scores, self.input_y, name=""AUC"")\n'"
ABCNN/train_abcnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_abcnn import TextABCNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_abcnn():\n    """"""Training ABCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and abcnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            abcnn = TextABCNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=abcnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(abcnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=abcnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", abcnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load abcnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(abcnn.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    abcnn.input_x_front: x_batch_front,\n                    abcnn.input_x_behind: x_batch_behind,\n                    abcnn.input_y: y_batch,\n                    abcnn.dropout_keep_prob: args.dropout_rate,\n                    abcnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, abcnn.global_step, train_summary_op, abcnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        abcnn.input_x_front: x_batch_val_front,\n                        abcnn.input_x_behind: x_batch_val_behind,\n                        abcnn.input_y: y_batch_val,\n                        abcnn.dropout_keep_prob: 1.0,\n                        abcnn.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [abcnn.global_step, validation_summary_op,\n                         abcnn.topKPreds, abcnn.predictions, abcnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, abcnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_abcnn()\n'"
ANN/test_ann.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_ann():\n    """"""Test ANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load ann model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-ann-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_ann()\n'"
ANN/text_ann.py,45,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextANN(object):\n    """"""A ANN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, fc_hidden_size,\n            num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Combine two sentence embedding representation\n        self.embedded_sentence_combine = tf.concat([self.embedded_sentence_front,\n                                                    self.embedded_sentence_behind], axis=2)\n\n        # Average Vectors\n        self.embedded_sentence_average = tf.reduce_mean(self.embedded_sentence_combine, axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[embedding_size * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.embedded_sentence_average, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
ANN/train_ann.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorboard.plugins import projector\nfrom text_ann import TextANN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_ann():\n    """"""Training ANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and ann object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            ann = TextANN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=ann.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(ann.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=ann.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", ann.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load ann model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(ann.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    ann.input_x_front: x_batch_front,\n                    ann.input_x_behind: x_batch_behind,\n                    ann.input_y: y_batch,\n                    ann.dropout_keep_prob: args.dropout_rate,\n                    ann.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, ann.global_step, train_summary_op, ann.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        ann.input_x_front: x_batch_val_front,\n                        ann.input_x_behind: x_batch_val_behind,\n                        ann.input_y: y_batch_val,\n                        ann.dropout_keep_prob: 1.0,\n                        ann.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [ann.global_step, validation_summary_op,\n                         ann.topKPreds, ann.predictions, ann.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                    if writer:\n                        writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, ann.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_ann()\n'"
CNN/test_cnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_cnn():\n    """"""Test CNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load cnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-cnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_cnn()\n'"
CNN/text_cnn.py,62,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextCNN(object):\n    """"""A CNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, filter_sizes, num_filters,\n            fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n            self.embedded_sentence_expanded_front = tf.expand_dims(self.embedded_sentence_front, axis=-1)\n            self.embedded_sentence_expanded_behind = tf.expand_dims(self.embedded_sentence_behind, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs_front = []\n        pooled_outputs_behind = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv_front = tf.nn.conv2d(\n                    self.embedded_sentence_expanded_front,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_front"")\n\n                conv_behind = tf.nn.conv2d(\n                    self.embedded_sentence_expanded_behind,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_behind"")\n\n                # Batch Normalization Layer\n                conv_bn_front = tf.layers.batch_normalization(tf.nn.bias_add(conv_front, b), training=self.is_training)\n                conv_bn_behind = tf.layers.batch_normalization(tf.nn.bias_add(conv_behind, b), training=self.is_training)\n\n                # Apply nonlinearity\n                conv_out_front = tf.nn.relu(conv_bn_front, name=""relu_front"")\n                conv_out_behind = tf.nn.relu(conv_bn_behind, name=""relu_behind"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                pooled_front = tf.nn.max_pool(\n                    conv_out_front,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool_front"")\n\n                pooled_behind = tf.nn.max_pool(\n                    conv_out_behind,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool_behind"")\n\n            pooled_outputs_front.append(pooled_front)\n            pooled_outputs_behind.append(pooled_behind)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.pool_front = tf.concat(pooled_outputs_front, axis=3)\n        self.pool_behind = tf.concat(pooled_outputs_behind, axis=3)\n        self.pool_flat_front = tf.reshape(self.pool_front, shape=[-1, num_filters_total])\n        self.pool_flat_behind = tf.reshape(self.pool_behind, shape=[-1, num_filters_total])\n\n        self.pool_flat_combine = tf.concat([self.pool_flat_front, self.pool_flat_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.pool_flat_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
CNN/train_cnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorboard.plugins import projector\nfrom text_cnn import TextCNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_cnn():\n    """"""Training CNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and cnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            cnn = TextCNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=cnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(cnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=cnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load cnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(cnn.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    cnn.input_x_front: x_batch_front,\n                    cnn.input_x_behind: x_batch_behind,\n                    cnn.input_y: y_batch,\n                    cnn.dropout_keep_prob: args.dropout_rate,\n                    cnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, cnn.global_step, train_summary_op, cnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        cnn.input_x_front: x_batch_val_front,\n                        cnn.input_x_behind: x_batch_val_behind,\n                        cnn.input_y: y_batch_val,\n                        cnn.dropout_keep_prob: 1.0,\n                        cnn.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [cnn.global_step, validation_summary_op,\n                         cnn.topKPreds, cnn.predictions, cnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, cnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_cnn()\n'"
CRNN/test_crnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_crnn():\n    """"""Test CRNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load crnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-crnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_crnn()\n'"
CRNN/text_crnn.py,77,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextCRNN(object):\n    """"""A CRNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, filter_sizes, num_filters,\n            lstm_hidden_size, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n            self.embedded_sentence_expanded_front = tf.expand_dims(self.embedded_sentence_front, axis=-1)\n            self.embedded_sentence_expanded_behind = tf.expand_dims(self.embedded_sentence_behind, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs_front = []\n        pooled_outputs_behind = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv_front = tf.nn.conv2d(\n                    self.embedded_sentence_expanded_front,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_front"")\n\n                conv_behind = tf.nn.conv2d(\n                    self.embedded_sentence_expanded_behind,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_behind"")\n\n                # Batch Normalization Layer\n                conv_bn_front = tf.layers.batch_normalization(tf.nn.bias_add(conv_front, b), training=self.is_training)\n                conv_bn_behind = tf.layers.batch_normalization(tf.nn.bias_add(conv_behind, b), training=self.is_training)\n\n                # Apply nonlinearity\n                conv_out_front = tf.nn.relu(conv_bn_front, name=""relu_front"")\n                conv_out_behind = tf.nn.relu(conv_bn_behind, name=""relu_behind"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                pooled_front = tf.nn.max_pool(\n                    conv_out_front,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool_front"")\n\n                pooled_behind = tf.nn.max_pool(\n                    conv_out_behind,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool_behind"")\n\n            pooled_outputs_front.append(pooled_front)\n            pooled_outputs_behind.append(pooled_behind)\n\n        # Combine all the pooled features\n        pool_flat_outputs_front = []\n        pool_flat_outputs_behind = []\n\n        for i in pooled_outputs_front:\n            pool_flat = tf.reshape(i, shape=[-1, 1, num_filters])\n            pool_flat = tf.nn.dropout(pool_flat, self.dropout_keep_prob)\n            pool_flat_outputs_front.append(pool_flat)\n\n        for i in pooled_outputs_behind:\n            pool_flat = tf.reshape(i, shape=[-1, 1, num_filters])\n            pool_flat = tf.nn.dropout(pool_flat, self.dropout_keep_prob)\n            pool_flat_outputs_behind.append(pool_flat)\n\n        lstm_outputs_front = []\n        lstm_outputs_behind = []\n\n        # Bi-LSTM Layer\n        for i in range(len(pool_flat_outputs_front)):\n            with tf.variable_scope(""Bi-lstm-{0}"".format(i)):\n                lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n                lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n                if self.dropout_keep_prob is not None:\n                    lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                    lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n                # Creates a dynamic bidirectional recurrent neural network\n                # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n                # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n                # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n                # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n\n                outputs_front, state_front = tf.nn.bidirectional_dynamic_rnn(\n                    lstm_fw_cell, lstm_bw_cell, pool_flat_outputs_front[i], dtype=tf.float32)\n                outputs_behind, state_behind = tf.nn.bidirectional_dynamic_rnn(\n                    lstm_fw_cell, lstm_bw_cell, pool_flat_outputs_behind[i], dtype=tf.float32)\n\n                # Concat output\n                # [batch_size, sequence_length, lstm_hidden_size * 2]\n                lstm_concat_front = tf.concat(outputs_front, axis=2)\n                lstm_concat_behind = tf.concat(outputs_behind, axis=2)\n\n                # [batch_size, lstm_hidden_size * 2]\n                lstm_out_front = tf.reduce_mean(lstm_concat_front, axis=1)\n                lstm_out_behind = tf.reduce_mean(lstm_concat_behind, axis=1)\n\n                # shape of `lstm_outputs`: list -> len(filter_sizes) * [batch_size, lstm_hidden_size * 2]\n                lstm_outputs_front.append(lstm_out_front)\n                lstm_outputs_behind.append(lstm_out_behind)\n\n        # [batch_size, lstm_hidden_size * 2 * len(filter_sizes)]\n        self.lstm_out_front = tf.concat(lstm_outputs_front, axis=1)\n        self.lstm_out_behind = tf.concat(lstm_outputs_behind, axis=1)\n\n        # [batch_size, lstm_hidden_size * 2 * len(filter_sizes) * 2]\n        self.lstm_out_combine = tf.concat([self.lstm_out_front, self.lstm_out_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[lstm_hidden_size * 2 * len(filter_sizes) * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.lstm_out_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = tf.layers.batch_normalization(self.fc, training=self.is_training)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
CRNN/train_crnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_crnn import TextCRNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_crnn():\n    """"""Training CRNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and crnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            crnn = TextCRNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=crnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(crnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=crnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", crnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load crnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(crnn.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    crnn.input_x_front: x_batch_front,\n                    crnn.input_x_behind: x_batch_behind,\n                    crnn.input_y: y_batch,\n                    crnn.dropout_keep_prob: args.dropout_rate,\n                    crnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, crnn.global_step, train_summary_op, crnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        crnn.input_x_front: x_batch_val_front,\n                        crnn.input_x_behind: x_batch_val_behind,\n                        crnn.input_y: y_batch_val,\n                        crnn.dropout_keep_prob: 1.0,\n                        crnn.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [crnn.global_step, validation_summary_op,\n                         crnn.topKPreds, crnn.predictions, crnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, crnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_crnn()\n'"
FastText/test_fast.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_fasttext():\n    """"""Test FASTTEXT model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load fasttext model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-fasttext-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_fasttext()\n'"
FastText/text_fast.py,38,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\n\n\nclass TextFAST(object):\n    """"""A FASTTEXT for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, num_classes,\n             l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Combine two sentence embedding representation\n        self.embedded_sentence_combine = tf.concat([self.embedded_sentence_front,\n                                                    self.embedded_sentence_behind], axis=2)\n\n        # Average Vectors\n        self.embedded_sentence_average = tf.reduce_mean(self.embedded_sentence_combine, axis=1)\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.embedded_sentence_average,\n                                          self.embedded_sentence_average.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[embedding_size * 2, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
FastText/train_fast.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_fast import TextFAST\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_fasttext():\n    """"""Training FASTTEXT model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and fasttext object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            fasttext = TextFAST(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=fasttext.global_step,\n                                                           decay_steps=args.decay_steps, decay_rate=args.decay_rate,\n                                                           staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(fasttext.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=fasttext.global_step,\n                                                     name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", fasttext.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load fasttext model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(fasttext.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    fasttext.input_x_front: x_batch_front,\n                    fasttext.input_x_behind: x_batch_behind,\n                    fasttext.input_y: y_batch,\n                    fasttext.dropout_keep_prob: args.dropout_rate,\n                    fasttext.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, fasttext.global_step, train_summary_op, fasttext.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        fasttext.input_x_front: x_batch_val_front,\n                        fasttext.input_x_behind: x_batch_val_behind,\n                        fasttext.input_y: y_batch_val,\n                        fasttext.dropout_keep_prob: 1.0,\n                        fasttext.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [fasttext.global_step, validation_summary_op,\n                         fasttext.topKPreds, fasttext.predictions, fasttext.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, fasttext.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_fasttext()\n'"
HAN/test_han.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_han():\n    """"""Test HAN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load han model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-han-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_han()\n'"
HAN/text_han.py,64,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextHAN(object):\n    """"""A HAN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size,\n            fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs_front, state_front = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_front, dtype=tf.float32)\n            outputs_behind, state_behind = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_behind, dtype=tf.float32)\n\n        # Concat output\n        # [batch_size, sequence_length, lstm_hidden_size * 2]\n        self.lstm_concat_front = tf.concat(outputs_front, axis=2)\n        self.lstm_concat_behind = tf.concat(outputs_behind, axis=2)\n\n        # Attention Layer\n        with tf.name_scope(""attention""):\n            num_units = self.lstm_concat_front.get_shape().as_list()[-1]  # Get last dimension [lstm_hidden_size * 2]\n            u_attention = tf.Variable(tf.truncated_normal(shape=[num_units], stddev=0.1, dtype=tf.float32),\n                                      name=""u_attention"")\n            # 1. One-Layer MLP\n            # shape of `u`: [batch_size, sequence_length, num_units]\n            u_front = tf.layers.dense(self.lstm_concat_front, num_units, activation=tf.nn.tanh, use_bias=True)\n            u_behind = tf.layers.dense(self.lstm_concat_behind, num_units, activation=tf.nn.tanh, use_bias=True)\n\n            # 2. Compute weight by computing similarity of u and attention vector u_attention\n            # [batch_size, sequence_length, num_units]\n            score_front = tf.multiply(u_front, u_attention)\n            score_behind = tf.multiply(u_behind, u_attention)\n\n            # [batch_size, sequence_length, 1]\n            weight_front = tf.reduce_mean(score_front, axis=2, keepdims=True)\n            weight_behind = tf.reduce_mean(score_behind, axis=2, keepdims=True)\n\n            # 3. Weight sum\n            # [batch_size, num_units]\n            self.attention_front = tf.reduce_sum(tf.multiply(u_front, weight_front), axis=1)\n            self.attention_behind = tf.reduce_sum(tf.multiply(u_behind, weight_behind), axis=1)\n\n            # [batch_size, num_units * 2]\n            self.attention_combine = tf.concat([self.attention_front, self.attention_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_units * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.attention_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
HAN/train_han.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_han import TextHAN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_han():\n    """"""Training HAN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and han object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            han = TextHAN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=han.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(han.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=han.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", han.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load han model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(han.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    han.input_x_front: x_batch_front,\n                    han.input_x_behind: x_batch_behind,\n                    han.input_y: y_batch,\n                    han.dropout_keep_prob: args.dropout_rate,\n                    han.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, han.global_step, train_summary_op, han.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        han.input_x_front: x_batch_val_front,\n                        han.input_x_behind: x_batch_val_behind,\n                        han.input_y: y_batch_val,\n                        han.dropout_keep_prob: 1.0,\n                        han.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [han.global_step, validation_summary_op,\n                         han.topKPreds, han.predictions, han.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, han.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_han()\n'"
RCNN/test_rcnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_rcnn():\n    """"""Test RCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load rcnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-rcnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_rcnn()\n'"
RCNN/text_rcnn.py,79,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextRCNN(object):\n    """"""A RCNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size, filter_sizes,\n            num_filters, fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Add dropout\n        with tf.name_scope(""dropout-input""):\n            self.embedded_sentence_front_drop = tf.nn.dropout(self.embedded_sentence_front, self.dropout_keep_prob)\n            self.embedded_sentence_behind_drop = tf.nn.dropout(self.embedded_sentence_behind, self.dropout_keep_prob)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, lstm_hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs_front, state_front = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_front_drop, dtype=tf.float32)\n            outputs_behind, state_behind = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_behind_drop, dtype=tf.float32)\n\n            # Concat output\n            # shape of `lstm_concat`: [batch_size, sequence_length, lstm_hidden_size * 2]\n            self.lstm_concat_front = tf.concat(outputs_front, axis=2)\n            self.lstm_concat_behind = tf.concat(outputs_behind, axis=2)\n\n            # shape of `lstm_out`: [batch_size, sequence_length, lstm_hidden_size * 2, 1]\n            self.lstm_out_front = tf.expand_dims(self.lstm_concat_front, axis=-1)\n            self.lstm_out_behind = tf.expand_dims(self.lstm_concat_behind, axis=-1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs_front = []\n        pooled_outputs_behind = []\n\n        for filter_size in filter_sizes:\n            with tf.name_scope(""conv-filter{0}"".format(filter_size)):\n                # Convolution Layer\n                filter_shape = [filter_size, lstm_hidden_size * 2, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1, dtype=tf.float32), name=""W"")\n                b = tf.Variable(tf.constant(value=0.1, shape=[num_filters], dtype=tf.float32), name=""b"")\n                conv_front = tf.nn.conv2d(\n                    self.lstm_out_front,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n\n                conv_behind = tf.nn.conv2d(\n                    self.lstm_out_behind,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv_behind"")\n\n                conv_front = tf.nn.bias_add(conv_front, b)\n                conv_behind = tf.nn.bias_add(conv_behind, b)\n\n                # Batch Normalization Layer\n                conv_bn_front = batch_norm(conv_front, is_training=self.is_training,\n                                           trainable=True, updates_collections=None)\n                conv_bn_behind = batch_norm(conv_behind, is_training=self.is_training,\n                                            trainable=True, updates_collections=None)\n\n                # Apply nonlinearity\n                conv_out_front = tf.nn.relu(conv_bn_front, name=""relu_front"")\n                conv_out_behind = tf.nn.relu(conv_bn_behind, name=""relu_behind"")\n\n            with tf.name_scope(""pool-filter{0}"".format(filter_size)):\n                # Maxpooling over the outputs\n                avg_pooled_front = tf.nn.avg_pool(\n                    conv_out_front,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                max_pooled_front = tf.nn.max_pool(\n                    conv_out_front,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                avg_pooled_behind = tf.nn.avg_pool(\n                    conv_out_behind,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                max_pooled_behind = tf.nn.max_pool(\n                    conv_out_behind,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""pool"")\n\n                # shape of `pooled_combine`: [batch_size, 1, 1, num_filters * 2]\n                pooled_combine_front = tf.concat([avg_pooled_front, max_pooled_front], axis=3)\n                pooled_combine_behind = tf.concat([avg_pooled_behind, max_pooled_behind], axis=3)\n\n            pooled_outputs_front.append(pooled_combine_front)\n            pooled_outputs_behind.append(pooled_combine_behind)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n\n        # shape of `pool`: [batch_size, 1, 1, num_filters_total * 2]\n        self.pool_front = tf.concat(pooled_outputs_front, axis=3)\n        self.pool_behind = tf.concat(pooled_outputs_behind, axis=3)\n\n        self.pool_flat_front = tf.reshape(self.pool_front, shape=[-1, num_filters_total * 2])\n        self.pool_flat_behind = tf.reshape(self.pool_behind, shape=[-1, num_filters_total * 2])\n\n        # shape of `pool_flat_combine`: [batch_size, num_filters_total * 2 * 2]\n        self.pool_flat_combine = tf.concat([self.pool_flat_front, self.pool_flat_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[num_filters_total * 2 * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.pool_flat_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
RCNN/train_rcnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_rcnn import TextRCNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_rcnn():\n    """"""Training RCNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and rcnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            rcnn = TextRCNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                filter_sizes=args.filter_sizes,\n                num_filters=args.num_filters,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=rcnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(rcnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=rcnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", rcnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load rcnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(rcnn.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    rcnn.input_x_front: x_batch_front,\n                    rcnn.input_x_behind: x_batch_behind,\n                    rcnn.input_y: y_batch,\n                    rcnn.dropout_keep_prob: args.dropout_rate,\n                    rcnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, rcnn.global_step, train_summary_op, rcnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        rcnn.input_x_front: x_batch_val_front,\n                        rcnn.input_x_behind: x_batch_val_behind,\n                        rcnn.input_y: y_batch_val,\n                        rcnn.dropout_keep_prob: 1.0,\n                        rcnn.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [rcnn.global_step, validation_summary_op,\n                         rcnn.topKPreds, rcnn.predictions, rcnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, rcnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_rcnn()\n'"
RNN/test_rnn.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_rnn():\n    """"""Test RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load rnn model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-rnn-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_rnn()\n'"
RNN/text_rnn.py,56,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass TextRNN(object):\n    """"""A RNN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size,\n            fc_hidden_size, num_classes, l2_reg_lambda=0.0, pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            outputs_sentence_front, state_sentence_front = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_front, dtype=tf.float32)\n\n            outputs_sentence_behind, state_sentence_behind = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_behind, dtype=tf.float32)\n\n        # Concat output\n        # shape of `lstm_concat_front`: [batch_size, sequence_length, lstm_hidden_size * 2]\n        self.lstm_concat_front = tf.concat(outputs_sentence_front, axis=2)\n        self.lstm_concat_behind = tf.concat(outputs_sentence_behind, axis=2)\n\n        # shape of `lstm_out_front`: [batch_size, lstm_hidden_size * 2]\n        self.lstm_out_front = tf.reduce_mean(self.lstm_concat_front, axis=1)\n        self.lstm_out_behind = tf.reduce_mean(self.lstm_concat_behind, axis=1)\n\n        # shape of `lstm_out_concat`: [batch_size, lstm_hidden_size * 2 * 2]\n        self.lstm_out_concat = tf.concat([self.lstm_out_front, self.lstm_out_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[lstm_hidden_size * 2 * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.lstm_out_concat, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
RNN/train_rnn.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_rnn import TextRNN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_rnn():\n    """"""Training RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and rnn object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            rnn = TextRNN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=rnn.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(rnn.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=rnn.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", rnn.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load rnn model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(rnn.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    rnn.input_x_front: x_batch_front,\n                    rnn.input_x_behind: x_batch_behind,\n                    rnn.input_y: y_batch,\n                    rnn.dropout_keep_prob: args.dropout_rate,\n                    rnn.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, rnn.global_step, train_summary_op, rnn.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        rnn.input_x_front: x_batch_val_front,\n                        rnn.input_x_behind: x_batch_val_behind,\n                        rnn.input_y: y_batch_val,\n                        rnn.dropout_keep_prob: 1.0,\n                        rnn.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [rnn.global_step, validation_summary_op,\n                         rnn.topKPreds, rnn.predictions, rnn.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, rnn.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_rnn()\n'"
SANN/test_sann.py,7,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\nimport numpy as np\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport tensorflow as tf\n\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nMODEL = dh.get_model_name()\nlogger = dh.logger_fn(""tflog"", ""logs/Test-{0}.log"".format(time.asctime()))\n\nCPT_DIR = \'runs/\' + MODEL + \'/checkpoints/\'\nBEST_CPT_DIR = \'runs/\' + MODEL + \'/bestcheckpoints/\'\nSAVE_DIR = \'output/\' + MODEL\n\n\ndef test_sann():\n    """"""Test SANN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load data\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    test_data = dh.load_data_and_labels(args.test_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_test_front, x_test_behind, y_test = dh.pad_data(test_data, args.pad_seq_len)\n\n    # Load sann model\n    OPTION = dh._option(pattern=1)\n    if OPTION == \'B\':\n        logger.info(""Loading best model..."")\n        checkpoint_file = cm.get_best_checkpoint(BEST_CPT_DIR, select_maximum_value=True)\n    else:\n        logger.info(""Loading latest model..."")\n        checkpoint_file = tf.train.latest_checkpoint(CPT_DIR)\n    logger.info(checkpoint_file)\n\n    graph = tf.Graph()\n    with graph.as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            # Load the saved meta graph and restore variables\n            saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n            saver.restore(sess, checkpoint_file)\n\n            # Get the placeholders from the graph by name\n            input_x_front = graph.get_operation_by_name(""input_x_front"").outputs[0]\n            input_x_behind = graph.get_operation_by_name(""input_x_behind"").outputs[0]\n            input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n            dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n            is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n\n            # Tensors we want to evaluate\n            predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n            topKPreds = graph.get_operation_by_name(""output/topKPreds"").outputs[0]\n            loss = graph.get_operation_by_name(""loss/loss"").outputs[0]\n\n            # Split the output nodes name by \'|\' if you have several output nodes\n            output_node_names = ""output/predictions|output/topKPreds""\n\n            # Save the .pb model file\n            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,\n                                                                            output_node_names.split(""|""))\n            tf.train.write_graph(output_graph_def, ""graph"", ""graph-sann-{0}.pb"".format(MODEL), as_text=False)\n\n            # Generate batches for one epoch\n            batches_test = dh.batch_iter(list(zip(x_test_front, x_test_behind, y_test)),\n                                         args.batch_size, 1, shuffle=False)\n\n            # Collect the predictions here\n            test_counter, test_loss = 0, 0.0\n            all_labels = []\n            all_predicted_labels = []\n            all_predicted_scores = []\n\n            for batch_test in batches_test:\n                x_batch_test_front, x_batch_test_behind, y_batch_test = zip(*batch_test)\n                feed_dict = {\n                    input_x_front: x_batch_test_front,\n                    input_x_behind: x_batch_test_behind,\n                    input_y: y_batch_test,\n                    dropout_keep_prob: 1.0,\n                    is_training: False\n                }\n\n                batch_predicted_labels, batch_predicted_scores, batch_loss \\\n                    = sess.run([predictions, topKPreds, loss], feed_dict)\n\n                for i in y_batch_test:\n                    all_labels.append(np.argmax(i))\n                for j in batch_predicted_scores:\n                    all_predicted_scores.append(j[0])\n                for k in batch_predicted_labels:\n                    all_predicted_labels.append(k)\n\n                test_loss = test_loss + batch_loss\n                test_counter = test_counter + 1\n\n            test_loss = float(test_loss / test_counter)\n\n            # Calculate Precision & Recall & F1\n            test_acc = accuracy_score(y_true=np.array(all_labels), y_pred=np.array(all_predicted_labels))\n            test_pre = precision_score(y_true=np.array(all_labels),\n                                       y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_rec = recall_score(y_true=np.array(all_labels),\n                                    y_pred=np.array(all_predicted_labels), average=\'micro\')\n            test_F1 = f1_score(y_true=np.array(all_labels),\n                               y_pred=np.array(all_predicted_labels), average=\'micro\')\n\n            # Calculate the average AUC\n            test_auc = roc_auc_score(y_true=np.array(all_labels),\n                                     y_score=np.array(all_predicted_scores), average=\'micro\')\n\n            logger.info(""All Test Dataset: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                        ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                        .format(test_loss, test_acc, test_pre, test_rec, test_F1, test_auc))\n\n            # Save the prediction result\n            if not os.path.exists(SAVE_DIR):\n                os.makedirs(SAVE_DIR)\n            dh.create_prediction_file(output_file=SAVE_DIR + ""/predictions.json"", front_data_id=test_data.front_testid,\n                                      behind_data_id=test_data.behind_testid, all_labels=all_labels,\n                                      all_predict_labels=all_predicted_labels, all_predict_scores=all_predicted_scores)\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    test_sann()\n'"
SANN/text_sann.py,87,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport numpy as np\nimport tensorflow as tf\nimport tflearn\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow import sigmoid\nfrom tensorflow import tanh\nfrom tensorflow.contrib import rnn\nfrom tensorflow.contrib.layers import batch_norm\n\n\nclass BatchNormLSTMCell(rnn.RNNCell):\n    """"""Batch normalized LSTM (cf. http://arxiv.org/abs/1603.09025)""""""\n\n    def __init__(self, num_units, is_training=False, forget_bias=1.0,\n                 activation=tanh, reuse=None):\n        """"""Initialize the BNLSTM cell.\n\n        Args:\n          num_units: int, The number of units in the BNLSTM cell.\n          forget_bias: float, The bias added to forget gates (see above).\n            Must set to `0.0` manually when restoring from CudnnLSTM-trained\n            checkpoints.\n          activation: Activation function of the inner states.  Default: `tanh`.\n          reuse: (optional) Python boolean describing whether to reuse variables\n            in an existing scope.  If not `True`, and the existing scope already has\n            the given variables, an error is raised.\n        """"""\n        self._num_units = num_units\n        self._is_training = is_training\n        self._forget_bias = forget_bias\n        self._activation = activation\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return rnn.LSTMStateTuple(self._num_units, self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__, reuse=self._reuse):\n            c, h = state\n            input_size = inputs.get_shape().as_list()[1]\n            W_xh = tf.get_variable(\'W_xh\',\n                                   [input_size, 4 * self._num_units],\n                                   initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\',\n                                   [self._num_units, 4 * self._num_units],\n                                   initializer=bn_lstm_identity_initializer(0.95))\n            bias = tf.get_variable(\'bias\', [4 * self._num_units])\n\n            xh = tf.matmul(inputs, W_xh)\n            hh = tf.matmul(h, W_hh)\n\n            bn_xh = batch_norm(xh, self._is_training)\n            bn_hh = batch_norm(hh, self._is_training)\n\n            hidden = bn_xh + bn_hh + bias\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            i, j, f, o = array_ops.split(value=hidden, num_or_size_splits=4, axis=1)\n\n            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n            bn_new_c = batch_norm(new_c, \'c\', self._is_training)\n            new_h = self._activation(bn_new_c) * sigmoid(o)\n            new_state = rnn.LSTMStateTuple(new_c, new_h)\n\n            return new_h, new_state\n\n\ndef orthogonal(shape):\n    flat_shape = (shape[0], np.prod(shape[1:]))\n    a = np.random.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    return q.reshape(shape)\n\n\ndef bn_lstm_identity_initializer(scale):\n\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        """"""\n        Ugly cause LSTM params calculated in one matrix multiply\n        """"""\n\n        size = shape[0]\n        # gate (j) is identity\n        t = np.zeros(shape)\n        t[:, size:size * 2] = np.identity(size) * scale\n        t[:, :size] = orthogonal([size, size])\n        t[:, size * 2:size * 3] = orthogonal([size, size])\n        t[:, size * 3:] = orthogonal([size, size])\n        return tf.constant(t, dtype=dtype)\n\n    return _initializer\n\n\ndef orthogonal_initializer():\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        return tf.constant(orthogonal(shape), dtype)\n    return _initializer\n\n\nclass TextSANN(object):\n    """"""A SANN for text classification.""""""\n\n    def __init__(\n            self, sequence_length, vocab_size, embedding_type, embedding_size, lstm_hidden_size,\n            attention_unit_size, attention_hops_size, fc_hidden_size, num_classes, l2_reg_lambda=0.0,\n            pretrained_embedding=None):\n\n        # Placeholders for input, output, dropout_prob and training_tag\n        self.input_x_front = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_front"")\n        self.input_x_behind = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x_behind"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n        self.is_training = tf.placeholder(tf.bool, name=""is_training"")\n\n        self.global_step = tf.Variable(0, trainable=False, name=""Global_Step"")\n\n        def _linear(input_, output_size, scope=""SimpleLinear""):\n            """"""\n            Linear map: output[k] = sum_i(Matrix[k, i] * args[i] ) + Bias[k]\n            Args:\n                input_: a tensor or a list of 2D, batch x n, Tensors.\n                output_size: int, second dimension of W[i].\n                scope: VariableScope for the created subgraph; defaults to ""SimpleLinear"".\n            Returns:\n                A 2D Tensor with shape [batch x output_size] equal to\n                sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n            Raises:\n                ValueError: if some of the arguments has unspecified or wrong shape.\n            """"""\n\n            shape = input_.get_shape().as_list()\n            if len(shape) != 2:\n                raise ValueError(""Linear is expecting 2D arguments: {0}"".format(str(shape)))\n            if not shape[1]:\n                raise ValueError(""Linear expects shape[1] of arguments: {0}"".format(str(shape)))\n            input_size = shape[1]\n\n            # Now the computation.\n            with tf.variable_scope(scope):\n                W = tf.get_variable(""W"", [input_size, output_size], dtype=input_.dtype)\n                b = tf.get_variable(""b"", [output_size], dtype=input_.dtype)\n\n            return tf.nn.xw_plus_b(input_, W, b)\n\n        def _highway_layer(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu):\n            """"""\n            Highway Network (cf. http://arxiv.org/abs/1505.00387).\n            t = sigmoid(Wy + b)\n            z = t * g(Wy + b) + (1 - t) * y\n            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n            """"""\n\n            for idx in range(num_layers):\n                g = f(_linear(input_, size, scope=(""highway_lin_{0}"".format(idx))))\n                t = tf.sigmoid(_linear(input_, size, scope=(""highway_gate_{0}"".format(idx))) + bias)\n                output = t * g + (1. - t) * input_\n                input_ = output\n\n            return output\n\n        # Embedding Layer\n        with tf.device(""/cpu:0""), tf.name_scope(""embedding""):\n            # Use random generated the word vector by default\n            # Can also be obtained through our own word vectors trained by our corpus\n            if pretrained_embedding is None:\n                self.embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], minval=-1.0, maxval=1.0,\n                                                               dtype=tf.float32), trainable=True, name=""embedding"")\n            else:\n                if embedding_type == 0:\n                    self.embedding = tf.constant(pretrained_embedding, dtype=tf.float32, name=""embedding"")\n                if embedding_type == 1:\n                    self.embedding = tf.Variable(pretrained_embedding, trainable=True,\n                                                 dtype=tf.float32, name=""embedding"")\n            self.embedded_sentence_front = tf.nn.embedding_lookup(self.embedding, self.input_x_front)\n            self.embedded_sentence_behind = tf.nn.embedding_lookup(self.embedding, self.input_x_behind)\n\n        # Bi-LSTM Layer\n        with tf.name_scope(""Bi-lstm""):\n            lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # forward direction cell\n            lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size)  # backward direction cell\n            if self.dropout_keep_prob is not None:\n                lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob)\n                lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=self.dropout_keep_prob)\n\n            # Creates a dynamic bidirectional recurrent neural network\n            # shape of `outputs`: tuple -> (outputs_fw, outputs_bw)\n            # shape of `outputs_fw`: [batch_size, sequence_length, hidden_size]\n\n            # shape of `state`: tuple -> (outputs_state_fw, output_state_bw)\n            # shape of `outputs_state_fw`: tuple -> (c, h) c: memory cell; h: hidden state\n            outputs_front, state_front = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_front, dtype=tf.float32)\n            outputs_behind, state_front = tf.nn.bidirectional_dynamic_rnn(\n                lstm_fw_cell, lstm_bw_cell, self.embedded_sentence_behind, dtype=tf.float32)\n\n        # Concat output\n        # [batch_size, sequence_length, lstm_hidden_size * 2]\n        self.lstm_out_front = tf.concat(outputs_front, axis=2)\n        self.lstm_out_behind = tf.concat(outputs_behind, axis=2)\n\n        # Add attention\n        with tf.name_scope(""attention""):\n            W_s1 = tf.Variable(tf.truncated_normal(shape=[attention_unit_size, lstm_hidden_size * 2],\n                                                   stddev=0.1, dtype=tf.float32), name=""W_s1"")\n            W_s2 = tf.Variable(tf.truncated_normal(shape=[attention_hops_size, attention_unit_size],\n                                                   stddev=0.1, dtype=tf.float32), name=""W_s2"")\n            self.attention_front = tf.map_fn(\n                fn=lambda x: tf.matmul(W_s2, x),\n                elems=tf.tanh(\n                    tf.map_fn(\n                        fn=lambda x: tf.matmul(W_s1, tf.transpose(x)),\n                        elems=self.lstm_out_front,\n                        dtype=tf.float32\n                    )\n                )\n            )\n\n            self.attention_behind = tf.map_fn(\n                fn=lambda x: tf.matmul(W_s2, x),\n                elems=tf.tanh(\n                    tf.map_fn(\n                        fn=lambda x: tf.matmul(W_s1, tf.transpose(x)),\n                        elems=self.lstm_out_behind,\n                        dtype=tf.float32\n                    )\n                )\n            )\n\n            # [batch_size, attention_hops_size, sequence_length]\n            self.attention_out_front = tf.nn.softmax(self.attention_front)\n            self.attention_out_behind = tf.nn.softmax(self.attention_behind)\n\n        # [batch_size, attention_hops_size, lstm_hidden_size * 2]\n        self.M_front = tf.matmul(self.attention_out_front, self.lstm_out_front)\n        self.M_behind = tf.matmul(self.attention_out_behind, self.lstm_out_behind)\n\n        # shape of `M_flat`: [batch_size, attention_hops_size * lstm_hidden_size * 2]\n        self.M_flat_front = tf.reshape(self.M_front, shape=[-1, attention_hops_size * lstm_hidden_size * 2])\n        self.M_flat_behind = tf.reshape(self.M_behind, shape=[-1, attention_hops_size * lstm_hidden_size * 2])\n\n        # shape of `M_flat_combine`: [batch_size, attention_hops_size * lstm_hidden_size * 2 * 2]\n        self.M_flat_combine = tf.concat([self.M_flat_front, self.M_flat_behind], axis=1)\n\n        # Fully Connected Layer\n        with tf.name_scope(""fc""):\n            W = tf.Variable(tf.truncated_normal(shape=[attention_hops_size * lstm_hidden_size * 2 * 2, fc_hidden_size],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[fc_hidden_size], dtype=tf.float32), name=""b"")\n            self.fc = tf.nn.xw_plus_b(self.M_flat_combine, W, b)\n\n            # Batch Normalization Layer\n            self.fc_bn = batch_norm(self.fc, is_training=self.is_training, trainable=True, updates_collections=None)\n\n            # Apply nonlinearity\n            self.fc_out = tf.nn.relu(self.fc_bn, name=""relu"")\n\n        # Highway Layer\n        with tf.name_scope(""highway""):\n            self.highway = _highway_layer(self.fc_out, self.fc_out.get_shape()[1], num_layers=1, bias=0)\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.highway, self.dropout_keep_prob)\n\n        # Final scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.Variable(tf.truncated_normal(shape=[fc_hidden_size, num_classes],\n                                                stddev=0.1, dtype=tf.float32), name=""W"")\n            b = tf.Variable(tf.constant(value=0.1, shape=[num_classes], dtype=tf.float32), name=""b"")\n            self.logits = tf.nn.xw_plus_b(self.h_drop, W, b, name=""logits"")\n            self.softmax_scores = tf.nn.softmax(self.logits, name=""softmax_scores"")\n            self.predictions = tf.argmax(self.logits, 1, name=""predictions"")\n            self.topKPreds = tf.nn.top_k(self.softmax_scores, k=1, sorted=True, name=""topKPreds"")\n\n        # Calculate mean cross-entropy loss, L2 loss and attention penalization loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.input_y, logits=self.logits)\n            losses = tf.reduce_mean(losses, name=""softmax_losses"")\n            l2_losses = tf.add_n([tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()],\n                                 name=""l2_losses"") * l2_reg_lambda\n            self.loss = tf.add(losses, l2_losses, name=""loss"")'"
SANN/train_sann.py,21,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport sys\nimport time\nimport logging\n\nsys.path.append(\'../\')\nlogging.getLogger(\'tensorflow\').disabled = True\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorboard.plugins import projector\nfrom text_sann import TextSANN\nfrom utils import checkmate as cm\nfrom utils import data_helpers as dh\nfrom utils import param_parser as parser\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\nargs = parser.parameter_parser()\nOPTION = dh._option(pattern=0)\nlogger = dh.logger_fn(""tflog"", ""logs/{0}-{1}.log"".format(\'Train\' if OPTION == \'T\' else \'Restore\', time.asctime()))\n\n\ndef train_sann():\n    """"""Training RNN model.""""""\n    # Print parameters used for the model\n    dh.tab_printer(args, logger)\n\n    # Load sentences, labels, and training parameters\n    logger.info(""Loading data..."")\n    logger.info(""Data processing..."")\n    train_data = dh.load_data_and_labels(args.train_file, args.word2vec_file)\n    validation_data = dh.load_data_and_labels(args.validation_file, args.word2vec_file)\n\n    logger.info(""Data padding..."")\n    x_train_front, x_train_behind, y_train = dh.pad_data(train_data, args.pad_seq_len)\n    x_validation_front, x_validation_behind, y_validation = dh.pad_data(validation_data, args.pad_seq_len)\n\n    # Build vocabulary\n    VOCAB_SIZE, EMBEDDING_SIZE, pretrained_word2vec_matrix = dh.load_word2vec_matrix(args.word2vec_file)\n\n    # Build a graph and sann object\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement=args.allow_soft_placement,\n            log_device_placement=args.log_device_placement)\n        session_conf.gpu_options.allow_growth = args.gpu_options_allow_growth\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            sann = TextSANN(\n                sequence_length=args.pad_seq_len,\n                vocab_size=VOCAB_SIZE,\n                embedding_type=args.embedding_type,\n                embedding_size=EMBEDDING_SIZE,\n                lstm_hidden_size=args.lstm_dim,\n                attention_unit_size=args.attention_dim,\n                attention_hops_size=args.attention_hops_dim,\n                fc_hidden_size=args.fc_dim,\n                num_classes=y_train.shape[1],\n                l2_reg_lambda=args.l2_lambda,\n                pretrained_embedding=pretrained_word2vec_matrix)\n\n            # Define training procedure\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                learning_rate = tf.train.exponential_decay(learning_rate=args.learning_rate,\n                                                           global_step=sann.global_step, decay_steps=args.decay_steps,\n                                                           decay_rate=args.decay_rate, staircase=True)\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n                grads, vars = zip(*optimizer.compute_gradients(sann.loss))\n                grads, _ = tf.clip_by_global_norm(grads, clip_norm=args.norm_ratio)\n                train_op = optimizer.apply_gradients(zip(grads, vars), global_step=sann.global_step, name=""train_op"")\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in zip(grads, vars):\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(""{0}/grad/hist"".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(""{0}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            out_dir = dh.get_out_dir(OPTION, logger)\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n            best_checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""bestcheckpoints""))\n\n            # Summaries for loss\n            loss_summary = tf.summary.scalar(""loss"", sann.loss)\n\n            # Train summaries\n            train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Validation summaries\n            validation_summary_op = tf.summary.merge([loss_summary])\n            validation_summary_dir = os.path.join(out_dir, ""summaries"", ""validation"")\n            validation_summary_writer = tf.summary.FileWriter(validation_summary_dir, sess.graph)\n\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=args.num_checkpoints)\n            best_saver = cm.BestCheckpointSaver(save_dir=best_checkpoint_dir, num_to_keep=3, maximize=True)\n\n            if OPTION == \'R\':\n                # Load sann model\n                logger.info(""Loading model..."")\n                checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n                logger.info(checkpoint_file)\n\n                # Load the saved meta graph and restore variables\n                saver = tf.train.import_meta_graph(""{0}.meta"".format(checkpoint_file))\n                saver.restore(sess, checkpoint_file)\n            if OPTION == \'T\':\n                if not os.path.exists(checkpoint_dir):\n                    os.makedirs(checkpoint_dir)\n                sess.run(tf.global_variables_initializer())\n                sess.run(tf.local_variables_initializer())\n\n                # Embedding visualization config\n                config = projector.ProjectorConfig()\n                embedding_conf = config.embeddings.add()\n                embedding_conf.tensor_name = ""embedding""\n                embedding_conf.metadata_path = args.metadata_file\n\n                projector.visualize_embeddings(train_summary_writer, config)\n                projector.visualize_embeddings(validation_summary_writer, config)\n\n                # Save the embedding visualization\n                saver.save(sess, os.path.join(out_dir, ""embedding"", ""embedding.ckpt""))\n\n            current_step = sess.run(sann.global_step)\n\n            def train_step(x_batch_front, x_batch_behind, y_batch):\n                """"""A single training step""""""\n                feed_dict = {\n                    sann.input_x_front: x_batch_front,\n                    sann.input_x_behind: x_batch_behind,\n                    sann.input_y: y_batch,\n                    sann.dropout_keep_prob: args.dropout_rate,\n                    sann.is_training: True\n                }\n                _, step, summaries, loss = sess.run(\n                    [train_op, sann.global_step, train_summary_op, sann.loss], feed_dict)\n                logger.info(""step {0}: loss {1:g}"".format(step, loss))\n                train_summary_writer.add_summary(summaries, step)\n\n            def validation_step(x_batch_front, x_batch_behind, y_batch, writer=None):\n                """"""Evaluates model on a validation set""""""\n                batches_validation = dh.batch_iter(list(zip(x_batch_front, x_batch_behind, y_batch)),\n                                                   args.batch_size, 1)\n                eval_counter, eval_loss = 0, 0.0\n                true_labels = []\n                predicted_scores = []\n                predicted_labels = []\n\n                for batch_validation in batches_validation:\n                    x_batch_val_front, x_batch_val_behind, y_batch_val = zip(*batch_validation)\n                    feed_dict = {\n                        sann.input_x_front: x_batch_val_front,\n                        sann.input_x_behind: x_batch_val_behind,\n                        sann.input_y: y_batch_val,\n                        sann.dropout_keep_prob: 1.0,\n                        sann.is_training: False\n                    }\n                    step, summaries, scores, predictions, cur_loss = sess.run(\n                        [sann.global_step, validation_summary_op,\n                         sann.topKPreds, sann.predictions, sann.loss], feed_dict)\n\n                    # Prepare for calculating metrics\n                    for i in y_batch_val:\n                        true_labels.append(np.argmax(i))\n                    for j in scores[0]:\n                        predicted_scores.append(j[0])\n                    for k in predictions:\n                        predicted_labels.append(k)\n\n                    eval_loss = eval_loss + cur_loss\n                    eval_counter = eval_counter + 1\n\n                if writer:\n                    writer.add_summary(summaries, step)\n\n                eval_loss = float(eval_loss / eval_counter)\n\n                # Calculate Precision & Recall & F1\n                eval_acc = accuracy_score(y_true=np.array(true_labels), y_pred=np.array(predicted_labels))\n                eval_pre = precision_score(y_true=np.array(true_labels),\n                                           y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_rec = recall_score(y_true=np.array(true_labels),\n                                        y_pred=np.array(predicted_labels), average=\'micro\')\n                eval_F1 = f1_score(y_true=np.array(true_labels),\n                                   y_pred=np.array(predicted_labels), average=\'micro\')\n\n                # Calculate the average AUC\n                eval_auc = roc_auc_score(y_true=np.array(true_labels),\n                                         y_score=np.array(predicted_scores), average=\'micro\')\n\n                return eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc\n\n            # Generate batches\n            batches_train = dh.batch_iter(\n                list(zip(x_train_front, x_train_behind, y_train)), args.batch_size, args.epochs)\n\n            num_batches_per_epoch = int((len(x_train_front) - 1) / args.batch_size) + 1\n\n            # Training loop. For each batch...\n            for batch_train in batches_train:\n                x_batch_front, x_batch_behind, y_batch = zip(*batch_train)\n                train_step(x_batch_front, x_batch_behind, y_batch)\n                current_step = tf.train.global_step(sess, sann.global_step)\n\n                if current_step % args.evaluate_steps == 0:\n                    logger.info(""\\nEvaluation:"")\n                    eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc = \\\n                        validation_step(x_validation_front, x_validation_behind,\n                                        y_validation, writer=validation_summary_writer)\n                    logger.info(""All Validation set: Loss {0:g} | Acc {1:g} | Precision {2:g} | ""\n                                ""Recall {3:g} | F1 {4:g} | AUC {5:g}""\n                                .format(eval_loss, eval_acc, eval_pre, eval_rec, eval_F1, eval_auc))\n                    best_saver.handle(eval_acc, sess, current_step)\n                if current_step % args.checkpoint_steps == 0:\n                    checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    logger.info(""Saved model checkpoint to {0}\\n"".format(path))\n                if current_step % num_batches_per_epoch == 0:\n                    current_epoch = current_step // num_batches_per_epoch\n                    logger.info(""Epoch {0} has finished!"".format(current_epoch))\n\n    logger.info(""All Done."")\n\n\nif __name__ == \'__main__\':\n    train_sann()\n'"
utils/checkmate.py,6,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport glob\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BestCheckpointSaver(object):\n    """"""Maintains a directory containing only the best n checkpoints\n    Inside the directory is a best_checkpoints JSON file containing a dictionary\n    mapping of the best checkpoint filepaths to the values by which the checkpoints\n    are compared.  Only the best n checkpoints are contained in the directory and JSON file.\n    This is a light-weight wrapper class only intended to work in simple,\n    non-distributed settings.  It is not intended to work with the tf.Estimator\n    framework.\n    """"""\n    def __init__(self, save_dir, num_to_keep=1, maximize=True, saver=None):\n        """"""Creates a `BestCheckpointSaver`\n        `BestCheckpointSaver` acts as a wrapper class around a `tf.train.Saver`\n        Args:\n            save_dir: The directory in which the checkpoint files will be saved\n            num_to_keep: The number of best checkpoint files to retain\n            maximize: Define \'best\' values to be the highest values.  For example,\n              set this to True if selecting for the checkpoints with the highest\n              given accuracy.  Or set to False to select for checkpoints with the\n              lowest given error rate.\n            saver: A `tf.train.Saver` to use for saving checkpoints.  A default\n              `tf.train.Saver` will be created if none is provided.\n        """"""\n        self._num_to_keep = num_to_keep\n        self._save_dir = save_dir\n        self._save_path = os.path.join(save_dir, \'model\')\n        self._maximize = maximize\n        self._saver = saver if saver else tf.train.Saver(\n            max_to_keep=None,\n            save_relative_paths=True\n        )\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        self.best_checkpoints_file = os.path.join(save_dir, \'best_checkpoints\')\n\n    def handle(self, value, sess, global_step):\n        """"""Updates the set of best checkpoints based on the given result.\n        Args:\n            value: The value by which to rank the checkpoint.\n            sess: A tf.Session to use to save the checkpoint\n            global_step: The global step\n        """"""\n        current_ckpt = \'model-{}\'.format(global_step)\n        value = float(value)\n        if not os.path.exists(self.best_checkpoints_file):\n            self._save_best_checkpoints_file({current_ckpt: value})\n            self._saver.save(sess, self._save_path, global_step)\n            return\n\n        best_checkpoints = self._load_best_checkpoints_file()\n\n        if len(best_checkpoints) < self._num_to_keep:\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n            self._saver.save(sess, self._save_path, global_step)\n            return\n\n        if self._maximize:\n            should_save = not all(current_best >= value\n                                  for current_best in best_checkpoints.values())\n        else:\n            should_save = not all(current_best <= value\n                                  for current_best in best_checkpoints.values())\n        if should_save:\n            best_checkpoint_list = self._sort(best_checkpoints)\n\n            worst_checkpoint = os.path.join(self._save_dir,\n                                            best_checkpoint_list.pop(-1)[0])\n            self._remove_outdated_checkpoint_files(worst_checkpoint)\n            self._update_internal_saver_state(best_checkpoint_list)\n\n            best_checkpoints = dict(best_checkpoint_list)\n            best_checkpoints[current_ckpt] = value\n            self._save_best_checkpoints_file(best_checkpoints)\n\n            self._saver.save(sess, self._save_path, global_step)\n\n    def _save_best_checkpoints_file(self, updated_best_checkpoints):\n        with open(self.best_checkpoints_file, \'w\') as f:\n            json.dump(updated_best_checkpoints, f, indent=3)\n\n    def _remove_outdated_checkpoint_files(self, worst_checkpoint):\n        os.remove(os.path.join(self._save_dir, \'checkpoint\'))\n        for ckpt_file in glob.glob(worst_checkpoint + \'.*\'):\n            os.remove(ckpt_file)\n\n    def _update_internal_saver_state(self, best_checkpoint_list):\n        best_checkpoint_files = [\n            (ckpt[0], np.inf)  # TODO: Try to use actual file timestamp\n            for ckpt in best_checkpoint_list\n        ]\n        self._saver.set_last_checkpoints_with_time(best_checkpoint_files)\n\n    def _load_best_checkpoints_file(self):\n        with open(self.best_checkpoints_file, \'r\') as f:\n            best_checkpoints = json.load(f)\n        return best_checkpoints\n\n    def _sort(self, best_checkpoints):\n        best_checkpoints = [\n            (ckpt, best_checkpoints[ckpt])\n            for ckpt in sorted(best_checkpoints,\n                               key=best_checkpoints.get,\n                               reverse=self._maximize)\n        ]\n        return best_checkpoints\n\n\ndef get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True):\n    """"""\n    Returns filepath to the best checkpoint\n    Reads the best_checkpoints file in the best_checkpoint_dir directory.\n    Returns the filepath in the best_checkpoints file associated with\n    the highest value if select_maximum_value is True, or the filepath\n    associated with the lowest value if select_maximum_value is False.\n    Args:\n        best_checkpoint_dir: Directory containing best_checkpoints JSON file\n        select_maximum_value: If True, select the filepath associated\n          with the highest value.  Otherwise, select the filepath associated\n          with the lowest value.\n    Returns:\n        The full path to the best checkpoint file\n    """"""\n    best_checkpoints_file = os.path.join(best_checkpoint_dir, \'best_checkpoints\')\n    assert os.path.exists(best_checkpoints_file)\n    with open(best_checkpoints_file, \'r\') as f:\n        best_checkpoints = json.load(f)\n    best_checkpoints = [\n        ckpt for ckpt in sorted(best_checkpoints,\n                                key=best_checkpoints.get,\n                                reverse=select_maximum_value)\n    ]\n    return os.path.join(os.path.abspath(best_checkpoint_dir),  best_checkpoints[0])\n'"
utils/data_helpers.py,0,"b'# -*- coding:utf-8 -*-\n__author__ = \'Randolph\'\n\nimport os\nimport gensim\nimport logging\nimport json\n\nfrom collections import OrderedDict\nfrom pylab import *\nfrom texttable import Texttable\nfrom gensim.models import word2vec\nfrom tflearn.data_utils import to_categorical, pad_sequences\n\nANALYSIS_DIR = \'../data/data_analysis/\'\n\n\ndef _option(pattern):\n    """"""\n    Get the option according to the pattern.\n    (pattern 0: Choose training or restore; pattern 1: Choose best or latest checkpoint.)\n\n    Args:\n        pattern: 0 for training step. 1 for testing step.\n    Returns:\n        The OPTION\n    """"""\n    if pattern == 0:\n        OPTION = input(""[Input] Train or Restore? (T/R): "")\n        while not (OPTION.upper() in [\'T\', \'R\']):\n            OPTION = input(""[Warning] The format of your input is illegal, please re-input: "")\n    if pattern == 1:\n        OPTION = input(""Load Best or Latest Model? (B/L): "")\n        while not (OPTION.isalpha() and OPTION.upper() in [\'B\', \'L\']):\n            OPTION = input(""[Warning] The format of your input is illegal, please re-input: "")\n    return OPTION.upper()\n\n\ndef logger_fn(name, input_file, level=logging.INFO):\n    """"""\n    The Logger.\n\n    Args:\n        name: The name of the logger\n        input_file: The logger file path\n        level: The logger level\n    Returns:\n        The logger\n    """"""\n    tf_logger = logging.getLogger(name)\n    tf_logger.setLevel(level)\n    log_dir = os.path.dirname(input_file)\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    fh = logging.FileHandler(input_file, mode=\'w\')\n    formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\n    fh.setFormatter(formatter)\n    tf_logger.addHandler(fh)\n    return tf_logger\n\n\ndef tab_printer(args, logger):\n    """"""\n    Function to print the logs in a nice tabular format.\n\n    Args:\n        args: Parameters used for the model.\n        logger: The logger\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    t.add_rows([[""Parameter"", ""Value""]])\n    logger.info(\'\\n\' + t.draw())\n\n\ndef get_out_dir(option, logger):\n    """"""\n    Get the out dir.\n\n    Args:\n        option: Train or Restore\n        logger: The logger\n    Returns:\n        The output dir\n    """"""\n    if option == \'T\':\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n        logger.info(""Writing to {0}\\n"".format(out_dir))\n    if option == \'R\':\n        MODEL = input(""[Input] Please input the checkpoints model you want to restore, ""\n                      ""it should be like (1490175368): "")  # The model you want to restore\n\n        while not (MODEL.isdigit() and len(MODEL) == 10):\n            MODEL = input(""[Warning] The format of your input is illegal, please re-input: "")\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", MODEL))\n        logger.info(""Writing to {0}\\n"".format(out_dir))\n    return out_dir\n\n\ndef get_model_name():\n    """"""\n    Get the model name used for test.\n\n    Returns:\n        The model name\n    """"""\n    MODEL = input(""[Input] Please input the model file you want to test, it should be like (1490175368): "")\n\n    while not (MODEL.isdigit() and len(MODEL) == 10):\n        MODEL = input(""[Warning] The format of your input is illegal, ""\n                      ""it should be like (1490175368), please re-input: "")\n    return MODEL\n\n\ndef create_prediction_file(output_file, front_data_id, behind_data_id,\n                           all_labels, all_predict_labels, all_predict_scores):\n    """"""\n    Create the prediction file.\n\n    Args:\n        output_file: The all classes predicted results provided by network\n        front_data_id: The front data record id info provided by class Data\n        behind_data_id: The behind data record id info provided by class Data\n        all_labels: The all origin labels\n        all_predict_labels: The all predict labels by threshold\n        all_predict_scores: The all predict scores by threshold\n    Raises:\n        IOError: If the prediction file is not a .json file\n    """"""\n    # TODO\n    if not output_file.endswith(\'.json\'):\n        raise IOError(""[Error] The prediction file is not a json file.""\n                      ""Please make sure the prediction data is a json file."")\n    with open(output_file, \'w\') as fout:\n        data_size = len(all_predict_labels)\n        for i in range(data_size):\n            labels = int(all_labels[i])\n            predict_labels = int(all_predict_labels[i])\n            predict_scores = round(float(all_predict_scores[i]), 4)\n            data_record = OrderedDict([\n                (\'front_testid\', front_data_id[i]),\n                (\'behind_testid\', behind_data_id[i]),\n                (\'labels\', labels),\n                (\'predict_labels\', predict_labels),\n                (\'predict_scores\', predict_scores)\n            ])\n            fout.write(json.dumps(data_record, ensure_ascii=True) + \'\\n\')\n\n\ndef create_metadata_file(word2vec_file, output_file):\n    """"""\n    Create the metadata file based on the corpus file (Used for the Embedding Visualization later).\n\n    Args:\n        word2vec_file: The word2vec file\n        output_file: The metadata file path\n    Raises:\n        IOError: If word2vec model file doesn\'t exist\n    """"""\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist."")\n\n    model = gensim.models.Word2Vec.load(word2vec_file)\n    word2idx = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n    word2idx_sorted = [(k, word2idx[k]) for k in sorted(word2idx, key=word2idx.get, reverse=False)]\n\n    with open(output_file, \'w+\') as fout:\n        for word in word2idx_sorted:\n            if word[0] is None:\n                print(""[Warning] Empty Line, should replaced by any thing else, or will cause a bug of tensorboard"")\n                fout.write(\'<Empty Line>\' + \'\\n\')\n            else:\n                fout.write(word[0] + \'\\n\')\n\n\ndef load_word2vec_matrix(word2vec_file):\n    """"""\n    Return the word2vec model matrix.\n\n    Args:\n        word2vec_file: The word2vec file\n    Returns:\n        The word2vec model matrix\n    Raises:\n        IOError: If word2vec model file doesn\'t exist\n    """"""\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist. "")\n\n    model = gensim.models.Word2Vec.load(word2vec_file)\n    vocab_size = model.wv.vectors.shape[0]\n    embedding_size = model.vector_size\n    vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n    embedding_matrix = np.zeros([vocab_size, embedding_size])\n    for key, value in vocab.items():\n        if key is not None:\n            embedding_matrix[value] = model[key]\n    return vocab_size, embedding_size, embedding_matrix\n\n\ndef data_word2vec(input_file, word2vec_model):\n    """"""\n    Create the research data tokenindex based on the word2vec model file.\n    Return the class Data (includes the data tokenindex and data labels).\n\n    Args:\n        input_file: The research data\n        word2vec_model: The word2vec model file\n    Returns:\n        The Class Data (includes the data tokenindex and data labels)\n    Raises:\n        IOError: If the input file is not the .json file\n    """"""\n    vocab = dict([(k, v.index) for (k, v) in word2vec_model.wv.vocab.items()])\n\n    def _token_to_index(content):\n        result = []\n        for item in content:\n            word2id = vocab.get(item)\n            if word2id is None:\n                word2id = 0\n            result.append(word2id)\n        return result\n\n    if not input_file.endswith(\'.json\'):\n        raise IOError(""[Error] The research data is not a json file. ""\n                      ""Please preprocess the research data into the json file."")\n    with open(input_file) as fin:\n        labels = []\n        front_testid = []\n        behind_testid = []\n        front_content_indexlist = []\n        behind_content_indexlist = []\n        total_line = 0\n        for eachline in fin:\n            data = json.loads(eachline)\n            front_testid.append(data[\'front_testid\'])\n            behind_testid.append(data[\'behind_testid\'])\n            labels.append(data[\'label\'])\n            front_content_indexlist.append(_token_to_index(data[\'front_features\']))\n            behind_content_indexlist.append(_token_to_index(data[\'behind_features\']))\n            total_line += 1\n\n    class _Data:\n        def __init__(self):\n            pass\n\n        @property\n        def number(self):\n            return total_line\n\n        @property\n        def front_testid(self):\n            return front_testid\n\n        @property\n        def behind_testid(self):\n            return behind_testid\n\n        @property\n        def front_tokenindex(self):\n            return front_content_indexlist\n\n        @property\n        def behind_tokenindex(self):\n            return behind_content_indexlist\n\n        @property\n        def labels(self):\n            return labels\n\n    return _Data()\n\n\ndef load_data_and_labels(data_file, word2vec_file):\n    """"""\n    Load research data from files, splits the data into words and generates labels.\n    Return split sentences, labels and the max sentence length of the research data.\n\n    Args:\n        data_file: The research data\n        word2vec_file: The word2vec model file\n    Returns:\n        The class Data\n    """"""\n    # Load word2vec file\n    if not os.path.isfile(word2vec_file):\n        raise IOError(""[Error] The word2vec file doesn\'t exist. "")\n\n    model = word2vec.Word2Vec.load(word2vec_file)\n\n    # Load data from files and split by words\n    data = data_word2vec(input_file=data_file, word2vec_model=model)\n    # plot_seq_len(data_file, data)\n\n    return data\n\n\ndef pad_data(data, pad_seq_len):\n    """"""\n    Padding each sentence of research data according to the max sentence length.\n    Return the padded data and data labels.\n\n    Args:\n        data: The research data\n        pad_seq_len: The max sentence length of research data\n    Returns:\n        data_front: The padded front data\n        data_behind: The padded behind data\n        onehot_labels: The one-hot labels\n    """"""\n    data_front = pad_sequences(data.front_tokenindex, maxlen=pad_seq_len, value=0.)\n    data_behind = pad_sequences(data.behind_tokenindex, maxlen=pad_seq_len, value=0.)\n    onehot_labels = to_categorical(data.labels, nb_classes=2)\n    return data_front, data_behind, onehot_labels\n\n\ndef plot_seq_len(data_file, data, percentage=0.98):\n    """"""\n    Visualizing the sentence length of each data sentence.\n\n    Args:\n        data_file: The data_file\n        data: The class Data (includes the data tokenindex and data labels)\n        percentage: The percentage of the total data you want to show\n    """"""\n    if \'train\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Train Sequence Length Distribution Histogram.png\'\n    if \'validation\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Validation Sequence Length Distribution Histogram.png\'\n    if \'test\' in data_file.lower():\n        output_file = ANALYSIS_DIR + \'Test Sequence Length Distribution Histogram.png\'\n    result = dict()\n    for x in (data.front_tokenindex + data.behind_tokenindex):\n        if len(x) not in result.keys():\n            result[len(x)] = 1\n        else:\n            result[len(x)] += 1\n    freq_seq = [(key, result[key]) for key in sorted(result.keys())]\n    x = []\n    y = []\n    avg = 0\n    count = 0\n    border_index = []\n    for item in freq_seq:\n        x.append(item[0])\n        y.append(item[1])\n        avg += item[0] * item[1]\n        count += item[1]\n        if (count / 2) > data.number * percentage:\n            border_index.append(item[0])\n    avg = avg / data.number\n    print(\'The average of the data sequence length is {0}\'.format(avg))\n    print(\'The recommend of padding sequence length should more than {0}\'.format(border_index[0]))\n    xlim(0, 200)\n    plt.bar(x, y)\n    plt.savefig(output_file)\n    plt.close()\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    """"""\n    \xe5\x90\xab\xe6\x9c\x89 yield \xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x99\xae\xe9\x80\x9a\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa Generator.\n    \xe5\x87\xbd\xe6\x95\xb0\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x9a\xe5\xaf\xb9 data\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe5\x88\x86\xe6\x88\x90 num_epochs \xe4\xb8\xaa\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x88epoch\xef\xbc\x89\xef\xbc\x8c\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa epoch \xe5\x86\x85\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c shuffle=True\xef\xbc\x8c\xe5\xb0\xb1\xe5\xb0\x86 data \xe9\x87\x8d\xe6\x96\xb0\xe6\xb4\x97\xe7\x89\x8c\xef\xbc\x8c\n    \xe6\x89\xb9\xe9\x87\x8f\xe7\x94\x9f\xe6\x88\x90 (yield) \xe4\xb8\x80\xe6\x89\xb9\xe4\xb8\x80\xe6\x89\xb9\xe7\x9a\x84\xe9\x87\x8d\xe6\xb4\x97\xe8\xbf\x87\xe7\x9a\x84 data\xef\xbc\x8c\xe6\xaf\x8f\xe6\x89\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf batch_size\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe7\x94\x9f\xe6\x88\x90 int(len(data)/batch_size)+1 \xe6\x89\xb9\xe3\x80\x82\n\n    Args:\n        data: The data\n        batch_size: The size of the data batch\n        num_epochs: The number of epochs\n        shuffle: Shuffle or not (default: True)\n    Returns:\n        A batch iterator for data set\n    """"""\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n'"
utils/param_parser.py,0,"b'import argparse\n\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    The default hyperparameters give good results without cross-validation.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run MTC Task."")\n\n    # Data Parameters\n    parser.add_argument(""--train-file"",\n                        nargs=""?"",\n                        default=""../data/Train_sample.json"",\n                        help=""Training data."")\n\n    parser.add_argument(""--validation-file"",\n                        nargs=""?"",\n                        default=""../data/Validation_sample.json"",\n                        help=""Validation data."")\n\n    parser.add_argument(""--test-file"",\n                        nargs=""?"",\n                        default=""../data/Test_sample.json"",\n                        help=""Testing data."")\n\n    parser.add_argument(""--metadata-file"",\n                        nargs=""?"",\n                        default=""../data/metadata.tsv"",\n                        help=""Metadata file for embedding visualization."")\n\n    parser.add_argument(""--word2vec-file"",\n                        nargs=""?"",\n                        default=""../data/word2vec_100.model"",\n                        help=""Word2vec file for embedding characters (the dim need to be the same as embedding dim)."")\n\n    # Model Hyperparameters\n    parser.add_argument(""--pad-seq-len"",\n                        type=int,\n                        default=150,\n                        help=""Padding sequence length of data. (depends on the data)"")\n\n    parser.add_argument(""--embedding-type"",\n                        type=int,\n                        default=1,\n                        help=""The embedding type. (default: 1)"")\n\n    parser.add_argument(""--embedding-dim"",\n                        type=int,\n                        default=100,\n                        help=""Dimensionality of character embedding. (default: 100)"")\n\n    parser.add_argument(""--filter-sizes"",\n                        type=list,\n                        default=[3, 4, 5],\n                        help=""Filter sizes. (default: [3, 4, 5])"")\n\n    parser.add_argument(""--num-filters"",\n                        type=int,\n                        default=128,\n                        help=""Number of filters per filter size. (default: 128)"")\n\n    parser.add_argument(""--pooling-size"",\n                        type=int,\n                        default=3,\n                        help=""Pooling sizes. (default: 3)"")\n\n    parser.add_argument(""--lstm-dim"",\n                        type=int,\n                        default=256,\n                        help=""Dimensionality of LSTM neurons. (default: 256)"")\n\n    parser.add_argument(""--lstm-layers"",\n                        type=int,\n                        default=1,\n                        help=""Number of LSTM layers. (default: 1)"")\n\n    parser.add_argument(""--attention-dim"",\n                        type=int,\n                        default=200,\n                        help=""Dimensionality of Attention neurons. (default: 200)"")\n\n    parser.add_argument(""--attention-hops-dim"",\n                        type=int,\n                        default=30,\n                        help=""Dimensionality of Attention hops. (default: 30)"")\n\n    parser.add_argument(""--fc-dim"",\n                        type=int,\n                        default=512,\n                        help=""Dimensionality for FC neurons. (default: 512)"")\n\n    parser.add_argument(""--dropout-rate"",\n                        type=float,\n                        default=0.5,\n                        help=""Dropout keep probability. (default: 0.5)"")\n\n    parser.add_argument(""--num-classes"",\n                        type=int,\n                        default=661,\n                        help=""Total number of labels. (depends on the task)"")\n\n    parser.add_argument(""--topK"",\n                        type=int,\n                        default=5,\n                        help=""Number of top K prediction classes. (default: 5)"")\n\n    parser.add_argument(""--threshold"",\n                        type=float,\n                        default=0.5,\n                        help=""Threshold for prediction classes. (default: 0.5)"")\n\n    # Training Parameters\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=100,\n                        help=""Number of training epochs. (default: 100)."")\n\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        default=64,\n                        help=""Batch size. (default: 64)"")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.001,\n                        help=""Learning rate. (default: 0.001)"")\n\n    parser.add_argument(""--decay-rate"",\n                        type=float,\n                        default=0.95,\n                        help=""Rate of decay for learning rate. (default: 0.95)"")\n\n    parser.add_argument(""--decay-steps"",\n                        type=int,\n                        default=500,\n                        help=""How many steps before decay learning rate. (default: 500)"")\n\n    parser.add_argument(""--evaluate-steps"",\n                        type=int,\n                        default=50,\n                        help=""Evaluate model on val set after how many steps. (default: 50)"")\n\n    parser.add_argument(""--norm-ratio"",\n                        type=float,\n                        default=1.25,\n                        help=""The ratio of the sum of gradients norms of trainable variable. (default: 1.25)"")\n\n    parser.add_argument(""--l2-lambda"",\n                        type=float,\n                        default=0.0,\n                        help=""L2 regularization lambda. (default: 0.0)"")\n\n    parser.add_argument(""--checkpoint-steps"",\n                        type=int,\n                        default=50,\n                        help=""Save model after how many steps. (default: 50)"")\n\n    parser.add_argument(""--num-checkpoints"",\n                        type=int,\n                        default=10,\n                        help=""Number of checkpoints to store. (default: 10)"")\n\n    # Misc Parameters\n    parser.add_argument(""--allow-soft-placement"",\n                        type=bool,\n                        default=True,\n                        help=""Allow device soft device placement. (default: True)"")\n\n    parser.add_argument(""--log-device-placement"",\n                        type=bool,\n                        default=False,\n                        help=""Log placement of ops on devices. (default: False)"")\n\n    parser.add_argument(""--gpu-options-allow-growth"",\n                        type=bool,\n                        default=True,\n                        help=""Allow gpu options growth. (default: True)"")\n\n    return parser.parse_args()'"
