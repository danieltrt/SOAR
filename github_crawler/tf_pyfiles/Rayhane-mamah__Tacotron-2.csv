file_path,api_count,code
hparams.py,1,"b'import numpy as np\nimport tensorflow as tf\n\n# Default hyperparameters\nhparams = tf.contrib.training.HParams(\n\t# Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n\t# text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"".\n\tcleaners=\'english_cleaners\',\n\n\n\t#If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the GPU idx on run. example:\n\t\t#expample 1 GPU of index 2 (train on ""/gpu2"" only): CUDA_VISIBLE_DEVICES=2 python train.py --model=\'Tacotron\' --hparams=\'tacotron_gpu_start_idx=2\'\n\t#If you want to train on multiple GPUs, simply specify the number of GPUs available, and the idx of the first GPU to use. example:\n\t\t#example 4 GPUs starting from index 0 (train on ""/gpu0""->""/gpu3""): python train.py --model=\'Tacotron\' --hparams=\'tacotron_num_gpus=4, tacotron_gpu_start_idx=0\'\n\t#The hparams arguments can be directly modified on this hparams.py file instead of being specified on run if preferred!\n\n\t#If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be trained on True mel spectrograms), one needs to specify different GPU idxes.\n\t#example Tacotron+WaveNet on a machine with 4 or more GPUs. Two GPUs for each model: \n\t\t# CUDA_VISIBLE_DEVICES=0,1 python train.py --model=\'Tacotron\' --hparams=\'tacotron_num_gpus=2\'\n\t\t# Cuda_VISIBLE_DEVICES=2,3 python train.py --model=\'WaveNet\' --hparams=\'wavenet_num_gpus=2\'\n\n\t#IMPORTANT NOTES: The Multi-GPU performance highly depends on your hardware and optimal parameters change between rigs. Default are optimized for servers.\n\t#If using N GPUs, please multiply the tacotron_batch_size by N below in the hparams! (tacotron_batch_size = 32 * N)\n\t#Never use lower batch size than 32 on a single GPU!\n\t#Same applies for Wavenet: wavenet_batch_size = 8 * N (wavenet_batch_size can be smaller than 8 if GPU is having OOM, minimum 2)\n\t#Please also apply the synthesis batch size modification likewise. (if N GPUs are used for synthesis, minimal batch size must be N, minimum of 1 sample per GPU)\n\t#We did not add an automatic multi-GPU batch size computation to avoid confusion in the user\'s mind and to provide more control to the user for\n\t#resources related decisions.\n\n\t#Acknowledgement:\n\t#\tMany thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a little faster than the original\n\t#\tpipeline for a single GPU as well. Great work!\n\n\t#Hardware setup: Default supposes user has only one GPU: ""/gpu:0"" (Both Tacotron and WaveNet can be trained on multi-GPU: data parallelization)\n\t#Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis.\n\ttacotron_num_gpus = 1, #Determines the number of gpus in use for Tacotron training.\n\twavenet_num_gpus = 1, #Determines the number of gpus in use for WaveNet training.\n\tsplit_on_cpu = True, #Determines whether to split data on CPU or on first GPU. This is automatically True when more than 1 GPU is used. \n\t\t#(Recommend: False on slow CPUs/Disks, True otherwise for small speed boost)\n\t###########################################################################################################################################\n\n\t#Audio\n\t#Audio parameters are the most important parameters to tune when using this work on your personal data. Below are the beginner steps to adapt\n\t#this work to your personal data:\n\t#\t1- Determine my data sample rate: First you need to determine your audio sample_rate (how many samples are in a second of audio). This can be done using sox: ""sox --i <filename>""\n\t#\t\t(For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz, so there are plenty of examples to refer to)\n\t#\t2- set sample_rate parameter to your data correct sample rate\n\t#\t3- Fix win_size and and hop_size accordingly: (Supposing you will follow our advice: 50ms window_size, and 12.5ms frame_shift(hop_size))\n\t#\t\ta- win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200\n\t#\t\tb- hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)\n\t#\t4- Fix n_fft, num_freq and upsample_scales parameters accordingly.\n\t#\t\ta- n_fft can be either equal to win_size or the first power of 2 that comes after win_size. I usually recommend using the latter\n\t#\t\t\tto be more consistent with signal processing friends. No big difference to be seen however. For the tuto example: n_fft = 2048 = 2**11\n\t#\t\tb- num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 + 1 = 1025.\n\t#\t\tc- For WaveNet, upsample_scales products must be equal to hop_size. For the tuto example: upsample_scales=[15, 20] where 15 * 20 = 300\n\t#\t\t\tit is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only keep in mind that upsample_kernel_size[0] = 2*upsample_scales[0]\n\t#\t\t\tso the training segments should be long enough (2.8~3x upsample_scales[0] * hop_size or longer) so that the first kernel size can see the middle \n\t#\t\t\tof the samples efficiently. The length of WaveNet training segments is under the parameter ""max_time_steps"".\n\t#\t5- Finally comes the silence trimming. This very much data dependent, so I suggest trying preprocessing (or part of it, ctrl-C to stop), then use the\n\t#\t\t.ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That will first give you some idea about your above parameters, and\n\t#\t\tit will also give you an idea about trimming. If silences persist, try reducing trim_top_db slowly. If samples are trimmed mid words, try increasing it.\n\t#\t6- If audio quality is too metallic or fragmented (or if linear spectrogram plots are showing black silent regions on top), then restart from step 2.\n\tnum_mels = 80, #Number of mel-spectrogram channels and local conditioning dimensionality\n\tnum_freq = 1025, # (= n_fft / 2 + 1) only used when adding linear spectrograms post processing network\n\trescale = True, #Whether to rescale audio prior to preprocessing\n\trescaling_max = 0.999, #Rescaling value\n\n\t#train samples of lengths between 3sec and 14sec are more than enough to make a model capable of generating consistent speech.\n\tclip_mels_length = True, #For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors, also consider clipping your samples to smaller chunks)\n\tmax_mel_frames = 900,  #Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3 and still getting OOM errors.\n\n\t# Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\n\t# It\'s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\n\t# Does not work if n_ffit is not multiple of hop_size!!\n\tuse_lws=False, #Only used to set as True if using WaveNet, no difference in performance is observed in either cases.\n\tsilence_threshold=2, #silence threshold used for sound trimming for wavenet preprocessing\n\n\t#Mel spectrogram\n\tn_fft = 2048, #Extra window size is filled with 0 paddings to match this parameter\n\thop_size = 275, #For 22050Hz, 275 ~= 12.5 ms (0.0125 * sample_rate)\n\twin_size = 1100, #For 22050Hz, 1100 ~= 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n\tsample_rate = 22050, #22050 Hz (corresponding to ljspeech dataset) (sox --i <filename>)\n\tframe_shift_ms = None, #Can replace hop_size parameter. (Recommended: 12.5)\n\tmagnitude_power = 2., #The power of the spectrogram magnitude (1. for energy, 2. for power)\n\n\t#M-AILABS (and other datasets) trim params (there parameters are usually correct for any data, but definitely must be tuned for specific speakers)\n\ttrim_silence = True, #Whether to clip silence in Audio (at beginning and end of audio only, not the middle)\n\ttrim_fft_size = 2048, #Trimming window size\n\ttrim_hop_size = 512, #Trimmin hop length\n\ttrim_top_db = 40, #Trimming db difference from reference db (smaller==harder trim.)\n\n\t#Mel and Linear spectrograms normalization/scaling and clipping\n\tsignal_normalization = True, #Whether to normalize mel spectrograms to some predefined range (following below parameters)\n\tallow_clipping_in_normalization = True, #Only relevant if mel_normalization = True\n\tsymmetric_mels = True, #Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2, faster and cleaner convergence)\n\tmax_abs_value = 4., #max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not be too big to avoid gradient explosion, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #not too small for fast convergence)\n\tnormalize_for_wavenet = True, #whether to rescale to [0, 1] for wavenet. (better audio quality)\n\tclip_for_wavenet = True, #whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n\twavenet_pad_sides = 1, #Can be 1 or 2. 1 for pad right only, 2 for both sides padding.\n\n\t#Contribution by @begeekmyfriend\n\t#Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude levels. Also allows for better G&L phase reconstruction)\n\tpreemphasize = True, #whether to apply filter\n\tpreemphasis = 0.97, #filter coefficient.\n\n\t#Limits\n\tmin_level_db = -100,\n\tref_level_db = 20,\n\tfmin = 55, #Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n\tfmax = 7600, #To be increased/reduced depending on data.\n\n\t#Griffin Lim\n\tpower = 1.5, #Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n\tgriffin_lim_iters = 60, #Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n\tGL_on_GPU = True, #Whether to use G&L GPU version as part of tensorflow graph. (Usually much faster than CPU but slightly worse quality too).\n\t###########################################################################################################################################\n\n\t#Tacotron\n\t#Model general type\n\toutputs_per_step = 1, #number of frames to generate at each decoding step (increase to speed up computation and allows for higher batch size, decreases G&L audio quality)\n\tstop_at_any = True, #Determines whether the decoder should stop when predicting <stop> to any frame or to all of them (True works pretty well)\n\tbatch_norm_position = \'after\', #Can be in (\'before\', \'after\'). Determines whether we use batch norm before or after the activation function (relu). Matter for debate.\n\tclip_outputs = True, #Whether to clip spectrograms to T2_output_range (even in loss computation). ie: Don\'t penalize model for exceeding output range and bring back to borders.\n\tlower_bound_decay = 0.1, #Small regularizer for noise synthesis by adding small range of penalty for silence regions. Set to 0 to clip in Tacotron range.\n\n\t#Input parameters\n\tembedding_dim = 512, #dimension of embedding space\n\n\t#Encoder parameters\n\tenc_conv_num_layers = 3, #number of encoder convolutional layers\n\tenc_conv_kernel_size = (5, ), #size of encoder convolution filters for each layer\n\tenc_conv_channels = 512, #number of encoder convolutions filters for each layer\n\tencoder_lstm_units = 256, #number of lstm units for each direction (forward and backward)\n\n\t#Attention mechanism\n\tsmoothing = False, #Whether to smooth the attention normalization function\n\tattention_dim = 128, #dimension of attention space\n\tattention_filters = 32, #number of attention convolution filters\n\tattention_kernel = (31, ), #kernel size of attention convolution\n\tcumulative_weights = True, #Whether to cumulate (sum) all previous attention weights or simply feed previous weights (Recommended: True)\n\n\t#Attention synthesis constraints\n\t#""Monotonic"" constraint forces the model to only look at the forwards attention_win_size steps.\n\t#""Window"" allows the model to look at attention_win_size neighbors, both forward and backward steps.\n\tsynthesis_constraint = False,  #Whether to use attention windows constraints in synthesis only (Useful for long utterances synthesis)\n\tsynthesis_constraint_type = \'window\', #can be in (\'window\', \'monotonic\'). \n\tattention_win_size = 7, #Side of the window. Current step does not count. If mode is window and attention_win_size is not pair, the 1 extra is provided to backward part of the window.\n\n\t#Decoder\n\tprenet_layers = [256, 256], #number of layers and number of units of prenet\n\tdecoder_layers = 2, #number of decoder lstm layers\n\tdecoder_lstm_units = 1024, #number of decoder lstm units on each layer\n\tmax_iters = 10000, #Max decoder steps during inference (Just for safety from infinite loop cases)\n\n\t#Residual postnet\n\tpostnet_num_layers = 5, #number of postnet convolutional layers\n\tpostnet_kernel_size = (5, ), #size of postnet convolution filters for each layer\n\tpostnet_channels = 512, #number of postnet convolution filters for each layer\n\n\t#CBHG mel->linear postnet\n\tcbhg_kernels = 8, #All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act as ""K-grams""\n\tcbhg_conv_channels = 128, #Channels of the convolution bank\n\tcbhg_pool_size = 2, #pooling size of the CBHG\n\tcbhg_projection = 256, #projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)\n\tcbhg_projection_kernel_size = 3, #kernel_size of the CBHG projections\n\tcbhg_highwaynet_layers = 4, #Number of HighwayNet layers\n\tcbhg_highway_units = 128, #Number of units used in HighwayNet fully connected layers\n\tcbhg_rnn_units = 128, #Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in shape\n\n\t#Loss params\n\tmask_encoder = True, #whether to mask encoder padding while computing attention. Set to True for better prosody but slower convergence.\n\tmask_decoder = False, #Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not be weighted, else recommended pos_weight = 20)\n\tcross_entropy_pos_weight = 1, #Use class weights to reduce the stop token classes imbalance (by adding more penalty on False Negatives (FN)) (1 = disabled)\n\tpredict_linear = True, #Whether to add a post-processing network to the Tacotron to predict linear spectrograms (True mode Not tested!!)\n\t###########################################################################################################################################\n\n\t#Wavenet\n\t# Input type:\n\t# 1. raw [-1, 1]\n\t# 2. mulaw [-1, 1]\n\t# 3. mulaw-quantize [0, mu]\n\t# If input_type is raw or mulaw, network assumes scalar input and\n\t# discretized mixture of logistic distributions output, otherwise one-hot\n\t# input and softmax output are assumed.\n\t#Model general type\n\tinput_type=""raw"", #Raw has better quality but harder to train. mulaw-quantize is easier to train but has lower quality.\n\tquantize_channels=2**16,  # 65536 (16-bit) (raw) or 256 (8-bit) (mulaw or mulaw-quantize) // number of classes = 256 <=> mu = 255\n\tuse_bias = True, #Whether to use bias in convolutional layers of the Wavenet\n\tlegacy = True, #Whether to use legacy mode: Multiply all skip outputs but the first one with sqrt(0.5) (True for more early training stability, especially for large models)\n\tresidual_legacy = True, #Whether to scale residual blocks outputs by a factor of sqrt(0.5) (True for input variance preservation early in training and better overall stability)\n\n\t#Model Losses parmeters\n\t#Minimal scales ranges for MoL and Gaussian modeling\n\tlog_scale_min=float(np.log(1e-14)), #Mixture of logistic distributions minimal log scale\n\tlog_scale_min_gauss = float(np.log(1e-7)), #Gaussian distribution minimal allowed log scale\n\t#Loss type\n\tcdf_loss = False, #Whether to use CDF loss in Gaussian modeling. Advantages: non-negative loss term and more training stability. (Automatically True for MoL)\n\n\t#model parameters\n\t#To use Gaussian distribution as output distribution instead of mixture of logistics, set ""out_channels = 2"" instead of ""out_channels = 10 * 3"". (UNDER TEST)\n\tout_channels = 2, #This should be equal to quantize channels when input type is \'mulaw-quantize\' else: num_distributions * 3 (prob, mean, log_scale).\n\tlayers = 20, #Number of dilated convolutions (Default: Simplified Wavenet of Tacotron-2 paper)\n\tstacks = 2, #Number of dilated convolution stacks (Default: Simplified Wavenet of Tacotron-2 paper)\n\tresidual_channels = 128, #Number of residual block input/output channels.\n\tgate_channels = 256, #split in 2 in gated convolutions\n\tskip_out_channels = 128, #Number of residual block skip convolution channels.\n\tkernel_size = 3, #The number of inputs to consider in dilated convolutions.\n\n\t#Upsampling parameters (local conditioning)\n\tcin_channels = 80, #Set this to -1 to disable local conditioning, else it must be equal to num_mels!!\n\t#Upsample types: (\'1D\', \'2D\', \'Resize\', \'SubPixel\', \'NearestNeighbor\')\n\t#All upsampling initialization/kernel_size are chosen to omit checkerboard artifacts as much as possible. (Resize is designed to omit that by nature).\n\t#To be specific, all initial upsample weights/biases (when NN_init=True) ensure that the upsampling layers act as a ""Nearest neighbor upsample"" of size ""hop_size"" (checkerboard free).\n\t#1D spans all frequency bands for each frame (channel-wise) while 2D spans ""freq_axis_kernel_size"" bands at a time. Both are vanilla transpose convolutions.\n\t#Resize is a 2D convolution that follows a Nearest Neighbor (NN) resize. For reference, this is: ""NN resize->convolution"".\n\t#SubPixel (2D) is the ICNR version (initialized to be equivalent to ""convolution->NN resize"") of Sub-Pixel convolutions. also called ""checkered artifact free sub-pixel conv"".\n\t#Finally, NearestNeighbor is a non-trainable upsampling layer that just expands each frame (or ""pixel"") to the equivalent hop size. Ignores all upsampling parameters.\n\tupsample_type = \'SubPixel\', #Type of the upsampling deconvolution. Can be (\'1D\' or \'2D\', \'Resize\', \'SubPixel\' or simple \'NearestNeighbor\').\n\tupsample_activation = \'Relu\', #Activation function used during upsampling. Can be (\'LeakyRelu\', \'Relu\' or None)\n\tupsample_scales = [11, 25], #prod(upsample_scales) should be equal to hop_size\n\tfreq_axis_kernel_size = 3, #Only used for 2D upsampling types. This is the number of requency bands that are spanned at a time for each frame.\n\tleaky_alpha = 0.4, #slope of the negative portion of LeakyRelu (LeakyRelu: y=x if x>0 else y=alpha * x)\n\tNN_init = True, #Determines whether we want to initialize upsampling kernels/biases in a way to ensure upsample is initialize to Nearest neighbor upsampling. (Mostly for debug)\n\tNN_scaler = 0.3, #Determines the initial Nearest Neighbor upsample values scale. i.e: upscaled_input_values = input_values * NN_scaler (1. to disable)\n\n\t#global conditioning\n\tgin_channels = -1, #Set this to -1 to disable global conditioning, Only used for multi speaker dataset. It defines the depth of the embeddings (Recommended: 16)\n\tuse_speaker_embedding = True, #whether to make a speaker embedding\n\tn_speakers = 5, #number of speakers (rows of the embedding)\n\tspeakers_path = None, #Defines path to speakers metadata. Can be either in ""speaker\\tglobal_id"" (with header) tsv format, or a single column tsv with speaker names. If None, use ""speakers"".\n\tspeakers = [\'speaker0\', \'speaker1\', #List of speakers used for embeddings visualization. (Consult ""wavenet_vocoder/train.py"" if you want to modify the speaker names source).\n\t\t\t\t\'speaker2\', \'speaker3\', \'speaker4\'], #Must be consistent with speaker ids specified for global conditioning for correct visualization.\n\t###########################################################################################################################################\n\n\t#Tacotron Training\n\t#Reproduction seeds\n\ttacotron_random_seed = 5339, #Determines initial graph and operations (i.e: model) random state for reproducibility\n\ttacotron_data_random_state = 1234, #random state for train test split repeatability\n\n\t#performance parameters\n\ttacotron_swap_with_cpu = False, #Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause major slowdowns! Only use when critical!)\n\n\t#train/test split ratios, mini-batches sizes\n\ttacotron_batch_size = 32, #number of training samples on each training steps\n\t#Tacotron Batch synthesis supports ~16x the training batch size (no gradients during testing). \n\t#Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times different from training. We thus recommend masking the encoder.\n\ttacotron_synthesis_batch_size = 1, #DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN\'T TRAIN TACOTRON WITH ""mask_encoder=True""!!\n\ttacotron_test_size = 0.05, #% of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is enough to have a good idea about overfit)\n\ttacotron_test_batches = None, #number of test batches.\n\n\t#Learning rate schedule\n\ttacotron_decay_learning_rate = True, #boolean, determines if the learning rate will follow an exponential decay\n\ttacotron_start_decay = 40000, #Step at which learning decay starts\n\ttacotron_decay_steps = 18000, #Determines the learning rate decay slope (UNDER TEST)\n\ttacotron_decay_rate = 0.5, #learning rate decay rate (UNDER TEST)\n\ttacotron_initial_learning_rate = 1e-3, #starting learning rate\n\ttacotron_final_learning_rate = 1e-4, #minimal learning rate\n\n\t#Optimization parameters\n\ttacotron_adam_beta1 = 0.9, #AdamOptimizer beta1 parameter\n\ttacotron_adam_beta2 = 0.999, #AdamOptimizer beta2 parameter\n\ttacotron_adam_epsilon = 1e-6, #AdamOptimizer Epsilon parameter\n\n\t#Regularization parameters\n\ttacotron_reg_weight = 1e-6, #regularization weight (for L2 regularization)\n\ttacotron_scale_regularization = False, #Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is high and biasing the model)\n\ttacotron_zoneout_rate = 0.1, #zoneout rate for all LSTM cells in the network\n\ttacotron_dropout_rate = 0.5, #dropout rate for all convolutional layers + prenet\n\ttacotron_clip_gradients = True, #whether to clip gradients\n\n\t#Evaluation parameters\n\ttacotron_natural_eval = False, #Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same teacher-forcing ratio as in training (just for overfit)\n\n\t#Decoder RNN learning can take be done in one of two ways:\n\t#\tTeacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode=\'constant\'\n\t#\tScheduled Sampling Scheme: From Teacher-Forcing to sampling from previous outputs is function of global step. (teacher forcing ratio decay) mode=\'scheduled\'\n\t#The second approach is inspired by:\n\t#Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.\n\t#Can be found under: https://arxiv.org/pdf/1506.03099.pdf\n\ttacotron_teacher_forcing_mode = \'constant\', #Can be (\'constant\' or \'scheduled\'). \'scheduled\' mode applies a cosine teacher forcing ratio decay. (Preference: scheduled)\n\ttacotron_teacher_forcing_ratio = 1., #Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder inputs, Only relevant if mode=\'constant\'\n\ttacotron_teacher_forcing_init_ratio = 1., #initial teacher forcing ratio. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_final_ratio = 0., #final teacher forcing ratio. (Set None to use alpha instead) Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_start_decay = 10000, #starting point of teacher forcing ratio decay. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_decay_steps = 40000, #Determines the teacher forcing ratio decay slope. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_decay_alpha = None, #teacher forcing ratio decay rate. Defines the final tfr as a ratio of initial tfr. Relevant if mode=\'scheduled\'\n\n\t#Speaker adaptation parameters\n\ttacotron_fine_tuning = False, #Set to True to freeze encoder and only keep training pretrained decoder. Used for speaker adaptation with small data.\n\t###########################################################################################################################################\n\n\t#Wavenet Training\n\twavenet_random_seed = 5339, # S=5, E=3, D=9 :)\n\twavenet_data_random_state = 1234, #random state for train test split repeatability\n\n\t#performance parameters\n\twavenet_swap_with_cpu = False, #Whether to use cpu as support to gpu for synthesis computation (while loop).(Not recommended: may cause major slowdowns! Only use when critical!)\n\n\t#train/test split ratios, mini-batches sizes\n\twavenet_batch_size = 8, #batch size used to train wavenet.\n\t#During synthesis, there is no max_time_steps limitation so the model can sample much longer audio than 8k(or 13k) steps. (Audio can go up to 500k steps, equivalent to ~21sec on 24kHz)\n\t#Usually your GPU can handle ~2x wavenet_batch_size during synthesis for the same memory amount during training (because no gradients to keep and ops to register for backprop)\n\twavenet_synthesis_batch_size = 10 * 2, #This ensure that wavenet synthesis goes up to 4x~8x faster when synthesizing multiple sentences. Watch out for OOM with long audios.\n\twavenet_test_size = None, #% of data to keep as test data, if None, wavenet_test_batches must be not None\n\twavenet_test_batches = 1, #number of test batches.\n\n\t#Learning rate schedule\n\twavenet_lr_schedule = \'exponential\', #learning rate schedule. Can be (\'exponential\', \'noam\')\n\twavenet_learning_rate = 1e-3, #wavenet initial learning rate\n\twavenet_warmup = float(4000), #Only used with \'noam\' scheme. Defines the number of ascending learning rate steps.\n\twavenet_decay_rate = 0.5, #Only used with \'exponential\' scheme. Defines the decay rate.\n\twavenet_decay_steps = 200000, #Only used with \'exponential\' scheme. Defines the decay steps.\n\n\t#Optimization parameters\n\twavenet_adam_beta1 = 0.9, #Adam beta1\n\twavenet_adam_beta2 = 0.999, #Adam beta2\n\twavenet_adam_epsilon = 1e-6, #Adam Epsilon\n\n\t#Regularization parameters\n\twavenet_clip_gradients = True, #Whether the clip the gradients during wavenet training.\n\twavenet_ema_decay = 0.9999, #decay rate of exponential moving average\n\twavenet_weight_normalization = False, #Whether to Apply Saliman & Kingma Weight Normalization (reparametrization) technique. (Used in DeepVoice3, not critical here)\n\twavenet_init_scale = 1., #Only relevent if weight_normalization=True. Defines the initial scale in data dependent initialization of parameters.\n\twavenet_dropout = 0.05, #drop rate of wavenet layers\n\twavenet_gradient_max_norm = 100.0, #Norm used to clip wavenet gradients\n\twavenet_gradient_max_value = 5.0, #Value used to clip wavenet gradients\n\n\t#training samples length\n\tmax_time_sec = None, #Max time of audio for training. If None, we use max_time_steps.\n\tmax_time_steps = 11000, #Max time steps in audio used to train wavenet (decrease to save memory) (Recommend: 8000 on modest GPUs, 13000 on stronger ones)\n\n\t#Evaluation parameters\n\twavenet_natural_eval = False, #Whether to use 100% natural eval (to evaluate autoregressivity performance) or with teacher forcing to evaluate overfit and model consistency.\n\n\t#Tacotron-2 integration parameters\n\ttrain_with_GTA = True, #Whether to use GTA mels to train WaveNet instead of ground truth mels.\n\t###########################################################################################################################################\n\n\t#Eval/Debug parameters\n\t#Eval sentences (if no eval text file was specified during synthesis, these sentences are used for eval)\n\tsentences = [\n\t# From July 8, 2017 New York Times:\n\t\'Scientists at the CERN laboratory say they have discovered a new particle.\',\n\t\'There\\\'s a way to measure the acute emotional intelligence that has never gone out of style.\',\n\t\'President Trump met with other leaders at the Group of 20 conference.\',\n\t\'The Senate\\\'s bill to repeal and replace the Affordable Care Act is now imperiled.\',\n\t# From Google\'s Tacotron example page:\n\t\'Generative adversarial network or variational auto-encoder.\',\n\t\'Basilar membrane and otolaryngology are not auto-correlations.\',\n\t\'He has read the whole thing.\',\n\t\'He reads books.\',\n\t\'He thought it was time to present the present.\',\n\t\'Thisss isrealy awhsome.\',\n\t\'The big brown fox jumps over the lazy dog.\',\n\t\'Did the big brown fox jump over the lazy dog?\',\n\t""Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?"",\n\t""She sells sea-shells on the sea-shore. The shells she sells are sea-shells I\'m sure."",\n\t""Tajima Airport serves Toyooka."",\n\t#From The web (random long utterance)\n\t# \'On offering to help the blind man, the man who then stole his car, had not, at that precise moment, had any evil intention, quite the contrary, \\\n\t# what he did was nothing more than obey those feelings of generosity and altruism which, as everyone knows, \\\n\t# are the two best traits of human nature and to be found in much more hardened criminals than this one, a simple car-thief without any hope of advancing in his profession, \\\n\t# exploited by the real owners of this enterprise, for it is they who take advantage of the needs of the poor.\',\n\t# A final Thank you note!\n\t\'Thank you so much for your support!\',\n\t],\n\n\t#Wavenet Debug\n\twavenet_synth_debug = False, #Set True to use target as debug in WaveNet synthesis. \n\twavenet_debug_wavs = [\'training_data/audio/audio-LJ001-0008.npy\'], #Path to debug audios. Must be multiple of wavenet_num_gpus.\n\twavenet_debug_mels = [\'training_data/mels/mel-LJ001-0008.npy\'], #Path to corresponding mels. Must be of same length and order as wavenet_debug_wavs.\n\n\t)\n\ndef hparams_debug_string():\n\tvalues = hparams.values()\n\thp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values) if name != \'sentences\']\n\treturn \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
infolog.py,0,"b""import atexit\nimport json\nfrom datetime import datetime\nfrom threading import Thread\nfrom urllib.request import Request, urlopen\n\n_format = '%Y-%m-%d %H:%M:%S.%f'\n_file = None\n_run_name = None\n_slack_url = None\n\n\ndef init(filename, run_name, slack_url=None):\n\tglobal _file, _run_name, _slack_url\n\t_close_logfile()\n\t_file = open(filename, 'a')\n\t_file = open(filename, 'a')\n\t_file.write('\\n-----------------------------------------------------------------\\n')\n\t_file.write('Starting new {} training run\\n'.format(run_name))\n\t_file.write('-----------------------------------------------------------------\\n')\n\t_run_name = run_name\n\t_slack_url = slack_url\n\n\ndef log(msg, end='\\n', slack=False):\n\tprint(msg, end=end)\n\tif _file is not None:\n\t\t_file.write('[%s]  %s\\n' % (datetime.now().strftime(_format)[:-3], msg))\n\tif slack and _slack_url is not None:\n\t\tThread(target=_send_slack, args=(msg,)).start()\n\n\ndef _close_logfile():\n\tglobal _file\n\tif _file is not None:\n\t\t_file.close()\n\t\t_file = None\n\n\ndef _send_slack(msg):\n\treq = Request(_slack_url)\n\treq.add_header('Content-Type', 'application/json')\n\turlopen(req, json.dumps({\n\t\t'username': 'tacotron',\n\t\t'icon_emoji': ':taco:',\n\t\t'text': '*%s*: %s' % (_run_name, msg)\n\t}).encode())\n\n\natexit.register(_close_logfile)\n"""
paper_hparams.py,1,"b'import numpy as np\nimport tensorflow as tf\n\n# Default hyperparameters\nhparams = tf.contrib.training.HParams(\n\t# Comma-separated list of cleaners to run on text prior to training and eval. For non-English\n\t# text, you may want to use ""basic_cleaners"" or ""transliteration_cleaners"".\n\tcleaners=\'english_cleaners\',\n\n\n\t#If you only have 1 GPU or want to use only one GPU, please set num_gpus=0 and specify the GPU idx on run. example:\n\t\t#expample 1 GPU of index 2 (train on ""/gpu2"" only): CUDA_VISIBLE_DEVICES=2 python train.py --model=\'Tacotron\' --hparams=\'tacotron_gpu_start_idx=2\'\n\t#If you want to train on multiple GPUs, simply specify the number of GPUs available, and the idx of the first GPU to use. example:\n\t\t#example 4 GPUs starting from index 0 (train on ""/gpu0""->""/gpu3""): python train.py --model=\'Tacotron\' --hparams=\'tacotron_num_gpus=4, tacotron_gpu_start_idx=0\'\n\t#The hparams arguments can be directly modified on this hparams.py file instead of being specified on run if preferred!\n\n\t#If one wants to train both Tacotron and WaveNet in parallel (provided WaveNet will be trained on True mel spectrograms), one needs to specify different GPU idxes.\n\t#example Tacotron+WaveNet on a machine with 4 or more GPUs. Two GPUs for each model: \n\t\t# CUDA_VISIBLE_DEVICES=0,1 python train.py --model=\'Tacotron\' --hparams=\'tacotron_num_gpus=2\'\n\t\t# Cuda_VISIBLE_DEVICES=2,3 python train.py --model=\'WaveNet\' --hparams=\'wavenet_num_gpus=2\'\n\n\t#IMPORTANT NOTES: The Multi-GPU performance highly depends on your hardware and optimal parameters change between rigs. Default are optimized for servers.\n\t#If using N GPUs, please multiply the tacotron_batch_size by N below in the hparams! (tacotron_batch_size = 32 * N)\n\t#Never use lower batch size than 32 on a single GPU!\n\t#Same applies for Wavenet: wavenet_batch_size = 8 * N (wavenet_batch_size can be smaller than 8 if GPU is having OOM, minimum 2)\n\t#Please also apply the synthesis batch size modification likewise. (if N GPUs are used for synthesis, minimal batch size must be N, minimum of 1 sample per GPU)\n\t#We did not add an automatic multi-GPU batch size computation to avoid confusion in the user\'s mind and to provide more control to the user for\n\t#resources related decisions.\n\n\t#Acknowledgement:\n\t#\tMany thanks to @MlWoo for his awesome work on multi-GPU Tacotron which showed to work a little faster than the original\n\t#\tpipeline for a single GPU as well. Great work!\n\n\t#Hardware setup: Default supposes user has only one GPU: ""/gpu:0"" (Tacotron only for now! WaveNet does not support multi GPU yet, WIP)\n\t#Synthesis also uses the following hardware parameters for multi-GPU parallel synthesis.\n\ttacotron_num_gpus = 1, #Determines the number of gpus in use for Tacotron training.\n\twavenet_num_gpus = 1, #Determines the number of gpus in use for WaveNet training. (WIP)\n\tsplit_on_cpu = True, #Determines whether to split data on CPU or on first GPU. This is automatically True when more than 1 GPU is used. \n\t\t#(Recommend: False on slow CPUs/Disks, True otherwise for small speed boost)\n\t###########################################################################################################################################\n\n\t#Audio\n\t#Audio parameters are the most important parameters to tune when using this work on your personal data. Below are the beginner steps to adapt\n\t#this work to your personal data:\n\t#\t1- Determine my data sample rate: First you need to determine your audio sample_rate (how many samples are in a second of audio). This can be done using sox: ""sox --i <filename>""\n\t#\t\t(For this small tuto, I will consider 24kHz (24000 Hz), and defaults are 22050Hz, so there are plenty of examples to refer to)\n\t#\t2- set sample_rate parameter to your data correct sample rate\n\t#\t3- Fix win_size and and hop_size accordingly: (Supposing you will follow our advice: 50ms window_size, and 12.5ms frame_shift(hop_size))\n\t#\t\ta- win_size = 0.05 * sample_rate. In the tuto example, 0.05 * 24000 = 1200\n\t#\t\tb- hop_size = 0.25 * win_size. Also equal to 0.0125 * sample_rate. In the tuto example, 0.25 * 1200 = 0.0125 * 24000 = 300 (Can set frame_shift_ms=12.5 instead)\n\t#\t4- Fix n_fft, num_freq and upsample_scales parameters accordingly.\n\t#\t\ta- n_fft can be either equal to win_size or the first power of 2 that comes after win_size. I usually recommend using the latter\n\t#\t\t\tto be more consistent with signal processing friends. No big difference to be seen however. For the tuto example: n_fft = 2048 = 2**11\n\t#\t\tb- num_freq = (n_fft / 2) + 1. For the tuto example: num_freq = 2048 / 2 + 1 = 1024 + 1 = 1025.\n\t#\t\tc- For WaveNet, upsample_scales products must be equal to hop_size. For the tuto example: upsample_scales=[15, 20] where 15 * 20 = 300\n\t#\t\t\tit is also possible to use upsample_scales=[3, 4, 5, 5] instead. One must only keep in mind that upsample_kernel_size[0] = 2*upsample_scales[0]\n\t#\t\t\tso the training segments should be long enough (2.8~3x upsample_scales[0] * hop_size or longer) so that the first kernel size can see the middle \n\t#\t\t\tof the samples efficiently. The length of WaveNet training segments is under the parameter ""max_time_steps"".\n\t#\t5- Finally comes the silence trimming. This very much data dependent, so I suggest trying preprocessing (or part of it, ctrl-C to stop), then use the\n\t#\t\t.ipynb provided in the repo to listen to some inverted mel/linear spectrograms. That will first give you some idea about your above parameters, and\n\t#\t\tit will also give you an idea about trimming. If silences persist, try reducing trim_top_db slowly. If samples are trimmed mid words, try increasing it.\n\t#\t6- If audio quality is too metallic or fragmented (or if linear spectrogram plots are showing black silent regions on top), then restart from step 2.\n\tnum_mels = 80, #Number of mel-spectrogram channels and local conditioning dimensionality\n\tnum_freq = 1025, # (= n_fft / 2 + 1) only used when adding linear spectrograms post processing network\n\trescale = True, #Whether to rescale audio prior to preprocessing\n\trescaling_max = 0.999, #Rescaling value\n\n\t#train samples of lengths between 3sec and 14sec are more than enough to make a model capable of generating consistent speech.\n\tclip_mels_length = True, #For cases of OOM (Not really recommended, only use if facing unsolvable OOM errors, also consider clipping your samples to smaller chunks)\n\tmax_mel_frames = 1000,  #Only relevant when clip_mels_length = True, please only use after trying output_per_steps=3 and still getting OOM errors.\n\n\t# Use LWS (https://github.com/Jonathan-LeRoux/lws) for STFT and phase reconstruction\n\t# It\'s preferred to set True to use with https://github.com/r9y9/wavenet_vocoder\n\t# Does not work if n_ffit is not multiple of hop_size!!\n\tuse_lws=False, #Only used to set as True if using WaveNet, no difference in performance is observed in either cases.\n\tsilence_threshold=2, #silence threshold used for sound trimming for wavenet preprocessing\n\n\t#Mel spectrogram\n\tn_fft = 2048, #Extra window size is filled with 0 paddings to match this parameter\n\thop_size = 275, #For 22050Hz, 275 ~= 12.5 ms (0.0125 * sample_rate)\n\twin_size = 1100, #For 22050Hz, 1100 ~= 50 ms (If None, win_size = n_fft) (0.05 * sample_rate)\n\tsample_rate = 22050, #22050 Hz (corresponding to ljspeech dataset) (sox --i <filename>)\n\tframe_shift_ms = None, #Can replace hop_size parameter. (Recommended: 12.5)\n\tmagnitude_power = 2., #The power of the spectrogram magnitude (1. for energy, 2. for power)\n\n\t#M-AILABS (and other datasets) trim params (there parameters are usually correct for any data, but definitely must be tuned for specific speakers)\n\ttrim_silence = True, #Whether to clip silence in Audio (at beginning and end of audio only, not the middle)\n\ttrim_fft_size = 2048, #Trimming window size\n\ttrim_hop_size = 512, #Trimmin hop length\n\ttrim_top_db = 45, #Trimming db difference from reference db (smaller==harder trim.)\n\n\t#Mel and Linear spectrograms normalization/scaling and clipping\n\tsignal_normalization = True, #Whether to normalize mel spectrograms to some predefined range (following below parameters)\n\tallow_clipping_in_normalization = True, #Only relevant if mel_normalization = True\n\tsymmetric_mels = True, #Whether to scale the data to be symmetric around 0. (Also multiplies the output range by 2, faster and cleaner convergence)\n\tmax_abs_value = 4., #max absolute value of data. If symmetric, data will be [-max, max] else [0, max] (Must not be too big to avoid gradient explosion, \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #not too small for fast convergence)\n\tnormalize_for_wavenet = True, #whether to rescale to [0, 1] for wavenet. (better audio quality)\n\tclip_for_wavenet = True, #whether to clip [-max, max] before training/synthesizing with wavenet (better audio quality)\n\twavenet_pad_sides = 1, #Can be 1 or 2. 1 for pad right only, 2 for both sides padding.\n\n\t#Contribution by @begeekmyfriend\n\t#Spectrogram Pre-Emphasis (Lfilter: Reduce spectrogram noise and helps model certitude levels. Also allows for better G&L phase reconstruction)\n\tpreemphasize = False, #whether to apply filter\n\tpreemphasis = 0.97, #filter coefficient.\n\n\t#Limits\n\tmin_level_db = -100,\n\tref_level_db = 20,\n\tfmin = 75, #Set this to 55 if your speaker is male! if female, 95 should help taking off noise. (To test depending on dataset. Pitch info: male~[65, 260], female~[100, 525])\n\tfmax = 7600, #To be increased/reduced depending on data.\n\n\t#Griffin Lim\n\tpower = 1.5, #Only used in G&L inversion, usually values between 1.2 and 1.5 are a good choice.\n\tgriffin_lim_iters = 60, #Number of G&L iterations, typically 30 is enough but we use 60 to ensure convergence.\n\t###########################################################################################################################################\n\n\t#Tacotron\n\t#Model general type\n\toutputs_per_step = 1, #number of frames to generate at each decoding step (increase to speed up computation and allows for higher batch size, decreases G&L audio quality)\n\tstop_at_any = True, #Determines whether the decoder should stop when predicting <stop> to any frame or to all of them (True works pretty well)\n\tbatch_norm_position = \'after\', #Can be in (\'before\', \'after\'). Determines whether we use batch norm before or after the activation function (relu). Matter for debate.\n\n\t#Input parameters\n\tembedding_dim = 512, #dimension of embedding space\n\n\t#Encoder parameters\n\tenc_conv_num_layers = 3, #number of encoder convolutional layers\n\tenc_conv_kernel_size = (5, ), #size of encoder convolution filters for each layer\n\tenc_conv_channels = 512, #number of encoder convolutions filters for each layer\n\tencoder_lstm_units = 256, #number of lstm units for each direction (forward and backward)\n\n\t#Attention mechanism\n\tsmoothing = False, #Whether to smooth the attention normalization function\n\tattention_dim = 128, #dimension of attention space\n\tattention_filters = 32, #number of attention convolution filters\n\tattention_kernel = (31, ), #kernel size of attention convolution\n\tcumulative_weights = True, #Whether to cumulate (sum) all previous attention weights or simply feed previous weights (Recommended: True)\n\n\t#Attention synthesis constraints\n\t#""Monotonic"" constraint forces the model to only look at the forwards attention_win_size steps.\n\t#""Window"" allows the model to look at attention_win_size neighbors, both forward and backward steps.\n\tsynthesis_constraint = False,  #Whether to use attention windows constraints in synthesis only\n\tsynthesis_constraint_type = \'window\', #can be in (\'window\', \'monotonic\'). \n\tattention_win_size = 7, #Side of the window. Current step does not count. If mode is window and attention_win_size is not pair, the 1 extra is provided to forward part of the window.\n\n\t#Decoder\n\tprenet_layers = [256, 256], #number of layers and number of units of prenet\n\tdecoder_layers = 2, #number of decoder lstm layers\n\tdecoder_lstm_units = 1024, #number of decoder lstm units on each layer\n\tmax_iters = 10000, #Max decoder steps during inference (Just for safety from infinite loop cases)\n\n\t#Residual postnet\n\tpostnet_num_layers = 5, #number of postnet convolutional layers\n\tpostnet_kernel_size = (5, ), #size of postnet convolution filters for each layer\n\tpostnet_channels = 512, #number of postnet convolution filters for each layer\n\n\t#CBHG mel->linear postnet\n\tcbhg_kernels = 8, #All kernel sizes from 1 to cbhg_kernels will be used in the convolution bank of CBHG to act as ""K-grams""\n\tcbhg_conv_channels = 128, #Channels of the convolution bank\n\tcbhg_pool_size = 2, #pooling size of the CBHG\n\tcbhg_projection = 256, #projection channels of the CBHG (1st projection, 2nd is automatically set to num_mels)\n\tcbhg_projection_kernel_size = 3, #kernel_size of the CBHG projections\n\tcbhg_highwaynet_layers = 4, #Number of HighwayNet layers\n\tcbhg_highway_units = 128, #Number of units used in HighwayNet fully connected layers\n\tcbhg_rnn_units = 128, #Number of GRU units used in bidirectional RNN of CBHG block. CBHG output is 2x rnn_units in shape\n\n\t#Loss params\n\tmask_encoder = True, #whether to mask encoder padding while computing attention. Set to True for better prosody but slower convergence.\n\tmask_decoder = False, #Whether to use loss mask for padded sequences (if False, <stop_token> loss function will not be weighted, else recommended pos_weight = 20)\n\tcross_entropy_pos_weight = 1, #Use class weights to reduce the stop token classes imbalance (by adding more penalty on False Negatives (FN)) (1 = disabled)\n\tpredict_linear = False, #Whether to add a post-processing network to the Tacotron to predict linear spectrograms (True mode Not tested!!)\n\t###########################################################################################################################################\n\n\t#Wavenet\n\t# Input type:\n\t# 1. raw [-1, 1]\n\t# 2. mulaw [-1, 1]\n\t# 3. mulaw-quantize [0, mu]\n\t# If input_type is raw or mulaw, network assumes scalar input and\n\t# discretized mixture of logistic distributions output, otherwise one-hot\n\t# input and softmax output are assumed.\n\t#Model general type\n\tinput_type=""raw"", #Raw has better quality but harder to train. mulaw-quantize is easier to train but has lower quality.\n\tquantize_channels=2**16,  # 65536 (16-bit) (raw) or 256 (8-bit) (mulaw or mulaw-quantize) // number of classes = 256 <=> mu = 255\n\tuse_bias = True, #Whether to use bias in convolutional layers of the Wavenet\n\tlegacy = False, #Whether to use legacy mode: Multiply all skip outputs but the first one with sqrt(0.5) (True for more early training stability, especially for large models)\n\tresidual_legacy = False, #Whether to scale residual blocks outputs by a factor of sqrt(0.5) (True for input variance preservation early in training and better overall stability)\n\n\t#Model Losses parmeters\n\t#Minimal scales ranges for MoL and Gaussian modeling\n\tlog_scale_min=float(np.log(1e-14)), #Mixture of logistic distributions minimal log scale\n\tlog_scale_min_gauss = float(np.log(9.1188196 * 1e-4)), #Gaussian distribution minimal allowed log scale\n\t#Loss type\n\tcdf_loss = True, #Whether to use CDF loss in Gaussian modeling. Advantages: non-negative loss term and more training stability. (Automatically True for MoL)\n\n\t#model parameters\n\t#To use Gaussian distribution as output distribution instead of mixture of logistics, set ""out_channels = 2"" instead of ""out_channels = 10 * 3"". (UNDER TEST)\n\tout_channels = 30, #This should be equal to quantize channels when input type is \'mulaw-quantize\' else: num_distributions * 3 (prob, mean, log_scale).\n\tlayers = 24, #Number of dilated convolutions (Default: Simplified Wavenet of Tacotron-2 paper)\n\tstacks = 4, #Number of dilated convolution stacks (Default: Simplified Wavenet of Tacotron-2 paper)\n\tresidual_channels = 256, #Number of residual block input/output channels.\n\tgate_channels = 512, #split in 2 in gated convolutions\n\tskip_out_channels = 256, #Number of residual block skip convolution channels.\n\tkernel_size = 3, #The number of inputs to consider in dilated convolutions.\n\n\t#Upsampling parameters (local conditioning)\n\tcin_channels = 80, #Set this to -1 to disable local conditioning, else it must be equal to num_mels!!\n\tupsample_conditional_features = True, #Whether to repeat conditional features or upsample them (The latter is recommended)\n\t#Upsample types: (\'1D\', \'2D\', \'Resize\', \'SubPixel\')\n\t#All upsampling initialization/kernel_size are chosen to omit checkerboard artifacts as much as possible. (Resize is designed to omit that by nature).\n\t#To be specific, all initial upsample weights/biases (when NN_init=True) ensure that the upsampling layers act as a ""Nearest neighbor upsample"" of size ""hop_size"" (checkerboard free).\n\t#1D spans all frequency bands for each frame (channel-wise) while 2D spans ""freq_axis_kernel_size"" bands at a time. Both are vanilla transpose convolutions.\n\t#Resize is a 2D convolution that follows a Nearest Neighbor (NN) resize. For reference, this is: ""NN resize->convolution"".\n\t#Finally, SubPixel (2D) is the ICNR version (initialized to be equivalent to ""convolution->NN resize"") of Sub-Pixel convolutions. also called ""checkered artifact free sub-pixel conv"".\n\tupsample_type = \'2D\', #Type of the upsampling deconvolution. Can be (\'1D\' or \'2D\', \'Resize\', \'SubPixel\').\n\tupsample_activation = \'Relu\', #Activation function used during upsampling. Can be (\'LeakyRelu\', \'Relu\' or None)\n\tupsample_scales = [5, 5, 11], #prod(upsample_scales) should be equal to hop_size\n\tfreq_axis_kernel_size = 3, #Only used for 2D upsampling types. This is the number of requency bands that are spanned at a time for each frame.\n\tleaky_alpha = 0.4, #slope of the negative portion of LeakyRelu (LeakyRelu: y=x if x>0 else y=alpha * x)\n\tNN_init = True, #Determines whether we want to initialize upsampling kernels/biases in a way to ensure upsample is initialize to Nearest neighbor upsampling. (Mostly for debug)\n\tNN_scaler = 0.1, #Determines the initial Nearest Neighbor upsample values scale. i.e: upscaled_input_values = input_values * NN_scaler (1. to disable)\n\n\t#global conditioning\n\tgin_channels = -1, #Set this to -1 to disable global conditioning, Only used for multi speaker dataset. It defines the depth of the embeddings (Recommended: 16)\n\tuse_speaker_embedding = True, #whether to make a speaker embedding\n\tn_speakers = 5, #number of speakers (rows of the embedding)\n\tspeakers_path = None, #Defines path to speakers metadata. Can be either in ""speaker\\tglobal_id"" (with header) tsv format, or a single column tsv with speaker names. If None, use ""speakers"".\n\tspeakers = [\'speaker0\', \'speaker1\', #List of speakers used for embeddings visualization. (Consult ""wavenet_vocoder/train.py"" if you want to modify the speaker names source).\n\t\t\t\t\'speaker2\', \'speaker3\', \'speaker4\'], #Must be consistent with speaker ids specified for global conditioning for correct visualization.\n\t###########################################################################################################################################\n\n\t#Tacotron Training\n\t#Reproduction seeds\n\ttacotron_random_seed = 5339, #Determines initial graph and operations (i.e: model) random state for reproducibility\n\ttacotron_data_random_state = 1234, #random state for train test split repeatability\n\n\t#performance parameters\n\ttacotron_swap_with_cpu = False, #Whether to use cpu as support to gpu for decoder computation (Not recommended: may cause major slowdowns! Only use when critical!)\n\n\t#train/test split ratios, mini-batches sizes\n\ttacotron_batch_size = 32, #number of training samples on each training steps\n\t#Tacotron Batch synthesis supports ~16x the training batch size (no gradients during testing). \n\t#Training Tacotron with unmasked paddings makes it aware of them, which makes synthesis times different from training. We thus recommend masking the encoder.\n\ttacotron_synthesis_batch_size = 1, #DO NOT MAKE THIS BIGGER THAN 1 IF YOU DIDN\'T TRAIN TACOTRON WITH ""mask_encoder=True""!!\n\ttacotron_test_size = 0.05, #% of data to keep as test data, if None, tacotron_test_batches must be not None. (5% is enough to have a good idea about overfit)\n\ttacotron_test_batches = None, #number of test batches.\n\n\t#Learning rate schedule\n\ttacotron_decay_learning_rate = True, #boolean, determines if the learning rate will follow an exponential decay\n\ttacotron_start_decay = 40000, #Step at which learning decay starts\n\ttacotron_decay_steps = 24500, #Determines the learning rate decay slope (UNDER TEST)\n\ttacotron_decay_rate = 0.5, #learning rate decay rate (UNDER TEST)\n\ttacotron_initial_learning_rate = 1e-3, #starting learning rate\n\ttacotron_final_learning_rate = 1e-5, #minimal learning rate\n\n\t#Optimization parameters\n\ttacotron_adam_beta1 = 0.9, #AdamOptimizer beta1 parameter\n\ttacotron_adam_beta2 = 0.999, #AdamOptimizer beta2 parameter\n\ttacotron_adam_epsilon = 1e-6, #AdamOptimizer Epsilon parameter\n\n\t#Regularization parameters\n\ttacotron_reg_weight = 1e-7, #regularization weight (for L2 regularization)\n\ttacotron_scale_regularization = False, #Whether to rescale regularization weight to adapt for outputs range (used when reg_weight is high and biasing the model)\n\ttacotron_zoneout_rate = 0.1, #zoneout rate for all LSTM cells in the network\n\ttacotron_dropout_rate = 0.5, #dropout rate for all convolutional layers + prenet\n\ttacotron_clip_gradients = True, #whether to clip gradients\n\n\t#Evaluation parameters\n\ttacotron_natural_eval = False, #Whether to use 100% natural eval (to evaluate Curriculum Learning performance) or with same teacher-forcing ratio as in training (just for overfit)\n\n\t#Decoder RNN learning can take be done in one of two ways:\n\t#\tTeacher Forcing: vanilla teacher forcing (usually with ratio = 1). mode=\'constant\'\n\t#\tScheduled Sampling Scheme: From Teacher-Forcing to sampling from previous outputs is function of global step. (teacher forcing ratio decay) mode=\'scheduled\'\n\t#The second approach is inspired by:\n\t#Bengio et al. 2015: Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.\n\t#Can be found under: https://arxiv.org/pdf/1506.03099.pdf\n\ttacotron_teacher_forcing_mode = \'constant\', #Can be (\'constant\' or \'scheduled\'). \'scheduled\' mode applies a cosine teacher forcing ratio decay. (Preference: scheduled)\n\ttacotron_teacher_forcing_ratio = 1., #Value from [0., 1.], 0.=0%, 1.=100%, determines the % of times we force next decoder inputs, Only relevant if mode=\'constant\'\n\ttacotron_teacher_forcing_init_ratio = 1., #initial teacher forcing ratio. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_final_ratio = 0., #final teacher forcing ratio. (Set None to use alpha instead) Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_start_decay = 10000, #starting point of teacher forcing ratio decay. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_decay_steps = 40000, #Determines the teacher forcing ratio decay slope. Relevant if mode=\'scheduled\'\n\ttacotron_teacher_forcing_decay_alpha = None, #teacher forcing ratio decay rate. Defines the final tfr as a ratio of initial tfr. Relevant if mode=\'scheduled\'\n\n\t#Speaker adaptation parameters\n\ttacotron_fine_tuning = False, #Set to True to freeze encoder and only keep training pretrained decoder. Used for speaker adaptation with small data.\n\t###########################################################################################################################################\n\n\t#Wavenet Training\n\twavenet_random_seed = 5339, # S=5, E=3, D=9 :)\n\twavenet_data_random_state = 1234, #random state for train test split repeatability\n\n\t#performance parameters\n\twavenet_swap_with_cpu = False, #Whether to use cpu as support to gpu for synthesis computation (while loop).(Not recommended: may cause major slowdowns! Only use when critical!)\n\n\t#train/test split ratios, mini-batches sizes\n\twavenet_batch_size = 8, #batch size used to train wavenet.\n\t#During synthesis, there is no max_time_steps limitation so the model can sample much longer audio than 8k(or 13k) steps. (Audio can go up to 500k steps, equivalent to ~21sec on 24kHz)\n\t#Usually your GPU can handle ~2x wavenet_batch_size during synthesis for the same memory amount during training (because no gradients to keep and ops to register for backprop)\n\twavenet_synthesis_batch_size = 10 * 2, #This ensure that wavenet synthesis goes up to 4x~8x faster when synthesizing multiple sentences. Watch out for OOM with long audios.\n\twavenet_test_size = None, #% of data to keep as test data, if None, wavenet_test_batches must be not None\n\twavenet_test_batches = 1, #number of test batches.\n\n\t#Learning rate schedule\n\twavenet_lr_schedule = \'exponential\', #learning rate schedule. Can be (\'exponential\', \'noam\')\n\twavenet_learning_rate = 1e-4, #wavenet initial learning rate\n\twavenet_warmup = float(4000), #Only used with \'noam\' scheme. Defines the number of ascending learning rate steps.\n\twavenet_decay_rate = 0.5, #Only used with \'exponential\' scheme. Defines the decay rate.\n\twavenet_decay_steps = 200000, #Only used with \'exponential\' scheme. Defines the decay steps.\n\n\t#Optimization parameters\n\twavenet_adam_beta1 = 0.9, #Adam beta1\n\twavenet_adam_beta2 = 0.999, #Adam beta2\n\twavenet_adam_epsilon = 1e-6, #Adam Epsilon\n\n\t#Regularization parameters\n\twavenet_clip_gradients = True, #Whether the clip the gradients during wavenet training.\n\twavenet_ema_decay = 0.9999, #decay rate of exponential moving average\n\twavenet_weight_normalization = False, #Whether to Apply Saliman & Kingma Weight Normalization (reparametrization) technique. (Used in DeepVoice3, not critical here)\n\twavenet_init_scale = 1., #Only relevent if weight_normalization=True. Defines the initial scale in data dependent initialization of parameters.\n\twavenet_dropout = 0.05, #drop rate of wavenet layers\n\twavenet_gradient_max_norm = 100.0, #Norm used to clip wavenet gradients\n\twavenet_gradient_max_value = 5.0, #Value used to clip wavenet gradients\n\n\t#training samples length\n\tmax_time_sec = None, #Max time of audio for training. If None, we use max_time_steps.\n\tmax_time_steps = 11000, #Max time steps in audio used to train wavenet (decrease to save memory) (Recommend: 8000 on modest GPUs, 13000 on stronger ones)\n\n\t#Evaluation parameters\n\twavenet_natural_eval = False, #Whether to use 100% natural eval (to evaluate autoregressivity performance) or with teacher forcing to evaluate overfit and model consistency.\n\n\t#Tacotron-2 integration parameters\n\ttrain_with_GTA = True, #Whether to use GTA mels to train WaveNet instead of ground truth mels.\n\t###########################################################################################################################################\n\n\t#Eval/Debug parameters\n\t#Eval sentences (if no eval text file was specified during synthesis, these sentences are used for eval)\n\tsentences = [\n\t# From July 8, 2017 New York Times:\n\t\'Scientists at the CERN laboratory say they have discovered a new particle.\',\n\t\'There\\\'s a way to measure the acute emotional intelligence that has never gone out of style.\',\n\t\'President Trump met with other leaders at the Group of 20 conference.\',\n\t\'The Senate\\\'s bill to repeal and replace the Affordable Care Act is now imperiled.\',\n\t# From Google\'s Tacotron example page:\n\t\'Generative adversarial network or variational auto-encoder.\',\n\t\'Basilar membrane and otolaryngology are not auto-correlations.\',\n\t\'He has read the whole thing.\',\n\t\'He reads books.\',\n\t\'He thought it was time to present the present.\',\n\t\'Thisss isrealy awhsome.\',\n\t\'Punctuation sensitivity, is working.\',\n\t\'Punctuation sensitivity is working.\',\n\t""Peter Piper picked a peck of pickled peppers. How many pickled peppers did Peter Piper pick?"",\n\t""She sells sea-shells on the sea-shore. The shells she sells are sea-shells I\'m sure."",\n\t""Tajima Airport serves Toyooka."",\n\t#From The web (random long utterance)\n\t# \'On offering to help the blind man, the man who then stole his car, had not, at that precise moment, had any evil intention, quite the contrary, \\\n\t# what he did was nothing more than obey those feelings of generosity and altruism which, as everyone knows, \\\n\t# are the two best traits of human nature and to be found in much more hardened criminals than this one, a simple car-thief without any hope of advancing in his profession, \\\n\t# exploited by the real owners of this enterprise, for it is they who take advantage of the needs of the poor.\',\n\t# A final Thank you note!\n\t\'Thank you so much for your support!\',\n\t],\n\n\t#Wavenet Debug\n\twavenet_synth_debug = False, #Set True to use target as debug in WaveNet synthesis. \n\twavenet_debug_wavs = [\'training_data/audio/audio-LJ001-0008.npy\'], #Path to debug audios. Must be multiple of wavenet_num_gpus.\n\twavenet_debug_mels = [\'training_data/mels/mel-LJ001-0008.npy\'], #Path to corresponding mels. Must be of same length and order as wavenet_debug_wavs.\n\n\t)\n\ndef hparams_debug_string():\n\tvalues = hparams.values()\n\thp = [\'  %s: %s\' % (name, values[name]) for name in sorted(values) if name != \'sentences\']\n\treturn \'Hyperparameters:\\n\' + \'\\n\'.join(hp)\n'"
preprocess.py,1,"b""import argparse\nimport os\nfrom multiprocessing import cpu_count\n\nfrom datasets import preprocessor\nfrom hparams import hparams\nfrom tqdm import tqdm\n\n\ndef preprocess(args, input_folders, out_dir, hparams):\n\tmel_dir = os.path.join(out_dir, 'mels')\n\twav_dir = os.path.join(out_dir, 'audio')\n\tlinear_dir = os.path.join(out_dir, 'linear')\n\tos.makedirs(mel_dir, exist_ok=True)\n\tos.makedirs(wav_dir, exist_ok=True)\n\tos.makedirs(linear_dir, exist_ok=True)\n\tmetadata = preprocessor.build_from_path(hparams, input_folders, mel_dir, linear_dir, wav_dir, args.n_jobs, tqdm=tqdm)\n\twrite_metadata(metadata, out_dir)\n\ndef write_metadata(metadata, out_dir):\n\twith open(os.path.join(out_dir, 'train.txt'), 'w', encoding='utf-8') as f:\n\t\tfor m in metadata:\n\t\t\tf.write('|'.join([str(x) for x in m]) + '\\n')\n\tmel_frames = sum([int(m[4]) for m in metadata])\n\ttimesteps = sum([int(m[3]) for m in metadata])\n\tsr = hparams.sample_rate\n\thours = timesteps / sr / 3600\n\tprint('Write {} utterances, {} mel frames, {} audio timesteps, ({:.2f} hours)'.format(\n\t\tlen(metadata), mel_frames, timesteps, hours))\n\tprint('Max input length (text chars): {}'.format(max(len(m[5]) for m in metadata)))\n\tprint('Max mel frames length: {}'.format(max(int(m[4]) for m in metadata)))\n\tprint('Max audio timesteps length: {}'.format(max(m[3] for m in metadata)))\n\ndef norm_data(args):\n\n\tmerge_books = (args.merge_books=='True')\n\n\tprint('Selecting data folders..')\n\tsupported_datasets = ['LJSpeech-1.0', 'LJSpeech-1.1', 'M-AILABS']\n\tif args.dataset not in supported_datasets:\n\t\traise ValueError('dataset value entered {} does not belong to supported datasets: {}'.format(\n\t\t\targs.dataset, supported_datasets))\n\n\tif args.dataset.startswith('LJSpeech'):\n\t\treturn [os.path.join(args.base_dir, args.dataset)]\n\n\n\tif args.dataset == 'M-AILABS':\n\t\tsupported_languages = ['en_US', 'en_UK', 'fr_FR', 'it_IT', 'de_DE', 'es_ES', 'ru_RU',\n\t\t\t'uk_UK', 'pl_PL', 'nl_NL', 'pt_PT', 'fi_FI', 'se_SE', 'tr_TR', 'ar_SA']\n\t\tif args.language not in supported_languages:\n\t\t\traise ValueError('Please enter a supported language to use from M-AILABS dataset! \\n{}'.format(\n\t\t\t\tsupported_languages))\n\n\t\tsupported_voices = ['female', 'male', 'mix']\n\t\tif args.voice not in supported_voices:\n\t\t\traise ValueError('Please enter a supported voice option to use from M-AILABS dataset! \\n{}'.format(\n\t\t\t\tsupported_voices))\n\n\t\tpath = os.path.join(args.base_dir, args.language, 'by_book', args.voice)\n\t\tsupported_readers = [e for e in os.listdir(path) if os.path.isdir(os.path.join(path,e))]\n\t\tif args.reader not in supported_readers:\n\t\t\traise ValueError('Please enter a valid reader for your language and voice settings! \\n{}'.format(\n\t\t\t\tsupported_readers))\n\n\t\tpath = os.path.join(path, args.reader)\n\t\tsupported_books = [e for e in os.listdir(path) if os.path.isdir(os.path.join(path,e))]\n\t\tif merge_books:\n\t\t\treturn [os.path.join(path, book) for book in supported_books]\n\n\t\telse:\n\t\t\tif args.book not in supported_books:\n\t\t\t\traise ValueError('Please enter a valid book for your reader settings! \\n{}'.format(\n\t\t\t\t\tsupported_books))\n\n\t\t\treturn [os.path.join(path, args.book)]\n\n\ndef run_preprocess(args, hparams):\n\tinput_folders = norm_data(args)\n\toutput_folder = os.path.join(args.base_dir, args.output)\n\n\tpreprocess(args, input_folders, output_folder, hparams)\n\n\ndef main():\n\tprint('initializing preprocessing..')\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', default='')\n\tparser.add_argument('--hparams', default='',\n\t\thelp='Hyperparameter overrides as a comma-separated list of name=value pairs')\n\tparser.add_argument('--dataset', default='LJSpeech-1.1')\n\tparser.add_argument('--language', default='en_US')\n\tparser.add_argument('--voice', default='female')\n\tparser.add_argument('--reader', default='mary_ann')\n\tparser.add_argument('--merge_books', default='False')\n\tparser.add_argument('--book', default='northandsouth')\n\tparser.add_argument('--output', default='training_data')\n\tparser.add_argument('--n_jobs', type=int, default=cpu_count())\n\targs = parser.parse_args()\n\n\tmodified_hp = hparams.parse(args.hparams)\n\n\tassert args.merge_books in ('False', 'True')\n\n\trun_preprocess(args, modified_hp)\n\n\nif __name__ == '__main__':\n\tmain()\n"""
synthesize.py,1,"b'import argparse\nimport os\nfrom warnings import warn\nfrom time import sleep\n\nimport tensorflow as tf\n\nfrom hparams import hparams\nfrom infolog import log\nfrom tacotron.synthesize import tacotron_synthesize\nfrom wavenet_vocoder.synthesize import wavenet_synthesize\n\n\ndef prepare_run(args):\n\tmodified_hp = hparams.parse(args.hparams)\n\tos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\trun_name = args.name or args.tacotron_name or args.model\n\ttaco_checkpoint = os.path.join(\'logs-\' + run_name, \'taco_\' + args.checkpoint)\n\n\trun_name = args.name or args.wavenet_name or args.model\n\twave_checkpoint = os.path.join(\'logs-\' + run_name, \'wave_\' + args.checkpoint)\n\treturn taco_checkpoint, wave_checkpoint, modified_hp\n\ndef get_sentences(args):\n\tif args.text_list != \'\':\n\t\twith open(args.text_list, \'rb\') as f:\n\t\t\tsentences = list(map(lambda l: l.decode(""utf-8"")[:-1], f.readlines()))\n\telse:\n\t\tsentences = hparams.sentences\n\treturn sentences\n\ndef synthesize(args, hparams, taco_checkpoint, wave_checkpoint, sentences):\n\tlog(\'Running End-to-End TTS Evaluation. Model: {}\'.format(args.name or args.model))\n\tlog(\'Synthesizing mel-spectrograms from text..\')\n\twavenet_in_dir = tacotron_synthesize(args, hparams, taco_checkpoint, sentences)\n\t#Delete Tacotron model from graph\n\ttf.reset_default_graph()\n\t#Sleep 1/2 second to let previous graph close and avoid error messages while Wavenet is synthesizing\n\tsleep(0.5)\n\tlog(\'Synthesizing audio from mel-spectrograms.. (This may take a while)\')\n\twavenet_synthesize(args, hparams, wave_checkpoint)\n\tlog(\'Tacotron-2 TTS synthesis complete!\')\n\n\n\ndef main():\n\taccepted_modes = [\'eval\', \'synthesis\', \'live\']\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--checkpoint\', default=\'pretrained/\', help=\'Path to model checkpoint\')\n\tparser.add_argument(\'--hparams\', default=\'\',\n\t\thelp=\'Hyperparameter overrides as a comma-separated list of name=value pairs\')\n\tparser.add_argument(\'--name\', help=\'Name of logging directory if the two models were trained together.\')\n\tparser.add_argument(\'--tacotron_name\', help=\'Name of logging directory of Tacotron. If trained separately\')\n\tparser.add_argument(\'--wavenet_name\', help=\'Name of logging directory of WaveNet. If trained separately\')\n\tparser.add_argument(\'--model\', default=\'Tacotron-2\')\n\tparser.add_argument(\'--input_dir\', default=\'training_data/\', help=\'folder to contain inputs sentences/targets\')\n\tparser.add_argument(\'--mels_dir\', default=\'tacotron_output/eval/\', help=\'folder to contain mels to synthesize audio from using the Wavenet\')\n\tparser.add_argument(\'--output_dir\', default=\'output/\', help=\'folder to contain synthesized mel spectrograms\')\n\tparser.add_argument(\'--mode\', default=\'eval\', help=\'mode of run: can be one of {}\'.format(accepted_modes))\n\tparser.add_argument(\'--GTA\', default=\'True\', help=\'Ground truth aligned synthesis, defaults to True, only considered in synthesis mode\')\n\tparser.add_argument(\'--text_list\', default=\'\', help=\'Text file contains list of texts to be synthesized. Valid if mode=eval\')\n\tparser.add_argument(\'--speaker_id\', default=None, help=\'Defines the speakers ids to use when running standalone Wavenet on a folder of mels. this variable must be a comma-separated list of ids\')\n\targs = parser.parse_args()\n\n\taccepted_models = [\'Tacotron\', \'WaveNet\', \'Tacotron-2\']\n\n\tif args.model not in accepted_models:\n\t\traise ValueError(\'please enter a valid model to synthesize with: {}\'.format(accepted_models))\n\n\tif args.mode not in accepted_modes:\n\t\traise ValueError(\'accepted modes are: {}, found {}\'.format(accepted_modes, args.mode))\n\n\tif args.mode == \'live\' and args.model == \'Wavenet\':\n\t\traise RuntimeError(\'Wavenet vocoder cannot be tested live due to its slow generation. Live only works with Tacotron!\')\n\n\tif args.GTA not in (\'True\', \'False\'):\n\t\traise ValueError(\'GTA option must be either True or False\')\n\n\tif args.model == \'Tacotron-2\':\n\t\tif args.mode == \'live\':\n\t\t\twarn(\'Requested a live evaluation with Tacotron-2, Wavenet will not be used!\')\n\t\tif args.mode == \'synthesis\':\n\t\t\traise ValueError(\'I don\\\'t recommend running WaveNet on entire dataset.. The world might end before the synthesis :) (only eval allowed)\')\n\n\ttaco_checkpoint, wave_checkpoint, hparams = prepare_run(args)\n\tsentences = get_sentences(args)\n\n\tif args.model == \'Tacotron\':\n\t\t_ = tacotron_synthesize(args, hparams, taco_checkpoint, sentences)\n\telif args.model == \'WaveNet\':\n\t\twavenet_synthesize(args, hparams, wave_checkpoint)\n\telif args.model == \'Tacotron-2\':\n\t\tsynthesize(args, hparams, taco_checkpoint, wave_checkpoint, sentences)\n\telse:\n\t\traise ValueError(\'Model provided {} unknown! {}\'.format(args.model, accepted_models))\n\n\nif __name__ == \'__main__\':\n\tmain()\n'"
test_wavenet_feeder.py,0,"b""import numpy as np \nimport os\nimport argparse \nfrom hparams import hparams\nfrom datasets import audio\nfrom tqdm import tqdm\n\n\n\ndef _limit_time(hparams):\n\t'''Limit time resolution to save GPU memory.\n\t'''\n\tif hparams.max_time_sec is not None:\n\t\treturn int(hparams.max_time_sec * hparams.sample_rate)\n\telif hparams.max_time_steps is not None:\n\t\treturn hparams.max_time_steps\n\telse:\n\t\treturn None\n\n\ndef get_groups(args, hparams, meta, local_condition):\n\tif hparams.train_with_GTA:\n\t\tmel_file = meta[2]\n\telse:\n\t\tmel_file = meta[1]\n\taudio_file = meta[0]\n\n\tinput_data = np.load(os.path.join(args.base_dir, audio_file))\n\n\tif local_condition:\n\t\tlocal_condition_features = np.load(os.path.join(args.base_dir, mel_file))\n\telse:\n\t\tlocal_condition_features = None\n\n\treturn (input_data, local_condition_features, None, len(input_data))\n\ndef _adjust_time_resolution(hparams, batch, local_condition, max_time_steps):\n\t\t'''Adjust time resolution between audio and local condition\n\t\t'''\n\t\tif local_condition:\n\t\t\tnew_batch = []\n\t\t\tfor b in batch:\n\t\t\t\tx, c, g, l = b\n\t\t\t\t_assert_ready_for_upsample(hparams, x, c)\n\t\t\t\tif max_time_steps is not None:\n\t\t\t\t\tmax_steps = _ensure_divisible(max_time_steps, audio.get_hop_size(hparams), True)\n\t\t\t\t\tif len(x) > max_time_steps:\n\t\t\t\t\t\tmax_time_frames = max_steps // audio.get_hop_size(hparams)\n\t\t\t\t\t\tstart = np.random.randint(0, len(c) - max_time_frames)\n\t\t\t\t\t\ttime_start = start * audio.get_hop_size(hparams)\n\t\t\t\t\t\tx = x[time_start: time_start + max_time_frames * audio.get_hop_size(hparams)]\n\t\t\t\t\t\tc = c[start: start + max_time_frames, :]\n\t\t\t\t\t\t_assert_ready_for_upsample(hparams, x, c)\n\n\t\t\t\tnew_batch.append((x, c, g, l))\n\t\t\treturn new_batch\n\t\telse:\n\t\t\tnew_batch = []\n\t\t\tfor b in batch:\n\t\t\t\tx, c, g, l = b\n\t\t\t\tx = audio.trim_silence(x, hparams)\n\t\t\t\tif max_time_steps is not None and len(x) > max_time_steps:\n\t\t\t\t\tstart = np.random.randint(0, len(c) - max_time_steps)\n\t\t\t\t\tx = x[start: start + max_time_steps]\n\t\t\t\tnew_batch.append((x, c, g, l))\n\t\t\treturn new_batch\n\ndef _assert_ready_for_upsample(hparams, x, c):\n\tassert len(x) % len(c) == 0 and len(x) // len(c) == audio.get_hop_size(hparams)\n\ndef check_time_alignment(hparams, batch, local_condition):\n\t#No need to check beyond this step when preparing data\n\t#Limit time steps to save GPU Memory usage\n\tmax_time_steps = _limit_time(hparams)\n\t#Adjust time resolution for upsampling\n\tbatch = _adjust_time_resolution(hparams, batch, local_condition, max_time_steps)\n\ndef _ensure_divisible(length, divisible_by=256, lower=True):\n\tif length % divisible_by == 0:\n\t\treturn length\n\tif lower:\n\t\treturn length - length % divisible_by\n\telse:\n\t\treturn length + (divisible_by - length % divisible_by)\n\ndef run(args, hparams):\n\twith open(args.metadata, 'r') as file:\n\t\tmetadata = [line.strip().split('|') for line in file]\n\n\tlocal_condition = hparams.cin_channels > 0\n\n\texamples = [get_groups(args, hparams, meta, local_condition) for meta in metadata]\n\tbatches = [examples[i: i+hparams.wavenet_batch_size] for i in range(0, len(examples), hparams.wavenet_batch_size)]\n\n\tfor batch in tqdm(batches):\n\t\tcheck_time_alignment(hparams, batch, local_condition)\n\n\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', default='')\n\tparser.add_argument('--hparams', default='',\n\t\thelp='Hyperparameter overrides as a comma-separated list of name=value pairs')\n\tparser.add_argument('--metadata', default='tacotron_output/gta/map.txt')\n\targs = parser.parse_args()\n\n\tmodified_hparams = hparams.parse(args.hparams)\n\trun(args, modified_hparams)\n\n\nif __name__ == '__main__':\n\tmain()"""
train.py,3,"b""import argparse\nimport os\nfrom time import sleep\n\nimport infolog\nimport tensorflow as tf\nfrom hparams import hparams\nfrom infolog import log\nfrom tacotron.synthesize import tacotron_synthesize\nfrom tacotron.train import tacotron_train\nfrom wavenet_vocoder.train import wavenet_train\n\nlog = infolog.log\n\n\ndef save_seq(file, sequence, input_path):\n\t'''Save Tacotron-2 training state to disk. (To skip for future runs)\n\t'''\n\tsequence = [str(int(s)) for s in sequence] + [input_path]\n\twith open(file, 'w') as f:\n\t\tf.write('|'.join(sequence))\n\ndef read_seq(file):\n\t'''Load Tacotron-2 training state from disk. (To skip if not first run)\n\t'''\n\tif os.path.isfile(file):\n\t\twith open(file, 'r') as f:\n\t\t\tsequence = f.read().split('|')\n\n\t\treturn [bool(int(s)) for s in sequence[:-1]], sequence[-1]\n\telse:\n\t\treturn [0, 0, 0], ''\n\ndef prepare_run(args):\n\tmodified_hp = hparams.parse(args.hparams)\n\tos.environ['TF_CPP_MIN_LOG_LEVEL'] = str(args.tf_log_level)\n\trun_name = args.name or args.model\n\tlog_dir = os.path.join(args.base_dir, 'logs-{}'.format(run_name))\n\tos.makedirs(log_dir, exist_ok=True)\n\tinfolog.init(os.path.join(log_dir, 'Terminal_train_log'), run_name, args.slack_url)\n\treturn log_dir, modified_hp\n\ndef train(args, log_dir, hparams):\n\tstate_file = os.path.join(log_dir, 'state_log')\n\t#Get training states\n\t(taco_state, GTA_state, wave_state), input_path = read_seq(state_file)\n\n\tif not taco_state:\n\t\tlog('\\n#############################################################\\n')\n\t\tlog('Tacotron Train\\n')\n\t\tlog('###########################################################\\n')\n\t\tcheckpoint = tacotron_train(args, log_dir, hparams)\n\t\ttf.reset_default_graph()\n\t\t#Sleep 1/2 second to let previous graph close and avoid error messages while synthesis\n\t\tsleep(0.5)\n\t\tif checkpoint is None:\n\t\t\traise('Error occured while training Tacotron, Exiting!')\n\t\ttaco_state = 1\n\t\tsave_seq(state_file, [taco_state, GTA_state, wave_state], input_path)\n\telse:\n\t\tcheckpoint = os.path.join(log_dir, 'taco_pretrained/')\n\n\tif not GTA_state:\n\t\tlog('\\n#############################################################\\n')\n\t\tlog('Tacotron GTA Synthesis\\n')\n\t\tlog('###########################################################\\n')\n\t\tinput_path = tacotron_synthesize(args, hparams, checkpoint)\n\t\ttf.reset_default_graph()\n\t\t#Sleep 1/2 second to let previous graph close and avoid error messages while Wavenet is training\n\t\tsleep(0.5)\n\t\tGTA_state = 1\n\t\tsave_seq(state_file, [taco_state, GTA_state, wave_state], input_path)\n\telse:\n\t\tinput_path = os.path.join('tacotron_' + args.output_dir, 'gta', 'map.txt')\n\n\tif input_path == '' or input_path is None:\n\t\traise RuntimeError('input_path has an unpleasant value -> {}'.format(input_path))\n\n\tif not wave_state:\n\t\tlog('\\n#############################################################\\n')\n\t\tlog('Wavenet Train\\n')\n\t\tlog('###########################################################\\n')\n\t\tcheckpoint = wavenet_train(args, log_dir, hparams, input_path)\n\t\tif checkpoint is None:\n\t\t\traise ('Error occured while training Wavenet, Exiting!')\n\t\twave_state = 1\n\t\tsave_seq(state_file, [taco_state, GTA_state, wave_state], input_path)\n\n\tif wave_state and GTA_state and taco_state:\n\t\tlog('TRAINING IS ALREADY COMPLETE!!')\n\ndef main():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', default='')\n\tparser.add_argument('--hparams', default='',\n\t\thelp='Hyperparameter overrides as a comma-separated list of name=value pairs')\n\tparser.add_argument('--tacotron_input', default='training_data/train.txt')\n\tparser.add_argument('--wavenet_input', default='tacotron_output/gta/map.txt')\n\tparser.add_argument('--name', help='Name of logging directory.')\n\tparser.add_argument('--model', default='Tacotron-2')\n\tparser.add_argument('--input_dir', default='training_data', help='folder to contain inputs sentences/targets')\n\tparser.add_argument('--output_dir', default='output', help='folder to contain synthesized mel spectrograms')\n\tparser.add_argument('--mode', default='synthesis', help='mode for synthesis of tacotron after training')\n\tparser.add_argument('--GTA', default='True', help='Ground truth aligned synthesis, defaults to True, only considered in Tacotron synthesis mode')\n\tparser.add_argument('--restore', type=bool, default=True, help='Set this to False to do a fresh training')\n\tparser.add_argument('--summary_interval', type=int, default=250,\n\t\thelp='Steps between running summary ops')\n\tparser.add_argument('--embedding_interval', type=int, default=5000,\n\t\thelp='Steps between updating embeddings projection visualization')\n\tparser.add_argument('--checkpoint_interval', type=int, default=2500,\n\t\thelp='Steps between writing checkpoints')\n\tparser.add_argument('--eval_interval', type=int, default=5000,\n\t\thelp='Steps between eval on test data')\n\tparser.add_argument('--tacotron_train_steps', type=int, default=100000, help='total number of tacotron training steps')\n\tparser.add_argument('--wavenet_train_steps', type=int, default=500000, help='total number of wavenet training steps')\n\tparser.add_argument('--tf_log_level', type=int, default=1, help='Tensorflow C++ log level.')\n\tparser.add_argument('--slack_url', default=None, help='slack webhook notification destination link')\n\targs = parser.parse_args()\n\n\taccepted_models = ['Tacotron', 'WaveNet', 'Tacotron-2']\n\n\tif args.model not in accepted_models:\n\t\traise ValueError('please enter a valid model to train: {}'.format(accepted_models))\n\n\tlog_dir, hparams = prepare_run(args)\n\n\tif args.model == 'Tacotron':\n\t\ttacotron_train(args, log_dir, hparams)\n\telif args.model == 'WaveNet':\n\t\twavenet_train(args, log_dir, hparams, args.wavenet_input)\n\telif args.model == 'Tacotron-2':\n\t\ttrain(args, log_dir, hparams)\n\telse:\n\t\traise ValueError('Model provided {} unknown! {}'.format(args.model, accepted_models))\n\n\nif __name__ == '__main__':\n\tmain()\n"""
wavenet_preprocess.py,1,"b""import argparse\nimport os\nfrom multiprocessing import cpu_count\n\nfrom datasets import wavenet_preprocessor\nfrom hparams import hparams\nfrom tqdm import tqdm\n\n\ndef preprocess(args, input_dir, out_dir, hparams):\n\tmel_dir = os.path.join(out_dir, 'mels')\n\twav_dir = os.path.join(out_dir, 'audio')\n\tos.makedirs(mel_dir, exist_ok=True)\n\tos.makedirs(wav_dir, exist_ok=True)\n\tmetadata = wavenet_preprocessor.build_from_path(hparams, input_dir, mel_dir, wav_dir, args.n_jobs, tqdm=tqdm)\n\twrite_metadata(metadata, out_dir)\n\ndef write_metadata(metadata, out_dir):\n\twith open(os.path.join(out_dir, 'map.txt'), 'w', encoding='utf-8') as f:\n\t\tfor m in metadata:\n\t\t\tf.write('|'.join([str(x) for x in m]) + '\\n')\n\tmel_frames = sum([int(m[5]) for m in metadata])\n\ttimesteps = sum([int(m[4]) for m in metadata])\n\tsr = hparams.sample_rate\n\thours = timesteps / sr / 3600\n\tprint('Write {} utterances, {} audio timesteps, ({:.2f} hours)'.format(\n\t\tlen(metadata), timesteps, hours))\n\tprint('Max mel frames length: {}'.format(max(int(m[5]) for m in metadata)))\n\tprint('Max audio timesteps length: {}'.format(max(m[4] for m in metadata)))\n\ndef run_preprocess(args, hparams):\n\toutput_folder = os.path.join(args.base_dir, args.output)\n\n\tpreprocess(args, args.input_dir, output_folder, hparams)\n\ndef main():\n\tprint('initializing preprocessing..')\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--base_dir', default='')\n\tparser.add_argument('--hparams', default='',\n\t\thelp='Hyperparameter overrides as a comma-separated list of name=value pairs')\n\tparser.add_argument('--input_dir', default='LJSpeech-1.1/wavs')\n\tparser.add_argument('--output', default='tacotron_output/gta/')\n\tparser.add_argument('--n_jobs', type=int, default=cpu_count())\n\targs = parser.parse_args()\n\n\tmodified_hp = hparams.parse(args.hparams)\n\n\trun_preprocess(args, modified_hp)\n\nif __name__ == '__main__':\n\tmain()\n"""
datasets/__init__.py,0,b'#'
datasets/audio.py,16,"b'import librosa\nimport librosa.filters\nimport numpy as np\nimport tensorflow as tf\nfrom scipy import signal\nfrom scipy.io import wavfile\n\n\ndef load_wav(path, sr):\n\treturn librosa.core.load(path, sr=sr)[0]\n\ndef save_wav(wav, path, sr):\n\twav *= 32767 / max(0.01, np.max(np.abs(wav)))\n\t#proposed by @dsmiller\n\twavfile.write(path, sr, wav.astype(np.int16))\n\ndef save_wavenet_wav(wav, path, sr, inv_preemphasize, k):\n\t# wav = inv_preemphasis(wav, k, inv_preemphasize)\n\twav *= 32767 / max(0.01, np.max(np.abs(wav)))\n\twavfile.write(path, sr, wav.astype(np.int16))\n\ndef preemphasis(wav, k, preemphasize=True):\n\tif preemphasize:\n\t\treturn signal.lfilter([1, -k], [1], wav)\n\treturn wav\n\ndef inv_preemphasis(wav, k, inv_preemphasize=True):\n\tif inv_preemphasize:\n\t\treturn signal.lfilter([1], [1, -k], wav)\n\treturn wav\n\n#From https://github.com/r9y9/wavenet_vocoder/blob/master/audio.py\ndef start_and_end_indices(quantized, silence_threshold=2):\n\tfor start in range(quantized.size):\n\t\tif abs(quantized[start] - 127) > silence_threshold:\n\t\t\tbreak\n\tfor end in range(quantized.size - 1, 1, -1):\n\t\tif abs(quantized[end] - 127) > silence_threshold:\n\t\t\tbreak\n\n\tassert abs(quantized[start] - 127) > silence_threshold\n\tassert abs(quantized[end] - 127) > silence_threshold\n\n\treturn start, end\n\ndef trim_silence(wav, hparams):\n\t\'\'\'Trim leading and trailing silence\n\n\tUseful for M-AILABS dataset if we choose to trim the extra 0.5 silence at beginning and end.\n\t\'\'\'\n\t#Thanks @begeekmyfriend and @lautjy for pointing out the params contradiction. These params are separate and tunable per dataset.\n\treturn librosa.effects.trim(wav, top_db= hparams.trim_top_db, frame_length=hparams.trim_fft_size, hop_length=hparams.trim_hop_size)[0]\n\ndef get_hop_size(hparams):\n\thop_size = hparams.hop_size\n\tif hop_size is None:\n\t\tassert hparams.frame_shift_ms is not None\n\t\thop_size = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n\treturn hop_size\n\ndef linearspectrogram(wav, hparams):\n\t# D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n\tD = _stft(wav, hparams)\n\tS = _amp_to_db(np.abs(D)**hparams.magnitude_power, hparams) - hparams.ref_level_db\n\n\tif hparams.signal_normalization:\n\t\treturn _normalize(S, hparams)\n\treturn S\n\ndef melspectrogram(wav, hparams):\n\t# D = _stft(preemphasis(wav, hparams.preemphasis, hparams.preemphasize), hparams)\n\tD = _stft(wav, hparams)\n\tS = _amp_to_db(_linear_to_mel(np.abs(D)**hparams.magnitude_power, hparams), hparams) - hparams.ref_level_db\n\n\tif hparams.signal_normalization:\n\t\treturn _normalize(S, hparams)\n\treturn S\n\ndef inv_linear_spectrogram(linear_spectrogram, hparams):\n\t\'\'\'Converts linear spectrogram to waveform using librosa\'\'\'\n\tif hparams.signal_normalization:\n\t\tD = _denormalize(linear_spectrogram, hparams)\n\telse:\n\t\tD = linear_spectrogram\n\n\tS = _db_to_amp(D + hparams.ref_level_db)**(1/hparams.magnitude_power) #Convert back to linear\n\n\tif hparams.use_lws:\n\t\tprocessor = _lws_processor(hparams)\n\t\tD = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n\t\ty = processor.istft(D).astype(np.float32)\n\t\treturn inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n\telse:\n\t\treturn inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\n\ndef inv_mel_spectrogram(mel_spectrogram, hparams):\n\t\'\'\'Converts mel spectrogram to waveform using librosa\'\'\'\n\tif hparams.signal_normalization:\n\t\tD = _denormalize(mel_spectrogram, hparams)\n\telse:\n\t\tD = mel_spectrogram\n\n\tS = _mel_to_linear(_db_to_amp(D + hparams.ref_level_db)**(1/hparams.magnitude_power), hparams)  # Convert back to linear\n\n\tif hparams.use_lws:\n\t\tprocessor = _lws_processor(hparams)\n\t\tD = processor.run_lws(S.astype(np.float64).T ** hparams.power)\n\t\ty = processor.istft(D).astype(np.float32)\n\t\treturn inv_preemphasis(y, hparams.preemphasis, hparams.preemphasize)\n\telse:\n\t\treturn inv_preemphasis(_griffin_lim(S ** hparams.power, hparams), hparams.preemphasis, hparams.preemphasize)\n\n###########################################################################################\n# tensorflow Griffin-Lim\n# Thanks to @begeekmyfriend: https://github.com/begeekmyfriend/Tacotron-2/blob/mandarin-new/datasets/audio.py\n\ndef inv_linear_spectrogram_tensorflow(spectrogram, hparams):\n\t\'\'\'Builds computational graph to convert spectrogram to waveform using TensorFlow.\n\tUnlike inv_spectrogram, this does NOT invert the preemphasis. The caller should call\n\tinv_preemphasis on the output after running the graph.\n\t\'\'\'\n\tif hparams.signal_normalization:\n\t\tD = _denormalize_tensorflow(spectrogram, hparams)\n\telse:\n\t\tD = linear_spectrogram\n\n\tS = tf.pow(_db_to_amp_tensorflow(D + hparams.ref_level_db), (1/hparams.magnitude_power))\n\treturn _griffin_lim_tensorflow(tf.pow(S, hparams.power), hparams)\n\ndef inv_mel_spectrogram_tensorflow(mel_spectrogram, hparams):\n\t\'\'\'Builds computational graph to convert mel spectrogram to waveform using TensorFlow.\n\tUnlike inv_mel_spectrogram, this does NOT invert the preemphasis. The caller should call\n\tinv_preemphasis on the output after running the graph.\n\t\'\'\'\n\tif hparams.signal_normalization:\n\t\tD = _denormalize_tensorflow(mel_spectrogram, hparams)\n\telse:\n\t\tD = mel_spectrogram\n\n\tS = tf.pow(_db_to_amp_tensorflow(D + hparams.ref_level_db), (1/hparams.magnitude_power))\n\tS = _mel_to_linear_tensorflow(S, hparams)  # Convert back to linear\n\treturn _griffin_lim_tensorflow(tf.pow(S, hparams.power), hparams)\n\n###########################################################################################\n\ndef _lws_processor(hparams):\n\timport lws\n\treturn lws.lws(hparams.n_fft, get_hop_size(hparams), fftsize=hparams.win_size, mode=""speech"")\n\ndef _griffin_lim(S, hparams):\n\t\'\'\'librosa implementation of Griffin-Lim\n\tBased on https://github.com/librosa/librosa/issues/434\n\t\'\'\'\n\tangles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n\tS_complex = np.abs(S).astype(np.complex)\n\ty = _istft(S_complex * angles, hparams)\n\tfor i in range(hparams.griffin_lim_iters):\n\t\tangles = np.exp(1j * np.angle(_stft(y, hparams)))\n\t\ty = _istft(S_complex * angles, hparams)\n\treturn y\n\ndef _griffin_lim_tensorflow(S, hparams):\n\t\'\'\'TensorFlow implementation of Griffin-Lim\n\tBased on https://github.com/Kyubyong/tensorflow-exercises/blob/master/Audio_Processing.ipynb\n\t\'\'\'\n\twith tf.variable_scope(\'griffinlim\'):\n\t\t# TensorFlow\'s stft and istft operate on a batch of spectrograms; create batch of size 1\n\t\tS = tf.expand_dims(S, 0)\n\t\tS_complex = tf.identity(tf.cast(S, dtype=tf.complex64))\n\t\ty = tf.contrib.signal.inverse_stft(S_complex, hparams.win_size, get_hop_size(hparams), hparams.n_fft)\n\t\tfor i in range(hparams.griffin_lim_iters):\n\t\t\test = tf.contrib.signal.stft(y, hparams.win_size, get_hop_size(hparams), hparams.n_fft)\n\t\t\tangles = est / tf.cast(tf.maximum(1e-8, tf.abs(est)), tf.complex64)\n\t\t\ty = tf.contrib.signal.inverse_stft(S_complex * angles, hparams.win_size, get_hop_size(hparams), hparams.n_fft)\n\treturn tf.squeeze(y, 0)\n\ndef _stft(y, hparams):\n\tif hparams.use_lws:\n\t\treturn _lws_processor(hparams).stft(y).T\n\telse:\n\t\treturn librosa.stft(y=y, n_fft=hparams.n_fft, hop_length=get_hop_size(hparams), win_length=hparams.win_size, pad_mode=\'constant\')\n\ndef _istft(y, hparams):\n\treturn librosa.istft(y, hop_length=get_hop_size(hparams), win_length=hparams.win_size)\n\n##########################################################\n#Those are only correct when using lws!!! (This was messing with Wavenet quality for a long time!)\ndef num_frames(length, fsize, fshift):\n\t""""""Compute number of time frames of spectrogram\n\t""""""\n\tpad = (fsize - fshift)\n\tif length % fshift == 0:\n\t\tM = (length + pad * 2 - fsize) // fshift + 1\n\telse:\n\t\tM = (length + pad * 2 - fsize) // fshift + 2\n\treturn M\n\n\ndef pad_lr(x, fsize, fshift):\n\t""""""Compute left and right padding\n\t""""""\n\tM = num_frames(len(x), fsize, fshift)\n\tpad = (fsize - fshift)\n\tT = len(x) + 2 * pad\n\tr = (M - 1) * fshift + fsize - T\n\treturn pad, pad + r\n##########################################################\n#Librosa correct padding\ndef librosa_pad_lr(x, fsize, fshift, pad_sides=1):\n\t\'\'\'compute right padding (final frame) or both sides padding (first and final frames)\n\t\'\'\'\n\tassert pad_sides in (1, 2)\n\t# return int(fsize // 2)\n\tpad = (x.shape[0] // fshift + 1) * fshift - x.shape[0]\n\tif pad_sides == 1:\n\t\treturn 0, pad\n\telse:\n\t\treturn pad // 2, pad // 2 + pad % 2\n\n# Conversions\n_mel_basis = None\n_inv_mel_basis = None\n\ndef _linear_to_mel(spectogram, hparams):\n\tglobal _mel_basis\n\tif _mel_basis is None:\n\t\t_mel_basis = _build_mel_basis(hparams)\n\treturn np.dot(_mel_basis, spectogram)\n\ndef _mel_to_linear(mel_spectrogram, hparams):\n\tglobal _inv_mel_basis\n\tif _inv_mel_basis is None:\n\t\t_inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n\treturn np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n\ndef _mel_to_linear_tensorflow(mel_spectrogram, hparams):\n\tglobal _inv_mel_basis\n\tif _inv_mel_basis is None:\n\t\t_inv_mel_basis = np.linalg.pinv(_build_mel_basis(hparams))\n\treturn tf.transpose(tf.maximum(1e-10, tf.matmul(tf.cast(_inv_mel_basis, tf.float32), tf.transpose(mel_spectrogram, [1, 0]))), [1, 0])\n\ndef _build_mel_basis(hparams):\n\tassert hparams.fmax <= hparams.sample_rate // 2\n\treturn librosa.filters.mel(hparams.sample_rate, hparams.n_fft, n_mels=hparams.num_mels,\n\t\t\t\t\t\t\t   fmin=hparams.fmin, fmax=hparams.fmax)\n\ndef _amp_to_db(x, hparams):\n\tmin_level = np.exp(hparams.min_level_db / 20 * np.log(10))\n\treturn 20 * np.log10(np.maximum(min_level, x))\n\ndef _db_to_amp(x):\n\treturn np.power(10.0, (x) * 0.05)\n\ndef _db_to_amp_tensorflow(x):\n\treturn tf.pow(tf.ones(tf.shape(x)) * 10.0, x * 0.05)\n\ndef _normalize(S, hparams):\n\tif hparams.allow_clipping_in_normalization:\n\t\tif hparams.symmetric_mels:\n\t\t\treturn np.clip((2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value,\n\t\t\t -hparams.max_abs_value, hparams.max_abs_value)\n\t\telse:\n\t\t\treturn np.clip(hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db)), 0, hparams.max_abs_value)\n\n\tassert S.max() <= 0 and S.min() - hparams.min_level_db >= 0\n\tif hparams.symmetric_mels:\n\t\treturn (2 * hparams.max_abs_value) * ((S - hparams.min_level_db) / (-hparams.min_level_db)) - hparams.max_abs_value\n\telse:\n\t\treturn hparams.max_abs_value * ((S - hparams.min_level_db) / (-hparams.min_level_db))\n\ndef _denormalize(D, hparams):\n\tif hparams.allow_clipping_in_normalization:\n\t\tif hparams.symmetric_mels:\n\t\t\treturn (((np.clip(D, -hparams.max_abs_value,\n\t\t\t\thparams.max_abs_value) + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value))\n\t\t\t\t+ hparams.min_level_db)\n\t\telse:\n\t\t\treturn ((np.clip(D, 0, hparams.max_abs_value) * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n\n\tif hparams.symmetric_mels:\n\t\treturn (((D + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value)) + hparams.min_level_db)\n\telse:\n\t\treturn ((D * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n\ndef _denormalize_tensorflow(D, hparams):\n\tif hparams.allow_clipping_in_normalization:\n\t\tif hparams.symmetric_mels:\n\t\t\treturn (((tf.clip_by_value(D, -hparams.max_abs_value,\n\t\t\t\thparams.max_abs_value) + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value))\n\t\t\t\t+ hparams.min_level_db)\n\t\telse:\n\t\t\treturn ((tf.clip_by_value(D, 0, hparams.max_abs_value) * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n\n\tif hparams.symmetric_mels:\n\t\treturn (((D + hparams.max_abs_value) * -hparams.min_level_db / (2 * hparams.max_abs_value)) + hparams.min_level_db)\n\telse:\n\t\treturn ((D * -hparams.min_level_db / hparams.max_abs_value) + hparams.min_level_db)\n'"
datasets/preprocessor.py,0,"b'import os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nfrom datasets import audio\nfrom wavenet_vocoder.util import is_mulaw, is_mulaw_quantize, mulaw, mulaw_quantize\n\n\ndef build_from_path(hparams, input_dirs, mel_dir, linear_dir, wav_dir, n_jobs=12, tqdm=lambda x: x):\n\t""""""\n\tPreprocesses the speech dataset from a gven input path to given output directories\n\n\tArgs:\n\t\t- hparams: hyper parameters\n\t\t- input_dir: input directory that contains the files to prerocess\n\t\t- mel_dir: output directory of the preprocessed speech mel-spectrogram dataset\n\t\t- linear_dir: output directory of the preprocessed speech linear-spectrogram dataset\n\t\t- wav_dir: output directory of the preprocessed speech audio dataset\n\t\t- n_jobs: Optional, number of worker process to parallelize across\n\t\t- tqdm: Optional, provides a nice progress bar\n\n\tReturns:\n\t\t- A list of tuple describing the train examples. this should be written to train.txt\n\t""""""\n\n\t# We use ProcessPoolExecutor to parallelize across processes, this is just for\n\t# optimization purposes and it can be omited\n\texecutor = ProcessPoolExecutor(max_workers=n_jobs)\n\tfutures = []\n\tindex = 1\n\tfor input_dir in input_dirs:\n\t\twith open(os.path.join(input_dir, \'metadata.csv\'), encoding=\'utf-8\') as f:\n\t\t\tfor line in f:\n\t\t\t\tparts = line.strip().split(\'|\')\n\t\t\t\tbasename = parts[0]\n\t\t\t\twav_path = os.path.join(input_dir, \'wavs\', \'{}.wav\'.format(basename))\n\t\t\t\ttext = parts[2]\n\t\t\t\tfutures.append(executor.submit(partial(_process_utterance, mel_dir, linear_dir, wav_dir, basename, wav_path, text, hparams)))\n\t\t\t\tindex += 1\n\n\treturn [future.result() for future in tqdm(futures) if future.result() is not None]\n\n\ndef _process_utterance(mel_dir, linear_dir, wav_dir, index, wav_path, text, hparams):\n\t""""""\n\tPreprocesses a single utterance wav/text pair\n\n\tthis writes the mel scale spectogram to disk and return a tuple to write\n\tto the train.txt file\n\n\tArgs:\n\t\t- mel_dir: the directory to write the mel spectograms into\n\t\t- linear_dir: the directory to write the linear spectrograms into\n\t\t- wav_dir: the directory to write the preprocessed wav into\n\t\t- index: the numeric index to use in the spectogram filename\n\t\t- wav_path: path to the audio file containing the speech input\n\t\t- text: text spoken in the input audio file\n\t\t- hparams: hyper parameters\n\n\tReturns:\n\t\t- A tuple: (audio_filename, mel_filename, linear_filename, time_steps, mel_frames, linear_frames, text)\n\t""""""\n\ttry:\n\t\t# Load the audio as numpy array\n\t\twav = audio.load_wav(wav_path, sr=hparams.sample_rate)\n\texcept FileNotFoundError: #catch missing wav exception\n\t\tprint(\'file {} present in csv metadata is not present in wav folder. skipping!\'.format(\n\t\t\twav_path))\n\t\treturn None\n\n\t#Trim lead/trail silences\n\tif hparams.trim_silence:\n\t\twav = audio.trim_silence(wav, hparams)\n\n\t#Pre-emphasize\n\tpreem_wav = audio.preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\n\t#rescale wav\n\tif hparams.rescale:\n\t\twav = wav / np.abs(wav).max() * hparams.rescaling_max\n\t\tpreem_wav = preem_wav / np.abs(preem_wav).max() * hparams.rescaling_max\n\n\t\t#Assert all audio is in [-1, 1]\n\t\tif (wav > 1.).any() or (wav < -1.).any():\n\t\t\traise RuntimeError(\'wav has invalid value: {}\'.format(wav_path))\n\t\tif (preem_wav > 1.).any() or (preem_wav < -1.).any():\n\t\t\traise RuntimeError(\'wav has invalid value: {}\'.format(wav_path))\n\n\t#Mu-law quantize\n\tif is_mulaw_quantize(hparams.input_type):\n\t\t#[0, quantize_channels)\n\t\tout = mulaw_quantize(wav, hparams.quantize_channels)\n\n\t\t#Trim silences\n\t\tstart, end = audio.start_and_end_indices(out, hparams.silence_threshold)\n\t\twav = wav[start: end]\n\t\tpreem_wav = preem_wav[start: end]\n\t\tout = out[start: end]\n\n\t\tconstant_values = mulaw_quantize(0, hparams.quantize_channels)\n\t\tout_dtype = np.int16\n\n\telif is_mulaw(hparams.input_type):\n\t\t#[-1, 1]\n\t\tout = mulaw(wav, hparams.quantize_channels)\n\t\tconstant_values = mulaw(0., hparams.quantize_channels)\n\t\tout_dtype = np.float32\n\n\telse:\n\t\t#[-1, 1]\n\t\tout = wav\n\t\tconstant_values = 0.\n\t\tout_dtype = np.float32\n\n\t# Compute the mel scale spectrogram from the wav\n\tmel_spectrogram = audio.melspectrogram(preem_wav, hparams).astype(np.float32)\n\tmel_frames = mel_spectrogram.shape[1]\n\n\tif mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n\t\treturn None\n\n\t#Compute the linear scale spectrogram from the wav\n\tlinear_spectrogram = audio.linearspectrogram(preem_wav, hparams).astype(np.float32)\n\tlinear_frames = linear_spectrogram.shape[1]\n\n\t#sanity check\n\tassert linear_frames == mel_frames\n\n\tif hparams.use_lws:\n\t\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\t\tfft_size = hparams.n_fft if hparams.win_size is None else hparams.win_size\n\t\tl, r = audio.pad_lr(wav, fft_size, audio.get_hop_size(hparams))\n\n\t\t#Zero pad audio signal\n\t\tout = np.pad(out, (l, r), mode=\'constant\', constant_values=constant_values)\n\telse:\n\t\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\t\tl_pad, r_pad = audio.librosa_pad_lr(wav, hparams.n_fft, audio.get_hop_size(hparams), hparams.wavenet_pad_sides)\n\n\t\t#Reflect pad audio signal on the right (Just like it\'s done in Librosa to avoid frame inconsistency)\n\t\tout = np.pad(out, (l_pad, r_pad), mode=\'constant\', constant_values=constant_values)\n\n\tassert len(out) >= mel_frames * audio.get_hop_size(hparams)\n\n\t#time resolution adjustement\n\t#ensure length of raw audio is multiple of hop size so that we can use\n\t#transposed convolution to upsample\n\tout = out[:mel_frames * audio.get_hop_size(hparams)]\n\tassert len(out) % audio.get_hop_size(hparams) == 0\n\ttime_steps = len(out)\n\n\t# Write the spectrogram and audio to disk\n\taudio_filename = \'audio-{}.npy\'.format(index)\n\tmel_filename = \'mel-{}.npy\'.format(index)\n\tlinear_filename = \'linear-{}.npy\'.format(index)\n\tnp.save(os.path.join(wav_dir, audio_filename), out.astype(out_dtype), allow_pickle=False)\n\tnp.save(os.path.join(mel_dir, mel_filename), mel_spectrogram.T, allow_pickle=False)\n\tnp.save(os.path.join(linear_dir, linear_filename), linear_spectrogram.T, allow_pickle=False)\n\n\t# Return a tuple describing this training example\n\treturn (audio_filename, mel_filename, linear_filename, time_steps, mel_frames, text)\n'"
datasets/wavenet_preprocessor.py,0,"b'import os\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\n\nimport numpy as np\nfrom datasets import audio\nfrom wavenet_vocoder.util import is_mulaw, is_mulaw_quantize, mulaw, mulaw_quantize\n\n\ndef build_from_path(hparams, input_dir, mel_dir, wav_dir, n_jobs=12, tqdm=lambda x: x):\n\t""""""\n\tPreprocesses the speech dataset from a gven input path to given output directories\n\n\tArgs:\n\t\t- hparams: hyper parameters\n\t\t- input_dir: input directory that contains the files to prerocess\n\t\t- mel_dir: output directory of the preprocessed speech mel-spectrogram dataset\n\t\t- linear_dir: output directory of the preprocessed speech linear-spectrogram dataset\n\t\t- wav_dir: output directory of the preprocessed speech audio dataset\n\t\t- n_jobs: Optional, number of worker process to parallelize across\n\t\t- tqdm: Optional, provides a nice progress bar\n\n\tReturns:\n\t\t- A list of tuple describing the train examples. this should be written to train.txt\n\t""""""\n\n\t# We use ProcessPoolExecutor to parallelize across processes, this is just for\n\t# optimization purposes and it can be omited\n\texecutor = ProcessPoolExecutor(max_workers=n_jobs)\n\tfutures = []\n\tfor file in os.listdir(input_dir):\n\t\twav_path = os.path.join(input_dir, file)\n\t\tbasename = os.path.basename(wav_path).replace(\'.wav\', \'\')\n\t\tfutures.append(executor.submit(partial(_process_utterance, mel_dir, wav_dir, basename, wav_path, hparams)))\n\n\treturn [future.result() for future in tqdm(futures) if future.result() is not None]\n\n\ndef _process_utterance(mel_dir, wav_dir, index, wav_path, hparams):\n\t""""""\n\tPreprocesses a single utterance wav/text pair\n\n\tthis writes the mel scale spectogram to disk and return a tuple to write\n\tto the train.txt file\n\n\tArgs:\n\t\t- mel_dir: the directory to write the mel spectograms into\n\t\t- linear_dir: the directory to write the linear spectrograms into\n\t\t- wav_dir: the directory to write the preprocessed wav into\n\t\t- index: the numeric index to use in the spectrogram filename\n\t\t- wav_path: path to the audio file containing the speech input\n\t\t- text: text spoken in the input audio file\n\t\t- hparams: hyper parameters\n\n\tReturns:\n\t\t- A tuple: (audio_filename, mel_filename, linear_filename, time_steps, mel_frames, linear_frames, text)\n\t""""""\n\ttry:\n\t\t# Load the audio as numpy array\n\t\twav = audio.load_wav(wav_path, sr=hparams.sample_rate)\n\texcept FileNotFoundError: #catch missing wav exception\n\t\tprint(\'file {} present in csv metadata is not present in wav folder. skipping!\'.format(\n\t\t\twav_path))\n\t\treturn None\n\n\t#M-AILABS extra silence specific\n\tif hparams.trim_silence:\n\t\twav = audio.trim_silence(wav, hparams)\n\n\t#Pre-emphasize\n\tpreem_wav = audio.preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\n\t#rescale wav\n\tif hparams.rescale:\n\t\twav = wav / np.abs(wav).max() * hparams.rescaling_max\n\t\tpreem_wav = preem_wav / np.abs(preem_wav).max() * hparams.rescaling_max\n\n\t\t#Assert all audio is in [-1, 1]\n\t\tif (wav > 1.).any() or (wav < -1.).any():\n\t\t\traise RuntimeError(\'wav has invalid value: {}\'.format(wav_path))\n\t\tif (preem_wav > 1.).any() or (preem_wav < -1.).any():\n\t\t\traise RuntimeError(\'wav has invalid value: {}\'.format(wav_path))\n\n\t#Mu-law quantize\n\tif is_mulaw_quantize(hparams.input_type):\n\t\t#[0, quantize_channels)\n\t\tout = mulaw_quantize(wav, hparams.quantize_channels)\n\n\t\t#Trim silences\n\t\tstart, end = audio.start_and_end_indices(out, hparams.silence_threshold)\n\t\twav = wav[start: end]\n\t\tpreem_wav = preem_wav[start: end]\n\t\tout = out[start: end]\n\n\t\tconstant_values = mulaw_quantize(0, hparams.quantize_channels)\n\t\tout_dtype = np.int16\n\n\telif is_mulaw(hparams.input_type):\n\t\t#[-1, 1]\n\t\tout = mulaw(wav, hparams.quantize_channels)\n\t\tconstant_values = mulaw(0., hparams.quantize_channels)\n\t\tout_dtype = np.float32\n\n\telse:\n\t\t#[-1, 1]\n\t\tout = wav\n\t\tconstant_values = 0.\n\t\tout_dtype = np.float32\n\n\t# Compute the mel scale spectrogram from the wav\n\tmel_spectrogram = audio.melspectrogram(preem_wav, hparams).astype(np.float32)\n\tmel_frames = mel_spectrogram.shape[1]\n\n\tif mel_frames > hparams.max_mel_frames and hparams.clip_mels_length:\n\t\treturn None\n\n\tif hparams.use_lws:\n\t\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\t\tfft_size = hparams.n_fft if hparams.win_size is None else hparams.win_size\n\t\tl, r = audio.pad_lr(wav, fft_size, audio.get_hop_size(hparams))\n\n\t\t#Zero pad audio signal\n\t\tout = np.pad(out, (l, r), mode=\'constant\', constant_values=constant_values)\n\telse:\n\t\t#Ensure time resolution adjustement between audio and mel-spectrogram\n\t\tl_pad, r_pad = audio.librosa_pad_lr(wav, hparams.n_fft, audio.get_hop_size(hparams))\n\n\t\t#Reflect pad audio signal (Just like it\'s done in Librosa to avoid frame inconsistency)\n\t\tout = np.pad(out, (l_pad, r_pad), mode=\'constant\', constant_values=constant_values)\n\n\tassert len(out) >= mel_frames * audio.get_hop_size(hparams)\n\n\t#time resolution adjustement\n\t#ensure length of raw audio is multiple of hop size so that we can use\n\t#transposed convolution to upsample\n\tout = out[:mel_frames * audio.get_hop_size(hparams)]\n\tassert len(out) % audio.get_hop_size(hparams) == 0\n\ttime_steps = len(out)\n\n\t# Write the spectrogram and audio to disk\n\taudio_filename = os.path.join(wav_dir, \'audio-{}.npy\'.format(index))\n\tmel_filename = os.path.join(mel_dir, \'mel-{}.npy\'.format(index))\n\tnp.save(audio_filename, out.astype(out_dtype), allow_pickle=False)\n\tnp.save(mel_filename, mel_spectrogram.T, allow_pickle=False)\n\n\t#global condition features\n\tif hparams.gin_channels > 0:\n\t\traise RuntimeError(\'When activating global conditions, please set your speaker_id rules in line 129 of datasets/wavenet_preprocessor.py to use them during training\')\n\t\tspeaker_id = \'<no_g>\' #put the rule to determine how to assign speaker ids (using file names maybe? file basenames are available in ""index"" variable)\n\telse:\n\t\tspeaker_id = \'<no_g>\'\n\n\t# Return a tuple describing this training example\n\treturn (audio_filename, mel_filename, mel_filename, speaker_id, time_steps, mel_frames)\n'"
tacotron/__init__.py,0,b'#'
tacotron/feeder.py,10,"b'import os\nimport threading\nimport time\nimport traceback\n\nimport numpy as np\nimport tensorflow as tf\nfrom infolog import log\nfrom sklearn.model_selection import train_test_split\nfrom tacotron.utils.text import text_to_sequence\n\n_batches_per_group = 64\n\nclass Feeder:\n\t""""""\n\t\tFeeds batches of data into queue on a background thread.\n\t""""""\n\n\tdef __init__(self, coordinator, metadata_filename, hparams):\n\t\tsuper(Feeder, self).__init__()\n\t\tself._coord = coordinator\n\t\tself._hparams = hparams\n\t\tself._cleaner_names = [x.strip() for x in hparams.cleaners.split(\',\')]\n\t\tself._train_offset = 0\n\t\tself._test_offset = 0\n\n\t\t# Load metadata\n\t\tself._mel_dir = os.path.join(os.path.dirname(metadata_filename), \'mels\')\n\t\tself._linear_dir = os.path.join(os.path.dirname(metadata_filename), \'linear\')\n\t\twith open(metadata_filename, encoding=\'utf-8\') as f:\n\t\t\tself._metadata = [line.strip().split(\'|\') for line in f]\n\t\t\tframe_shift_ms = hparams.hop_size / hparams.sample_rate\n\t\t\thours = sum([int(x[4]) for x in self._metadata]) * frame_shift_ms / (3600)\n\t\t\tlog(\'Loaded metadata for {} examples ({:.2f} hours)\'.format(len(self._metadata), hours))\n\n\t\t#Train test split\n\t\tif hparams.tacotron_test_size is None:\n\t\t\tassert hparams.tacotron_test_batches is not None\n\n\t\ttest_size = (hparams.tacotron_test_size if hparams.tacotron_test_size is not None\n\t\t\telse hparams.tacotron_test_batches * hparams.tacotron_batch_size)\n\t\tindices = np.arange(len(self._metadata))\n\t\ttrain_indices, test_indices = train_test_split(indices,\n\t\t\ttest_size=test_size, random_state=hparams.tacotron_data_random_state)\n\n\t\t#Make sure test_indices is a multiple of batch_size else round down\n\t\tlen_test_indices = self._round_down(len(test_indices), hparams.tacotron_batch_size)\n\t\textra_test = test_indices[len_test_indices:]\n\t\ttest_indices = test_indices[:len_test_indices]\n\t\ttrain_indices = np.concatenate([train_indices, extra_test])\n\n\t\tself._train_meta = list(np.array(self._metadata)[train_indices])\n\t\tself._test_meta = list(np.array(self._metadata)[test_indices])\n\n\t\tself.test_steps = len(self._test_meta) // hparams.tacotron_batch_size\n\n\t\tif hparams.tacotron_test_size is None:\n\t\t\tassert hparams.tacotron_test_batches == self.test_steps\n\n\t\t#pad input sequences with the <pad_token> 0 ( _ )\n\t\tself._pad = 0\n\t\t#explicitely setting the padding to a value that doesn\'t originally exist in the spectogram\n\t\t#to avoid any possible conflicts, without affecting the output range of the model too much\n\t\tif hparams.symmetric_mels:\n\t\t\tself._target_pad = -hparams.max_abs_value\n\t\telse:\n\t\t\tself._target_pad = 0.\n\t\t#Mark finished sequences with 1s\n\t\tself._token_pad = 1.\n\n\t\twith tf.device(\'/cpu:0\'):\n\t\t\t# Create placeholders for inputs and targets. Don\'t specify batch size because we want\n\t\t\t# to be able to feed different batch sizes at eval time.\n\t\t\tself._placeholders = [\n\t\t\ttf.placeholder(tf.int32, shape=(None, None), name=\'inputs\'),\n\t\t\ttf.placeholder(tf.int32, shape=(None, ), name=\'input_lengths\'),\n\t\t\ttf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), name=\'mel_targets\'),\n\t\t\ttf.placeholder(tf.float32, shape=(None, None), name=\'token_targets\'),\n\t\t\ttf.placeholder(tf.float32, shape=(None, None, hparams.num_freq), name=\'linear_targets\'),\n\t\t\ttf.placeholder(tf.int32, shape=(None, ), name=\'targets_lengths\'),\n\t\t\ttf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name=\'split_infos\'),\n\t\t\t]\n\n\t\t\t# Create queue for buffering data\n\t\t\tqueue = tf.FIFOQueue(8, [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32], name=\'input_queue\')\n\t\t\tself._enqueue_op = queue.enqueue(self._placeholders)\n\t\t\tself.inputs, self.input_lengths, self.mel_targets, self.token_targets, self.linear_targets, self.targets_lengths, self.split_infos = queue.dequeue()\n\n\t\t\tself.inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.input_lengths.set_shape(self._placeholders[1].shape)\n\t\t\tself.mel_targets.set_shape(self._placeholders[2].shape)\n\t\t\tself.token_targets.set_shape(self._placeholders[3].shape)\n\t\t\tself.linear_targets.set_shape(self._placeholders[4].shape)\n\t\t\tself.targets_lengths.set_shape(self._placeholders[5].shape)\n\t\t\tself.split_infos.set_shape(self._placeholders[6].shape)\n\n\t\t\t# Create eval queue for buffering eval data\n\t\t\teval_queue = tf.FIFOQueue(1, [tf.int32, tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32], name=\'eval_queue\')\n\t\t\tself._eval_enqueue_op = eval_queue.enqueue(self._placeholders)\n\t\t\tself.eval_inputs, self.eval_input_lengths, self.eval_mel_targets, self.eval_token_targets, \\\n\t\t\t\tself.eval_linear_targets, self.eval_targets_lengths, self.eval_split_infos = eval_queue.dequeue()\n\n\t\t\tself.eval_inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.eval_input_lengths.set_shape(self._placeholders[1].shape)\n\t\t\tself.eval_mel_targets.set_shape(self._placeholders[2].shape)\n\t\t\tself.eval_token_targets.set_shape(self._placeholders[3].shape)\n\t\t\tself.eval_linear_targets.set_shape(self._placeholders[4].shape)\n\t\t\tself.eval_targets_lengths.set_shape(self._placeholders[5].shape)\n\t\t\tself.eval_split_infos.set_shape(self._placeholders[6].shape)\n\n\tdef start_threads(self, session):\n\t\tself._session = session\n\t\tthread = threading.Thread(name=\'background\', target=self._enqueue_next_train_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\t\tthread = threading.Thread(name=\'background\', target=self._enqueue_next_test_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\tdef _get_test_groups(self):\n\t\tmeta = self._test_meta[self._test_offset]\n\t\tself._test_offset += 1\n\n\t\ttext = meta[5]\n\n\t\tinput_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n\t\tmel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n\t\t#Create parallel sequences containing zeros to represent a non finished sequence\n\t\ttoken_target = np.asarray([0.] * (len(mel_target) - 1))\n\t\tlinear_target = np.load(os.path.join(self._linear_dir, meta[2]))\n\t\treturn (input_data, mel_target, token_target, linear_target, len(mel_target))\n\n\tdef make_test_batches(self):\n\t\tstart = time.time()\n\n\t\t# Read a group of examples\n\t\tn = self._hparams.tacotron_batch_size\n\t\tr = self._hparams.outputs_per_step\n\n\t\t#Test on entire test set\n\t\texamples = [self._get_test_groups() for i in range(len(self._test_meta))]\n\n\t\t# Bucket examples based on similar output sequence length for efficiency\n\t\texamples.sort(key=lambda x: x[-1])\n\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\tnp.random.shuffle(batches)\n\n\t\tlog(\'\\nGenerated {} test batches of size {} in {:.3f} sec\'.format(len(batches), n, time.time() - start))\n\t\treturn batches, r\n\n\tdef _enqueue_next_train_group(self):\n\t\twhile not self._coord.should_stop():\n\t\t\tstart = time.time()\n\n\t\t\t# Read a group of examples\n\t\t\tn = self._hparams.tacotron_batch_size\n\t\t\tr = self._hparams.outputs_per_step\n\t\t\texamples = [self._get_next_example() for i in range(n * _batches_per_group)]\n\n\t\t\t# Bucket examples based on similar output sequence length for efficiency\n\t\t\texamples.sort(key=lambda x: x[-1])\n\t\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\t\tnp.random.shuffle(batches)\n\n\t\t\tlog(\'\\nGenerated {} train batches of size {} in {:.3f} sec\'.format(len(batches), n, time.time() - start))\n\t\t\tfor batch in batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n\t\t\t\tself._session.run(self._enqueue_op, feed_dict=feed_dict)\n\n\tdef _enqueue_next_test_group(self):\n\t\t#Create test batches once and evaluate on them for all test steps\n\t\ttest_batches, r = self.make_test_batches()\n\t\twhile not self._coord.should_stop():\n\t\t\tfor batch in test_batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch, r)))\n\t\t\t\tself._session.run(self._eval_enqueue_op, feed_dict=feed_dict)\n\n\tdef _get_next_example(self):\n\t\t""""""Gets a single example (input, mel_target, token_target, linear_target, mel_length) from_ disk\n\t\t""""""\n\t\tif self._train_offset >= len(self._train_meta):\n\t\t\tself._train_offset = 0\n\t\t\tnp.random.shuffle(self._train_meta)\n\n\t\tmeta = self._train_meta[self._train_offset]\n\t\tself._train_offset += 1\n\n\t\ttext = meta[5]\n\n\t\tinput_data = np.asarray(text_to_sequence(text, self._cleaner_names), dtype=np.int32)\n\t\tmel_target = np.load(os.path.join(self._mel_dir, meta[1]))\n\t\t#Create parallel sequences containing zeros to represent a non finished sequence\n\t\ttoken_target = np.asarray([0.] * (len(mel_target) - 1))\n\t\tlinear_target = np.load(os.path.join(self._linear_dir, meta[2]))\n\t\treturn (input_data, mel_target, token_target, linear_target, len(mel_target))\n\n\tdef _prepare_batch(self, batches, outputs_per_step):\n\t\tassert 0 == len(batches) % self._hparams.tacotron_num_gpus\n\t\tsize_per_device = int(len(batches) / self._hparams.tacotron_num_gpus)\n\t\tnp.random.shuffle(batches)\n\n\t\tinputs = None\n\t\tmel_targets = None\n\t\ttoken_targets = None\n\t\tlinear_targets = None\n\t\ttargets_lengths = None\n\t\tsplit_infos = []\n\n\t\ttargets_lengths = np.asarray([x[-1] for x in batches], dtype=np.int32) #Used to mask loss\n\t\tinput_lengths = np.asarray([len(x[0]) for x in batches], dtype=np.int32)\n\n\t\t#Produce inputs/targets of variables lengths for different GPUs\n\t\tfor i in range(self._hparams.tacotron_num_gpus):\n\t\t\tbatch = batches[size_per_device * i: size_per_device * (i + 1)]\n\t\t\tinput_cur_device, input_max_len = self._prepare_inputs([x[0] for x in batch])\n\t\t\tinputs = np.concatenate((inputs, input_cur_device), axis=1) if inputs is not None else input_cur_device\n\t\t\tmel_target_cur_device, mel_target_max_len = self._prepare_targets([x[1] for x in batch], outputs_per_step)\n\t\t\tmel_targets = np.concatenate(( mel_targets, mel_target_cur_device), axis=1) if mel_targets is not None else mel_target_cur_device\n\n\t\t\t#Pad sequences with 1 to infer that the sequence is done\n\t\t\ttoken_target_cur_device, token_target_max_len = self._prepare_token_targets([x[2] for x in batch], outputs_per_step)\n\t\t\ttoken_targets = np.concatenate((token_targets, token_target_cur_device),axis=1) if token_targets is not None else token_target_cur_device\n\t\t\tlinear_targets_cur_device, linear_target_max_len = self._prepare_targets([x[3] for x in batch], outputs_per_step)\n\t\t\tlinear_targets = np.concatenate((linear_targets, linear_targets_cur_device), axis=1) if linear_targets is not None else linear_targets_cur_device\n\t\t\tsplit_infos.append([input_max_len, mel_target_max_len, token_target_max_len, linear_target_max_len])\n\n\t\tsplit_infos = np.asarray(split_infos, dtype=np.int32)\n\t\treturn (inputs, input_lengths, mel_targets, token_targets, linear_targets, targets_lengths, split_infos)\n\n\tdef _prepare_inputs(self, inputs):\n\t\tmax_len = max([len(x) for x in inputs])\n\t\treturn np.stack([self._pad_input(x, max_len) for x in inputs]), max_len\n\n\tdef _prepare_targets(self, targets, alignment):\n\t\tmax_len = max([len(t) for t in targets])\n\t\tdata_len = self._round_up(max_len, alignment)\n\t\treturn np.stack([self._pad_target(t, data_len) for t in targets]), data_len\n\n\tdef _prepare_token_targets(self, targets, alignment):\n\t\tmax_len = max([len(t) for t in targets]) + 1\n\t\tdata_len = self._round_up(max_len, alignment)\n\t\treturn np.stack([self._pad_token_target(t, data_len) for t in targets]), data_len\n\n\tdef _pad_input(self, x, length):\n\t\treturn np.pad(x, (0, length - x.shape[0]), mode=\'constant\', constant_values=self._pad)\n\n\tdef _pad_target(self, t, length):\n\t\treturn np.pad(t, [(0, length - t.shape[0]), (0, 0)], mode=\'constant\', constant_values=self._target_pad)\n\n\tdef _pad_token_target(self, t, length):\n\t\treturn np.pad(t, (0, length - t.shape[0]), mode=\'constant\', constant_values=self._token_pad)\n\n\tdef _round_up(self, x, multiple):\n\t\tremainder = x % multiple\n\t\treturn x if remainder == 0 else x + multiple - remainder\n\n\tdef _round_down(self, x, multiple):\n\t\tremainder = x % multiple\n\t\treturn x if remainder == 0 else x - remainder\n'"
tacotron/synthesize.py,1,"b""import argparse\nimport os\nimport re\nimport time\nfrom time import sleep\n\nimport tensorflow as tf\nfrom hparams import hparams, hparams_debug_string\nfrom infolog import log\nfrom tacotron.synthesizer import Synthesizer\nfrom tqdm import tqdm\n\n\ndef generate_fast(model, text):\n\tmodel.synthesize([text], None, None, None, None)\n\n\ndef run_live(args, checkpoint_path, hparams):\n\t#Log to Terminal without keeping any records in files\n\tlog(hparams_debug_string())\n\tsynth = Synthesizer()\n\tsynth.load(checkpoint_path, hparams)\n\n\t#Generate fast greeting message\n\tgreetings = 'Hello, Welcome to the Live testing tool. Please type a message and I will try to read it!'\n\tlog(greetings)\n\tgenerate_fast(synth, greetings)\n\n\t#Interaction loop\n\twhile True:\n\t\ttry:\n\t\t\ttext = input()\n\t\t\tgenerate_fast(synth, text)\n\n\t\texcept KeyboardInterrupt:\n\t\t\tleave = 'Thank you for testing our features. see you soon.'\n\t\t\tlog(leave)\n\t\t\tgenerate_fast(synth, leave)\n\t\t\tsleep(2)\n\t\t\tbreak\n\ndef run_eval(args, checkpoint_path, output_dir, hparams, sentences):\n\teval_dir = os.path.join(output_dir, 'eval')\n\tlog_dir = os.path.join(output_dir, 'logs-eval')\n\n\tif args.model == 'Tacotron-2':\n\t\tassert os.path.normpath(eval_dir) == os.path.normpath(args.mels_dir)\n\n\t#Create output path if it doesn't exist\n\tos.makedirs(eval_dir, exist_ok=True)\n\tos.makedirs(log_dir, exist_ok=True)\n\tos.makedirs(os.path.join(log_dir, 'wavs'), exist_ok=True)\n\tos.makedirs(os.path.join(log_dir, 'plots'), exist_ok=True)\n\n\tlog(hparams_debug_string())\n\tsynth = Synthesizer()\n\tsynth.load(checkpoint_path, hparams)\n\n\t#Set inputs batch wise\n\tsentences = [sentences[i: i+hparams.tacotron_synthesis_batch_size] for i in range(0, len(sentences), hparams.tacotron_synthesis_batch_size)]\n\n\tlog('Starting Synthesis')\n\twith open(os.path.join(eval_dir, 'map.txt'), 'w') as file:\n\t\tfor i, texts in enumerate(tqdm(sentences)):\n\t\t\tstart = time.time()\n\t\t\tbasenames = ['batch_{}_sentence_{}'.format(i, j) for j in range(len(texts))]\n\t\t\tmel_filenames, speaker_ids = synth.synthesize(texts, basenames, eval_dir, log_dir, None)\n\n\t\t\tfor elems in zip(texts, mel_filenames, speaker_ids):\n\t\t\t\tfile.write('|'.join([str(x) for x in elems]) + '\\n')\n\tlog('synthesized mel spectrograms at {}'.format(eval_dir))\n\treturn eval_dir\n\ndef run_synthesis(args, checkpoint_path, output_dir, hparams):\n\tGTA = (args.GTA == 'True')\n\tif GTA:\n\t\tsynth_dir = os.path.join(output_dir, 'gta')\n\n\t\t#Create output path if it doesn't exist\n\t\tos.makedirs(synth_dir, exist_ok=True)\n\telse:\n\t\tsynth_dir = os.path.join(output_dir, 'natural')\n\n\t\t#Create output path if it doesn't exist\n\t\tos.makedirs(synth_dir, exist_ok=True)\n\n\n\tmetadata_filename = os.path.join(args.input_dir, 'train.txt')\n\tlog(hparams_debug_string())\n\tsynth = Synthesizer()\n\tsynth.load(checkpoint_path, hparams, gta=GTA)\n\twith open(metadata_filename, encoding='utf-8') as f:\n\t\tmetadata = [line.strip().split('|') for line in f]\n\t\tframe_shift_ms = hparams.hop_size / hparams.sample_rate\n\t\thours = sum([int(x[4]) for x in metadata]) * frame_shift_ms / (3600)\n\t\tlog('Loaded metadata for {} examples ({:.2f} hours)'.format(len(metadata), hours))\n\n\t#Set inputs batch wise\n\tmetadata = [metadata[i: i+hparams.tacotron_synthesis_batch_size] for i in range(0, len(metadata), hparams.tacotron_synthesis_batch_size)]\n\n\tlog('Starting Synthesis')\n\tmel_dir = os.path.join(args.input_dir, 'mels')\n\twav_dir = os.path.join(args.input_dir, 'audio')\n\twith open(os.path.join(synth_dir, 'map.txt'), 'w') as file:\n\t\tfor i, meta in enumerate(tqdm(metadata)):\n\t\t\ttexts = [m[5] for m in meta]\n\t\t\tmel_filenames = [os.path.join(mel_dir, m[1]) for m in meta]\n\t\t\twav_filenames = [os.path.join(wav_dir, m[0]) for m in meta]\n\t\t\tbasenames = [os.path.basename(m).replace('.npy', '').replace('mel-', '') for m in mel_filenames]\n\t\t\tmel_output_filenames, speaker_ids = synth.synthesize(texts, basenames, synth_dir, None, mel_filenames)\n\n\t\t\tfor elems in zip(wav_filenames, mel_filenames, mel_output_filenames, speaker_ids, texts):\n\t\t\t\tfile.write('|'.join([str(x) for x in elems]) + '\\n')\n\tlog('synthesized mel spectrograms at {}'.format(synth_dir))\n\treturn os.path.join(synth_dir, 'map.txt')\n\ndef tacotron_synthesize(args, hparams, checkpoint, sentences=None):\n\toutput_dir = 'tacotron_' + args.output_dir\n\n\ttry:\n\t\tcheckpoint_path = tf.train.get_checkpoint_state(checkpoint).model_checkpoint_path\n\t\tlog('loaded model at {}'.format(checkpoint_path))\n\texcept:\n\t\traise RuntimeError('Failed to load checkpoint at {}'.format(checkpoint))\n\n\tif hparams.tacotron_synthesis_batch_size < hparams.tacotron_num_gpus:\n\t\traise ValueError('Defined synthesis batch size {} is smaller than minimum required {} (num_gpus)! Please verify your synthesis batch size choice.'.format(\n\t\t\thparams.tacotron_synthesis_batch_size, hparams.tacotron_num_gpus))\n\n\tif hparams.tacotron_synthesis_batch_size % hparams.tacotron_num_gpus != 0:\n\t\traise ValueError('Defined synthesis batch size {} is not a multiple of {} (num_gpus)! Please verify your synthesis batch size choice!'.format(\n\t\t\thparams.tacotron_synthesis_batch_size, hparams.tacotron_num_gpus))\n\n\tif args.mode == 'eval':\n\t\treturn run_eval(args, checkpoint_path, output_dir, hparams, sentences)\n\telif args.mode == 'synthesis':\n\t\treturn run_synthesis(args, checkpoint_path, output_dir, hparams)\n\telse:\n\t\trun_live(args, checkpoint_path, hparams)\n"""
tacotron/synthesizer.py,11,"b'import os\nimport wave\nfrom datetime import datetime\n\nimport numpy as np\nimport pyaudio\nimport sounddevice as sd\nimport tensorflow as tf\nfrom datasets import audio\nfrom infolog import log\nfrom librosa import effects\nfrom tacotron.models import create_model\nfrom tacotron.utils import plot\nfrom tacotron.utils.text import text_to_sequence\n\n\nclass Synthesizer:\n\tdef load(self, checkpoint_path, hparams, gta=False, model_name=\'Tacotron\'):\n\t\tlog(\'Constructing model: %s\' % model_name)\n\t\t#Force the batch size to be known in order to use attention masking in batch synthesis\n\t\tinputs = tf.placeholder(tf.int32, (None, None), name=\'inputs\')\n\t\tinput_lengths = tf.placeholder(tf.int32, (None), name=\'input_lengths\')\n\t\ttargets = tf.placeholder(tf.float32, (None, None, hparams.num_mels), name=\'mel_targets\')\n\t\tsplit_infos = tf.placeholder(tf.int32, shape=(hparams.tacotron_num_gpus, None), name=\'split_infos\')\n\t\twith tf.variable_scope(\'Tacotron_model\', reuse=tf.AUTO_REUSE) as scope:\n\t\t\tself.model = create_model(model_name, hparams)\n\t\t\tif gta:\n\t\t\t\tself.model.initialize(inputs, input_lengths, targets, gta=gta, split_infos=split_infos)\n\t\t\telse:\n\t\t\t\tself.model.initialize(inputs, input_lengths, split_infos=split_infos)\n\n\t\t\tself.mel_outputs = self.model.tower_mel_outputs\n\t\t\tself.linear_outputs = self.model.tower_linear_outputs if (hparams.predict_linear and not gta) else None\n\t\t\tself.alignments = self.model.tower_alignments\n\t\t\tself.stop_token_prediction = self.model.tower_stop_token_prediction\n\t\t\tself.targets = targets\n\n\t\tif hparams.GL_on_GPU:\n\t\t\tself.GLGPU_mel_inputs = tf.placeholder(tf.float32, (None, hparams.num_mels), name=\'GLGPU_mel_inputs\')\n\t\t\tself.GLGPU_lin_inputs = tf.placeholder(tf.float32, (None, hparams.num_freq), name=\'GLGPU_lin_inputs\')\n\n\t\t\tself.GLGPU_mel_outputs = audio.inv_mel_spectrogram_tensorflow(self.GLGPU_mel_inputs, hparams)\n\t\t\tself.GLGPU_lin_outputs = audio.inv_linear_spectrogram_tensorflow(self.GLGPU_lin_inputs, hparams)\n\n\t\tself.gta = gta\n\t\tself._hparams = hparams\n\t\t#pad input sequences with the <pad_token> 0 ( _ )\n\t\tself._pad = 0\n\t\t#explicitely setting the padding to a value that doesn\'t originally exist in the spectogram\n\t\t#to avoid any possible conflicts, without affecting the output range of the model too much\n\t\tif hparams.symmetric_mels:\n\t\t\tself._target_pad = -hparams.max_abs_value\n\t\telse:\n\t\t\tself._target_pad = 0.\n\n\t\tself.inputs = inputs\n\t\tself.input_lengths = input_lengths\n\t\tself.targets = targets\n\t\tself.split_infos = split_infos\n\n\t\tlog(\'Loading checkpoint: %s\' % checkpoint_path)\n\t\t#Memory allocation on the GPUs as needed\n\t\tconfig = tf.ConfigProto()\n\t\tconfig.gpu_options.allow_growth = True\n\t\tconfig.allow_soft_placement = True\n\n\t\tself.session = tf.Session(config=config)\n\t\tself.session.run(tf.global_variables_initializer())\n\n\t\tsaver = tf.train.Saver()\n\t\tsaver.restore(self.session, checkpoint_path)\n\n\n\tdef synthesize(self, texts, basenames, out_dir, log_dir, mel_filenames):\n\t\thparams = self._hparams\n\t\tcleaner_names = [x.strip() for x in hparams.cleaners.split(\',\')]\n\t\t#[-max, max] or [0,max]\n\t\tT2_output_range = (-hparams.max_abs_value, hparams.max_abs_value) if hparams.symmetric_mels else (0, hparams.max_abs_value)\n\n\t\t#Repeat last sample until number of samples is dividable by the number of GPUs (last run scenario)\n\t\twhile len(texts) % hparams.tacotron_synthesis_batch_size != 0:\n\t\t\ttexts.append(texts[-1])\n\t\t\tbasenames.append(basenames[-1])\n\t\t\tif mel_filenames is not None:\n\t\t\t\tmel_filenames.append(mel_filenames[-1])\n\n\t\tassert 0 == len(texts) % self._hparams.tacotron_num_gpus\n\t\tseqs = [np.asarray(text_to_sequence(text, cleaner_names)) for text in texts]\n\t\tinput_lengths = [len(seq) for seq in seqs]\n\n\t\tsize_per_device = len(seqs) // self._hparams.tacotron_num_gpus\n\n\t\t#Pad inputs according to each GPU max length\n\t\tinput_seqs = None\n\t\tsplit_infos = []\n\t\tfor i in range(self._hparams.tacotron_num_gpus):\n\t\t\tdevice_input = seqs[size_per_device*i: size_per_device*(i+1)]\n\t\t\tdevice_input, max_seq_len = self._prepare_inputs(device_input)\n\t\t\tinput_seqs = np.concatenate((input_seqs, device_input), axis=1) if input_seqs is not None else device_input\n\t\t\tsplit_infos.append([max_seq_len, 0, 0, 0])\n\n\t\tfeed_dict = {\n\t\t\tself.inputs: input_seqs,\n\t\t\tself.input_lengths: np.asarray(input_lengths, dtype=np.int32),\n\t\t}\n\n\t\tif self.gta:\n\t\t\tnp_targets = [np.load(mel_filename) for mel_filename in mel_filenames]\n\t\t\ttarget_lengths = [len(np_target) for np_target in np_targets]\n\n\t\t\t#pad targets according to each GPU max length\n\t\t\ttarget_seqs = None\n\t\t\tfor i in range(self._hparams.tacotron_num_gpus):\n\t\t\t\tdevice_target = np_targets[size_per_device*i: size_per_device*(i+1)]\n\t\t\t\tdevice_target, max_target_len = self._prepare_targets(device_target, self._hparams.outputs_per_step)\n\t\t\t\ttarget_seqs = np.concatenate((target_seqs, device_target), axis=1) if target_seqs is not None else device_target\n\t\t\t\tsplit_infos[i][1] = max_target_len #Not really used but setting it in case for future development maybe?\n\n\t\t\tfeed_dict[self.targets] = target_seqs\n\t\t\tassert len(np_targets) == len(texts)\n\n\t\tfeed_dict[self.split_infos] = np.asarray(split_infos, dtype=np.int32)\n\n\t\tif self.gta or not hparams.predict_linear:\n\t\t\tmels, alignments, stop_tokens = self.session.run([self.mel_outputs, self.alignments, self.stop_token_prediction], feed_dict=feed_dict)\n\n\t\t\t#Linearize outputs (n_gpus -> 1D)\n\t\t\tmels = [mel for gpu_mels in mels for mel in gpu_mels]\n\t\t\talignments = [align for gpu_aligns in alignments for align in gpu_aligns]\n\t\t\tstop_tokens = [token for gpu_token in stop_tokens for token in gpu_token]\n\n\t\t\tif not self.gta:\n\t\t\t\t#Natural batch synthesis\n\t\t\t\t#Get Mel lengths for the entire batch from stop_tokens predictions\n\t\t\t\ttarget_lengths = self._get_output_lengths(stop_tokens)\n\n\t\t\t#Take off the batch wise padding\n\t\t\tmels = [mel[:target_length, :] for mel, target_length in zip(mels, target_lengths)]\n\t\t\tassert len(mels) == len(texts)\n\n\t\telse:\n\t\t\tlinears, mels, alignments, stop_tokens = self.session.run([self.linear_outputs, self.mel_outputs, self.alignments, self.stop_token_prediction], feed_dict=feed_dict)\n\t\t\t\n\t\t\t#Linearize outputs (1D arrays)\n\t\t\tlinears = [linear for gpu_linear in linears for linear in gpu_linear]\n\t\t\tmels = [mel for gpu_mels in mels for mel in gpu_mels]\n\t\t\talignments = [align for gpu_aligns in alignments for align in gpu_aligns]\n\t\t\tstop_tokens = [token for gpu_token in stop_tokens for token in gpu_token]\n\n\t\t\t#Natural batch synthesis\n\t\t\t#Get Mel/Linear lengths for the entire batch from stop_tokens predictions\n\t\t\ttarget_lengths = self._get_output_lengths(stop_tokens)\n\n\t\t\t#Take off the batch wise padding\n\t\t\tmels = [mel[:target_length, :] for mel, target_length in zip(mels, target_lengths)]\n\t\t\tlinears = [linear[:target_length, :] for linear, target_length in zip(linears, target_lengths)]\n\t\t\tlinears = np.clip(linears, T2_output_range[0], T2_output_range[1])\n\t\t\tassert len(mels) == len(linears) == len(texts)\n\n\t\tmels = np.clip(mels, T2_output_range[0], T2_output_range[1])\n\n\t\tif basenames is None:\n\t\t\t#Generate wav and read it\n\t\t\tif hparams.GL_on_GPU:\n\t\t\t\twav = self.session.run(self.GLGPU_mel_outputs, feed_dict={self.GLGPU_mel_inputs: mels[0]})\n\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\telse:\n\t\t\t\twav = audio.inv_mel_spectrogram(mels[0].T, hparams)\n\t\t\taudio.save_wav(wav, \'temp.wav\', sr=hparams.sample_rate) #Find a better way\n\n\t\t\tif platform.system() == \'Linux\':\n\t\t\t\t#Linux wav reader\n\t\t\t\tos.system(\'aplay temp.wav\')\n\n\t\t\telif platform.system() == \'Windows\':\n\t\t\t\t#windows wav reader\n\t\t\t\tos.system(\'start /min mplay32 /play /close temp.wav\')\n\n\t\t\telse:\n\t\t\t\traise RuntimeError(\'Your OS type is not supported yet, please add it to ""tacotron/synthesizer.py, line-165"" and feel free to make a Pull Request ;) Thanks!\')\n\n\t\t\treturn\n\n\n\t\tsaved_mels_paths = []\n\t\tspeaker_ids = []\n\t\tfor i, mel in enumerate(mels):\n\t\t\t#Get speaker id for global conditioning (only used with GTA generally)\n\t\t\tif hparams.gin_channels > 0:\n\t\t\t\traise RuntimeError(\'Please set the speaker_id rule in line 99 of tacotron/synthesizer.py to allow for global condition usage later.\')\n\t\t\t\tspeaker_id = \'<no_g>\' #set the rule to determine speaker id. By using the file basename maybe? (basenames are inside ""basenames"" variable)\n\t\t\t\tspeaker_ids.append(speaker_id) #finish by appending the speaker id. (allows for different speakers per batch if your model is multispeaker)\n\t\t\telse:\n\t\t\t\tspeaker_id = \'<no_g>\'\n\t\t\t\tspeaker_ids.append(speaker_id)\n\n\t\t\t# Write the spectrogram to disk\n\t\t\t# Note: outputs mel-spectrogram files and target ones have same names, just different folders\n\t\t\tmel_filename = os.path.join(out_dir, \'mel-{}.npy\'.format(basenames[i]))\n\t\t\tnp.save(mel_filename, mel, allow_pickle=False)\n\t\t\tsaved_mels_paths.append(mel_filename)\n\n\t\t\tif log_dir is not None:\n\t\t\t\t#save wav (mel -> wav)\n\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\twav = self.session.run(self.GLGPU_mel_outputs, feed_dict={self.GLGPU_mel_inputs: mel})\n\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\telse:\n\t\t\t\t\twav = audio.inv_mel_spectrogram(mel.T, hparams)\n\t\t\t\taudio.save_wav(wav, os.path.join(log_dir, \'wavs/wav-{}-mel.wav\'.format(basenames[i])), sr=hparams.sample_rate)\n\n\t\t\t\t#save alignments\n\t\t\t\tplot.plot_alignment(alignments[i], os.path.join(log_dir, \'plots/alignment-{}.png\'.format(basenames[i])),\n\t\t\t\t\ttitle=\'{}\'.format(texts[i]), split_title=True, max_len=target_lengths[i])\n\n\t\t\t\t#save mel spectrogram plot\n\t\t\t\tplot.plot_spectrogram(mel, os.path.join(log_dir, \'plots/mel-{}.png\'.format(basenames[i])),\n\t\t\t\t\ttitle=\'{}\'.format(texts[i]), split_title=True)\n\n\t\t\t\tif hparams.predict_linear:\n\t\t\t\t\t#save wav (linear -> wav)\n\t\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\t\twav = self.session.run(self.GLGPU_lin_outputs, feed_dict={self.GLGPU_lin_inputs: linears[i]})\n\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\t\telse:\n\t\t\t\t\t\twav = audio.inv_linear_spectrogram(linears[i].T, hparams)\n\t\t\t\t\taudio.save_wav(wav, os.path.join(log_dir, \'wavs/wav-{}-linear.wav\'.format(basenames[i])), sr=hparams.sample_rate)\n\n\t\t\t\t\t#save linear spectrogram plot\n\t\t\t\t\tplot.plot_spectrogram(linears[i], os.path.join(log_dir, \'plots/linear-{}.png\'.format(basenames[i])),\n\t\t\t\t\t\ttitle=\'{}\'.format(texts[i]), split_title=True, auto_aspect=True)\n\n\t\treturn saved_mels_paths, speaker_ids\n\n\tdef _round_up(self, x, multiple):\n\t\tremainder = x % multiple\n\t\treturn x if remainder == 0 else x + multiple - remainder\n\n\tdef _prepare_inputs(self, inputs):\n\t\tmax_len = max([len(x) for x in inputs])\n\t\treturn np.stack([self._pad_input(x, max_len) for x in inputs]), max_len\n\n\tdef _pad_input(self, x, length):\n\t\treturn np.pad(x, (0, length - x.shape[0]), mode=\'constant\', constant_values=self._pad)\n\n\tdef _prepare_targets(self, targets, alignment):\n\t\tmax_len = max([len(t) for t in targets])\n\t\tdata_len = self._round_up(max_len, alignment)\n\t\treturn np.stack([self._pad_target(t, data_len) for t in targets]), data_len\n\n\tdef _pad_target(self, t, length):\n\t\treturn np.pad(t, [(0, length - t.shape[0]), (0, 0)], mode=\'constant\', constant_values=self._target_pad)\n\n\tdef _get_output_lengths(self, stop_tokens):\n\t\t#Determine each mel length by the stop token predictions. (len = first occurence of 1 in stop_tokens row wise)\n\t\toutput_lengths = [row.index(1) if 1 in row else len(row) for row in np.round(stop_tokens).tolist()]\n\t\treturn output_lengths\n'"
tacotron/train.py,42,"b""import argparse\nimport os\nimport subprocess\nimport time\nimport traceback\nfrom datetime import datetime\n\nimport infolog\nimport numpy as np\nimport tensorflow as tf\nfrom datasets import audio\nfrom hparams import hparams_debug_string\nfrom tacotron.feeder import Feeder\nfrom tacotron.models import create_model\nfrom tacotron.utils import ValueWindow, plot\nfrom tacotron.utils.text import sequence_to_text\nfrom tacotron.utils.symbols import symbols\nfrom tqdm import tqdm\n\nlog = infolog.log\n\n\ndef time_string():\n\treturn datetime.now().strftime('%Y-%m-%d %H:%M')\n\ndef add_embedding_stats(summary_writer, embedding_names, paths_to_meta, checkpoint_path):\n\t#Create tensorboard projector\n\tconfig = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n\tconfig.model_checkpoint_path = checkpoint_path\n\n\tfor embedding_name, path_to_meta in zip(embedding_names, paths_to_meta):\n\t\t#Initialize config\n\t\tembedding = config.embeddings.add()\n\t\t#Specifiy the embedding variable and the metadata\n\t\tembedding.tensor_name = embedding_name\n\t\tembedding.metadata_path = path_to_meta\n\t\n\t#Project the embeddings to space dimensions for visualization\n\ttf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n\ndef add_train_stats(model, hparams):\n\twith tf.variable_scope('stats') as scope:\n\t\tfor i in range(hparams.tacotron_num_gpus):\n\t\t\ttf.summary.histogram('mel_outputs %d' % i, model.tower_mel_outputs[i])\n\t\t\ttf.summary.histogram('mel_targets %d' % i, model.tower_mel_targets[i])\n\t\ttf.summary.scalar('before_loss', model.before_loss)\n\t\ttf.summary.scalar('after_loss', model.after_loss)\n\n\t\tif hparams.predict_linear:\n\t\t\ttf.summary.scalar('linear_loss', model.linear_loss)\n\t\t\tfor i in range(hparams.tacotron_num_gpus):\n\t\t\t\ttf.summary.histogram('linear_outputs %d' % i, model.tower_linear_outputs[i])\n\t\t\t\ttf.summary.histogram('linear_targets %d' % i, model.tower_linear_targets[i])\n\t\t\n\t\ttf.summary.scalar('regularization_loss', model.regularization_loss)\n\t\ttf.summary.scalar('stop_token_loss', model.stop_token_loss)\n\t\ttf.summary.scalar('loss', model.loss)\n\t\ttf.summary.scalar('learning_rate', model.learning_rate) #Control learning rate decay speed\n\t\tif hparams.tacotron_teacher_forcing_mode == 'scheduled':\n\t\t\ttf.summary.scalar('teacher_forcing_ratio', model.ratio) #Control teacher forcing ratio decay when mode = 'scheduled'\n\t\tgradient_norms = [tf.norm(grad) for grad in model.gradients]\n\t\ttf.summary.histogram('gradient_norm', gradient_norms)\n\t\ttf.summary.scalar('max_gradient_norm', tf.reduce_max(gradient_norms)) #visualize gradients (in case of explosion)\n\t\treturn tf.summary.merge_all()\n\ndef add_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss, stop_token_loss, loss):\n\tvalues = [\n\ttf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_before_loss', simple_value=before_loss),\n\ttf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_after_loss', simple_value=after_loss),\n\ttf.Summary.Value(tag='Tacotron_eval_model/eval_stats/stop_token_loss', simple_value=stop_token_loss),\n\ttf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_loss', simple_value=loss),\n\t]\n\tif linear_loss is not None:\n\t\tvalues.append(tf.Summary.Value(tag='Tacotron_eval_model/eval_stats/eval_linear_loss', simple_value=linear_loss))\n\ttest_summary = tf.Summary(value=values)\n\tsummary_writer.add_summary(test_summary, step)\n\ndef model_train_mode(args, feeder, hparams, global_step):\n\twith tf.variable_scope('Tacotron_model', reuse=tf.AUTO_REUSE) as scope:\n\t\tmodel_name = None\n\t\tif args.model == 'Tacotron-2':\n\t\t\tmodel_name = 'Tacotron'\n\t\tmodel = create_model(model_name or args.model, hparams)\n\t\tif hparams.predict_linear:\n\t\t\tmodel.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.token_targets, linear_targets=feeder.linear_targets,\n\t\t\t\ttargets_lengths=feeder.targets_lengths, global_step=global_step,\n\t\t\t\tis_training=True, split_infos=feeder.split_infos)\n\t\telse:\n\t\t\tmodel.initialize(feeder.inputs, feeder.input_lengths, feeder.mel_targets, feeder.token_targets,\n\t\t\t\ttargets_lengths=feeder.targets_lengths, global_step=global_step,\n\t\t\t\tis_training=True, split_infos=feeder.split_infos)\n\t\tmodel.add_loss()\n\t\tmodel.add_optimizer(global_step)\n\t\tstats = add_train_stats(model, hparams)\n\t\treturn model, stats\n\ndef model_test_mode(args, feeder, hparams, global_step):\n\twith tf.variable_scope('Tacotron_model', reuse=tf.AUTO_REUSE) as scope:\n\t\tmodel_name = None\n\t\tif args.model == 'Tacotron-2':\n\t\t\tmodel_name = 'Tacotron'\n\t\tmodel = create_model(model_name or args.model, hparams)\n\t\tif hparams.predict_linear:\n\t\t\tmodel.initialize(feeder.eval_inputs, feeder.eval_input_lengths, feeder.eval_mel_targets, feeder.eval_token_targets,\n\t\t\t\tlinear_targets=feeder.eval_linear_targets, targets_lengths=feeder.eval_targets_lengths, global_step=global_step,\n\t\t\t\tis_training=False, is_evaluating=True, split_infos=feeder.eval_split_infos)\n\t\telse:\n\t\t\tmodel.initialize(feeder.eval_inputs, feeder.eval_input_lengths, feeder.eval_mel_targets, feeder.eval_token_targets,\n\t\t\t\ttargets_lengths=feeder.eval_targets_lengths, global_step=global_step, is_training=False, is_evaluating=True, \n\t\t\t\tsplit_infos=feeder.eval_split_infos)\n\t\tmodel.add_loss()\n\t\treturn model\n\ndef train(log_dir, args, hparams):\n\tsave_dir = os.path.join(log_dir, 'taco_pretrained')\n\tplot_dir = os.path.join(log_dir, 'plots')\n\twav_dir = os.path.join(log_dir, 'wavs')\n\tmel_dir = os.path.join(log_dir, 'mel-spectrograms')\n\teval_dir = os.path.join(log_dir, 'eval-dir')\n\teval_plot_dir = os.path.join(eval_dir, 'plots')\n\teval_wav_dir = os.path.join(eval_dir, 'wavs')\n\ttensorboard_dir = os.path.join(log_dir, 'tacotron_events')\n\tmeta_folder = os.path.join(log_dir, 'metas')\n\tos.makedirs(save_dir, exist_ok=True)\n\tos.makedirs(plot_dir, exist_ok=True)\n\tos.makedirs(wav_dir, exist_ok=True)\n\tos.makedirs(mel_dir, exist_ok=True)\n\tos.makedirs(eval_dir, exist_ok=True)\n\tos.makedirs(eval_plot_dir, exist_ok=True)\n\tos.makedirs(eval_wav_dir, exist_ok=True)\n\tos.makedirs(tensorboard_dir, exist_ok=True)\n\tos.makedirs(meta_folder, exist_ok=True)\n\n\tcheckpoint_path = os.path.join(save_dir, 'tacotron_model.ckpt')\n\tinput_path = os.path.join(args.base_dir, args.tacotron_input)\n\n\tif hparams.predict_linear:\n\t\tlinear_dir = os.path.join(log_dir, 'linear-spectrograms')\n\t\tos.makedirs(linear_dir, exist_ok=True)\n\n\tlog('Checkpoint path: {}'.format(checkpoint_path))\n\tlog('Loading training data from: {}'.format(input_path))\n\tlog('Using model: {}'.format(args.model))\n\tlog(hparams_debug_string())\n\n\t#Start by setting a seed for repeatability\n\ttf.set_random_seed(hparams.tacotron_random_seed)\n\n\t#Set up data feeder\n\tcoord = tf.train.Coordinator()\n\twith tf.variable_scope('datafeeder') as scope:\n\t\tfeeder = Feeder(coord, input_path, hparams)\n\n\t#Set up model:\n\tglobal_step = tf.Variable(0, name='global_step', trainable=False)\n\tmodel, stats = model_train_mode(args, feeder, hparams, global_step)\n\teval_model = model_test_mode(args, feeder, hparams, global_step)\n\n\t#Embeddings metadata\n\tchar_embedding_meta = os.path.join(meta_folder, 'CharacterEmbeddings.tsv')\n\tif not os.path.isfile(char_embedding_meta):\n\t\twith open(char_embedding_meta, 'w', encoding='utf-8') as f:\n\t\t\tfor symbol in symbols:\n\t\t\t\tif symbol == ' ':\n\t\t\t\t\tsymbol = '\\\\s' #For visual purposes, swap space with \\s\n\n\t\t\t\tf.write('{}\\n'.format(symbol))\n\n\tchar_embedding_meta = char_embedding_meta.replace(log_dir, '..')\n\n\t#Potential Griffin-Lim GPU setup\n\tif hparams.GL_on_GPU:\n\t\tGLGPU_mel_inputs = tf.placeholder(tf.float32, (None, hparams.num_mels), name='GLGPU_mel_inputs')\n\t\tGLGPU_lin_inputs = tf.placeholder(tf.float32, (None, hparams.num_freq), name='GLGPU_lin_inputs')\n\n\t\tGLGPU_mel_outputs = audio.inv_mel_spectrogram_tensorflow(GLGPU_mel_inputs, hparams)\n\t\tGLGPU_lin_outputs = audio.inv_linear_spectrogram_tensorflow(GLGPU_lin_inputs, hparams)\n\n\t#Book keeping\n\tstep = 0\n\ttime_window = ValueWindow(100)\n\tloss_window = ValueWindow(100)\n\tsaver = tf.train.Saver(max_to_keep=20)\n\n\tlog('Tacotron training set to a maximum of {} steps'.format(args.tacotron_train_steps))\n\n\t#Memory allocation on the GPU as needed\n\tconfig = tf.ConfigProto()\n\tconfig.gpu_options.allow_growth = True\n\tconfig.allow_soft_placement = True\n\n\t#Train\n\twith tf.Session(config=config) as sess:\n\t\ttry:\n\t\t\tsummary_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n\n\t\t\tsess.run(tf.global_variables_initializer())\n\n\t\t\t#saved model restoring\n\t\t\tif args.restore:\n\t\t\t\t# Restore saved model if the user requested it, default = True\n\t\t\t\ttry:\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\n\n\t\t\t\t\tif (checkpoint_state and checkpoint_state.model_checkpoint_path):\n\t\t\t\t\t\tlog('Loading checkpoint {}'.format(checkpoint_state.model_checkpoint_path), slack=True)\n\t\t\t\t\t\tsaver.restore(sess, checkpoint_state.model_checkpoint_path)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tlog('No model to load at {}'.format(save_dir), slack=True)\n\t\t\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\n\n\t\t\t\texcept tf.errors.OutOfRangeError as e:\n\t\t\t\t\tlog('Cannot restore checkpoint: {}'.format(e), slack=True)\n\t\t\telse:\n\t\t\t\tlog('Starting new training!', slack=True)\n\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\n\n\t\t\t#initializing feeder\n\t\t\tfeeder.start_threads(sess)\n\n\t\t\t#Training loop\n\t\t\twhile not coord.should_stop() and step < args.tacotron_train_steps:\n\t\t\t\tstart_time = time.time()\n\t\t\t\tstep, loss, opt = sess.run([global_step, model.loss, model.optimize])\n\t\t\t\ttime_window.append(time.time() - start_time)\n\t\t\t\tloss_window.append(loss)\n\t\t\t\tmessage = 'Step {:7d} [{:.3f} sec/step, loss={:.5f}, avg_loss={:.5f}]'.format(\n\t\t\t\t\tstep, time_window.average, loss, loss_window.average)\n\t\t\t\tlog(message, end='\\r', slack=(step % args.checkpoint_interval == 0))\n\n\t\t\t\tif np.isnan(loss) or loss > 100.:\n\t\t\t\t\tlog('Loss exploded to {:.5f} at step {}'.format(loss, step))\n\t\t\t\t\traise Exception('Loss exploded')\n\n\t\t\t\tif step % args.summary_interval == 0:\n\t\t\t\t\tlog('\\nWriting summary at step {}'.format(step))\n\t\t\t\t\tsummary_writer.add_summary(sess.run(stats), step)\n\n\t\t\t\tif step % args.eval_interval == 0:\n\t\t\t\t\t#Run eval and save eval stats\n\t\t\t\t\tlog('\\nRunning evaluation at step {}'.format(step))\n\n\t\t\t\t\teval_losses = []\n\t\t\t\t\tbefore_losses = []\n\t\t\t\t\tafter_losses = []\n\t\t\t\t\tstop_token_losses = []\n\t\t\t\t\tlinear_losses = []\n\t\t\t\t\tlinear_loss = None\n\n\t\t\t\t\tif hparams.predict_linear:\n\t\t\t\t\t\tfor i in tqdm(range(feeder.test_steps)):\n\t\t\t\t\t\t\teloss, before_loss, after_loss, stop_token_loss, linear_loss, mel_p, mel_t, t_len, align, lin_p, lin_t = sess.run([\n\t\t\t\t\t\t\t\teval_model.tower_loss[0], eval_model.tower_before_loss[0], eval_model.tower_after_loss[0],\n\t\t\t\t\t\t\t\teval_model.tower_stop_token_loss[0], eval_model.tower_linear_loss[0], eval_model.tower_mel_outputs[0][0],\n\t\t\t\t\t\t\t\teval_model.tower_mel_targets[0][0], eval_model.tower_targets_lengths[0][0],\n\t\t\t\t\t\t\t\teval_model.tower_alignments[0][0], eval_model.tower_linear_outputs[0][0],\n\t\t\t\t\t\t\t\teval_model.tower_linear_targets[0][0],\n\t\t\t\t\t\t\t\t])\n\t\t\t\t\t\t\teval_losses.append(eloss)\n\t\t\t\t\t\t\tbefore_losses.append(before_loss)\n\t\t\t\t\t\t\tafter_losses.append(after_loss)\n\t\t\t\t\t\t\tstop_token_losses.append(stop_token_loss)\n\t\t\t\t\t\t\tlinear_losses.append(linear_loss)\n\t\t\t\t\t\tlinear_loss = sum(linear_losses) / len(linear_losses)\n\n\t\t\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\t\t\twav = sess.run(GLGPU_lin_outputs, feed_dict={GLGPU_lin_inputs: lin_p})\n\t\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\twav = audio.inv_linear_spectrogram(lin_p.T, hparams)\n\t\t\t\t\t\taudio.save_wav(wav, os.path.join(eval_wav_dir, 'step-{}-eval-wave-from-linear.wav'.format(step)), sr=hparams.sample_rate)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in tqdm(range(feeder.test_steps)):\n\t\t\t\t\t\t\teloss, before_loss, after_loss, stop_token_loss, mel_p, mel_t, t_len, align = sess.run([\n\t\t\t\t\t\t\t\teval_model.tower_loss[0], eval_model.tower_before_loss[0], eval_model.tower_after_loss[0],\n\t\t\t\t\t\t\t\teval_model.tower_stop_token_loss[0], eval_model.tower_mel_outputs[0][0], eval_model.tower_mel_targets[0][0],\n\t\t\t\t\t\t\t\teval_model.tower_targets_lengths[0][0], eval_model.tower_alignments[0][0]\n\t\t\t\t\t\t\t\t])\n\t\t\t\t\t\t\teval_losses.append(eloss)\n\t\t\t\t\t\t\tbefore_losses.append(before_loss)\n\t\t\t\t\t\t\tafter_losses.append(after_loss)\n\t\t\t\t\t\t\tstop_token_losses.append(stop_token_loss)\n\n\t\t\t\t\teval_loss = sum(eval_losses) / len(eval_losses)\n\t\t\t\t\tbefore_loss = sum(before_losses) / len(before_losses)\n\t\t\t\t\tafter_loss = sum(after_losses) / len(after_losses)\n\t\t\t\t\tstop_token_loss = sum(stop_token_losses) / len(stop_token_losses)\n\n\t\t\t\t\tlog('Saving eval log to {}..'.format(eval_dir))\n\t\t\t\t\t#Save some log to monitor model improvement on same unseen sequence\n\t\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\t\twav = sess.run(GLGPU_mel_outputs, feed_dict={GLGPU_mel_inputs: mel_p})\n\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\t\telse:\n\t\t\t\t\t\twav = audio.inv_mel_spectrogram(mel_p.T, hparams)\n\t\t\t\t\taudio.save_wav(wav, os.path.join(eval_wav_dir, 'step-{}-eval-wave-from-mel.wav'.format(step)), sr=hparams.sample_rate)\n\n\t\t\t\t\tplot.plot_alignment(align, os.path.join(eval_plot_dir, 'step-{}-eval-align.png'.format(step)),\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss),\n\t\t\t\t\t\tmax_len=t_len // hparams.outputs_per_step)\n\t\t\t\t\tplot.plot_spectrogram(mel_p, os.path.join(eval_plot_dir, 'step-{}-eval-mel-spectrogram.png'.format(step)),\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss), target_spectrogram=mel_t,\n\t\t\t\t\t\tmax_len=t_len)\n\n\t\t\t\t\tif hparams.predict_linear:\n\t\t\t\t\t\tplot.plot_spectrogram(lin_p, os.path.join(eval_plot_dir, 'step-{}-eval-linear-spectrogram.png'.format(step)),\n\t\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, eval_loss), target_spectrogram=lin_t,\n\t\t\t\t\t\t\tmax_len=t_len, auto_aspect=True)\n\n\t\t\t\t\tlog('Eval loss for global step {}: {:.3f}'.format(step, eval_loss))\n\t\t\t\t\tlog('Writing eval summary!')\n\t\t\t\t\tadd_eval_stats(summary_writer, step, linear_loss, before_loss, after_loss, stop_token_loss, eval_loss)\n\n\n\t\t\t\tif step % args.checkpoint_interval == 0 or step == args.tacotron_train_steps or step == 300:\n\t\t\t\t\t#Save model and current global step\n\t\t\t\t\tsaver.save(sess, checkpoint_path, global_step=global_step)\n\n\t\t\t\t\tlog('\\nSaving alignment, Mel-Spectrograms and griffin-lim inverted waveform..')\n\t\t\t\t\tif hparams.predict_linear:\n\t\t\t\t\t\tinput_seq, mel_prediction, linear_prediction, alignment, target, target_length, linear_target = sess.run([\n\t\t\t\t\t\t\tmodel.tower_inputs[0][0],\n\t\t\t\t\t\t\tmodel.tower_mel_outputs[0][0],\n\t\t\t\t\t\t\tmodel.tower_linear_outputs[0][0],\n\t\t\t\t\t\t\tmodel.tower_alignments[0][0],\n\t\t\t\t\t\t\tmodel.tower_mel_targets[0][0],\n\t\t\t\t\t\t\tmodel.tower_targets_lengths[0][0],\n\t\t\t\t\t\t\tmodel.tower_linear_targets[0][0],\n\t\t\t\t\t\t\t])\n\n\t\t\t\t\t\t#save predicted linear spectrogram to disk (debug)\n\t\t\t\t\t\tlinear_filename = 'linear-prediction-step-{}.npy'.format(step)\n\t\t\t\t\t\tnp.save(os.path.join(linear_dir, linear_filename), linear_prediction.T, allow_pickle=False)\n\n\t\t\t\t\t\t#save griffin lim inverted wav for debug (linear -> wav)\n\t\t\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\t\t\twav = sess.run(GLGPU_lin_outputs, feed_dict={GLGPU_lin_inputs: linear_prediction})\n\t\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\twav = audio.inv_linear_spectrogram(linear_prediction.T, hparams)\n\t\t\t\t\t\taudio.save_wav(wav, os.path.join(wav_dir, 'step-{}-wave-from-linear.wav'.format(step)), sr=hparams.sample_rate)\n\n\t\t\t\t\t\t#Save real and predicted linear-spectrogram plot to disk (control purposes)\n\t\t\t\t\t\tplot.plot_spectrogram(linear_prediction, os.path.join(plot_dir, 'step-{}-linear-spectrogram.png'.format(step)),\n\t\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss), target_spectrogram=linear_target,\n\t\t\t\t\t\t\tmax_len=target_length, auto_aspect=True)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tinput_seq, mel_prediction, alignment, target, target_length = sess.run([\n\t\t\t\t\t\t\tmodel.tower_inputs[0][0],\n\t\t\t\t\t\t\tmodel.tower_mel_outputs[0][0],\n\t\t\t\t\t\t\tmodel.tower_alignments[0][0],\n\t\t\t\t\t\t\tmodel.tower_mel_targets[0][0],\n\t\t\t\t\t\t\tmodel.tower_targets_lengths[0][0],\n\t\t\t\t\t\t\t])\n\n\t\t\t\t\t#save predicted mel spectrogram to disk (debug)\n\t\t\t\t\tmel_filename = 'mel-prediction-step-{}.npy'.format(step)\n\t\t\t\t\tnp.save(os.path.join(mel_dir, mel_filename), mel_prediction.T, allow_pickle=False)\n\n\t\t\t\t\t#save griffin lim inverted wav for debug (mel -> wav)\n\t\t\t\t\tif hparams.GL_on_GPU:\n\t\t\t\t\t\twav = sess.run(GLGPU_mel_outputs, feed_dict={GLGPU_mel_inputs: mel_prediction})\n\t\t\t\t\t\twav = audio.inv_preemphasis(wav, hparams.preemphasis, hparams.preemphasize)\n\t\t\t\t\telse:\n\t\t\t\t\t\twav = audio.inv_mel_spectrogram(mel_prediction.T, hparams)\n\t\t\t\t\taudio.save_wav(wav, os.path.join(wav_dir, 'step-{}-wave-from-mel.wav'.format(step)), sr=hparams.sample_rate)\n\n\t\t\t\t\t#save alignment plot to disk (control purposes)\n\t\t\t\t\tplot.plot_alignment(alignment, os.path.join(plot_dir, 'step-{}-align.png'.format(step)),\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss),\n\t\t\t\t\t\tmax_len=target_length // hparams.outputs_per_step)\n\t\t\t\t\t#save real and predicted mel-spectrogram plot to disk (control purposes)\n\t\t\t\t\tplot.plot_spectrogram(mel_prediction, os.path.join(plot_dir, 'step-{}-mel-spectrogram.png'.format(step)),\n\t\t\t\t\t\ttitle='{}, {}, step={}, loss={:.5f}'.format(args.model, time_string(), step, loss), target_spectrogram=target,\n\t\t\t\t\t\tmax_len=target_length)\n\t\t\t\t\tlog('Input at step {}: {}'.format(step, sequence_to_text(input_seq)))\n\n\t\t\t\tif step % args.embedding_interval == 0 or step == args.tacotron_train_steps or step == 1:\n\t\t\t\t\t#Get current checkpoint state\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\n\n\t\t\t\t\t#Update Projector\n\t\t\t\t\tlog('\\nSaving Model Character Embeddings visualization..')\n\t\t\t\t\tadd_embedding_stats(summary_writer, [model.embedding_table.name], [char_embedding_meta], checkpoint_state.model_checkpoint_path)\n\t\t\t\t\tlog('Tacotron Character embeddings have been updated on tensorboard!')\n\n\t\t\tlog('Tacotron training complete after {} global steps!'.format(args.tacotron_train_steps), slack=True)\n\t\t\treturn save_dir\n\n\t\texcept Exception as e:\n\t\t\tlog('Exiting due to exception: {}'.format(e), slack=True)\n\t\t\ttraceback.print_exc()\n\t\t\tcoord.request_stop(e)\n\ndef tacotron_train(args, log_dir, hparams):\n\treturn train(log_dir, args, hparams)\n"""
wavenet_vocoder/__init__.py,0,b'#'
wavenet_vocoder/feeder.py,15,"b'import os\nimport threading\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nfrom datasets import audio\nfrom infolog import log\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\n\nfrom .util import is_mulaw_quantize, is_scalar_input\n\n\n\n_batches_per_group = 64\n\n\nclass Feeder:\n\t""""""\n\t\tFeeds batches of data into queue in a background thread.\n\t""""""\n\tdef __init__(self, coordinator, metadata_filename, base_dir, hparams):\n\t\tsuper(Feeder, self).__init__()\n\n\t\tself._coord = coordinator\n\t\tself._hparams = hparams\n\t\tself._train_offset = 0\n\t\tself._test_offset = 0\n\n\t\tif hparams.symmetric_mels:\n\t\t\tself._spec_pad = -hparams.max_abs_value\n\t\telse:\n\t\t\tself._spec_pad = 0.\n\n\t\t#Base directory of the project (to map files from different locations)\n\t\tself._base_dir = base_dir\n\n\t\t#Load metadata\n\t\tself._data_dir = os.path.dirname(metadata_filename)\n\t\twith open(metadata_filename, \'r\') as f:\n\t\t\tself._metadata = [line.strip().split(\'|\') for line in f]\n\n\t\t#Train test split\n\t\tif hparams.wavenet_test_size is None:\n\t\t\tassert hparams.wavenet_test_batches is not None\n\n\t\ttest_size = (hparams.wavenet_test_size if hparams.wavenet_test_size is not None\n\t\t\telse hparams.wavenet_test_batches * hparams.wavenet_batch_size)\n\t\tindices = np.arange(len(self._metadata))\n\t\ttrain_indices, test_indices = train_test_split(indices,\n\t\t\ttest_size=test_size, random_state=hparams.wavenet_data_random_state)\n\n\t\t#Make sure test size is a multiple of batch size else round up\n\t\tlen_test_indices = _round_down(len(test_indices), hparams.wavenet_batch_size)\n\t\textra_test = test_indices[len_test_indices:]\n\t\ttest_indices = test_indices[:len_test_indices]\n\t\ttrain_indices = np.concatenate([train_indices, extra_test])\n\n\t\tself._train_meta = list(np.array(self._metadata)[train_indices])\n\t\tself._test_meta = list(np.array(self._metadata)[test_indices])\n\n\t\tself.test_steps = len(self._test_meta) // hparams.wavenet_batch_size\n\n\t\tif hparams.wavenet_test_size is None:\n\t\t\tassert hparams.wavenet_test_batches == self.test_steps\n\n\t\t#Get conditioning status\n\t\tself.local_condition, self.global_condition = self._check_conditions()\n\n\t\twith tf.device(\'/cpu:0\'):\n\t\t\t# Create placeholders for inputs and targets. Don\'t specify batch size because we want\n\t\t\t# to be able to feed different batch sizes at eval time.\n\t\t\tif is_scalar_input(hparams.input_type):\n\t\t\t\tinput_placeholder = tf.placeholder(tf.float32, shape=(None, 1, None), name=\'audio_inputs\')\n\t\t\t\ttarget_placeholder = tf.placeholder(tf.float32, shape=(None, None, 1), name=\'audio_targets\')\n\t\t\t\ttarget_type = tf.float32\n\t\t\telse:\n\t\t\t\tinput_placeholder = tf.placeholder(tf.float32, shape=(None, hparams.quantize_channels, None), name=\'audio_inputs\')\n\t\t\t\ttarget_placeholder = tf.placeholder(tf.int32, shape=(None, None, 1), name=\'audio_targets\')\n\t\t\t\ttarget_type = tf.int32\n\n\t\t\tself._placeholders = [\n\t\t\tinput_placeholder,\n\t\t\ttarget_placeholder,\n\t\t\ttf.placeholder(tf.int32, shape=(None, ), name=\'input_lengths\'),\n\t\t\t]\n\n\t\t\tqueue_types = [tf.float32, target_type, tf.int32]\n\n\t\t\tif self.local_condition:\n\t\t\t\tself._placeholders.append(tf.placeholder(tf.float32, shape=(None, hparams.num_mels, None), name=\'local_condition_features\'))\n\t\t\t\tqueue_types.append(tf.float32)\n\t\t\tif self.global_condition:\n\t\t\t\tself._placeholders.append(tf.placeholder(tf.int32, shape=(None, 1), name=\'global_condition_features\'))\n\t\t\t\tqueue_types.append(tf.int32)\n\n\t\t\t# Create queue for buffering data\n\t\t\tqueue = tf.FIFOQueue(8, queue_types, name=\'input_queue\')\n\t\t\tself._enqueue_op = queue.enqueue(self._placeholders)\n\t\t\tvariables = queue.dequeue()\n\n\t\t\tself.inputs = variables[0]\n\t\t\tself.inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.targets = variables[1]\n\t\t\tself.targets.set_shape(self._placeholders[1].shape)\n\t\t\tself.input_lengths = variables[2]\n\t\t\tself.input_lengths.set_shape(self._placeholders[2].shape)\n\n\t\t\tidx = 3\n\n\t\t\t#If local conditioning disabled override c inputs with None\n\t\t\tif hparams.cin_channels < 0:\n\t\t\t\tself.local_condition_features = None\n\t\t\telse:\n\t\t\t\tself.local_condition_features = variables[idx]\n\t\t\t\tself.local_condition_features.set_shape(self._placeholders[idx].shape)\n\t\t\t\tidx += 1\n\n\t\t\t#If global conditioning disabled override g inputs with None\n\t\t\tif hparams.gin_channels < 0:\n\t\t\t\tself.global_condition_features = None\n\t\t\telse:\n\t\t\t\tself.global_condition_features = variables[idx]\n\t\t\t\tself.global_condition_features.set_shape(self._placeholders[idx].shape)\n\n\t\t\t# Create queue for buffering eval data\n\t\t\teval_queue = tf.FIFOQueue(1, queue_types, name=\'eval_queue\')\n\t\t\tself._eval_enqueue_op = eval_queue.enqueue(self._placeholders)\n\t\t\teval_variables = eval_queue.dequeue()\n\n\t\t\tself.eval_inputs = eval_variables[0]\n\t\t\tself.eval_inputs.set_shape(self._placeholders[0].shape)\n\t\t\tself.eval_targets = eval_variables[1]\n\t\t\tself.eval_targets.set_shape(self._placeholders[1].shape)\n\t\t\tself.eval_input_lengths = eval_variables[2]\n\t\t\tself.eval_input_lengths.set_shape(self._placeholders[2].shape)\n\n\t\t\teval_idx = 3\n\n\t\t\t#If local conditioning disabled override c inputs with None\n\t\t\tif hparams.cin_channels < 0:\n\t\t\t\tself.eval_local_condition_features = None\n\t\t\telse:\n\t\t\t\tself.eval_local_condition_features = eval_variables[eval_idx]\n\t\t\t\tself.eval_local_condition_features.set_shape(self._placeholders[eval_idx].shape)\n\t\t\t\teval_idx += 1\n\n\t\t\t#If global conditioning disabled override g inputs with None\n\t\t\tif hparams.gin_channels < 0:\n\t\t\t\tself.eval_global_condition_features = None\n\t\t\telse:\n\t\t\t\tself.eval_global_condition_features = eval_variables[eval_idx]\n\t\t\t\tself.eval_global_condition_features.set_shape(self._placeholders[eval_idx].shape)\n\n\n\tdef start_threads(self, session):\n\t\tself._session = session\n\t\tthread = threading.Thread(name=\'background\', target=self._enqueue_next_train_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\t\tthread = threading.Thread(name=\'background\', target=self._enqueue_next_test_group)\n\t\tthread.daemon = True #Thread will close when parent quits\n\t\tthread.start()\n\n\tdef _get_test_groups(self):\n\t\tmeta = self._test_meta[self._test_offset]\n\t\tself._test_offset += 1\n\n\t\tif self._hparams.train_with_GTA:\n\t\t\tmel_file = meta[2]\n\t\telse:\n\t\t\tmel_file = meta[1]\n\t\taudio_file = meta[0]\n\n\t\tinput_data = np.load(os.path.join(self._base_dir, audio_file))\n\n\t\tif self.local_condition:\n\t\t\tlocal_condition_features = np.load(os.path.join(self._base_dir, mel_file))\n\t\telse:\n\t\t\tlocal_condition_features = None\n\n\t\tif self.global_condition:\n\t\t\tglobal_condition_features = meta[3]\n\t\t\tif global_condition_features == \'<no_g>\':\n\t\t\t\traise RuntimeError(\'Please redo the wavenet preprocessing (or GTA synthesis) to assign global condition features!\')\n\t\telse:\n\t\t\tglobal_condition_features = None\n\n\t\treturn (input_data, local_condition_features, global_condition_features, len(input_data))\n\n\tdef make_test_batches(self):\n\t\tstart = time.time()\n\n\t\t#Read one example for evaluation\n\t\tn = 1\n\n\t\t#Test on entire test set (one sample at an evaluation step)\n\t\texamples = [self._get_test_groups() for i in range(len(self._test_meta))]\n\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\tnp.random.shuffle(batches)\n\n\t\tlog(\'\\nGenerated {} test batches of size {} in {:.3f} sec\'.format(len(batches), n, time.time() - start))\n\t\treturn batches\n\n\tdef _enqueue_next_train_group(self):\n\t\twhile not self._coord.should_stop():\n\t\t\tstart = time.time()\n\n\t\t\t# Read a group of examples\n\t\t\tn = self._hparams.wavenet_batch_size\n\t\t\texamples = [self._get_next_example() for i in range(n * _batches_per_group)]\n\n\t\t\t# Bucket examples base on similiar output length for efficiency\n\t\t\texamples.sort(key=lambda x: x[-1])\n\t\t\tbatches = [examples[i: i+n] for i in range(0, len(examples), n)]\n\t\t\tnp.random.shuffle(batches)\n\n\t\t\tlog(\'\\nGenerated {} train batches of size {} in {:.3f} sec\'.format(len(batches), n, time.time() - start))\n\t\t\tfor batch in batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch)))\n\t\t\t\tself._session.run(self._enqueue_op, feed_dict=feed_dict)\n\n\tdef _enqueue_next_test_group(self):\n\t\ttest_batches = self.make_test_batches()\n\t\twhile not self._coord.should_stop():\n\t\t\tfor batch in test_batches:\n\t\t\t\tfeed_dict = dict(zip(self._placeholders, self._prepare_batch(batch)))\n\t\t\t\tself._session.run(self._eval_enqueue_op, feed_dict=feed_dict)\n\n\tdef _get_next_example(self):\n\t\t\'\'\'Get a single example (input, output, len_output) from disk\n\t\t\'\'\'\n\t\tif self._train_offset >= len(self._train_meta):\n\t\t\tself._train_offset = 0\n\t\t\tnp.random.shuffle(self._train_meta)\n\t\tmeta = self._train_meta[self._train_offset]\n\t\tself._train_offset += 1\n\n\t\tif self._hparams.train_with_GTA:\n\t\t\tmel_file = meta[2]\n\t\t\tif \'linear\' in mel_file:\n\t\t\t\traise RuntimeError(\'Linear spectrogram files selected instead of GTA mels, did you specify the wrong metadata?\')\n\t\telse:\n\t\t\tmel_file = meta[1]\n\t\taudio_file = meta[0]\n\n\t\tinput_data = np.load(os.path.join(self._base_dir, audio_file))\n\n\t\tif self.local_condition:\n\t\t\tlocal_condition_features = np.load(os.path.join(self._base_dir, mel_file))\n\t\telse:\n\t\t\tlocal_condition_features = None\n\n\t\tif self.global_condition:\n\t\t\tglobal_condition_features = meta[3]\n\t\t\tif global_condition_features == \'<no_g>\':\n\t\t\t\traise RuntimeError(\'Please redo the wavenet preprocessing (or GTA synthesis) to assign global condition features!\')\n\t\telse:\n\t\t\tglobal_condition_features = None\n\n\t\treturn (input_data, local_condition_features, global_condition_features, len(input_data))\n\n\n\tdef _prepare_batch(self, batches):\n\t\tassert 0 == len(batches) % self._hparams.wavenet_num_gpus\n\t\tsize_per_device = int(len(batches) / self._hparams.wavenet_num_gpus)\n\t\tnp.random.shuffle(batches)\n\n\t\t#Limit time steps to save GPU Memory usage\n\t\tmax_time_steps = self._limit_time()\n\t\t#Adjust time resolution for upsampling\n\t\tbatches = self._adjust_time_resolution(batches, self.local_condition, max_time_steps)\n\n\t\t#time lengths\n\t\tinput_lengths = np.asarray([len(x[0]) for x in batches], np.int32)\n\t\tmax_input_length = max(input_lengths)\n\n\t\t#Since all inputs/targets will have the same lengths for all GPUs, we can simply treat all GPUs batches as one big batch and stack all data. (fixed length)\n\t\tinputs = self._prepare_inputs([x[0] for x in batches], max_input_length)\n\t\ttargets = self._prepare_targets([x[0] for x in batches], max_input_length)\n\t\tlocal_condition_features = self._prepare_local_conditions(self.local_condition, [x[1] for x in batches])\n\t\tglobal_condition_features = self._prepare_global_conditions(self.global_condition, [x[2] for x in batches])\n\n\t\t#Create final batches\n\t\tnew_batches = (inputs, targets, input_lengths)\n\t\tif local_condition_features is not None:\n\t\t\tnew_batches += (local_condition_features, )\n\t\tif global_condition_features is not None:\n\t\t\tnew_batches += (global_condition_features, )\n\n\t\treturn new_batches\n\n\tdef _prepare_inputs(self, inputs, maxlen):\n\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\t#[batch_size, time_steps, quantize_channels]\n\t\t\tx_batch = np.stack([_pad_inputs(np_utils.to_categorical(\n\t\t\t\tx, num_classes=self._hparams.quantize_channels), maxlen) for x in inputs]).astype(np.float32)\n\t\telse:\n\t\t\t#[batch_size, time_steps, 1]\n\t\t\tx_batch = np.stack([_pad_inputs(x.reshape(-1, 1), maxlen) for x in inputs]).astype(np.float32)\n\t\tassert len(x_batch.shape) == 3\n\t\t#Convert to channels first [batch_size, quantize_channels (or 1), time_steps]\n\t\tx_batch = np.transpose(x_batch, (0, 2, 1))\n\t\treturn x_batch\n\n\tdef _prepare_targets(self, targets, maxlen):\n\t\t#[batch_size, time_steps]\n\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\ty_batch = np.stack([_pad_targets(x, maxlen) for x in targets]).astype(np.int32)\n\t\telse:\n\t\t\ty_batch = np.stack([_pad_targets(x, maxlen) for x in targets]).astype(np.float32)\n\t\tassert len(y_batch.shape) == 2\n\t\t#Add extra axis (make 3 dimension)\n\t\ty_batch = np.expand_dims(y_batch, axis=-1)\n\t\treturn y_batch\n\n\tdef _prepare_local_conditions(self, local_condition, c_features):\n\t\tif local_condition:\n\t\t\tmaxlen = max([len(x) for x in c_features])\n\t\t\t#[-max, max] or [0,max]\n\t\t\tT2_output_range = (-self._hparams.max_abs_value, self._hparams.max_abs_value) if self._hparams.symmetric_mels else (0, self._hparams.max_abs_value)\n\n\t\t\tif self._hparams.clip_for_wavenet:\n\t\t\t\tc_features = [np.clip(x, T2_output_range[0], T2_output_range[1]) for x in c_features]\n\t\t\t\t\n\t\t\tc_batch = np.stack([_pad_inputs(x, maxlen, _pad=T2_output_range[0]) for x in c_features]).astype(np.float32)\n\t\t\tassert len(c_batch.shape) == 3\n\t\t\t#[batch_size, c_channels, time_steps]\n\t\t\tc_batch = np.transpose(c_batch, (0, 2, 1))\n\n\t\t\tif self._hparams.normalize_for_wavenet:\n\t\t\t\t#rerange to [0, 1]\n\t\t\t\tc_batch = _interp(c_batch, T2_output_range).astype(np.float32)\n\n\t\telse:\n\t\t\tc_batch = None\n\n\t\treturn c_batch\n\n\tdef _prepare_global_conditions(self, global_condition, g_features):\n\t\tif global_condition:\n\t\t\tg_batch = np.array(g_features).astype(np.int32).reshape(-1, 1)\n\n\t\telse:\n\t\t\tg_batch = None\n\n\t\treturn g_batch\n\n\tdef _check_conditions(self):\n\t\tlocal_condition = self._hparams.cin_channels > 0\n\t\tglobal_condition = self._hparams.gin_channels > 0\n\t\treturn local_condition, global_condition\n\n\tdef _limit_time(self):\n\t\t\'\'\'Limit time resolution to save GPU memory.\n\t\t\'\'\'\n\t\tif self._hparams.max_time_sec is not None:\n\t\t\treturn int(self._hparams.max_time_sec * self._hparams.sample_rate)\n\n\t\telif self._hparams.max_time_steps is not None:\n\t\t\treturn self._hparams.max_time_steps\n\n\t\telse:\n\t\t\treturn None\n\n\tdef _adjust_time_resolution(self, batch, local_condition, max_time_steps):\n\t\t\'\'\'Adjust time resolution between audio and local condition\n\t\t\'\'\'\n\t\tif local_condition:\n\t\t\tnew_batch = []\n\t\t\tfor b in batch:\n\t\t\t\tx, c, g, l = b\n\t\t\t\tself._assert_ready_for_upsample(x, c)\n\t\t\t\tif max_time_steps is not None:\n\t\t\t\t\tmax_steps = _ensure_divisible(max_time_steps, audio.get_hop_size(self._hparams), True)\n\t\t\t\t\tif len(x) > max_time_steps:\n\t\t\t\t\t\tmax_time_frames = max_steps // audio.get_hop_size(self._hparams)\n\t\t\t\t\t\tstart = np.random.randint(0, len(c) - max_time_frames)\n\t\t\t\t\t\ttime_start = start * audio.get_hop_size(self._hparams)\n\t\t\t\t\t\tx = x[time_start: time_start + max_time_frames * audio.get_hop_size(self._hparams)]\n\t\t\t\t\t\tc = c[start: start + max_time_frames, :]\n\t\t\t\t\t\tself._assert_ready_for_upsample(x, c)\n\n\t\t\t\tnew_batch.append((x, c, g, l))\n\t\t\treturn new_batch\n\n\t\telse:\n\t\t\tnew_batch = []\n\t\t\tfor b in batch:\n\t\t\t\tx, c, g, l = b\n\t\t\t\tx = audio.trim_silence(x, hparams)\n\t\t\t\tif max_time_steps is not None and len(x) > max_time_steps:\n\t\t\t\t\tstart = np.random.randint(0, len(c) - max_time_steps)\n\t\t\t\t\tx = x[start: start + max_time_steps]\n\t\t\t\tnew_batch.append((x, c, g, l))\n\t\t\treturn new_batch\n\n\tdef _assert_ready_for_upsample(self, x, c):\n\t\tassert len(x) % len(c) == 0 and len(x) // len(c) == audio.get_hop_size(self._hparams)\n\n\ndef _pad_inputs(x, maxlen, _pad=0):\n\treturn np.pad(x, [(0, maxlen - len(x)), (0, 0)], mode=\'constant\', constant_values=_pad)\n\ndef _pad_targets(x, maxlen, _pad=0):\n\treturn np.pad(x, (0, maxlen - len(x)), mode=\'constant\', constant_values=_pad)\n\ndef _round_up(x, multiple):\n\tremainder = x % multiple\n\treturn x if remainder == 0 else x + multiple - remainder\n\ndef _round_down(x, multiple):\n\tremainder = x % multiple\n\treturn x if remainder == 0 else x - remainder\n\ndef _ensure_divisible(length, divisible_by=256, lower=True):\n\tif length % divisible_by == 0:\n\t\treturn length\n\tif lower:\n\t\treturn length - length % divisible_by\n\telse:\n\t\treturn length + (divisible_by - length % divisible_by)\n\ndef _interp(feats, in_range):\n\t#rescales from [-max, max] (or [0, max]) to [0, 1]\n\treturn (feats - in_range[0]) / (in_range[1] - in_range[0])\n'"
wavenet_vocoder/synthesize.py,1,"b""import argparse\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nfrom hparams import hparams, hparams_debug_string\nfrom infolog import log\nfrom tqdm import tqdm\nfrom wavenet_vocoder.synthesizer import Synthesizer\n\n\ndef run_synthesis(args, checkpoint_path, output_dir, hparams):\n\tlog_dir = os.path.join(output_dir, 'plots')\n\twav_dir = os.path.join(output_dir, 'wavs')\n\n\t#We suppose user will provide correct folder depending on training method\n\tlog(hparams_debug_string())\n\tsynth = Synthesizer()\n\tsynth.load(checkpoint_path, hparams)\n\n\tif args.model == 'Tacotron-2':\n\t\t#If running all Tacotron-2, synthesize audio from evaluated mels\n\t\tmetadata_filename = os.path.join(args.mels_dir, 'map.txt')\n\t\twith open(metadata_filename, encoding='utf-8') as f:\n\t\t\tmetadata = np.array([line.strip().split('|') for line in f])\n\n\t\tspeaker_ids = metadata[:, 2]\n\t\tmel_files = metadata[:, 1]\n\t\ttexts = metadata[:, 0]\n\n\t\tspeaker_ids = None if (speaker_ids == '<no_g>').all() else speaker_ids\n\telse:\n\t\t#else Get all npy files in input_dir (supposing they are mels)\n\t\tmel_files  = sorted([os.path.join(args.mels_dir, f) for f in os.listdir(args.mels_dir) if f.split('.')[-1] == 'npy'])\n\t\tspeaker_ids = None if args.speaker_id is None else args.speaker_id.replace(' ', '').split(',')\n\t\tif speaker_ids is not None:\n\t\t\tassert len(speaker_ids) == len(mel_files)\n\n\t\ttexts = None\n\n\tlog('Starting synthesis! (this will take a while..)')\n\tos.makedirs(log_dir, exist_ok=True)\n\tos.makedirs(wav_dir, exist_ok=True)\n\n\tmel_files = [mel_files[i: i+hparams.wavenet_synthesis_batch_size] for i in range(0, len(mel_files), hparams.wavenet_synthesis_batch_size)]\n\tspeaker_ids = None if speaker_ids is None else [speaker_ids[i: i+hparams.wavenet_synthesis_batch_size] for i in range(0, len(speaker_ids), hparams.wavenet_synthesis_batch_size)]\n\ttexts = None if texts is None else [texts[i: i+hparams.wavenet_synthesis_batch_size] for i in range(0, len(texts), hparams.wavenet_synthesis_batch_size)]\n\n\twith open(os.path.join(wav_dir, 'map.txt'), 'w') as file:\n\t\tfor i, mel_batch in enumerate(tqdm(mel_files)):\n\t\t\tmel_spectros = [np.load(mel_file) for mel_file in mel_batch]\n\n\t\t\tbasenames = [os.path.basename(mel_file).replace('.npy', '') for mel_file in mel_batch]\n\t\t\tspeaker_id_batch = None if speaker_ids is None else speaker_ids[i]\n\t\t\taudio_files = synth.synthesize(mel_spectros, speaker_id_batch, basenames, wav_dir, log_dir)\n\n\t\t\tspeaker_logs = ['<no_g>'] * len(mel_batch) if speaker_id_batch is None else speaker_id_batch\n\n\t\t\tfor j, mel_file in enumerate(mel_batch):\n\t\t\t\tif texts is None:\n\t\t\t\t\tfile.write('{}|{}\\n'.format(mel_file, audio_files[j], speaker_logs[j]))\n\t\t\t\telse:\n\t\t\t\t\tfile.write('{}|{}|{}\\n'.format(texts[i][j], mel_file, audio_files[j], speaker_logs[j]))\n\n\tlog('synthesized audio waveforms at {}'.format(wav_dir))\n\n\n\ndef wavenet_synthesize(args, hparams, checkpoint):\n\toutput_dir = 'wavenet_' + args.output_dir\n\n\ttry:\n\t\tcheckpoint_path = tf.train.get_checkpoint_state(checkpoint).model_checkpoint_path\n\t\tlog('loaded model at {}'.format(checkpoint_path))\n\texcept:\n\t\traise RuntimeError('Failed to load checkpoint at {}'.format(checkpoint))\n\n\trun_synthesis(args, checkpoint_path, output_dir, hparams)\n"""
wavenet_vocoder/synthesizer.py,9,"b""import os\n\nimport numpy as np\nimport tensorflow as tf\nfrom datasets.audio import save_wavenet_wav, get_hop_size, melspectrogram\nfrom infolog import log\nfrom wavenet_vocoder.models import create_model\nfrom wavenet_vocoder.train import create_shadow_saver, load_averaged_model\nfrom wavenet_vocoder.feeder import _interp\n\nfrom . import util\n\n\nclass Synthesizer:\n\tdef load(self, checkpoint_path, hparams, model_name='WaveNet'):\n\t\tlog('Constructing model: {}'.format(model_name))\n\t\tself._hparams = hparams\n\t\tlocal_cond, global_cond = self._check_conditions()\n\n\t\tself.local_conditions = tf.placeholder(tf.float32, shape=(None, None, hparams.num_mels), name='local_condition_features') if local_cond else None\n\t\tself.global_conditions = tf.placeholder(tf.int32, shape=(None, 1), name='global_condition_features') if global_cond else None\n\t\tself.synthesis_length = tf.placeholder(tf.int32, shape=(), name='synthesis_length') if not local_cond else None\n\t\tself.targets = tf.placeholder(tf.float32, shape=(1, None, 1), name='audio_targets') if hparams.wavenet_synth_debug else None #Debug only with 1 wav\n\t\tself.input_lengths = tf.placeholder(tf.int32, shape=(1, ), name='input_lengths') if hparams.wavenet_synth_debug else None\n\t\tself.synth_debug = hparams.wavenet_synth_debug\n\n\t\twith tf.variable_scope('WaveNet_model') as scope:\n\t\t\tself.model = create_model(model_name, hparams)\n\t\t\tself.model.initialize(y=None, c=self.local_conditions, g=self.global_conditions,\n\t\t\t\tinput_lengths=self.input_lengths, synthesis_length=self.synthesis_length, test_inputs=self.targets)\n\n\t\t\tself._hparams = hparams\n\t\t\tsh_saver = create_shadow_saver(self.model)\n\n\t\t\tlog('Loading checkpoint: {}'.format(checkpoint_path))\n\t\t\t#Memory allocation on the GPU as needed\n\t\t\tconfig = tf.ConfigProto()\n\t\t\tconfig.gpu_options.allow_growth = True\n\t\t\tconfig.allow_soft_placement = True\n\n\t\t\tself.session = tf.Session(config=config)\n\t\t\tself.session.run(tf.global_variables_initializer())\n\n\t\tload_averaged_model(self.session, sh_saver, checkpoint_path)\n\n\tdef synthesize(self, mel_spectrograms, speaker_ids, basenames, out_dir, log_dir):\n\t\thparams = self._hparams\n\t\tlocal_cond, global_cond = self._check_conditions()\n\n\t\t#Switch mels in case of debug\n\t\tif self.synth_debug:\n\t\t\tassert len(hparams.wavenet_debug_mels) == len(hparams.wavenet_debug_wavs)\n\t\t\tmel_spectrograms = [np.load(mel_file) for mel_file in hparams.wavenet_debug_mels]\n\n\t\t#Get True length of audio to be synthesized: audio_len = mel_len * hop_size\n\t\taudio_lengths = [len(x) * get_hop_size(self._hparams) for x in mel_spectrograms]\n\n\t\t#Prepare local condition batch\n\t\tmaxlen = max([len(x) for x in mel_spectrograms])\n\t\t#[-max, max] or [0,max]\n\t\tT2_output_range = (-self._hparams.max_abs_value, self._hparams.max_abs_value) if self._hparams.symmetric_mels else (0, self._hparams.max_abs_value)\n\n\t\tif self._hparams.clip_for_wavenet:\n\t\t\tmel_spectrograms = [np.clip(x, T2_output_range[0], T2_output_range[1]) for x in mel_spectrograms]\n\n\t\tc_batch = np.stack([_pad_inputs(x, maxlen, _pad=T2_output_range[0]) for x in mel_spectrograms]).astype(np.float32)\n\n\t\tif self._hparams.normalize_for_wavenet:\n\t\t\t#rerange to [0, 1]\n\t\t\tc_batch = _interp(c_batch, T2_output_range).astype(np.float32)\n\n\t\tg = None if speaker_ids is None else np.asarray(speaker_ids, dtype=np.int32).reshape(len(c_batch), 1)\n\t\tfeed_dict = {}\n\n\t\tif local_cond:\n\t\t\tfeed_dict[self.local_conditions] = c_batch\n\t\telse:\n\t\t\tfeed_dict[self.synthesis_length] = 100\n\n\t\tif global_cond:\n\t\t\tfeed_dict[self.global_conditions] = g\n\n\t\tif self.synth_debug:\n\t\t\tdebug_wavs = hparams.wavenet_debug_wavs\n\t\t\tassert len(debug_wavs) % hparams.wavenet_num_gpus == 0\n\t\t\ttest_wavs = [np.load(debug_wav).reshape(-1, 1) for debug_wav in debug_wavs]\n\n\t\t\t#pad wavs to same length\n\t\t\tmax_test_len = max([len(x) for x in test_wavs])\n\t\t\ttest_wavs = np.stack([_pad_inputs(x, max_test_len) for x in test_wavs]).astype(np.float32)\n\n\t\t\tassert len(test_wavs) == len(debug_wavs)\n\t\t\tfeed_dict[self.targets] = test_wavs.reshape(len(test_wavs), max_test_len, 1)\n\t\t\tfeed_dict[self.input_lengths] = np.asarray([test_wavs.shape[1]])\n\n\t\t#Generate wavs and clip extra padding to select Real speech parts\n\t\tgenerated_wavs, upsampled_features = self.session.run([self.model.tower_y_hat, self.model.tower_synth_upsampled_local_features], feed_dict=feed_dict)\n\n\t\t#Linearize outputs (n_gpus -> 1D)\n\t\tgenerated_wavs = [wav for gpu_wavs in generated_wavs for wav in gpu_wavs]\n\t\tupsampled_features = [feat for gpu_feats in upsampled_features for feat in gpu_feats]\n\n\t\tgenerated_wavs = [generated_wav[:length] for generated_wav, length in zip(generated_wavs, audio_lengths)]\n\t\tupsampled_features = [upsampled_feature[:, :length] for upsampled_feature, length in zip(upsampled_features, audio_lengths)]\n\n\t\taudio_filenames = []\n\t\tfor i, (generated_wav, input_mel, upsampled_feature) in enumerate(zip(generated_wavs, mel_spectrograms, upsampled_features)):\n\t\t\t#Save wav to disk\n\t\t\taudio_filename = os.path.join(out_dir, 'wavenet-audio-{}.wav'.format(basenames[i]))\n\t\t\tsave_wavenet_wav(generated_wav, audio_filename, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\t\t\taudio_filenames.append(audio_filename)\n\n\t\t\t#Compare generated wav mel with original input mel to evaluate wavenet audio reconstruction performance\n\t\t\t#Both mels should match on low frequency information, wavenet mel should contain more high frequency detail when compared to Tacotron mels.\n\t\t\tgenerated_mel = melspectrogram(generated_wav, hparams).T\n\t\t\tutil.plot_spectrogram(generated_mel, os.path.join(log_dir, 'wavenet-mel-spectrogram-{}.png'.format(basenames[i])),\n\t\t\t\ttitle='Local Condition vs Reconstructed Audio Mel-Spectrogram analysis', target_spectrogram=input_mel)\n\t\t\t#Save upsampled features to visualize checkerboard artifacts.\n\t\t\tutil.plot_spectrogram(upsampled_feature.T, os.path.join(log_dir, 'wavenet-upsampled_features-{}.png'.format(basenames[i])),\n\t\t\t\ttitle='Upmsampled Local Condition features', auto_aspect=True)\n\n\t\t\t#Save waveplot to disk\n\t\t\tif log_dir is not None:\n\t\t\t\tplot_filename = os.path.join(log_dir, 'wavenet-waveplot-{}.png'.format(basenames[i]))\n\t\t\t\tutil.waveplot(plot_filename, generated_wav, None, hparams, title='WaveNet generated Waveform.')\n\n\t\treturn audio_filenames\n\n\tdef _check_conditions(self):\n\t\tlocal_condition = self._hparams.cin_channels > 0\n\t\tglobal_condition = self._hparams.gin_channels > 0\n\t\treturn local_condition, global_condition\n\n\ndef _pad_inputs(x, maxlen, _pad=0):\n\treturn np.pad(x, [(0, maxlen - len(x)), (0, 0)], mode='constant', constant_values=_pad)\n"""
wavenet_vocoder/train.py,30,"b""import argparse\nimport os\nimport sys\nimport time\nimport traceback\nfrom datetime import datetime\n\nimport infolog\nimport librosa\nimport numpy as np\nimport tensorflow as tf\nfrom hparams import hparams_debug_string\nfrom datasets.audio import save_wavenet_wav, melspectrogram\nfrom tacotron.utils import ValueWindow\nfrom wavenet_vocoder.feeder import Feeder, _interp\nfrom wavenet_vocoder.models import create_model\n\nfrom . import util\n\nlog = infolog.log\n\n\ndef time_string():\n\treturn datetime.now().strftime('%Y-%m-%d %H:%M')\n\ndef add_embedding_stats(summary_writer, embedding_names, paths_to_meta, checkpoint_path):\n\t#Create tensorboard projector\n\tconfig = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n\tconfig.model_checkpoint_path = checkpoint_path\n\n\tfor embedding_name, path_to_meta in zip(embedding_names, paths_to_meta):\n\t\t#Initialize config\n\t\tembedding = config.embeddings.add()\n\t\t#Specifiy the embedding variable and the metadata\n\t\tembedding.tensor_name = embedding_name\n\t\tembedding.metadata_path = path_to_meta\n\t\n\t#Project the embeddings to space dimensions for visualization\n\ttf.contrib.tensorboard.plugins.projector.visualize_embeddings(summary_writer, config)\n\ndef add_train_stats(model, hparams):\n\twith tf.variable_scope('stats') as scope:\n\t\tfor i in range(hparams.wavenet_num_gpus):\n\t\t\ttf.summary.histogram('wav_outputs %d' % i, model.tower_y_hat_log[i])\n\t\t\ttf.summary.histogram('wav_targets %d' % i, model.tower_y_log[i])\n\t\t\tif model.tower_means[i] is not None:\n\t\t\t\ttf.summary.histogram('gaussian_means %d' % i, model.tower_means[i])\n\t\t\t\ttf.summary.histogram('gaussian_log_scales %d' % i, model.tower_log_scales[i])\n\n\t\ttf.summary.scalar('wavenet_learning_rate', model.learning_rate)\n\t\ttf.summary.scalar('wavenet_loss', model.loss)\n\n\t\tgradient_norms = [tf.norm(grad) for grad in model.gradients if grad is not None]\n\t\ttf.summary.histogram('gradient_norm', gradient_norms)\n\t\ttf.summary.scalar('max_gradient_norm', tf.reduce_max(gradient_norms)) #visualize gradients (in case of explosion)\n\t\treturn tf.summary.merge_all()\n\ndef add_test_stats(summary_writer, step, eval_loss, hparams):\n\tvalues = [\n\ttf.Summary.Value(tag='Wavenet_eval_model/eval_stats/wavenet_eval_loss', simple_value=eval_loss),\n\t]\n\n\ttest_summary = tf.Summary(value=values)\n\tsummary_writer.add_summary(test_summary, step)\n\n\ndef create_shadow_saver(model, global_step=None):\n\t'''Load shadow variables of saved model.\n\n\tInspired by: https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\n\tCan also use: shadow_dict = model.ema.variables_to_restore()\n\t'''\n\t#Add global step to saved variables to save checkpoints correctly\n\tshadow_variables = [model.ema.average_name(v) for v in model.variables]\n\tvariables = model.variables\n\n\tif global_step is not None:\n\t\tshadow_variables += ['global_step']\n\t\tvariables += [global_step]\n\n\tshadow_dict = dict(zip(shadow_variables, variables)) #dict(zip(keys, values)) -> {key1: value1, key2: value2, ...}\n\treturn tf.train.Saver(shadow_dict, max_to_keep=20)\n\ndef load_averaged_model(sess, sh_saver, checkpoint_path):\n\tsh_saver.restore(sess, checkpoint_path)\n\n\ndef eval_step(sess, global_step, model, plot_dir, wav_dir, summary_writer, hparams, model_name):\n\t'''Evaluate model during training.\n\tSupposes that model variables are averaged.\n\t'''\n\tstart_time = time.time()\n\ty_hat, y_target, loss, input_mel, upsampled_features = sess.run([model.tower_y_hat[0], model.tower_y_target[0],\n\t\tmodel.eval_loss, model.tower_eval_c[0], model.tower_eval_upsampled_local_features[0]])\n\tduration = time.time() - start_time\n\tlog('Time Evaluation: Generation of {} audio frames took {:.3f} sec ({:.3f} frames/sec)'.format(\n\t\tlen(y_target), duration, len(y_target)/duration))\n\n\t#Make audio and plot paths\n\tpred_wav_path = os.path.join(wav_dir, 'step-{}-pred.wav'.format(global_step))\n\ttarget_wav_path = os.path.join(wav_dir, 'step-{}-real.wav'.format(global_step))\n\tplot_path = os.path.join(plot_dir, 'step-{}-waveplot.png'.format(global_step))\n\tmel_path = os.path.join(plot_dir, 'step-{}-reconstruction-mel-spectrogram.png'.format(global_step))\n\tupsampled_path = os.path.join(plot_dir, 'step-{}-upsampled-features.png'.format(global_step))\n\n\t#Save figure\n\tutil.waveplot(plot_path, y_hat, y_target, model._hparams, title='{}, {}, step={}, loss={:.5f}'.format(model_name, time_string(), global_step, loss))\n\tlog('Eval loss for global step {}: {:.3f}'.format(global_step, loss))\n\n\t#Compare generated wav mel with original input mel to evaluate wavenet audio reconstruction performance\n\t#Both mels should match on low frequency information, wavenet mel should contain more high frequency detail when compared to Tacotron mels.\n\tT2_output_range = (-hparams.max_abs_value, hparams.max_abs_value) if hparams.symmetric_mels else (0, hparams.max_abs_value)\n\tgenerated_mel = _interp(melspectrogram(y_hat, hparams).T, T2_output_range)\n\tutil.plot_spectrogram(generated_mel, mel_path, title='Local Condition vs Reconst. Mel-Spectrogram, step={}, loss={:.5f}'.format(\n\t\tglobal_step, loss), target_spectrogram=input_mel.T)\n\tutil.plot_spectrogram(upsampled_features.T, upsampled_path, title='Upsampled Local Condition features, step={}, loss={:.5f}'.format(\n\t\tglobal_step, loss), auto_aspect=True)\n\n\t#Save Audio\n\tsave_wavenet_wav(y_hat, pred_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\tsave_wavenet_wav(y_target, target_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\n\t#Write eval summary to tensorboard\n\tlog('Writing eval summary!')\n\tadd_test_stats(summary_writer, global_step, loss, hparams=hparams)\n\ndef save_log(sess, global_step, model, plot_dir, wav_dir, hparams, model_name):\n\tlog('\\nSaving intermediate states at step {}'.format(global_step))\n\tidx = 0\n\ty_hat, y, loss, length, input_mel, upsampled_features = sess.run([model.tower_y_hat_log[0][idx], \n\t\tmodel.tower_y_log[0][idx], \n\t\tmodel.loss,\n\t\tmodel.tower_input_lengths[0][idx], \n\t\tmodel.tower_c[0][idx], model.tower_upsampled_local_features[0][idx]])\n\n\t#mask by length\n\ty_hat[length:] = 0\n\ty[length:] = 0\n\n\t#Make audio and plot paths\n\tpred_wav_path = os.path.join(wav_dir, 'step-{}-pred.wav'.format(global_step))\n\ttarget_wav_path = os.path.join(wav_dir, 'step-{}-real.wav'.format(global_step))\n\tplot_path = os.path.join(plot_dir, 'step-{}-waveplot.png'.format(global_step))\n\tmel_path = os.path.join(plot_dir, 'step-{}-reconstruction-mel-spectrogram.png'.format(global_step))\n\tupsampled_path = os.path.join(plot_dir, 'step-{}-upsampled-features.png'.format(global_step))\n\n\t#Save figure\n\tutil.waveplot(plot_path, y_hat, y, hparams, title='{}, {}, step={}, loss={:.5f}'.format(model_name, time_string(), global_step, loss))\n\n\t#Compare generated wav mel with original input mel to evaluate wavenet audio reconstruction performance\n\t#Both mels should match on low frequency information, wavenet mel should contain more high frequency detail when compared to Tacotron mels.\n\tT2_output_range = (-hparams.max_abs_value, hparams.max_abs_value) if hparams.symmetric_mels else (0, hparams.max_abs_value)\n\tgenerated_mel = _interp(melspectrogram(y_hat, hparams).T, T2_output_range)\n\tutil.plot_spectrogram(generated_mel, mel_path, title='Local Condition vs Reconst. Mel-Spectrogram, step={}, loss={:.5f}'.format(\n\t\tglobal_step, loss), target_spectrogram=input_mel.T)\n\tutil.plot_spectrogram(upsampled_features.T, upsampled_path, title='Upsampled Local Condition features, step={}, loss={:.5f}'.format(\n\t\tglobal_step, loss), auto_aspect=True)\n\n\t#Save audio\n\tsave_wavenet_wav(y_hat, pred_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\tsave_wavenet_wav(y, target_wav_path, sr=hparams.sample_rate, inv_preemphasize=hparams.preemphasize, k=hparams.preemphasis)\n\ndef save_checkpoint(sess, saver, checkpoint_path, global_step):\n\tsaver.save(sess, checkpoint_path, global_step=global_step)\n\n\ndef model_train_mode(args, feeder, hparams, global_step, init=False):\n\twith tf.variable_scope('WaveNet_model', reuse=tf.AUTO_REUSE) as scope:\n\t\tmodel_name = None\n\t\tif args.model == 'Tacotron-2':\n\t\t\tmodel_name = 'WaveNet'\n\t\tmodel = create_model(model_name or args.model, hparams, init)\n\t\t#initialize model to train mode\n\t\tmodel.initialize(feeder.targets, feeder.local_condition_features, feeder.global_condition_features,\n\t\t\tfeeder.input_lengths, x=feeder.inputs)\n\t\tmodel.add_loss()\n\t\tmodel.add_optimizer(global_step)\n\t\tstats = add_train_stats(model, hparams)\n\t\treturn model, stats\n\ndef model_test_mode(args, feeder, hparams, global_step):\n\twith tf.variable_scope('WaveNet_model', reuse=tf.AUTO_REUSE) as scope:\n\t\tmodel_name = None\n\t\tif args.model == 'Tacotron-2':\n\t\t\tmodel_name = 'WaveNet'\n\t\tmodel = create_model(model_name or args.model, hparams)\n\t\t#initialize model to test mode\n\t\tmodel.initialize(feeder.eval_targets, feeder.eval_local_condition_features, feeder.eval_global_condition_features,\n\t\t\tfeeder.eval_input_lengths)\n\t\tmodel.add_loss()\n\t\treturn model\n\ndef train(log_dir, args, hparams, input_path):\n\tsave_dir = os.path.join(log_dir, 'wave_pretrained')\n\tplot_dir = os.path.join(log_dir, 'plots')\n\twav_dir = os.path.join(log_dir, 'wavs')\n\teval_dir = os.path.join(log_dir, 'eval-dir')\n\teval_plot_dir = os.path.join(eval_dir, 'plots')\n\teval_wav_dir = os.path.join(eval_dir, 'wavs')\n\ttensorboard_dir = os.path.join(log_dir, 'wavenet_events')\n\tmeta_folder = os.path.join(log_dir, 'metas')\n\tos.makedirs(save_dir, exist_ok=True)\n\tos.makedirs(plot_dir, exist_ok=True)\n\tos.makedirs(wav_dir, exist_ok=True)\n\tos.makedirs(eval_dir, exist_ok=True)\n\tos.makedirs(eval_plot_dir, exist_ok=True)\n\tos.makedirs(eval_wav_dir, exist_ok=True)\n\tos.makedirs(tensorboard_dir, exist_ok=True)\n\tos.makedirs(meta_folder, exist_ok=True)\n\n\tcheckpoint_path = os.path.join(save_dir, 'wavenet_model.ckpt')\n\tinput_path = os.path.join(args.base_dir, input_path)\n\n\tlog('Checkpoint_path: {}'.format(checkpoint_path))\n\tlog('Loading training data from: {}'.format(input_path))\n\tlog('Using model: {}'.format(args.model))\n\tlog(hparams_debug_string())\n\n\t#Start by setting a seed for repeatability\n\ttf.set_random_seed(hparams.wavenet_random_seed)\n\n\t#Set up data feeder\n\tcoord = tf.train.Coordinator()\n\twith tf.variable_scope('datafeeder') as scope:\n\t\tfeeder = Feeder(coord, input_path, args.base_dir, hparams)\n\n\t#Set up model\n\tglobal_step = tf.Variable(0, name='global_step', trainable=False)\n\tmodel, stats = model_train_mode(args, feeder, hparams, global_step)\n\teval_model = model_test_mode(args, feeder, hparams, global_step)\n\n\t#Speaker Embeddings metadata\n\tif hparams.speakers_path is not None:\n\t\tspeaker_embedding_meta = hparams.speakers_path\n\n\telse:\n\t\tspeaker_embedding_meta = os.path.join(meta_folder, 'SpeakerEmbeddings.tsv')\n\t\tif not os.path.isfile(speaker_embedding_meta):\n\t\t\twith open(speaker_embedding_meta, 'w', encoding='utf-8') as f:\n\t\t\t\tfor speaker in hparams.speakers:\n\t\t\t\t\tf.write('{}\\n'.format(speaker))\n\n\t\tspeaker_embedding_meta = speaker_embedding_meta.replace(log_dir, '..')\n\n\t#book keeping\n\tstep = 0\n\ttime_window = ValueWindow(100)\n\tloss_window = ValueWindow(100)\n\tsh_saver = create_shadow_saver(model, global_step)\n\n\tlog('Wavenet training set to a maximum of {} steps'.format(args.wavenet_train_steps))\n\n\t#Memory allocation on the memory\n\tconfig = tf.ConfigProto()\n\tconfig.gpu_options.allow_growth = True\n\tconfig.allow_soft_placement = True\n\trun_init = False\n\n\t#Train\n\twith tf.Session(config=config) as sess:\n\t\ttry:\n\t\t\tsummary_writer = tf.summary.FileWriter(tensorboard_dir, sess.graph)\n\t\t\tsess.run(tf.global_variables_initializer())\n\n\t\t\t#saved model restoring\n\t\t\tif args.restore:\n\t\t\t\t# Restore saved model if the user requested it, default = True\n\t\t\t\ttry:\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\n\n\t\t\t\t\tif (checkpoint_state and checkpoint_state.model_checkpoint_path):\n\t\t\t\t\t\tlog('Loading checkpoint {}'.format(checkpoint_state.model_checkpoint_path), slack=True)\n\t\t\t\t\t\tload_averaged_model(sess, sh_saver, checkpoint_state.model_checkpoint_path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tlog('No model to load at {}'.format(save_dir), slack=True)\n\t\t\t\t\t\tif hparams.wavenet_weight_normalization:\n\t\t\t\t\t\t\trun_init = True\n\n\t\t\t\texcept tf.errors.OutOfRangeError as e:\n\t\t\t\t\tlog('Cannot restore checkpoint: {}'.format(e), slack=True)\n\t\t\telse:\n\t\t\t\tlog('Starting new training!', slack=True)\n\t\t\t\tif hparams.wavenet_weight_normalization:\n\t\t\t\t\trun_init = True\n\n\t\t\tif run_init:\n\t\t\t\tlog('\\nApplying Weight normalization in fresh training. Applying data dependent initialization forward pass..')\n\t\t\t\t#Create init_model\n\t\t\t\tinit_model, _ = model_train_mode(args, feeder, hparams, global_step, init=True)\n\n\t\t\t#initializing feeder\n\t\t\tfeeder.start_threads(sess)\n\n\t\t\tif run_init:\n\t\t\t\t#Run one forward pass for model parameters initialization (make prediction on init_batch)\n\t\t\t\t_ = sess.run(init_model.tower_y_hat)\n\t\t\t\tlog('Data dependent initialization done. Starting training!')\n\t\t\t\n\t\t\t#Training loop\n\t\t\twhile not coord.should_stop() and step < args.wavenet_train_steps:\n\t\t\t\tstart_time = time.time()\n\t\t\t\tstep, loss, opt = sess.run([global_step, model.loss, model.optimize])\n\t\t\t\ttime_window.append(time.time() - start_time)\n\t\t\t\tloss_window.append(loss)\n\n\t\t\t\tmessage = 'Step {:7d} [{:.3f} sec/step, loss={:.5f}, avg_loss={:.5f}]'.format(\n\t\t\t\t\tstep, time_window.average, loss, loss_window.average)\n\t\t\t\tlog(message, end='\\r', slack=(step % args.checkpoint_interval == 0))\n\n\t\t\t\tif np.isnan(loss) or loss > 100:\n\t\t\t\t\tlog('Loss exploded to {:.5f} at step {}'.format(loss, step))\n\t\t\t\t\traise Exception('Loss exploded')\n\n\t\t\t\tif step % args.summary_interval == 0:\n\t\t\t\t\tlog('\\nWriting summary at step {}'.format(step))\n\t\t\t\t\tsummary_writer.add_summary(sess.run(stats), step)\n\n\t\t\t\tif step % args.checkpoint_interval == 0 or step == args.wavenet_train_steps:\n\t\t\t\t\tsave_log(sess, step, model, plot_dir, wav_dir, hparams=hparams, model_name=args.model)\n\t\t\t\t\tsave_checkpoint(sess, sh_saver, checkpoint_path, global_step)\n\n\t\t\t\tif step % args.eval_interval == 0:\n\t\t\t\t\tlog('\\nEvaluating at step {}'.format(step))\n\t\t\t\t\teval_step(sess, step, eval_model, eval_plot_dir, eval_wav_dir, summary_writer=summary_writer , hparams=model._hparams, model_name=args.model)\n\n\t\t\t\tif hparams.gin_channels > 0 and (step % args.embedding_interval == 0 or step == args.wavenet_train_steps or step == 1):\n\t\t\t\t\t#Get current checkpoint state\n\t\t\t\t\tcheckpoint_state = tf.train.get_checkpoint_state(save_dir)\n\n\t\t\t\t\t#Update Projector\n\t\t\t\t\tlog('\\nSaving Model Speaker Embeddings visualization..')\n\t\t\t\t\tadd_embedding_stats(summary_writer, [model.embedding_table.name], [speaker_embedding_meta], checkpoint_state.model_checkpoint_path)\n\t\t\t\t\tlog('WaveNet Speaker embeddings have been updated on tensorboard!')\n\n\t\t\tlog('Wavenet training complete after {} global steps'.format(args.wavenet_train_steps), slack=True)\n\t\t\treturn save_dir\n\n\t\texcept Exception as e:\n\t\t\tlog('Exiting due to exception: {}'.format(e), slack=True)\n\t\t\ttraceback.print_exc()\n\t\t\tcoord.request_stop(e)\n\n\ndef wavenet_train(args, log_dir, hparams, input_path):\n\treturn train(log_dir, args, hparams, input_path)\n"""
wavenet_vocoder/util.py,8,"b'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport librosa.display as dsp\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _assert_valid_input_type(s):\n\tassert s == \'mulaw-quantize\' or s == \'mulaw\' or s == \'raw\'\n\ndef is_mulaw_quantize(s):\n\t_assert_valid_input_type(s)\n\treturn s == \'mulaw-quantize\'\n\ndef is_mulaw(s):\n\t_assert_valid_input_type(s)\n\treturn s == \'mulaw\'\n\ndef is_raw(s):\n\t_assert_valid_input_type(s)\n\treturn s == \'raw\'\n\ndef is_scalar_input(s):\n\treturn is_raw(s) or is_mulaw(s)\n\n\n#From https://github.com/r9y9/nnmnkwii/blob/master/nnmnkwii/preprocessing/generic.py\ndef mulaw(x, mu=256):\n\t""""""Mu-Law companding\n\tMethod described in paper [1]_.\n\t.. math::\n\t\tf(x) = sign(x) ln (1 + mu |x|) / ln (1 + mu)\n\tArgs:\n\t\tx (array-like): Input signal. Each value of input signal must be in\n\t\t  range of [-1, 1].\n\t\tmu (number): Compression parameter ``\xce\xbc``.\n\tReturns:\n\t\tarray-like: Compressed signal ([-1, 1])\n\tSee also:\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n\t.. [1] Brokish, Charles W., and Michele Lewis. ""A-law and mu-law companding\n\t\timplementations using the tms320c54x."" SPRA163 (1997).\n\t""""""\n\tmu = 255\n\treturn _sign(x) * _log1p(mu * _abs(x)) / _log1p(mu)\n\n\ndef inv_mulaw(y, mu=256):\n\t""""""Inverse of mu-law companding (mu-law expansion)\n\t.. math::\n\t\tf^{-1}(x) = sign(y) (1 / mu) (1 + mu)^{|y|} - 1)\n\tArgs:\n\t\ty (array-like): Compressed signal. Each value of input signal must be in\n\t\t  range of [-1, 1].\n\t\tmu (number): Compression parameter ``\xce\xbc``.\n\tReturns:\n\t\tarray-like: Uncomprresed signal (-1 <= x <= 1)\n\tSee also:\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n\t""""""\n\tmu = 255\n\treturn _sign(y) * (1.0 / mu) * ((1.0 + mu)**_abs(y) - 1.0)\n\n\ndef mulaw_quantize(x, mu=256):\n\t""""""Mu-Law companding + quantize\n\tArgs:\n\t\tx (array-like): Input signal. Each value of input signal must be in\n\t\t  range of [-1, 1].\n\t\tmu (number): Compression parameter ``\xce\xbc``.\n\tReturns:\n\t\tarray-like: Quantized signal (dtype=int)\n\t\t  - y \xe2\x88\x88 [0, mu] if x \xe2\x88\x88 [-1, 1]\n\t\t  - y \xe2\x88\x88 [0, mu) if x \xe2\x88\x88 [-1, 1)\n\t.. note::\n\t\tIf you want to get quantized values of range [0, mu) (not [0, mu]),\n\t\tthen you need to provide input signal of range [-1, 1).\n\tExamples:\n\t\t>>> from scipy.io import wavfile\n\t\t>>> import pysptk\n\t\t>>> import numpy as np\n\t\t>>> from nnmnkwii import preprocessing as P\n\t\t>>> fs, x = wavfile.read(pysptk.util.example_audio_file())\n\t\t>>> x = (x / 32768.0).astype(np.float32)\n\t\t>>> y = P.mulaw_quantize(x)\n\t\t>>> print(y.min(), y.max(), y.dtype)\n\t\t15 246 int64\n\tSee also:\n\t\t:func:`nnmnkwii.preprocessing.mulaw`\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw_quantize`\n\t""""""\n\tmu = 255\n\ty = mulaw(x, mu)\n\t# scale [-1, 1] to [0, mu]\n\treturn _asint((y + 1) / 2 * mu)\n\n\ndef inv_mulaw_quantize(y, mu=256):\n\t""""""Inverse of mu-law companding + quantize\n\tArgs:\n\t\ty (array-like): Quantized signal (\xe2\x88\x88 [0, mu]).\n\t\tmu (number): Compression parameter ``\xce\xbc``.\n\tReturns:\n\t\tarray-like: Uncompressed signal ([-1, 1])\n\tExamples:\n\t\t>>> from scipy.io import wavfile\n\t\t>>> import pysptk\n\t\t>>> import numpy as np\n\t\t>>> from nnmnkwii import preprocessing as P\n\t\t>>> fs, x = wavfile.read(pysptk.util.example_audio_file())\n\t\t>>> x = (x / 32768.0).astype(np.float32)\n\t\t>>> x_hat = P.inv_mulaw_quantize(P.mulaw_quantize(x))\n\t\t>>> x_hat = (x_hat * 32768).astype(np.int16)\n\tSee also:\n\t\t:func:`nnmnkwii.preprocessing.mulaw`\n\t\t:func:`nnmnkwii.preprocessing.inv_mulaw`\n\t\t:func:`nnmnkwii.preprocessing.mulaw_quantize`\n\t""""""\n\t# [0, m) to [-1, 1]\n\tmu = 255\n\ty = 2 * _asfloat(y) / mu - 1\n\treturn inv_mulaw(y, mu)\n\ndef _sign(x):\n\t#wrapper to support tensorflow tensors/numpy arrays\n\tisnumpy = isinstance(x, np.ndarray)\n\tisscalar = np.isscalar(x)\n\treturn np.sign(x) if (isnumpy or isscalar) else tf.sign(x)\n\n\ndef _log1p(x):\n\t#wrapper to support tensorflow tensors/numpy arrays\n\tisnumpy = isinstance(x, np.ndarray)\n\tisscalar = np.isscalar(x)\n\treturn np.log1p(x) if (isnumpy or isscalar) else tf.log1p(x)\n\n\ndef _abs(x):\n\t#wrapper to support tensorflow tensors/numpy arrays\n\tisnumpy = isinstance(x, np.ndarray)\n\tisscalar = np.isscalar(x)\n\treturn np.abs(x) if (isnumpy or isscalar) else tf.abs(x)\n\n\ndef _asint(x):\n\t#wrapper to support tensorflow tensors/numpy arrays\n\tisnumpy = isinstance(x, np.ndarray)\n\tisscalar = np.isscalar(x)\n\treturn x.astype(np.int) if isnumpy else int(x) if isscalar else tf.cast(x, tf.int32)\n\n\ndef _asfloat(x):\n\t#wrapper to support tensorflow tensors/numpy arrays\n\tisnumpy = isinstance(x, np.ndarray)\n\tisscalar = np.isscalar(x)\n\treturn x.astype(np.float32) if isnumpy else float(x) if isscalar else tf.cast(x, tf.float32)\n\ndef sequence_mask(input_lengths, max_len=None, expand=True):\n\tif max_len is None:\n\t\tmax_len = tf.reduce_max(input_lengths)\n\n\tif expand:\n\t\treturn tf.expand_dims(tf.sequence_mask(input_lengths, max_len, dtype=tf.float32), axis=-1)\n\treturn tf.sequence_mask(input_lengths, max_len, dtype=tf.float32)\n\n\ndef waveplot(path, y_hat, y_target, hparams, title=None):\n\tsr = hparams.sample_rate\n\n\tfig = plt.figure(figsize=(12, 4))\n\tif y_target is not None:\n\t\tax = plt.subplot(3, 1, 1)\n\t\tdsp.waveplot(y_target, sr=sr)\n\t\tax.set_title(\'Target waveform\')\n\t\tax = plt.subplot(3, 1, 2)\n\t\tdsp.waveplot(y_hat, sr=sr)\n\t\tax.set_title(\'Predicted waveform\')\n\telse:\n\t\tax = plt.subplot(2, 1, 1)\n\t\tdsp.waveplot(y_hat, sr=sr)\n\t\tax.set_title(\'Generated waveform\')\n\n\tif title is not None:\n\t\t# Set common labels\n\t\tfig.text(0.5, 0.18, title, horizontalalignment=\'center\', fontsize=16)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=""png"")\n\tplt.close()\n\ndef plot_spectrogram(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False):\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=\'center\', fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=\'auto\', interpolation=\'none\')\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=\'none\')\n\t\tax1.set_title(\'Target Mel-Spectrogram\')\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=\'horizontal\', ax=ax1)\n\t\tax2.set_title(\'Predicted Mel-Spectrogram\')\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=\'auto\', interpolation=\'none\')\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=\'none\')\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=\'horizontal\', ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=\'png\')\n\tplt.close()\n'"
tacotron/models/Architecture_wrappers.py,4,"b'""""""A set of wrappers usefull for tacotron 2 architecture\nAll notations and variable names were used in concordance with originial tensorflow implementation\n""""""\nimport collections\n\nimport numpy as np\nimport tensorflow as tf\nfrom tacotron.models.attention import _compute_attention\nfrom tensorflow.contrib.rnn import RNNCell\nfrom tensorflow.python.framework import ops, tensor_shape\nfrom tensorflow.python.ops import array_ops, check_ops, rnn_cell_impl, tensor_array_ops\nfrom tensorflow.python.util import nest\n\n_zero_state_tensors = rnn_cell_impl._zero_state_tensors\n\n\n\nclass TacotronEncoderCell(RNNCell):\n\t""""""Tacotron 2 Encoder Cell\n\tPasses inputs through a stack of convolutional layers then through a bidirectional LSTM\n\tlayer to predict the hidden representation vector (or memory)\n\t""""""\n\n\tdef __init__(self, convolutional_layers, lstm_layer):\n\t\t""""""Initialize encoder parameters\n\n\t\tArgs:\n\t\t\tconvolutional_layers: Encoder convolutional block class\n\t\t\tlstm_layer: encoder bidirectional lstm layer class\n\t\t""""""\n\t\tsuper(TacotronEncoderCell, self).__init__()\n\t\t#Initialize encoder layers\n\t\tself._convolutions = convolutional_layers\n\t\tself._cell = lstm_layer\n\n\tdef __call__(self, inputs, input_lengths=None):\n\t\t#Pass input sequence through a stack of convolutional layers\n\t\tconv_output = self._convolutions(inputs)\n\n\t\t#Extract hidden representation from encoder lstm cells\n\t\thidden_representation = self._cell(conv_output, input_lengths)\n\n\t\t#For shape visualization\n\t\tself.conv_output_shape = conv_output.shape\n\t\treturn hidden_representation\n\n\nclass TacotronDecoderCellState(\n\tcollections.namedtuple(""TacotronDecoderCellState"",\n\t (""cell_state"", ""attention"", ""time"", ""alignments"",\n\t  ""alignment_history"", ""max_attentions""))):\n\t""""""`namedtuple` storing the state of a `TacotronDecoderCell`.\n\tContains:\n\t  - `cell_state`: The state of the wrapped `RNNCell` at the previous time\n\t\tstep.\n\t  - `attention`: The attention emitted at the previous time step.\n\t  - `time`: int32 scalar containing the current time step.\n\t  - `alignments`: A single or tuple of `Tensor`(s) containing the alignments\n\t\t emitted at the previous time step for each attention mechanism.\n\t  - `alignment_history`: a single or tuple of `TensorArray`(s)\n\t\t containing alignment matrices from all time steps for each attention\n\t\t mechanism. Call `stack()` on each to convert to a `Tensor`.\n\t""""""\n\tdef replace(self, **kwargs):\n\t\t""""""Clones the current state while overwriting components provided by kwargs.\n\t\t""""""\n\t\treturn super(TacotronDecoderCellState, self)._replace(**kwargs)\n\nclass TacotronDecoderCell(RNNCell):\n\t""""""Tactron 2 Decoder Cell\n\tDecodes encoder output and previous mel frames into next r frames\n\n\tDecoder Step i:\n\t\t1) Prenet to compress last output information\n\t\t2) Concat compressed inputs with previous context vector (input feeding) *\n\t\t3) Decoder RNN (actual decoding) to predict current state s_{i} *\n\t\t4) Compute new context vector c_{i} based on s_{i} and a cumulative sum of previous alignments *\n\t\t5) Predict new output y_{i} using s_{i} and c_{i} (concatenated)\n\t\t6) Predict <stop_token> output ys_{i} using s_{i} and c_{i} (concatenated)\n\n\t* : This is typically taking a vanilla LSTM, wrapping it using tensorflow\'s attention wrapper,\n\tand wrap that with the prenet before doing an input feeding, and with the prediction layer\n\tthat uses RNN states to project on output space. Actions marked with (*) can be replaced with\n\ttensorflow\'s attention wrapper call if it was using cumulative alignments instead of previous alignments only.\n\t""""""\n\n\tdef __init__(self, prenet, attention_mechanism, rnn_cell, frame_projection, stop_projection):\n\t\t""""""Initialize decoder parameters\n\n\t\tArgs:\n\t\t    prenet: A tensorflow fully connected layer acting as the decoder pre-net\n\t\t    attention_mechanism: A _BaseAttentionMechanism instance, usefull to\n\t\t\t    learn encoder-decoder alignments\n\t\t    rnn_cell: Instance of RNNCell, main body of the decoder\n\t\t    frame_projection: tensorflow fully connected layer with r * num_mels output units\n\t\t    stop_projection: tensorflow fully connected layer, expected to project to a scalar\n\t\t\t    and through a sigmoid activation\n\t\t\tmask_finished: Boolean, Whether to mask decoder frames after the <stop_token>\n\t\t""""""\n\t\tsuper(TacotronDecoderCell, self).__init__()\n\t\t#Initialize decoder layers\n\t\tself._prenet = prenet\n\t\tself._attention_mechanism = attention_mechanism\n\t\tself._cell = rnn_cell\n\t\tself._frame_projection = frame_projection\n\t\tself._stop_projection = stop_projection\n\n\t\tself._attention_layer_size = self._attention_mechanism.values.get_shape()[-1].value\n\n\tdef _batch_size_checks(self, batch_size, error_message):\n\t\treturn [check_ops.assert_equal(batch_size,\n\t\t  self._attention_mechanism.batch_size,\n\t\t  message=error_message)]\n\n\t@property\n\tdef output_size(self):\n\t\treturn self._frame_projection.shape\n\n\t@property\n\tdef state_size(self):\n\t\t""""""The `state_size` property of `TacotronDecoderCell`.\n\n\t\tReturns:\n\t\t  An `TacotronDecoderCell` tuple containing shapes used by this object.\n\t\t""""""\n\t\treturn TacotronDecoderCellState(\n\t\t\tcell_state=self._cell._cell.state_size,\n\t\t\ttime=tensor_shape.TensorShape([]),\n\t\t\tattention=self._attention_layer_size,\n\t\t\talignments=self._attention_mechanism.alignments_size,\n\t\t\talignment_history=(),\n\t\t\tmax_attentions=())\n\n\tdef zero_state(self, batch_size, dtype):\n\t\t""""""Return an initial (zero) state tuple for this `AttentionWrapper`.\n\n\t\tArgs:\n\t\t  batch_size: `0D` integer tensor: the batch size.\n\t\t  dtype: The internal state data type.\n\t\tReturns:\n\t\t  An `TacotronDecoderCellState` tuple containing zeroed out tensors and,\n\t\t  possibly, empty `TensorArray` objects.\n\t\tRaises:\n\t\t  ValueError: (or, possibly at runtime, InvalidArgument), if\n\t\t\t`batch_size` does not match the output size of the encoder passed\n\t\t\tto the wrapper object at initialization time.\n\t\t""""""\n\t\twith ops.name_scope(type(self).__name__ + ""ZeroState"", values=[batch_size]):\n\t\t\tcell_state = self._cell._cell.zero_state(batch_size, dtype)\n\t\t\terror_message = (\n\t\t\t\t""When calling zero_state of TacotronDecoderCell %s: "" % self._base_name +\n\t\t\t\t""Non-matching batch sizes between the memory ""\n\t\t\t\t""(encoder output) and the requested batch size."")\n\t\t\twith ops.control_dependencies(\n\t\t\t\tself._batch_size_checks(batch_size, error_message)):\n\t\t\t\tcell_state = nest.map_structure(\n\t\t\t\t\tlambda s: array_ops.identity(s, name=""checked_cell_state""),\n\t\t\t\t\tcell_state)\n\t\t\treturn TacotronDecoderCellState(\n\t\t\t\tcell_state=cell_state,\n\t\t\t\ttime=array_ops.zeros([], dtype=tf.int32),\n\t\t\t\tattention=_zero_state_tensors(self._attention_layer_size, batch_size,\n\t\t\t\t  dtype),\n\t\t\t\talignments=self._attention_mechanism.initial_alignments(batch_size, dtype),\n\t\t\t\talignment_history=tensor_array_ops.TensorArray(dtype=dtype, size=0,\n\t\t\t\tdynamic_size=True),\n\t\t\t\tmax_attentions=tf.zeros((batch_size, ), dtype=tf.int32))\n\n\tdef __call__(self, inputs, state):\n\t\t#Information bottleneck (essential for learning attention)\n\t\tprenet_output = self._prenet(inputs)\n\n\t\t#Concat context vector and prenet output to form LSTM cells input (input feeding)\n\t\tLSTM_input = tf.concat([prenet_output, state.attention], axis=-1)\n\n\t\t#Unidirectional LSTM layers\n\t\tLSTM_output, next_cell_state = self._cell(LSTM_input, state.cell_state)\n\n\n\t\t#Compute the attention (context) vector and alignments using\n\t\t#the new decoder cell hidden state as query vector\n\t\t#and cumulative alignments to extract location features\n\t\t#The choice of the new cell hidden state (s_{i}) of the last\n\t\t#decoder RNN Cell is based on Luong et Al. (2015):\n\t\t#https://arxiv.org/pdf/1508.04025.pdf\n\t\tprevious_alignments = state.alignments\n\t\tprevious_alignment_history = state.alignment_history\n\t\tcontext_vector, alignments, cumulated_alignments, max_attentions = _compute_attention(self._attention_mechanism,\n\t\t\tLSTM_output,\n\t\t\tprevious_alignments,\n\t\t\tattention_layer=None,\n\t\t\tprev_max_attentions=state.max_attentions)\n\n\t\t#Concat LSTM outputs and context vector to form projections inputs\n\t\tprojections_input = tf.concat([LSTM_output, context_vector], axis=-1)\n\n\t\t#Compute predicted frames and predicted <stop_token>\n\t\tcell_outputs = self._frame_projection(projections_input)\n\t\tstop_tokens = self._stop_projection(projections_input)\n\n\t\t#Save alignment history\n\t\talignment_history = previous_alignment_history.write(state.time, alignments)\n\n\t\t#Prepare next decoder state\n\t\tnext_state = TacotronDecoderCellState(\n\t\t\ttime=state.time + 1,\n\t\t\tcell_state=next_cell_state,\n\t\t\tattention=context_vector,\n\t\t\talignments=cumulated_alignments,\n\t\t\talignment_history=alignment_history,\n\t\t\tmax_attentions=max_attentions)\n\n\t\treturn (cell_outputs, stop_tokens), next_state\n'"
tacotron/models/__init__.py,0,"b""from .tacotron import Tacotron\n\n\ndef create_model(name, hparams):\n  if name == 'Tacotron':\n    return Tacotron(hparams)\n  else:\n    raise Exception('Unknown model: ' + name)\n"""
tacotron/models/attention.py,23,"b'""""""Attention file for location based attention (compatible with tensorflow attention wrapper)""""""\n\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq.python.ops.attention_wrapper import BahdanauAttention\nfrom tensorflow.python.layers import core as layers_core\nfrom tensorflow.python.ops import array_ops, math_ops, nn_ops, variable_scope\n\n\n#From https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\ndef _compute_attention(attention_mechanism, cell_output, attention_state,\n\t\t\t\t\t   attention_layer, prev_max_attentions):\n\t""""""Computes the attention and alignments for a given attention_mechanism.""""""\n\talignments, next_attention_state, max_attentions = attention_mechanism(\n\t\tcell_output, state=attention_state, prev_max_attentions=prev_max_attentions)\n\n\t# Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\n\texpanded_alignments = array_ops.expand_dims(alignments, 1)\n\t# Context is the inner product of alignments and values along the\n\t# memory time dimension.\n\t# alignments shape is\n\t#   [batch_size, 1, memory_time]\n\t# attention_mechanism.values shape is\n\t#   [batch_size, memory_time, memory_size]\n\t# the batched matmul is over memory_time, so the output shape is\n\t#   [batch_size, 1, memory_size].\n\t# we then squeeze out the singleton dim.\n\tcontext = math_ops.matmul(expanded_alignments, attention_mechanism.values)\n\tcontext = array_ops.squeeze(context, [1])\n\n\tif attention_layer is not None:\n\t\tattention = attention_layer(array_ops.concat([cell_output, context], 1))\n\telse:\n\t\tattention = context\n\n\treturn attention, alignments, next_attention_state, max_attentions\n\n\ndef _location_sensitive_score(W_query, W_fil, W_keys):\n\t""""""Impelements Bahdanau-style (cumulative) scoring function.\n\tThis attention is described in:\n\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t  vances in Neural Information Processing Systems, 2015, pp.\n\t  577\xe2\x80\x93585.\n\n\t#############################################################################\n\t\t\t  hybrid attention (content-based + location-based)\n\t\t\t\t\t\t\t   f = F * \xce\xb1_{i-1}\n\t   energy = dot(v_a, tanh(W_keys(h_enc) + W_query(h_dec) + W_fil(f) + b_a))\n\t#############################################################################\n\n\tArgs:\n\t\tW_query: Tensor, shape \'[batch_size, 1, attention_dim]\' to compare to location features.\n\t\tW_location: processed previous alignments into location features, shape \'[batch_size, max_time, attention_dim]\'\n\t\tW_keys: Tensor, shape \'[batch_size, max_time, attention_dim]\', typically the encoder outputs.\n\tReturns:\n\t\tA \'[batch_size, max_time]\' attention score (energy)\n\t""""""\n\t# Get the number of hidden units from the trailing dimension of keys\n\tdtype = W_query.dtype\n\tnum_units = W_keys.shape[-1].value or array_ops.shape(W_keys)[-1]\n\n\tv_a = tf.get_variable(\n\t\t\'attention_variable_projection\', shape=[num_units], dtype=dtype,\n\t\tinitializer=tf.contrib.layers.xavier_initializer())\n\tb_a = tf.get_variable(\n\t\t\'attention_bias\', shape=[num_units], dtype=dtype,\n\t\tinitializer=tf.zeros_initializer())\n\n\treturn tf.reduce_sum(v_a * tf.tanh(W_keys + W_query + W_fil + b_a), [2])\n\ndef _smoothing_normalization(e):\n\t""""""Applies a smoothing normalization function instead of softmax\n\tIntroduced in:\n\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t  vances in Neural Information Processing Systems, 2015, pp.\n\t  577\xe2\x80\x93585.\n\n\t############################################################################\n\t\t\t\t\t\tSmoothing normalization function\n\t\t\t\ta_{i, j} = sigmoid(e_{i, j}) / sum_j(sigmoid(e_{i, j}))\n\t############################################################################\n\n\tArgs:\n\t\te: matrix [batch_size, max_time(memory_time)]: expected to be energy (score)\n\t\t\tvalues of an attention mechanism\n\tReturns:\n\t\tmatrix [batch_size, max_time]: [0, 1] normalized alignments with possible\n\t\t\tattendance to multiple memory time steps.\n\t""""""\n\treturn tf.nn.sigmoid(e) / tf.reduce_sum(tf.nn.sigmoid(e), axis=-1, keepdims=True)\n\n\nclass LocationSensitiveAttention(BahdanauAttention):\n\t""""""Impelements Bahdanau-style (cumulative) scoring function.\n\tUsually referred to as ""hybrid"" attention (content-based + location-based)\n\tExtends the additive attention described in:\n\t""D. Bahdanau, K. Cho, and Y. Bengio, \xe2\x80\x9cNeural machine transla-\n  tion by jointly learning to align and translate,\xe2\x80\x9d in Proceedings\n  of ICLR, 2015.""\n\tto use previous alignments as additional location features.\n\n\tThis attention is described in:\n\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n  vances in Neural Information Processing Systems, 2015, pp.\n  577\xe2\x80\x93585.\n\t""""""\n\n\tdef __init__(self,\n\t\t\t\t num_units,\n\t\t\t\t memory,\n\t\t\t\t hparams,\n\t\t\t\t is_training,\n\t\t\t\t mask_encoder=True,\n\t\t\t\t memory_sequence_length=None,\n\t\t\t\t smoothing=False,\n\t\t\t\t cumulate_weights=True,\n\t\t\t\t name=\'LocationSensitiveAttention\'):\n\t\t""""""Construct the Attention mechanism.\n\t\tArgs:\n\t\t\tnum_units: The depth of the query mechanism.\n\t\t\tmemory: The memory to query; usually the output of an RNN encoder.  This\n\t\t\t\ttensor should be shaped `[batch_size, max_time, ...]`.\n\t\t\tmask_encoder (optional): Boolean, whether to mask encoder paddings.\n\t\t\tmemory_sequence_length (optional): Sequence lengths for the batch entries\n\t\t\t\tin memory.  If provided, the memory tensor rows are masked with zeros\n\t\t\t\tfor values past the respective sequence lengths. Only relevant if mask_encoder = True.\n\t\t\tsmoothing (optional): Boolean. Determines which normalization function to use.\n\t\t\t\tDefault normalization function (probablity_fn) is softmax. If smoothing is\n\t\t\t\tenabled, we replace softmax with:\n\t\t\t\t\t\ta_{i, j} = sigmoid(e_{i, j}) / sum_j(sigmoid(e_{i, j}))\n\t\t\t\tIntroduced in:\n\t\t\t\t\tJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\n\t\t\t\t  gio, \xe2\x80\x9cAttention-based models for speech recognition,\xe2\x80\x9d in Ad-\n\t\t\t\t  vances in Neural Information Processing Systems, 2015, pp.\n\t\t\t\t  577\xe2\x80\x93585.\n\t\t\t\tThis is mainly used if the model wants to attend to multiple input parts\n\t\t\t\tat the same decoding step. We probably won\'t be using it since multiple sound\n\t\t\t\tframes may depend on the same character/phone, probably not the way around.\n\t\t\t\tNote:\n\t\t\t\t\tWe still keep it implemented in case we want to test it. They used it in the\n\t\t\t\t\tpaper in the context of speech recognition, where one phoneme may depend on\n\t\t\t\t\tmultiple subsequent sound frames.\n\t\t\tname: Name to use when creating ops.\n\t\t""""""\n\t\t#Create normalization function\n\t\t#Setting it to None defaults in using softmax\n\t\tnormalization_function = _smoothing_normalization if (smoothing == True) else None\n\t\tmemory_length = memory_sequence_length if (mask_encoder==True) else None\n\t\tsuper(LocationSensitiveAttention, self).__init__(\n\t\t\t\tnum_units=num_units,\n\t\t\t\tmemory=memory,\n\t\t\t\tmemory_sequence_length=memory_length,\n\t\t\t\tprobability_fn=normalization_function,\n\t\t\t\tname=name)\n\n\t\tself.location_convolution = tf.layers.Conv1D(filters=hparams.attention_filters,\n\t\t\tkernel_size=hparams.attention_kernel, padding=\'same\', use_bias=True,\n\t\t\tbias_initializer=tf.zeros_initializer(), name=\'location_features_convolution\')\n\t\tself.location_layer = tf.layers.Dense(units=num_units, use_bias=False,\n\t\t\tdtype=tf.float32, name=\'location_features_layer\')\n\t\tself._cumulate = cumulate_weights\n\t\tself.synthesis_constraint = hparams.synthesis_constraint and not is_training\n\t\tself.attention_win_size = tf.convert_to_tensor(hparams.attention_win_size, dtype=tf.int32)\n\t\tself.constraint_type = hparams.synthesis_constraint_type\n\n\tdef __call__(self, query, state, prev_max_attentions):\n\t\t""""""Score the query based on the keys and values.\n\t\tArgs:\n\t\t\tquery: Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, query_depth]`.\n\t\t\tstate (previous alignments): Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, alignments_size]`\n\t\t\t\t(`alignments_size` is memory\'s `max_time`).\n\t\tReturns:\n\t\t\talignments: Tensor of dtype matching `self.values` and shape\n\t\t\t\t`[batch_size, alignments_size]` (`alignments_size` is memory\'s\n\t\t\t\t`max_time`).\n\t\t""""""\n\t\tprevious_alignments = state\n\t\twith variable_scope.variable_scope(None, ""Location_Sensitive_Attention"", [query]):\n\n\t\t\t# processed_query shape [batch_size, query_depth] -> [batch_size, attention_dim]\n\t\t\tprocessed_query = self.query_layer(query) if self.query_layer else query\n\t\t\t# -> [batch_size, 1, attention_dim]\n\t\t\tprocessed_query = tf.expand_dims(processed_query, 1)\n\n\t\t\t# processed_location_features shape [batch_size, max_time, attention dimension]\n\t\t\t# [batch_size, max_time] -> [batch_size, max_time, 1]\n\t\t\texpanded_alignments = tf.expand_dims(previous_alignments, axis=2)\n\t\t\t# location features [batch_size, max_time, filters]\n\t\t\tf = self.location_convolution(expanded_alignments)\n\t\t\t# Projected location features [batch_size, max_time, attention_dim]\n\t\t\tprocessed_location_features = self.location_layer(f)\n\n\t\t\t# energy shape [batch_size, max_time]\n\t\t\tenergy = _location_sensitive_score(processed_query, processed_location_features, self.keys)\n\n\t\tif self.synthesis_constraint:\n\t\t\tTx = tf.shape(energy)[-1]\n\t\t\t# prev_max_attentions = tf.squeeze(prev_max_attentions, [-1])\n\t\t\tif self.constraint_type == \'monotonic\':\n\t\t\t\tkey_masks = tf.sequence_mask(prev_max_attentions, Tx)\n\t\t\t\treverse_masks = tf.sequence_mask(Tx - self.attention_win_size - prev_max_attentions, Tx)[:, ::-1]\n\t\t\telse:\n\t\t\t\tassert self.constraint_type == \'window\'\n\t\t\t\tkey_masks = tf.sequence_mask(prev_max_attentions - (self.attention_win_size // 2 + (self.attention_win_size % 2 != 0)), Tx)\n\t\t\t\treverse_masks = tf.sequence_mask(Tx - (self.attention_win_size // 2) - prev_max_attentions, Tx)[:, ::-1]\n\t\t\t\n\t\t\tmasks = tf.logical_or(key_masks, reverse_masks)\n\t\t\tpaddings = tf.ones_like(energy) * (-2 ** 32 + 1)  # (N, Ty/r, Tx)\n\t\t\tenergy = tf.where(tf.equal(masks, False), energy, paddings)\n\n\t\t# alignments shape = energy shape = [batch_size, max_time]\n\t\talignments = self._probability_fn(energy, previous_alignments)\n\t\tmax_attentions = tf.argmax(alignments, -1, output_type=tf.int32) # (N, Ty/r)\n\n\t\t# Cumulate alignments\n\t\tif self._cumulate:\n\t\t\tnext_state = alignments + previous_alignments\n\t\telse:\n\t\t\tnext_state = alignments\n\n\t\treturn alignments, next_state, max_attentions\n'"
tacotron/models/custom_decoder.py,3,"b'from __future__ import absolute_import, division, print_function\n\nimport collections\n\nimport tensorflow as tf\nfrom tacotron.models.helpers import TacoTestHelper, TacoTrainingHelper\nfrom tensorflow.contrib.seq2seq.python.ops import decoder\nfrom tensorflow.contrib.seq2seq.python.ops import helper as helper_py\nfrom tensorflow.python.framework import ops, tensor_shape\nfrom tensorflow.python.layers import base as layers_base\nfrom tensorflow.python.ops import rnn_cell_impl\nfrom tensorflow.python.util import nest\n\n\nclass CustomDecoderOutput(\n\t\tcollections.namedtuple(""CustomDecoderOutput"", (""rnn_output"", ""token_output"", ""sample_id""))):\n\tpass\n\n\nclass CustomDecoder(decoder.Decoder):\n\t""""""Custom sampling decoder.\n\n\tAllows for stop token prediction at inference time\n\tand returns equivalent loss in training time.\n\n\tNote:\n\tOnly use this decoder with Tacotron 2 as it only accepts tacotron custom helpers\n\t""""""\n\n\tdef __init__(self, cell, helper, initial_state, output_layer=None):\n\t\t""""""Initialize CustomDecoder.\n\t\tArgs:\n\t\t\tcell: An `RNNCell` instance.\n\t\t\thelper: A `Helper` instance.\n\t\t\tinitial_state: A (possibly nested tuple of...) tensors and TensorArrays.\n\t\t\t\tThe initial state of the RNNCell.\n\t\t\toutput_layer: (Optional) An instance of `tf.layers.Layer`, i.e.,\n\t\t\t\t`tf.layers.Dense`. Optional layer to apply to the RNN output prior\n\t\t\t\tto storing the result or sampling.\n\t\tRaises:\n\t\t\tTypeError: if `cell`, `helper` or `output_layer` have an incorrect type.\n\t\t""""""\n\t\trnn_cell_impl.assert_like_rnncell(type(cell), cell)\n\t\tif not isinstance(helper, helper_py.Helper):\n\t\t\traise TypeError(""helper must be a Helper, received: %s"" % type(helper))\n\t\tif (output_layer is not None\n\t\t\t\tand not isinstance(output_layer, layers_base.Layer)):\n\t\t\traise TypeError(\n\t\t\t\t\t""output_layer must be a Layer, received: %s"" % type(output_layer))\n\t\tself._cell = cell\n\t\tself._helper = helper\n\t\tself._initial_state = initial_state\n\t\tself._output_layer = output_layer\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._helper.batch_size\n\n\tdef _rnn_output_size(self):\n\t\tsize = self._cell.output_size\n\t\tif self._output_layer is None:\n\t\t\treturn size\n\t\telse:\n\t\t\t# To use layer\'s compute_output_shape, we need to convert the\n\t\t\t# RNNCell\'s output_size entries into shapes with an unknown\n\t\t\t# batch size.  We then pass this through the layer\'s\n\t\t\t# compute_output_shape and read off all but the first (batch)\n\t\t\t# dimensions to get the output size of the rnn with the layer\n\t\t\t# applied to the top.\n\t\t\toutput_shape_with_unknown_batch = nest.map_structure(\n\t\t\t\t\tlambda s: tensor_shape.TensorShape([None]).concatenate(s),\n\t\t\t\t\tsize)\n\t\t\tlayer_output_shape = self._output_layer._compute_output_shape(  # pylint: disable=protected-access\n\t\t\t\t\toutput_shape_with_unknown_batch)\n\t\t\treturn nest.map_structure(lambda s: s[1:], layer_output_shape)\n\n\t@property\n\tdef output_size(self):\n\t\t# Return the cell output and the id\n\t\treturn CustomDecoderOutput(\n\t\t\t\trnn_output=self._rnn_output_size(),\n\t\t\t\ttoken_output=self._helper.token_output_size,\n\t\t\t\tsample_id=self._helper.sample_ids_shape)\n\n\t@property\n\tdef output_dtype(self):\n\t\t# Assume the dtype of the cell is the output_size structure\n\t\t# containing the input_state\'s first component\'s dtype.\n\t\t# Return that structure and the sample_ids_dtype from the helper.\n\t\tdtype = nest.flatten(self._initial_state)[0].dtype\n\t\treturn CustomDecoderOutput(\n\t\t\t\tnest.map_structure(lambda _: dtype, self._rnn_output_size()),\n\t\t\t\ttf.float32,\n\t\t\t\tself._helper.sample_ids_dtype)\n\n\tdef initialize(self, name=None):\n\t\t""""""Initialize the decoder.\n\t\tArgs:\n\t\t\tname: Name scope for any created operations.\n\t\tReturns:\n\t\t\t`(finished, first_inputs, initial_state)`.\n\t\t""""""\n\t\treturn self._helper.initialize() + (self._initial_state,)\n\n\tdef step(self, time, inputs, state, name=None):\n\t\t""""""Perform a custom decoding step.\n\t\tEnables for dyanmic <stop_token> prediction\n\t\tArgs:\n\t\t\ttime: scalar `int32` tensor.\n\t\t\tinputs: A (structure of) input tensors.\n\t\t\tstate: A (structure of) state tensors and TensorArrays.\n\t\t\tname: Name scope for any created operations.\n\t\tReturns:\n\t\t\t`(outputs, next_state, next_inputs, finished)`.\n\t\t""""""\n\t\twith ops.name_scope(name, ""CustomDecoderStep"", (time, inputs, state)):\n\t\t\t#Call outputprojection wrapper cell\n\t\t\t(cell_outputs, stop_token), cell_state = self._cell(inputs, state)\n\n\t\t\t#apply output_layer (if existant)\n\t\t\tif self._output_layer is not None:\n\t\t\t\tcell_outputs = self._output_layer(cell_outputs)\n\t\t\tsample_ids = self._helper.sample(\n\t\t\t\t\ttime=time, outputs=cell_outputs, state=cell_state)\n\n\t\t\t(finished, next_inputs, next_state) = self._helper.next_inputs(\n\t\t\t\t\ttime=time,\n\t\t\t\t\toutputs=cell_outputs,\n\t\t\t\t\tstate=cell_state,\n\t\t\t\t\tsample_ids=sample_ids,\n\t\t\t\t\tstop_token_prediction=stop_token)\n\n\t\toutputs = CustomDecoderOutput(cell_outputs, stop_token, sample_ids)\n\t\treturn (outputs, next_state, next_inputs, finished)\n'"
tacotron/models/helpers.py,24,"b""import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.seq2seq import Helper\n\n\nclass TacoTestHelper(Helper):\n\tdef __init__(self, batch_size, hparams):\n\t\twith tf.name_scope('TacoTestHelper'):\n\t\t\tself._batch_size = batch_size\n\t\t\tself._output_dim = hparams.num_mels\n\t\t\tself._reduction_factor = hparams.outputs_per_step\n\t\t\tself.stop_at_any = hparams.stop_at_any\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._batch_size\n\n\t@property\n\tdef token_output_size(self):\n\t\treturn self._reduction_factor\n\n\t@property\n\tdef sample_ids_shape(self):\n\t\treturn tf.TensorShape([])\n\n\t@property\n\tdef sample_ids_dtype(self):\n\t\treturn np.int32\n\n\tdef initialize(self, name=None):\n\t\treturn (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n\tdef sample(self, time, outputs, state, name=None):\n\t\treturn tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n\tdef next_inputs(self, time, outputs, state, sample_ids, stop_token_prediction, name=None):\n\t\t'''Stop on EOS. Otherwise, pass the last output as the next input and pass through state.'''\n\t\twith tf.name_scope('TacoTestHelper'):\n\t\t\t#A sequence is finished when the output probability is > 0.5\n\t\t\tfinished = tf.cast(tf.round(stop_token_prediction), tf.bool)\n\n\t\t\t#Since we are predicting r frames at each step, two modes are\n\t\t\t#then possible:\n\t\t\t#\tStop when the model outputs a p > 0.5 for any frame between r frames (Recommended)\n\t\t\t#\tStop when the model outputs a p > 0.5 for all r frames (Safer)\n\t\t\t#Note:\n\t\t\t#\tWith enough training steps, the model should be able to predict when to stop correctly\n\t\t\t#\tand the use of stop_at_any = True would be recommended. If however the model didn't\n\t\t\t#\tlearn to stop correctly yet, (stops too soon) one could choose to use the safer option\n\t\t\t#\tto get a correct synthesis\n\t\t\tif self.stop_at_any:\n\t\t\t\tfinished = tf.reduce_any(tf.reduce_all(finished, axis=0)) #Recommended\n\t\t\telse:\n\t\t\t\tfinished = tf.reduce_all(tf.reduce_all(finished, axis=0)) #Safer option\n\n\t\t\t# Feed last output frame as next input. outputs is [N, output_dim * r]\n\t\t\tnext_inputs = outputs[:, -self._output_dim:]\n\t\t\tnext_state = state\n\t\t\treturn (finished, next_inputs, next_state)\n\n\nclass TacoTrainingHelper(Helper):\n\tdef __init__(self, batch_size, targets, hparams, gta, evaluating, global_step):\n\t\t# inputs is [N, T_in], targets is [N, T_out, D]\n\t\twith tf.name_scope('TacoTrainingHelper'):\n\t\t\tself._batch_size = batch_size\n\t\t\tself._output_dim = hparams.num_mels\n\t\t\tself._reduction_factor = hparams.outputs_per_step\n\t\t\tself._ratio = tf.convert_to_tensor(hparams.tacotron_teacher_forcing_ratio)\n\t\t\tself.gta = gta\n\t\t\tself.eval = evaluating\n\t\t\tself._hparams = hparams\n\t\t\tself.global_step = global_step\n\n\t\t\tr = self._reduction_factor\n\t\t\t# Feed every r-th target frame as input\n\t\t\tself._targets = targets[:, r-1::r, :]\n\n\t\t\t#Maximal sequence length\n\t\t\tself._lengths = tf.tile([tf.shape(self._targets)[1]], [self._batch_size])\n\n\t@property\n\tdef batch_size(self):\n\t\treturn self._batch_size\n\n\t@property\n\tdef token_output_size(self):\n\t\treturn self._reduction_factor\n\n\t@property\n\tdef sample_ids_shape(self):\n\t\treturn tf.TensorShape([])\n\n\t@property\n\tdef sample_ids_dtype(self):\n\t\treturn np.int32\n\n\tdef initialize(self, name=None):\n\t\t#Compute teacher forcing ratio for this global step.\n\t\t#In GTA mode, override teacher forcing scheme to work with full teacher forcing\n\t\tif self.gta:\n\t\t\tself._ratio = tf.convert_to_tensor(1.) #Force GTA model to always feed ground-truth\n\t\telif self.eval and self._hparams.tacotron_natural_eval:\n\t\t\tself._ratio = tf.convert_to_tensor(0.) #Force eval model to always feed predictions\n\t\telse:\n\t\t\tif self._hparams.tacotron_teacher_forcing_mode == 'scheduled':\n\t\t\t\tself._ratio = _teacher_forcing_ratio_decay(self._hparams.tacotron_teacher_forcing_init_ratio,\n\t\t\t\t\tself.global_step, self._hparams)\n\n\t\treturn (tf.tile([False], [self._batch_size]), _go_frames(self._batch_size, self._output_dim))\n\n\tdef sample(self, time, outputs, state, name=None):\n\t\treturn tf.tile([0], [self._batch_size])  # Return all 0; we ignore them\n\n\tdef next_inputs(self, time, outputs, state, sample_ids, stop_token_prediction, name=None):\n\t\twith tf.name_scope(name or 'TacoTrainingHelper'):\n\t\t\t#synthesis stop (we let the model see paddings as we mask them when computing loss functions)\n\t\t\tfinished = (time + 1 >= self._lengths)\n\n\t\t\t#Pick previous outputs randomly with respect to teacher forcing ratio\n\t\t\tnext_inputs = tf.cond(\n\t\t\t\ttf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32), self._ratio),\n\t\t\t\tlambda: self._targets[:, time, :], #Teacher-forcing: return true frame\n\t\t\t\tlambda: outputs[:,-self._output_dim:])\n\n\t\t\t#Pass on state\n\t\t\tnext_state = state\n\t\t\treturn (finished, next_inputs, next_state)\n\n\ndef _go_frames(batch_size, output_dim):\n\t'''Returns all-zero <GO> frames for a given batch size and output dimension'''\n\treturn tf.tile([[0.0]], [batch_size, output_dim])\n\ndef _teacher_forcing_ratio_decay(init_tfr, global_step, hparams):\n\t\t#################################################################\n\t\t# Narrow Cosine Decay:\n\n\t\t# Phase 1: tfr = init\n\t\t# We only start learning rate decay after 10k steps\n\n\t\t# Phase 2: tfr in ]init, final[\n\t\t# decay reach minimal value at step ~40k\n\n\t\t# Phase 3: tfr = final\n\t\t# clip by minimal teacher forcing ratio value (step >~ 40k)\n\t\t#################################################################\n\t\t#Pick final teacher forcing rate value\n\t\tif hparams.tacotron_teacher_forcing_final_ratio is not None:\n\t\t\talpha = float(hparams.tacotron_teacher_forcing_final_ratio / hparams.tacotron_teacher_forcing_init_ratio)\n\n\t\telse:\n\t\t\tassert hparams.tacotron_teacher_forcing_decay_alpha is not None\n\t\t\talpha = hparams.tacotron_teacher_forcing_decay_alpha\n\n\t\t#Compute natural cosine decay\n\t\ttfr = tf.train.cosine_decay(init_tfr,\n\t\t\tglobal_step=global_step - hparams.tacotron_teacher_forcing_start_decay, #tfr ~= init at step 10k\n\t\t\tdecay_steps=hparams.tacotron_teacher_forcing_decay_steps, #tfr ~= final at step ~40k\n\t\t\talpha=alpha, #tfr = alpha% of init_tfr as final value\n\t\t\tname='tfr_cosine_decay')\n\n\t\t#force teacher forcing ratio to take initial value when global step < start decay step.\n\t\tnarrow_tfr = tf.cond(\n\t\t\ttf.less(global_step, tf.convert_to_tensor(hparams.tacotron_teacher_forcing_start_decay)),\n\t\t\tlambda: tf.convert_to_tensor(init_tfr),\n\t\t\tlambda: tfr)\n\n\t\treturn narrow_tfr"""
tacotron/models/modules.py,67,"b'import tensorflow as tf\n\n\nclass HighwayNet:\n\tdef __init__(self, units, name=None):\n\t\tself.units = units\n\t\tself.scope = \'HighwayNet\' if name is None else name\n\n\t\tself.H_layer = tf.layers.Dense(units=self.units, activation=tf.nn.relu, name=\'H\')\n\t\tself.T_layer = tf.layers.Dense(units=self.units, activation=tf.nn.sigmoid, name=\'T\', bias_initializer=tf.constant_initializer(-1.))\n\n\tdef __call__(self, inputs):\n\t\twith tf.variable_scope(self.scope):\n\t\t\tH = self.H_layer(inputs)\n\t\t\tT = self.T_layer(inputs)\n\t\t\treturn H * T + inputs * (1. - T)\n\n\nclass CBHG:\n\tdef __init__(self, K, conv_channels, pool_size, projections, projection_kernel_size, n_highwaynet_layers, highway_units, rnn_units, bnorm, is_training, name=None):\n\t\tself.K = K\n\t\tself.conv_channels = conv_channels\n\t\tself.pool_size = pool_size\n\n\t\tself.projections = projections\n\t\tself.projection_kernel_size = projection_kernel_size\n\t\tself.bnorm = bnorm\n\n\t\tself.is_training = is_training\n\t\tself.scope = \'CBHG\' if name is None else name\n\n\t\tself.highway_units = highway_units\n\t\tself.highwaynet_layers = [HighwayNet(highway_units, name=\'{}_highwaynet_{}\'.format(self.scope, i+1)) for i in range(n_highwaynet_layers)]\n\t\tself._fw_cell = tf.nn.rnn_cell.GRUCell(rnn_units, name=\'{}_forward_RNN\'.format(self.scope))\n\t\tself._bw_cell = tf.nn.rnn_cell.GRUCell(rnn_units, name=\'{}_backward_RNN\'.format(self.scope))\n\n\tdef __call__(self, inputs, input_lengths):\n\t\twith tf.variable_scope(self.scope):\n\t\t\twith tf.variable_scope(\'conv_bank\'):\n\t\t\t\t#Convolution bank: concatenate on the last axis to stack channels from all convolutions\n\t\t\t\t#The convolution bank uses multiple different kernel sizes to have many insights of the input sequence\n\t\t\t\t#This makes one of the strengths of the CBHG block on sequences.\n\t\t\t\tconv_outputs = tf.concat(\n\t\t\t\t\t[conv1d(inputs, k, self.conv_channels, tf.nn.relu, self.is_training, 0., self.bnorm, \'conv1d_{}\'.format(k)) for k in range(1, self.K+1)],\n\t\t\t\t\taxis=-1\n\t\t\t\t\t)\n\n\t\t\t#Maxpooling (dimension reduction, Using max instead of average helps finding ""Edges"" in mels)\n\t\t\tmaxpool_output = tf.layers.max_pooling1d(\n\t\t\t\tconv_outputs,\n\t\t\t\tpool_size=self.pool_size,\n\t\t\t\tstrides=1,\n\t\t\t\tpadding=\'same\')\n\n\t\t\t#Two projection layers\n\t\t\tproj1_output = conv1d(maxpool_output, self.projection_kernel_size, self.projections[0], tf.nn.relu, self.is_training, 0., self.bnorm, \'proj1\')\n\t\t\tproj2_output = conv1d(proj1_output, self.projection_kernel_size, self.projections[1], lambda _: _, self.is_training, 0., self.bnorm, \'proj2\')\n\n\t\t\t#Residual connection\n\t\t\thighway_input = proj2_output + inputs\n\n\t\t\t#Additional projection in case of dimension mismatch (for HighwayNet ""residual"" connection)\n\t\t\tif highway_input.shape[2] != self.highway_units:\n\t\t\t\thighway_input = tf.layers.dense(highway_input, self.highway_units)\n\n\t\t\t#4-layer HighwayNet\n\t\t\tfor highwaynet in self.highwaynet_layers:\n\t\t\t\thighway_input = highwaynet(highway_input)\n\t\t\trnn_input = highway_input\n\n\t\t\t#Bidirectional RNN\n\t\t\toutputs, states = tf.nn.bidirectional_dynamic_rnn(\n\t\t\t\tself._fw_cell,\n\t\t\t\tself._bw_cell,\n\t\t\t\trnn_input,\n\t\t\t\tsequence_length=input_lengths,\n\t\t\t\tdtype=tf.float32)\n\t\t\treturn tf.concat(outputs, axis=2) #Concat forward and backward outputs\n\n\nclass ZoneoutLSTMCell(tf.nn.rnn_cell.RNNCell):\n\t\'\'\'Wrapper for tf LSTM to create Zoneout LSTM Cell\n\n\tinspired by:\n\thttps://github.com/teganmaharaj/zoneout/blob/master/zoneout_tensorflow.py\n\n\tPublished by one of \'https://arxiv.org/pdf/1606.01305.pdf\' paper writers.\n\n\tMany thanks to @Ondal90 for pointing this out. You sir are a hero!\n\t\'\'\'\n\tdef __init__(self, num_units, is_training, zoneout_factor_cell=0., zoneout_factor_output=0., state_is_tuple=True, name=None):\n\t\t\'\'\'Initializer with possibility to set different zoneout values for cell/hidden states.\n\t\t\'\'\'\n\t\tzm = min(zoneout_factor_output, zoneout_factor_cell)\n\t\tzs = max(zoneout_factor_output, zoneout_factor_cell)\n\n\t\tif zm < 0. or zs > 1.:\n\t\t\traise ValueError(\'One/both provided Zoneout factors are not in [0, 1]\')\n\n\t\tself._cell = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=state_is_tuple, name=name)\n\t\tself._zoneout_cell = zoneout_factor_cell\n\t\tself._zoneout_outputs = zoneout_factor_output\n\t\tself.is_training = is_training\n\t\tself.state_is_tuple = state_is_tuple\n\n\t@property\n\tdef state_size(self):\n\t\treturn self._cell.state_size\n\n\t@property\n\tdef output_size(self):\n\t\treturn self._cell.output_size\n\n\tdef __call__(self, inputs, state, scope=None):\n\t\t\'\'\'Runs vanilla LSTM Cell and applies zoneout.\n\t\t\'\'\'\n\t\t#Apply vanilla LSTM\n\t\toutput, new_state = self._cell(inputs, state, scope)\n\n\t\tif self.state_is_tuple:\n\t\t\t(prev_c, prev_h) = state\n\t\t\t(new_c, new_h) = new_state\n\t\telse:\n\t\t\tnum_proj = self._cell._num_units if self._cell._num_proj is None else self._cell._num_proj\n\t\t\tprev_c = tf.slice(state, [0, 0], [-1, self._cell._num_units])\n\t\t\tprev_h = tf.slice(state, [0, self._cell._num_units], [-1, num_proj])\n\t\t\tnew_c = tf.slice(new_state, [0, 0], [-1, self._cell._num_units])\n\t\t\tnew_h = tf.slice(new_state, [0, self._cell._num_units], [-1, num_proj])\n\n\t\t#Apply zoneout\n\t\tif self.is_training:\n\t\t\t#nn.dropout takes keep_prob (probability to keep activations) not drop_prob (probability to mask activations)!\n\t\t\tc = (1 - self._zoneout_cell) * tf.nn.dropout(new_c - prev_c, (1 - self._zoneout_cell)) + prev_c\n\t\t\th = (1 - self._zoneout_outputs) * tf.nn.dropout(new_h - prev_h, (1 - self._zoneout_outputs)) + prev_h\n\n\t\telse:\n\t\t\tc = (1 - self._zoneout_cell) * new_c + self._zoneout_cell * prev_c\n\t\t\th = (1 - self._zoneout_outputs) * new_h + self._zoneout_outputs * prev_h\n\n\t\tnew_state = tf.nn.rnn_cell.LSTMStateTuple(c, h) if self.state_is_tuple else tf.concat(1, [c, h])\n\n\t\treturn output, new_state\n\n\nclass EncoderConvolutions:\n\t""""""Encoder convolutional layers used to find local dependencies in inputs characters.\n\t""""""\n\tdef __init__(self, is_training, hparams, activation=tf.nn.relu, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tis_training: Boolean, determines if the model is training or in inference to control dropout\n\t\t\tkernel_size: tuple or integer, The size of convolution kernels\n\t\t\tchannels: integer, number of convolutional kernels\n\t\t\tactivation: callable, postnet activation function for each convolutional layer\n\t\t\tscope: Postnet scope.\n\t\t""""""\n\t\tsuper(EncoderConvolutions, self).__init__()\n\t\tself.is_training = is_training\n\n\t\tself.kernel_size = hparams.enc_conv_kernel_size\n\t\tself.channels = hparams.enc_conv_channels\n\t\tself.activation = activation\n\t\tself.scope = \'enc_conv_layers\' if scope is None else scope\n\t\tself.drop_rate = hparams.tacotron_dropout_rate\n\t\tself.enc_conv_num_layers = hparams.enc_conv_num_layers\n\t\tself.bnorm = hparams.batch_norm_position\n\n\tdef __call__(self, inputs):\n\t\twith tf.variable_scope(self.scope):\n\t\t\tx = inputs\n\t\t\tfor i in range(self.enc_conv_num_layers):\n\t\t\t\tx = conv1d(x, self.kernel_size, self.channels, self.activation,\n\t\t\t\t\tself.is_training, self.drop_rate, self.bnorm, \'conv_layer_{}_\'.format(i + 1)+self.scope)\n\t\treturn x\n\n\nclass EncoderRNN:\n\t""""""Encoder bidirectional one layer LSTM\n\t""""""\n\tdef __init__(self, is_training, size=256, zoneout=0.1, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tis_training: Boolean, determines if the model is training or in inference to control zoneout\n\t\t\tsize: integer, the number of LSTM units for each direction\n\t\t\tzoneout: the zoneout factor\n\t\t\tscope: EncoderRNN scope.\n\t\t""""""\n\t\tsuper(EncoderRNN, self).__init__()\n\t\tself.is_training = is_training\n\n\t\tself.size = size\n\t\tself.zoneout = zoneout\n\t\tself.scope = \'encoder_LSTM\' if scope is None else scope\n\n\t\t#Create forward LSTM Cell\n\t\tself._fw_cell = ZoneoutLSTMCell(size, is_training,\n\t\t\tzoneout_factor_cell=zoneout,\n\t\t\tzoneout_factor_output=zoneout,\n\t\t\tname=\'encoder_fw_LSTM\')\n\n\t\t#Create backward LSTM Cell\n\t\tself._bw_cell = ZoneoutLSTMCell(size, is_training,\n\t\t\tzoneout_factor_cell=zoneout,\n\t\t\tzoneout_factor_output=zoneout,\n\t\t\tname=\'encoder_bw_LSTM\')\n\n\tdef __call__(self, inputs, input_lengths):\n\t\twith tf.variable_scope(self.scope):\n\t\t\toutputs, (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n\t\t\t\tself._fw_cell,\n\t\t\t\tself._bw_cell,\n\t\t\t\tinputs,\n\t\t\t\tsequence_length=input_lengths,\n\t\t\t\tdtype=tf.float32,\n\t\t\t\tswap_memory=True)\n\n\t\t\treturn tf.concat(outputs, axis=2) # Concat and return forward + backward outputs\n\n\nclass Prenet:\n\t""""""Two fully connected layers used as an information bottleneck for the attention.\n\t""""""\n\tdef __init__(self, is_training, layers_sizes=[256, 256], drop_rate=0.5, activation=tf.nn.relu, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tlayers_sizes: list of integers, the length of the list represents the number of pre-net\n\t\t\t\tlayers and the list values represent the layers number of units\n\t\t\tactivation: callable, activation functions of the prenet layers.\n\t\t\tscope: Prenet scope.\n\t\t""""""\n\t\tsuper(Prenet, self).__init__()\n\t\tself.drop_rate = drop_rate\n\n\t\tself.layers_sizes = layers_sizes\n\t\tself.activation = activation\n\t\tself.is_training = is_training\n\n\t\tself.scope = \'prenet\' if scope is None else scope\n\n\tdef __call__(self, inputs):\n\t\tx = inputs\n\n\t\twith tf.variable_scope(self.scope):\n\t\t\tfor i, size in enumerate(self.layers_sizes):\n\t\t\t\tdense = tf.layers.dense(x, units=size, activation=self.activation,\n\t\t\t\t\tname=\'dense_{}\'.format(i + 1))\n\t\t\t\t#The paper discussed introducing diversity in generation at inference time\n\t\t\t\t#by using a dropout of 0.5 only in prenet layers (in both training and inference).\n\t\t\t\tx = tf.layers.dropout(dense, rate=self.drop_rate, training=True,\n\t\t\t\t\tname=\'dropout_{}\'.format(i + 1) + self.scope)\n\t\treturn x\n\n\nclass DecoderRNN:\n\t""""""Decoder two uni directional LSTM Cells\n\t""""""\n\tdef __init__(self, is_training, layers=2, size=1024, zoneout=0.1, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tis_training: Boolean, determines if the model is in training or inference to control zoneout\n\t\t\tlayers: integer, the number of LSTM layers in the decoder\n\t\t\tsize: integer, the number of LSTM units in each layer\n\t\t\tzoneout: the zoneout factor\n\t\t""""""\n\t\tsuper(DecoderRNN, self).__init__()\n\t\tself.is_training = is_training\n\n\t\tself.layers = layers\n\t\tself.size = size\n\t\tself.zoneout = zoneout\n\t\tself.scope = \'decoder_rnn\' if scope is None else scope\n\n\t\t#Create a set of LSTM layers\n\t\tself.rnn_layers = [ZoneoutLSTMCell(size, is_training,\n\t\t\tzoneout_factor_cell=zoneout,\n\t\t\tzoneout_factor_output=zoneout,\n\t\t\tname=\'decoder_LSTM_{}\'.format(i+1)) for i in range(layers)]\n\n\t\tself._cell = tf.contrib.rnn.MultiRNNCell(self.rnn_layers, state_is_tuple=True)\n\n\tdef __call__(self, inputs, states):\n\t\twith tf.variable_scope(self.scope):\n\t\t\treturn self._cell(inputs, states)\n\n\nclass FrameProjection:\n\t""""""Projection layer to r * num_mels dimensions or num_mels dimensions\n\t""""""\n\tdef __init__(self, shape=80, activation=None, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tshape: integer, dimensionality of output space (r*n_mels for decoder or n_mels for postnet)\n\t\t\tactivation: callable, activation function\n\t\t\tscope: FrameProjection scope.\n\t\t""""""\n\t\tsuper(FrameProjection, self).__init__()\n\n\t\tself.shape = shape\n\t\tself.activation = activation\n\n\t\tself.scope = \'Linear_projection\' if scope is None else scope\n\t\tself.dense = tf.layers.Dense(units=shape, activation=activation, name=\'projection_{}\'.format(self.scope))\n\n\tdef __call__(self, inputs):\n\t\twith tf.variable_scope(self.scope):\n\t\t\t#If activation==None, this returns a simple Linear projection\n\t\t\t#else the projection will be passed through an activation function\n\t\t\t# output = tf.layers.dense(inputs, units=self.shape, activation=self.activation,\n\t\t\t# \tname=\'projection_{}\'.format(self.scope))\n\t\t\toutput = self.dense(inputs)\n\n\t\t\treturn output\n\n\nclass StopProjection:\n\t""""""Projection to a scalar and through a sigmoid activation\n\t""""""\n\tdef __init__(self, is_training, shape=1, activation=tf.nn.sigmoid, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tis_training: Boolean, to control the use of sigmoid function as it is useless to use it\n\t\t\t\tduring training since it is integrate inside the sigmoid_crossentropy loss\n\t\t\tshape: integer, dimensionality of output space. Defaults to 1 (scalar)\n\t\t\tactivation: callable, activation function. only used during inference\n\t\t\tscope: StopProjection scope.\n\t\t""""""\n\t\tsuper(StopProjection, self).__init__()\n\t\tself.is_training = is_training\n\n\t\tself.shape = shape\n\t\tself.activation = activation\n\t\tself.scope = \'stop_token_projection\' if scope is None else scope\n\n\tdef __call__(self, inputs):\n\t\twith tf.variable_scope(self.scope):\n\t\t\toutput = tf.layers.dense(inputs, units=self.shape,\n\t\t\t\tactivation=None, name=\'projection_{}\'.format(self.scope))\n\n\t\t\t#During training, don\'t use activation as it is integrated inside the sigmoid_cross_entropy loss function\n\t\t\tif self.is_training:\n\t\t\t\treturn output\n\t\t\treturn self.activation(output)\n\n\nclass Postnet:\n\t""""""Postnet that takes final decoder output and fine tunes it (using vision on past and future frames)\n\t""""""\n\tdef __init__(self, is_training, hparams, activation=tf.nn.tanh, scope=None):\n\t\t""""""\n\t\tArgs:\n\t\t\tis_training: Boolean, determines if the model is training or in inference to control dropout\n\t\t\tkernel_size: tuple or integer, The size of convolution kernels\n\t\t\tchannels: integer, number of convolutional kernels\n\t\t\tactivation: callable, postnet activation function for each convolutional layer\n\t\t\tscope: Postnet scope.\n\t\t""""""\n\t\tsuper(Postnet, self).__init__()\n\t\tself.is_training = is_training\n\n\t\tself.kernel_size = hparams.postnet_kernel_size\n\t\tself.channels = hparams.postnet_channels\n\t\tself.activation = activation\n\t\tself.scope = \'postnet_convolutions\' if scope is None else scope\n\t\tself.postnet_num_layers = hparams.postnet_num_layers\n\t\tself.drop_rate = hparams.tacotron_dropout_rate\n\t\tself.bnorm = hparams.batch_norm_position\n\n\tdef __call__(self, inputs):\n\t\twith tf.variable_scope(self.scope):\n\t\t\tx = inputs\n\t\t\tfor i in range(self.postnet_num_layers - 1):\n\t\t\t\tx = conv1d(x, self.kernel_size, self.channels, self.activation,\n\t\t\t\t\tself.is_training, self.drop_rate, self.bnorm, \'conv_layer_{}_\'.format(i + 1)+self.scope)\n\t\t\tx = conv1d(x, self.kernel_size, self.channels, lambda _: _, self.is_training, self.drop_rate, self.bnorm,\n\t\t\t\t\'conv_layer_{}_\'.format(5)+self.scope)\n\t\treturn x\n\n\ndef conv1d(inputs, kernel_size, channels, activation, is_training, drop_rate, bnorm, scope):\n\tassert bnorm in (\'before\', \'after\')\n\twith tf.variable_scope(scope):\n\t\tconv1d_output = tf.layers.conv1d(\n\t\t\tinputs,\n\t\t\tfilters=channels,\n\t\t\tkernel_size=kernel_size,\n\t\t\tactivation=activation if bnorm == \'after\' else None,\n\t\t\tpadding=\'same\')\n\t\tbatched = tf.layers.batch_normalization(conv1d_output, training=is_training)\n\t\tactivated = activation(batched) if bnorm == \'before\' else batched\n\t\treturn tf.layers.dropout(activated, rate=drop_rate, training=is_training,\n\t\t\t\t\t\t\t\tname=\'dropout_{}\'.format(scope))\n\ndef _round_up_tf(x, multiple):\n\t# Tf version of remainder = x % multiple\n\tremainder = tf.mod(x, multiple)\n\t# Tf version of return x if remainder == 0 else x + multiple - remainder\n\tx_round =  tf.cond(tf.equal(remainder, tf.zeros(tf.shape(remainder), dtype=tf.int32)),\n\t\tlambda: x,\n\t\tlambda: x + multiple - remainder)\n\n\treturn x_round\n\ndef sequence_mask(lengths, r, expand=True):\n\t\'\'\'Returns a 2-D or 3-D tensorflow sequence mask depending on the argument \'expand\'\n\t\'\'\'\n\tmax_len = tf.reduce_max(lengths)\n\tmax_len = _round_up_tf(max_len, tf.convert_to_tensor(r))\n\tif expand:\n\t\treturn tf.expand_dims(tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32), axis=-1)\n\treturn tf.sequence_mask(lengths, maxlen=max_len, dtype=tf.float32)\n\ndef MaskedMSE(targets, outputs, targets_lengths, hparams, mask=None):\n\t\'\'\'Computes a masked Mean Squared Error\n\t\'\'\'\n\n\t#[batch_size, time_dimension, 1]\n\t#example:\n\t#sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],\n\t#\t\t\t\t\t\t\t    [[1., 1., 1., 0., 0.]],\n\t#\t\t\t\t\t\t\t    [[1., 1., 0., 0., 0.]]]\n\t#Note the maxlen argument that ensures mask shape is compatible with r>1\n\t#This will by default mask the extra paddings caused by r>1\n\tif mask is None:\n\t\tmask = sequence_mask(targets_lengths, hparams.outputs_per_step, True)\n\n\t#[batch_size, time_dimension, channel_dimension(mels)]\n\tones = tf.ones(shape=[tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]], dtype=tf.float32)\n\tmask_ = mask * ones\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask_))]):\n\t\treturn tf.losses.mean_squared_error(labels=targets, predictions=outputs, weights=mask_)\n\ndef MaskedSigmoidCrossEntropy(targets, outputs, targets_lengths, hparams, mask=None):\n\t\'\'\'Computes a masked SigmoidCrossEntropy with logits\n\t\'\'\'\n\n\t#[batch_size, time_dimension]\n\t#example:\n\t#sequence_mask([1, 3, 2], 5) = [[1., 0., 0., 0., 0.],\n\t#\t\t\t\t\t\t\t    [1., 1., 1., 0., 0.],\n\t#\t\t\t\t\t\t\t    [1., 1., 0., 0., 0.]]\n\t#Note the maxlen argument that ensures mask shape is compatible with r>1\n\t#This will by default mask the extra paddings caused by r>1\n\tif mask is None:\n\t\tmask = sequence_mask(targets_lengths, hparams.outputs_per_step, False)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask))]):\n\t\t#Use a weighted sigmoid cross entropy to measure the <stop_token> loss. Set hparams.cross_entropy_pos_weight to 1\n\t\t#will have the same effect as  vanilla tf.nn.sigmoid_cross_entropy_with_logits.\n\t\tlosses = tf.nn.weighted_cross_entropy_with_logits(targets=targets, logits=outputs, pos_weight=hparams.cross_entropy_pos_weight)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(mask), tf.shape(losses))]):\n\t\tmasked_loss = losses * mask\n\n\treturn tf.reduce_sum(masked_loss) / tf.count_nonzero(masked_loss, dtype=tf.float32)\n\ndef MaskedLinearLoss(targets, outputs, targets_lengths, hparams, mask=None):\n\t\'\'\'Computes a masked MAE loss with priority to low frequencies\n\t\'\'\'\n\n\t#[batch_size, time_dimension, 1]\n\t#example:\n\t#sequence_mask([1, 3, 2], 5) = [[[1., 0., 0., 0., 0.]],\n\t#\t\t\t\t\t\t\t    [[1., 1., 1., 0., 0.]],\n\t#\t\t\t\t\t\t\t    [[1., 1., 0., 0., 0.]]]\n\t#Note the maxlen argument that ensures mask shape is compatible with r>1\n\t#This will by default mask the extra paddings caused by r>1\n\tif mask is None:\n\t\tmask = sequence_mask(targets_lengths, hparams.outputs_per_step, True)\n\n\t#[batch_size, time_dimension, channel_dimension(freq)]\n\tones = tf.ones(shape=[tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]], dtype=tf.float32)\n\tmask_ = mask * ones\n\n\tl1 = tf.abs(targets - outputs)\n\tn_priority_freq = int(2000 / (hparams.sample_rate * 0.5) * hparams.num_freq)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask_))]):\n\t\tmasked_l1 = l1 * mask_\n\t\tmasked_l1_low = masked_l1[:,:,0:n_priority_freq]\n\n\tmean_l1 = tf.reduce_sum(masked_l1) / tf.reduce_sum(mask_)\n\tmean_l1_low = tf.reduce_sum(masked_l1_low) / tf.reduce_sum(mask_)\n\n\treturn 0.5 * mean_l1 + 0.5 * mean_l1_low\n'"
tacotron/models/tacotron.py,50,"b'import tensorflow as tf \nfrom tacotron.utils.symbols import symbols\nfrom infolog import log\nfrom tacotron.models.helpers import TacoTrainingHelper, TacoTestHelper\nfrom tacotron.models.modules import *\nfrom tensorflow.contrib.seq2seq import dynamic_decode\nfrom tacotron.models.Architecture_wrappers import TacotronEncoderCell, TacotronDecoderCell\nfrom tacotron.models.custom_decoder import CustomDecoder\nfrom tacotron.models.attention import LocationSensitiveAttention\n\nimport numpy as np\n\ndef split_func(x, split_pos):\n\trst = []\n\tstart = 0\n\t# x will be a numpy array with the contents of the placeholder below\n\tfor i in range(split_pos.shape[0]):\n\t\trst.append(x[:,start:start+split_pos[i]])\n\t\tstart += split_pos[i]\n\treturn rst\n\nclass Tacotron():\n\t""""""Tacotron-2 Feature prediction Model.\n\t""""""\n\tdef __init__(self, hparams):\n\t\tself._hparams = hparams\n\n\tdef initialize(self, inputs, input_lengths, mel_targets=None, stop_token_targets=None, linear_targets=None, targets_lengths=None, gta=False,\n\t\t\tglobal_step=None, is_training=False, is_evaluating=False, split_infos=None):\n\t\t""""""\n\t\tInitializes the model for inference\n\t\tsets ""mel_outputs"" and ""alignments"" fields.\n\t\tArgs:\n\t\t\t- inputs: int32 Tensor with shape [N, T_in] where N is batch size, T_in is number of\n\t\t\t  steps in the input time series, and values are character IDs\n\t\t\t- input_lengths: int32 Tensor with shape [N] where N is batch size and values are the lengths\n\t\t\tof each sequence in inputs.\n\t\t\t- mel_targets: float32 Tensor with shape [N, T_out, M] where N is batch size, T_out is number\n\t\t\tof steps in the output time series, M is num_mels, and values are entries in the mel\n\t\t\tspectrogram. Only needed for training.\n\t\t""""""\n\t\tif mel_targets is None and stop_token_targets is not None:\n\t\t\traise ValueError(\'no multi targets were provided but token_targets were given\')\n\t\tif mel_targets is not None and stop_token_targets is None and not gta:\n\t\t\traise ValueError(\'Mel targets are provided without corresponding token_targets\')\n\t\tif not gta and self._hparams.predict_linear==True and linear_targets is None and is_training:\n\t\t\traise ValueError(\'Model is set to use post processing to predict linear spectrograms in training but no linear targets given!\')\n\t\tif gta and linear_targets is not None:\n\t\t\traise ValueError(\'Linear spectrogram prediction is not supported in GTA mode!\')\n\t\tif is_training and self._hparams.mask_decoder and targets_lengths is None:\n\t\t\traise RuntimeError(\'Model set to mask paddings but no targets lengths provided for the mask!\')\n\t\tif is_training and is_evaluating:\n\t\t\traise RuntimeError(\'Model can not be in training and evaluation modes at the same time!\')\n\n\t\tsplit_device = \'/cpu:0\' if self._hparams.tacotron_num_gpus > 1 or self._hparams.split_on_cpu else \'/gpu:0\'\n\t\twith tf.device(split_device):\n\t\t\thp = self._hparams\n\t\t\tlout_int = [tf.int32]*hp.tacotron_num_gpus\n\t\t\tlout_float = [tf.float32]*hp.tacotron_num_gpus\n\n\t\t\ttower_input_lengths = tf.split(input_lengths, num_or_size_splits=hp.tacotron_num_gpus, axis=0)\n\t\t\ttower_targets_lengths = tf.split(targets_lengths, num_or_size_splits=hp.tacotron_num_gpus, axis=0) if targets_lengths is not None else targets_lengths\n\n\t\t\tp_inputs = tf.py_func(split_func, [inputs, split_infos[:, 0]], lout_int)\n\t\t\tp_mel_targets = tf.py_func(split_func, [mel_targets, split_infos[:,1]], lout_float) if mel_targets is not None else mel_targets\n\t\t\tp_stop_token_targets = tf.py_func(split_func, [stop_token_targets, split_infos[:,2]], lout_float) if stop_token_targets is not None else stop_token_targets\n\t\t\tp_linear_targets = tf.py_func(split_func, [linear_targets, split_infos[:,3]], lout_float) if linear_targets is not None else linear_targets\n\n\t\t\ttower_inputs = []\n\t\t\ttower_mel_targets = []\n\t\t\ttower_stop_token_targets = []\n\t\t\ttower_linear_targets = []\n\n\t\t\tbatch_size = tf.shape(inputs)[0]\n\t\t\tmel_channels = hp.num_mels\n\t\t\tlinear_channels = hp.num_freq\n\t\t\tfor i in range (hp.tacotron_num_gpus):\n\t\t\t\ttower_inputs.append(tf.reshape(p_inputs[i], [batch_size, -1]))\n\t\t\t\tif p_mel_targets is not None:\n\t\t\t\t\ttower_mel_targets.append(tf.reshape(p_mel_targets[i], [batch_size, -1, mel_channels]))\n\t\t\t\tif p_stop_token_targets is not None:\n\t\t\t\t\ttower_stop_token_targets.append(tf.reshape(p_stop_token_targets[i], [batch_size, -1]))\n\t\t\t\tif p_linear_targets is not None:\n\t\t\t\t\ttower_linear_targets.append(tf.reshape(p_linear_targets[i], [batch_size, -1, linear_channels]))\n\n\t\tT2_output_range = (-hp.max_abs_value, hp.max_abs_value) if hp.symmetric_mels else (0, hp.max_abs_value)\n\n\t\tself.tower_decoder_output = []\n\t\tself.tower_alignments = []\n\t\tself.tower_stop_token_prediction = []\n\t\tself.tower_mel_outputs = []\n\t\tself.tower_linear_outputs = []\n\n\t\ttower_embedded_inputs = []\n\t\ttower_enc_conv_output_shape = []\n\t\ttower_encoder_outputs = []\n\t\ttower_residual = []\n\t\ttower_projected_residual = []\n\t\t\n\t\t# 1. Declare GPU Devices\n\t\tgpus = [""/gpu:{}"".format(i) for i in range(hp.tacotron_num_gpus)]\n\t\tfor i in range(hp.tacotron_num_gpus):\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"", worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'inference\') as scope:\n\t\t\t\t\tassert hp.tacotron_teacher_forcing_mode in (\'constant\', \'scheduled\')\n\t\t\t\t\tif hp.tacotron_teacher_forcing_mode == \'scheduled\' and is_training:\n\t\t\t\t\t\tassert global_step is not None\n\n\t\t\t\t\t#GTA is only used for predicting mels to train Wavenet vocoder, so we ommit post processing when doing GTA synthesis\n\t\t\t\t\tpost_condition = hp.predict_linear and not gta\n\n\t\t\t\t\t# Embeddings ==> [batch_size, sequence_length, embedding_dim]\n\t\t\t\t\tself.embedding_table = tf.get_variable(\n\t\t\t\t\t\t\'inputs_embedding\', [len(symbols), hp.embedding_dim], dtype=tf.float32)\n\t\t\t\t\tembedded_inputs = tf.nn.embedding_lookup(self.embedding_table, tower_inputs[i])\n\n\n\t\t\t\t\t#Encoder Cell ==> [batch_size, encoder_steps, encoder_lstm_units]\n\t\t\t\t\tencoder_cell = TacotronEncoderCell(\n\t\t\t\t\t\tEncoderConvolutions(is_training, hparams=hp, scope=\'encoder_convolutions\'),\n\t\t\t\t\t\tEncoderRNN(is_training, size=hp.encoder_lstm_units,\n\t\t\t\t\t\t\tzoneout=hp.tacotron_zoneout_rate, scope=\'encoder_LSTM\'))\n\n\t\t\t\t\tencoder_outputs = encoder_cell(embedded_inputs, tower_input_lengths[i])\n\n\t\t\t\t\t#For shape visualization purpose\n\t\t\t\t\tenc_conv_output_shape = encoder_cell.conv_output_shape\n\n\n\t\t\t\t\t#Decoder Parts\n\t\t\t\t\t#Attention Decoder Prenet\n\t\t\t\t\tprenet = Prenet(is_training, layers_sizes=hp.prenet_layers, drop_rate=hp.tacotron_dropout_rate, scope=\'decoder_prenet\')\n\t\t\t\t\t#Attention Mechanism\n\t\t\t\t\tattention_mechanism = LocationSensitiveAttention(hp.attention_dim, encoder_outputs, hparams=hp, is_training=is_training,\n\t\t\t\t\t\tmask_encoder=hp.mask_encoder, memory_sequence_length=tf.reshape(tower_input_lengths[i], [-1]), smoothing=hp.smoothing,\n\t\t\t\t\t\tcumulate_weights=hp.cumulative_weights)\n\t\t\t\t\t#Decoder LSTM Cells\n\t\t\t\t\tdecoder_lstm = DecoderRNN(is_training, layers=hp.decoder_layers,\n\t\t\t\t\t\tsize=hp.decoder_lstm_units, zoneout=hp.tacotron_zoneout_rate, scope=\'decoder_LSTM\')\n\t\t\t\t\t#Frames Projection layer\n\t\t\t\t\tframe_projection = FrameProjection(hp.num_mels * hp.outputs_per_step, scope=\'linear_transform_projection\')\n\t\t\t\t\t#<stop_token> projection layer\n\t\t\t\t\tstop_projection = StopProjection(is_training or is_evaluating, shape=hp.outputs_per_step, scope=\'stop_token_projection\')\n\n\n\t\t\t\t\t#Decoder Cell ==> [batch_size, decoder_steps, num_mels * r] (after decoding)\n\t\t\t\t\tdecoder_cell = TacotronDecoderCell(\n\t\t\t\t\t\tprenet,\n\t\t\t\t\t\tattention_mechanism,\n\t\t\t\t\t\tdecoder_lstm,\n\t\t\t\t\t\tframe_projection,\n\t\t\t\t\t\tstop_projection)\n\n\n\t\t\t\t\t#Define the helper for our decoder\n\t\t\t\t\tif is_training or is_evaluating or gta:\n\t\t\t\t\t\tself.helper = TacoTrainingHelper(batch_size, tower_mel_targets[i], hp, gta, is_evaluating, global_step)\n\t\t\t\t\telse:\n\t\t\t\t\t\tself.helper = TacoTestHelper(batch_size, hp)\n\n\n\t\t\t\t\t#initial decoder state\n\t\t\t\t\tdecoder_init_state = decoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n\n\t\t\t\t\t#Only use max iterations at synthesis time\n\t\t\t\t\tmax_iters = hp.max_iters if not (is_training or is_evaluating) else None\n\n\t\t\t\t\t#Decode\n\t\t\t\t\t(frames_prediction, stop_token_prediction, _), final_decoder_state, _ = dynamic_decode(\n\t\t\t\t\t\tCustomDecoder(decoder_cell, self.helper, decoder_init_state),\n\t\t\t\t\t\timpute_finished=False,\n\t\t\t\t\t\tmaximum_iterations=max_iters,\n\t\t\t\t\t\tswap_memory=hp.tacotron_swap_with_cpu)\n\n\n\t\t\t\t\t# Reshape outputs to be one output per entry \n\t\t\t\t\t#==> [batch_size, non_reduced_decoder_steps (decoder_steps * r), num_mels]\n\t\t\t\t\tdecoder_output = tf.reshape(frames_prediction, [batch_size, -1, hp.num_mels])\n\t\t\t\t\tstop_token_prediction = tf.reshape(stop_token_prediction, [batch_size, -1])\n\n\t\t\t\t\tif hp.clip_outputs:\n\t\t\t\t\t\t\tdecoder_output = tf.minimum(tf.maximum(decoder_output, T2_output_range[0] - hp.lower_bound_decay), T2_output_range[1])\n\n\t\t\t\t\t#Postnet\n\t\t\t\t\tpostnet = Postnet(is_training, hparams=hp, scope=\'postnet_convolutions\')\n\n\t\t\t\t\t#Compute residual using post-net ==> [batch_size, decoder_steps * r, postnet_channels]\n\t\t\t\t\tresidual = postnet(decoder_output)\n\n\t\t\t\t\t#Project residual to same dimension as mel spectrogram \n\t\t\t\t\t#==> [batch_size, decoder_steps * r, num_mels]\n\t\t\t\t\tresidual_projection = FrameProjection(hp.num_mels, scope=\'postnet_projection\')\n\t\t\t\t\tprojected_residual = residual_projection(residual)\n\n\n\t\t\t\t\t#Compute the mel spectrogram\n\t\t\t\t\tmel_outputs = decoder_output + projected_residual\n\n\t\t\t\t\tif hp.clip_outputs:\n\t\t\t\t\t\t\tmel_outputs = tf.minimum(tf.maximum(mel_outputs, T2_output_range[0] - hp.lower_bound_decay), T2_output_range[1])\n\n\n\t\t\t\t\tif post_condition:\n\t\t\t\t\t\t# Add post-processing CBHG. This does a great job at extracting features from mels before projection to Linear specs.\n\t\t\t\t\t\tpost_cbhg = CBHG(hp.cbhg_kernels, hp.cbhg_conv_channels, hp.cbhg_pool_size, [hp.cbhg_projection, hp.num_mels],\n\t\t\t\t\t\t\thp.cbhg_projection_kernel_size, hp.cbhg_highwaynet_layers, \n\t\t\t\t\t\t\thp.cbhg_highway_units, hp.cbhg_rnn_units, hp.batch_norm_position, is_training, name=\'CBHG_postnet\')\n\n\t\t\t\t\t\t#[batch_size, decoder_steps(mel_frames), cbhg_channels]\n\t\t\t\t\t\tpost_outputs = post_cbhg(mel_outputs, None)\n\n\t\t\t\t\t\t#Linear projection of extracted features to make linear spectrogram\n\t\t\t\t\t\tlinear_specs_projection = FrameProjection(hp.num_freq, scope=\'cbhg_linear_specs_projection\')\n\n\t\t\t\t\t\t#[batch_size, decoder_steps(linear_frames), num_freq]\n\t\t\t\t\t\tlinear_outputs = linear_specs_projection(post_outputs)\n\n\t\t\t\t\t\tif hp.clip_outputs:\n\t\t\t\t\t\t\tlinear_outputs = tf.minimum(tf.maximum(linear_outputs, T2_output_range[0] - hp.lower_bound_decay), T2_output_range[1])\n\n\t\t\t\t\t#Grab alignments from the final decoder state\n\t\t\t\t\talignments = tf.transpose(final_decoder_state.alignment_history.stack(), [1, 2, 0])\n\n\t\t\t\t\tself.tower_decoder_output.append(decoder_output)\n\t\t\t\t\tself.tower_alignments.append(alignments)\n\t\t\t\t\tself.tower_stop_token_prediction.append(stop_token_prediction)\n\t\t\t\t\tself.tower_mel_outputs.append(mel_outputs)\n\t\t\t\t\ttower_embedded_inputs.append(embedded_inputs)\n\t\t\t\t\ttower_enc_conv_output_shape.append(enc_conv_output_shape)\n\t\t\t\t\ttower_encoder_outputs.append(encoder_outputs)\n\t\t\t\t\ttower_residual.append(residual)\n\t\t\t\t\ttower_projected_residual.append(projected_residual)\n\n\t\t\t\t\tif post_condition:\n\t\t\t\t\t\tself.tower_linear_outputs.append(linear_outputs)\n\t\t\tlog(\'initialisation done {}\'.format(gpus[i]))\n\n\n\t\tif is_training:\n\t\t\tself.ratio = self.helper._ratio\n\t\tself.tower_inputs = tower_inputs\n\t\tself.tower_input_lengths = tower_input_lengths\n\t\tself.tower_mel_targets = tower_mel_targets\n\t\tself.tower_linear_targets = tower_linear_targets\n\t\tself.tower_targets_lengths = tower_targets_lengths\n\t\tself.tower_stop_token_targets = tower_stop_token_targets\n\n\t\tself.all_vars = tf.trainable_variables()\n\n\t\tlog(\'Initialized Tacotron model. Dimensions (? = dynamic shape): \')\n\t\tlog(\'  Train mode:               {}\'.format(is_training))\n\t\tlog(\'  Eval mode:                {}\'.format(is_evaluating))\n\t\tlog(\'  GTA mode:                 {}\'.format(gta))\n\t\tlog(\'  Synthesis mode:           {}\'.format(not (is_training or is_evaluating)))\n\t\tlog(\'  Input:                    {}\'.format(inputs.shape))\n\t\tfor i in range(hp.tacotron_num_gpus):\n\t\t\tlog(\'  device:                   {}\'.format(i))\n\t\t\tlog(\'  embedding:                {}\'.format(tower_embedded_inputs[i].shape))\n\t\t\tlog(\'  enc conv out:             {}\'.format(tower_enc_conv_output_shape[i]))\n\t\t\tlog(\'  encoder out:              {}\'.format(tower_encoder_outputs[i].shape))\n\t\t\tlog(\'  decoder out:              {}\'.format(self.tower_decoder_output[i].shape))\n\t\t\tlog(\'  residual out:             {}\'.format(tower_residual[i].shape))\n\t\t\tlog(\'  projected residual out:   {}\'.format(tower_projected_residual[i].shape))\n\t\t\tlog(\'  mel out:                  {}\'.format(self.tower_mel_outputs[i].shape))\n\t\t\tif post_condition:\n\t\t\t\tlog(\'  linear out:               {}\'.format(self.tower_linear_outputs[i].shape))\n\t\t\tlog(\'  <stop_token> out:         {}\'.format(self.tower_stop_token_prediction[i].shape))\n\n\t\t#1_000_000 is causing syntax problems for some people?! Python please :)\n\t\tlog(\'  Tacotron Parameters       {:.3f} Million.\'.format(np.sum([np.prod(v.get_shape().as_list()) for v in self.all_vars]) / 1000000))\n\n\n\tdef add_loss(self):\n\t\t\'\'\'Adds loss to the model. Sets ""loss"" field. initialize must have been called.\'\'\'\n\t\thp = self._hparams\n\n\t\tself.tower_before_loss = []\n\t\tself.tower_after_loss= []\n\t\tself.tower_stop_token_loss = []\n\t\tself.tower_regularization_loss = []\n\t\tself.tower_linear_loss = []\n\t\tself.tower_loss = []\n\n\t\ttotal_before_loss = 0\n\t\ttotal_after_loss= 0\n\t\ttotal_stop_token_loss = 0\n\t\ttotal_regularization_loss = 0\n\t\ttotal_linear_loss = 0\n\t\ttotal_loss = 0\n\n\t\tgpus = [""/gpu:{}"".format(i) for i in range(hp.tacotron_num_gpus)]\n\n\t\tfor i in range(hp.tacotron_num_gpus):\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"", worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'loss\') as scope:\n\t\t\t\t\tif hp.mask_decoder:\n\t\t\t\t\t\t# Compute loss of predictions before postnet\n\t\t\t\t\t\tbefore = MaskedMSE(self.tower_mel_targets[i], self.tower_decoder_output[i], self.tower_targets_lengths[i],\n\t\t\t\t\t\t\thparams=self._hparams)\n\t\t\t\t\t\t# Compute loss after postnet\n\t\t\t\t\t\tafter = MaskedMSE(self.tower_mel_targets[i], self.tower_mel_outputs[i], self.tower_targets_lengths[i],\n\t\t\t\t\t\t\thparams=self._hparams)\n\t\t\t\t\t\t#Compute <stop_token> loss (for learning dynamic generation stop)\n\t\t\t\t\t\tstop_token_loss = MaskedSigmoidCrossEntropy(self.tower_stop_token_targets[i],\n\t\t\t\t\t\t\tself.tower_stop_token_prediction[i], self.tower_targets_lengths[i], hparams=self._hparams)\n\t\t\t\t\t\t#Compute masked linear loss\n\t\t\t\t\t\tif hp.predict_linear:\n\t\t\t\t\t\t\t#Compute Linear L1 mask loss (priority to low frequencies)\n\t\t\t\t\t\t\tlinear_loss = MaskedLinearLoss(self.tower_linear_targets[i], self.tower_linear_outputs[i],\n\t\t\t\t\t\t\t\tself.targets_lengths, hparams=self._hparams)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tlinear_loss=0.\n\t\t\t\t\telse:\n\t\t\t\t\t\t# Compute loss of predictions before postnet\n\t\t\t\t\t\tbefore = tf.losses.mean_squared_error(self.tower_mel_targets[i], self.tower_decoder_output[i])\n\t\t\t\t\t\t# Compute loss after postnet\n\t\t\t\t\t\tafter = tf.losses.mean_squared_error(self.tower_mel_targets[i], self.tower_mel_outputs[i])\n\t\t\t\t\t\t#Compute <stop_token> loss (for learning dynamic generation stop)\n\t\t\t\t\t\tstop_token_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n\t\t\t\t\t\t\tlabels=self.tower_stop_token_targets[i],\n\t\t\t\t\t\t\tlogits=self.tower_stop_token_prediction[i]))\n\n\t\t\t\t\t\tif hp.predict_linear:\n\t\t\t\t\t\t\t#Compute linear loss\n\t\t\t\t\t\t\t#From https://github.com/keithito/tacotron/blob/tacotron2-work-in-progress/models/tacotron.py\n\t\t\t\t\t\t\t#Prioritize loss for frequencies under 2000 Hz.\n\t\t\t\t\t\t\tl1 = tf.abs(self.tower_linear_targets[i] - self.tower_linear_outputs[i])\n\t\t\t\t\t\t\tn_priority_freq = int(2000 / (hp.sample_rate * 0.5) * hp.num_freq)\n\t\t\t\t\t\t\tlinear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(l1[:,:,0:n_priority_freq])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tlinear_loss = 0.\n\n\t\t\t\t\t# Compute the regularization weight\n\t\t\t\t\tif hp.tacotron_scale_regularization:\n\t\t\t\t\t\treg_weight_scaler = 1. / (2 * hp.max_abs_value) if hp.symmetric_mels else 1. / (hp.max_abs_value)\n\t\t\t\t\t\treg_weight = hp.tacotron_reg_weight * reg_weight_scaler\n\t\t\t\t\telse:\n\t\t\t\t\t\treg_weight = hp.tacotron_reg_weight\n\n\t\t\t\t\t# Regularize variables\n\t\t\t\t\t# Exclude all types of bias, RNN (Bengio et al. On the difficulty of training recurrent neural networks), embeddings and prediction projection layers.\n\t\t\t\t\t# Note that we consider attention mechanism v_a weights as a prediction projection layer and we don\'t regularize it. (This gave better stability)\n\t\t\t\t\tregularization = tf.add_n([tf.nn.l2_loss(v) for v in self.all_vars\n\t\t\t\t\t\tif not(\'bias\' in v.name or \'Bias\' in v.name or \'_projection\' in v.name or \'inputs_embedding\' in v.name\n\t\t\t\t\t\t\tor \'RNN\' in v.name or \'LSTM\' in v.name)]) * reg_weight\n\n\t\t\t\t\t# Compute final loss term\n\t\t\t\t\tself.tower_before_loss.append(before)\n\t\t\t\t\tself.tower_after_loss.append(after)\n\t\t\t\t\tself.tower_stop_token_loss.append(stop_token_loss)\n\t\t\t\t\tself.tower_regularization_loss.append(regularization)\n\t\t\t\t\tself.tower_linear_loss.append(linear_loss)\n\n\t\t\t\t\ttower_loss = before + after + stop_token_loss + regularization + linear_loss\n\t\t\t\t\tself.tower_loss.append(tower_loss)\n\n\t\t\ttotal_before_loss += before\n\t\t\ttotal_after_loss += after\n\t\t\ttotal_stop_token_loss += stop_token_loss\n\t\t\ttotal_regularization_loss += regularization\n\t\t\ttotal_linear_loss += linear_loss\n\t\t\ttotal_loss += tower_loss\n\n\t\tself.before_loss = total_before_loss / hp.tacotron_num_gpus\n\t\tself.after_loss = total_after_loss / hp.tacotron_num_gpus\n\t\tself.stop_token_loss = total_stop_token_loss / hp.tacotron_num_gpus\n\t\tself.regularization_loss = total_regularization_loss / hp.tacotron_num_gpus\n\t\tself.linear_loss = total_linear_loss / hp.tacotron_num_gpus\n\t\tself.loss = total_loss / hp.tacotron_num_gpus\n\n\tdef add_optimizer(self, global_step):\n\t\t\'\'\'Adds optimizer. Sets ""gradients"" and ""optimize"" fields. add_loss must have been called.\n\t\tArgs:\n\t\t\tglobal_step: int32 scalar Tensor representing current global step in training\n\t\t\'\'\'\n\t\thp = self._hparams\n\t\ttower_gradients = []\n\n\t\t# 1. Declare GPU Devices\n\t\tgpus = [""/gpu:{}"".format(i) for i in range(hp.tacotron_num_gpus)]\n\n\t\tgrad_device = \'/cpu:0\' if hp.tacotron_num_gpus > 1 else gpus[0]\n\n\t\twith tf.device(grad_device):\n\t\t\twith tf.variable_scope(\'optimizer\') as scope:\n\t\t\t\tif hp.tacotron_decay_learning_rate:\n\t\t\t\t\tself.decay_steps = hp.tacotron_decay_steps\n\t\t\t\t\tself.decay_rate = hp.tacotron_decay_rate\n\t\t\t\t\tself.learning_rate = self._learning_rate_decay(hp.tacotron_initial_learning_rate, global_step)\n\t\t\t\telse:\n\t\t\t\t\tself.learning_rate = tf.convert_to_tensor(hp.tacotron_initial_learning_rate)\n\n\t\t\t\toptimizer = tf.train.AdamOptimizer(self.learning_rate, hp.tacotron_adam_beta1,\n\t\t\t\t\thp.tacotron_adam_beta2, hp.tacotron_adam_epsilon)\n\n\t\t# 2. Compute Gradient\n\t\tfor i in range(hp.tacotron_num_gpus):\n\t\t\t#  Device placement\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=""/cpu:0"", worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'optimizer\') as scope:\n\t\t\t\t\tupdate_vars = [v for v in self.all_vars if not (\'inputs_embedding\' in v.name or \'encoder_\' in v.name)] if hp.tacotron_fine_tuning else None\n\t\t\t\t\tgradients = optimizer.compute_gradients(self.tower_loss[i], var_list=update_vars)\n\t\t\t\t\ttower_gradients.append(gradients)\n\n\t\t# 3. Average Gradient\n\t\twith tf.device(grad_device):\n\t\t\tavg_grads = []\n\t\t\tvariables = []\n\t\t\tfor grad_and_vars in zip(*tower_gradients):\n\t\t\t\t# each_grads_vars = ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n\t\t\t\tgrads = []\n\t\t\t\tfor g,_ in grad_and_vars:\n\t\t\t\t\texpanded_g = tf.expand_dims(g, 0)\n\t\t\t\t\t# Append on a \'tower\' dimension which we will average over below.\n\t\t\t\t\tgrads.append(expanded_g)\n\t\t\t\t\t# Average over the \'tower\' dimension.\n\n\t\t\t\tgrad = tf.concat(axis=0, values=grads)\n\t\t\t\tgrad = tf.reduce_mean(grad, 0)\n\n\t\t\t\tv = grad_and_vars[0][1]\n\t\t\t\tavg_grads.append(grad)\n\t\t\t\tvariables.append(v)\n\n\t\t\tself.gradients = avg_grads\n\t\t\t#Just for caution\n\t\t\t#https://github.com/Rayhane-mamah/Tacotron-2/issues/11\n\t\t\tif hp.tacotron_clip_gradients:\n\t\t\t\tclipped_gradients, _ = tf.clip_by_global_norm(avg_grads, 1.) # __mark 0.5 refer\n\t\t\telse:\n\t\t\t\tclipped_gradients = avg_grads\n\n\t\t\t# Add dependency on UPDATE_OPS; otherwise batchnorm won\'t work correctly. See:\n\t\t\t# https://github.com/tensorflow/tensorflow/issues/1122\n\t\t\twith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n\t\t\t\tself.optimize = optimizer.apply_gradients(zip(clipped_gradients, variables),\n\t\t\t\t\tglobal_step=global_step)\n\n\tdef _learning_rate_decay(self, init_lr, global_step):\n\t\t#################################################################\n\t\t# Narrow Exponential Decay:\n\n\t\t# Phase 1: lr = 1e-3\n\t\t# We only start learning rate decay after 50k steps\n\n\t\t# Phase 2: lr in ]1e-5, 1e-3[\n\t\t# decay reach minimal value at step 310k\n\n\t\t# Phase 3: lr = 1e-5\n\t\t# clip by minimal learning rate value (step > 310k)\n\t\t#################################################################\n\t\thp = self._hparams\n\n\t\t#Compute natural exponential decay\n\t\tlr = tf.train.exponential_decay(init_lr, \n\t\t\tglobal_step - hp.tacotron_start_decay, #lr = 1e-3 at step 50k\n\t\t\tself.decay_steps, \n\t\t\tself.decay_rate, #lr = 1e-5 around step 310k\n\t\t\tname=\'lr_exponential_decay\')\n\n\n\t\t#clip learning rate by max and min values (initial and final values)\n\t\treturn tf.minimum(tf.maximum(lr, hp.tacotron_final_learning_rate), init_lr)'"
tacotron/utils/__init__.py,0,"b'class ValueWindow():\n  def __init__(self, window_size=100):\n    self._window_size = window_size\n    self._values = []\n\n  def append(self, x):\n    self._values = self._values[-(self._window_size - 1):] + [x]\n\n  @property\n  def sum(self):\n    return sum(self._values)\n\n  @property\n  def count(self):\n    return len(self._values)\n\n  @property\n  def average(self):\n    return self.sum / max(1, self.count)\n\n  def reset(self):\n    self._values = []\n'"
tacotron/utils/cleaners.py,0,"b'\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\n\nfrom unidecode import unidecode\n\nfrom .numbers import normalize_numbers\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  \'\'\'lowercase input tokens.\n  \'\'\'\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  # text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
tacotron/utils/cmudict.py,0,"b'import re\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
tacotron/utils/numbers.py,0,"b""import re\n\nimport inflect\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split('.')\n  if len(parts) > 2:\n    return match + ' dollars'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    return '%s %s' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s' % (cents, cent_unit)\n  else:\n    return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return 'two thousand'\n    elif num > 2000 and num < 2010:\n      return 'two thousand ' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + ' hundred'\n    else:\n      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n  else:\n    return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r'\\1 pounds', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n"""
tacotron/utils/plot.py,0,"b'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\ndef split_title_line(title_text, max_words=5):\n\t""""""\n\tA function that splits any string based on specific character\n\t(returning it with the string), with maximum number of words on it\n\t""""""\n\tseq = title_text.split()\n\treturn \'\\n\'.join([\' \'.join(seq[i:i + max_words]) for i in range(0, len(seq), max_words)])\n\ndef plot_alignment(alignment, path, title=None, split_title=False, max_len=None):\n\tif max_len is not None:\n\t\talignment = alignment[:, :max_len]\n\n\tfig = plt.figure(figsize=(8, 6))\n\tax = fig.add_subplot(111)\n\n\tim = ax.imshow(\n\t\talignment,\n\t\taspect=\'auto\',\n\t\torigin=\'lower\',\n\t\tinterpolation=\'none\')\n\tfig.colorbar(im, ax=ax)\n\txlabel = \'Decoder timestep\'\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tplt.xlabel(xlabel)\n\tplt.title(title)\n\tplt.ylabel(\'Encoder timestep\')\n\tplt.tight_layout()\n\tplt.savefig(path, format=\'png\')\n\tplt.close()\n\n\ndef plot_spectrogram(pred_spectrogram, path, title=None, split_title=False, target_spectrogram=None, max_len=None, auto_aspect=False):\n\tif max_len is not None:\n\t\ttarget_spectrogram = target_spectrogram[:max_len]\n\t\tpred_spectrogram = pred_spectrogram[:max_len]\n\n\tif split_title:\n\t\ttitle = split_title_line(title)\n\n\tfig = plt.figure(figsize=(10, 8))\n\t# Set common labels\n\tfig.text(0.5, 0.18, title, horizontalalignment=\'center\', fontsize=16)\n\n\t#target spectrogram subplot\n\tif target_spectrogram is not None:\n\t\tax1 = fig.add_subplot(311)\n\t\tax2 = fig.add_subplot(312)\n\n\t\tif auto_aspect:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), aspect=\'auto\', interpolation=\'none\')\n\t\telse:\n\t\t\tim = ax1.imshow(np.rot90(target_spectrogram), interpolation=\'none\')\n\t\tax1.set_title(\'Target Mel-Spectrogram\')\n\t\tfig.colorbar(mappable=im, shrink=0.65, orientation=\'horizontal\', ax=ax1)\n\t\tax2.set_title(\'Predicted Mel-Spectrogram\')\n\telse:\n\t\tax2 = fig.add_subplot(211)\n\n\tif auto_aspect:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), aspect=\'auto\', interpolation=\'none\')\n\telse:\n\t\tim = ax2.imshow(np.rot90(pred_spectrogram), interpolation=\'none\')\n\tfig.colorbar(mappable=im, shrink=0.65, orientation=\'horizontal\', ax=ax2)\n\n\tplt.tight_layout()\n\tplt.savefig(path, format=\'png\')\n\tplt.close()\n'"
tacotron/utils/symbols.py,0,"b'\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom . import cmudict\n\n_pad        = \'_\'\n_eos        = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'\\""(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n#_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) #+ _arpabet\n'"
tacotron/utils/text.py,0,"b'import re\n\nfrom . import cleaners\nfrom .symbols import symbols\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\'~\'])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
wavenet_vocoder/models/__init__.py,0,"b'from .wavenet import WaveNet\nfrom warnings import warn\nfrom wavenet_vocoder.util import is_mulaw_quantize\n\ndef create_model(name, hparams, init=False):\n\tif is_mulaw_quantize(hparams.input_type):\n\t\tif hparams.out_channels != hparams.quantize_channels:\n\t\t\traise RuntimeError(\n\t\t\t\t""out_channels must equal to quantize_chennels if input_type is \'mulaw-quantize\'"")\n\n\tif name == \'WaveNet\':\n\t\treturn WaveNet(hparams, init)\n\telse:\n\t\traise Exception(\'Unknow model: {}\'.format(name))\n'"
wavenet_vocoder/models/gaussian.py,15,"b""import numpy as np\nimport tensorflow as tf\n\n\ndef gaussian_maximum_likelihood_estimation_loss(y_hat, y, log_scale_min_gauss, num_classes, use_cdf=True, reduce=True):\n\t'''compute the gaussian MLE loss'''\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(y_hat)[1], 2), tf.assert_equal(tf.rank(y_hat), 3)]):\n\t\t#[batch_size, time_steps, channels]\n\t\ty_hat = tf.transpose(y_hat, [0, 2, 1])\n\n\t#Unpack parameters: mean and log_scale outputs\n\tmean = y_hat[:, :, 0]\n\tlog_scale = tf.maximum(y_hat[:, :, 1], log_scale_min_gauss)\n\ty = tf.squeeze(y, [-1])\n\n\tif use_cdf:\n\t\t#Compute log_probs using CDF trick (Normalized loss value and more stable training than with natural log prob)\n\t\t#Instantiate a Normal distribution with model outputs\n\t\tgaussian = tf.contrib.distributions.Normal(loc=mean, scale=tf.exp(log_scale))\n\n\t\t#Draw CDF+ and CDF- neighbors to the true sample y\n\t\tcdf_plus = gaussian.cdf(y + 1. / (num_classes - 1))\n\t\tcdf_min = gaussian.cdf(y - 1. / (num_classes - 1))\n\n\t\t#Maximize the difference between CDF+ and CDF- (or its log)\n\t\tlog_prob = tf.log(tf.maximum(cdf_plus - cdf_min, 1e-12))\n\n\telse:\n\t\t#Get log probability of each sample under this distribution in a computationally stable fashion\n\t\t#This is the log(PDF)\n\t\tlog_prob = -0.5 * (np.log(2. * np.pi) + 2. * log_scale + tf.square(y - mean) * tf.exp(-2. * log_scale))\n\n\t#Loss (Maximize log probability by minimizing its negative)\n\tif reduce:\n\t\treturn -tf.reduce_sum(log_prob)\n\telse:\n\t\treturn -tf.expand_dims(log_prob, [-1])\n\ndef sample_from_gaussian(y, log_scale_min_gauss):\n\t'''sample from learned gaussian distribution'''\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(y)[1], 2)]):\n\t\t#[batch_size, time_length, channels]\n\t\ty = tf.transpose(y, [0, 2, 1])\n\n\tmean = y[:, :, 0]\n\tlog_scale = tf.maximum(y[:, :, 1], log_scale_min_gauss)\n\tscale = tf.exp(log_scale)\n\n\tgaussian_dist = tf.contrib.distributions.Normal(loc=mean, scale=scale, allow_nan_stats=False)\n\tx = gaussian_dist.sample()\n\n\treturn tf.minimum(tf.maximum(x, -1.), 1.)\n"""
wavenet_vocoder/models/mixture.py,35,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef log_sum_exp(x):\n\t"""""" numerically stable log_sum_exp implementation that prevents overflow """"""\n\taxis = len(x.get_shape())-1\n\tm = tf.reduce_max(x, axis)\n\tm2 = tf.reduce_max(x, axis, keepdims=True)\n\treturn m + tf.log(tf.reduce_sum(tf.exp(x-m2), axis))\n\ndef log_prob_from_logits(x):\n\t"""""" numerically stable log_softmax implementation that prevents overflow """"""\n\taxis = len(x.get_shape())-1\n\tm = tf.reduce_max(x, axis, keepdims=True)\n\treturn x - m - tf.log(tf.reduce_sum(tf.exp(x-m), axis, keepdims=True))\n\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=256,\n\t\tlog_scale_min=-7.0, reduce=True):\n\t\'\'\'Discretized mix of logistic distributions loss.\n\n\tNote that it is assumed that input is scaled to [-1, 1]\n\n\tArgs:\n\t\ty_hat: Tensor [batch_size, channels, time_length], predicted output.\n\t\ty: Tensor [batch_size, time_length, 1], Target.\n\tReturns:\n\t\tTensor loss\n\t\'\'\'\n\twith tf.control_dependencies([tf.assert_equal(tf.mod(tf.shape(y_hat)[1], 3), 0), tf.assert_equal(tf.rank(y_hat), 3)]):\n\t\tnr_mix = tf.shape(y_hat)[1] // 3\n\n\t#[Batch_size, time_length, channels]\n\ty_hat = tf.transpose(y_hat, [0, 2, 1])\n\n\t#unpack parameters. [batch_size, time_length, num_mixtures] x 3\n\tlogit_probs = y_hat[:, :, :nr_mix]\n\tmeans = y_hat[:, :, nr_mix:2 * nr_mix]\n\tlog_scales = tf.maximum(y_hat[:, :, 2* nr_mix: 3 * nr_mix], log_scale_min)\n\n\t#[batch_size, time_length, 1] -> [batch_size, time_length, num_mixtures]\n\ty = y * tf.ones(shape=[1, 1, nr_mix], dtype=tf.float32)\n\n\tcentered_y = y - means\n\tinv_stdv = tf.exp(-log_scales)\n\tplus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n\tcdf_plus = tf.nn.sigmoid(plus_in)\n\tmin_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n\tcdf_min = tf.nn.sigmoid(min_in)\n\n\tlog_cdf_plus = plus_in - tf.nn.softplus(plus_in) # log probability for edge case of 0 (before scaling)\n\tlog_one_minus_cdf_min = -tf.nn.softplus(min_in) # log probability for edge case of 255 (before scaling)\n\n\t#probability for all other cases\n\tcdf_delta = cdf_plus - cdf_min\n\n\tmid_in = inv_stdv * centered_y\n\t#log probability in the center of the bin, to be used in extreme cases\n\t#(not actually used in this code)\n\tlog_pdf_mid = mid_in - log_scales - 2. * tf.nn.softplus(mid_in)\n\n\tlog_probs = tf.where(y < -0.999, log_cdf_plus,\n\t\ttf.where(y > 0.999, log_one_minus_cdf_min,\n\t\t\ttf.where(cdf_delta > 1e-5,\n\t\t\t\ttf.log(tf.maximum(cdf_delta, 1e-12)),\n\t\t\t\tlog_pdf_mid - np.log((num_classes - 1) / 2))))\n\t\n\t#log_probs = log_probs + tf.nn.log_softmax(logit_probs, -1)\n\tlog_probs = log_probs + log_prob_from_logits(logit_probs)\n\n\tif reduce:\n\t\treturn -tf.reduce_sum(log_sum_exp(log_probs))\n\telse:\n\t\treturn -tf.expand_dims(log_sum_exp(log_probs), [-1])\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=-7.):\n\t\'\'\'\n\tArgs:\n\t\ty: Tensor, [batch_size, channels, time_length]\n\tReturns:\n\t\tTensor: sample in range of [-1, 1]\n\t\'\'\'\n\twith tf.control_dependencies([tf.assert_equal(tf.mod(tf.shape(y)[1], 3), 0)]):\n\t\tnr_mix = tf.shape(y)[1] // 3\n\n\t#[batch_size, time_length, channels]\n\ty = tf.transpose(y, [0, 2, 1])\n\tlogit_probs = y[:, :, :nr_mix]\n\n\t#sample mixture indicator from softmax\n\ttemp = tf.random_uniform(tf.shape(logit_probs), minval=1e-5, maxval=1. - 1e-5)\n\ttemp = logit_probs - tf.log(-tf.log(temp))\n\targmax = tf.argmax(temp, -1)\n\n\t#[batch_size, time_length] -> [batch_size, time_length, nr_mix]\n\tone_hot = tf.one_hot(argmax, depth=nr_mix, dtype=tf.float32)\n\t#select logistic parameters\n\tmeans = tf.reduce_sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, axis=-1)\n\tlog_scales = tf.maximum(tf.reduce_sum(\n\t\ty[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, axis=-1), log_scale_min)\n\n\t#sample from logistic & clip to interval\n\t#we don\'t actually round to the nearest 8-bit value when sampling\n\tu = tf.random_uniform(tf.shape(means), minval=1e-5, maxval=1. - 1e-5)\n\tx = means + tf.exp(log_scales) * (tf.log(u) - tf.log(1 -u))\n\n\treturn tf.minimum(tf.maximum(x, -1.), 1.)\n'"
wavenet_vocoder/models/modules.py,100,"b'import numpy as np\nimport tensorflow as tf\nfrom keras.utils import np_utils\nfrom wavenet_vocoder.util import sequence_mask\n\nfrom .gaussian import gaussian_maximum_likelihood_estimation_loss\nfrom .mixture import discretized_mix_logistic_loss\n\n\nclass Embedding:\n\t""""""Embedding class for global conditions.\n\t""""""\n\tdef __init__(self, num_embeddings, embedding_dim, std=0.1, name=\'gc_embedding\'):\n\t\t#Create embedding table\n\t\tself.embedding_table = tf.get_variable(name,\n\t\t\t[num_embeddings, embedding_dim], dtype=tf.float32,\n\t\t\tinitializer=tf.truncated_normal_initializer(mean=0., stddev=std))\n\n\tdef __call__(self, inputs):\n\t\t#Do the actual embedding\n\t\treturn tf.nn.embedding_lookup(self.embedding_table, inputs)\n\nclass ReluActivation:\n\t""""""Simple class to wrap relu activation function in class for later call.\n\t""""""\n\tdef __init__(self, name=None):\n\t\tself.name = name\n\n\tdef __call__(self, inputs):\n\t\treturn tf.nn.relu(inputs, name=self.name)\n\n\nclass LeakyReluActivation:\n\t\'\'\'Simple class to wrap leaky relu activation function in class for later call.\n\t\'\'\'\n\tdef __init__(self, alpha=0.3, name=None):\n\t\tself.alpha = alpha\n\t\tself.name = name\n\n\tdef __call__(self, inputs):\n\t\treturn tf.nn.leaky_relu(inputs, alpha=self.alpha, name=self.name)\n\n\nclass WeightNorm(tf.keras.layers.Wrapper):\n\t"""""" This wrapper reparameterizes a layer by decoupling the weight\'s\n\tmagnitude and direction. This speeds up convergence by improving the\n\tconditioning of the optimization problem.\n\tWeight Normalization: A Simple Reparameterization to Accelerate\n\tTraining of Deep Neural Networks: https://arxiv.org/abs/1602.07868\n\tTim Salimans, Diederik P. Kingma (2016)\n\tWeightNorm wrapper works for tf layers.\n\t```python\n\t\tnormed_layer = WeightNorm(tf.layers.Conv2D(2, 2, activation=\'relu\'),\n\t\t\t\t\t\tinput_shape=(32, 32, 3), data_init=True)\n\n\t\toutput = normed_layer(input)\n\t```\n\tArguments:\n\t\tlayer: a layer instance.\n\t\tdata_init: If `True` use data dependant variable initialization (Requires an initialization forward pass or behavior will be wrong)\n\tRaises:\n\t\tValueError: If not initialized with a `Layer` instance.\n\t\tValueError: If `Layer` does not contain a `kernel` of weights\n\t\tNotImplementedError: If `data_init` is True and running graph execution\n\t""""""\n\tdef __init__(self, layer, init=False, init_scale=1., name=None, **kwargs):\n\t\tif not isinstance(layer, tf.layers.Layer):\n\t\t\traise ValueError(\n\t\t\t\t\t\'Please initialize `WeightNorm` layer with a \'\n\t\t\t\t\t\'`Layer` instance. You passed: {input}\'.format(input=layer))\n\n\t\tself.init = init\n\t\tself.init_scale = init_scale\n\t\tself.scope = \'WeightNorm\' if name is None else name\n\n\t\tif hasattr(layer, \'kw\'):\n\t\t\tself.kw = layer.kw\n\n\t\tif hasattr(layer, \'dilation_rate\'):\n\t\t\tself.dilation_rate = layer.dilation_rate\n\n\t\tif hasattr(layer, \'filters\'):\n\t\t\tself.filters = layer.filters\n\n\t\tif hasattr(layer, \'kernel_size\'):\n\t\t\tself.kernel_size = layer.kernel_size\n\n\t\tif hasattr(layer, \'use_bias\'):\n\t\t\tself.use_bias = layer.use_bias\n\n\t\tsuper(WeightNorm, self).__init__(layer, name=name, **kwargs)\n\t\tself._track_checkpointable(layer, name=\'layer\')\n\n\tdef set_mode(self, is_training):\n\t\tself.layer.set_mode(is_training)\n\n\tdef _compute_weights(self):\n\t\t""""""Generate weights by combining the direction of weight vector\n\t\t with it\'s norm """"""\n\t\twith tf.variable_scope(\'compute_weights\'):\n\t\t\tself.layer.kernel = tf.nn.l2_normalize(\n\t\t\t\t\tself.layer.v, axis=self.norm_axes) * self.layer.g\n\n\tdef _init_norm(self, weights):\n\t\t""""""Set the norm of the weight vector""""""\n\t\twith tf.variable_scope(\'init_norm\'):\n\t\t\tflat = tf.reshape(weights, [-1, self.layer_depth])\n\t\t\treturn tf.reshape(tf.norm(flat, axis=0), (self.layer_depth,))\n\n\tdef _data_dep_init(self, inputs):\n\t\t""""""Data dependent initialization (Done by Calling a feedforward pass at step 0 of training)""""""\n\t\twith tf.variable_scope(\'data_dep_init\'):\n\t\t\t# Generate data dependant init values\n\t\t\tactivation = self.layer.activation\n\t\t\tself.layer.activation = None\n\t\t\tx_init = self.layer.call(inputs)\n\t\t\tm_init, v_init = tf.nn.moments(x_init, self.norm_axes)\n\t\t\tscale_init = self.init_scale / tf.sqrt(v_init + 1e-10)\n\n\t\t# Assign data dependant init values and return x_init\n\t\tself.layer.g = self.layer.g * scale_init\n\t\tself.layer.bias = (-m_init * scale_init)\n\t\tself.layer.activation = activation\n\t\tself.initialized = True\n\n\t\treturn x_init\n\n\n\tdef build(self, input_shape):\n\t\t""""""Build `Layer`""""""\n\t\tinput_shape = tf.TensorShape(input_shape).as_list()\n\t\tself.input_spec = tf.layers.InputSpec(shape=input_shape)\n\n\t\tif not self.layer.built:\n\t\t\tif hasattr(self, \'data_format\'):\n\t\t\t\tself.layer.data_format = self.data_format\n\n\t\t\tself.layer.build(input_shape)\n\t\t\tself.layer.built = False\n\n\t\t\tif not hasattr(self.layer, \'kernel\'):\n\t\t\t\traise ValueError(\n\t\t\t\t\t\t\'`WeightNorm` must wrap a layer that\'\n\t\t\t\t\t\t\' contains a `kernel` for weights\'\n\t\t\t\t)\n\n\t\t\t# The kernel\'s filter or unit dimension is -1\n\t\t\tself.layer_depth = int(self.layer.kernel.shape[-1])\n\t\t\tself.norm_axes = list(range(self.layer.kernel.shape.ndims - 1))\n\n\t\t\tself.kernel = self.layer.kernel\n\t\t\tself.bias = self.layer.bias\n\n\t\t\tself.layer.v = self.layer.kernel\n\t\t\tself.layer.g = self.layer.add_variable(\n\t\t\t\t\tname=""g"",\n\t\t\t\t\tshape=(self.layer_depth,),\n\t\t\t\t\tinitializer=tf.constant_initializer(1.),\n\t\t\t\t\tdtype=self.layer.kernel.dtype,\n\t\t\t\t\ttrainable=True)\n\n\t\t\twith tf.control_dependencies([self.layer.g.assign(\n\t\t\t\t\tself._init_norm(self.layer.v))]):\n\t\t\t\tself._compute_weights()\n\n\t\t\tself.layer.built = True\n\n\t\tsuper(WeightNorm, self).build()\n\t\tself.built = True\n\n\tdef call(self, inputs):\n\t\t""""""Call `Layer`""""""\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\tif self.init:\n\t\t\t\treturn self._data_dep_init(inputs)\n\t\t\telse:\n\t\t\t\treturn self.layer.call(inputs)\n\n\t# def incremental_step(self, inputs, convolution_queue=None):\n\t# \t""""""Call wrapped layer""""""\n\t# \treturn self.layer.incremental_step(inputs, convolution_queue)\n\n\nclass CausalConv1D(tf.keras.layers.Wrapper):\n\tdef __init__(self, filters,\n\t\t\t\t kernel_size,\n\t\t\t\t strides=1,\n\t\t\t\t data_format=\'channels_first\',\n\t\t\t\t dilation_rate=1,\n\t\t\t\t activation=None,\n\t\t\t\t use_bias=True,\n\t\t\t\t weight_normalization = True,\n\t\t\t\t weight_normalization_init = True,\n\t\t\t\t weight_normalization_init_scale = 1.,\n\t\t\t\t kernel_initializer=None,\n\t\t\t\t bias_initializer=tf.zeros_initializer(),\n\t\t\t\t kernel_regularizer=None,\n\t\t\t\t bias_regularizer=None,\n\t\t\t\t activity_regularizer=None,\n\t\t\t\t kernel_constraint=None,\n\t\t\t\t bias_constraint=None,\n\t\t\t\t trainable=True,\n\t\t\t\t name=None,\n\t\t\t\t **kwargs):\n\n\t\tlayer = tf.layers.Conv1D(\n\t\t\tfilters=filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstrides=strides,\n\t\t\tpadding=\'valid\',\n\t\t\tdata_format=data_format,\n\t\t\tdilation_rate=dilation_rate,\n\t\t\tactivation=activation,\n\t\t\tuse_bias=use_bias,\n\t\t\tkernel_initializer=kernel_initializer,\n\t\t\tbias_initializer=bias_initializer,\n\t\t\tkernel_regularizer=kernel_regularizer,\n\t\t\tbias_regularizer=bias_regularizer,\n\t\t\tactivity_regularizer=activity_regularizer,\n\t\t\tkernel_constraint=kernel_constraint,\n\t\t\tbias_constraint=bias_constraint,\n\t\t\ttrainable=trainable,\n\t\t\tname=name, **kwargs\n\t\t)\n\n\t\tif weight_normalization:\n\t\t\tlayer = WeightNorm(layer, weight_normalization_init, weight_normalization_init_scale)\n\n\t\tsuper(CausalConv1D, self).__init__(layer, name=name, **kwargs)\n\t\tself._track_checkpointable(layer, name=\'layer\')\n\t\tself.kw = kernel_size\n\t\tself.dilation_rate = self.layer.dilation_rate\n\t\tself.scope = \'CausalConv1D\' if name is None else name\n\n\tdef set_mode(self, is_training):\n\t\tself.training = is_training\n\n\tdef _get_linearized_weight(self, in_channels):\n\t\t#layers.Conv1D\n\t\tif tf.shape(self.layer.kernel) == (self.layer.filters, in_channels, self.kw):\n\t\t\t#[filters, in, kw]\n\t\t\tweight = tf.transpose(self.layer.kernel, [2, 1, 0])\n\t\telse:\n\t\t\t#[kw, in, filters]\n\t\t\tweight = self.layer.kernel\n\n\t\t#[kw, in, filters]\n\t\tassert weight.shape == (self.kw, in_channels, self.layer.filters)\n\t\tself.in_channels = in_channels\n\n\t\treturn tf.cast(tf.reshape(weight, [-1, self.layer.filters]), dtype=tf.float32)\n\n\tdef build(self, input_shape):\n\t\t""""""Build `Layer`""""""\n\t\tinput_shape = tf.TensorShape(input_shape).as_list()\n\t\tself.input_spec = tf.layers.InputSpec(shape=input_shape)\n\n\t\tself.layer.data_format = \'channels_first\' if self.training else \'channels_last\'\n\t\tin_channels = input_shape[1] if self.layer.data_format == \'channels_first\' else input_shape[-1]\n\n\t\t#Build layer\n\t\tself.layer.build(input_shape)\n\t\tself.built = False\n\n\t\t#Create Linearized weights\n\t\tself.linearized_weights = self._get_linearized_weight(in_channels)\n\t\tsuper(CausalConv1D, self).build()\n\t\tself.built = True\n\t\t \n\tdef call(self, inputs, incremental=False, convolution_queue=None):\n\t\t""""""Call \'Layer\'""""""\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\tif incremental:\n\t\t\t\t#Incremental run\n\t\t\t\t#input [batch_size, time_length, channels]\n\t\t\t\tif self.training:\n\t\t\t\t\traise RuntimeError(\'incremental step only supported during synthesis\')\n\n\t\t\t\tbatch_size = tf.shape(inputs)[0]\n\n\t\t\t\t#Fast dilation\n\t\t\t\t#Similar to using tf FIFOQueue to schedule states of dilated convolutions\n\t\t\t\tif self.kw > 1:\n\t\t\t\t\t#shift queue (remove first element for following append)\n\t\t\t\t\tconvolution_queue = convolution_queue[:, 1:, :]\n\n\t\t\t\t\t#append next input\n\t\t\t\t\tconvolution_queue = tf.concat([convolution_queue, tf.expand_dims(inputs[:, -1, :], axis=1)], axis=1)\n\n\t\t\t\t\tinputs = convolution_queue\n\t\t\t\t\tif self.dilation_rate[0] > 1:\n\t\t\t\t\t\tinputs = inputs[:, 0::self.dilation_rate[0], :]\n\n\t\t\t\t#Compute step prediction\n\t\t\t\toutput = tf.matmul(tf.reshape(inputs, [batch_size, -1]), self.linearized_weights)\n\t\t\t\tif self.layer.use_bias:\n\t\t\t\t\toutput = tf.nn.bias_add(output, self.layer.bias)\n\n\t\t\t\t#[batch_size, 1(time_step), channels(filters)]\n\t\t\t\tif convolution_queue is None:\n\t\t\t\t\treturn tf.reshape(output, [batch_size, 1, self.layer.filters])\n\t\t\t\telse:\n\t\t\t\t\treturn [tf.reshape(output, [batch_size, 1, self.layer.filters]), convolution_queue]\n\n\t\t\t#Normal run\n\t\t\t#Causal convolutions are only padded on the left side\n\t\t\tassert self.layer.kernel_size[0] == self.kw\n\t\t\tpadding = (self.kw - 1) * self.dilation_rate[0]\n\n\t\t\t#Pad depending on data format\n\t\t\tif self.layer.data_format == \'channels_first\':\n\t\t\t\ttime_dim = -1\n\t\t\t\tinputs_ = tf.pad(inputs, tf.constant([(0, 0), (0, 0), (padding, 0)]))\n\t\t\telse:\n\t\t\t\tassert self.layer.data_format == \'channels_last\'\n\t\t\t\ttime_dim = 1\n\t\t\t\tinputs_ = tf.pad(inputs, tf.constant([(0, 0), (padding, 0), (0, 0)]))\n\n\t\t\t#Compute convolution\n\t\t\toutputs = self.layer.call(inputs_)\n\n\t\t\t#Assert time step consistency\n\t\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(outputs)[time_dim], tf.shape(inputs)[time_dim])]):\n\t\t\t\toutputs = tf.identity(outputs, name=\'time_dimension_check\')\n\t\t\treturn outputs\n\n\tdef incremental_step(self, inputs, convolution_queue=None):\n\t\t\'\'\'At sequential inference times:\n\t\twe adopt fast wavenet convolution queues approach by saving pre-computed states for faster generation\n\n\t\tinputs: [batch_size, time_length, channels] (\'NWC\')! Channels last!\n\t\t\'\'\'\n\t\treturn self(inputs, incremental=True, convolution_queue=convolution_queue)\n\n\nclass Conv1D1x1(CausalConv1D):\n\t""""""Conv1D 1x1 is literally a causal convolution with kernel_size = 1""""""\n\tdef __init__(self, filters,\n\t\t\t\t strides=1,\n\t\t\t\t data_format=\'channels_first\',\n\t\t\t\t dilation_rate=1,\n\t\t\t\t activation=None,\n\t\t\t\t use_bias=True,\n\t\t\t\t weight_normalization = True,\n\t\t\t\t weight_normalization_init = True,\n\t\t\t\t weight_normalization_init_scale = 1.,\n\t\t\t\t kernel_initializer=None,\n\t\t\t\t bias_initializer=tf.zeros_initializer(),\n\t\t\t\t kernel_regularizer=None,\n\t\t\t\t bias_regularizer=None,\n\t\t\t\t activity_regularizer=None,\n\t\t\t\t kernel_constraint=None,\n\t\t\t\t bias_constraint=None,\n\t\t\t\t trainable=True,\n\t\t\t\t name=None,\n\t\t\t\t **kwargs):\n\t\tsuper(Conv1D1x1, self).__init__(\n\t\t\tfilters=filters,\n\t\t\tkernel_size=1,\n\t\t\tstrides=strides,\n\t\t\tdata_format=data_format,\n\t\t\tdilation_rate=dilation_rate,\n\t\t\tactivation=activation,\n\t\t\tuse_bias=use_bias,\n\t\t\tweight_normalization = weight_normalization,\n\t\t\tweight_normalization_init = weight_normalization_init,\n\t\t\tweight_normalization_init_scale = weight_normalization_init_scale,\n\t\t\tkernel_initializer=kernel_initializer,\n\t\t\tbias_initializer=bias_initializer,\n\t\t\tkernel_regularizer=kernel_regularizer,\n\t\t\tbias_regularizer=bias_regularizer,\n\t\t\tactivity_regularizer=activity_regularizer,\n\t\t\tkernel_constraint=kernel_constraint,\n\t\t\tbias_constraint=bias_constraint,\n\t\t\ttrainable=trainable,\n\t\t\tname=name, **kwargs\n\t\t)\n\n\t\tself.scope = \'Conv1D1x1\' if name is None else name\n\n\tdef call(self, inputs, incremental=False, convolution_queue=None):\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\t#Call parent class call function\n\t\t\treturn super(Conv1D1x1, self).call(inputs, incremental=incremental, convolution_queue=convolution_queue)\n\n\tdef incremental_step(self, inputs, unused_queue=None):\n\t\t#Call parent class incremental function\n\t\toutput = self(inputs, incremental=True, convolution_queue=unused_queue) #Drop unused queue\n\t\treturn output\n\n\nclass ResidualConv1DGLU(tf.keras.layers.Wrapper):\n\t\'\'\'Dilated conv1d + Gated Linear Unit + condition convolutions + residual and skip convolutions\n\n\tDilated convolution is considered to be the most important part of the block so we use it as main layer\n\t\'\'\'\n\tdef __init__(self, residual_channels, gate_channels, kernel_size,\n\t\t\tskip_out_channels=None, cin_channels=-1, gin_channels=-1,\n\t\t\tdropout=1 - .95, dilation_rate=1, use_bias=True,\n\t\t\tweight_normalization=True, init=False, init_scale=1., residual_legacy=True, name=\'ResidualConv1DGLU\', **kwargs):\n\t\tself.dropout = dropout\n\t\tself.scope = name\n\n\t\tif skip_out_channels is None:\n\t\t\tskip_out_channels = residual_channels\n\n\t\tconv = CausalConv1D(gate_channels, kernel_size,\n\t\t\tdilation_rate=dilation_rate, use_bias=use_bias, \n\t\t\tweight_normalization=weight_normalization, \n\t\t\tweight_normalization_init=init, \n\t\t\tweight_normalization_init_scale=init_scale,\n\t\t\tname=\'residual_block_causal_conv_{}\'.format(name))\n\n\n\t\t#Local conditioning\n\t\tif cin_channels > 0:\n\t\t\tself.conv1x1c = Conv1D1x1(gate_channels, use_bias=use_bias,\n\t\t\t\tweight_normalization=weight_normalization, \n\t\t\t\tweight_normalization_init=init, \n\t\t\t\tweight_normalization_init_scale=init_scale, \n\t\t\t\tname=\'residual_block_cin_conv_{}\'.format(name))\n\n\t\telse:\n\t\t\tself.conv1x1c = None\n\n\t\t#Global conditioning\n\t\tif gin_channels > 0:\n\t\t\tself.conv1x1g = Conv1D1x1(gate_channels, use_bias=use_bias,\n\t\t\t\tweight_normalization=weight_normalization, \n\t\t\t\tweight_normalization_init=init, \n\t\t\t\tweight_normalization_init_scale=init_scale,\n\t\t\t\tname=\'residual_block_gin_conv_{}\'.format(name))\n\n\t\telse:\n\t\t\tself.conv1x1g = None\n\n\n\t\tgate_out_channels = gate_channels // 2\n\n\t\tself.conv1x1_out = Conv1D1x1(residual_channels, use_bias=use_bias, \n\t\t\tweight_normalization=weight_normalization, \n\t\t\tweight_normalization_init=init, \n\t\t\tweight_normalization_init_scale=init_scale,\n\t\t\tname=\'residual_block_out_conv_{}\'.format(name))\n\n\t\tself.conv1x1_skip = Conv1D1x1(skip_out_channels, use_bias=use_bias, \n\t\t\tweight_normalization=weight_normalization, \n\t\t\tweight_normalization_init=init, \n\t\t\tweight_normalization_init_scale=init_scale,\n\t\t\tname=\'residual_block_skip_conv_{}\'.format(name))\n\n\t\tsuper(ResidualConv1DGLU, self).__init__(conv, name=name, **kwargs)\n\t\tself.residual_legacy = residual_legacy\n\t\tself.scope = name\n\n\tdef set_mode(self, is_training):\n\t\tfor conv in [self.layer, self.conv1x1c, self.conv1x1g, self.conv1x1_out, self.conv1x1_skip]:\n\t\t\ttry:\n\t\t\t\tconv.set_mode(is_training)\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\n\n\tdef call(self, x, c=None, g=None):\n\t\tx, s, _ = self.step(x, c=c, g=g, is_incremental=False)\n\t\treturn [x, s]\n\n\tdef incremental_step(self, x, c=None, g=None, queue=None):\n\t\treturn self.step(x, c=c, g=g, is_incremental=True, queue=queue)\n\n\tdef step(self, x, c, g, is_incremental, queue=None):\n\t\t\'\'\'\n\n\t\tArgs:\n\t\t\tx: Tensor [batch_size, channels, time_length]\n\t\t\tc: Tensor [batch_size, c_channels, time_length]. Local conditioning features\n\t\t\tg: Tensor [batch_size, g_channels, time_length], global conditioning features\n\t\t\tis_incremental: Boolean, whether incremental mode is on\n\t\tReturns:\n\t\t\tTensor output\n\t\t\'\'\'\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\tresidual = x\n\t\t\tx = tf.layers.dropout(x, rate=self.dropout, training=not is_incremental)\n\t\t\tif is_incremental:\n\t\t\t\tsplitdim = -1\n\t\t\t\tx, queue = self.layer.incremental_step(x, queue)\n\t\t\telse:\n\t\t\t\tsplitdim = 1\n\t\t\t\tx = self.layer(x)\n\t\t\t\t#Remove future time steps (They normally don\'t exist but for safety)\n\t\t\t\tx = x[:, :, :tf.shape(residual)[-1]]\n\n\t\t\ta, b = tf.split(x, num_or_size_splits=2, axis=splitdim)\n\n\t\t\t#local conditioning\n\t\t\tif c is not None:\n\t\t\t\tassert self.conv1x1c is not None\n\t\t\t\tc = _conv1x1_forward(self.conv1x1c, c, is_incremental)\n\t\t\t\tca, cb = tf.split(c, num_or_size_splits=2, axis=splitdim)\n\t\t\t\ta, b = a + ca, b + cb\n\n\t\t\t#global conditioning\n\t\t\tif g is not None:\n\t\t\t\tassert self.conv1x1g is not None\n\t\t\t\tg = _conv1x1_forward(self.conv1x1g, g, is_incremental)\n\t\t\t\tga, gb = tf.split(g, num_or_size_splits=2, axis=splitdim)\n\t\t\t\ta, b = a + ga, b + gb\n\n\t\t\tx = tf.nn.tanh(a) * tf.nn.sigmoid(b)\n\t\t\t#For Skip connection\n\t\t\ts = _conv1x1_forward(self.conv1x1_skip, x, is_incremental)\n\n\t\t\t#For Residual connection\n\t\t\tx = _conv1x1_forward(self.conv1x1_out, x, is_incremental)\n\n\t\t\tif self.residual_legacy:\n\t\t\t\tx = (x + residual) * np.sqrt(0.5)\n\t\t\telse:\n\t\t\t\tx = (x + residual)\n\t\t\treturn x, s, queue\n\n\nclass NearestNeighborUpsample:\n\tdef __init__(self, strides):\n\t\t#Save upsample params\n\t\tself.resize_strides = strides\n\n\tdef __call__(self, inputs):\n\t\t#inputs are supposed [batch_size, freq, time_steps, channels]\n\t\toutputs = tf.image.resize_images(\n\t\t\tinputs,\n\t\t\tsize=[inputs.shape[1] * self.resize_strides[0], tf.shape(inputs)[2] * self.resize_strides[1]],\n\t\t\tmethod=1) #BILINEAR = 0, NEAREST_NEIGHBOR = 1, BICUBIC = 2, AREA = 3\n\n\t\treturn outputs\n\n\nclass SubPixelConvolution(tf.layers.Conv2D):\n\t\'\'\'Sub-Pixel Convolutions are vanilla convolutions followed by Periodic Shuffle.\n\n\tThey serve the purpose of upsampling (like deconvolutions) but are faster and less prone to checkerboard artifact with the right initialization.\n\tIn contrast to ResizeConvolutions, SubPixel have the same computation speed (when using same n\xc2\xb0 of params), but a larger receptive fields as they operate on low resolution.\n\t\'\'\'\n\tdef __init__(self, filters, kernel_size, padding, strides, NN_init, NN_scaler, up_layers, name=None, **kwargs):\n\t\t#Output channels = filters * H_upsample * W_upsample\n\t\tconv_filters = filters * strides[0] * strides[1]\n\n\t\t#Create initial kernel\n\t\tself.NN_init = NN_init\n\t\tself.up_layers = up_layers\n\t\tself.NN_scaler = NN_scaler\n\t\tinit_kernel = tf.constant_initializer(self._init_kernel(kernel_size, strides, conv_filters), dtype=tf.float32) if NN_init else None\n\n\t\t#Build convolution component and save Shuffle parameters.\n\t\tsuper(SubPixelConvolution, self).__init__(\n\t\t\tfilters=conv_filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstrides=(1, 1),\n\t\t\tpadding=padding,\n\t\t\tkernel_initializer=init_kernel,\n\t\t\tbias_initializer=tf.zeros_initializer(),\n\t\t\tdata_format=\'channels_last\',\n\t\t\tname=name, **kwargs)\n\n\t\tself.out_filters = filters\n\t\tself.shuffle_strides = strides\n\t\tself.scope = \'SubPixelConvolution\' if None else name\n\n\tdef build(self, input_shape):\n\t\t\'\'\'Build SubPixel initial weights (ICNR: avoid checkerboard artifacts).\n\n\t\tTo ensure checkerboard free SubPixel Conv, initial weights must make the subpixel conv equivalent to conv->NN resize.\n\t\tTo do that, we replace initial kernel with the special kernel W_n == W_0 for all n <= out_channels.\n\t\tIn other words, we want our initial kernel to extract feature maps then apply Nearest neighbor upsampling.\n\t\tNN upsampling is guaranteed to happen when we force all our output channels to be equal (neighbor pixels are duplicated).\n\t\tWe can think of this as limiting our initial subpixel conv to a low resolution conv (1 channel) followed by a duplication (made by PS).\n\n\t\tRef: https://arxiv.org/pdf/1707.02937.pdf\n\t\t\'\'\'\n\t\t#Initialize layer\n\t\tsuper(SubPixelConvolution, self).build(input_shape)\n\n\t\tif not self.NN_init:\n\t\t\t#If no NN init is used, ensure all channel-wise parameters are equal.\n\t\t\tself.built = False\n\n\t\t\t#Get W_0 which is the first filter of the first output channels\n\t\t\tW_0 = tf.expand_dims(self.kernel[:, :, :, 0], axis=3) #[H_k, W_k, in_c, 1]\n\n\t\t\t#Tile W_0 across all output channels and replace original kernel\n\t\t\tself.kernel = tf.tile(W_0, [1, 1, 1, self.filters]) #[H_k, W_k, in_c, out_c]\n\n\t\tself.built = True\n\n\tdef call(self, inputs):\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\t#Inputs are supposed [batch_size, freq, time_steps, channels]\n\t\t\tconvolved = super(SubPixelConvolution, self).call(inputs)\n\n\t\t\t#[batch_size, up_freq, up_time_steps, channels]\n\t\t\treturn self.PS(convolved)\n\n\tdef PS(self, inputs):\n\t\t#Get different shapes\n\t\t#[batch_size, H, W, C(out_c * r1 * r2)]\n\t\tbatch_size = tf.shape(inputs)[0]\n\t\tH = inputs.shape[1]\n\t\tW = tf.shape(inputs)[2]\n\t\tC = inputs.shape[-1]\n\t\tr1, r2 = self.shuffle_strides #supposing strides = (freq_stride, time_stride)\n\t\tout_c = self.out_filters #number of filters as output of the convolution (usually 1 for this model)\n\n\t\tassert C == r1 * r2 * out_c\n\n\t\t#Split and shuffle (output) channels separately. (Split-Concat block)\n\t\tXc = tf.split(inputs, out_c, axis=3) # out_c x [batch_size, H, W, C/out_c]\n\t\toutputs = tf.concat([self._phase_shift(x, batch_size, H, W, r1, r2) for x in Xc], 3) #[batch_size, r1 * H, r2 * W, out_c]\n\n\t\twith tf.control_dependencies([tf.assert_equal(out_c, tf.shape(outputs)[-1]),\n\t\t\ttf.assert_equal(H * r1, tf.shape(outputs)[1])]):\n\t\t\toutputs = tf.identity(outputs, name=\'SubPixelConv_output_check\')\n\n\t\treturn tf.reshape(outputs, [tf.shape(outputs)[0], r1 * H, tf.shape(outputs)[2], out_c])\n\n\tdef _phase_shift(self, inputs, batch_size, H, W, r1, r2):\n\t\t#Do a periodic shuffle on each output channel separately\n\t\tx = tf.reshape(inputs, [batch_size, H, W, r1, r2]) #[batch_size, H, W, r1, r2]\n\n\t\t#Width dim shuffle\n\t\tx = tf.transpose(x, [4, 2, 3, 1, 0]) #[r2, W, r1, H, batch_size]\n\t\tx = tf.batch_to_space_nd(x, [r2], [[0, 0]]) #[1, r2*W, r1, H, batch_size]\n\t\tx = tf.squeeze(x, [0]) #[r2*W, r1, H, batch_size]\n\n\t\t#Height dim shuffle\n\t\tx = tf.transpose(x, [1, 2, 0, 3]) #[r1, H, r2*W, batch_size]\n\t\tx = tf.batch_to_space_nd(x, [r1], [[0, 0]]) #[1, r1*H, r2*W, batch_size]\n\t\tx = tf.transpose(x, [3, 1, 2, 0]) #[batch_size, r1*H, r2*W, 1]\n\n\t\treturn x\n\n\tdef _init_kernel(self, kernel_size, strides, filters):\n\t\t\'\'\'Nearest Neighbor Upsample (Checkerboard free) init kernel size\n\t\t\'\'\'\n\t\toverlap = kernel_size[1] // strides[1]\n\t\tinit_kernel = np.zeros(kernel_size, dtype=np.float32)\n\t\ti = kernel_size[0] // 2\n\t\tj = [kernel_size[1] // 2 - 1, kernel_size[1] // 2] if kernel_size[1] % 2 == 0 else [kernel_size[1] // 2]\n\t\tfor j_i in j:\n\t\t\tinit_kernel[i, j_i] = 1. / max(overlap, 1.) if kernel_size[1] % 2 == 0 else 1.\n\n\t\tinit_kernel = np.tile(np.expand_dims(init_kernel, 3), [1, 1, 1, filters])\n\n\t\treturn init_kernel * (self.NN_scaler)**(1/self.up_layers)\n\n\nclass ResizeConvolution(tf.layers.Conv2D):\n\tdef __init__(self, filters, kernel_size, padding, strides, NN_init, NN_scaler, up_layers, name=None, **kwargs):\n\t\t#Create initial kernel\n\t\tself.up_layers = up_layers\n\t\tself.NN_scaler = NN_scaler\n\t\tinit_kernel = tf.constant_initializer(self._init_kernel(kernel_size, strides), dtype=tf.float32) if NN_init else None\n\n\t\t#Build convolution component and save resize parameters\n\t\tsuper(ResizeConvolution, self).__init__(\n\t\t\tfilters=filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstrides=(1, 1),\n\t\t\tpadding=padding,\n\t\t\tkernel_initializer=init_kernel,\n\t\t\tbias_initializer=tf.zeros_initializer(),\n\t\t\tdata_format=\'channels_last\',\n\t\t\tname=name, **kwargs)\n\n\t\tself.resize_layer = NearestNeighborUpsample(strides=strides)\n\t\tself.scope = \'ResizeConvolution\' if None else name\n\n\tdef call(self, inputs):\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\t#Inputs are supposed [batch_size, freq, time_steps, channels]\n\t\t\tresized = self.resize_layer(inputs)\n\n\t\t\treturn super(ResizeConvolution, self).call(resized)\n\n\tdef _init_kernel(self, kernel_size, strides):\n\t\t\'\'\'Nearest Neighbor Upsample (Checkerboard free) init kernel size\n\t\t\'\'\'\n\t\toverlap = kernel_size[1] // strides[1]\n\t\tinit_kernel = np.zeros(kernel_size, dtype=np.float32)\n\t\ti = kernel_size[0] // 2\n\t\tj = [kernel_size[1] // 2 - 1, kernel_size[1] // 2] if kernel_size[1] % 2 == 0 else [kernel_size[1] // 2]\n\t\tfor j_i in j:\n\t\t\tinit_kernel[i, j_i] = 1. / max(overlap, 1.) if kernel_size[1] % 2 == 0 else 1.\n\n\t\treturn init_kernel * (self.NN_scaler)**(1/self.up_layers)\n\nclass ConvTranspose1D(tf.layers.Conv2DTranspose):\n\tdef __init__(self, filters, kernel_size, padding, strides, NN_init, NN_scaler, up_layers, name=None, **kwargs):\n\t\t#convert 1D filters to 2D.\n\t\tkernel_size = (1, ) + kernel_size #(ks, ) -> (1, ks). Inputs supposed [batch_size, channels, freq, time_steps].\n\t\tstrides = (1, ) + strides #(s, ) -> (1, s).\n\n\t\t#Create initial kernel\n\t\tself.up_layers = up_layers\n\t\tself.NN_scaler = NN_scaler\n\t\tinit_kernel = tf.constant_initializer(self._init_kernel(kernel_size, strides, filters), dtype=tf.float32) if NN_init else None\n\n\t\tsuper(ConvTranspose1D, self).__init__(\n\t\t\tfilters=filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstrides=strides,\n\t\t\tpadding=padding,\n\t\t\tkernel_initializer=init_kernel,\n\t\t\tbias_initializer=tf.zeros_initializer(),\n\t\t\tdata_format=\'channels_first\',\n\t\t\tname=name, **kwargs)\n\n\t\tself.scope = \'ConvTranspose1D\' if None else name\n\n\tdef call(self, inputs):\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\treturn super(ConvTranspose1D, self).call(inputs)\n\n\tdef _init_kernel(self, kernel_size, strides, filters):\n\t\t\'\'\'Nearest Neighbor Upsample (Checkerboard free) init kernel size\n\t\t\'\'\'\n\t\toverlap = float(kernel_size[1] // strides[1])\n\t\tinit_kernel = np.arange(filters)\n\t\tinit_kernel = np_utils.to_categorical(init_kernel, num_classes=len(init_kernel)).reshape(1, 1, -1, filters).astype(np.float32)\n\t\tinit_kernel = np.tile(init_kernel, [kernel_size[0], kernel_size[1], 1, 1])\n\t\tinit_kernel = init_kernel / max(overlap, 1.) if kernel_size[1] % 2 == 0 else init_kernel\n\n\t\treturn init_kernel * (self.NN_scaler)**(1/self.up_layers)\n\n\nclass ConvTranspose2D(tf.layers.Conv2DTranspose):\n\tdef __init__(self, filters, kernel_size, padding, strides, NN_init, NN_scaler, up_layers, name=None, **kwargs):\n\t\tfreq_axis_kernel_size = kernel_size[0]\n\n\t\t#Create initial kernel\n\t\tself.up_layers = up_layers\n\t\tself.NN_scaler = NN_scaler\n\t\tinit_kernel = tf.constant_initializer(self._init_kernel(kernel_size, strides), dtype=tf.float32) if NN_init else None\n\n\t\tsuper(ConvTranspose2D, self).__init__(\n\t\t\tfilters=filters,\n\t\t\tkernel_size=kernel_size,\n\t\t\tstrides=strides,\n\t\t\tpadding=padding,\n\t\t\tkernel_initializer=init_kernel,\n\t\t\tbias_initializer=tf.zeros_initializer(),\n\t\t\tdata_format=\'channels_first\',\n\t\t\tname=name, **kwargs)\n\n\t\tself.scope = \'ConvTranspose2D\' if None else name\n\n\tdef call(self, inputs):\n\t\twith tf.variable_scope(self.scope) as scope:\n\t\t\treturn super(ConvTranspose2D, self).call(inputs)\n\n\tdef _init_kernel(self, kernel_size, strides):\n\t\t\'\'\'Nearest Neighbor Upsample (Checkerboard free) init kernel size\n\t\t\'\'\'\n\t\toverlap = kernel_size[1] // strides[1]\n\t\tinit_kernel = np.zeros(kernel_size, dtype=np.float32)\n\t\ti = kernel_size[0] // 2\n\t\tfor j_i in range(kernel_size[1]):\n\t\t\tinit_kernel[i, j_i] = 1. / max(overlap, 1.) if kernel_size[1] % 2 == 0 else 1.\n\n\t\treturn init_kernel * (self.NN_scaler)**(1/self.up_layers)\n\n\ndef _conv1x1_forward(conv, x, is_incremental):\n\t""""""conv1x1 step\n\t""""""\n\tif is_incremental:\n\t\treturn conv.incremental_step(x)\n\telse:\n\t\treturn conv(x)\n\ndef MaskedCrossEntropyLoss(outputs, targets, lengths=None, mask=None, max_len=None):\n\tif lengths is None and mask is None:\n\t\traise RuntimeError(\'Please provide either lengths or mask\')\n\n\t#[batch_size, time_length]\n\tif mask is None:\n\t\tmask = sequence_mask(lengths, max_len, False)\n\n\t#One hot encode targets (outputs.shape[-1] = hparams.quantize_channels)\n\ttargets_ = tf.one_hot(targets, depth=tf.shape(outputs)[-1])\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(outputs), tf.shape(targets_))]):\n\t\tlosses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets_)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(mask), tf.shape(losses))]):\n\t\tmasked_loss = losses * mask\n\n\treturn tf.reduce_sum(masked_loss) / tf.count_nonzero(masked_loss, dtype=tf.float32)\n\ndef DiscretizedMixtureLogisticLoss(outputs, targets, hparams, lengths=None, mask=None, max_len=None):\n\tif lengths is None and mask is None:\n\t\traise RuntimeError(\'Please provide either lengths or mask\')\n\n\t#[batch_size, time_length, 1]\n\tif mask is None:\n\t\tmask = sequence_mask(lengths, max_len, True)\n\n\t#[batch_size, time_length, dimension]\n\tones = tf.ones([tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]], tf.float32)\n\tmask_ = mask * ones\n\n\tlosses = discretized_mix_logistic_loss(\n\t\toutputs, targets, num_classes=hparams.quantize_channels,\n\t\tlog_scale_min=hparams.log_scale_min, reduce=False)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(losses), tf.shape(targets))]):\n\t\treturn tf.reduce_sum(losses * mask_) / tf.reduce_sum(mask_)\n\ndef GaussianMaximumLikelihoodEstimation(outputs, targets, hparams, lengths=None, mask=None, max_len=None):\n\tif lengths is None and mask is None:\n\t\traise RuntimeError(\'Please provide either lengths or mask\')\n\n\t#[batch_size, time_length, 1]\n\tif mask is None:\n\t\tmask = sequence_mask(lengths, max_len, True)\n\n\t#[batch_size, time_length, dimension]\n\tones = tf.ones([tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]], tf.float32)\n\tmask_ = mask * ones\n\n\tlosses = gaussian_maximum_likelihood_estimation_loss(\n\t\toutputs, targets, log_scale_min_gauss=hparams.log_scale_min_gauss,\n\t\tnum_classes=hparams.quantize_channels, use_cdf=hparams.cdf_loss, reduce=False)\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(losses), tf.shape(targets))]):\n\t\treturn tf.reduce_sum(losses * mask_) / tf.reduce_sum(mask_)\n\n\ndef MaskedMeanSquaredError(outputs, targets, lengths=None, mask=None, max_len=None):\n\tif lengths is None and mask is None:\n\t\traise RuntimeError(\'Please provide either lengths or mask\')\n\n\t#[batch_size, frames, 1]\n\tif mask is None:\n\t\tmask = sequence_mask(lengths, max_len, True)\n\n\t#[batch_size, frames, freq]\n\tones = tf.ones([tf.shape(mask)[0], tf.shape(mask)[1], tf.shape(targets)[-1]], tf.float32)\n\tmask_ = mask * ones\n\n\twith tf.control_dependencies([tf.assert_equal(tf.shape(targets), tf.shape(mask_))]):\n\t\treturn tf.losses.mean_squared_error(labels=targets, predictions=outputs, weights=mask_)'"
wavenet_vocoder/models/wavenet.py,132,"b'import numpy as np\nimport tensorflow as tf\nfrom datasets import audio\nfrom infolog import log\nfrom wavenet_vocoder import util\nfrom wavenet_vocoder.util import *\n\nfrom .gaussian import sample_from_gaussian\nfrom .mixture import sample_from_discretized_mix_logistic\nfrom .modules import (Conv1D1x1, ConvTranspose2D, ConvTranspose1D, ResizeConvolution, SubPixelConvolution, NearestNeighborUpsample, DiscretizedMixtureLogisticLoss, \n\tGaussianMaximumLikelihoodEstimation, MaskedMeanSquaredError, LeakyReluActivation, MaskedCrossEntropyLoss, ReluActivation, ResidualConv1DGLU, WeightNorm, Embedding)\n\n\ndef _expand_global_features(batch_size, time_length, global_features, data_format=\'BCT\'):\n\t""""""Expand global conditioning features to all time steps\n\n\tArgs:\n\t\tbatch_size: int\n\t\ttime_length: int\n\t\tglobal_features: Tensor of shape [batch_size, channels] or [batch_size, channels, 1]\n\t\tdata_format: string, \'BCT\' to get output of shape [batch_size, channels, time_length]\n\t\t\tor \'BTC\' to get output of shape [batch_size, time_length, channels]\n\n\tReturns:\n\t\tNone or Tensor of shape [batch_size, channels, time_length] or [batch_size, time_length, channels]\n\t""""""\n\taccepted_formats = [\'BCT\', \'BTC\']\n\tif not (data_format in accepted_formats):\n\t\traise ValueError(\'{} is an unknow data format, accepted formats are ""BCT"" and ""BTC""\'.format(data_format))\n\n\tif global_features is None:\n\t\treturn None\n\n\t#[batch_size, channels] ==> [batch_size, channels, 1]\n\t# g = tf.cond(tf.equal(tf.rank(global_features), 2),\n\t# \tlambda: tf.expand_dims(global_features, axis=-1),\n\t# \tlambda: global_features)\n\tg = tf.reshape(global_features, [tf.shape(global_features)[0], tf.shape(global_features)[1], 1])\n\tg_shape = tf.shape(g)\n\n\t#[batch_size, channels, 1] ==> [batch_size, channels, time_length]\n\t# ones = tf.ones([g_shape[0], g_shape[1], time_length], tf.int32)\n\t# g = g * ones\n\tg = tf.tile(g, [1, 1, time_length])\n\n\tif data_format == \'BCT\':\n\t\treturn g\n\n\telse:\n\t\t#[batch_size, channels, time_length] ==> [batch_size, time_length, channels]\n\t\treturn tf.transpose(g, [0, 2, 1])\n\n\ndef receptive_field_size(total_layers, num_cycles, kernel_size, dilation=lambda x: 2**x):\n\t""""""Compute receptive field size.\n\n\tArgs:\n\t\ttotal_layers; int\n\t\tnum_cycles: int\n\t\tkernel_size: int\n\t\tdilation: callable, function used to compute dilation factor.\n\t\t\tuse ""lambda x: 1"" to disable dilated convolutions.\n\n\tReturns:\n\t\tint: receptive field size in sample.\n\t""""""\n\tassert total_layers % num_cycles == 0\n\n\tlayers_per_cycle = total_layers // num_cycles\n\tdilations = [dilation(i % layers_per_cycle) for i in range(total_layers)]\n\treturn (kernel_size - 1) * sum(dilations) + 1\n\ndef maybe_Normalize_weights(layer, weight_normalization=True, init=False, init_scale=1.):\n\t""""""Maybe Wraps layer with Weight Normalization wrapper.\n\n\tArgs;\n\t\tlayer: tf layers instance, the layer candidate for normalization\n\t\tweight_normalization: Boolean, determines whether to normalize the layer\n\t\tinit: Boolean, determines if the current run is the data dependent initialization run\n\t\tinit_scale: Float, Initialisation scale of the data dependent initialization. Usually 1.\n\t""""""\n\tif weight_normalization:\n\t\treturn WeightNorm(layer, init, init_scale)\n\treturn layer\n\nclass WaveNet():\n\t""""""Tacotron-2 Wavenet Vocoder model.\n\t""""""\n\tdef __init__(self, hparams, init):\n\t\t#Get hparams\n\t\tself._hparams = hparams\n\n\t\tif self.local_conditioning_enabled():\n\t\t\tassert hparams.num_mels == hparams.cin_channels\n\n\t\t#Initialize model architecture\n\t\tassert hparams.layers % hparams.stacks == 0\n\t\tlayers_per_stack = hparams.layers // hparams.stacks\n\n\t\tself.scalar_input = is_scalar_input(hparams.input_type)\n\n\t\t#first (embedding) convolution\n\t\twith tf.variable_scope(\'input_convolution\'):\n\t\t\tif self.scalar_input:\n\t\t\t\tself.first_conv = Conv1D1x1(hparams.residual_channels, \n\t\t\t\t\tweight_normalization=hparams.wavenet_weight_normalization, \n\t\t\t\t\tweight_normalization_init=init, \n\t\t\t\t\tweight_normalization_init_scale=hparams.wavenet_init_scale,\n\t\t\t\t\tname=\'input_convolution\')\n\t\t\telse:\n\t\t\t\tself.first_conv = Conv1D1x1(hparams.residual_channels, \n\t\t\t\t\tweight_normalization=hparams.wavenet_weight_normalization, \n\t\t\t\t\tweight_normalization_init=init, \n\t\t\t\t\tweight_normalization_init_scale=hparams.wavenet_init_scale,\n\t\t\t\t\tname=\'input_convolution\')\n\n\t\t#Residual Blocks\n\t\tself.residual_layers = []\n\t\tfor layer in range(hparams.layers):\n\t\t\tself.residual_layers.append(ResidualConv1DGLU(\n\t\t\thparams.residual_channels, hparams.gate_channels,\n\t\t\tkernel_size=hparams.kernel_size,\n\t\t\tskip_out_channels=hparams.skip_out_channels,\n\t\t\tuse_bias=hparams.use_bias,\n\t\t\tdilation_rate=2**(layer % layers_per_stack),\n\t\t\tdropout=hparams.wavenet_dropout,\n\t\t\tcin_channels=hparams.cin_channels,\n\t\t\tgin_channels=hparams.gin_channels,\n\t\t\tweight_normalization=hparams.wavenet_weight_normalization, \n\t\t\tinit=init, \n\t\t\tinit_scale=hparams.wavenet_init_scale,\n\t\t\tresidual_legacy=hparams.residual_legacy,\n\t\t\tname=\'ResidualConv1DGLU_{}\'.format(layer)))\n\n\t\t#Final (skip) convolutions\n\t\twith tf.variable_scope(\'skip_convolutions\'):\n\t\t\tself.last_conv_layers = [\n\t\t\tReluActivation(name=\'final_conv_relu1\'),\n\t\t\tConv1D1x1(hparams.skip_out_channels, \n\t\t\t\tweight_normalization=hparams.wavenet_weight_normalization, \n\t\t\t\tweight_normalization_init=init, \n\t\t\t\tweight_normalization_init_scale=hparams.wavenet_init_scale,\n\t\t\t\tname=\'final_convolution_1\'), \n\t\t\tReluActivation(name=\'final_conv_relu2\'),\n\t\t\tConv1D1x1(hparams.out_channels, \n\t\t\t\tweight_normalization=hparams.wavenet_weight_normalization, \n\t\t\t\tweight_normalization_init=init, \n\t\t\t\tweight_normalization_init_scale=hparams.wavenet_init_scale,\n\t\t\t\tname=\'final_convolution_2\'),]\n\n\t\t#Global conditionning embedding\n\t\tif hparams.gin_channels > 0 and hparams.use_speaker_embedding:\n\t\t\tassert hparams.n_speakers is not None\n\t\t\tself.embed_speakers = Embedding(\n\t\t\t\thparams.n_speakers, hparams.gin_channels, std=0.1, name=\'gc_embedding\')\n\t\t\tself.embedding_table = self.embed_speakers.embedding_table\n\t\telse:\n\t\t\tself.embed_speakers = None\n\n\t\tself.all_convs = [self.first_conv] + self.residual_layers + self.last_conv_layers\n\n\t\t#Upsample conv net\n\t\tif self.local_conditioning_enabled():\n\t\t\tself.upsample_conv = []\n\t\t\tif hparams.upsample_type == \'NearestNeighbor\':\n\t\t\t\t#Nearest neighbor upsampling (non-learnable)\n\t\t\t\tself.upsample_conv.append(NearestNeighborUpsample(strides=(1, audio.get_hop_size(hparams))))\n\n\t\t\telse:\n\t\t\t\t#Learnable upsampling layers\n\t\t\t\tfor i, s in enumerate(hparams.upsample_scales):\n\t\t\t\t\twith tf.variable_scope(\'local_conditioning_upsampling_{}\'.format(i+1)):\n\t\t\t\t\t\tif hparams.upsample_type == \'2D\':\n\t\t\t\t\t\t\tconvt = ConvTranspose2D(1, (hparams.freq_axis_kernel_size, s),\n\t\t\t\t\t\t\t\tpadding=\'same\', strides=(1, s), NN_init=hparams.NN_init, NN_scaler=hparams.NN_scaler,\n\t\t\t\t\t\t\t\tup_layers=len(hparams.upsample_scales), name=\'ConvTranspose2D_layer_{}\'.format(i))\n\n\t\t\t\t\t\telif hparams.upsample_type == \'1D\':\n\t\t\t\t\t\t\tconvt = ConvTranspose1D(hparams.cin_channels, (s, ),\n\t\t\t\t\t\t\t\tpadding=\'same\', strides=(s, ), NN_init=hparams.NN_init, NN_scaler=hparams.NN_scaler,\n\t\t\t\t\t\t\t\tup_layers=len(hparams.upsample_scales), name=\'ConvTranspose1D_layer_{}\'.format(i))\n\n\t\t\t\t\t\telif hparams.upsample_type == \'Resize\':\n\t\t\t\t\t\t\tconvt = ResizeConvolution(1, (hparams.freq_axis_kernel_size, s),\n\t\t\t\t\t\t\t\tpadding=\'same\', strides=(1, s), NN_init=hparams.NN_init, NN_scaler=hparams.NN_scaler,\n\t\t\t\t\t\t\t\tup_layers=len(hparams.upsample_scales), name=\'ResizeConvolution_layer_{}\'.format(i))\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tassert hparams.upsample_type == \'SubPixel\'\n\t\t\t\t\t\t\tconvt = SubPixelConvolution(1, (hparams.freq_axis_kernel_size, 3),\n\t\t\t\t\t\t\t\tpadding=\'same\', strides=(1, s), NN_init=hparams.NN_init, NN_scaler=hparams.NN_scaler,\n\t\t\t\t\t\t\t\tup_layers=len(hparams.upsample_scales), name=\'SubPixelConvolution_layer_{}\'.format(i))\n\n\t\t\t\t\t\tself.upsample_conv.append(maybe_Normalize_weights(convt, \n\t\t\t\t\t\t\thparams.wavenet_weight_normalization, init, hparams.wavenet_init_scale))\n\n\t\t\t\t\t\tif hparams.upsample_activation == \'LeakyRelu\':\n\t\t\t\t\t\t\tself.upsample_conv.append(LeakyReluActivation(alpha=hparams.leaky_alpha,\n\t\t\t\t\t\t\t\tname=\'upsample_leaky_relu_{}\'.format(i+1)))\n\t\t\t\t\t\telif hparams.upsample_activation == \'Relu\':\n\t\t\t\t\t\t\tself.upsample_conv.append(ReluActivation(name=\'upsample_relu_{}\'.format(i+1)))\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tassert hparams.upsample_activation == None\n\n\t\t\tself.all_convs += self.upsample_conv\n\n\t\tself.receptive_field = receptive_field_size(hparams.layers,\n\t\t\thparams.stacks, hparams.kernel_size)\n\n\n\tdef set_mode(self, is_training):\n\t\tfor conv in self.all_convs:\n\t\t\ttry:\n\t\t\t\tconv.set_mode(is_training)\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\n\tdef initialize(self, y, c, g, input_lengths, x=None, synthesis_length=None, test_inputs=None, split_infos=None):\n\t\t\'\'\'Initialize wavenet graph for train, eval and test cases.\n\t\t\'\'\'\n\t\thparams = self._hparams\n\t\tself.is_training = x is not None\n\t\tself.is_evaluating = not self.is_training and y is not None\n\t\t#Set all convolutions to corresponding mode\n\t\tself.set_mode(self.is_training)\n\n\t\tsplit_device = \'/cpu:0\' if self._hparams.wavenet_num_gpus > 1 or self._hparams.split_on_cpu else \'/gpu:0\'\n\t\twith tf.device(split_device):\n\t\t\thp = self._hparams\n\t\t\tlout_int = [tf.int32] * hp.wavenet_num_gpus\n\t\t\tlout_float = [tf.float32] * hp.wavenet_num_gpus\n\n\t\t\ttower_input_lengths = tf.split(input_lengths, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if input_lengths is not None else [input_lengths] * hp.wavenet_num_gpus\n\n\t\t\ttower_y = tf.split(y, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if y is not None else [y] * hp.wavenet_num_gpus\n\t\t\ttower_x = tf.split(x, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if x is not None else [x] * hp.wavenet_num_gpus\n\t\t\ttower_c = tf.split(c, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if self.local_conditioning_enabled() else [None] * hp.wavenet_num_gpus\n\t\t\ttower_g = tf.split(g, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if self.global_conditioning_enabled() else [None] * hp.wavenet_num_gpus\n\t\t\ttower_test_inputs = tf.split(test_inputs, num_or_size_splits=hp.wavenet_num_gpus, axis=0) if test_inputs is not None else [test_inputs] * hp.wavenet_num_gpus\n\n\t\tself.tower_y_hat_q = []\n\t\tself.tower_y_hat_train = []\n\t\tself.tower_y = []\n\t\tself.tower_input_lengths = []\n\t\tself.tower_means = []\n\t\tself.tower_log_scales = []\n\t\tself.tower_y_hat_log = []\n\t\tself.tower_y_log = []\n\t\tself.tower_c = []\n\t\tself.tower_y_eval = []\n\t\tself.tower_eval_length = []\n\t\tself.tower_y_hat = []\n\t\tself.tower_y_target = []\n\t\tself.tower_eval_c = []\n\t\tself.tower_mask = []\n\t\tself.tower_upsampled_local_features = []\n\t\tself.tower_eval_upsampled_local_features = []\n\t\tself.tower_synth_upsampled_local_features = []\n\n\t\tlog(\'Initializing Wavenet model.  Dimensions (? = dynamic shape): \')\n\t\tlog(\'  Train mode:                {}\'.format(self.is_training))\n\t\tlog(\'  Eval mode:                 {}\'.format(self.is_evaluating))\n\t\tlog(\'  Synthesis mode:            {}\'.format(not (self.is_training or self.is_evaluating)))\n\n\t\t#1. Declare GPU devices\n\t\tgpus = [\'/gpu:{}\'.format(i) for i in range(hp.wavenet_num_gpus)]\n\t\tfor i in range(hp.wavenet_num_gpus):\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=\'/cpu:0\', worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'inference\') as scope:\n\t\t\t\t\tlog(\'  device:                    {}\'.format(i))\n\t\t\t\t\t#Training\n\t\t\t\t\tif self.is_training:\n\t\t\t\t\t\tbatch_size = tf.shape(x)[0]\n\t\t\t\t\t\t#[batch_size, time_length, 1]\n\t\t\t\t\t\tself.tower_mask.append(self.get_mask(tower_input_lengths[i], maxlen=tf.shape(tower_x[i])[-1])) #To be used in loss computation\n\t\t\t\t\t\t#[batch_size, channels, time_length]\n\t\t\t\t\t\ty_hat_train = self.step(tower_x[i], tower_c[i], tower_g[i], softmax=False) #softmax is automatically computed inside softmax_cross_entropy if needed\n\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\t#[batch_size, time_length, channels]\n\t\t\t\t\t\t\tself.tower_y_hat_q.append(tf.transpose(y_hat_train, [0, 2, 1]))\n\n\t\t\t\t\t\tself.tower_y_hat_train.append(y_hat_train)\n\t\t\t\t\t\tself.tower_y.append(tower_y[i])\n\t\t\t\t\t\tself.tower_input_lengths.append(tower_input_lengths[i])\n\n\t\t\t\t\t\t#Add mean and scale stats if using Guassian distribution output (there would be too many logistics if using MoL)\n\t\t\t\t\t\tif self._hparams.out_channels == 2:\n\t\t\t\t\t\t\tself.tower_means.append(y_hat_train[:, 0, :])\n\t\t\t\t\t\t\tself.tower_log_scales.append(y_hat_train[:, 1, :])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.tower_means.append(None)\n\n\t\t\t\t\t\t#Graph extension for log saving\n\t\t\t\t\t\t#[batch_size, time_length]\n\t\t\t\t\t\tshape_control = (batch_size, tf.shape(tower_x[i])[-1], 1)\n\t\t\t\t\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(tower_y[i]), shape_control)]):\n\t\t\t\t\t\t\ty_log = tf.squeeze(tower_y[i], [-1])\n\t\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\t\tself.tower_y[i] = y_log\n\n\t\t\t\t\t\ty_hat_log = tf.cond(tf.equal(tf.rank(y_hat_train), 4),\n\t\t\t\t\t\t\tlambda: tf.squeeze(y_hat_train, [-1]),\n\t\t\t\t\t\t\tlambda: y_hat_train)\n\t\t\t\t\t\ty_hat_log = tf.reshape(y_hat_log, [batch_size, hparams.out_channels, -1])\n\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\t#[batch_size, time_length]\n\t\t\t\t\t\t\ty_hat_log = tf.argmax(tf.nn.softmax(y_hat_log, axis=1), 1)\n\n\t\t\t\t\t\t\ty_hat_log = util.inv_mulaw_quantize(y_hat_log, hparams.quantize_channels)\n\t\t\t\t\t\t\ty_log = util.inv_mulaw_quantize(y_log, hparams.quantize_channels)\n\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t#[batch_size, time_length]\n\t\t\t\t\t\t\tif hparams.out_channels == 2:\n\t\t\t\t\t\t\t\ty_hat_log = sample_from_gaussian(\n\t\t\t\t\t\t\t\t\ty_hat_log, log_scale_min_gauss=hparams.log_scale_min_gauss)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\ty_hat_log = sample_from_discretized_mix_logistic(\n\t\t\t\t\t\t\t\t\ty_hat_log, log_scale_min=hparams.log_scale_min)\n\n\t\t\t\t\t\t\tif is_mulaw(hparams.input_type):\n\t\t\t\t\t\t\t\ty_hat_log = util.inv_mulaw(y_hat_log, hparams.quantize_channels)\n\t\t\t\t\t\t\t\ty_log = util.inv_mulaw(y_log, hparams.quantize_channels)\n\n\t\t\t\t\t\tself.tower_y_hat_log.append(y_hat_log)\n\t\t\t\t\t\tself.tower_y_log.append(y_log)\n\t\t\t\t\t\tself.tower_c.append(tower_c[i])\n\t\t\t\t\t\tself.tower_upsampled_local_features.append(self.upsampled_local_features)\n\n\t\t\t\t\t\tlog(\'  inputs:                    {}\'.format(tower_x[i].shape))\n\t\t\t\t\t\tif self.local_conditioning_enabled():\n\t\t\t\t\t\t\tlog(\'  local_condition:           {}\'.format(tower_c[i].shape))\n\t\t\t\t\t\tif self.has_speaker_embedding():\n\t\t\t\t\t\t\tlog(\'  global_condition:          {}\'.format(tower_g[i].shape))\n\t\t\t\t\t\tlog(\'  targets:                   {}\'.format(y_log.shape))\n\t\t\t\t\t\tlog(\'  outputs:                   {}\'.format(y_hat_log.shape))\n\n\n\t\t\t\t\t#evaluating\n\t\t\t\t\telif self.is_evaluating:\n\t\t\t\t\t\t#[time_length, ]\n\t\t\t\t\t\tidx = 0\n\t\t\t\t\t\tlength = tower_input_lengths[i][idx]\n\t\t\t\t\t\ty_target = tf.reshape(tower_y[i][idx], [-1])[:length]\n\t\t\t\t\t\ttest_inputs = tf.reshape(y_target, [1, -1, 1]) if not hparams.wavenet_natural_eval else None\n\n\t\t\t\t\t\tif tower_c[i] is not None:\n\t\t\t\t\t\t\ttower_c[i] = tf.expand_dims(tower_c[i][idx, :, :length], axis=0)\n\t\t\t\t\t\t\twith tf.control_dependencies([tf.assert_equal(tf.rank(tower_c[i]), 3)]):\n\t\t\t\t\t\t\t\ttower_c[i] = tf.identity(tower_c[i], name=\'eval_assert_c_rank_op\')\n\n\t\t\t\t\t\tif tower_g[i] is not None:\n\t\t\t\t\t\t\ttower_g[i] = tf.expand_dims(tower_g[i][idx], axis=0)\n\n\t\t\t\t\t\tbatch_size = tf.shape(tower_c[i])[0]\n\n\t\t\t\t\t\t#Start silence frame\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\tinitial_value = mulaw_quantize(0, hparams.quantize_channels)\n\t\t\t\t\t\telif is_mulaw(hparams.input_type):\n\t\t\t\t\t\t\tinitial_value = mulaw(0.0, hparams.quantize_channels)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tinitial_value = 0.0\n\n\t\t\t\t\t\t#[channels, ]\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\tinitial_input = tf.one_hot(indices=initial_value, depth=hparams.quantize_channels, dtype=tf.float32)\n\t\t\t\t\t\t\tinitial_input = tf.tile(tf.reshape(initial_input, [1, 1, hparams.quantize_channels]), [batch_size, 1, 1])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tinitial_input = tf.ones([batch_size, 1, 1], tf.float32) * initial_value\n\n\t\t\t\t\t\t#Fast eval\n\t\t\t\t\t\ty_hat = self.incremental(initial_input, c=tower_c[i], g=tower_g[i], time_length=length, test_inputs=test_inputs,\n\t\t\t\t\t\t\tsoftmax=False, quantize=True, log_scale_min=hparams.log_scale_min, log_scale_min_gauss=hparams.log_scale_min_gauss)\n\n\t\t\t\t\t\t#Save targets and length for eval loss computation\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\tself.tower_y_eval.append(tf.reshape(y[idx], [1, -1])[:, :length])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tself.tower_y_eval.append(tf.expand_dims(y[idx], axis=0)[:, :length, :])\n\t\t\t\t\t\tself.tower_eval_length.append(length)\n\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\ty_hat = tf.reshape(tf.argmax(y_hat, axis=1), [-1])\n\t\t\t\t\t\t\ty_hat = inv_mulaw_quantize(y_hat, hparams.quantize_channels)\n\t\t\t\t\t\t\ty_target = inv_mulaw_quantize(y_target, hparams.quantize_channels)\n\t\t\t\t\t\telif is_mulaw(hparams.input_type):\n\t\t\t\t\t\t\ty_hat = inv_mulaw(tf.reshape(y_hat, [-1]), hparams.quantize_channels)\n\t\t\t\t\t\t\ty_target = inv_mulaw(y_target, hparams.quantize_channels)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ty_hat = tf.reshape(y_hat, [-1])\n\n\t\t\t\t\t\tself.tower_y_hat.append(y_hat)\n\t\t\t\t\t\tself.tower_y_target.append(y_target)\n\t\t\t\t\t\tself.tower_eval_c.append(tower_c[i][idx])\n\t\t\t\t\t\tself.tower_eval_upsampled_local_features.append(self.upsampled_local_features[idx])\n\n\t\t\t\t\t\tif self.local_conditioning_enabled():\n\t\t\t\t\t\t\tlog(\'  local_condition:           {}\'.format(tower_c[i].shape))\n\t\t\t\t\t\tif self.has_speaker_embedding():\n\t\t\t\t\t\t\tlog(\'  global_condition:          {}\'.format(tower_g[i].shape))\n\t\t\t\t\t\tlog(\'  targets:                   {}\'.format(y_target.shape))\n\t\t\t\t\t\tlog(\'  outputs:                   {}\'.format(y_hat.shape))\n\n\t\t\t\t\t#synthesizing\n\t\t\t\t\telse:\n\t\t\t\t\t\tbatch_size = tf.shape(tower_c[i])[0]\n\t\t\t\t\t\tif c is None:\n\t\t\t\t\t\t\tassert synthesis_length is not None\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t#[batch_size, local_condition_time, local_condition_dimension(num_mels)]\n\t\t\t\t\t\t\tmessage = (\'Expected 3 dimension shape [batch_size(1), time_length, {}] for local condition features but found {}\'.format(\n\t\t\t\t\t\t\t\t\thparams.cin_channels, tower_c[i].shape))\n\t\t\t\t\t\t\twith tf.control_dependencies([tf.assert_equal(tf.rank(tower_c[i]), 3, message=message)]):\n\t\t\t\t\t\t\t\ttower_c[i] = tf.identity(tower_c[i], name=\'synthesis_assert_c_rank_op\')\n\n\t\t\t\t\t\t\tTc = tf.shape(tower_c[i])[1]\n\t\t\t\t\t\t\tupsample_factor = audio.get_hop_size(self._hparams)\n\n\t\t\t\t\t\t\t#Overwrite length with respect to local condition features\n\t\t\t\t\t\t\tsynthesis_length = Tc * upsample_factor\n\n\t\t\t\t\t\t\t#[batch_size, local_condition_dimension, local_condition_time]\n\t\t\t\t\t\t\t#time_length will be corrected using the upsample network\n\t\t\t\t\t\t\ttower_c[i] = tf.transpose(tower_c[i], [0, 2, 1])\n\n\t\t\t\t\t\tif tower_g[i] is not None:\n\t\t\t\t\t\t\tassert tower_g[i].shape == (batch_size, 1)\n\n\t\t\t\t\t\t#Start silence frame\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\tinitial_value = mulaw_quantize(0, hparams.quantize_channels)\n\t\t\t\t\t\telif is_mulaw(hparams.input_type):\n\t\t\t\t\t\t\tinitial_value = mulaw(0.0, hparams.quantize_channels)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tinitial_value = 0.0\n\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\tassert initial_value >= 0 and initial_value < hparams.quantize_channels\n\t\t\t\t\t\t\tinitial_input = tf.one_hot(indices=initial_value, depth=hparams.quantize_channels, dtype=tf.float32)\n\t\t\t\t\t\t\tinitial_input = tf.tile(tf.reshape(initial_input, [1, 1, hparams.quantize_channels]), [batch_size, 1, 1])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tinitial_input = tf.ones([batch_size, 1, 1], tf.float32) * initial_value\n\n\t\t\t\t\t\ty_hat = self.incremental(initial_input, c=tower_c[i], g=tower_g[i], time_length=synthesis_length, test_inputs=tower_test_inputs[i],\n\t\t\t\t\t\t\tsoftmax=False, quantize=True, log_scale_min=hparams.log_scale_min, log_scale_min_gauss=hparams.log_scale_min_gauss)\n\n\t\t\t\t\t\tif is_mulaw_quantize(hparams.input_type):\n\t\t\t\t\t\t\ty_hat = tf.reshape(tf.argmax(y_hat, axis=1), [batch_size, -1])\n\t\t\t\t\t\t\ty_hat = util.inv_mulaw_quantize(y_hat, hparams.quantize_channels)\n\t\t\t\t\t\telif is_mulaw(hparams.input_type):\n\t\t\t\t\t\t\ty_hat = util.inv_mulaw(tf.reshape(y_hat, [batch_size, -1]), hparams.quantize_channels)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ty_hat = tf.reshape(y_hat, [batch_size, -1])\n\n\t\t\t\t\t\tself.tower_y_hat.append(y_hat)\n\t\t\t\t\t\tself.tower_synth_upsampled_local_features.append(self.upsampled_local_features)\n\n\t\t\t\t\t\tif self.local_conditioning_enabled():\n\t\t\t\t\t\t\tlog(\'  local_condition:           {}\'.format(tower_c[i].shape))\n\t\t\t\t\t\tif self.has_speaker_embedding():\n\t\t\t\t\t\t\tlog(\'  global_condition:          {}\'.format(tower_g[i].shape))\n\t\t\t\t\t\tlog(\'  outputs:                   {}\'.format(y_hat.shape))\n\n\t\tself.variables = tf.trainable_variables()\n\t\tlog(\'  Receptive Field:           ({} samples / {:.1f} ms)\'.format(self.receptive_field, self.receptive_field / hparams.sample_rate * 1000.))\n\n\t\t#1_000_000 is causing syntax problems for some people?! Python please :)\n\t\tlog(\'  WaveNet Parameters:        {:.3f} Million.\'.format(np.sum([np.prod(v.get_shape().as_list()) for v in self.variables]) / 1000000))\n\n\t\tself.ema = tf.train.ExponentialMovingAverage(decay=hparams.wavenet_ema_decay)\n\n\n\tdef add_loss(self):\n\t\t\'\'\'Adds loss computation to the graph. Supposes that initialize function has already been called.\n\t\t\'\'\'\n\t\tself.tower_loss = []\n\t\ttotal_loss = 0\n\t\tgpus = [\'/gpu:{}\'.format(i) for i in range(self._hparams.wavenet_num_gpus)]\n\n\t\tfor i in range(self._hparams.wavenet_num_gpus):\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=\'/cpu:0\', worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'loss\') as scope:\n\t\t\t\t\tif self.is_training:\n\t\t\t\t\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\t\t\t\t\ttower_loss = MaskedCrossEntropyLoss(self.tower_y_hat_q[i][:, :-1, :], self.tower_y[i][:, 1:], mask=self.tower_mask[i])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif self._hparams.out_channels == 2:\n\t\t\t\t\t\t\t\ttower_loss = GaussianMaximumLikelihoodEstimation(self.tower_y_hat_train[i][:, :, :-1], self.tower_y[i][:, 1:, :], \n\t\t\t\t\t\t\t\t\thparams=self._hparams, mask=self.tower_mask[i])\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\ttower_loss = DiscretizedMixtureLogisticLoss(self.tower_y_hat_train[i][:, :, :-1], self.tower_y[i][:, 1:, :], \n\t\t\t\t\t\t\t\t\thparams=self._hparams, mask=self.tower_mask[i])\n\t\t\t\t\t\t\t\t\n\t\t\t\t\telif self.is_evaluating:\n\t\t\t\t\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\t\t\t\t\ttower_loss = MaskedCrossEntropyLoss(self.tower_y_hat_eval[i], self.tower_y_eval[i], lengths=[self.tower_eval_length[i]])\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tif self._hparams.out_channels == 2:\n\t\t\t\t\t\t\t\ttower_loss = GaussianMaximumLikelihoodEstimation(self.tower_y_hat_eval[i], self.tower_y_eval[i], \n\t\t\t\t\t\t\t\t\thparams=self._hparams, lengths=[self.tower_eval_length[i]])\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\ttower_loss = DiscretizedMixtureLogisticLoss(self.tower_y_hat_eval[i], self.tower_y_eval[i], \n\t\t\t\t\t\t\t\t\thparams=self._hparams, lengths=[self.tower_eval_length[i]])\n\n\t\t\t\t\telse:\n\t\t\t\t\t\traise RuntimeError(\'Model not in train/eval mode but computing loss: Where did this go wrong?\')\n\n\t\t\t#Compute final loss\n\t\t\tself.tower_loss.append(tower_loss)\n\t\t\ttotal_loss += tower_loss\n\n\t\tif self.is_training:\n\t\t\tself.loss = total_loss / self._hparams.wavenet_num_gpus\n\n\t\telse:\n\t\t\tself.eval_loss = total_loss / self._hparams.wavenet_num_gpus\n\n\n\tdef add_optimizer(self, global_step):\n\t\t\'\'\'Adds optimizer to the graph. Supposes that initialize function has already been called.\n\t\t\'\'\'\n\t\thp = self._hparams\n\t\ttower_gradients = []\n\n\t\t# 1. Declare GPU devices\n\t\tgpus = [\'/gpu:{}\'.format(i) for i in range(hp.wavenet_num_gpus)]\n\n\t\tgrad_device = \'/cpu:0\' if hp.tacotron_num_gpus > 1 else gpus[0]\n\n\t\twith tf.device(grad_device):\n\t\t\twith tf.variable_scope(\'optimizer\'):\n\t\t\t\t#Create lr schedule\n\t\t\t\tif hp.wavenet_lr_schedule == \'noam\':\n\t\t\t\t\tlearning_rate = self._noam_learning_rate_decay(hp.wavenet_learning_rate, \n\t\t\t\t\t\tglobal_step,\n\t\t\t\t\t\twarmup_steps=hp.wavenet_warmup)\n\t\t\t\telse:\n\t\t\t\t\tassert hp.wavenet_lr_schedule == \'exponential\'\n\t\t\t\t\tlearning_rate = self._exponential_learning_rate_decay(hp.wavenet_learning_rate,\n\t\t\t\t\t\tglobal_step,\n\t\t\t\t\t\thp.wavenet_decay_rate,\n\t\t\t\t\t\thp.wavenet_decay_steps)\n\n\t\t\t\t#Adam optimization\n\t\t\t\tself.learning_rate = learning_rate\n\t\t\t\toptimizer = tf.train.AdamOptimizer(learning_rate, hp.wavenet_adam_beta1,\n\t\t\t\t\thp.wavenet_adam_beta2, hp.wavenet_adam_epsilon)\n\n\t\t# 2. Compute Gradient\n\t\tfor i in range(hp.wavenet_num_gpus):\n\t\t\t#Device placemenet\n\t\t\twith tf.device(tf.train.replica_device_setter(ps_tasks=1, ps_device=\'/cpu:0\', worker_device=gpus[i])):\n\t\t\t\twith tf.variable_scope(\'optimizer\') as scope:\n\t\t\t\t\tgradients = optimizer.compute_gradients(self.tower_loss[i])\n\t\t\t\t\ttower_gradients.append(gradients)\n\n\t\t# 3. Average Gradient\n\t\twith tf.device(grad_device):\n\t\t\tavg_grads = []\n\t\t\tvariables = []\n\t\t\tfor grad_and_vars in zip(*tower_gradients):\n\t\t\t\t# each_grads_vars = ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n\t\t\t\tif grad_and_vars[0][0] is not None:\n\t\t\t\t\tgrads = []\n\t\t\t\t\tfor g, _ in grad_and_vars:\n\t\t\t\t\t\texpanded_g = tf.expand_dims(g, 0)\n\t\t\t\t\t\t#Append on a ""tower"" dimension which we will average over below.\n\t\t\t\t\t\tgrads.append(expanded_g)\n\n\t\t\t\t\t#Average over the \'tower\' dimension.\n\t\t\t\t\tgrad = tf.concat(axis=0, values=grads)\n\t\t\t\t\tgrad = tf.reduce_mean(grad, 0)\n\t\t\t\telse:\n\t\t\t\t\tgrad = grad_and_vars[0][0]\n\n\t\t\t\tv = grad_and_vars[0][1]\n\t\t\t\tavg_grads.append(grad)\n\t\t\t\tvariables.append(v)\n\n\t\t\tself.gradients = avg_grads\n\n\t\t\t#Gradients clipping\n\t\t\tif hp.wavenet_clip_gradients:\n\t\t\t\t#Clip each gradient by a [min, max] range of values and its norm by [0, max_norm_value]\n\t\t\t\tclipped_grads = []\n\t\t\t\tfor g in avg_grads:\n\t\t\t\t\tif g is not None:\n\t\t\t\t\t\tclipped_g = tf.clip_by_norm(g, hp.wavenet_gradient_max_norm)\n\t\t\t\t\t\tclipped_g = tf.clip_by_value(clipped_g, -hp.wavenet_gradient_max_value, hp.wavenet_gradient_max_value)\n\t\t\t\t\t\tclipped_grads.append(clipped_g)\n\n\t\t\t\t\telse:\n\t\t\t\t\t\tclipped_grads.append(g)\n\n\t\t\telse:\n\t\t\t\tclipped_grads = avg_grads\n\n\t\t\twith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n\t\t\t\tadam_optimize = optimizer.apply_gradients(zip(clipped_grads, variables),\n\t\t\t\t\tglobal_step=global_step)\n\n\t\t\t#Add exponential moving average\n\t\t\t#https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\t\t\t#Use adam optimization process as a dependency\n\t\t\twith tf.control_dependencies([adam_optimize]):\n\t\t\t\t#Create the shadow variables and add ops to maintain moving averages\n\t\t\t\t#Also updates moving averages after each update step\n\t\t\t\t#This is the optimize call instead of traditional adam_optimize one.\n\t\t\t\tassert set(self.variables) == set(variables) #Verify all trainable variables are being averaged\n\t\t\t\tself.optimize = self.ema.apply(variables)\n\n\tdef _noam_learning_rate_decay(self, init_lr, global_step, warmup_steps=4000.0):\n\t\t# Noam scheme from tensor2tensor:\n\t\tstep = tf.cast(global_step + 1, dtype=tf.float32)\n\t\treturn tf.maximum(init_lr * warmup_steps**0.5 * tf.minimum(step * warmup_steps**-1.5, step**-0.5), 1e-4)\n\n\tdef _exponential_learning_rate_decay(self, init_lr, global_step,\n\t\t\t\t\t\t\t decay_rate=0.5,\n\t\t\t\t\t\t\t decay_steps=300000):\n\t\t#Compute natural exponential decay\n\t\tlr = tf.train.exponential_decay(init_lr,\n\t\t\tglobal_step,\n\t\t\tdecay_steps,\n\t\t\tdecay_rate,\n\t\t\tname=\'wavenet_lr_exponential_decay\')\n\t\treturn lr\n\n\n\tdef get_mask(self, input_lengths, maxlen=None):\n\t\texpand = not is_mulaw_quantize(self._hparams.input_type)\n\t\tmask = sequence_mask(input_lengths, max_len=maxlen, expand=expand)\n\n\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\treturn mask[:, 1:]\n\t\treturn mask[:, 1:, :]\n\n\t#Sanity check functions\n\tdef has_speaker_embedding(self):\n\t\treturn self.embed_speakers is not None\n\n\tdef local_conditioning_enabled(self):\n\t\treturn self._hparams.cin_channels > 0\n\n\tdef global_conditioning_enabled(self):\n\t\treturn self._hparams.gin_channels > 0\n\n\tdef step(self, x, c=None, g=None, softmax=False):\n\t\t""""""Forward step\n\n\t\tArgs:\n\t\t\tx: Tensor of shape [batch_size, channels, time_length], One-hot encoded audio signal.\n\t\t\tc: Tensor of shape [batch_size, cin_channels, time_length], Local conditioning features.\n\t\t\tg: Tensor of shape [batch_size, gin_channels, 1] or Ids of shape [batch_size, 1],\n\t\t\t\tGlobal conditioning features.\n\t\t\t\tNote: set hparams.use_speaker_embedding to False to disable embedding layer and\n\t\t\t\tuse extrnal One-hot encoded features.\n\t\t\tsoftmax: Boolean, Whether to apply softmax.\n\n\t\tReturns:\n\t\t\ta Tensor of shape [batch_size, out_channels, time_length]\n\t\t""""""\n\t\t#[batch_size, channels, time_length] -> [batch_size, time_length, channels]\n\t\tbatch_size = tf.shape(x)[0]\n\t\ttime_length = tf.shape(x)[-1]\n\n\t\tif g is not None:\n\t\t\tif self.embed_speakers is not None:\n\t\t\t\t#[batch_size, 1] ==> [batch_size, 1, gin_channels]\n\t\t\t\tg = self.embed_speakers(tf.reshape(g, [batch_size, -1]))\n\t\t\t\t#[batch_size, gin_channels, 1]\n\t\t\t\twith tf.control_dependencies([tf.assert_equal(tf.rank(g), 3)]):\n\t\t\t\t\tg = tf.transpose(g, [0, 2, 1])\n\n\t\t#Expand global conditioning features to all time steps\n\t\tg_bct = _expand_global_features(batch_size, time_length, g, data_format=\'BCT\')\n\n\t\tif c is not None:\n\t\t\tif self._hparams.upsample_type == \'2D\':\n\t\t\t\t#[batch_size, 1, cin_channels, time_length]\n\t\t\t\texpand_dim = 1\n\t\t\telif self._hparams.upsample_type == \'1D\':\n\t\t\t\t#[batch_size, cin_channels, 1, time_length]\n\t\t\t\texpand_dim = 2\n\t\t\telse:\n\t\t\t\tassert self._hparams.upsample_type in (\'Resize\', \'SubPixel\', \'NearestNeighbor\')\n\t\t\t\t#[batch_size, cin_channels, time_length, 1]\n\t\t\t\texpand_dim = 3\n\n\t\t\tc = tf.expand_dims(c, axis=expand_dim)\n\n\t\t\tfor transposed_conv in self.upsample_conv:\n\t\t\t\tc = transposed_conv(c)\n\n\t\t\t#[batch_size, cin_channels, time_length]\n\t\t\tc = tf.squeeze(c, [expand_dim])\n\t\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(c)[-1], tf.shape(x)[-1])]):\n\t\t\t\tc = tf.identity(c, name=\'control_c_and_x_shape\')\n\n\t\t\tself.upsampled_local_features = c\n\n\t\t#Feed data to network\n\t\tx = self.first_conv(x)\n\t\tskips = None\n\t\tfor conv in self.residual_layers:\n\t\t\tx, h = conv(x, c=c, g=g_bct)\n\t\t\tif skips is None:\n\t\t\t\tskips = h\n\t\t\telse:\n\t\t\t\tskips = skips + h\n\n\t\t\t\tif self._hparams.legacy:\n\t\t\t\t\tskips = skips * np.sqrt(0.5)\n\t\tx = skips\n\n\t\tfor conv in self.last_conv_layers:\n\t\t\tx = conv(x)\n\n\t\treturn tf.nn.softmax(x, axis=1) if softmax else x\n\n\n\tdef incremental(self, initial_input, c=None, g=None,\n\t\ttime_length=100, test_inputs=None,\n\t\tsoftmax=True, quantize=True, log_scale_min=-7.0, log_scale_min_gauss=-7.0):\n\t\t""""""Inceremental forward step\n\n\t\tInputs of shape [batch_size, channels, time_length] are reshaped to [batch_size, time_length, channels]\n\t\tInput of each time step is of shape [batch_size, 1, channels]\n\n\t\tArgs:\n\t\t\tInitial input: Tensor of shape [batch_size, channels, 1], initial recurrence input.\n\t\t\tc: Tensor of shape [batch_size, cin_channels, time_length], Local conditioning features\n\t\t\tg: Tensor of shape [batch_size, gin_channels, time_length] or [batch_size, gin_channels, 1]\n\t\t\t\tglobal conditioning features\n\t\t\tT: int, number of timesteps to generate\n\t\t\ttest_inputs: Tensor, teacher forcing inputs (debug)\n\t\t\tsoftmax: Boolean, whether to apply softmax activation\n\t\t\tquantize: Whether to quantize softmax output before feeding to\n\t\t\t\tnext time step input\n\t\t\tlog_scale_min: float, log scale minimum value.\n\n\t\tReturns:\n\t\t\tTensor of shape [batch_size, channels, time_length] or [batch_size, channels, 1]\n\t\t\t\tGenerated one_hot encoded samples\n\t\t""""""\n\t\tbatch_size = tf.shape(initial_input)[0]\n\n\t\t#Note: should reshape to [batch_size, time_length, channels]\n\t\t#not [batch_size, channels, time_length]\n\t\tif test_inputs is not None:\n\t\t\tif self.scalar_input:\n\t\t\t\tif tf.shape(test_inputs)[1] == 1:\n\t\t\t\t\ttest_inputs = tf.transpose(test_inputs, [0, 2, 1])\n\t\t\telse:\n\t\t\t\ttest_inputs = tf.cast(test_inputs, tf.int32)\n\t\t\t\ttest_inputs = tf.one_hot(indices=test_inputs, depth=self._hparams.quantize_channels, dtype=tf.float32)\n\t\t\t\ttest_inputs = tf.squeeze(test_inputs, [2])\n\n\t\t\t\tif tf.shape(test_inputs)[1] == self._hparams.out_channels:\n\t\t\t\t\ttest_inputs = tf.transpose(test_inputs, [0, 2, 1])\n\n\t\t\tbatch_size = tf.shape(test_inputs)[0]\n\t\t\tif time_length is None:\n\t\t\t\ttime_length = tf.shape(test_inputs)[1]\n\t\t\telse:\n\t\t\t\ttime_length = tf.maximum(time_length, tf.shape(test_inputs)[1])\n\n\t\t#Global conditioning\n\t\tif g is not None:\n\t\t\tif self.embed_speakers is not None:\n\t\t\t\tg = self.embed_speakers(tf.reshape(g, [batch_size, -1]))\n\t\t\t\t#[batch_size, channels, 1]\n\t\t\t\twith tf.control_dependencies([tf.assert_equal(tf.rank(g), 3)]):\n\t\t\t\t\tg = tf.transpose(g, [0, 2, 1])\n\n\t\tself.g_btc = _expand_global_features(batch_size, time_length, g, data_format=\'BTC\')\n\n\t\t#Local conditioning\n\t\tif c is not None:\n\t\t\tif self._hparams.upsample_type == \'2D\':\n\t\t\t\t#[batch_size, 1, cin_channels, time_length]\n\t\t\t\texpand_dim = 1\n\t\t\telif self._hparams.upsample_type == \'1D\':\n\t\t\t\t#[batch_size, cin_channels, 1, time_length]\n\t\t\t\texpand_dim = 2\n\t\t\telse:\n\t\t\t\tassert self._hparams.upsample_type in (\'Resize\', \'SubPixel\', \'NearestNeighbor\')\n\t\t\t\t#[batch_size, cin_channels, time_length, 1]\n\t\t\t\texpand_dim = 3\n\n\t\t\tc = tf.expand_dims(c, axis=expand_dim)\n\n\t\t\tfor upsample_conv in self.upsample_conv:\n\t\t\t\tc = upsample_conv(c)\n\n\t\t\t#[batch_size, channels, time_length]\n\t\t\tc = tf.squeeze(c, [expand_dim])\n\t\t\twith tf.control_dependencies([tf.assert_equal(tf.shape(c)[-1], time_length)]):\n\t\t\t\tself.c = tf.transpose(c, [0, 2, 1])\n\n\t\t\tself.upsampled_local_features = c\n\n\t\t#Initialize loop variables\n\t\tif initial_input.shape[1] == self._hparams.out_channels:\n\t\t\tinitial_input = tf.transpose(initial_input, [0, 2, 1])\n\n\t\tinitial_time = tf.constant(0, dtype=tf.int32)\n\t\t# if test_inputs is not None:\n\t\t# \tinitial_input = tf.expand_dims(test_inputs[:, 0, :], axis=1)\n\t\tinitial_outputs_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\t\tinitial_loss_outputs_ta = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n\t\t#Only use convolutions queues for Residual Blocks main convolutions (only ones with kernel size 3 and dilations, all others are 1x1)\n\t\tinitial_queues = [tf.zeros((batch_size, res_conv.layer.kw + (res_conv.layer.kw - 1) * (res_conv.layer.dilation_rate[0] - 1), self._hparams.residual_channels),\n\t\t\tname=\'convolution_queue_{}\'.format(i+1)) for i, res_conv in enumerate(self.residual_layers)]\n\n\t\tdef condition(time, unused_outputs_ta, unused_current_input, unused_loss_outputs_ta, unused_queues):\n\t\t\treturn tf.less(time, time_length)\n\n\t\tdef body(time, outputs_ta, current_input, loss_outputs_ta, queues):\n\t\t\t#conditioning features for single time step\n\t\t\tct = None if self.c is None else tf.expand_dims(self.c[:, time, :], axis=1)\n\t\t\tgt = None if self.g_btc is None else tf.expand_dims(self.g_btc[:, time, :], axis=1)\n\n\t\t\tx = self.first_conv.incremental_step(current_input)\n\n\t\t\tskips = None\n\t\t\tnew_queues = []\n\t\t\tfor conv, queue in zip(self.residual_layers, queues):\n\t\t\t\tx, h, new_queue = conv.incremental_step(x, c=ct, g=gt, queue=queue)\n\t\t\t\t\n\t\t\t\tif self._hparams.legacy:\n\t\t\t\t\tskips = h if skips is None else (skips + h) * np.sqrt(0.5)\n\t\t\t\telse:\n\t\t\t\t\tskips = h if skips is None else (skips + h)\n\t\t\t\tnew_queues.append(new_queue)\n\t\t\tx = skips\n\n\t\t\tfor conv in self.last_conv_layers:\n\t\t\t\ttry:\n\t\t\t\t\tx = conv.incremental_step(x)\n\t\t\t\texcept AttributeError: #When calling Relu activation\n\t\t\t\t\tx = conv(x)\n\n\t\t\t#Save x for eval loss computation\n\t\t\tloss_outputs_ta = loss_outputs_ta.write(time, tf.squeeze(x, [1])) #squeeze time_length dimension (=1)\n\n\t\t\t#Generate next input by sampling\n\t\t\tif self.scalar_input:\n\t\t\t\tif self._hparams.out_channels == 2:\n\t\t\t\t\tx = sample_from_gaussian(\n\t\t\t\t\t\ttf.reshape(x, [batch_size, -1, 1]),\n\t\t\t\t\t\tlog_scale_min_gauss=log_scale_min_gauss)\n\t\t\t\telse:\n\t\t\t\t\tx = sample_from_discretized_mix_logistic(\n\t\t\t\t\t\ttf.reshape(x, [batch_size, -1, 1]), log_scale_min=log_scale_min)\n\n\t\t\t\tnext_input = tf.expand_dims(x, axis=-1) #Expand on the channels dimension\n\t\t\telse:\n\t\t\t\tx = tf.nn.softmax(tf.reshape(x, [batch_size, -1]), axis=1) if softmax \\\n\t\t\t\t\telse tf.reshape(x, [batch_size, -1])\n\t\t\t\tif quantize:\n\t\t\t\t\t#[batch_size, 1]\n\t\t\t\t\tsample = tf.multinomial(x, 1) #Pick a sample using x as probability (one for each batch)\n\t\t\t\t\t#[batch_size, 1, quantize_channels] (time dimension extended by default)\n\t\t\t\t\tx = tf.one_hot(sample, depth=self._hparams.quantize_channels)\n\n\t\t\t\tnext_input = x\n\n\t\t\tif len(x.shape) == 3:\n\t\t\t\tx = tf.squeeze(x, [1])\n\n\t\t\toutputs_ta = outputs_ta.write(time, x)\n\n\t\t\t#Override input with ground truth\n\t\t\tif test_inputs is not None:\n\t\t\t\tnext_input = tf.expand_dims(test_inputs[:, time, :], axis=1)\n\n\t\t\ttime = tf.Print(time + 1, [time+1, time_length])\n\t\t\t#output = x (maybe next input)\n\t\t\t# if test_inputs is not None:\n\t\t\t# \t#override next_input with ground truth\n\t\t\t# \tnext_input = tf.expand_dims(test_inputs[:, time, :], axis=1)\n\n\t\t\treturn (time, outputs_ta, next_input, loss_outputs_ta, new_queues)\n\n\t\tres = tf.while_loop(\n\t\t\tcondition,\n\t\t\tbody,\n\t\t\tloop_vars=[\n\t\t\t\tinitial_time, initial_outputs_ta, initial_input, initial_loss_outputs_ta, initial_queues\n\t\t\t],\n\t\t\tparallel_iterations=32,\n\t\t\tswap_memory=self._hparams.wavenet_swap_with_cpu)\n\n\t\toutputs_ta = res[1]\n\t\t#[time_length, batch_size, channels]\n\t\toutputs = outputs_ta.stack()\n\n\t\t#Save eval prediction for eval loss computation\n\t\teval_outputs = res[3].stack()\n\n\t\tself.tower_y_hat_eval = []\n\t\tif is_mulaw_quantize(self._hparams.input_type):\n\t\t\tself.tower_y_hat_eval.append(tf.transpose(eval_outputs, [1, 0, 2]))\n\t\telse:\n\t\t\tself.tower_y_hat_eval.append(tf.transpose(eval_outputs, [1, 2, 0]))\n\n\t\t#[batch_size, channels, time_length]\n\t\treturn tf.transpose(outputs, [1, 2, 0])\n\n\tdef clear_queue(self):\n\t\tself.first_conv.clear_queue()\n\t\tfor f in self.conv_layers:\n\t\t\tf.clear_queue()\n\t\tfor f in self.last_conv_layers:\n\t\t\ttry:\n\t\t\t\tf.clear_queue()\n\t\t\texcept AttributeError:\n\t\t\t\tpass\n\n\n'"
