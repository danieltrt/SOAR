file_path,api_count,code
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'TensorFlow.NET\'\ncopyright = \'2019, Haiping Chen\'\nauthor = \'Haiping Chen\'\n\n# The short X.Y version\nversion = \'0.6.0\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.6.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.mathjax\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nfrom recommonmark.parser import CommonMarkParser\nsource_parsers = {\'.md\': CommonMarkParser}\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TensorFlowNETdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TensorFlowNET.tex\', \'TensorFlow.NET Documentation\',\n     \'Haiping Chen\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tensorflownet\', \'TensorFlow.NET Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'TensorFlowNET\', \'TensorFlow.NET Documentation\',\n     author, \'TensorFlowNET\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n'"
test/TensorFlowNET.UnitTest/GradientTest/gradients_test.py,2,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow.ops.gradients.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport warnings\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import function as framework_function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework.constant_op import constant\nfrom tensorflow.python.layers import core as core_layers\nfrom tensorflow.python.ops import array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import custom_gradient\nfrom tensorflow.python.ops import data_flow_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import data_flow_ops  # pylint: disable=unused-import\nfrom tensorflow.python.ops import functional_ops  # pylint: disable=unused-import\nfrom tensorflow.python.ops import gradients\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import list_ops\nfrom tensorflow.python.ops import math_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import state_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.ops.nn_ops import bias_add\nfrom tensorflow.python.platform import googletest\n\n\nclass GradientsTest(test_util.TensorFlowTestCase):\n\n  def testGradients(self):\n    with ops.Graph().as_default():\n      inp = constant(1.0, shape=[32, 100], name=""in"")\n      w = constant(1.0, shape=[100, 10], name=""w"")\n      b = constant(1.0, shape=[10], name=""b"")\n      xw = math_ops.matmul(inp, w, name=""xw"")\n      h = bias_add(xw, b, name=""h"")\n      w_grad = gradients.gradients(h, w)[0]\n    self.assertEquals(""MatMul"", w_grad.op.type)\n    self.assertEquals(w_grad.op._original_op, xw.op)\n    self.assertTrue(w_grad.op.get_attr(""transpose_a""))\n    self.assertFalse(w_grad.op.get_attr(""transpose_b""))\n\n  def testUnusedOutput(self):\n    with ops.Graph().as_default():\n      w = constant(1.0, shape=[2, 2])\n      x = constant(1.0, shape=[2, 2])\n      wx = math_ops.matmul(w, x)\n      split_wx = array_ops.split(value=wx, num_or_size_splits=2, axis=0)\n      c = math_ops.reduce_sum(split_wx[1])\n      gw = gradients.gradients(c, [w])[0]\n    self.assertEquals(""MatMul"", gw.op.type)\n\n  def testColocateGradients(self):\n    with ops.Graph().as_default() as g:\n      w = constant(1.0, shape=[1, 1])\n      x = constant(1.0, shape=[1, 2])\n      with g.device(""/device:GPU:0""):\n        wx = math_ops.matmul(w, x)\n      gw = gradients.gradients(wx, [w], colocate_gradients_with_ops=True)[0]\n    self.assertEqual(gw.op.colocation_groups(), wx.op.colocation_groups())\n\n  def testColocateGradientsWithAggregation(self):\n    with ops.Graph().as_default() as g:\n      with g.device(""/device:GPU:1""):\n        w = constant(1.0, shape=[1, 1])\n      x = constant(1.0, shape=[1, 2])\n      y = constant(1.0, shape=[1, 2])\n      wx = math_ops.matmul(w, x)\n      wy = math_ops.matmul(w, y)\n      with g.device(""/device:GPU:0""):\n        z = wx + wy\n\n      gw1 = gradients.gradients(z, [w], colocate_gradients_with_ops=True)[0]\n      self.assertEqual(gw1.op.colocation_groups(), wx.op.colocation_groups())\n\n      gw2 = gradients.gradients(z, [w], colocate_gradients_with_ops=False)[0]\n      self.assertTrue(wx.op.colocation_groups() != gw2.op.colocation_groups())\n\n  def testColocateGradientsWithAggregationInMultipleDevices(self):\n    with ops.Graph().as_default() as g:\n      with g.device(""/device:GPU:1""):\n        w = constant(1.0, shape=[1, 1])\n      x = constant(1.0, shape=[1, 2])\n      y = constant(1.0, shape=[1, 2])\n      with g.device(""/task:1""):\n        wx = math_ops.matmul(w, x)\n      with g.device(""/task:2""):\n        wy = math_ops.matmul(w, y)\n      with g.device(""/device:GPU:0""):\n        z = wx + wy\n\n      gw1 = gradients.gradients(z, [w], colocate_gradients_with_ops=True)[0]\n      self.assertEqual(gw1.op.colocation_groups(), w.op.colocation_groups())\n\n      gw2 = gradients.gradients(z, [w], colocate_gradients_with_ops=False)[0]\n      self.assertTrue(w.op.colocation_groups() != gw2.op.colocation_groups())\n\n  def testColocateGradientsWithGateGradients(self):\n    if not test_util.is_gpu_available():\n      self.skipTest(""No GPU available"")\n    with ops.Graph().as_default() as g:\n      with g.device(""/device:CPU:0""):\n        x = constant(1.0, shape=[1, 1])\n        y = constant(1.0, shape=[1, 1])\n        s = x + y\n      with g.device(""/device:GPU:0""):\n        z = math_ops.reduce_sum(s)\n\n      gz_x = gradients.gradients(z, [x], colocate_gradients_with_ops=True,\n                                 gate_gradients=True)[0]\n      with session.Session():\n        # Make sure the placer doesn\'t complain.\n        self.evaluate(gz_x)\n\n  def testBoundaryStop(self):\n    # Test that we don\'t differentiate \'x\'. The gradient function for \'x\' is\n    # set explicitly to None so we will get an exception if the gradient code\n    # tries to differentiate \'x\'.\n    with ops.Graph().as_default():\n      c = constant(1.0)\n      x = array_ops.identity(c)\n      y = x + 1.0\n      z = y + 1\n      grads = gradients.gradients(z, [x])\n      self.assertTrue(all(x is not None for x in grads))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testBoundaryContinue(self):\n    # Test that we differentiate both \'x\' and \'y\' correctly when x is a\n    # predecessor of y.\n    with self.cached_session():\n      x = constant(1.0)\n      y = x * 2.0\n      z = y * 3.0\n      grads = gradients.gradients(z, [x, y])\n      self.assertTrue(all(x is not None for x in grads))\n      self.assertEqual(6.0, grads[0].eval())\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testAggregationMethodAccumulateN(self):\n    with self.cached_session():\n      x = constant(1.0)\n      y = x * 2.0\n      z = y + y + y + y + y + y + y + y + y + y\n      grads = gradients.gradients(\n          z, [x, y],\n          aggregation_method=gradients.AggregationMethod.\n          EXPERIMENTAL_ACCUMULATE_N)\n      self.assertTrue(all(x is not None for x in grads))\n      self.assertEqual(20.0, grads[0].eval())\n      self.assertEqual(10.0, grads[1].eval())\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testAggregationMethodAddN(self):\n    with self.cached_session():\n      x = constant(1.0)\n      y = x * 2.0\n      z = y + y + y + y + y + y + y + y + y + y\n      grads = gradients.gradients(\n          z, [x, y], aggregation_method=gradients.AggregationMethod.ADD_N)\n      self.assertTrue(all(x is not None for x in grads))\n      self.assertEqual(20.0, grads[0].eval())\n      self.assertEqual(10.0, grads[1].eval())\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testAggregationMethodTree(self):\n    with self.cached_session():\n      x = constant(1.0)\n      y = x * 2.0\n      z = y + y + y + y + y + y + y + y + y + y\n      grads = gradients.gradients(\n          z, [x, y],\n          aggregation_method=gradients.AggregationMethod.EXPERIMENTAL_TREE)\n      self.assertTrue(all(x is not None for x in grads))\n      self.assertEqual(20.0, grads[0].eval())\n      self.assertEqual(10.0, grads[1].eval())\n\n  def testNoGradientForStringOutputs(self):\n    with ops.Graph().as_default():\n\n      def _TestOpGrad(_, float_grad, string_grad):\n        """"""Gradient function for TestStringOutput.""""""\n        self.assertEquals(float_grad.dtype, dtypes.float32)\n        self.assertFalse(string_grad)\n        return float_grad\n\n      ops.RegisterGradient(""TestStringOutput"")(_TestOpGrad)\n\n      c = constant(1.0)\n      x, _ = test_ops.test_string_output(c)\n      z = x * 2.0\n      w = z * 3.0\n      grads = gradients.gradients(z, [c])\n      self.assertTrue(isinstance(grads[0], ops.Tensor))\n      grads = gradients.gradients(w, [c])\n      self.assertTrue(isinstance(grads[0], ops.Tensor))\n\n  def testSingletonIndexedSlices(self):\n    with ops.Graph().as_default():\n      x = array_ops.placeholder(dtypes.float32)\n      y = array_ops.identity(x)\n      dy = ops.IndexedSlices(\n          array_ops.placeholder(dtypes.float32),\n          array_ops.placeholder(dtypes.int32))\n      dx, = gradients.gradients(y, x, grad_ys=dy)\n      # The IndexedSlices gradient of tf.identity is the identity map.\n      with self.cached_session() as sess:\n        vdx, vdy = sess.run(\n            [dx, dy], feed_dict={x: [1.0], dy.indices: [0], dy.values: [2.0]})\n      self.assertEqual(vdx, vdy)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testNonDifferentiableSwitchInWhileLoop(self):\n    with ops.Graph().as_default():\n      v = array_ops.placeholder(dtypes.float32, [])\n\n      def _Step(i, a, ta):\n        a += math_ops.cast(v, dtypes.int32)\n        return (i + 1, a, ta.write(i, a))\n\n      n = 4\n      i, _, ta = control_flow_ops.while_loop(\n          lambda i, *_: i < n,\n          _Step, [0, 0, tensor_array_ops.TensorArray(\n              dtypes.int32, size=n)])\n      target = ta.read(i - 1)\n      grad, = gradients.gradients(target, v)\n      self.assertIsNone(grad)\n\n  def testVariableReadValueGradient(self):\n    with ops.Graph().as_default():\n      init = constant_op.constant(100.0)\n      var = variables.Variable(init)\n      gradient = gradients.gradients(var.read_value(), var)\n      self.assertIsNotNone(gradient)\n\n  def testVariableAsGraphElementGradient(self):\n    with ops.Graph().as_default() as graph:\n      init = constant_op.constant(100.0)\n      var = variables.Variable(init)\n      gradient = gradients.gradients(graph.as_graph_element(var), var)\n      self.assertIsNotNone(gradient)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testVariableRefGradient(self):\n    with ops.Graph().as_default():\n      init = constant_op.constant(100.0)\n      var = variables.VariableV1(init)\n      gradient = gradients.gradients(var._ref(), var)\n      self.assertIsNotNone(gradient)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testDependentYs(self):\n    with self.cached_session():\n      x = constant_op.constant(3.0)\n      y = math_ops.square(x)\n      y1 = math_ops.square(y)\n      y2 = math_ops.square(y1)\n      g = gradients.gradients([y, y2], x)\n      self.assertAllClose(17502.0, g[0].eval())\n      g = gradients.gradients(y + y2, x)\n      self.assertAllClose(17502.0, g[0].eval())\n      z = array_ops.identity(y)\n      z2 = array_ops.identity(y2)\n      g = gradients.gradients([z, z2], x)\n      self.assertAllClose(17502.0, g[0].eval())\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testPartialDerivatives(self):\n    with self.cached_session():\n      x = constant_op.constant(1.)\n      y = 2 * x\n      z = x + y\n      totalg = gradients.gradients(z, [x, y])\n      self.assertEqual([3.0, 1.0], [g.eval() for g in totalg])\n      partialg = gradients.gradients(z, [x, y], stop_gradients=[x, y])\n      self.assertEqual([1.0, 1.0], [g.eval() for g in partialg])\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testStopGradients(self):\n    def _MakeGraph(rng, stop_gradients=()):\n      def _FunctionOf(xs, k=3):\n        return ops.convert_to_tensor(\n            sum(math_ops.matmul(rng.rand(k, k), x) for x in xs)\n            + rng.rand(k, k))\n\n      a = _FunctionOf([])\n      if ""a"" in stop_gradients: a = array_ops.stop_gradient(a)\n      b = _FunctionOf([a])\n      if ""b"" in stop_gradients: b = array_ops.stop_gradient(b)\n      c = _FunctionOf([a, b])\n      if ""c"" in stop_gradients: c = array_ops.stop_gradient(c)\n      d = _FunctionOf([b, c])\n      if ""d"" in stop_gradients: d = array_ops.stop_gradient(d)\n      return dict(a=a, b=b, c=c, d=d)\n\n    def _Gradients(ys, xs, **kwargs):\n      dydxs = gradients.gradients(ys, xs, **kwargs)\n      dydxs = [0. * x if dydx is None else dydx\n               for x, dydx in zip(xs, dydxs)]\n      return dydxs\n\n    seed = np.random.randint(1000)\n    cases = []\n    subsets = [""""] + ""a b c d ab ac ad bc bd cd abc abd acd bcd abcd"".split()\n    graph = _MakeGraph(np.random.RandomState(seed))\n    for constants in subsets:\n      graph_with_stops = _MakeGraph(np.random.RandomState(seed), constants)\n      for variables_ in subsets:\n        # compute the gradient when stopped using tf.stop_gradients\n        grad1 = _Gradients([graph_with_stops[""d""]],\n                           [graph_with_stops[v] for v in variables_])\n        # compute the gradient when stopped using the stop_gradients kwarg\n        grad2 = _Gradients([graph[""d""]],\n                           [graph[v] for v in variables_],\n                           stop_gradients=[graph[v] for v in constants])\n        cases.append(dict(grad1=grad1, grad2=grad2,\n                          constants=constants, variables=variables_))\n\n    # evaluate all tensors in one call to session.run for speed\n    with self.cached_session() as sess:\n      results = sess.run([(case[""grad1""], case[""grad2""]) for case in cases])\n\n    for (npgrad1, npgrad2), case in zip(results, cases):\n      for a, b in zip(npgrad1, npgrad2):\n        np.testing.assert_allclose(a, b)\n\n  def testUnconnectedGradientsNoneUnconnectedGradients(self):\n    with ops.Graph().as_default():\n      x = constant(1.0, shape=[2, 2])\n      y = constant(3.0, shape=[3, 1])\n      grad = gradients.gradients(\n          [y], [x], unconnected_gradients=""none"")\n    self.assertIsNone(grad[0])\n\n  def testUnconnectedGradientsZerosUnconnectedGradients(self):\n    with ops.Graph().as_default():\n      x = constant(1.0, shape=[2, 2])\n      y = constant(3.0, shape=[3, 1])\n      grads = gradients.gradients(\n          [y], [x], unconnected_gradients=""zero"")\n      with self.cached_session() as sess:\n        self.assertAllEqual([[0.0, 0.0], [0.0, 0.0]], self.evaluate(grads)[0])\n\n  def testUnconnectedGradientsZeroConnectedGradients(self):\n    with ops.Graph().as_default():\n      x = constant(1.0)\n      y = x * 3.0\n      grad = gradients.gradients(\n          [y], [x], unconnected_gradients=""zero"")\n      with self.cached_session() as sess:\n        self.assertEquals(3.0, self.evaluate(grad)[0])\n\n  def testUnknownUnconnectedGradientsValueGiven(self):\n    with ops.Graph().as_default():\n      x = constant(1.0)\n      y = constant(1.0)\n      with self.assertRaisesRegexp(\n          ValueError, ""Unknown value for unconnected_gradients: \'nonsense\'""):\n        gradients.gradients([y], [x], unconnected_gradients=""nonsense"")\n\n\nclass FunctionGradientsTest(test_util.TensorFlowTestCase):\n\n  @classmethod\n  def XSquarePlusB(cls, x, b):\n    return x * x + b\n\n  @classmethod\n  def XSquarePlusBGradient(cls, x, b, g):\n    # Perturb gradients (multiply by 2), so we can test that this was called.\n    g *= 2.0\n    return g * 2.0 * x, g\n\n  @classmethod\n  def _PythonGradient(cls, op, grad):\n    # Perturb gradients (multiply by 3), so we can test that this was called.\n    grad *= 3.0\n    return grad * op.inputs[0] * 2.0, grad\n\n  @classmethod\n  def _GetFunc(cls, **kwargs):\n    return framework_function.Defun(dtypes.float32, dtypes.float32, **\n                                    kwargs)(cls.XSquarePlusB)\n\n  def _GetFuncGradients(self, f, x_value, b_value):\n    x = constant_op.constant(x_value, name=""x"")\n    b = constant_op.constant(b_value, name=""b"")\n\n    y = f(x, b)\n    grads = gradients.gradients(y, [x, b])\n    with self.cached_session() as sess:\n      return sess.run(grads)\n\n  def testFunctionGradientsBasic(self):\n    g = ops.Graph()\n    with g.as_default():\n      f = self._GetFunc()\n      # Get gradients (should add SymbolicGradient node for function).\n      grads = self._GetFuncGradients(f, [2.0], [1.0])\n      self.assertAllEqual([4.0], grads[0])\n      self.assertAllEqual([1.0], grads[1])\n\n  def testFunctionGradientsComposition(self):\n    with ops.Graph().as_default():\n      f = self._GetFunc()\n      x = constant_op.constant([2.0], name=""x"")\n      b1 = constant_op.constant([1.0], name=""b1"")\n      b2 = constant_op.constant([1.0], name=""b2"")\n\n      y = f(f(x, b1), b2)\n      # Build gradient graph (should add SymbolicGradient node for function).\n      grads = gradients.gradients(y, [x, b1])\n\n      with self.cached_session() as sess:\n        self.assertAllEqual([40.0], self.evaluate(grads)[0])\n        self.assertAllEqual([10.0], self.evaluate(grads)[1])\n\n  def testFunctionGradientsWithGradFunc(self):\n    g = ops.Graph()\n    with g.as_default():\n      grad_func = framework_function.Defun(dtypes.float32, dtypes.float32,\n                                           dtypes.float32)(\n                                               self.XSquarePlusBGradient)\n      f = self._GetFunc(grad_func=grad_func)\n      # Get gradients (should add SymbolicGradient node for function, which\n      # uses the grad_func above, which multiplies all gradients by 2).\n      grads = self._GetFuncGradients(f, [2.0], [1.0])\n      self.assertAllEqual([4.0 * 2], grads[0])\n      self.assertAllEqual([1.0 * 2], grads[1])\n\n  def testFunctionGradientWithRegistration(self):\n    g = ops.Graph()\n    with g.as_default():\n      f = self._GetFunc(python_grad_func=self._PythonGradient)\n      # Get gradients, using the python gradient function. It multiplies the\n      # gradients by 3.\n      grads = self._GetFuncGradients(f, [2.0], [1.0])\n      self.assertAllEqual([4.0 * 3], grads[0])\n      self.assertAllEqual([1.0 * 3], grads[1])\n\n  def testFunctionGradientWithGradFuncAndRegistration(self):\n    g = ops.Graph()\n    with g.as_default():\n      grad_func = framework_function.Defun(dtypes.float32, dtypes.float32,\n                                           dtypes.float32)(\n                                               self.XSquarePlusBGradient)\n      with self.assertRaisesRegexp(ValueError, ""Gradient defined twice""):\n        f = self._GetFunc(\n            grad_func=grad_func, python_grad_func=self._PythonGradient)\n        f.add_to_graph(ops.Graph())\n\n  def testGradientWrtCaptured(self):\n    with ops.Graph().as_default():\n      x = constant_op.constant(1.0, name=""x"")\n\n      @function.defun()\n      def Foo():\n        y = math_ops.multiply(x, 2.0, name=""y"")\n        g = gradients_impl.gradients(y, x)\n        return g[0]\n\n      f = Foo()\n      with self.cached_session() as sess:\n        self.assertEqual(self.evaluate(f), 2.0)\n\n  def testGradientOfCaptured(self):\n    with ops.Graph().as_default():\n      x = constant_op.constant(1.0, name=""x"")\n      y = math_ops.multiply(x, 2.0, name=""y"")\n\n      @framework_function.Defun()\n      def Foo():\n        g = gradients_impl.gradients(y, x)\n        return g[0]\n\n      f = Foo()\n      with self.cached_session() as sess:\n        self.assertEqual(self.evaluate(f), 2.0)\n\n  def testCapturedResourceVariable(self):\n    with ops.Graph().as_default():\n      var = resource_variable_ops.ResourceVariable(1.0, name=""var"")\n\n      @function.defun()\n      def Foo():\n        y = math_ops.multiply(var, 2.0, name=""y"")\n        g = gradients_impl.gradients(y, var)\n        return g[0]\n\n      f = Foo()\n      with self.cached_session() as sess:\n        self.evaluate(variables.global_variables_initializer())\n        self.assertEqual(self.evaluate(f), 2.0)\n\n  def testCapturedNested(self):\n    with ops.Graph().as_default():\n      x1 = constant_op.constant(1.0, name=""x1"")\n      x2 = constant_op.constant(2.0, name=""x2"")\n      x3 = math_ops.multiply(x1, x2, name=""x3"")\n\n      @function.defun()\n      def Outer():\n        outer1 = array_ops.identity(x1, name=""outer1"")\n\n        @function.defun()\n        def Inner():\n          inner1 = array_ops.identity(outer1, name=""inner1"")\n          inner2 = array_ops.identity(x2, name=""inner2"")\n          inner3 = array_ops.identity(x3, name=""inner3"")\n          return gradients_impl.gradients([inner1, inner2, inner3, x1],\n                                          [x1, x2])\n\n        return Inner()\n\n      x1_grad, x2_grad = Outer()\n      with self.cached_session() as sess:\n        # 1.0 + None + 2.0 + 1.0 = 4.0\n        self.assertEqual(self.evaluate(x1_grad), 4.0)\n        # None + 1.0 + 1.0 + None = 2.0\n        self.assertEqual(self.evaluate(x2_grad), 2.0)\n\n  def testCapturedFromFunction(self):\n    with ops.Graph().as_default():\n      x = constant_op.constant(1.0, name=""x"")\n\n      @function.defun()\n      def Outer():\n        y = math_ops.multiply(x, 2.0, name=""y"")\n\n        @function.defun()\n        def Inner():\n          z = math_ops.multiply(y, 3.0, name=""z"")\n          g = gradients_impl.gradients(z, y)\n          return g[0]\n\n        return Inner()\n\n      z_grad = Outer()\n      with self.cached_session() as sess:\n        self.assertEqual(self.evaluate(z_grad), 3.0)\n\n  def testCapturedEagerTensors(self):\n    # Test that we can handle captured eager tensors unrelated to the gradient\n    # computation (i.e. we need to ignore them).\n    # TODO(skyewm): make it an error if you try to take the gradient wrt a\n    # captured EagerTensor\n    with context.eager_mode():\n      c = constant_op.constant(2.0, name=""c"")\n\n      @function.defun\n      def Foo():\n        x = constant_op.constant(10.0, name=""x"")\n        y = math_ops.multiply(x, c, name=""y"")\n        z = math_ops.multiply(y, 3.0, name=""z"")\n        g = gradients_impl.gradients(z, x)\n        return g[0]\n\n      self.assertEqual(Foo().numpy(), 6.0)\n\n\nclass StopGradientTest(test_util.TensorFlowTestCase):\n\n  def testStopGradient(self):\n    with ops.Graph().as_default():\n      inp = constant(1.0, shape=[100, 32], name=""in"")\n      out = array_ops.stop_gradient(inp)\n      igrad = gradients.gradients(out, inp)[0]\n    assert igrad is None\n\n\nclass PreventGradientTest(test_util.TensorFlowTestCase):\n\n  def testPreventGradient(self):\n    with ops.Graph().as_default():\n      inp = constant(1.0, shape=[100, 32], name=""in"")\n      out = array_ops.prevent_gradient(inp)\n      with self.assertRaisesRegexp(LookupError, ""explicitly disabled""):\n        _ = gradients.gradients(out, inp)\n\n\nclass HessianVectorProductTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessianVectorProduct(self):\n    # Manually compute the Hessian explicitly for a low-dimensional problem\n    # and check that HessianVectorProduct matches multiplication by the\n    # explicit Hessian.\n    # Specifically, the Hessian of f(x) = x^T A x is\n    # H = A + A^T.\n    # We expect HessianVectorProduct(f(x), x, v) to be H v.\n    m = 4\n    rng = np.random.RandomState([1, 2, 3])\n    mat_value = rng.randn(m, m).astype(""float32"")\n    v_value = rng.randn(m, 1).astype(""float32"")\n    x_value = rng.randn(m, 1).astype(""float32"")\n    hess_value = mat_value + mat_value.T\n    hess_v_value = np.dot(hess_value, v_value)\n    for use_gpu in [False, True]:\n      with self.cached_session(use_gpu=use_gpu):\n        mat = constant_op.constant(mat_value)\n        v = constant_op.constant(v_value)\n        x = constant_op.constant(x_value)\n        mat_x = math_ops.matmul(mat, x, name=""Ax"")\n        x_mat_x = math_ops.matmul(array_ops.transpose(x), mat_x, name=""xAx"")\n        hess_v = gradients_impl._hessian_vector_product(x_mat_x, [x], [v])[0]\n        hess_v_actual = self.evaluate(hess_v)\n      self.assertAllClose(hess_v_value, hess_v_actual)\n\n\nclass HessianTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessian1D(self):\n    # Manually compute the Hessian explicitly for a low-dimensional problem\n    # and check that `hessian` matches. Specifically, the Hessian of\n    # f(x) = x^T A x is H = A + A^T.\n    m = 4\n    rng = np.random.RandomState([1, 2, 3])\n    mat_value = rng.randn(m, m).astype(""float32"")\n    x_value = rng.randn(m).astype(""float32"")\n    hess_value = mat_value + mat_value.T\n    with self.session(use_gpu=True):\n      mat = constant_op.constant(mat_value)\n      x = constant_op.constant(x_value)\n      x_mat_x = math_ops.reduce_sum(x[:, None] * mat * x[None, :])\n      hess = gradients.hessians(x_mat_x, x)[0]\n      hess_actual = self.evaluate(hess)\n    self.assertAllClose(hess_value, hess_actual)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessian1D_multi(self):\n    # Test the computation of the hessian with respect to multiple tensors\n    m = 4\n    n = 3\n    rng = np.random.RandomState([1, 2, 3])\n    mat_values = [rng.randn(m, m).astype(""float32"") for _ in range(n)]\n    x_values = [rng.randn(m).astype(""float32"") for _ in range(n)]\n    hess_values = [mat_value + mat_value.T for mat_value in mat_values]\n    with self.session(use_gpu=True):\n      mats = [constant_op.constant(mat_value) for mat_value in mat_values]\n      xs = [constant_op.constant(x_value) for x_value in x_values]\n      xs_mats_xs = [\n          math_ops.reduce_sum(x[:, None] * mat * x[None, :])\n          for x, mat in zip(xs, mats)\n      ]\n      hessians = gradients.hessians(xs_mats_xs, xs)\n      hessians_actual = [hess.eval() for hess in hessians]\n    for hess_value, hess_actual in zip(hess_values, hessians_actual):\n      self.assertAllClose(hess_value, hess_actual)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessianInvalidDimension(self):\n    for shape in [(10, 10), None]:\n      with self.cached_session(use_gpu=True):\n        x = array_ops.placeholder(dtypes.float32, shape)\n        # Expect a ValueError because the dimensions are wrong\n        with self.assertRaises(ValueError):\n          gradients.hessians(x, x)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessian2D_square_matrix(self):\n    # Manually compute the Hessian explicitly for a low-dimensional problem\n    # and check that `hessian` matches. Specifically, the Hessian of\n    # f(x) = 1/2 * x^T * x is H = constant (block identity matrix)\n    m = 3\n    rng = np.random.RandomState([1, 2, 3])\n    x_value = rng.randn(m, m).astype(""float32"")\n    with self.session(use_gpu=True):\n      x = constant_op.constant(x_value)\n      x_square = math_ops.reduce_sum(\n          math_ops.matmul(array_ops.transpose(x), x) * 0.5\n      )\n      hess = gradients.hessians(x_square, x)[0]\n      hess_actual = self.evaluate(hess)\n    hess_value = np.bmat([\n        [elem*np.ones((m, m)) for elem in vec]\n        for vec in np.eye(m)\n    ]).astype(""float32"")\n    self.assertAllEqual((m, m, m, m), hess_actual.shape)\n    self.assertAllClose(hess_value, hess_actual.reshape((m * m, m * m)))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testHessian2D_non_square_matrix(self):\n    m = 3\n    n = 4\n    rng = np.random.RandomState([1, 2, 3])\n    x_value = rng.randn(m, n).astype(""float32"")\n    with self.session(use_gpu=True):\n      x = constant_op.constant(x_value)\n      x_square = math_ops.reduce_sum(\n          math_ops.matmul(array_ops.transpose(x), x) * 0.5\n      )\n      hess = gradients.hessians(x_square, x)[0]\n      hess_actual = self.evaluate(hess)\n    hess_value = np.bmat([\n        [elem*np.ones((n, n)) for elem in vec]\n        for vec in np.eye(m)\n    ]).astype(""float32"")\n    self.assertAllEqual((m, n, m, n), hess_actual.shape)\n    self.assertAllClose(hess_value, hess_actual.reshape((m * n, m * n)))\n\n\nclass IndexedSlicesToTensorTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testIndexedSlicesToTensor(self):\n    with self.cached_session():\n      np_val = np.random.rand(4, 4, 4, 4).astype(np.float32)\n      c = constant_op.constant(np_val)\n      c_sparse = math_ops._as_indexed_slices(c)\n      self.assertAllEqual(np_val.shape, c_sparse.dense_shape.eval())\n      c_dense = math_ops.multiply(c_sparse, 1.0)\n      self.assertAllClose(np_val, self.evaluate(c_dense))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testIndexedSlicesToTensorList(self):\n    with self.cached_session():\n      numpy_list = []\n      dense_list = []\n      sparse_list = []\n      for _ in range(3):\n        np_val = np.random.rand(4, 4, 4, 4).astype(np.float32)\n        c = constant_op.constant(np_val)\n        c_sparse = math_ops._as_indexed_slices(c)\n        numpy_list.append(np_val)\n        dense_list.append(c)\n        sparse_list.append(c_sparse)\n      packed_dense = array_ops.stack(dense_list)\n      packed_sparse = array_ops.stack(sparse_list)\n      self.assertAllClose(packed_dense.eval(), self.evaluate(packed_sparse))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testInt64Indices(self):\n    with self.cached_session():\n      np_val = np.random.rand(4, 4, 4, 4).astype(np.float32)\n      c = constant_op.constant(np_val)\n      c_sparse = math_ops._as_indexed_slices(c)\n      c_sparse = ops.IndexedSlices(\n          c_sparse.values,\n          math_ops.cast(c_sparse.indices, dtypes.int64), c_sparse.dense_shape)\n      self.assertAllEqual(np_val.shape, c_sparse.dense_shape.eval())\n      c_dense = math_ops.multiply(c_sparse, 1.0)\n      self.assertAllClose(np_val, self.evaluate(c_dense))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testWarnings(self):\n    # TODO(gunan) Reenable after this issue is fixed:\n    # https://github.com/google/protobuf/issues/2812\n    if sys.version_info >= (3, 5):\n      self.skipTest(""Skipped test for Python 3.5+"")\n\n    # Smaller than the threshold: no warning.\n    c_sparse = ops.IndexedSlices(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.int32), constant([4, 4, 4, 4]))\n    with warnings.catch_warnings(record=True) as w:\n      math_ops.multiply(c_sparse, 1.0)\n    self.assertEqual(0, len(w))\n\n    # Greater than or equal to the threshold: warning.\n    c_sparse = ops.IndexedSlices(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.int32), constant([100, 100, 100, 100]))\n    # ""always"" filter prevents the warning from being suppressed if it was\n    # already triggered in a different test.\n    warnings.simplefilter(""always"")\n    with warnings.catch_warnings(record=True) as w:\n      math_ops.multiply(c_sparse, 1.0)\n    self.assertEqual(1, len(w))\n    self.assertTrue(\n        ""with 100000000 elements. This may consume a large amount of memory."" in\n        str(w[0].message))\n\n    # Unknown dense shape: warning.\n    c_sparse = ops.IndexedSlices(\n        array_ops.placeholder(dtypes.float32),\n        array_ops.placeholder(dtypes.int32),\n        array_ops.placeholder(dtypes.int32))\n    with warnings.catch_warnings(record=True) as w:\n      math_ops.multiply(c_sparse, 1.0)\n    self.assertEqual(1, len(w))\n    self.assertTrue(\n        ""of unknown shape. This may consume a large amount of memory."" in\n        str(w[0].message))\n\n\nclass OnlyRealGradientsTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testRealOnly(self):\n    x = constant_op.constant(7+3j, dtype=dtypes.complex64)\n    y = math_ops.square(x)\n    with self.assertRaisesRegexp(\n        TypeError,\n        r""Gradients of complex tensors must set grad_ys ""\n        r""\\(y\\.dtype = tf\\.complex64\\)""):\n      gradients.gradients(y, x)\n\n\nclass ResourceCondTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testBasic(self):\n    gamma = resource_variable_ops.ResourceVariable(\n        np.random.random((3,)),\n        dtype=""float32"", name=""gamma"")\n\n    inputs = array_ops.ones(shape=(3,), dtype=""float32"")\n\n    def TestFn():\n      output = inputs + gamma\n      return output\n\n    training = array_ops.placeholder_with_default(True, shape=())\n    output = control_flow_ops.cond(\n        training, TestFn, lambda: inputs)\n\n    loss = output\n\n    grads = gradients.gradients(\n        loss, [gamma])\n    self.assertTrue(None not in grads)\n\n\nclass CustomGradientTest(test_util.TensorFlowTestCase):\n\n  def testCustomGradientTrivial(self):\n\n    @custom_gradient.custom_gradient\n    def MyIdentity(x):\n\n      def Grad(dy):\n        return [3 * dy]\n\n      return x, Grad\n\n    with ops.Graph().as_default():\n      x = constant(3.)\n      y = MyIdentity(MyIdentity(x))\n      dy = gradients.gradients(y, x)[0]\n      with session.Session():\n        self.assertEqual(9., self.evaluate(dy))\n\n  def testCustomGradient(self):\n\n    @custom_gradient.custom_gradient\n    def MyMultiply(x1, x2):\n      result = x1 * x2\n\n      def Grad(dy):\n        # Switched the ordering here.\n        return [dy * x1, dy * x2]\n\n      return result, Grad\n\n    with ops.Graph().as_default():\n      x1 = constant(3.)\n      x2 = constant(5.)\n      y = MyMultiply(x1, x2)\n      dy = gradients.gradients(y, [x1, x2])\n      with session.Session() as sess:\n        self.assertAllEqual([3., 5.], self.evaluate(dy))\n\n  def testCustomGradientErrors(self):\n\n    @custom_gradient.custom_gradient\n    def F(x):\n\n      def Grad(_):\n        raise RuntimeError(""x"")\n\n      return x, Grad\n\n    with ops.Graph().as_default():\n      x = constant(1.0)\n      y = F(x)\n      with self.assertRaises(RuntimeError):\n        gradients.gradients(y, x)\n\n  def testCustomGradientWithVariables(self):\n\n    @custom_gradient.custom_gradient\n    def F(x):\n      out = core_layers.dense(x, 3, use_bias=False)\n\n      def Grad(out_grad, variables=None):  # pylint: disable=redefined-outer-name\n        self.assertEqual(1, len(variables))\n        grads = gradients.gradients(out, [x, variables[0]], grad_ys=out_grad)\n        return grads[0], [array_ops.ones((4, 3))]\n\n      return out, Grad\n\n    with ops.Graph().as_default():\n      x = array_ops.ones((2, 4))\n      with variable_scope.variable_scope(""f"", use_resource=True) as vs:\n        y = F(x)\n        all_vars = vs.global_variables()\n        assert len(all_vars) == 1\n      grads = gradients.gradients(y, [x, all_vars[0]])\n      for g in grads:\n        self.assertTrue(g is not None)\n      with session.Session() as sess:\n        self.evaluate(variables.global_variables_initializer())\n        dw = sess.run(math_ops.reduce_sum(grads[1]))\n        self.assertEqual(12., dw)\n\n  def testCustomGradientWithVariablesEager(self):\n    with context.eager_mode():\n      layer = core_layers.Dense(4, use_bias=False)\n\n      @custom_gradient.custom_gradient\n      def F(x):\n        out = layer(x)\n\n        def Grad(out_grad, variables=None):  # pylint: disable=redefined-outer-name\n          del out_grad\n          self.assertEqual(1, len(variables))\n          return (array_ops.ones((3, 2)),\n                  [array_ops.ones((2, 4))])\n\n        return out, Grad\n\n      x = array_ops.ones((3, 2)) + 2.\n      with backprop.GradientTape() as tape:\n        tape.watch(x)\n        y = F(x)\n      w, = layer.variables\n      dx, dw = tape.gradient(y, [x, w])\n      self.assertEqual(6., math_ops.reduce_sum(dx).numpy())\n      self.assertEqual(8., math_ops.reduce_sum(dw).numpy())\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testCustomGradientErrorsWithNonResourceVariables(self):\n\n    def F(x, use_resource=False):\n      with variable_scope.variable_scope(""f"", use_resource=use_resource):\n        out = core_layers.dense(x, 4, use_bias=False)\n\n      def Grad(out_grad, variables=None):  # pylint: disable=redefined-outer-name\n        del out_grad\n        self.assertEqual(1, len(variables))\n        return (array_ops.ones((3, 2)), [array_ops.ones((2, 4))])\n\n      return out, Grad\n\n    @custom_gradient.custom_gradient\n    def FResource(x):\n      return F(x, use_resource=True)\n\n    @custom_gradient.custom_gradient\n    def FNonResource(x):\n      return F(x, use_resource=False)\n\n    x = array_ops.ones((3, 2)) + 2.\n\n    # Wrapping scope has use_resource=True but inner scope sets to False. Fails.\n    with variable_scope.variable_scope(""vs1"", use_resource=True):\n      with self.assertRaisesWithPredicateMatch(TypeError,\n                                               ""must be `ResourceVariable`s""):\n        FNonResource(x)\n\n    # Wrapping scope has use_resource=False but inner scope sets to True.\n    # Passes.\n    with variable_scope.variable_scope(""vs2"", use_resource=False):\n      FResource(x)\n\n  def testWithNumpyInputs(self):\n    with context.eager_mode():\n\n      @custom_gradient.custom_gradient\n      def F(x):\n        out = x\n\n        def Grad(_):\n          return (None, None)\n\n        return out, Grad\n\n      x = np.ones((3, 2), dtype=np.float32)\n      # Smoke test to ensure numpy inputs are accepted\n      F(x)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testRVGradientsDynamicCond(self):\n    with self.cached_session():\n      alpha = resource_variable_ops.ResourceVariable(\n          np.random.random((1,)),\n          dtype=""float32"")\n\n      conditional = array_ops.placeholder_with_default(True, shape=())\n      output = control_flow_ops.cond(\n          conditional, lambda: alpha * 2, lambda: alpha * 3)\n\n      g, = gradients_impl.gradients(output, alpha)\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(g.eval(), [2.0])\n      self.assertAllEqual(g.eval(feed_dict={conditional: False}), [3.0])\n\n\nclass AggregateIndexedSlicesGradientsTest(test_util.TensorFlowTestCase):\n\n  def _assert_indexed_slices_equal(self, left, right):\n    self.assertAllEqual(\n        self.evaluate(ops.convert_to_tensor(left)),\n        self.evaluate(ops.convert_to_tensor(right)))\n\n  def testNoGradients(self):\n    self.assertIsNone(gradients_impl._AggregateIndexedSlicesGradients([]))\n\n  def testOneGradient(self):\n    t = math_ops._as_indexed_slices(constant_op.constant(\n        [[1., 2.], [0, 0], [3., 4.]]))\n    result = gradients_impl._AggregateIndexedSlicesGradients([t])\n    self._assert_indexed_slices_equal(t, result)\n\n  def testMultipleGradients(self):\n    t0 = math_ops._as_indexed_slices(constant_op.constant(\n        [[1., 2.], [0, 0], [3., 4.]]))\n    t1 = math_ops._as_indexed_slices(constant_op.constant(\n        [[0., 0.], [5, 6], [7., 8.]]))\n    total = constant_op.constant(\n        [[1., 2.], [5, 6], [10., 12.]])\n    result = gradients_impl._AggregateIndexedSlicesGradients([t0, t1])\n    self._assert_indexed_slices_equal(total, result)\n\n  def testMultipleGradientsWithNones(self):\n    t0 = math_ops._as_indexed_slices(constant_op.constant(\n        [[1., 2.], [0, 0], [3., 4.]]))\n    t1 = math_ops._as_indexed_slices(constant_op.constant(\n        [[0., 0.], [5, 6], [7., 8.]]))\n    t3 = None\n    total = constant_op.constant(\n        [[1., 2.], [5, 6], [10., 12.]])\n    result = gradients_impl._AggregateIndexedSlicesGradients([t0, t1, t3])\n    self._assert_indexed_slices_equal(total, result)\n\n  def testMixedTensorAndIndexedSlices(self):\n    t0 = math_ops._as_indexed_slices(constant_op.constant(\n        [[1., 2.], [0, 0], [3., 4.]]))\n    t1 = constant_op.constant(\n        [[0., 0.], [5, 6], [7., 8.]])\n    total = constant_op.constant(\n        [[1., 2.], [5, 6], [10., 12.]])\n    result = gradients_impl._AggregateIndexedSlicesGradients([t0, t1])\n    self._assert_indexed_slices_equal(total, result)\n\n\nclass TensorListGradientsTest(test_util.TensorFlowTestCase):\n\n  def testDefaultGradYs(self):\n    with ops.Graph().as_default():\n      tl = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=ops.convert_to_tensor([], dtype=dtypes.int32))\n      a = constant(1.0)\n      tl = list_ops.tensor_list_push_back(tl, a)\n\n      grad_tl = list_ops.empty_tensor_list(\n          element_dtype=dtypes.float32,\n          element_shape=ops.convert_to_tensor([], dtype=dtypes.int32))\n      grad_tl = list_ops.tensor_list_push_back(tl, constant(5.0))\n\n      grad = gradients.gradients(tl, a, grad_ys=grad_tl)[0]\n      with self.cached_session() as sess:\n        self.assertEquals(self.evaluate(grad), 5.)\n\n\nif __name__ == ""__main__"":\n  googletest.main()\n'"
test/TensorFlowNET.UnitTest/control_flow_ops_test/control_flow_ops_test.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for control_flow_ops.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport numpy as np\n\nfrom tensorflow.python import tf2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.tensor_array_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.training import momentum\nfrom tensorflow.python.util import nest\n\n\nTestTuple = collections.namedtuple(""TestTuple"", ""a b"")\nSingletonTestTuple = collections.namedtuple(""SingletonTestTuple"", ""a"")\n\n\nclass GroupTestCase(test_util.TensorFlowTestCase):\n\n  def _StripNode(self, nd):\n    snode = node_def_pb2.NodeDef(name=nd.name, op=nd.op, input=nd.input)\n    if nd.device:\n      snode.device = nd.device\n    return snode\n\n  def _StripGraph(self, gd):\n    """"""Copy gd keeping only, node.name, node.op, node.input, and node.device.""""""\n    return graph_pb2.GraphDef(node=[self._StripNode(nd) for nd in gd.node])\n\n  def testGroup_NoDevices(self):\n    with ops.Graph().as_default() as g:\n      a = constant_op.constant(0, name=""a"")\n      b = constant_op.constant(0, name=""b"")\n      c = constant_op.constant(0, name=""c"")\n      control_flow_ops.group(a.op, b.op, c.op, name=""root"")\n    gd = g.as_graph_def()\n    self.assertProtoEquals(""""""\n      node { name: ""a"" op: ""Const""}\n      node { name: ""b"" op: ""Const""}\n      node { name: ""c"" op: ""Const""}\n      node { name: ""root"" op: ""NoOp"" input: ""^a"" input: ""^b"" input: ""^c"" }\n    """""", self._StripGraph(gd))\n\n  def testGroup_OneDevice(self):\n    with ops.Graph().as_default() as g:\n      with g.device(""/task:0""):\n        a = constant_op.constant(0, name=""a"")\n        b = constant_op.constant(0, name=""b"")\n      control_flow_ops.group(a.op, b.op, name=""root"")\n    gd = g.as_graph_def()\n    self.assertProtoEquals(""""""\n      node { name: ""a"" op: ""Const"" device: ""/task:0"" }\n      node { name: ""b"" op: ""Const"" device: ""/task:0"" }\n      node { name: ""root"" op: ""NoOp"" input: ""^a"" input: ""^b"" device: ""/task:0"" }\n    """""", self._StripGraph(gd))\n\n  def testGroup_MultiDevice(self):\n    with ops.Graph().as_default() as g:\n      with g.device(""/task:0""):\n        a = constant_op.constant(0, name=""a"")\n        b = constant_op.constant(0, name=""b"")\n      with g.device(""/task:1""):\n        c = constant_op.constant(0, name=""c"")\n        d = constant_op.constant(0, name=""d"")\n      with g.device(""/task:2""):\n        control_flow_ops.group(a.op, b.op, c.op, d.op, name=""root"")\n    gd = g.as_graph_def()\n    self.assertProtoEquals(""""""\n      node { name: ""a"" op: ""Const"" device: ""/task:0""}\n      node { name: ""b"" op: ""Const"" device: ""/task:0""}\n      node { name: ""c"" op: ""Const"" device: ""/task:1""}\n      node { name: ""d"" op: ""Const"" device: ""/task:1""}\n      node { name: ""root/NoOp"" op: ""NoOp"" input: ""^a"" input: ""^b""\n             device: ""/task:0"" }\n      node { name: ""root/NoOp_1"" op: ""NoOp"" input: ""^c"" input: ""^d""\n             device: ""/task:1"" }\n      node { name: ""root"" op: ""NoOp"" input: ""^root/NoOp"" input: ""^root/NoOp_1""\n             device: ""/task:2"" }\n    """""", self._StripGraph(gd))\n\n  def testPassingList(self):\n    with ops.Graph().as_default() as g:\n      a = constant_op.constant(0, name=""a"")\n      b = constant_op.constant(0, name=""b"")\n      control_flow_ops.group([a.op, b.op], name=""root"")\n    gd = g.as_graph_def()\n    self.assertProtoEquals(""""""\n      node { name: ""a"" op: ""Const""}\n      node { name: ""b"" op: ""Const""}\n      node { name: ""root"" op: ""NoOp"" input: ""^a"" input: ""^b"" }\n    """""", self._StripGraph(gd))\n\n  @test_util.run_deprecated_v1\n  def testPassingNonTensors(self):\n    with self.assertRaises(TypeError):\n      control_flow_ops.group(1, 2)\n\n\nclass ShapeTestCase(test_util.TensorFlowTestCase):\n\n  def testShape(self):\n    tensor = constant_op.constant([1.0, 2.0])\n    self.assertEquals([2], tensor.get_shape())\n    self.assertEquals([2],\n                      control_flow_ops.with_dependencies(\n                          [constant_op.constant(1.0)], tensor).get_shape())\n\n\nclass WithDependenciesTestCase(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testTupleDependencies(self):\n    counter = variable_scope.get_variable(\n        ""my_counter"", shape=[], initializer=init_ops.zeros_initializer())\n    increment_counter = state_ops.assign_add(counter, 1)\n    const_with_dep = control_flow_ops.with_dependencies(\n        (increment_counter, constant_op.constant(42)),\n        constant_op.constant(7))\n\n    self.evaluate(variables.global_variables_initializer())\n    self.assertEquals(0, self.evaluate(counter))\n    self.assertEquals(7, self.evaluate(const_with_dep))\n    self.assertEquals(1, self.evaluate(counter))\n\n  @test_util.run_deprecated_v1\n  def testListDependencies(self):\n    counter = variable_scope.get_variable(\n        ""my_counter"", shape=[], initializer=init_ops.zeros_initializer())\n    increment_counter = state_ops.assign_add(counter, 1)\n    const_with_dep = control_flow_ops.with_dependencies(\n        [increment_counter, constant_op.constant(42)],\n        constant_op.constant(7))\n\n    self.evaluate(variables.global_variables_initializer())\n    self.assertEquals(0, self.evaluate(counter))\n    self.assertEquals(7, self.evaluate(const_with_dep))\n    self.assertEquals(1, self.evaluate(counter))\n\n\nclass SwitchTestCase(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesWithDenseShape(self):\n    with self.cached_session():\n      data = ops.IndexedSlices(\n          constant_op.constant([1, 2, 3]),\n          constant_op.constant([0, 1]),\n          dense_shape=constant_op.constant([3]))\n      zero = constant_op.constant(0)\n      one = constant_op.constant(1)\n      less_op = math_ops.less(zero, one)\n      _, switch_true = control_flow_ops.switch(data, less_op)\n      self.assertAllEqual([1, 2, 3], switch_true.values.eval())\n      self.assertAllEqual([0, 1], switch_true.indices.eval())\n\n  @test_util.run_deprecated_v1\n  def testIndexedSlicesGradient(self):\n    embedding_matrix = variable_scope.get_variable(\n        ""embedding_matrix"", [5, 5],\n        initializer=init_ops.random_normal_initializer())\n\n    def cond(it, _):\n      return it < 5\n\n    def body(it, cost):\n      embedding = embedding_ops.embedding_lookup(embedding_matrix + 0.0, [0])\n      cost += math_ops.reduce_sum(embedding)\n      return it + 1, cost\n\n    _, cost = control_flow_ops.while_loop(\n        cond, body, [constant_op.constant(0),\n                     constant_op.constant(0.0)])\n    optimizer = momentum.MomentumOptimizer(0.1, 0.9)\n    train_op = optimizer.minimize(cost)\n    with self.cached_session():\n      self.evaluate(variables.global_variables_initializer())\n      for _ in range(10):\n        self.evaluate([train_op])\n\n  def testResourceReadInLoop(self):\n    embedding_matrix = variable_scope.get_variable(\n        ""embedding_matrix"", initializer=[[2.0], [3.0]], use_resource=True)\n\n    def cond(it, _):\n      return it < 5\n\n    def body(it, cost):\n      embedding = embedding_ops.embedding_lookup(embedding_matrix, [0])\n      cost += math_ops.reduce_sum(embedding)\n      return it + 1, cost\n\n    _, cost = control_flow_ops.while_loop(\n        cond, body, [constant_op.constant(0),\n                     constant_op.constant(0.0)])\n    with self.cached_session():\n      self.evaluate(variables.global_variables_initializer())\n      self.assertAllEqual(10.0, self.evaluate(cost))\n\n  def doTestIndexedSlicesGradientInCondInWhileLoop(self, use_resource=False):\n    embedding_matrix = variable_scope.get_variable(\n        ""embedding_matrix"", [5, 5],\n        initializer=init_ops.random_normal_initializer(),\n        use_resource=use_resource)\n\n    def cond(it, _):\n      return it < 5\n\n    def body(it, cost):\n      embedding = embedding_ops.embedding_lookup(embedding_matrix, [0])\n      cost = control_flow_ops.cond(\n          math_ops.equal(it, 3), lambda: math_ops.square(cost),\n          (lambda: cost + math_ops.reduce_sum(embedding)))\n      return it + 1, cost\n\n      _, cost = control_flow_ops.while_loop(\n          cond, body, [constant_op.constant(0),\n                       constant_op.constant(0.0)])\n\n      dynamic_grads = gradients_impl.gradients(cost, [embedding_matrix])[0]\n      dynamic_grads = math_ops.segment_sum(dynamic_grads.values,\n                                           dynamic_grads.indices)\n\n      embedding = embedding_ops.embedding_lookup(embedding_matrix, [0])\n      static = math_ops.square(\n          math_ops.reduce_sum(embedding) + math_ops.reduce_sum(embedding) +\n          math_ops.reduce_sum(embedding)) + math_ops.reduce_sum(embedding)\n      static_grads = gradients_impl.gradients(static, [embedding_matrix])[0]\n      static_grads = math_ops.segment_sum(static_grads.values,\n                                          static_grads.indices)\n\n      with self.cached_session():\n        self.evaluate(variables.global_variables_initializer())\n        self.assertAllEqual(*self.evaluate([static_grads, dynamic_grads]))\n\n  def testIndexedSlicesGradientInCondInWhileLoop(self):\n    self.doTestIndexedSlicesGradientInCondInWhileLoop(use_resource=False)\n\n  def testIndexedSlicesGradientInCondInWhileLoopResource(self):\n    self.doTestIndexedSlicesGradientInCondInWhileLoop(use_resource=True)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testIndexedSlicesWithShapeGradientInWhileLoop(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      with self.cached_session() as sess:\n        num_steps = 9\n\n        inputs = array_ops.placeholder(dtype=dtype, shape=[num_steps])\n        initial_outputs = tensor_array_ops.TensorArray(\n            dtype=dtype, size=num_steps)\n        initial_i = constant_op.constant(0, dtype=dtypes.int32)\n\n        def cond(i, _):\n          return i < num_steps  # pylint: disable=cell-var-from-loop\n\n        def body(i, outputs):\n          x = array_ops.gather(inputs, i)  # pylint: disable=cell-var-from-loop\n          outputs = outputs.write(i, x)\n          return i + 1, outputs\n\n        _, outputs = control_flow_ops.while_loop(cond, body,\n                                                 [initial_i, initial_outputs])\n\n        outputs = math_ops.reduce_sum(outputs.stack())\n        r = gradients_impl.gradients([outputs], [inputs])[0]\n        grad_wr_inputs = ops.convert_to_tensor(r)\n        o, grad = sess.run([outputs, grad_wr_inputs],\n                           feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\n        self.assertEquals(o, 20)\n        self.assertAllEqual(grad, [1] * num_steps)\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testIndexedSlicesWithDynamicShapeGradientInWhileLoop(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      with self.cached_session() as sess:\n        inputs = array_ops.placeholder(dtype=dtype)\n        initial_outputs = tensor_array_ops.TensorArray(\n            dtype=dtype, dynamic_size=True, size=1)\n        initial_i = constant_op.constant(0, dtype=dtypes.int32)\n\n        def cond(i, _):\n          return i < array_ops.size(inputs)  # pylint: disable=cell-var-from-loop\n\n        def body(i, outputs):\n          x = array_ops.gather(inputs, i)  # pylint: disable=cell-var-from-loop\n          outputs = outputs.write(i, x)\n          return i + 1, outputs\n\n        _, outputs = control_flow_ops.while_loop(cond, body,\n                                                 [initial_i, initial_outputs])\n\n        outputs = math_ops.reduce_sum(outputs.stack())\n        r = gradients_impl.gradients([outputs], [inputs])[0]\n        grad_wr_inputs = ops.convert_to_tensor(r)\n        o, grad = sess.run([outputs, grad_wr_inputs],\n                           feed_dict={inputs: [1, 3, 2]})\n        self.assertEquals(o, 6)\n        self.assertAllEqual(grad, [1] * 3)\n\n  @test_util.run_deprecated_v1\n  def testGradientThroughSingleBranchOutsideOfContext(self):\n    x = constant_op.constant(2.)\n    s = constant_op.constant(True)\n    x_false, x_true = control_flow_ops.switch(x, s)\n    grad_x_true = gradients_impl.gradients(x_true, x)[0]\n    grad_x_false = gradients_impl.gradients(x_false, x)[0]\n    self.assertEquals(self.evaluate(grad_x_true), 1.)\n    self.assertEquals(self.evaluate(grad_x_false), 0.)\n\n\nclass CondTest(test_util.TensorFlowTestCase):\n\n  def testCondTrue(self):\n    x = constant_op.constant(2)\n    y = constant_op.constant(5)\n    z = control_flow_ops.cond(\n        math_ops.less(\n            x,\n            y), lambda: math_ops.multiply(x, 17), lambda: math_ops.add(y, 23))\n    self.assertEquals(self.evaluate(z), 34)\n\n  def testCondFalse(self):\n    x = constant_op.constant(2)\n    y = constant_op.constant(1)\n    z = control_flow_ops.cond(\n        math_ops.less(\n            x,\n            y), lambda: math_ops.multiply(x, 17), lambda: math_ops.add(y, 23))\n    self.assertEquals(self.evaluate(z), 24)\n\n  def testCondTrueLegacy(self):\n    x = constant_op.constant(2)\n    y = constant_op.constant(5)\n    z = control_flow_ops.cond(\n        math_ops.less(x, y),\n        fn1=lambda: math_ops.multiply(x, 17),\n        fn2=lambda: math_ops.add(y, 23))\n    self.assertEquals(self.evaluate(z), 34)\n\n  def testCondFalseLegacy(self):\n    x = constant_op.constant(2)\n    y = constant_op.constant(1)\n    z = control_flow_ops.cond(\n        math_ops.less(x, y),\n        fn1=lambda: math_ops.multiply(x, 17),\n        fn2=lambda: math_ops.add(y, 23))\n    self.assertEquals(self.evaluate(z), 24)\n\n  @test_util.run_deprecated_v1\n  def testCondModifyBoolPred(self):\n    # This test in particular used to fail only when running in GPU, hence\n    # use_gpu=True.\n    with test_util.use_gpu():\n      bool_var = variable_scope.get_variable(\n          ""bool_var"", dtype=dtypes.bool, initializer=True)\n      cond_on_bool_var = control_flow_ops.cond(\n          pred=bool_var,\n          true_fn=lambda: state_ops.assign(bool_var, False),\n          false_fn=lambda: True)\n      self.evaluate(bool_var.initializer)\n      self.assertEquals(self.evaluate(cond_on_bool_var), False)\n      self.assertEquals(self.evaluate(cond_on_bool_var), True)\n\n  def testCondMissingArg1(self):\n    x = constant_op.constant(1)\n    with self.assertRaises(TypeError):\n      control_flow_ops.cond(True, false_fn=lambda: x)\n\n  def testCondMissingArg2(self):\n    x = constant_op.constant(1)\n    with self.assertRaises(TypeError):\n      control_flow_ops.cond(True, lambda: x)\n\n  def testCondDuplicateArg1(self):\n    x = constant_op.constant(1)\n    with self.assertRaises(TypeError):\n      control_flow_ops.cond(True, lambda: x, lambda: x, fn1=lambda: x)\n\n  def testCondDuplicateArg2(self):\n    x = constant_op.constant(1)\n    with self.assertRaises(TypeError):\n      control_flow_ops.cond(True, lambda: x, lambda: x, fn2=lambda: x)\n\n\nclass ContextTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testCondContext(self):\n    with self.cached_session() as sess:\n      x = constant_op.constant(2)\n      y = constant_op.constant(5)\n      control_flow_ops.cond(\n          math_ops.less(x, y), lambda: math_ops.multiply(x, 17),\n          lambda: math_ops.add(y, 23))\n      for op in sess.graph.get_operations():\n        c = op._get_control_flow_context()\n        if c:\n          self.assertProtoEquals(\n              c.to_proto(),\n              control_flow_ops.CondContext.from_proto(c.to_proto()).to_proto())\n\n  def _testWhileContextHelper(self, maximum_iterations=None):\n    with self.cached_session() as sess:\n      i = constant_op.constant(0)\n      c = lambda i: math_ops.less(i, 10)\n      b = lambda i: math_ops.add(i, 1)\n      control_flow_ops.while_loop(\n          c, b, [i], maximum_iterations=maximum_iterations)\n      for op in sess.graph.get_operations():\n        control_flow_context = op._get_control_flow_context()\n        if control_flow_context:\n          self.assertProtoEquals(\n              control_flow_context.to_proto(),\n              control_flow_ops.WhileContext.from_proto(\n                  control_flow_context.to_proto()).to_proto())\n\n  @test_util.run_deprecated_v1\n  def testWhileContext(self):\n    self._testWhileContextHelper()\n\n  @test_util.run_deprecated_v1\n  def testWhileContextWithMaximumIterations(self):\n    self._testWhileContextHelper(maximum_iterations=10)\n\n  @test_util.run_deprecated_v1\n  def testControlContextImportScope(self):\n    class NoABCControlFlowContext(control_flow_ops.ControlFlowContext):\n      """"""A noop wrapper around `ControlFlowContext`.\n\n      `ControlFlowContext` is an ABC and therefore cannot be instantiated.\n      """"""\n      # pylint: disable=useless-super-delegation\n\n      def to_control_flow_context_def(self, context_def, export_scope=None):\n        super(NoABCControlFlowContext, self).to_control_flow_context_def(\n            context_def, export_scope)\n\n    with self.cached_session():\n      constant_op.constant(0, name=""a"")\n      constant_op.constant(2, name=""test_scope/a"")\n      b1 = constant_op.constant(1, name=""b"")\n      b2 = constant_op.constant(3, name=""test_scope/b"")\n\n      c = NoABCControlFlowContext()\n      c._values = [""a"", ""b""]\n      c._external_values = {""a"": b1}\n\n      c_with_scope = NoABCControlFlowContext(\n          values_def=c._to_values_def(), import_scope=""test_scope"")\n\n      # _values and _external_values should be have scope prepended.\n      self.assertEquals(\n          c_with_scope._values, set([""test_scope/a"", ""test_scope/b""]))\n      self.assertEquals(\n          c_with_scope._external_values, {""test_scope/a"": b2})\n\n      # Calling _to_proto() with export_scope should remove ""test_scope"".\n      self.assertProtoEquals(\n          c._to_values_def(),\n          c_with_scope._to_values_def(export_scope=""test_scope""))\n\n\ndef _get_nested_shape(nested):\n\n  def _get_shape(tensor):\n    if isinstance(tensor, tensor_array_ops.TensorArray):\n      return tensor_array_ops.TensorArray\n    elif isinstance(tensor, ops.IndexedSlices):\n      return tensor.dense_shape\n    else:\n      return tensor.get_shape()\n\n  return nest.map_structure(_get_shape, nested)\n\n\ndef _create_tensor_array(size, shape):\n  ta = tensor_array_ops.TensorArray(dtype=dtypes.float32, size=size,\n                                    clear_after_read=False)\n  for i in range(size):\n    ta = ta.write(i, array_ops.zeros(shape))\n  return ta\n\n\ndef _raw_nested_shape(nested_shape):\n\n  def _raw_shape(shape):\n    if isinstance(shape, tensor_shape.TensorShape) and shape.ndims is not None:\n      return [x.value for x in shape.dims]\n    else:\n      return None\n\n  return nest.map_structure(_raw_shape, nested_shape)\n\n\n# TODO(yori): Add tests for indexed slices.\nclass DataTypesTest(test_util.TensorFlowTestCase):\n\n  def assertAllEqualNested(self, a, b):\n    if isinstance(a, (list, tuple)):\n      for entry_a, entry_b in zip(a, b):\n        self.assertAllEqualNested(entry_a, entry_b)\n    else:\n      self.assertAllEqual(a, b)\n\n  def _testShape(self, fn_true, fn_false, expected_shape,\n                 strict=False):\n    condition = array_ops.placeholder(dtypes.bool)\n    output_cond = control_flow_ops.cond(condition, fn_true, fn_false,\n                                        strict=strict)\n    self.assertEqual(\n        _raw_nested_shape(_get_nested_shape(output_cond)),\n        _raw_nested_shape(expected_shape))\n\n    output_case = control_flow_ops.case([(condition, fn_true)], fn_false,\n                                        strict=strict)\n    self.assertEqual(\n        _raw_nested_shape(_get_nested_shape(output_case)),\n        _raw_nested_shape(expected_shape))\n\n  def _testReturnValues(self, fn_true, fn_false, expected_value_true,\n                        expected_value_false, strict=False,\n                        check_cond=True, feed_dict=None):\n    if feed_dict is None: feed_dict = {}\n\n    condition = array_ops.placeholder(dtypes.bool)\n    output_cond = control_flow_ops.cond(condition, fn_true, fn_false,\n                                        strict=strict)\n    output_case = control_flow_ops.case([(condition, fn_true)], fn_false,\n                                        strict=strict)\n\n    with self.cached_session() as sess:\n      self.evaluate(variables.global_variables_initializer())\n      true_feed_dict = {condition: True}\n      true_feed_dict.update(feed_dict)\n      result_cond, result_case = sess.run([output_cond, output_case],\n                                          feed_dict=true_feed_dict)\n      self.assertAllEqualNested(result_cond, expected_value_true)\n      if check_cond:\n        self.assertAllEqualNested(result_case, expected_value_true)\n      false_feed_dict = {condition: False}\n      false_feed_dict.update(feed_dict)\n      result_cond, result_case = sess.run([output_cond, output_case],\n                                          feed_dict=false_feed_dict)\n      self.assertAllEqualNested(result_cond, expected_value_false)\n      if check_cond:\n        self.assertAllEqualNested(result_case, expected_value_false)\n\n  @test_util.run_deprecated_v1\n  def test_int(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: 1\n    fn_false = lambda: 2\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, 1, 2)\n    self._testShape(fn_true, fn_false, shape, strict=True)\n    self._testReturnValues(fn_true, fn_false, 1, 2, strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_float(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: 1.0\n    fn_false = lambda: 2.0\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, 1.0, 2.0)\n\n  @test_util.run_deprecated_v1\n  def test_noop(self):\n    shape = tensor_shape.TensorShape(None)\n    self._testShape(control_flow_ops.no_op, control_flow_ops.no_op, shape)\n    self._testReturnValues(control_flow_ops.no_op, control_flow_ops.no_op,\n                           True, False, check_cond=False)\n\n  @test_util.run_deprecated_v1\n  def test_string(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: ""abc""\n    fn_false = lambda: ""xyz""\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, b""abc"", b""xyz"")\n\n  @test_util.run_deprecated_v1\n  def test_variable(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: variables.Variable(3.0)\n    fn_false = lambda: variables.Variable(4.0)\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, 3.0, 4.0)\n\n  @test_util.run_v1_only(""b/120553181"")\n  def test_none(self):\n    fn_none = lambda: None\n    fn_tensor = lambda: constant_op.constant(1)\n\n    with self.assertRaises(ValueError):\n      control_flow_ops.cond(constant_op.constant(True), fn_none, fn_tensor)\n\n    with self.assertRaises(ValueError):\n      control_flow_ops.cond(constant_op.constant(True), fn_tensor, fn_none)\n\n  @test_util.run_deprecated_v1\n  def test_tensors(self):\n\n    def _build_true_branch(dtype):\n\n      def _build():\n        return (array_ops.zeros([2, 2], dtype=dtype),\n                array_ops.ones([3, 3], dtype=dtype))\n\n      return _build\n\n    def _build_false_branch(dtype):\n\n      def _build():\n        return (array_ops.ones([2, 2], dtype=dtype),\n                array_ops.zeros([3, 3], dtype=dtype))\n\n      return _build\n\n    for dtype in (dtypes.float16, dtypes.int8, dtypes.int32, dtypes.uint8):\n      shape = (tensor_shape.TensorShape([2, 2]),\n               tensor_shape.TensorShape([3, 3]))\n      fn_true = _build_true_branch(dtype)\n      fn_false = _build_false_branch(dtype)\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false,\n                             (np.zeros([2, 2]), np.ones([3, 3])),\n                             (np.ones([2, 2]), np.zeros([3, 3])))\n\n  @test_util.run_deprecated_v1\n  def test_tensors_unknown_shape(self):\n\n    def _build_true_branch(dtype):\n      tensor = array_ops.placeholder(dtype=dtype, shape=None)\n\n      def _build():\n        return tensor\n\n      return _build, tensor\n\n    def _build_false_branch(dtype):\n      tensor = array_ops.placeholder(dtype=dtype, shape=None)\n\n      def _build():\n        return tensor\n\n      return _build, tensor\n\n    for dtype in (dtypes.float16, dtypes.int8, dtypes.int32, dtypes.uint8):\n      shape = tensor_shape.TensorShape(None)\n      fn_true, true_tensor = _build_true_branch(dtype)\n      fn_false, false_tensor = _build_false_branch(dtype)\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false,\n                             np.zeros([2, 2]), np.ones([2, 2]),\n                             feed_dict={true_tensor: np.zeros([2, 2]),\n                                        false_tensor: np.ones([2, 2])})\n\n  @test_util.run_deprecated_v1\n  def test_sparse_tensors(self):\n    shape = tensor_shape.TensorShape([None, None])\n\n    def true_fn():\n      return [sparse_tensor.SparseTensor(indices=[[0, 0], [1, 2]],\n                                         values=[1, 2], dense_shape=[3, 4])]\n\n    def false_fn():\n      return [sparse_tensor.SparseTensor(indices=[[0, 0], [2, 1]],\n                                         values=[3, 4], dense_shape=[3, 4])]\n\n    value1 = sparse_tensor.SparseTensorValue(indices=[[0, 0], [1, 2]],\n                                             values=[1, 2], dense_shape=[3, 4])\n    value2 = sparse_tensor.SparseTensorValue(indices=[[0, 0], [2, 1]],\n                                             values=[3, 4], dense_shape=[3, 4])\n    # Non-strict cond is only available in v1\n    if not tf2.enabled():\n      self._testShape(true_fn, false_fn, shape)\n      self._testReturnValues(true_fn, false_fn, value1, value2)\n    self._testShape(true_fn, false_fn, [shape], strict=True)\n    self._testReturnValues(true_fn, false_fn, [value1], [value2], strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_tensors_with_partially_specified_shapes(self):\n\n    def _build_branch(dtype, shape):\n      a = array_ops.placeholder(dtype=dtype, shape=shape[0])\n      b = array_ops.placeholder(dtype=dtype, shape=shape[1])\n      c = array_ops.placeholder(dtype=dtype, shape=shape[2])\n\n      def _build():\n        return a, b, c\n\n      return _build, (a, b, c)\n\n    for dtype in (dtypes.float16, dtypes.int8, dtypes.int32, dtypes.uint8):\n      shape = (tensor_shape.TensorShape([None, 2]),\n               tensor_shape.TensorShape([None]),\n               tensor_shape.TensorShape([3, None]))\n      fn_true, true_tensors = _build_branch(dtype, shape)\n      fn_false, false_tensors = _build_branch(dtype, shape)\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false,\n                             (np.zeros([2, 2]), np.zeros(5), np.ones([3, 3])),\n                             (np.zeros([2, 2]), np.zeros(5), np.ones([3, 3])),\n                             feed_dict={true_tensors[0]: np.zeros([2, 2]),\n                                        false_tensors[0]: np.zeros([2, 2]),\n                                        true_tensors[1]: np.zeros([5]),\n                                        false_tensors[1]: np.zeros([5]),\n                                        true_tensors[2]: np.ones([3, 3]),\n                                        false_tensors[2]: np.ones([3, 3])})\n\n  @test_util.run_deprecated_v1\n  def test_tensor_arrays(self):\n    element_shape = tensor_shape.TensorShape([2])\n    ta1 = _create_tensor_array(4, element_shape)\n    ta2 = _create_tensor_array(4, element_shape)\n    shape = tensor_array_ops.TensorArray\n    fn_true = lambda: ta1\n    fn_false = lambda: ta2\n    self._testShape(fn_true, fn_false, shape)\n\n  @test_util.run_deprecated_v1\n  def test_tensor_array_reads(self):\n    shape = tensor_shape.TensorShape([2])\n    ta = _create_tensor_array(4, shape)\n    fn_true = lambda: ta.read(0)\n    fn_false = lambda: ta.read(1)\n    self._testShape(fn_true, fn_false, shape)\n\n  @test_util.run_deprecated_v1\n  def test_list(self):\n    shape = [tensor_shape.TensorShape([]), tensor_shape.TensorShape([]),\n             tensor_shape.TensorShape([])]\n    fn_true = lambda: [constant_op.constant(1), 2, variables.Variable(3.0)]\n    fn_false = lambda: [constant_op.constant(3), 4, variables.Variable(5.0)]\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, [1, 2, 3.0], [3, 4, 5.0])\n\n  @test_util.run_v1_only(""Non-strict cond is only available in v1"")\n  def test_non_strict(self):\n    shape = tensor_shape.TensorShape([])\n    fn_tensor = lambda: constant_op.constant(1)\n    fn_list = lambda: [constant_op.constant(2)]\n    fn_tuple = lambda: (constant_op.constant(3),)\n    self._testShape(fn_tensor, fn_list, shape)\n    self._testShape(fn_tensor, fn_tuple, shape)\n    self._testShape(fn_list, fn_tuple, shape)\n    self._testReturnValues(fn_tensor, fn_list, 1, 2)\n    self._testReturnValues(fn_tensor, fn_tuple, 1, 3)\n    self._testReturnValues(fn_list, fn_tuple, 2, 3)\n\n  @test_util.run_v1_only(""b/120553181"")\n  def test_singleton_strict(self):\n    fn_tensor = lambda: constant_op.constant(1)\n    fn_list = lambda: [constant_op.constant(2)]\n    fn_tuple = lambda: (constant_op.constant(3),)\n\n    with self.assertRaises(ValueError):\n      control_flow_ops.cond(constant_op.constant(True), fn_tensor, fn_list,\n                            strict=True)\n\n    with self.assertRaises(TypeError):\n      control_flow_ops.cond(constant_op.constant(True), fn_list, fn_tuple,\n                            strict=True)\n\n    with self.assertRaises(ValueError):\n      control_flow_ops.case([(constant_op.constant(True), fn_tensor)], fn_list,\n                            strict=True)\n\n    with self.assertRaises(TypeError):\n      control_flow_ops.case([(constant_op.constant(True), fn_list)], fn_tuple,\n                            strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_singleton_list(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: [constant_op.constant(1)]\n    fn_false = lambda: [constant_op.constant(3)]\n    # Non-strict cond is only available in v1\n    if not tf2.enabled():\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false, 1, 3)\n    self._testShape(fn_true, fn_false, [shape], strict=True)\n    self._testReturnValues(fn_true, fn_false, [1], [3], strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_singleton_tuple(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: (constant_op.constant(1),)\n    fn_false = lambda: (constant_op.constant(3),)\n    # Non-strict cond is only available in v1\n    if not tf2.enabled():\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false, 1, 3)\n    self._testShape(fn_true, fn_false, (shape,), strict=True)\n    self._testReturnValues(fn_true, fn_false, (1,), (3,),\n                           strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_singleton_namedtuple(self):\n    shape = tensor_shape.TensorShape([])\n    fn_true = lambda: SingletonTestTuple(constant_op.constant(1))\n    fn_false = lambda: SingletonTestTuple(constant_op.constant(3))\n    # Non-strict cond is only available in v1\n    if not tf2.enabled():\n      self._testShape(fn_true, fn_false, shape)\n      self._testReturnValues(fn_true, fn_false, 1, 3)\n    self._testShape(fn_true, fn_false, SingletonTestTuple(shape),\n                    strict=True)\n    self._testReturnValues(fn_true, fn_false, SingletonTestTuple(1),\n                           SingletonTestTuple(3), strict=True)\n\n  @test_util.run_deprecated_v1\n  def test_tuple(self):\n    shape = (tensor_shape.TensorShape([]), tensor_shape.TensorShape([]))\n    fn_true = lambda: (constant_op.constant(1), 2)\n    fn_false = lambda: (constant_op.constant(3), 4)\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, (1, 2), (3, 4))\n\n  @test_util.run_deprecated_v1\n  def test_namedtuple(self):\n    shape = TestTuple(tensor_shape.TensorShape([]),\n                      tensor_shape.TensorShape([]))\n    fn_true = lambda: TestTuple(constant_op.constant(1), 2)\n    fn_false = lambda: TestTuple(constant_op.constant(3), 4)\n    self._testShape(fn_true, fn_false, shape)\n    self._testReturnValues(fn_true, fn_false, TestTuple(1, 2), TestTuple(3, 4))\n\n  @test_util.run_deprecated_v1\n  def test_nested(self):\n    shape = [tensor_shape.TensorShape([]),\n             TestTuple(tensor_shape.TensorShape([]),\n                       [tensor_shape.TensorShape([]),\n                        tensor_shape.TensorShape([])]),\n             tensor_shape.TensorShape([5, 5]),\n             tensor_shape.TensorShape([])]\n\n    def true_fn():\n      return [constant_op.constant(1),\n              TestTuple(constant_op.constant(2), [3, 4]),\n              array_ops.zeros([5, 5]), 6]\n\n    def false_fn():\n      return [constant_op.constant(11),\n              TestTuple(constant_op.constant(12), [13, 14]),\n              array_ops.ones([5, 5]), 16]\n\n    self._testShape(true_fn, false_fn, shape)\n    self._testReturnValues(\n        true_fn, false_fn,\n        [1, TestTuple(2, [3, 4]), np.zeros([5, 5]), 6],\n        [11, TestTuple(12, [13, 14]),\n         np.ones([5, 5]), 16])\n\n  @test_util.run_deprecated_v1\n  def test_cond_inside_while_loop(self):\n\n    def body(i, matrix):\n      result_tuple, unused_matrix = control_flow_ops.cond(\n          constant_op.constant(True),\n          lambda: (TestTuple(matrix * 2, matrix * 4), matrix),\n          lambda: (TestTuple(matrix * 4, matrix * 2), matrix))\n      return [i+1, result_tuple.a]\n\n    iteration, matrix = control_flow_ops.while_loop(\n        lambda i, matrix: i < 10,\n        body,\n        loop_vars=[constant_op.constant(0),\n                   array_ops.ones([2, 2])])\n\n    self.assertEqual(iteration.get_shape(), tensor_shape.TensorShape([]))\n    self.assertEqual(matrix.get_shape(), tensor_shape.TensorShape([2, 2]))\n\n\nclass CaseTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testCase_withDefault(self):\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(4))]\n    default = lambda: constant_op.constant(6)\n    output = control_flow_ops.case(conditions, default, exclusive=True)\n    with self.cached_session() as sess:\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2)\n      self.assertEqual(sess.run(output, feed_dict={x: 2}), 4)\n      self.assertEqual(sess.run(output, feed_dict={x: 3}), 6)\n\n  @test_util.run_deprecated_v1\n  def testCase_multiple_matches_exclusive(self):\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(4)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(6))]\n    default = lambda: constant_op.constant(8)\n    output = control_flow_ops.case(conditions, default, exclusive=True)\n    with self.cached_session() as sess:\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2)\n      self.assertEqual(sess.run(output, feed_dict={x: 3}), 8)\n      with self.assertRaisesRegexp(errors.InvalidArgumentError, ""Input error:""):\n        sess.run(output, feed_dict={x: 2})\n\n  @test_util.run_deprecated_v1\n  def testCase_multiple_matches_non_exclusive(self):\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(4)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(6))]\n    default = lambda: constant_op.constant(8)\n    output = control_flow_ops.case(conditions, default, exclusive=False)\n    with self.cached_session() as sess:\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2)\n      self.assertEqual(sess.run(output, feed_dict={x: 2}), 4)\n      self.assertEqual(sess.run(output, feed_dict={x: 3}), 8)\n\n  @test_util.run_deprecated_v1\n  def testCase_withoutDefault(self):\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2)),\n                  (math_ops.equal(x, 2), lambda: constant_op.constant(4)),\n                  (math_ops.equal(x, 3), lambda: constant_op.constant(6))]\n    output = control_flow_ops.case(conditions, exclusive=True)\n    with self.cached_session() as sess:\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2)\n      self.assertEqual(sess.run(output, feed_dict={x: 2}), 4)\n      self.assertEqual(sess.run(output, feed_dict={x: 3}), 6)\n      with self.assertRaisesRegexp(errors.InvalidArgumentError, ""Input error:""):\n        sess.run(output, feed_dict={x: 4})\n\n  @test_util.run_deprecated_v1\n  def testCase_withoutDefault_oneCondition(self):\n    x = array_ops.placeholder(dtype=dtypes.int32, shape=[])\n    conditions = [(math_ops.equal(x, 1), lambda: constant_op.constant(2))]\n    output = control_flow_ops.case(conditions, exclusive=True)\n    with self.cached_session() as sess:\n      self.assertEqual(sess.run(output, feed_dict={x: 1}), 2)\n      with self.assertRaisesRegexp(errors.InvalidArgumentError, ""Input error:""):\n        sess.run(output, feed_dict={x: 4})\n\n  @test_util.run_in_graph_and_eager_modes\n  def testCase_dict(self):\n    x = constant_op.constant(2)\n    conditions = {\n        math_ops.equal(x, 1): lambda: constant_op.constant(2),\n        math_ops.equal(x, 2): lambda: constant_op.constant(4)\n    }\n    output = control_flow_ops.case(conditions, exclusive=True)\n    self.assertEqual(4, self.evaluate(output))\n\n\nclass WhileLoopTestCase(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testWhileLoopWithSingleVariable(self):\n    i = constant_op.constant(0)\n    c = lambda i: math_ops.less(i, 10)\n    b = lambda i: math_ops.add(i, 1)\n    r = control_flow_ops.while_loop(c, b, [i])\n\n    self.assertEqual(self.evaluate(r), 10)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testEagerWhileLoopWithSingleVariable_bodyReturnsTuple(self):\n    i = constant_op.constant(0)\n    c = lambda i: math_ops.less(i, 10)\n    b = lambda i: (math_ops.add(i, 1),)\n    r = control_flow_ops.while_loop(c, b, [i])\n\n    # Expect a tuple since that is what the body returns.\n    self.assertEqual(self.evaluate(r), (10,))\n\n  @test_util.run_deprecated_v1\n  def testWhileLoopSameReturnShape_False(self):\n    i = constant_op.constant(0)\n    c = lambda i, _: math_ops.less(i, 10)\n\n    # Body returns a [tensor, []]\n    b = lambda i, _: [math_ops.add(i, 1), []]\n\n    # Should only return the tensor.\n    r = control_flow_ops.while_loop(c, b, [i, []])\n    self.assertEqual(self.evaluate(r), 10)\n\n  def testWhileLoopSameReturnShape_True(self):\n    i = constant_op.constant(0)\n    c = lambda i, _: math_ops.less(i, 10)\n\n    # Body returns a [tensor, []]\n    b = lambda i, _: [math_ops.add(i, 1), []]\n\n    # Should only return the original structure.\n    r = control_flow_ops.while_loop(c, b, [i, []], return_same_structure=True)\n    self.assertEqual(self.evaluate(r), [10, []])\n\n\nclass AssertTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testAssert(self):\n    i = constant_op.constant(0)\n    c = control_flow_ops.Assert(i < 10, [i, [10], [i + 1]])\n    self.evaluate(c)\n\n    i = constant_op.constant(10)\n    c = control_flow_ops.Assert(i < 10, [i, [10], [i + 1]])\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(c)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testAssertInFunction(self):\n\n    @def_function.function\n    def whiny(value):\n      control_flow_ops.Assert(value, [""Raised false""])\n      return constant_op.constant(5)\n\n    with self.assertRaises(errors.InvalidArgumentError):\n      self.evaluate(whiny(False))\n\n    self.assertAllEqual(whiny(True), 5)\n\nif __name__ == ""__main__"":\n  googletest.main()\n'"
test/TensorFlowNET.UnitTest/nest_test/nest_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for utilities working with arbitrarily nested structures.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport time\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.util import nest\n\ntry:\n  import attr  # pylint:disable=g-import-not-at-top\nexcept ImportError:\n  attr = None\n\n\nclass _CustomMapping(collections.Mapping):\n\n  def __init__(self, *args, **kwargs):\n    self._wrapped = dict(*args, **kwargs)\n\n  def __getitem__(self, key):\n    return self._wrapped[key]\n\n  def __iter__(self):\n    return iter(self._wrapped)\n\n  def __len__(self):\n    return len(self._wrapped)\n\n\nclass NestTest(parameterized.TestCase, test.TestCase):\n\n  PointXY = collections.namedtuple(""Point"", [""x"", ""y""])  # pylint: disable=invalid-name\n\n  if attr:\n    class BadAttr(object):\n      """"""Class that has a non-iterable __attrs_attrs__.""""""\n      __attrs_attrs__ = None\n\n    @attr.s\n    class SampleAttr(object):\n      field1 = attr.ib()\n      field2 = attr.ib()\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testAttrsFlattenAndPack(self):\n    if attr is None:\n      self.skipTest(""attr module is unavailable."")\n\n    field_values = [1, 2]\n    sample_attr = NestTest.SampleAttr(*field_values)\n    self.assertFalse(nest._is_attrs(field_values))\n    self.assertTrue(nest._is_attrs(sample_attr))\n    flat = nest.flatten(sample_attr)\n    self.assertEqual(field_values, flat)\n    restructured_from_flat = nest.pack_sequence_as(sample_attr, flat)\n    self.assertIsInstance(restructured_from_flat, NestTest.SampleAttr)\n    self.assertEqual(restructured_from_flat, sample_attr)\n\n    # Check that flatten fails if attributes are not iterable\n    with self.assertRaisesRegexp(TypeError, ""object is not iterable""):\n      flat = nest.flatten(NestTest.BadAttr())\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testFlattenAndPack(self):\n    structure = ((3, 4), 5, (6, 7, (9, 10), 8))\n    flat = [""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h""]\n    self.assertEqual(nest.flatten(structure), [3, 4, 5, 6, 7, 9, 10, 8])\n    self.assertEqual(\n        nest.pack_sequence_as(structure, flat), ((""a"", ""b""), ""c"",\n                                                 (""d"", ""e"", (""f"", ""g""), ""h"")))\n    structure = (NestTest.PointXY(x=4, y=2),\n                 ((NestTest.PointXY(x=1, y=0),),))\n    flat = [4, 2, 1, 0]\n    self.assertEqual(nest.flatten(structure), flat)\n    restructured_from_flat = nest.pack_sequence_as(structure, flat)\n    self.assertEqual(restructured_from_flat, structure)\n    self.assertEqual(restructured_from_flat[0].x, 4)\n    self.assertEqual(restructured_from_flat[0].y, 2)\n    self.assertEqual(restructured_from_flat[1][0][0].x, 1)\n    self.assertEqual(restructured_from_flat[1][0][0].y, 0)\n\n    self.assertEqual([5], nest.flatten(5))\n    self.assertEqual([np.array([5])], nest.flatten(np.array([5])))\n\n    self.assertEqual(""a"", nest.pack_sequence_as(5, [""a""]))\n    self.assertEqual(\n        np.array([5]), nest.pack_sequence_as(""scalar"", [np.array([5])]))\n\n    with self.assertRaisesRegexp(ValueError, ""Structure is a scalar""):\n      nest.pack_sequence_as(""scalar"", [4, 5])\n\n    with self.assertRaisesRegexp(TypeError, ""flat_sequence""):\n      nest.pack_sequence_as([4, 5], ""bad_sequence"")\n\n    with self.assertRaises(ValueError):\n      nest.pack_sequence_as([5, 6, [7, 8]], [""a"", ""b"", ""c""])\n\n  @parameterized.parameters({""mapping_type"": collections.OrderedDict},\n                            {""mapping_type"": _CustomMapping})\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testFlattenDictOrder(self, mapping_type):\n    """"""`flatten` orders dicts by key, including OrderedDicts.""""""\n    ordered = mapping_type([(""d"", 3), (""b"", 1), (""a"", 0), (""c"", 2)])\n    plain = {""d"": 3, ""b"": 1, ""a"": 0, ""c"": 2}\n    ordered_flat = nest.flatten(ordered)\n    plain_flat = nest.flatten(plain)\n    self.assertEqual([0, 1, 2, 3], ordered_flat)\n    self.assertEqual([0, 1, 2, 3], plain_flat)\n\n  @parameterized.parameters({""mapping_type"": collections.OrderedDict},\n                            {""mapping_type"": _CustomMapping})\n  def testPackDictOrder(self, mapping_type):\n    """"""Packing orders dicts by key, including OrderedDicts.""""""\n    custom = mapping_type([(""d"", 0), (""b"", 0), (""a"", 0), (""c"", 0)])\n    plain = {""d"": 0, ""b"": 0, ""a"": 0, ""c"": 0}\n    seq = [0, 1, 2, 3]\n    custom_reconstruction = nest.pack_sequence_as(custom, seq)\n    plain_reconstruction = nest.pack_sequence_as(plain, seq)\n    self.assertIsInstance(custom_reconstruction, mapping_type)\n    self.assertIsInstance(plain_reconstruction, dict)\n    self.assertEqual(\n        mapping_type([(""d"", 3), (""b"", 1), (""a"", 0), (""c"", 2)]),\n        custom_reconstruction)\n    self.assertEqual({""d"": 3, ""b"": 1, ""a"": 0, ""c"": 2}, plain_reconstruction)\n\n  Abc = collections.namedtuple(""A"", (""b"", ""c""))  # pylint: disable=invalid-name\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testFlattenAndPack_withDicts(self):\n    # A nice messy mix of tuples, lists, dicts, and `OrderedDict`s.\n    mess = [\n        ""z"",\n        NestTest.Abc(3, 4), {\n            ""d"": _CustomMapping({\n                41: 4\n            }),\n            ""c"": [\n                1,\n                collections.OrderedDict([\n                    (""b"", 3),\n                    (""a"", 2),\n                ]),\n            ],\n            ""b"": 5\n        }, 17\n    ]\n\n    flattened = nest.flatten(mess)\n    self.assertEqual(flattened, [""z"", 3, 4, 5, 1, 2, 3, 4, 17])\n\n    structure_of_mess = [\n        14,\n        NestTest.Abc(""a"", True),\n        {\n            ""d"": _CustomMapping({\n                41: 42\n            }),\n            ""c"": [\n                0,\n                collections.OrderedDict([\n                    (""b"", 9),\n                    (""a"", 8),\n                ]),\n            ],\n            ""b"": 3\n        },\n        ""hi everybody"",\n    ]\n\n    unflattened = nest.pack_sequence_as(structure_of_mess, flattened)\n    self.assertEqual(unflattened, mess)\n\n    # Check also that the OrderedDict was created, with the correct key order.\n    unflattened_ordered_dict = unflattened[2][""c""][1]\n    self.assertIsInstance(unflattened_ordered_dict, collections.OrderedDict)\n    self.assertEqual(list(unflattened_ordered_dict.keys()), [""b"", ""a""])\n\n    unflattened_custom_mapping = unflattened[2][""d""]\n    self.assertIsInstance(unflattened_custom_mapping, _CustomMapping)\n    self.assertEqual(list(unflattened_custom_mapping.keys()), [41])\n\n  def testFlatten_numpyIsNotFlattened(self):\n    structure = np.array([1, 2, 3])\n    flattened = nest.flatten(structure)\n    self.assertEqual(len(flattened), 1)\n\n  def testFlatten_stringIsNotFlattened(self):\n    structure = ""lots of letters""\n    flattened = nest.flatten(structure)\n    self.assertEqual(len(flattened), 1)\n    unflattened = nest.pack_sequence_as(""goodbye"", flattened)\n    self.assertEqual(structure, unflattened)\n\n  def testPackSequenceAs_notIterableError(self):\n    with self.assertRaisesRegexp(TypeError,\n                                 ""flat_sequence must be a sequence""):\n      nest.pack_sequence_as(""hi"", ""bye"")\n\n  def testPackSequenceAs_wrongLengthsError(self):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Structure had 2 elements, but flat_sequence had 3 elements.""):\n      nest.pack_sequence_as([""hello"", ""world""],\n                            [""and"", ""goodbye"", ""again""])\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testIsSequence(self):\n    self.assertFalse(nest.is_sequence(""1234""))\n    self.assertTrue(nest.is_sequence([1, 3, [4, 5]]))\n    self.assertTrue(nest.is_sequence(((7, 8), (5, 6))))\n    self.assertTrue(nest.is_sequence([]))\n    self.assertTrue(nest.is_sequence({""a"": 1, ""b"": 2}))\n    self.assertFalse(nest.is_sequence(set([1, 2])))\n    ones = array_ops.ones([2, 3])\n    self.assertFalse(nest.is_sequence(ones))\n    self.assertFalse(nest.is_sequence(math_ops.tanh(ones)))\n    self.assertFalse(nest.is_sequence(np.ones((4, 5))))\n\n  @parameterized.parameters({""mapping_type"": _CustomMapping},\n                            {""mapping_type"": dict})\n  def testFlattenDictItems(self, mapping_type):\n    dictionary = mapping_type({(4, 5, (6, 8)): (""a"", ""b"", (""c"", ""d""))})\n    flat = {4: ""a"", 5: ""b"", 6: ""c"", 8: ""d""}\n    self.assertEqual(nest.flatten_dict_items(dictionary), flat)\n\n    with self.assertRaises(TypeError):\n      nest.flatten_dict_items(4)\n\n    bad_dictionary = mapping_type({(4, 5, (4, 8)): (""a"", ""b"", (""c"", ""d""))})\n    with self.assertRaisesRegexp(ValueError, ""not unique""):\n      nest.flatten_dict_items(bad_dictionary)\n\n    another_bad_dictionary = mapping_type({\n        (4, 5, (6, 8)): (""a"", ""b"", (""c"", (""d"", ""e"")))\n    })\n    with self.assertRaisesRegexp(\n        ValueError, ""Key had [0-9]* elements, but value had [0-9]* elements""):\n      nest.flatten_dict_items(another_bad_dictionary)\n\n  # pylint does not correctly recognize these as class names and\n  # suggests to use variable style under_score naming.\n  # pylint: disable=invalid-name\n  Named0ab = collections.namedtuple(""named_0"", (""a"", ""b""))\n  Named1ab = collections.namedtuple(""named_1"", (""a"", ""b""))\n  SameNameab = collections.namedtuple(""same_name"", (""a"", ""b""))\n  SameNameab2 = collections.namedtuple(""same_name"", (""a"", ""b""))\n  SameNamexy = collections.namedtuple(""same_name"", (""x"", ""y""))\n  SameName1xy = collections.namedtuple(""same_name_1"", (""x"", ""y""))\n  SameName1xy2 = collections.namedtuple(""same_name_1"", (""x"", ""y""))\n  NotSameName = collections.namedtuple(""not_same_name"", (""a"", ""b""))\n  # pylint: enable=invalid-name\n\n  class SameNamedType1(SameNameab):\n    pass\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testAssertSameStructure(self):\n    structure1 = (((1, 2), 3), 4, (5, 6))\n    structure2 = (((""foo1"", ""foo2""), ""foo3""), ""foo4"", (""foo5"", ""foo6""))\n    structure_different_num_elements = (""spam"", ""eggs"")\n    structure_different_nesting = (((1, 2), 3), 4, 5, (6,))\n    nest.assert_same_structure(structure1, structure2)\n    nest.assert_same_structure(""abc"", 1.0)\n    nest.assert_same_structure(""abc"", np.array([0, 1]))\n    nest.assert_same_structure(""abc"", constant_op.constant([0, 1]))\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""The two structures don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure:.*?\\n\\n""\n         ""Second structure:.*\\n\\n""\n         ""More specifically: Substructure ""\n         r\'""type=tuple str=\\(\\(1, 2\\), 3\\)"" is a sequence, while \'\n         \'substructure ""type=str str=spam"" is not\\n\'\n         ""Entire first structure:\\n""\n         r""\\(\\(\\(\\., \\.\\), \\.\\), \\., \\(\\., \\.\\)\\)\\n""\n         ""Entire second structure:\\n""\n         r""\\(\\., \\.\\)"")):\n      nest.assert_same_structure(structure1, structure_different_num_elements)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""The two structures don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure:.*?\\n\\n""\n         ""Second structure:.*\\n\\n""\n         r\'More specifically: Substructure ""type=list str=\\[0, 1\\]"" \'\n         r\'is a sequence, while substructure ""type=ndarray str=\\[0 1\\]"" \'\n         ""is not"")):\n      nest.assert_same_structure([0, 1], np.array([0, 1]))\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""The two structures don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure:.*?\\n\\n""\n         ""Second structure:.*\\n\\n""\n         r\'More specifically: Substructure ""type=list str=\\[0, 1\\]"" \'\n         \'is a sequence, while substructure ""type=int str=0"" \'\n         ""is not"")):\n      nest.assert_same_structure(0, [0, 1])\n\n    self.assertRaises(TypeError, nest.assert_same_structure, (0, 1), [0, 1])\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure: .*?\\n\\nSecond structure: "")):\n      nest.assert_same_structure(structure1, structure_different_nesting)\n\n    self.assertRaises(TypeError, nest.assert_same_structure, (0, 1),\n                      NestTest.Named0ab(""a"", ""b""))\n\n    nest.assert_same_structure(NestTest.Named0ab(3, 4),\n                               NestTest.Named0ab(""a"", ""b""))\n\n    self.assertRaises(TypeError, nest.assert_same_structure,\n                      NestTest.Named0ab(3, 4), NestTest.Named1ab(3, 4))\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure: .*?\\n\\nSecond structure: "")):\n      nest.assert_same_structure(NestTest.Named0ab(3, 4),\n                                 NestTest.Named0ab([3], 4))\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        (""don\'t have the same nested structure\\\\.\\n\\n""\n         ""First structure: .*?\\n\\nSecond structure: "")):\n      nest.assert_same_structure([[3], 4], [3, [4]])\n\n    structure1_list = [[[1, 2], 3], 4, [5, 6]]\n    with self.assertRaisesRegexp(TypeError,\n                                 ""don\'t have the same sequence type""):\n      nest.assert_same_structure(structure1, structure1_list)\n    nest.assert_same_structure(structure1, structure2, check_types=False)\n    nest.assert_same_structure(structure1, structure1_list, check_types=False)\n\n    with self.assertRaisesRegexp(ValueError,\n                                 ""don\'t have the same set of keys""):\n      nest.assert_same_structure({""a"": 1}, {""b"": 1})\n\n    nest.assert_same_structure(NestTest.SameNameab(0, 1),\n                               NestTest.SameNameab2(2, 3))\n\n    # This assertion is expected to pass: two namedtuples with the same\n    # name and field names are considered to be identical.\n    nest.assert_same_structure(\n        NestTest.SameNameab(NestTest.SameName1xy(0, 1), 2),\n        NestTest.SameNameab2(NestTest.SameName1xy2(2, 3), 4))\n\n    expected_message = ""The two structures don\'t have the same.*""\n    with self.assertRaisesRegexp(ValueError, expected_message):\n      nest.assert_same_structure(\n          NestTest.SameNameab(0, NestTest.SameNameab2(1, 2)),\n          NestTest.SameNameab2(NestTest.SameNameab(0, 1), 2))\n\n    self.assertRaises(TypeError, nest.assert_same_structure,\n                      NestTest.SameNameab(0, 1), NestTest.NotSameName(2, 3))\n\n    self.assertRaises(TypeError, nest.assert_same_structure,\n                      NestTest.SameNameab(0, 1), NestTest.SameNamexy(2, 3))\n\n    self.assertRaises(TypeError, nest.assert_same_structure,\n                      NestTest.SameNameab(0, 1), NestTest.SameNamedType1(2, 3))\n\n  EmptyNT = collections.namedtuple(""empty_nt"", """")  # pylint: disable=invalid-name\n\n  def testHeterogeneousComparison(self):\n    nest.assert_same_structure({""a"": 4}, _CustomMapping(a=3))\n    nest.assert_same_structure(_CustomMapping(b=3), {""b"": 4})\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testMapStructure(self):\n    structure1 = (((1, 2), 3), 4, (5, 6))\n    structure2 = (((7, 8), 9), 10, (11, 12))\n    structure1_plus1 = nest.map_structure(lambda x: x + 1, structure1)\n    nest.assert_same_structure(structure1, structure1_plus1)\n    self.assertAllEqual(\n        [2, 3, 4, 5, 6, 7],\n        nest.flatten(structure1_plus1))\n    structure1_plus_structure2 = nest.map_structure(\n        lambda x, y: x + y, structure1, structure2)\n    self.assertEqual(\n        (((1 + 7, 2 + 8), 3 + 9), 4 + 10, (5 + 11, 6 + 12)),\n        structure1_plus_structure2)\n\n    self.assertEqual(3, nest.map_structure(lambda x: x - 1, 4))\n\n    self.assertEqual(7, nest.map_structure(lambda x, y: x + y, 3, 4))\n\n    # Empty structures\n    self.assertEqual((), nest.map_structure(lambda x: x + 1, ()))\n    self.assertEqual([], nest.map_structure(lambda x: x + 1, []))\n    self.assertEqual({}, nest.map_structure(lambda x: x + 1, {}))\n    self.assertEqual(NestTest.EmptyNT(), nest.map_structure(lambda x: x + 1,\n                                                            NestTest.EmptyNT()))\n\n    # This is checking actual equality of types, empty list != empty tuple\n    self.assertNotEqual((), nest.map_structure(lambda x: x + 1, []))\n\n    with self.assertRaisesRegexp(TypeError, ""callable""):\n      nest.map_structure(""bad"", structure1_plus1)\n\n    with self.assertRaisesRegexp(ValueError, ""at least one structure""):\n      nest.map_structure(lambda x: x)\n\n    with self.assertRaisesRegexp(ValueError, ""same number of elements""):\n      nest.map_structure(lambda x, y: None, (3, 4), (3, 4, 5))\n\n    with self.assertRaisesRegexp(ValueError, ""same nested structure""):\n      nest.map_structure(lambda x, y: None, 3, (3,))\n\n    with self.assertRaisesRegexp(TypeError, ""same sequence type""):\n      nest.map_structure(lambda x, y: None, ((3, 4), 5), [(3, 4), 5])\n\n    with self.assertRaisesRegexp(ValueError, ""same nested structure""):\n      nest.map_structure(lambda x, y: None, ((3, 4), 5), (3, (4, 5)))\n\n    structure1_list = [[[1, 2], 3], 4, [5, 6]]\n    with self.assertRaisesRegexp(TypeError, ""same sequence type""):\n      nest.map_structure(lambda x, y: None, structure1, structure1_list)\n\n    nest.map_structure(lambda x, y: None, structure1, structure1_list,\n                       check_types=False)\n\n    with self.assertRaisesRegexp(ValueError, ""same nested structure""):\n      nest.map_structure(lambda x, y: None, ((3, 4), 5), (3, (4, 5)),\n                         check_types=False)\n\n    with self.assertRaisesRegexp(ValueError, ""Only valid keyword argument""):\n      nest.map_structure(lambda x: None, structure1, foo=""a"")\n\n    with self.assertRaisesRegexp(ValueError, ""Only valid keyword argument""):\n      nest.map_structure(lambda x: None, structure1, check_types=False, foo=""a"")\n\n  ABTuple = collections.namedtuple(""ab_tuple"", ""a, b"")  # pylint: disable=invalid-name\n\n  @test_util.assert_no_new_pyobjects_executing_eagerly\n  def testMapStructureWithStrings(self):\n    inp_a = NestTest.ABTuple(a=""foo"", b=(""bar"", ""baz""))\n    inp_b = NestTest.ABTuple(a=2, b=(1, 3))\n    out = nest.map_structure(lambda string, repeats: string * repeats,\n                             inp_a,\n                             inp_b)\n    self.assertEqual(""foofoo"", out.a)\n    self.assertEqual(""bar"", out.b[0])\n    self.assertEqual(""bazbazbaz"", out.b[1])\n\n    nt = NestTest.ABTuple(a=(""something"", ""something_else""),\n                          b=""yet another thing"")\n    rev_nt = nest.map_structure(lambda x: x[::-1], nt)\n    # Check the output is the correct structure, and all strings are reversed.\n    nest.assert_same_structure(nt, rev_nt)\n    self.assertEqual(nt.a[0][::-1], rev_nt.a[0])\n    self.assertEqual(nt.a[1][::-1], rev_nt.a[1])\n    self.assertEqual(nt.b[::-1], rev_nt.b)\n\n  @test_util.run_deprecated_v1\n  def testMapStructureOverPlaceholders(self):\n    inp_a = (array_ops.placeholder(dtypes.float32, shape=[3, 4]),\n             array_ops.placeholder(dtypes.float32, shape=[3, 7]))\n    inp_b = (array_ops.placeholder(dtypes.float32, shape=[3, 4]),\n             array_ops.placeholder(dtypes.float32, shape=[3, 7]))\n\n    output = nest.map_structure(lambda x1, x2: x1 + x2, inp_a, inp_b)\n\n    nest.assert_same_structure(output, inp_a)\n    self.assertShapeEqual(np.zeros((3, 4)), output[0])\n    self.assertShapeEqual(np.zeros((3, 7)), output[1])\n\n    feed_dict = {\n        inp_a: (np.random.randn(3, 4), np.random.randn(3, 7)),\n        inp_b: (np.random.randn(3, 4), np.random.randn(3, 7))\n    }\n\n    with self.cached_session() as sess:\n      output_np = sess.run(output, feed_dict=feed_dict)\n    self.assertAllClose(output_np[0],\n                        feed_dict[inp_a][0] + feed_dict[inp_b][0])\n    self.assertAllClose(output_np[1],\n                        feed_dict[inp_a][1] + feed_dict[inp_b][1])\n\n  def testAssertShallowStructure(self):\n    inp_ab = [""a"", ""b""]\n    inp_abc = [""a"", ""b"", ""c""]\n    expected_message = (\n        ""The two structures don\'t have the same sequence length. Input ""\n        ""structure has length 2, while shallow structure has length 3."")\n    with self.assertRaisesRegexp(ValueError, expected_message):\n      nest.assert_shallow_structure(inp_abc, inp_ab)\n\n    inp_ab1 = [(1, 1), (2, 2)]\n    inp_ab2 = [[1, 1], [2, 2]]\n    expected_message = (\n        ""The two structures don\'t have the same sequence type. Input structure ""\n        ""has type <(type|class) \'tuple\'>, while shallow structure has type ""\n        ""<(type|class) \'list\'>."")\n    with self.assertRaisesRegexp(TypeError, expected_message):\n      nest.assert_shallow_structure(inp_ab2, inp_ab1)\n    nest.assert_shallow_structure(inp_ab2, inp_ab1, check_types=False)\n\n    inp_ab1 = {""a"": (1, 1), ""b"": {""c"": (2, 2)}}\n    inp_ab2 = {""a"": (1, 1), ""b"": {""d"": (2, 2)}}\n    expected_message = (\n        r""The two structures don\'t have the same keys. Input ""\n        r""structure has keys \\[\'c\'\\], while shallow structure has ""\n        r""keys \\[\'d\'\\]."")\n\n    with self.assertRaisesRegexp(ValueError, expected_message):\n      nest.assert_shallow_structure(inp_ab2, inp_ab1)\n\n    inp_ab = collections.OrderedDict([(""a"", 1), (""b"", (2, 3))])\n    inp_ba = collections.OrderedDict([(""b"", (2, 3)), (""a"", 1)])\n    nest.assert_shallow_structure(inp_ab, inp_ba)\n\n    # This assertion is expected to pass: two namedtuples with the same\n    # name and field names are considered to be identical.\n    inp_shallow = NestTest.SameNameab(1, 2)\n    inp_deep = NestTest.SameNameab2(1, [1, 2, 3])\n    nest.assert_shallow_structure(inp_shallow, inp_deep, check_types=False)\n    nest.assert_shallow_structure(inp_shallow, inp_deep, check_types=True)\n\n  def testFlattenUpTo(self):\n    # Shallow tree ends at scalar.\n    input_tree = [[[2, 2], [3, 3]], [[4, 9], [5, 5]]]\n    shallow_tree = [[True, True], [False, True]]\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [[2, 2], [3, 3], [4, 9], [5, 5]])\n    self.assertEqual(flattened_shallow_tree, [True, True, False, True])\n\n    # Shallow tree ends at string.\n    input_tree = [[(""a"", 1), [(""b"", 2), [(""c"", 3), [(""d"", 4)]]]]]\n    shallow_tree = [[""level_1"", [""level_2"", [""level_3"", [""level_4""]]]]]\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    input_tree_flattened = nest.flatten(input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree,\n                     [(""a"", 1), (""b"", 2), (""c"", 3), (""d"", 4)])\n    self.assertEqual(input_tree_flattened, [""a"", 1, ""b"", 2, ""c"", 3, ""d"", 4])\n\n    # Make sure dicts are correctly flattened, yielding values, not keys.\n    input_tree = {""a"": 1, ""b"": {""c"": 2}, ""d"": [3, (4, 5)]}\n    shallow_tree = {""a"": 0, ""b"": 0, ""d"": [0, 0]}\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree,\n                     [1, {""c"": 2}, 3, (4, 5)])\n\n    # Namedtuples.\n    ab_tuple = NestTest.ABTuple\n    input_tree = ab_tuple(a=[0, 1], b=2)\n    shallow_tree = ab_tuple(a=0, b=1)\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree,\n                     [[0, 1], 2])\n\n    # Nested dicts, OrderedDicts and namedtuples.\n    input_tree = collections.OrderedDict(\n        [(""a"", ab_tuple(a=[0, {""b"": 1}], b=2)),\n         (""c"", {""d"": 3, ""e"": collections.OrderedDict([(""f"", 4)])})])\n    shallow_tree = input_tree\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree, [0, 1, 2, 3, 4])\n    shallow_tree = collections.OrderedDict([(""a"", 0), (""c"", {""d"": 3, ""e"": 1})])\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree,\n                     [ab_tuple(a=[0, {""b"": 1}], b=2),\n                      3,\n                      collections.OrderedDict([(""f"", 4)])])\n    shallow_tree = collections.OrderedDict([(""a"", 0), (""c"", 0)])\n    input_tree_flattened_as_shallow_tree = nest.flatten_up_to(shallow_tree,\n                                                              input_tree)\n    self.assertEqual(input_tree_flattened_as_shallow_tree,\n                     [ab_tuple(a=[0, {""b"": 1}], b=2),\n                      {""d"": 3, ""e"": collections.OrderedDict([(""f"", 4)])}])\n\n    ## Shallow non-list edge-case.\n    # Using iterable elements.\n    input_tree = [""input_tree""]\n    shallow_tree = ""shallow_tree""\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    input_tree = [""input_tree_0"", ""input_tree_1""]\n    shallow_tree = ""shallow_tree""\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    # Using non-iterable elements.\n    input_tree = [0]\n    shallow_tree = 9\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    input_tree = [0, 1]\n    shallow_tree = 9\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    ## Both non-list edge-case.\n    # Using iterable elements.\n    input_tree = ""input_tree""\n    shallow_tree = ""shallow_tree""\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    # Using non-iterable elements.\n    input_tree = 0\n    shallow_tree = 0\n    flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_input_tree, [input_tree])\n    self.assertEqual(flattened_shallow_tree, [shallow_tree])\n\n    ## Input non-list edge-case.\n    # Using iterable elements.\n    input_tree = ""input_tree""\n    shallow_tree = [""shallow_tree""]\n    expected_message = (""If shallow structure is a sequence, input must also ""\n                        ""be a sequence. Input has type: <(type|class) \'str\'>."")\n    with self.assertRaisesRegexp(TypeError, expected_message):\n      flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_shallow_tree, shallow_tree)\n\n    input_tree = ""input_tree""\n    shallow_tree = [""shallow_tree_9"", ""shallow_tree_8""]\n    with self.assertRaisesRegexp(TypeError, expected_message):\n      flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_shallow_tree, shallow_tree)\n\n    # Using non-iterable elements.\n    input_tree = 0\n    shallow_tree = [9]\n    expected_message = (""If shallow structure is a sequence, input must also ""\n                        ""be a sequence. Input has type: <(type|class) \'int\'>."")\n    with self.assertRaisesRegexp(TypeError, expected_message):\n      flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_shallow_tree, shallow_tree)\n\n    input_tree = 0\n    shallow_tree = [9, 8]\n    with self.assertRaisesRegexp(TypeError, expected_message):\n      flattened_input_tree = nest.flatten_up_to(shallow_tree, input_tree)\n    flattened_shallow_tree = nest.flatten_up_to(shallow_tree, shallow_tree)\n    self.assertEqual(flattened_shallow_tree, shallow_tree)\n\n  def testMapStructureUpTo(self):\n    # Named tuples.\n    ab_tuple = collections.namedtuple(""ab_tuple"", ""a, b"")\n    op_tuple = collections.namedtuple(""op_tuple"", ""add, mul"")\n    inp_val = ab_tuple(a=2, b=3)\n    inp_ops = ab_tuple(a=op_tuple(add=1, mul=2), b=op_tuple(add=2, mul=3))\n    out = nest.map_structure_up_to(\n        inp_val, lambda val, ops: (val + ops.add) * ops.mul, inp_val, inp_ops)\n    self.assertEqual(out.a, 6)\n    self.assertEqual(out.b, 15)\n\n    # Lists.\n    data_list = [[2, 4, 6, 8], [[1, 3, 5, 7, 9], [3, 5, 7]]]\n    name_list = [""evens"", [""odds"", ""primes""]]\n    out = nest.map_structure_up_to(\n        name_list, lambda name, sec: ""first_{}_{}"".format(len(sec), name),\n        name_list, data_list)\n    self.assertEqual(out, [""first_4_evens"", [""first_5_odds"", ""first_3_primes""]])\n\n    # Dicts.\n    inp_val = dict(a=2, b=3)\n    inp_ops = dict(a=dict(add=1, mul=2), b=dict(add=2, mul=3))\n    out = nest.map_structure_up_to(\n        inp_val,\n        lambda val, ops: (val + ops[""add""]) * ops[""mul""], inp_val, inp_ops)\n    self.assertEqual(out[""a""], 6)\n    self.assertEqual(out[""b""], 15)\n\n    # Non-equal dicts.\n    inp_val = dict(a=2, b=3)\n    inp_ops = dict(a=dict(add=1, mul=2), c=dict(add=2, mul=3))\n    with self.assertRaisesRegexp(ValueError, ""same keys""):\n      nest.map_structure_up_to(\n          inp_val,\n          lambda val, ops: (val + ops[""add""]) * ops[""mul""], inp_val, inp_ops)\n\n    # Dict+custom mapping.\n    inp_val = dict(a=2, b=3)\n    inp_ops = _CustomMapping(a=dict(add=1, mul=2), b=dict(add=2, mul=3))\n    out = nest.map_structure_up_to(\n        inp_val,\n        lambda val, ops: (val + ops[""add""]) * ops[""mul""], inp_val, inp_ops)\n    self.assertEqual(out[""a""], 6)\n    self.assertEqual(out[""b""], 15)\n\n    # Non-equal dict/mapping.\n    inp_val = dict(a=2, b=3)\n    inp_ops = _CustomMapping(a=dict(add=1, mul=2), c=dict(add=2, mul=3))\n    with self.assertRaisesRegexp(ValueError, ""same keys""):\n      nest.map_structure_up_to(\n          inp_val,\n          lambda val, ops: (val + ops[""add""]) * ops[""mul""], inp_val, inp_ops)\n\n  def testGetTraverseShallowStructure(self):\n    scalar_traverse_input = [3, 4, (1, 2, [0]), [5, 6], {""a"": (7,)}, []]\n    scalar_traverse_r = nest.get_traverse_shallow_structure(\n        lambda s: not isinstance(s, tuple),\n        scalar_traverse_input)\n    self.assertEqual(scalar_traverse_r,\n                     [True, True, False, [True, True], {""a"": False}, []])\n    nest.assert_shallow_structure(scalar_traverse_r,\n                                  scalar_traverse_input)\n\n    structure_traverse_input = [(1, [2]), ([1], 2)]\n    structure_traverse_r = nest.get_traverse_shallow_structure(\n        lambda s: (True, False) if isinstance(s, tuple) else True,\n        structure_traverse_input)\n    self.assertEqual(structure_traverse_r,\n                     [(True, False), ([True], False)])\n    nest.assert_shallow_structure(structure_traverse_r,\n                                  structure_traverse_input)\n\n    with self.assertRaisesRegexp(TypeError, ""returned structure""):\n      nest.get_traverse_shallow_structure(lambda _: [True], 0)\n\n    with self.assertRaisesRegexp(TypeError, ""returned a non-bool scalar""):\n      nest.get_traverse_shallow_structure(lambda _: 1, [1])\n\n    with self.assertRaisesRegexp(\n        TypeError, ""didn\'t return a depth=1 structure of bools""):\n      nest.get_traverse_shallow_structure(lambda _: [1], [1])\n\n  def testYieldFlatStringPaths(self):\n    for inputs_expected in ({""inputs"": [], ""expected"": []},\n                            {""inputs"": 3, ""expected"": [()]},\n                            {""inputs"": [3], ""expected"": [(0,)]},\n                            {""inputs"": {""a"": 3}, ""expected"": [(""a"",)]},\n                            {""inputs"": {""a"": {""b"": 4}},\n                             ""expected"": [(""a"", ""b"")]},\n                            {""inputs"": [{""a"": 2}], ""expected"": [(0, ""a"")]},\n                            {""inputs"": [{""a"": [2]}], ""expected"": [(0, ""a"", 0)]},\n                            {""inputs"": [{""a"": [(23, 42)]}],\n                             ""expected"": [(0, ""a"", 0, 0), (0, ""a"", 0, 1)]},\n                            {""inputs"": [{""a"": ([23], 42)}],\n                             ""expected"": [(0, ""a"", 0, 0), (0, ""a"", 1)]},\n                            {""inputs"": {""a"": {""a"": 2}, ""c"": [[[4]]]},\n                             ""expected"": [(""a"", ""a""), (""c"", 0, 0, 0)]},\n                            {""inputs"": {""0"": [{""1"": 23}]},\n                             ""expected"": [(""0"", 0, ""1"")]}):\n      inputs = inputs_expected[""inputs""]\n      expected = inputs_expected[""expected""]\n      self.assertEqual(list(nest.yield_flat_paths(inputs)), expected)\n\n  def testFlattenWithStringPaths(self):\n    for inputs_expected in (\n        {""inputs"": [], ""expected"": []},\n        {""inputs"": [23, ""42""], ""expected"": [(""0"", 23), (""1"", ""42"")]},\n        {""inputs"": [[[[108]]]], ""expected"": [(""0/0/0/0"", 108)]}):\n      inputs = inputs_expected[""inputs""]\n      expected = inputs_expected[""expected""]\n      self.assertEqual(\n          nest.flatten_with_joined_string_paths(inputs, separator=""/""),\n          expected)\n\n  # Need a separate test for namedtuple as we can\'t declare tuple definitions\n  # in the @parameterized arguments.\n  def testFlattenNamedTuple(self):\n    # pylint: disable=invalid-name\n    Foo = collections.namedtuple(""Foo"", [""a"", ""b""])\n    Bar = collections.namedtuple(""Bar"", [""c"", ""d""])\n    # pylint: enable=invalid-name\n    test_cases = [\n        (Foo(a=3, b=Bar(c=23, d=42)),\n         [(""a"", 3), (""b/c"", 23), (""b/d"", 42)]),\n        (Foo(a=Bar(c=23, d=42), b=Bar(c=0, d=""something"")),\n         [(""a/c"", 23), (""a/d"", 42), (""b/c"", 0), (""b/d"", ""something"")]),\n        (Bar(c=42, d=43),\n         [(""c"", 42), (""d"", 43)]),\n        (Bar(c=[42], d=43),\n         [(""c/0"", 42), (""d"", 43)]),\n    ]\n    for inputs, expected in test_cases:\n      self.assertEqual(\n          list(nest.flatten_with_joined_string_paths(inputs)), expected)\n\n  @parameterized.named_parameters(\n      (""tuples"", (1, 2), (3, 4), True, ((""0"", 4), (""1"", 6))),\n      (""dicts"", {""a"": 1, ""b"": 2}, {""b"": 4, ""a"": 3}, True,\n       {""a"": (""a"", 4), ""b"": (""b"", 6)}),\n      (""mixed"", (1, 2), [3, 4], False, ((""0"", 4), (""1"", 6))),\n      (""nested"",\n       {""a"": [2, 3], ""b"": [1, 2, 3]}, {""b"": [5, 6, 7], ""a"": [8, 9]}, True,\n       {""a"": [(""a/0"", 10), (""a/1"", 12)],\n        ""b"": [(""b/0"", 6), (""b/1"", 8), (""b/2"", 10)]}))\n  def testMapWithPathsCompatibleStructures(self, s1, s2, check_types, expected):\n    def format_sum(path, *values):\n      return (path, sum(values))\n    result = nest.map_structure_with_paths(format_sum, s1, s2,\n                                           check_types=check_types)\n    self.assertEqual(expected, result)\n\n  @parameterized.named_parameters(\n      (""tuples"", (1, 2), (3, 4, 5), ValueError),\n      (""dicts"", {""a"": 1}, {""b"": 2}, ValueError),\n      (""mixed"", (1, 2), [3, 4], TypeError),\n      (""nested"",\n       {""a"": [2, 3], ""b"": [1, 3]},\n       {""b"": [5, 6, 7], ""a"": [8, 9]},\n       ValueError\n      ))\n  def testMapWithPathsIncompatibleStructures(self, s1, s2, error_type):\n    with self.assertRaises(error_type):\n      nest.map_structure_with_paths(lambda path, *s: 0, s1, s2)\n\n\nclass NestBenchmark(test.Benchmark):\n\n  def run_and_report(self, s1, s2, name):\n    burn_iter, test_iter = 100, 30000\n\n    for _ in xrange(burn_iter):\n      nest.assert_same_structure(s1, s2)\n\n    t0 = time.time()\n    for _ in xrange(test_iter):\n      nest.assert_same_structure(s1, s2)\n    t1 = time.time()\n\n    self.report_benchmark(iters=test_iter, wall_time=(t1 - t0) / test_iter,\n                          name=name)\n\n  def benchmark_assert_structure(self):\n    s1 = (((1, 2), 3), 4, (5, 6))\n    s2 = (((""foo1"", ""foo2""), ""foo3""), ""foo4"", (""foo5"", ""foo6""))\n    self.run_and_report(s1, s2, ""assert_same_structure_6_elem"")\n\n    s1 = (((1, 2), 3), 4, (5, 6)) * 10\n    s2 = (((""foo1"", ""foo2""), ""foo3""), ""foo4"", (""foo5"", ""foo6"")) * 10\n    self.run_and_report(s1, s2, ""assert_same_structure_60_elem"")\n\n\nif __name__ == ""__main__"":\n  test.main()\n'"
test/TensorFlowNET.UnitTest/nn_test/nn_test.py,1,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for miscellaneous functionality in tensorflow.ops.nn.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_impl\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import partitioned_variables\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.ops.nn_impl import _compute_sampled_logits\nfrom tensorflow.python.platform import test as test_lib\n\n\nclass ZeroFractionTest(test_lib.TestCase):\n\n  def _ZeroFraction(self, x):\n    assert x.shape\n    total_elements = np.prod(x.shape)\n    nonzeros = np.count_nonzero(x.flatten())\n    return 1.0 - nonzeros / total_elements\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction(self):\n    x_shape = [5, 17]\n    x_np = np.random.randint(0, 2, size=x_shape).astype(np.float32)\n    y_np = self._ZeroFraction(x_np)\n\n    x_tf = constant_op.constant(x_np)\n    x_tf.set_shape(x_shape)\n    y_tf = nn_impl.zero_fraction(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n\n    eps = 1e-8\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  @test_util.run_deprecated_v1\n  def testZeroFractionEmpty(self):\n    x = np.zeros(0)\n    y = self.evaluate(nn_impl.zero_fraction(x))\n    self.assertTrue(np.isnan(y))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Zeros(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.zeros([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(1.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testZeroFraction2_27Ones(self):\n    sparsity = nn_impl.zero_fraction(\n        array_ops.ones([int(2**27 * 1.01)], dtype=dtypes.int8))\n    self.assertAllClose(0.0, self.evaluate(sparsity))\n\n  @test_util.run_deprecated_v1\n  def testUnknownSize(self):\n    value = array_ops.placeholder(dtype=dtypes.float32)\n    sparsity = nn_impl.zero_fraction(value)\n    with self.cached_session() as sess:\n      self.assertAllClose(\n          0.25,\n          sess.run(sparsity, {value: [[0., 1.], [0.3, 2.]]}))\n\n\nclass SoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = np.exp(x - m)\n    z = u.sum(1)[:, np.newaxis]\n    return u / z\n\n  @test_util.run_in_graph_and_eager_modes\n  def testSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.softmax_v2(x_tf)\n    y_tf_last_dim = nn_ops.softmax_v2(x_tf, 1)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_last_dim_np = self.evaluate(y_tf_last_dim)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_last_dim_np, y_np, eps)\n\n  def testSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 2e-8\n    self.assertLess(err, eps)\n\n\nclass LogPoissonLossTest(test_lib.TestCase):\n\n  def _log_poisson_loss(self, x, z, compute_full_loss=False):\n    lpl = np.exp(x) - z * x\n    if compute_full_loss:\n      stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\n      lpl += np.ma.masked_array(stirling_approx, mask=(z <= 1)).filled(0.)\n    return lpl\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogPoissonLoss(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float32)\n    y_np = self._log_poisson_loss(x_np, z_np, compute_full_loss=False)\n    y_np_stirling = self._log_poisson_loss(x_np, z_np, compute_full_loss=True)\n    y_tf = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=False)\n    y_tf_stirling = nn_impl.log_poisson_loss(z_np, x_np, compute_full_loss=True)\n    y_tf_np = self.evaluate(y_tf)\n    y_tf_np_stirling = self.evaluate(y_tf_stirling)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n    self.assertAllClose(y_tf_np_stirling, y_np_stirling, eps)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    z_np = np.random.randint(0, 5, size=x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_impl.log_poisson_loss(z_np, x_tf, compute_full_loss=False)\n      y_tf_stirling = nn_impl.log_poisson_loss(\n          z_np, x_tf, compute_full_loss=True)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n      err_stirling = gradient_checker.compute_gradient_error(\n          x_tf, x_shape, y_tf_stirling, x_shape)\n    eps = 1e-6\n    self.assertLess(err, eps)\n    self.assertLess(err_stirling, eps)\n\n\nclass LogSoftmaxTest(test_lib.TestCase, parameterized.TestCase):\n\n  def _log_softmax(self, x):\n    assert len(x.shape) == 2\n    m = x.max(1)[:, np.newaxis]\n    u = x - m\n    return u - np.log(np.sum(np.exp(u), 1, keepdims=True))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testLogSoftmax(self):\n    x_shape = [5, 10]\n    x_np = np.random.randn(*x_shape).astype(np.float32)\n    y_np = self._log_softmax(x_np)\n    x_tf = constant_op.constant(x_np)\n    y_tf = nn_ops.log_softmax_v2(x_tf)\n    y_tf_np = self.evaluate(y_tf)\n    eps = 1e-3\n    self.assertAllClose(y_tf_np, y_np, eps)\n\n  def testLogSoftmaxAxes(self):\n    arr = np.linspace(0., 1, 12).reshape(3, 4)\n    x_neg_axis = nn_ops.log_softmax_v2(arr, axis=-2)\n    y_pos_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    z_gt_axis = nn_ops.log_softmax_v2(arr, axis=0)\n    x_neg_axis_tf = self.evaluate(x_neg_axis)\n    y_pos_axis_tf = self.evaluate(y_pos_axis)\n    z_gt_axis_tf = self.evaluate(z_gt_axis)\n    eps = 1e-3\n    self.assertAllClose(x_neg_axis_tf, y_pos_axis_tf, eps)\n    self.assertAllClose(y_pos_axis_tf, z_gt_axis_tf, eps)\n\n  @parameterized.parameters(((5, 10),), ((2, 3, 4),))\n  @test_util.run_deprecated_v1\n  def testGradient(self, x_shape):\n    x_np = np.random.randn(*x_shape).astype(np.float64)\n    with self.cached_session():\n      x_tf = constant_op.constant(x_np)\n      y_tf = nn_ops.log_softmax_v2(x_tf)\n      err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                    x_shape)\n    eps = 1e-7\n    self.assertLess(err, eps)\n\n\nclass L2LossTest(test_lib.TestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Loss(self):\n    for dtype in [dtypes.float32, dtypes.float64]:\n      x = constant_op.constant(\n          [1.0, 0.0, 3.0, 2.0], shape=[2, 2], name=""x"", dtype=dtype)\n      l2loss = nn_ops.l2_loss(x)\n      value = self.evaluate(l2loss)\n      self.assertAllClose(7.0, value)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)  # Make it reproducible.\n    x_val = np.random.random_sample(x_shape).astype(np.float64)\n    with self.cached_session():\n      x = constant_op.constant(x_val, name=""x"")\n      output = nn_ops.l2_loss(x)\n      err = gradient_checker.compute_gradient_error(x, x_shape, output, [1])\n    print(""L2Loss gradient err = %g "" % err)\n    err_tolerance = 1e-10\n    self.assertLess(err, err_tolerance)\n\n\nclass L2NormalizeTest(test_lib.TestCase):\n\n  def _l2Normalize(self, x, dim):\n    if isinstance(dim, list):\n      norm = np.linalg.norm(x, axis=tuple(dim))\n      for d in dim:\n        norm = np.expand_dims(norm, d)\n      return x / norm\n    else:\n      norm = np.apply_along_axis(np.linalg.norm, dim, x)\n      return x / np.expand_dims(norm, dim)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2Normalize(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    for dim in range(len(x_shape)):\n      y_np = self._l2Normalize(x_np, dim)\n      x_tf = constant_op.constant(x_np, name=""x"")\n      y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n      self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testL2NormalizeDimArray(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float32)\n    dim = [1, 2]\n    y_np = self._l2Normalize(x_np, dim)\n    x_tf = constant_op.constant(x_np, name=""x"")\n    y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n    self.assertAllClose(y_np, self.evaluate(y_tf))\n\n  @test_util.run_deprecated_v1\n  def testL2NormalizeGradient(self):\n    x_shape = [20, 7, 3]\n    np.random.seed(1)\n    x_np = np.random.random_sample(x_shape).astype(np.float64)\n    for dim in range(len(x_shape)):\n      with self.cached_session():\n        x_tf = constant_op.constant(x_np, name=""x"")\n        y_tf = nn_impl.l2_normalize_v2(x_tf, dim)\n        err = gradient_checker.compute_gradient_error(x_tf, x_shape, y_tf,\n                                                      x_shape)\n      print(""L2Normalize gradient err = %g "" % err)\n      self.assertLess(err, 1e-4)\n\n\nclass DropoutTest(test_lib.TestCase):\n\n  def testDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, keep_prob)\n      final_count = 0\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropout(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability. This time with shaped\n    # noise.\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  def testShapedDropoutCorrelation(self):\n    # Runs a shaped dropout and tests that the correlations are correct.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      dropout = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        # Verifies that each y column as only one type of activation.\n        for i in xrange(x_dim):\n          sorted_value = np.unique(np.sort(value[i, :]))\n          self.assertEqual(sorted_value.size, 1)\n\n  @test_util.run_deprecated_v1\n  def testDropoutPlaceholderKeepProb(self):\n    # Runs dropout with 0-1 tensor 10 times, sum the number of ones and validate\n    # that it is producing approximately the right number of ones over a large\n    # number of samples, based on the keep probability.\n    x_dim = 40\n    y_dim = 30\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      with self.cached_session():\n        t = constant_op.constant(\n            1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n        keep_prob_placeholder = array_ops.placeholder(dtypes.float32)\n        dropout = nn_ops.dropout(t, keep_prob_placeholder)\n        final_count = 0\n        self.assertEqual([x_dim, y_dim], dropout.get_shape())\n        for _ in xrange(0, num_iter):\n          value = dropout.eval(feed_dict={keep_prob_placeholder: keep_prob})\n          final_count += np.count_nonzero(value)\n          # Verifies that there are only two values: 0 and 1/keep_prob.\n          sorted_value = np.unique(np.sort(value))\n          self.assertEqual(0, sorted_value[0])\n          self.assertAllClose(1 / keep_prob, sorted_value[1])\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutUnknownShape(self):\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    x = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    dropout_x = nn_ops.dropout(\n        x, keep_prob, noise_shape=array_ops.placeholder(dtypes.int32))\n    self.assertEqual(x.get_shape(), dropout_x.get_shape())\n\n  def testPartialShapedDropout(self):\n    x_dim = 40 * 30\n    y_dim = 3\n    num_iter = 10\n    for keep_prob in [0.1, 0.5, 0.8]:\n      t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n      # Set noise_shape=[None, 1] which means [x_dim, 1].\n      dropout = nn_ops.dropout(t, keep_prob, noise_shape=[None, 1])\n      self.assertEqual([x_dim, y_dim], dropout.get_shape())\n      final_count = 0\n      for _ in xrange(0, num_iter):\n        value = self.evaluate(dropout)\n        final_count += np.count_nonzero(value)\n        # Verifies that there are only two values: 0 and 1/keep_prob.\n        sorted_value = np.unique(np.sort(value))\n        self.assertEqual(0, sorted_value[0])\n        self.assertAllClose(1 / keep_prob, sorted_value[1])\n\n      # Check that we are in the 15% error range\n      expected_count = x_dim * y_dim * keep_prob * num_iter\n      rel_error = math.fabs(final_count - expected_count) / expected_count\n      print(rel_error)\n      self.assertTrue(rel_error < 0.15)\n\n  @test_util.run_deprecated_v1\n  def testInvalidKeepProb(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, [0.0, 1.0])\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float64))\n    with self.assertRaises(ValueError):\n      nn_ops.dropout(t, array_ops.placeholder(dtypes.float32, shape=[2]))\n\n  @test_util.run_deprecated_v1\n  def testInvalidRate(self):\n    x_dim = 40\n    y_dim = 30\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, -1.0)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, 1.1)\n    with self.assertRaises(ValueError):\n      nn_ops.dropout_v2(t, [0.0, 1.0])\n\n  @test_util.run_deprecated_v1\n  def testShapedDropoutShapeError(self):\n    # Runs shaped dropout and verifies an error is thrown on misshapen noise.\n    x_dim = 40\n    y_dim = 30\n    keep_prob = 0.5\n    t = constant_op.constant(1.0, shape=[x_dim, y_dim], dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, y_dim + 10])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, y_dim, 5])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim + 3])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim])\n    # test that broadcasting proceeds\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[y_dim])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[1, y_dim])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[x_dim, 1])\n    _ = nn_ops.dropout(t, keep_prob, noise_shape=[1, 1])\n\n  def testNoDropoutFast(self):\n    x = array_ops.zeros((5,))\n    y = nn_ops.dropout(x, keep_prob=1)\n    self.assertTrue(x is y)\n\n    y = nn_ops.dropout_v2(x, rate=0)\n    self.assertTrue(x is y)\n\n  def testDropoutWithIntegerInputs(self):\n    x = constant_op.constant([1, 1, 1, 1, 1])\n    with self.assertRaises(ValueError):\n      _ = nn_ops.dropout(x, 0.5)\n\n\nclass ComputeSampledLogitsTest(test_lib.TestCase):\n\n  def setUp(self):\n    self._eps = 1e-3\n\n  def _GenerateTestData(self, num_classes, dim, batch_size, num_true, labels,\n                        sampled, subtract_log_q):\n    """"""Randomly generates input/output data for a single test case.\n\n    This function returns numpy constants for use in a test case.\n\n    Args:\n      num_classes: An int. The number of embedding classes in the test case.\n      dim: An int. The dimension of the embedding.\n      batch_size: An int. The batch size.\n      num_true: An int. The number of target classes per training example.\n      labels: A list of batch_size * num_true ints. The target classes.\n      sampled: A list of indices in [0, num_classes).\n      subtract_log_q: A bool corresponding to the parameter in\n          _compute_sampled_logits().\n\n    Returns:\n      weights: Embedding weights to use as test input. It is a numpy array\n          of shape [num_classes, dim]\n      biases: Embedding biases to use as test input. It is a numpy array\n          of shape [num_classes].\n      hidden_acts: Forward activations of the network to use as test input.\n          It is a numpy array of shape [batch_size, dim].\n      sampled_vals: A tuple based on `sampled` to use as test input in the\n          format returned by a *_candidate_sampler function.\n      exp_logits: The output logits expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n      exp_labels: The output labels expected from _compute_sampled_logits().\n          It is a numpy array of shape [batch_size, num_true + len(sampled)].\n    """"""\n    weights = np.random.randn(num_classes, dim).astype(np.float32)\n    biases = np.random.randn(num_classes).astype(np.float32)\n    hidden_acts = np.random.randn(batch_size, dim).astype(np.float32)\n\n    true_exp = np.full([batch_size, 1], fill_value=0.5, dtype=np.float32)\n    sampled_exp = np.full([len(sampled)], fill_value=0.5, dtype=np.float32)\n    sampled_vals = (sampled, true_exp, sampled_exp)\n\n    sampled_w, sampled_b = weights[sampled], biases[sampled]\n    true_w, true_b = weights[labels], biases[labels]\n\n    true_logits = np.sum(\n        hidden_acts.reshape((batch_size, 1, dim)) * true_w.reshape(\n            (batch_size, num_true, dim)),\n        axis=2)\n    true_b = true_b.reshape((batch_size, num_true))\n    true_logits += true_b\n    sampled_logits = np.dot(hidden_acts, sampled_w.T) + sampled_b\n\n    if subtract_log_q:\n      true_logits -= np.log(true_exp)\n      sampled_logits -= np.log(sampled_exp[np.newaxis, :])\n\n    exp_logits = np.concatenate([true_logits, sampled_logits], axis=1)\n    exp_labels = np.hstack((np.ones_like(true_logits) / num_true,\n                            np.zeros_like(sampled_logits)))\n\n    return weights, biases, hidden_acts, sampled_vals, exp_logits, exp_labels\n\n  def _ShardTestEmbeddings(self, weights, biases, num_shards):\n    """"""Shards the weights and biases returned by _GenerateTestData.\n\n    Args:\n      weights: The weights returned by _GenerateTestData.\n      biases: The biases returned by _GenerateTestData.\n      num_shards: The number of shards to create.\n\n    Returns:\n      sharded_weights: A list of size `num_shards` containing all the weights.\n      sharded_biases: A list of size `num_shards` containing all the biases.\n    """"""\n    with ops.Graph().as_default() as g:\n      sharded_weights = variable_scope.get_variable(\n          ""w"",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(weights))\n      sharded_biases = variable_scope.get_variable(\n          ""b"",\n          partitioner=partitioned_variables.fixed_size_partitioner(num_shards),\n          initializer=constant_op.constant(biases))\n      with self.session(graph=g) as sess:\n        variables.global_variables_initializer().run()\n        return self.evaluate([list(sharded_weights), list(sharded_biases)])\n\n  def testShapes(self):\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=""div"",\n          name=""sampled_logits_basic_num_true_%d"" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertEqual(exp_logits.shape, got_logits.shape, self._eps)\n      self.assertEqual(exp_labels.shape, got_labels.shape, self._eps)\n\n  def testBasic(self):\n    """"""Without accidental hit removal or subtract_log_q.""""""\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=""div"",\n          name=""sampled_logits_basic_num_true_%d"" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testAccidentalHitRemoval(self):\n    """"""With accidental hit removal, no subtract_log_q.""""""\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    sampled = [1, 0, 2, 3]\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, _,\n       _) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=sampled,\n           subtract_log_q=False)\n      logits_tensor, _ = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=len(sampled),\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=True,\n          partition_strategy=""div"",\n          name=""sampled_logits_accidental_hit_removal_num_true_%d"" % num_true)\n      # Test that the exponentiated logits of accidental hits are near 0.\n      # First we need to find the hits in this random test run:\n      labels_reshape = labels.reshape((batch_size, num_true))\n      got_logits = self.evaluate(logits_tensor)\n      for row in xrange(batch_size):\n        row_labels = labels_reshape[row, :]\n        for col in xrange(len(sampled)):\n          if sampled[col] in row_labels:\n            # We need to add the num_true_test offset into logits_*\n            self.assertNear(\n                np.exp(got_logits[row, col + num_true]), 0., self._eps)\n\n  def testSubtractLogQ(self):\n    """"""With subtract_log_q, no accidental hit removal.""""""\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=True)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=constant_op.constant(weights),\n          biases=constant_op.constant(biases),\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=True,\n          remove_accidental_hits=False,\n          partition_strategy=""div"",\n          name=""sampled_logits_subtract_log_q_num_true_%d"" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testSharded(self):\n    """"""With sharded weights and sharded biases.""""""\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n\n    for num_true in range(1, 5):\n      labels = np.random.randint(\n          low=0, high=num_classes, size=batch_size * num_true)\n      (weights, biases, hidden_acts, sampled_vals, exp_logits,\n       exp_labels) = self._GenerateTestData(\n           num_classes=num_classes,\n           dim=10,\n           batch_size=batch_size,\n           num_true=num_true,\n           labels=labels,\n           sampled=[1, 0, 2, 3],\n           subtract_log_q=False)\n      weight_shards, bias_shards = self._ShardTestEmbeddings(\n          weights, biases, num_shards=3)\n      logits_tensor, labels_tensor = _compute_sampled_logits(\n          weights=[constant_op.constant(shard) for shard in weight_shards],\n          biases=[constant_op.constant(shard) for shard in bias_shards],\n          labels=constant_op.constant(\n              labels, dtype=dtypes.int64, shape=(batch_size, num_true)),\n          inputs=constant_op.constant(hidden_acts),\n          num_sampled=4,\n          num_classes=num_classes,\n          num_true=num_true,\n          sampled_values=sampled_vals,\n          subtract_log_q=False,\n          remove_accidental_hits=False,\n          partition_strategy=""div"",\n          name=""sampled_logits_sharded_num_true_%d"" % num_true)\n      got_logits, got_labels = self.evaluate([logits_tensor, labels_tensor])\n      self.assertAllClose(exp_logits, got_logits, self._eps)\n      self.assertAllClose(exp_labels, got_labels, self._eps)\n\n  def testNCELoss(self):\n    # A simple test to verify the numerics.\n\n    def _SigmoidCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      pred = 1. / (1. + np.exp(-logits))\n      eps = 0.0001\n      pred = np.minimum(np.maximum(pred, eps), 1 - eps)\n      return -targets * np.log(pred) - (1. - targets) * np.log(1. - pred)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_nce_loss = np.sum(\n        _SigmoidCrossEntropyWithLogits(exp_logits, exp_labels), 1)\n\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_nce_loss = nn_impl.nce_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals)\n\n    self.assertAllClose(exp_nce_loss, self.evaluate(got_nce_loss), 1e-4)\n\n  def testSampledSoftmaxLoss(self):\n    # A simple test to verify the numerics.\n\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    (weights, biases, hidden_acts, sampled_vals, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=[1, 0, 2, 3],\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=constant_op.constant(weights),\n        biases=constant_op.constant(biases),\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n    # Test with sharded weights and sharded biases.\n    weight_shards, bias_shards = self._ShardTestEmbeddings(\n        weights, biases, num_shards=3)\n    got_sampled_softmax_loss = nn_impl.sampled_softmax_loss_v2(\n        weights=[constant_op.constant(shard) for shard in weight_shards],\n        biases=[constant_op.constant(shard) for shard in bias_shards],\n        labels=constant_op.constant(labels, shape=(batch_size, 1)),\n        inputs=constant_op.constant(hidden_acts),\n        num_sampled=4,\n        num_classes=num_classes,\n        num_true=1,\n        sampled_values=sampled_vals,\n        remove_accidental_hits=False)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-4)\n\n  def testSampledSoftmaxLossBf16(self):\n    # A simple test to verify the numerics for bfloat16.\n    def _SoftmaxCrossEntropyWithLogits(logits, targets):\n      # logits, targets: float arrays of the same shape.\n      assert logits.shape == targets.shape\n      stable_exp_logits = np.exp(\n          logits - np.amax(logits, axis=1, keepdims=True))\n      pred = stable_exp_logits / np.sum(stable_exp_logits, 1, keepdims=True)\n      return -np.sum(targets * np.log(pred + 1.0e-20), axis=1)\n\n    np.random.seed(0)\n    num_classes = 5\n    batch_size = 3\n    labels = [0, 1, 2]\n    sampled = [1, 0, 2, 3]\n    (weights, biases, hidden_acts, _, exp_logits,\n     exp_labels) = self._GenerateTestData(\n         num_classes=num_classes,\n         dim=10,\n         batch_size=batch_size,\n         num_true=1,\n         labels=labels,\n         sampled=sampled,\n         subtract_log_q=True)\n    exp_sampled_softmax_loss = _SoftmaxCrossEntropyWithLogits(\n        exp_logits, exp_labels)\n\n    true_exp_bf16 = np.full([batch_size, 1],\n                            fill_value=0.5,\n                            dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_exp_bf16 = np.full([len(sampled)],\n                               fill_value=0.5,\n                               dtype=dtypes.bfloat16.as_numpy_dtype)\n    sampled_vals_bf16 = (sampled, true_exp_bf16, sampled_exp_bf16)\n\n    got_sampled_softmax_loss = math_ops.cast(\n        nn_impl.sampled_softmax_loss_v2(\n            weights=constant_op.constant(weights, dtype=dtypes.bfloat16),\n            biases=constant_op.constant(biases, dtype=dtypes.bfloat16),\n            labels=constant_op.constant(\n                labels, shape=(batch_size, 1), dtype=dtypes.bfloat16),\n            inputs=constant_op.constant(hidden_acts, dtype=dtypes.bfloat16),\n            num_sampled=4,\n            num_classes=num_classes,\n            num_true=1,\n            sampled_values=sampled_vals_bf16,\n            remove_accidental_hits=False), dtypes.float32)\n\n    self.assertAllClose(exp_sampled_softmax_loss,\n                        self.evaluate(got_sampled_softmax_loss), 1e-1)\n\n\nclass CReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.concatenate([x * (x > 0), -x * (x < 0)], axis=1)\n\n    z = self.evaluate(nn_ops.crelu(constant_op.constant(x)))\n    self.assertAllClose(y, z, 1e-4)\n\n\nclass ReluTest(test_lib.TestCase):\n\n  def test(self):\n    np.random.seed(1)  # Make it reproducible.\n    x = np.random.randn(3, 4).astype(np.float32)\n    y = np.maximum(x, 0.0)\n\n    z = self.evaluate(nn_ops.relu(constant_op.constant(x)))\n    self.assertAllEqual(y, z)\n\n  @test_util.run_deprecated_v1\n  def testNaNs(self):\n    # Test that relu(nan) = nan for various sizes.\n    for i in range(18):\n      x = np.zeros(i) + np.nan\n      with self.cached_session():\n        z = nn_ops.relu(constant_op.constant(x)).eval()\n        self.assertTrue(np.isnan(z).all())\n\n\nclass LeakyReluTest(test_lib.TestCase):\n\n  def testRange(self):\n    batch_size = 3\n    height, width = 4, 4\n    np.random.seed(1)  # Make it reproducible.\n    inputs = np.random.uniform(size=(batch_size, height, width, 3)).astype(\n        np.float32)\n    inputs = constant_op.constant(inputs)\n\n    outputs = nn_ops.leaky_relu(inputs)\n    self.assertEquals(inputs.shape, outputs.shape)\n\n    inputs, outputs = self.evaluate([inputs, outputs])\n\n    self.assertGreaterEqual(outputs.min(), 0.0)\n    self.assertLessEqual(outputs.max(), 1.0)\n    self.assertAllClose(inputs, outputs)\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n      np_values = np.array([-2, -1, 0, 1, 2], dtype=dtype)\n      outputs = nn_ops.leaky_relu(constant_op.constant(np_values))\n\n      outputs = self.evaluate(outputs)\n\n      tol = 2e-3 if dtype == np.float16 else 1e-6\n      self.assertAllClose(\n          outputs, [-0.4, -0.2, 0.0, 1.0, 2.0], rtol=tol, atol=tol)\n\n  @test_util.run_deprecated_v1\n  def testName(self):\n    np_values = np.array([-2, -1, 0, 1, 2], dtype=np.float64)\n    outputs_with_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values),\n        name=\'test_relu_op\')\n    self.assertEqual(outputs_with_name_set.name, \'test_relu_op:0\')\n    outputs_without_name_set = nn_ops.leaky_relu(\n        constant_op.constant(np_values))\n    self.assertEqual(outputs_without_name_set.name, \'LeakyRelu:0\')\n\n\nclass SwishTest(test_lib.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testValues(self):\n    np_values = np.array(\n        [np.linspace(-10.0, 0.0, 100),\n         np.linspace(0.0, 10.0, 100)],\n        dtype=np.float32)\n    tf_values = constant_op.constant(np_values)\n    actual_tf_outputs = nn_impl.swish(tf_values)\n    expected_tf_outputs = tf_values * math_ops.sigmoid(tf_values)\n\n    actual_outputs, expected_outputs = self.evaluate(\n        [actual_tf_outputs, expected_tf_outputs])\n\n    self.assertAllClose(actual_outputs, expected_outputs)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    shape = [5, 3, 4]\n    sigma = 5\n    input_values = np.random.randn(*shape) * sigma\n    x_tf = constant_op.constant(input_values)\n    y_tf = nn_impl.swish(x_tf)\n    with self.cached_session():\n      err = gradient_checker.compute_gradient_error(x_tf, shape, y_tf, shape)\n    self.assertLess(err, 1e-4)\n\n\nclass MomentsTest(test_lib.TestCase):\n\n  def doOutputTest(self,\n                   input_shape,\n                   moments_axes,\n                   tol=1e-4,\n                   check_gradients=False):\n    for mu in [0.0, 1.0, 1e3]:\n      for sigma in [1.0, 0.1]:\n        for keep_dims in [True, False]:\n          input_values = np.random.rand(*input_shape) * sigma + mu\n          expected_mean = np.mean(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          expected_var = np.var(\n              input_values, axis=moments_axes, keepdims=keep_dims)\n          with ops.Graph().as_default() as g:\n            with self.session(graph=g) as sess:\n              inputs = constant_op.constant(\n                  input_values, shape=input_shape, dtype=dtypes.float32)\n              mean, variance = nn_impl.moments_v2(\n                  inputs, moments_axes, keepdims=keep_dims)\n\n              if check_gradients:\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, mean, mean.shape.as_list())\n                self.assertLess(err, 1e-3)\n                err = gradient_checker.compute_gradient_error(\n                    inputs, input_shape, variance, variance.shape.as_list())\n                self.assertLess(err, 1e-3)\n\n              # Evaluate.\n              [mean, variance] = self.evaluate([mean, variance])\n              # Make sure that there are no NaNs\n              self.assertFalse(np.isnan(mean).any())\n              self.assertFalse(np.isnan(variance).any())\n              self.assertAllClose(mean, expected_mean, rtol=tol, atol=tol)\n              self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)\n\n  def testOutputAndGradient2DInput0(self):\n    self.doOutputTest((10, 10), (0,), check_gradients=True)\n\n  def testOutputAndGradient2DInput01(self):\n    self.doOutputTest((10, 10), (0, 1), check_gradients=True)\n\n  def testOutput2DInput0(self):\n    self.doOutputTest((10, 300), (0,))\n\n  def testOutput2DInput1(self):\n    self.doOutputTest((10, 300), (1,))\n\n  def testOutput2DInput01(self):\n    self.doOutputTest((10, 300), (0, 1))\n\n  def testOutput4DInput0(self):\n    self.doOutputTest((10, 10, 10, 30), (0,))\n\n  def testOutput4DInput1(self):\n    self.doOutputTest((10, 10, 10, 30), (1,))\n\n  def testOutput4DInput3(self):\n    self.doOutputTest((10, 10, 10, 30), (3,))\n\n  def testOutput4DInput012(self):\n    self.doOutputTest((10, 10, 10, 30), (0, 1, 2))\n\n  def testOutput4DInput123(self):\n    self.doOutputTest((10, 10, 10, 30), (1, 2, 3))\n\n\nclass DataFormatDimMapTest(test_lib.TestCase):\n\n  def _test(self, x_val, y_val_expected):\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x)\n\n    y_val = self.evaluate(y)\n    self.assertAllEqual(y_val, y_val_expected)\n\n  def test(self):\n    self._test(0, 0)\n    self._test(1, 2)\n    self._test(2, 3)\n    self._test(3, 1)\n    self._test(-1, 1)\n    self._test(-2, 3)\n    self._test(-3, 2)\n    self._test(-4, 0)\n    self._test([1, 3], [2, 1])\n    self._test([1, 3, -2], [2, 1, 3])\n    self._test([1, -3, -2], [2, 2, 3])\n    self._test([[1, -3], [1, -1]], [[2, 2], [2, 1]])\n\n  def testNHWCtoNCHW(self):\n    x_val = [1, -3, -2]\n    y_val_expected = [2, 2, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=""NHWC"", dst_format=""NCHW"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoHWNC(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [2, 0, 1, 3, 2, 0, 1, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=""NHWC"", dst_format=""HWNC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testNHWCtoWHCN(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 1, 0, 2, 3, 1, 0, 2]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=""NHWC"", dst_format=""WHCN"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n  def testArbitraryASCII(self):\n    x_val = [-4, -3, -2, -1, 0, 1, 2, 3]\n    y_val_expected = [3, 2, 1, 0, 3, 2, 1, 0]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_dim_map(x, src_format=""qwer"", dst_format=""rewq"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, y_val_expected)\n\n\nclass DataFormatVectorPermuteTest(test_lib.TestCase):\n\n  def testNHWCToNCHW(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 3, 4, 9])\n\n  def testNCHWToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""NCHW"", dst_format=""NHWC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [7, 9, 3, 4])\n\n  def testNHWCToHWNC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""NHWC"", dst_format=""HWNC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [4, 9, 7, 3])\n\n  def testHWNCToNHWC(self):\n    x_val = [7, 4, 9, 3]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""HWNC"", dst_format=""NHWC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [9, 7, 4, 3])\n\n  def testNHWCToNCHW2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x)\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [5, 1], [9, 3], [4, 5]])\n\n  def testNHWCToHWNC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""NHWC"", dst_format=""HWNC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[9, 3], [4, 5], [7, 4], [5, 1]])\n\n  def testHWNCToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""HWNC"", dst_format=""NHWC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[4, 5], [7, 4], [9, 3], [5, 1]])\n\n  def testNCHWToNHWC2D(self):\n    x_val = [[7, 4], [9, 3], [4, 5], [5, 1]]\n    x = constant_op.constant(x_val)\n    y = nn_ops.data_format_vec_permute(x, src_format=""NCHW"", dst_format=""NHWC"")\n    with test_util.use_gpu():\n      y_val = self.evaluate(y)\n      self.assertAllEqual(y_val, [[7, 4], [4, 5], [5, 1], [9, 3]])\n\n\nif __name__ == ""__main__"":\n  test_lib.main()\n'"
test/TensorFlowNET.UnitTest/ops_test/ops_test_r1.13.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow.python.framework.ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gc\nimport os\nimport threading\nimport weakref\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import function as eager_function\nfrom tensorflow.python.framework import common_shapes\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import device as pydev\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import function\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.framework import test_ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import versions\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import resource_variable_ops\nfrom tensorflow.python.ops import resources\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.gradients  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.util import compat\n\nops._set_call_cpp_shape_fn(common_shapes.call_cpp_shape_fn)\n\n\nclass ResourceTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testBuildGraph(self):\n    with self.cached_session():\n      pt = test_ops.stub_resource_handle_op(container=""a"", shared_name=""b"")\n      test_ops.resource_create_op(pt).run()\n\n  @test_util.run_deprecated_v1\n  def testInitialize(self):\n    with self.cached_session():\n      handle = test_ops.stub_resource_handle_op(container=""a"", shared_name=""b"")\n      resources.register_resource(\n          handle=handle,\n          create_op=test_ops.resource_create_op(handle),\n          is_initialized_op=test_ops.resource_initialized_op(handle))\n      self.assertEquals(\n          len(\n              resources.report_uninitialized_resources(\n                  resources.shared_resources()).eval()), 1)\n      resources.initialize_resources(resources.shared_resources()).run()\n      self.assertEquals(\n          len(\n              resources.report_uninitialized_resources(\n                  resources.shared_resources()).eval()), 0)\n\n\nclass TensorAndShapeTest(test_util.TensorFlowTestCase):\n\n  def testShape(self):\n    op = ops.Operation(\n        ops._NodeDef(""FloatOutput"", ""myop""), ops.Graph(), [], [dtypes.float32])\n    t = op.outputs[0]\n    self.assertEqual(tensor_shape.unknown_shape(), t.get_shape())\n    t.set_shape([1, 2, 3])\n    self.assertEqual([1, 2, 3], t.get_shape())\n\n  def testIterable(self):\n    op = ops.Operation(\n        ops._NodeDef(""FloatOutput"", ""myop""), ops.Graph(), [], [dtypes.float32])\n    t = op.outputs[0]\n    self.assertTrue(isinstance(t, ops.Tensor))\n    with self.assertRaisesRegexp(TypeError, ""iter""):\n      for _ in t:\n        pass\n\n  def testAddShape(self):\n    with self.cached_session():\n      a = array_ops.zeros([2, 3])\n      b = array_ops.ones([1, 3])\n      c = a + b\n      self.assertEqual([2, 3], c.shape)\n\n  @test_util.run_deprecated_v1\n  def testUnknownDim(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtype=dtypes.float32, shape=[2, None, 3])\n      b = array_ops.placeholder(dtype=dtypes.float32, shape=[2, None, 3])\n      c = a + b\n      self.assertEqual([2, None, 3], c.shape.as_list())\n\n  @test_util.run_deprecated_v1\n  def testUnknownShape(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtype=dtypes.float32, shape=None)\n      b = array_ops.ones([1, 3])\n      c = a + b\n      self.assertEqual(tensor_shape.unknown_shape(), c.shape)\n\n  @test_util.run_deprecated_v1\n  def testScalarShape(self):\n    with self.cached_session():\n      a = array_ops.placeholder(dtype=dtypes.float32, shape=[])\n      b = array_ops.ones([])\n      c = a + b\n      self.assertEqual(tensor_shape.scalar(), c.shape)\n\n  @test_util.run_deprecated_v1\n  def testShapeFunctionError(self):\n    with self.cached_session():\n      a = array_ops.ones([1, 2, 3])\n      b = array_ops.ones([4, 5, 6])\n      with self.assertRaisesRegexp(\n          ValueError,\n          r""Dimensions must be equal, but are 2 and 5 for \'add\' \\(op: \'Add\'\\) ""\n          r""with input shapes: \\[1,2,3\\], \\[4,5,6\\].""):\n        _ = a + b\n\n\nclass IndexedSlicesTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testToTensor(self):\n    values = constant_op.constant([2, 3, 5, 7], shape=[2, 2])\n    indices = constant_op.constant([0, 2])\n    dense_shape = constant_op.constant([3, 2])\n    x = ops.IndexedSlices(values, indices, dense_shape)\n    tensor = ops.convert_to_tensor(x, name=""tensor"")\n    self.assertAllEqual(self.evaluate(tensor), [[2, 3], [0, 0], [5, 7]])\n\n  @test_util.run_deprecated_v1\n  def testNegation(self):\n    with self.cached_session():\n      values = constant_op.constant([2, 3, 5, 7], shape=[2, 2])\n      indices = constant_op.constant([0, 2])\n      x = -ops.IndexedSlices(values, indices)\n      self.assertAllEqual(x.values.eval(), [[-2, -3], [-5, -7]])\n      self.assertAllEqual(x.indices.eval(), [0, 2])\n\n  @test_util.run_deprecated_v1\n  def testScalarMul(self):\n    with self.cached_session():\n      values = constant_op.constant([2, 3, 5, 7], shape=[2, 2])\n      indices = constant_op.constant([0, 2])\n      x = math_ops.scalar_mul(-2, ops.IndexedSlices(values, indices))\n      self.assertAllEqual(x.values.eval(), [[-4, -6], [-10, -14]])\n      self.assertAllEqual(x.indices.eval(), [0, 2])\n\n\nclass NodeDefConstructorTest(test_util.TensorFlowTestCase):\n\n  def testNoArgs(self):\n    nodedef = ops._NodeDef(""None"", ""bar"")\n    self.assertProtoEquals(""op: \'None\' name: \'bar\'"", nodedef)\n\n  def testArgs(self):\n    nodedef = ops._NodeDef(""foo"", ""bar"", device=""/device:baz:*"")\n    self.assertProtoEquals(""op:\'foo\' name:\'bar\' device:\'/device:baz:*\'"",\n                           nodedef)\n    nodedef = ops._NodeDef(""foo"", ""bar"", device=pydev.DeviceSpec(job=""j""))\n    self.assertProtoEquals(""op:\'foo\' name:\'bar\' device:\'/job:j\'"", nodedef)\n\n\ndef _apply_op(g, *args, **kwargs):\n  op = g.create_op(*args, **kwargs)\n  if len(op.outputs) == 1:\n    return op.outputs[0]\n  else:\n    return op.outputs\n\n\nclass OperationTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testNoInputs(self):\n    op = test_ops.float_output_string_output(name=""myop"").a.op\n    self.assertEqual(2, len(op.values()))\n    self.assertEqual(0, len(op.inputs))\n    self.assertEqual(""myop"", op.name)\n\n    float_t, label_str_t = op.values()\n    self.assertEqual(dtypes.float32, float_t.dtype)\n    self.assertEqual(op, float_t.op)\n    self.assertEqual(0, float_t._value_index)\n    self.assertEqual(0, len(float_t.consumers()))\n    self.assertEqual(""myop"", float_t._as_node_def_input())\n\n    self.assertEqual(dtypes.string, label_str_t.dtype)\n    self.assertEqual(op, label_str_t.op)\n    self.assertEqual(1, label_str_t._value_index)\n    self.assertEqual(0, len(label_str_t.consumers()))\n    self.assertEqual(""myop:1"", label_str_t._as_node_def_input())\n\n    self.assertProtoEquals(""op:\'FloatOutputStringOutput\' name:\'myop\'"",\n                           op.node_def)\n\n  @test_util.run_deprecated_v1\n  def testNoOutputs(self):\n    op1 = test_ops.float_output(name=""myop1"").op\n    float_t, = op1.values()\n    op2 = test_ops.float_input(float_t, name=""myop2"")\n    self.assertEqual(0, len(op2.values()))\n    self.assertEqual(1, len(op2.inputs))\n    self.assertIs(float_t, op2.inputs[0])\n\n    self.assertEqual(1, len(float_t.consumers()))\n    self.assertEqual(op2, float_t.consumers()[0])\n\n    self.assertProtoEquals(""op:\'FloatOutput\' name:\'myop1\'"", op1.node_def)\n    self.assertProtoEquals(""op:\'FloatInput\' name:\'myop2\' input:\'myop1\'"",\n                           op2.node_def)\n\n  @test_util.run_deprecated_v1\n  def testInputsAndOutputs(self):\n    op1 = test_ops.float_output(name=""myop1"").op\n    self.assertEqual(1, len(op1.values()))\n    float1_t, = op1.values()\n\n    op2 = test_ops.float_output_string_output(name=""myop2"").a.op\n    self.assertEqual(2, len(op2.values()))\n    float2_t, label2_str_t = op2.values()\n\n    # Note that we consume label2_str_t twice here.\n    op3 = test_ops.foo2(float1_t, label2_str_t, label2_str_t, name=""myop3"").d.op\n    self.assertEqual(2, len(op3.values()))\n\n    self.assertEqual(1, len(float1_t.consumers()))\n    self.assertEqual(op3, float1_t.consumers()[0])\n\n    self.assertEqual(0, len(float2_t.consumers()))\n\n    self.assertEqual(2, len(label2_str_t.consumers()))\n    self.assertEqual(op3, label2_str_t.consumers()[0])\n    self.assertEqual(op3, label2_str_t.consumers()[1])\n\n    self.assertProtoEquals(""""""\n    op:\'Foo2\' name:\'myop3\'\n    input:\'myop1\' input:\'myop2:1\' input:\'myop2:1\'\n    """""", op3.node_def)\n\n  def testDeviceFromNodeDef(self):\n    op = ops.Operation(\n        ops._NodeDef(""None"", ""myop"", device=""/job:goo/device:GPU:0""),\n        ops.Graph(), [], [])\n    self.assertEqual(""/job:goo/device:GPU:0"", op.device)\n\n  def testDeviceObject(self):\n    op = ops.Operation(ops._NodeDef(""None"", ""myop""), ops.Graph(), [], [])\n    op._set_device(""/job:goo/device:GPU:0"")\n    self.assertProtoEquals(\n        ""op:\'None\' name:\'myop\' device:\'/job:goo/device:GPU:0\' "", op.node_def)\n    op = ops.Operation(ops._NodeDef(""None"", ""op2""), ops.Graph(), [], [])\n    op._set_device(\n        pydev.DeviceSpec(\n            job=""muu"", device_type=""CPU"", device_index=0))\n    self.assertProtoEquals(\n        ""op:\'None\' name:\'op2\' device:\'/job:muu/device:CPU:0\'"", op.node_def)\n\n  def testReferenceInput(self):\n    g = ops.Graph()\n    op1 = ops.Operation(\n        ops._NodeDef(""RefOutputFloatOutput"", ""op1""), g, [],\n        [dtypes.float32_ref, dtypes.float32])\n    self.assertProtoEquals(""op:\'RefOutputFloatOutput\' name:\'op1\'"", op1.node_def)\n    self.assertEquals([], list(op1.inputs))\n    ref_t, nonref_t = op1.values()\n    # NOTE(mrry): Must specify input_types to preserve ref-typed input.\n    op2 = ops.Operation(\n        ops._NodeDef(""RefInputFloatInput"", ""op2""),\n        g, [ref_t, nonref_t], [],\n        input_types=[dtypes.float32_ref, dtypes.float32])\n    self.assertProtoEquals(\n        ""op:\'RefInputFloatInput\' name:\'op2\' input:\'op1\' input:\'op1:1\'"",\n        op2.node_def)\n    self.assertEquals([ref_t, nonref_t], list(op2.inputs))\n    op3 = ops.Operation(\n        ops._NodeDef(""TwoFloatInputs"", ""op3""), g, [ref_t, nonref_t], [])\n    self.assertProtoEquals(\n        ""op:\'TwoFloatInputs\' name:\'op3\' input:\'op1\' input:\'op1:1\'"",\n        op3.node_def)\n\n  def testInvalidNames(self):\n    g = ops.Graph()\n    with self.assertRaises(ValueError):\n      ops.Operation(ops._NodeDef(""op"", """"), g)\n    with self.assertRaises(ValueError):\n      ops.Operation(ops._NodeDef(""op"", ""_invalid""), g)\n    with self.assertRaises(ValueError):\n      ops.Operation(ops._NodeDef(""op"", ""-invalid""), g)\n    with self.assertRaises(ValueError):\n      ops.Operation(ops._NodeDef(""op"", ""/invalid""), g)\n    with self.assertRaises(ValueError):\n      ops.Operation(ops._NodeDef(""op"", ""invalid:0""), g)\n\n  @test_util.run_deprecated_v1\n  def testNoShapeFunction(self):\n    op = test_ops.a()\n    self.assertEqual(tensor_shape.unknown_shape(), op.get_shape())\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorNestedArray(self):\n    values = [[2], [3], [5], [7]]\n    tensor = ops.convert_to_tensor(values)\n    self.assertAllEqual((4, 1), tensor.get_shape().as_list())\n    self.assertAllEqual(values, self.evaluate(tensor))\n\n  def testShapeTuple(self):\n    with self.cached_session():\n      c = constant_op.constant(1)\n      self.assertEqual(c._shape_tuple(), ())  # pylint: disable=protected-access\n\n  def testConvertToTensorEager(self):\n    with context.eager_mode():\n      t = constant_op.constant(1)\n      self.assertTrue(isinstance(t, ops.EagerTensor))\n      converted = ops.convert_to_tensor(t)\n      self.assertTrue(isinstance(converted, ops.EagerTensor))\n      converted = ops.convert_to_tensor(1)\n      self.assertTrue(isinstance(converted, ops.EagerTensor))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorNestedTuple(self):\n    values = ((2,), (3,), (5,), (7,))\n    tensor = ops.convert_to_tensor(values)\n    self.assertAllEqual((4, 1), tensor.get_shape().as_list())\n    self.assertAllEqual(values, self.evaluate(ops.convert_to_tensor(values)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorNestedTensors(self):\n    values = ((2,), (3,), (5,), (7,))\n    tensor = ops.convert_to_tensor(\n        [constant_op.constant(row) for row in values])\n    self.assertAllEqual((4, 1), tensor.get_shape().as_list())\n    self.assertAllEqual(values, self.evaluate(tensor))\n    tensor = ops.convert_to_tensor(\n        [[constant_op.constant(v) for v in row] for row in values])\n    self.assertAllEqual((4, 1), tensor.get_shape().as_list())\n    self.assertAllEqual(values, self.evaluate(tensor))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorNestedMix(self):\n    values = ([2], (3,), [constant_op.constant(5)], constant_op.constant([7]))\n    tensor = ops.convert_to_tensor(values)\n    self.assertAllEqual((4, 1), tensor.get_shape().as_list())\n    self.assertAllEqual(((2,), (3,), (5,), (7,)), self.evaluate(tensor))\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorPreferred(self):\n    values = [2, 3, 5, 7]\n    tensor = ops.convert_to_tensor(values, preferred_dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, tensor.dtype)\n\n    # Convert empty tensor to anything.\n    values = []\n    tensor = ops.convert_to_tensor(values, preferred_dtype=dtypes.int64)\n    self.assertEqual(dtypes.int64, tensor.dtype)\n\n    # The preferred dtype is a type error and will convert to\n    # float32 instead.\n    values = [1.23]\n    tensor = ops.convert_to_tensor(values, preferred_dtype=dtypes.int64)\n    self.assertEqual(dtypes.float32, tensor.dtype)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToInvalidTensorType(self):\n    with self.assertRaises(TypeError):\n      # Forcing an invalid dtype should fail with a type error.\n      values = [1.23]\n      ops.convert_to_tensor(values, dtype=dtypes.int64)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testConvertToTensorFromInvalidTensor(self):\n    tensor = constant_op.constant(42.0, dtype=dtypes.float32)\n    with self.assertRaises(ValueError):\n      ops.convert_to_tensor(tensor, dtype=dtypes.int32)\n\n  @test_util.run_deprecated_v1\n  def testNoConvert(self):\n    # Operation cannot be converted to Tensor.\n    op = control_flow_ops.no_op()\n    with self.assertRaisesRegexp(TypeError,\n                                 r""Can\'t convert Operation \'.*\' to Tensor""):\n      ops.convert_to_tensor(op)\n\n  def testStr(self):\n    node_def = ops._NodeDef(""None"", ""op1"")\n    op = ops.Operation(node_def, ops.Graph(), [], [dtypes.float32])\n    self.assertEqual(str(node_def), str(op))\n\n  def testRepr(self):\n    op = ops.Operation(\n        ops._NodeDef(""None"", ""op1""), ops.Graph(), [], [dtypes.float32])\n    self.assertEqual(""<tf.Operation \'op1\' type=None>"", repr(op))\n\n  @test_util.run_deprecated_v1\n  def testGetAttr(self):\n    op = test_ops.default_attrs()\n    self.assertEqual(op.get_attr(""string_val""), b""abc"")\n    self.assertEqual(op.get_attr(""string_list_val""), [b""abc"", b""""])\n    self.assertEqual(op.get_attr(""int_val""), 123)\n    self.assertEqual(op.get_attr(""int_list_val""), [1, 2, 3])\n    self.assertEqual(op.get_attr(""float_val""), 10.0)\n    self.assertEqual(op.get_attr(""float_list_val""), [10.0])\n    self.assertEqual(op.get_attr(""bool_val""), True)\n    self.assertEqual(op.get_attr(""bool_list_val""), [True, False])\n    self.assertEqual(op.get_attr(""shape_val""),\n                     tensor_shape.as_shape([2, 1]).as_proto())\n    self.assertEqual(op.get_attr(""shape_list_val""),\n                     [tensor_shape.as_shape([]).as_proto(),\n                      tensor_shape.as_shape([1]).as_proto()])\n    self.assertEqual(op.get_attr(""tensor_val""),\n                     tensor_util.make_tensor_proto(1, dtypes.int32))\n    self.assertEqual(op.get_attr(""tensor_list_val""),\n                     [tensor_util.make_tensor_proto(1, dtypes.int32)])\n\n    type_val = op.get_attr(""type_val"")\n    # First check that type_val is a DType, because the assertEquals will work\n    # no matter what since DType overrides __eq__\n    self.assertIsInstance(type_val, dtypes.DType)\n    self.assertEqual(type_val, dtypes.int32)\n\n    type_list_val = op.get_attr(""type_list_val"")\n    self.assertTrue(all(isinstance(x, dtypes.DType) for x in type_list_val))\n    self.assertEqual(type_list_val, [dtypes.int32, dtypes.float32])\n\n    @function.Defun(dtypes.float32, func_name=""MyFunc"")\n    def func(x):\n      return x\n\n    op = test_ops.func_attr(func)\n    self.assertEqual(op.get_attr(""f""),\n                     attr_value_pb2.NameAttrList(name=""MyFunc""))\n\n    # Try fetching missing attr\n    with self.assertRaisesRegexp(\n        ValueError, ""Operation \'FuncAttr\' has no attr named \'FakeAttr\'.""):\n      op.get_attr(""FakeAttr"")\n\n  # TODO(b/65162920): remove this test when users who are directly mutating the\n  # node_def have been updated to proper usage.\n  @test_util.run_deprecated_v1\n  def testSetAttr(self):\n    op = test_ops.int_attr().op\n    op._set_attr(""foo"", attr_value_pb2.AttrValue(i=2))\n    # TODO(skyewm): add node_def check\n    self.assertEqual(op.get_attr(""foo""), 2)\n\n  # TODO(nolivia): test all error cases\n  def testAddControlInput(self):\n    with ops.Graph().as_default():\n      x = constant_op.constant(1).op\n      y = constant_op.constant(2).op\n      z = constant_op.constant(3).op\n    z._add_control_input(x)  # pylint: disable=protected-access\n    self.assertEqual(z.control_inputs, [x])\n    z._add_control_input(x)  # pylint: disable=protected-access\n    self.assertEqual(z.control_inputs, [x])\n    z._add_control_inputs([x, y, y])  # pylint: disable=protected-access\n    self.assertEqual(z.control_inputs, [x, y])\n    self.assertEqual(x._control_outputs, [z])\n\n  @test_util.run_deprecated_v1\n  def testRemoveAllControlInputs(self):\n    a = constant_op.constant(1)\n    with ops.control_dependencies([a]):\n      b = constant_op.constant(2)\n    c = constant_op.constant(3)\n    d = constant_op.constant(4)\n    e = constant_op.constant(5)\n    with ops.control_dependencies([a, c]):\n      f = d + e\n\n    self.assertEqual(a.op.control_inputs, [])\n    self.assertEqual(b.op.control_inputs, [a.op])\n    self.assertEqual(f.op.control_inputs, [a.op, c.op])\n\n    a.op._remove_all_control_inputs()  # pylint: disable=protected-access\n    self.assertEqual(a.op.control_inputs, [])\n\n    b.op._remove_all_control_inputs()  # pylint: disable=protected-access\n    self.assertEqual(b.op.control_inputs, [])\n\n    f.op._remove_all_control_inputs()  # pylint: disable=protected-access\n    self.assertEqual(f.op.control_inputs, [])\n    self.assertEqual(list(f.op.inputs), [d, e])\n\n  @test_util.run_deprecated_v1\n  def testControlInputCycle(self):\n    graph = ops.Graph()\n    with graph.as_default():\n      z = constant_op.constant(0)\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      y.op._add_control_input(z.op)  # pylint: disable=protected-access\n      y.op._add_control_input(x.op)  # pylint: disable=protected-access\n      x.op._add_control_input(y.op)  # pylint: disable=protected-access\n    with self.session(graph=graph) as sess:\n      with self.assertRaisesRegexp(\n          errors.InvalidArgumentError,\n          ""Graph is invalid, contains a cycle with 2 nodes""):\n        self.evaluate(x)\n\n  def testUpdateInput(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = constant_op.constant(1)\n      y = constant_op.constant(2)\n      z = x + y\n\n    z.op._update_input(0, y)  # pylint: disable=protected-access\n    self.assertEquals(list(z.op.inputs), [y, y])\n    self.assertEquals(x.consumers(), [])\n    self.assertEquals(y.consumers(), [z.op, z.op])\n    with session.Session(graph=g) as sess:\n      self.assertEquals(self.evaluate(z), 4)\n\n    z.op._update_input(0, x)  # pylint: disable=protected-access\n    self.assertEquals(list(z.op.inputs), [x, y])\n    self.assertEquals(x.consumers(), [z.op])\n    self.assertEquals(y.consumers(), [z.op])\n    with session.Session(graph=g) as sess:\n      self.assertEquals(self.evaluate(z), 3)\n\n    z.op._update_input(1, y)  # pylint: disable=protected-access\n    self.assertEquals(list(z.op.inputs), [x, y])\n    self.assertEquals(x.consumers(), [z.op])\n    self.assertEquals(y.consumers(), [z.op])\n    with session.Session(graph=g) as sess:\n      self.assertEquals(self.evaluate(z), 3)\n\n  def testUpdateInputGraphError(self):\n    g_0 = ops.Graph()\n    g_1 = ops.Graph()\n    with g_0.as_default():\n      x = constant_op.constant(1)\n    with g_1.as_default():\n      y = constant_op.constant(2)\n      z = y * 2\n      with self.assertRaisesRegexp(ValueError, ""must be from the same graph""):\n        z.op._update_input(0, x)  # pylint: disable=protected-access\n\n  def testUpdateInputTypeError(self):\n    g = ops.Graph()\n    with g.as_default():\n      w = constant_op.constant(0)\n      x = constant_op.constant("""")\n      y = constant_op.constant(1)\n      z = y + w\n      z.op._update_input(0, x)  # pylint: disable=protected-access\n    with session.Session(graph=g) as sess:\n      with self.assertRaisesRegexp(\n          errors.InvalidArgumentError,\n          ""Input 0 of node add was passed string from Const_1:0 incompatible ""\n          ""with expected int32""):\n        self.evaluate(z)\n\n  def testUpdateInputShapeError(self):\n    g = ops.Graph()\n    with g.as_default():\n      w = constant_op.constant(2, shape=[3, 1])\n      x = constant_op.constant(0, shape=[3, 1])\n      y = constant_op.constant(1, shape=[2, 2])\n      z = w + x\n    with self.assertRaisesRegexp(\n        errors.InvalidArgumentError,\n        r""Cannot update edge, incompatible shapes: \\[2,2\\] and \\[3,1\\]""):\n      z.op._update_input(0, y)  # pylint: disable=protected-access\n\n  def testUpdateInputOutOfRange(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = constant_op.constant(1)\n    with self.assertRaisesRegexp(\n        errors.OutOfRangeError,\n        r""Cannot update edge. Input index \\[1\\] is greater than the number of ""\n        r""total inputs \\[0\\].""\n    ):\n      x.op._update_input(1, x)  # pylint: disable=protected-access\n\n  @test_util.enable_control_flow_v2\n  @test_util.run_v1_only(""b/120545219"")\n  def testAddWhileInput(self):\n    @eager_function.defun\n    def test():\n      output = control_flow_ops.while_loop(lambda x: x < 3, lambda x: x + 1,\n                                           [1])\n      while_op = output.op.inputs[0].op\n      self.assertEqual(while_op.type, ""While"")\n      orig_num_inputs = len(while_op.inputs)\n\n      # Make sure we can handle the while op having a control input.\n      while_op._add_control_input(constant_op.constant(0).op)\n\n      new_input1 = constant_op.constant(1.0)\n      new_input2 = constant_op.constant(True)\n\n      while_op._set_type_list_attr(""T"",\n                                   [t.dtype for t in while_op.inputs] +\n                                   [new_input1.dtype, new_input2.dtype])\n\n      while_op._add_while_inputs([new_input1, new_input2])\n      # Can\'t add an edge beyond what\'s specified by ""T""\n      with self.assertRaises(errors.OutOfRangeError):\n        while_op._add_while_inputs([new_input2])\n      self.assertEqual(len(while_op.inputs), orig_num_inputs + 2)  # pylint: disable=g-deprecated-assert\n\n    test()\n\n  @test_util.run_deprecated_v1\n  def testOpDef(self):\n    x = constant_op.constant(0)\n    y = constant_op.constant(1)\n    z = x + y\n\n    self.assertEqual(x.op.op_def.name, ""Const"")\n    self.assertEqual(len(x.op.op_def.input_arg), 0)\n    self.assertEqual(len(x.op.op_def.output_arg), 1)\n\n    self.assertEqual(z.op.op_def.name, ""Add"")\n    self.assertEqual(len(z.op.op_def.input_arg), 2)\n    self.assertEqual(len(z.op.op_def.output_arg), 1)\n\n  def testInputFromDifferentGraphError(self):\n    g_0 = ops.Graph()\n    g_1 = ops.Graph()\n    with g_0.as_default():\n      x = constant_op.constant(1)\n    with g_1.as_default():\n      y = constant_op.constant(2)\n      with self.assertRaisesRegexp(ValueError, ""must be from the same graph""):\n        y * x  # pylint: disable=pointless-statement\n\n  def testInputsAreImmutable(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n      op = test_ops.int_input_int_output(x, name=""myop"").op\n    with self.assertRaisesRegexp(\n        AttributeError, ""\'_InputList\' object has no attribute \'append\'""):\n      op.inputs.append(None)\n\n\nclass CreateOpTest(test_util.TensorFlowTestCase):\n\n  def testNodeDefArgs(self):\n    g = ops.Graph()\n    op1 = g.create_op(""FloatOutput"", [], [dtypes.float32], None, name=""myop1"")\n    with g.device(""/device:GPU:0""):\n      op2 = g.create_op(\n          ""FloatOutputStringOutput"", [], [dtypes.float32, dtypes.string], None,\n          name=""myop2"")\n    op3 = g.create_op(\n        ""Foo3"",\n        [list(op1.values())[0], list(op2.values())[1], list(op2.values())[0]],\n        [dtypes.float32, dtypes.int32],\n        None,\n        name=""myop3"")\n    self.assertDeviceEqual(None, op1.device)\n    self.assertDeviceEqual(""/device:GPU:0"", op2.device)\n    self.assertDeviceEqual(None, op3.device)\n    self.assertProtoEquals(""name:\'myop1\' op:\'FloatOutput\'"", op1.node_def)\n    self.assertProtoEquals(\n        ""name:\'myop2\' op:\'FloatOutputStringOutput\' device:\'/device:GPU:0\'"",\n        op2.node_def)\n    self.assertProtoEquals(\n        ""name:\'myop3\' input:\'myop1\' input:\'myop2:1\' input:\'myop2\' op:\'Foo3\'"",\n        op3.node_def)\n\n  def testReferenceInput(self):\n    g = ops.Graph()\n    op1 = g.create_op(\n        ""RefOutputFloatOutput"", [], [dtypes.float32_ref, dtypes.float32],\n        name=""op1"")\n    self.assertProtoEquals(""op:\'RefOutputFloatOutput\' name:\'op1\'"", op1.node_def)\n    ref_t, nonref_t = op1.values()\n    # NOTE(mrry): Must specify input_types to preserve ref-typed input.\n    op2 = g.create_op(\n        ""RefInputFloatInput"", [ref_t, nonref_t], [],\n        input_types=[dtypes.float32_ref, dtypes.float32],\n        name=""op2"")\n    self.assertProtoEquals(\n        ""op:\'RefInputFloatInput\' name:\'op2\' input:\'op1\' input:\'op1:1\'"",\n        op2.node_def)\n    op3 = g.create_op(""TwoFloatInputs"", [ref_t, nonref_t], [], name=""op3"")\n    self.assertProtoEquals(\n        ""op:\'TwoFloatInputs\' name:\'op3\' input:\'op1\' input:\'op1:1\'"",\n        op3.node_def)\n\n  def testFinalized(self):\n    g = ops.Graph()\n    g.finalize()\n    with self.assertRaises(RuntimeError):\n      g.create_op(""FloatOutput"", [], [dtypes.float32], None, name=""myop1"")\n\n    # Test unfinalize.\n    g._unsafe_unfinalize()\n    g.create_op(""FloatOutput"", [], [dtypes.float32], None, name=""myop1"")\n\n\n# NOTE(skyewm): these cases test the private Graph._create_op_from_tf_operation\n# method. Arguably we should only test the public APIs that depend on this\n# method. However, this logic is complex and tricky, and it can be difficult to\n# ascertain if we have adequate coverage (e.g. a graph may run successfully if\n# the control flow context isn\'t set properly, but a more complicated use case\n# that might not be obvious to test will fail). Thus we instead explicitly test\n# the low-level behavior.\nclass CreateOpFromTFOperationTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testBasic(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n      c_op = ops._create_c_op(\n          g, ops._NodeDef(""IntInputIntOutput"", ""myop""), [x], [])\n      op = g._create_op_from_tf_operation(c_op)\n\n    self.assertEqual(op.name, ""myop"")\n    self.assertEqual(op.type, ""IntInputIntOutput"")\n    self.assertEqual(len(op.outputs), 1)\n    self.assertEqual(op.outputs[0].shape, tensor_shape.unknown_shape())\n    self.assertEqual(list(op.inputs), [x])\n    self.assertEqual(op.control_inputs, [])\n    self.assertEqual(op.graph, g)\n    self.assertEqual(x.consumers(), [op])\n    self.assertIsNotNone(op.traceback)\n    self.assertEqual(g.get_operation_by_name(""myop""), op)\n    self.assertEqual(g.get_tensor_by_name(""myop:0""), op.outputs[0])\n\n  def testShape(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = constant_op.constant([[1, 2, 3], [4, 5, 6]])\n      c_op = ops._create_c_op(g, ops._NodeDef(""Identity"", ""myop""), [x], [])\n      op = g._create_op_from_tf_operation(c_op)\n\n    self.assertEqual(op.name, ""myop"")\n    self.assertEqual(op.type, ""Identity"")\n    self.assertEqual(len(op.outputs), 1)\n    self.assertEqual(op.outputs[0].shape, tensor_shape.matrix(2, 3))\n\n  def testUniqueName(self):\n    g = ops.Graph()\n    with g.as_default():\n      c_op = ops._create_c_op(g, ops._NodeDef(""IntOutput"", ""myop""), [], [])\n      c_op2 = ops._create_c_op(g, ops._NodeDef(""IntOutput"", ""myop_1""), [], [])\n      op = g._create_op_from_tf_operation(c_op)\n      op2 = g._create_op_from_tf_operation(c_op2)\n\n      # Create ops with same names as op1 and op2. We expect the new names to be\n      # uniquified.\n      op3 = test_ops.int_output(name=""myop"").op\n      op4 = test_ops.int_output(name=""myop_1"").op\n\n    self.assertEqual(op.name, ""myop"")\n    self.assertEqual(op2.name, ""myop_1"")\n    self.assertEqual(op3.name, ""myop_2"")\n    self.assertEqual(op4.name, ""myop_1_1"")\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testCond(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n\n      def true_fn():\n        ops._create_c_op(ops.get_default_graph(),\n                         ops._NodeDef(""IntInput"", ""cond/myop""), [x], [])\n        new_ops = g._add_new_tf_operations()\n        self.assertEqual(len(new_ops), 1)\n        return x\n\n      control_flow_ops.cond(x < 10, true_fn, lambda: x)\n\n    op = g.get_operation_by_name(""cond/myop"")\n    self.assertIsNotNone(op)\n    self.assertEqual(op.name, ""cond/myop"")\n    self.assertEqual(op.type, ""IntInput"")\n    self.assertEqual(op.outputs, [])\n    op_input = op.inputs[0].op\n    self.assertEqual(op_input.type, ""Switch"")\n    self.assertEqual(op_input.inputs[0], x)\n    self.assertEqual(op.graph, g)\n    # pylint: disable=protected-access\n    self.assertIsNotNone(op._get_control_flow_context())\n    self.assertEqual(op._get_control_flow_context().name,\n                     ""cond/cond_text"")\n    # pylint: enable=protected-access\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testWhileLoop(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n\n      def body(i):\n        ops._create_c_op(ops.get_default_graph(),\n                         ops._NodeDef(""IntInput"", ""myloop/myop""), [x], [])\n        new_ops = g._add_new_tf_operations()\n        self.assertEqual(len(new_ops), 1)\n        return i\n\n      control_flow_ops.while_loop(lambda i: i < 10, body, [0], name=""myloop"")\n\n    op = g.get_operation_by_name(""myloop/myop"")\n    self.assertIsNotNone(op)\n    self.assertEqual(op.name, ""myloop/myop"")\n    self.assertEqual(op.type, ""IntInput"")\n    self.assertEqual(op.outputs, [])\n    op_input = op.inputs[0].op\n    self.assertEqual(op_input.type, ""Enter"")\n    self.assertEqual(list(op_input.inputs), [x])\n    self.assertEqual(op.graph, g)\n    # pylint: disable=protected-access\n    self.assertIsNotNone(op._get_control_flow_context())\n    self.assertEqual(op._get_control_flow_context().name,\n                     ""myloop/while_context"")\n    # pylint: enable=protected-access\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testWhileLoopWithInternalControlDep(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n\n      def body(i):\n        c = constant_op.constant(1.0, name=""c"")\n        ops._create_c_op(ops.get_default_graph(),\n                         ops._NodeDef(""IntInput"", ""myloop/myop""), [x], [])\n        with ops.control_dependencies([c]):\n          new_ops = g._add_new_tf_operations()\n          self.assertEqual(len(new_ops), 1)\n        return i\n\n      control_flow_ops.while_loop(lambda i: i < 10, body, [0], name=""myloop"")\n\n    op = g.get_operation_by_name(""myloop/myop"")\n    self.assertIsNotNone(op)\n    c = g.get_operation_by_name(""myloop/c"")\n    self.assertIsNotNone(c)\n    # Internal control dep is preserved\n    self.assertEqual(op.control_inputs, [c])\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testWhileLoopWithExternalControlDep(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.int_output()\n      c = constant_op.constant(1.0)\n\n      def body(i):\n        ops._create_c_op(ops.get_default_graph(),\n                         ops._NodeDef(""IntInput"", ""myloop/myop""), [x], [])\n        with ops.control_dependencies([c]):\n          new_ops = g._add_new_tf_operations()\n          self.assertEqual(len(new_ops), 1)\n        return i\n\n      control_flow_ops.while_loop(lambda i: i < 10, body, [0], name=""myloop"")\n\n    op = g.get_operation_by_name(""myloop/myop"")\n    self.assertIsNotNone(op)\n    # External control dep is removed and replaced with internal control dep\n    self.assertNotEqual(op.control_inputs[0], c.op)\n    self.assertIsNotNone(op.control_inputs[0]._get_control_flow_context())\n\n\nclass ApplyOpTest(test_util.TensorFlowTestCase):\n\n  def testNodeDefArgs(self):\n    g = ops.Graph()\n    t1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32], name=""myop1"")\n    with g.device(""/device:GPU:0""):\n      t2 = _apply_op(\n          g, ""TwoIntOutputs"", [], [dtypes.int32, dtypes.int32], name=""myop2"")\n    t3 = _apply_op(\n        g,\n        ""Foo1"", [t1, t2[1], t2[0]], [dtypes.float32, dtypes.int32],\n        name=""myop3"")\n    self.assertTrue(isinstance(t1, ops.Tensor))\n    self.assertTrue(isinstance(t2, list))\n    self.assertTrue(isinstance(t3, list))\n    self.assertTrue(isinstance(t3[0], ops.Tensor))\n    self.assertEqual(""myop1"", t1._as_node_def_input())\n    self.assertEqual(""myop2"", t2[0]._as_node_def_input())\n    self.assertEqual(""myop2:1"", t2[1]._as_node_def_input())\n    self.assertEqual(""myop3"", t3[0]._as_node_def_input())\n    # Validate that we got the right ops as well\n    self.assertProtoEquals(""name:\'myop1\' op:\'FloatOutput\'"", t1.op.node_def)\n    self.assertProtoEquals(\n        ""name:\'myop2\' op:\'TwoIntOutputs\' device:\'/device:GPU:0\'"",\n        t2[0].op.node_def)\n    self.assertProtoEquals(\n        ""name:\'myop3\' input:\'myop1\' input:\'myop2:1\' input:\'myop2\' op:\'Foo1\'"",\n        t3[0].op.node_def)\n\n  def testReferenceInput(self):\n    g = ops.Graph()\n    ref_t, nonref_t = _apply_op(\n        g, ""RefOutputFloatOutput"", [], [dtypes.float32_ref, dtypes.float32],\n        name=""op1"")\n    self.assertProtoEquals(""op:\'RefOutputFloatOutput\' name:\'op1\'"",\n                           ref_t.op.node_def)\n    # NOTE(mrry): Must specify input_types to preserve ref-typed input.\n    out_2 = _apply_op(\n        g,\n        ""RefInputFloatInputIntOutput"", [ref_t, nonref_t], [dtypes.int32],\n        input_types=[dtypes.float32_ref, dtypes.float32],\n        name=""op2"")\n    self.assertProtoEquals(\n        ""op:\'RefInputFloatInputIntOutput\' name:\'op2\' input:\'op1\' input:\'op1:1\'"",\n        out_2.op.node_def)\n    out_3 = _apply_op(\n        g, ""TwoFloatInputsIntOutput"", [ref_t, nonref_t], [dtypes.int32],\n        name=""op3"")\n    self.assertProtoEquals(\n        ""op:\'TwoFloatInputsIntOutput\' name:\'op3\' input:\'op1\' input:\'op1:1\'"",\n        out_3.op.node_def)\n\n\nclass NameStackTest(test_util.TensorFlowTestCase):\n\n  def testBasics(self):\n    g = ops.Graph()\n    self.assertEqual(""foo"", g.unique_name(""foo"", mark_as_used=False))\n    self.assertEqual(""foo"", g.unique_name(""foo"", mark_as_used=False))\n    self.assertEqual(""foo"", g.unique_name(""foo""))\n    self.assertEqual(""foo_1"", g.unique_name(""foo"", mark_as_used=False))\n    self.assertEqual(""foo_1"", g.unique_name(""foo""))\n    self.assertEqual(""foo_2"", g.unique_name(""foo"", mark_as_used=False))\n    self.assertEqual(""foo_2"", g.unique_name(""foo""))\n    self.assertEqual(""foo_1_1"", g.unique_name(""foo_1"", mark_as_used=False))\n    self.assertEqual(""foo_1_1"", g.unique_name(""foo_1""))\n    self.assertEqual(""foo_1_2"", g.unique_name(""foo_1"", mark_as_used=False))\n    self.assertEqual(""foo_1_2"", g.unique_name(""foo_1""))\n    self.assertEqual(""foo_1_2_1"", g.unique_name(""foo_1_2"", mark_as_used=False))\n    self.assertEqual(""foo_1_2_1"", g.unique_name(""foo_1_2""))\n    with g.name_scope(""bar""):\n      self.assertEqual(""bar/foo"", g.unique_name(""foo"", mark_as_used=False))\n      self.assertEqual(""bar/foo"", g.unique_name(""foo""))\n      self.assertEqual(""bar/foo_1"", g.unique_name(""foo"", mark_as_used=False))\n      self.assertEqual(""bar/foo_1"", g.unique_name(""foo""))\n      with g.name_scope(None):\n        self.assertEqual(""foo_3"", g.unique_name(""foo"", mark_as_used=False))\n        self.assertEqual(""foo_3"", g.unique_name(""foo""))\n      with g.name_scope(""baz""):\n        self.assertEqual(\n            ""bar/baz/foo"", g.unique_name(\n                ""foo"", mark_as_used=False))\n        self.assertEqual(""bar/baz/foo"", g.unique_name(""foo""))\n        self.assertEqual(\n            ""bar/baz/foo_1"", g.unique_name(\n                ""foo"", mark_as_used=False))\n        self.assertEqual(""bar/baz/foo_1"", g.unique_name(""foo""))\n      with g.name_scope(""baz""):\n        self.assertEqual(\n            ""bar/baz_1/foo"", g.unique_name(\n                ""foo"", mark_as_used=False))\n        self.assertEqual(""bar/baz_1/foo"", g.unique_name(""foo""))\n        self.assertEqual(\n            ""bar/baz_1/foo_1"", g.unique_name(\n                ""foo"", mark_as_used=False))\n        self.assertEqual(""bar/baz_1/foo_1"", g.unique_name(""foo""))\n    with g.name_scope(""quux""):\n      self.assertEqual(""quux/foo"", g.unique_name(""foo"", mark_as_used=False))\n      self.assertEqual(""quux/foo"", g.unique_name(""foo""))\n    with g.name_scope(""bar""):\n      with g.name_scope(""baz""):\n        self.assertEqual(\n            ""bar_1/baz/foo"", g.unique_name(\n                ""foo"", mark_as_used=False))\n        self.assertEqual(""bar_1/baz/foo"", g.unique_name(""foo""))\n    self.assertEqual(""foo_4"", g.unique_name(""foo"", mark_as_used=False))\n    self.assertEqual(""foo_4"", g.unique_name(""foo""))\n    self.assertEqual(""bar_2"", g.unique_name(""bar"", mark_as_used=False))\n    self.assertEqual(""bar_2"", g.unique_name(""bar""))\n\n  @test_util.run_deprecated_v1\n  def testNameAndVariableScope(self):\n    with self.cached_session() as sess:\n      with sess.graph.name_scope(""l0""):\n        with variable_scope.variable_scope(""l1""):\n          with sess.graph.name_scope(""l1"") as scope:\n            self.assertEqual(""l0/l1/l1/"", scope)\n            self.assertEqual(\n                ""l0/l1/l1/foo"",\n                sess.graph.unique_name(\n                    ""foo"", mark_as_used=False))\n            self.assertEqual(""l0/l1/l1/foo"", sess.graph.unique_name(""foo""))\n          with sess.graph.name_scope(""l2"") as scope:\n            self.assertEqual(""l0/l1/l2/"", scope)\n            self.assertEqual(\n                ""l0/l1/l2/foo"",\n                sess.graph.unique_name(\n                    ""foo"", mark_as_used=False))\n            self.assertEqual(""l0/l1/l2/foo"", sess.graph.unique_name(""foo""))\n\n  def testOutOfOrderUniqueName(self):\n    g = ops.Graph()\n    self.assertEqual(""foo_2"", g.unique_name(""foo_2""))\n    self.assertEqual(""foo"", g.unique_name(""foo""))\n    self.assertEqual(""foo_1"", g.unique_name(""foo""))\n    self.assertEqual(""foo_3"", g.unique_name(""foo""))\n\n  def testUniqueNameCaseInsensitivity(self):\n    g = ops.Graph()\n    self.assertEqual(""foo"", g.unique_name(""foo""))\n    self.assertEqual(""Foo_1"", g.unique_name(""Foo""))\n    with g.name_scope(""bar""):\n      self.assertEqual(""bar/foo"", g.unique_name(""foo""))\n    with g.name_scope(""Bar""):\n      self.assertEqual(""Bar_1/foo"", g.unique_name(""foo""))\n\n  def testInvalidNameRaisesError(self):\n    g = ops.Graph()\n    with g.name_scope(""""):  # Should not raise\n      pass\n    with g.name_scope(""foo/""):  # Should not raise\n      with g.name_scope(""_bar""):  # Should not raise\n        pass\n    with self.assertRaises(ValueError):\n      with g.name_scope(""foo:0""):\n        pass\n    with self.assertRaises(ValueError):\n      with g.name_scope(""_bar""):\n        pass\n\n\nclass NameTest(test_util.TensorFlowTestCase):\n\n  def testGenerateName(self):\n    g = ops.Graph()\n    op0 = g.create_op(""TwoFloatOutputs"", [], [dtypes.float32, dtypes.float32])\n    self.assertEqual(""TwoFloatOutputs"", op0.name)\n    self.assertEqual(""TwoFloatOutputs:0"", op0.outputs[0].name)\n    self.assertEqual(""TwoFloatOutputs:1"", op0.outputs[1].name)\n\n    op1 = g.create_op(""FloatOutput"", [], [dtypes.float32])\n    self.assertEqual(""FloatOutput"", op1.name)\n    self.assertEqual(""FloatOutput:0"", op1.outputs[0].name)\n\n    op2 = g.create_op(""FloatOutput"", [], [dtypes.float32])\n    self.assertEqual(""FloatOutput_1"", op2.name)\n    self.assertEqual(""FloatOutput_1:0"", op2.outputs[0].name)\n\n    op3 = g.create_op(""FloatOutput"", [], [dtypes.float32], name=""my_op"")\n    self.assertEqual(""my_op"", op3.name)\n    self.assertEqual(""my_op:0"", op3.outputs[0].name)\n\n  def testNameScope(self):\n    g = ops.Graph()\n\n    with g.name_scope(""foo"") as foo:\n      self.assertEqual(""foo/"", foo)\n      with g.name_scope(""foo2"") as foo2:\n        self.assertEqual(""foo/foo2/"", foo2)\n      with g.name_scope(None) as empty1:\n        self.assertEqual("""", empty1)\n        with g.name_scope(""foo3"") as foo3:\n          self.assertEqual(""foo3/"", foo3)\n      with g.name_scope("""") as empty2:\n        self.assertEqual("""", empty2)\n\n    self.assertEqual(""FloatOutput"",\n                     g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n    with g.name_scope(""bar"") as scope:\n      self.assertEqual(""bar/FloatOutput"",\n                       g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n      self.assertEqual(""bar/FloatOutput_1"",\n                       g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n      # If you use the value from ""with .. as"", that values is used as-is.\n      self.assertEqual(\n          ""bar"", g.create_op(\n              ""FloatOutput"", [], [dtypes.float32], name=scope).name)\n    with g.name_scope(""baz"") as scope:\n      with g.name_scope(""quux""):\n        self.assertEqual(""baz/quux/FloatOutput"",\n                         g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n      # If you use the value from the enclosing ""with .. as"", nothing is pushed.\n      with g.name_scope(scope):\n        self.assertEqual(""baz/FloatOutput"",\n                         g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n        self.assertEqual(\n            ""baz"", g.create_op(\n                ""FloatOutput"", [], [dtypes.float32], name=scope).name)\n        self.assertEqual(\n            ""trailing"",\n            g.create_op(\n                ""FloatOutput"", [], [dtypes.float32], name=""trailing/"").name)\n    with g.name_scope(""bar""):\n      self.assertEqual(""bar_1/FloatOutput"",\n                       g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n    with g.name_scope(""bar/""):\n      self.assertEqual(""bar/FloatOutput_2"",\n                       g.create_op(""FloatOutput"", [], [dtypes.float32]).name)\n\n\nclass DeviceTest(test_util.TensorFlowTestCase):\n\n  def testNoDevice(self):\n    g = ops.Graph()\n    op = g.create_op(""FloatOutput"", [], [dtypes.float32])\n    self.assertDeviceEqual(None, op.device)\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput"" }\n    """""", gd)\n\n  def testEagerBackingDevice(self):\n    with context.eager_mode():\n      with ops.device(""/device:CPU:0""):\n        t = constant_op.constant(1.0)\n        self.assertRegexpMatches(t.device, ""/device:CPU:0"")\n        self.assertRegexpMatches(t.backing_device, ""/device:CPU:0"")\n\n  def testDevicePartialString(self):\n    g = ops.Graph()\n    with g.device(""/job:worker/replica:2""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n    """""", gd)\n\n  def testDeviceFull(self):\n    g = ops.Graph()\n    with g.device(\n        pydev.DeviceSpec(\n            job=""worker"", replica=2, task=0, device_type=""CPU"",\n            device_index=3)):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/task:0/device:CPU:3"" }\n    """""", gd)\n\n  def testNesting(self):\n    g = ops.Graph()\n    with g.device(""/job:worker/replica:2""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/job:worker/replica:3/task:0""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:3/task:0"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n    """""", gd)\n\n  def testNestingString(self):\n    g = ops.Graph()\n    with g.device(""/job:worker/replica:2""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/job:worker/replica:3/task:0""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:3/task:0"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n    """""", gd)\n\n  def testNestingOverrideGpuCpu(self):\n    g = ops.Graph()\n    with g.device(""/job:worker/replica:2/device:CPU:1""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/job:worker/replica:2/device:GPU:2""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1""  }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:GPU:2"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1"" }\n    """""", gd)\n\n  def testNestingWithMergeDeviceFunction(self):\n    g = ops.Graph()\n\n    with g.device(pydev.merge_device(""/device:GPU:0"")):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(pydev.merge_device(""/job:worker"")):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n        with g.device(pydev.merge_device(""/device:CPU:0"")):\n          g.create_op(""FloatOutput"", [], [dtypes.float32])\n          with g.device(pydev.merge_device(""/job:ps"")):\n            g.create_op(""FloatOutput"", [], [dtypes.float32])\n            with g.device(pydev.merge_device(None)):\n              g.create_op(""FloatOutput"", [], [dtypes.float32])\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/device:GPU:0"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/device:GPU:0"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/device:CPU:0"" }\n      node { name: ""FloatOutput_3"" op: ""FloatOutput""\n             device: ""/job:ps/device:CPU:0"" }\n      node { name: ""FloatOutput_4"" op: ""FloatOutput""\n             device: ""/job:ps/device:CPU:0"" }\n    """""", gd)\n\n  def testNestingWithDeviceStrings(self):\n    g = ops.Graph()\n\n    with g.device(""/device:GPU:0""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/job:worker""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n        with g.device(""/device:CPU:0""):\n          g.create_op(""FloatOutput"", [], [dtypes.float32])\n          with g.device(""/job:ps""):\n            g.create_op(""FloatOutput"", [], [dtypes.float32])\n            with g.device(""""):\n              g.create_op(""FloatOutput"", [], [dtypes.float32])\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/device:GPU:0"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/device:GPU:0"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/device:CPU:0"" }\n      node { name: ""FloatOutput_3"" op: ""FloatOutput""\n             device: ""/job:ps/device:CPU:0"" }\n      node { name: ""FloatOutput_4"" op: ""FloatOutput""\n             device: ""/job:ps/device:CPU:0"" }\n    """""", gd)\n\n  def testNestingWithDeviceStringWildcard(self):\n    g = ops.Graph()\n\n    with g.device(""/device:GPU:7""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/device:GPU:*""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n\n    with g.device(""/device:CPU:*""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/device:CPU:5""):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/device:GPU:7"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/device:GPU:7"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/device:CPU:*"" }\n      node { name: ""FloatOutput_3"" op: ""FloatOutput""\n             device: ""/device:CPU:5"" }\n    """""", gd)\n\n  def testNoneClearsDefault(self):\n    g = ops.Graph()\n    with g.device(""/job:worker/replica:2/device:CPU:1""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(None):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1"" }\n    """""", gd)\n\n  def testNoneIgnoresOuterDeviceFunction(self):\n    g = ops.Graph()\n    with g.device(lambda op: ""/job:worker/replica:2/device:CPU:1""):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(None):\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2/device:CPU:1"" }\n    """""", gd)\n\n  def _overwritingDeviceFunction(self, unused_op):\n    # This device function unconditionally overwrites the device of ops.\n    #\n    # NOTE(mrry): Writing device functions like this is not\n    # recommended. Instead, in most cases you should use\n    # `pydev.merge_device(""/job:ps"")` or simply `""/job:ps""` as the\n    # argument to `tf.device()` and the device component will be merged in.\n    return ""/job:overwrite""\n\n  def testOverwritingBehavior(self):\n    g = ops.Graph()\n    with g.device(self._overwritingDeviceFunction):\n      g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(""/job:ps""):  # Will be overwritten.\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(pydev.merge_device(""/job:ps"")):  # Will be overwritten.\n        g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(None):  # Disables overwriting device function\n        with g.device(""/job:ps""):\n          g.create_op(""FloatOutput"", [], [dtypes.float32])\n      with g.device(None):  # Disables overwriting device function\n        with g.device(pydev.merge_device(""/job:ps"")):\n          g.create_op(""FloatOutput"", [], [dtypes.float32])\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput"" op: ""FloatOutput""\n             device: ""/job:overwrite"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:overwrite"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:overwrite"" }\n      node { name: ""FloatOutput_3"" op: ""FloatOutput""\n             device: ""/job:ps"" }\n      node { name: ""FloatOutput_4"" op: ""FloatOutput""\n             device: ""/job:ps"" }\n    """""", gd)\n\n\nclass MultithreadedGraphStateTest(test_util.TensorFlowTestCase):\n\n  class TestThread(threading.Thread):\n\n    def __init__(self, graph, replica_id):\n      super(MultithreadedGraphStateTest.TestThread, self).__init__()\n      self._graph = graph\n      self._replica_id = replica_id\n      # This thread sets this event when it mutated the graph.  The caller can\n      # wait for that.\n      self.has_mutated_graph = threading.Event()\n      # This thread waits for when it should continue.  The caller can set this\n      # event.\n      self.should_continue = threading.Event()\n\n    def run(self):\n      # Mutate a graph\'s stack, then set `has_mutated_graph`, then wait for\n      # `should_continue`, then add an op to the graph affected by the graph\'s\n      # stack.\n      raise NotImplementedError(""must be implemented in descendants"")\n\n  def testDeviceFunctionStack(self):\n\n    class DeviceSettingThread(self.TestThread):\n\n      def run(self):\n        with g.device(""/job:worker/replica:{}"".format(self._replica_id)):\n          self.has_mutated_graph.set()\n          self.should_continue.wait()\n          self.should_continue.clear()\n          g.create_op(\n              ""FloatOutput"", [], [dtypes.float32],\n              name=""FloatOutput_{}"".format(self._replica_id))\n\n    g = ops.Graph()\n    # If `switch_to_thread` isn\'t called, then device placement of the ops\n    # below is not deterministic.\n    g.switch_to_thread_local()\n    threads = [DeviceSettingThread(g, i) for i in range(3)]\n    for t in threads:\n      t.start()\n      t.has_mutated_graph.wait()\n      t.has_mutated_graph.clear()\n    for t in threads:\n      t.should_continue.set()\n      t.join()\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""FloatOutput_0"" op: ""FloatOutput""\n             device: ""/job:worker/replica:0"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:1"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n    """""", gd)\n\n  def testColocateWith(self):\n\n    class ColocatingThread(self.TestThread):\n\n      def __init__(self, graph, replica_id, op_to_colocate_with):\n        super(ColocatingThread, self).__init__(graph, replica_id)\n        self._op_to_colocate_with = op_to_colocate_with\n\n      def run(self):\n        with g.colocate_with(self._op_to_colocate_with):\n          self.has_mutated_graph.set()\n          self.should_continue.wait()\n          self.should_continue.clear()\n          g.create_op(\n              ""FloatOutput"", [], [dtypes.float32],\n              name=""FloatOutput_{}"".format(self._replica_id))\n\n    g = ops.Graph()\n    ops_to_colocate_with = []\n    for i in range(3):\n      with g.device(""/job:worker/replica:{}"".format(i)):\n        ops_to_colocate_with.append(\n            g.create_op(\n                ""FloatOutput"", [], [dtypes.float32],\n                name=""ColocateWithMe_{}"".format(i)))\n\n    # If `switch_to_thread` isn\'t called, then `device` and `attr` values for\n    # the ops below are not deterministic.\n    g.switch_to_thread_local()\n    threads = [\n        ColocatingThread(g, i, ops_to_colocate_with[i]) for i in range(3)\n    ]\n    for t in threads:\n      t.start()\n      t.has_mutated_graph.wait()\n      t.has_mutated_graph.clear()\n    for t in threads:\n      t.should_continue.set()\n      t.join()\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""ColocateWithMe_0"" op: ""FloatOutput""\n             device: ""/job:worker/replica:0"" }\n      node { name: ""ColocateWithMe_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:1"" }\n      node { name: ""ColocateWithMe_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2"" }\n      node { name: ""FloatOutput_0"" op: ""FloatOutput""\n             device: ""/job:worker/replica:0""\n             attr { key: ""_class""\n               value { list {\n                 s: ""loc:@ColocateWithMe_0""}}}}\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             device: ""/job:worker/replica:1""\n             attr { key: ""_class""\n               value { list {\n                 s: ""loc:@ColocateWithMe_1""}}}}\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             device: ""/job:worker/replica:2""\n             attr { key: ""_class""\n               value { list {\n                 s: ""loc:@ColocateWithMe_2""}}}}\n    """""", gd)\n\n  def testControlDependencies(self):\n\n    class DependingThread(self.TestThread):\n\n      def __init__(self, graph, replica_id, dependency_op):\n        super(DependingThread, self).__init__(graph, replica_id)\n        self._dependency_op = dependency_op\n\n      def run(self):\n        with g.control_dependencies([self._dependency_op]):\n          self.has_mutated_graph.set()\n          self.should_continue.wait()\n          self.should_continue.clear()\n          g.create_op(\n              ""FloatOutput"", [], [dtypes.float32],\n              name=""FloatOutput_{}"".format(self._replica_id))\n\n    g = ops.Graph()\n    dependency_ops = []\n    for i in range(3):\n      dependency_ops.append(\n          g.create_op(\n              ""FloatOutput"", [], [dtypes.float32],\n              name=""ColocateWithMe_{}"".format(i)))\n\n    # If `switch_to_thread` isn\'t called, then `input` values for the ops below\n    # are not deterministic.\n    g.switch_to_thread_local()\n    threads = [DependingThread(g, i, dependency_ops[i]) for i in range(3)]\n    for t in threads:\n      t.start()\n      t.has_mutated_graph.wait()\n      t.has_mutated_graph.clear()\n    for t in threads:\n      t.should_continue.set()\n      t.join()\n\n    gd = g.as_graph_def()\n    self.assertProtoEqualsVersion(""""""\n      node { name: ""ColocateWithMe_0"" op: ""FloatOutput"" }\n      node { name: ""ColocateWithMe_1"" op: ""FloatOutput"" }\n      node { name: ""ColocateWithMe_2"" op: ""FloatOutput"" }\n      node { name: ""FloatOutput_0"" op: ""FloatOutput""\n             input: ""^ColocateWithMe_0"" }\n      node { name: ""FloatOutput_1"" op: ""FloatOutput""\n             input: ""^ColocateWithMe_1"" }\n      node { name: ""FloatOutput_2"" op: ""FloatOutput""\n             input: ""^ColocateWithMe_2"" }\n    """""", gd)\n\n  def testNameStack(self):\n\n    class NameSettingThread(self.TestThread):\n\n      def run(self):\n        with g.name_scope(""foo""):\n          op1 = g.create_op(""FloatOutput"", [], [dtypes.float32])\n          self.has_mutated_graph.set()\n          self.should_continue.wait()\n          self.should_continue.clear()\n          op2 = g.create_op(""FloatOutput"", [], [dtypes.float32])\n          self.result = (op1, op2)\n\n    g = ops.Graph()\n    threads = [NameSettingThread(g, i) for i in range(3)]\n    for t in threads:\n      t.start()\n      t.has_mutated_graph.wait()\n      t.has_mutated_graph.clear()\n\n    for t in threads:\n      t.should_continue.set()\n      t.join()\n\n    suffixes = ["""", ""_1"", ""_2""]\n    for t, s in zip(threads, suffixes):\n      self.assertEquals(""foo"" + s + ""/FloatOutput"", t.result[0].name)\n      self.assertEquals(""foo"" + s + ""/FloatOutput_1"", t.result[1].name)\n\n\nclass ObjectWithName(object):\n\n  def __init__(self, name):\n    self._name = name\n\n  @property\n  def name(self):\n    return self._name\n\n\nclass CollectionTest(test_util.TensorFlowTestCase):\n\n  def test_get_collections(self):\n    g = ops.Graph()\n    self.assertSequenceEqual(g.collections, [])\n    g.add_to_collection(""key"", 12)\n    g.add_to_collection(""key"", 15)\n    self.assertSequenceEqual(g.collections, [""key""])\n    g.add_to_collection(""other"", ""foo"")\n    self.assertSequenceEqual(sorted(g.collections), [""key"", ""other""])\n\n  def test_add_to_collection(self):\n    g = ops.Graph()\n    g.add_to_collection(""key"", 12)\n    g.add_to_collection(""other"", ""foo"")\n    g.add_to_collection(""key"", 34)\n\n    # Note that only blank1 is returned.\n    g.add_to_collection(""blah"", 27)\n    blank1 = ObjectWithName(""prefix/foo"")\n    g.add_to_collection(""blah"", blank1)\n    blank2 = ObjectWithName(""junk/foo"")\n    g.add_to_collection(""blah"", blank2)\n\n    self.assertEqual([12, 34], g.get_collection(""key""))\n    self.assertEqual([], g.get_collection(""nothing""))\n    self.assertEqual([27, blank1, blank2], g.get_collection(""blah""))\n    self.assertEqual([blank1], g.get_collection(""blah"", ""prefix""))\n    self.assertEqual([blank1], g.get_collection(""blah"", "".*x""))\n\n    # Make sure that get_collection() returns a first-level\n    # copy of the collection, while get_collection_ref() returns\n    # the original list.\n    other_collection_snapshot = g.get_collection(""other"")\n    other_collection_ref = g.get_collection_ref(""other"")\n    self.assertEqual([""foo""], other_collection_snapshot)\n    self.assertEqual([""foo""], other_collection_ref)\n    g.add_to_collection(""other"", ""bar"")\n    self.assertEqual([""foo""], other_collection_snapshot)\n    self.assertEqual([""foo"", ""bar""], other_collection_ref)\n    self.assertEqual([""foo"", ""bar""], g.get_collection(""other""))\n    self.assertTrue(other_collection_ref is g.get_collection_ref(""other""))\n\n    # Verify that getting an empty collection ref returns a modifiable list.\n    empty_coll_ref = g.get_collection_ref(""empty"")\n    self.assertEqual([], empty_coll_ref)\n    empty_coll = g.get_collection(""empty"")\n    self.assertEqual([], empty_coll)\n    self.assertFalse(empty_coll is empty_coll_ref)\n    empty_coll_ref2 = g.get_collection_ref(""empty"")\n    self.assertTrue(empty_coll_ref2 is empty_coll_ref)\n    # Add to the collection.\n    empty_coll_ref.append(""something"")\n    self.assertEqual([""something""], empty_coll_ref)\n    self.assertEqual([""something""], empty_coll_ref2)\n    self.assertEqual([], empty_coll)\n    self.assertEqual([""something""], g.get_collection(""empty""))\n    empty_coll_ref3 = g.get_collection_ref(""empty"")\n    self.assertTrue(empty_coll_ref3 is empty_coll_ref)\n\n  def test_add_to_collections_uniquify(self):\n    g = ops.Graph()\n    g.add_to_collections([1, 2, 1], ""key"")\n    # Make sure ""key"" is not added twice\n    self.assertEqual([""key""], g.get_collection(1))\n\n  def test_add_to_collections_from_list(self):\n    g = ops.Graph()\n    g.add_to_collections([""abc"", ""123""], ""key"")\n    self.assertEqual([""key""], g.get_collection(""abc""))\n    self.assertEqual([""key""], g.get_collection(""123""))\n\n  def test_add_to_collections_from_tuple(self):\n    g = ops.Graph()\n    g.add_to_collections((""abc"", ""123""), ""key"")\n    self.assertEqual([""key""], g.get_collection(""abc""))\n    self.assertEqual([""key""], g.get_collection(""123""))\n\n  def test_add_to_collections_from_generator(self):\n    g = ops.Graph()\n\n    def generator():\n      yield ""abc""\n      yield ""123""\n\n    g.add_to_collections(generator(), ""key"")\n    self.assertEqual([""key""], g.get_collection(""abc""))\n    self.assertEqual([""key""], g.get_collection(""123""))\n\n  def test_add_to_collections_from_set(self):\n    g = ops.Graph()\n    g.add_to_collections(set([""abc"", ""123""]), ""key"")\n    self.assertEqual([""key""], g.get_collection(""abc""))\n    self.assertEqual([""key""], g.get_collection(""123""))\n\n  def test_add_to_collections_from_string(self):\n    g = ops.Graph()\n    g.add_to_collections(""abc"", ""key"")\n    self.assertEqual([""key""], g.get_collection(""abc""))\n\n  def test_default_graph(self):\n    with ops.Graph().as_default():\n      ops.add_to_collection(""key"", 90)\n      ops.add_to_collection(""key"", 100)\n      # Collections are ordered.\n      self.assertEqual([90, 100], ops.get_collection(""key""))\n\n  def test_defun(self):\n    with context.eager_mode():\n\n      @eager_function.defun\n      def defun():\n        ops.add_to_collection(""int"", 1)\n        ops.add_to_collection(""tensor"", constant_op.constant(2))\n\n        @eager_function.defun\n        def inner_defun():\n          self.assertEqual(ops.get_collection(""int""), [1])\n          three = ops.get_collection(""tensor"")[0] + ops.get_collection(""int"")[0]\n          ops.add_to_collection(""int"", 2)\n          self.assertEqual(ops.get_collection(""int""), [1, 2])\n          ops.add_to_collection(""foo"", ""bar"")\n          self.assertEqual(ops.get_collection(""foo""), [""bar""])\n          return three\n\n        self.assertEqual(ops.get_collection(""int""), [1])\n        three = inner_defun()\n        self.assertEqual(ops.get_collection(""int""), [1])\n        self.assertEqual(ops.get_collection(""foo""), [])\n        return three\n\n      three = defun()\n      self.assertEqual(three.numpy(), 3)\n\n\nops.NotDifferentiable(""FloatOutput"")\n\n\n@ops.RegisterGradient(""CopyOp"")\ndef _CopyGrad(op, x_grad):  # pylint: disable=invalid-name\n  _ = op\n  return x_grad\n\n\n@ops.RegisterGradient(""copy_override"")\ndef _CopyOverrideGrad(op, x_grad):  # pylint: disable=invalid-name\n  _ = op\n  return x_grad\n\n\nclass RegistrationTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testRegisterGradients(self):\n    x = test_ops.float_output()\n    y = test_ops.copy_op(x)\n    fn = ops.get_gradient_function(y.op)\n    self.assertEqual(_CopyGrad, fn)\n\n  def testOverrideGradients(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.float_output()\n      with g.gradient_override_map({""CopyOp"": ""copy_override""}):\n        y = test_ops.copy_op(x)\n      fn = ops.get_gradient_function(y.op)\n      self.assertEqual(_CopyOverrideGrad, fn)\n\n  def testNonExistentOverride(self):\n    g = ops.Graph()\n    with g.as_default():\n      x = test_ops.float_output()\n      with g.gradient_override_map({""CopyOp"": ""unknown_override""}):\n        y = test_ops.copy_op(x)\n      with self.assertRaisesRegexp(LookupError, ""unknown_override""):\n        ops.get_gradient_function(y.op)\n\n\nclass ComparisonTest(test_util.TensorFlowTestCase):\n\n  def testMembershipAllowed(self):\n    g = ops.Graph()\n    t1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32], name=""myop1"")\n    t2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32], name=""myop2"")\n    self.assertTrue(isinstance(t1, ops.Tensor))\n    self.assertTrue(isinstance(t2, ops.Tensor))\n    self.assertTrue(t1 in [t1])\n    self.assertTrue(t1 not in [t2])\n\n\nclass ControlDependenciesTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testBasic(self):\n    g = ops.Graph()\n    with g.as_default():\n      # Creating unregistered ops with _apply_op() doesn\'t work with the C API\n      # TODO(skyewm): address this more consistently. Possible solutions are\n      # to use registered ops in all tests, create a way to register ops in\n      # Python tests, or conditionally disable the op registration check in\n      # the C API.\n      a = constant_op.constant(1.0)\n      b = constant_op.constant(1.0)\n      with g.control_dependencies([a]):\n        c = constant_op.constant(1.0)\n        d = array_ops.identity(b)\n        e = array_ops.identity(c)\n\n    self.assertEqual(c.op.control_inputs, [a.op])\n    self.assertEqual(d.op.control_inputs, [a.op])\n    # e should be dominated by c.\n    self.assertEqual(e.op.control_inputs, [])\n\n  @test_util.run_in_graph_and_eager_modes\n  def testEager(self):\n    def future():\n      future.calls += 1\n      return constant_op.constant(2.0)\n    future.calls = 0\n\n    if context.executing_eagerly():\n      a = constant_op.constant(1.0)\n      b = future\n      with ops.control_dependencies([a, b]):\n        c = constant_op.constant(3.0)\n      self.assertEqual(future.calls, 1)\n    else:\n      g = ops.Graph()\n      with g.as_default():\n        a = constant_op.constant(1.0)\n        b = future()\n        with g.control_dependencies([a, b]):\n          c = constant_op.constant(3.0)\n      self.assertEqual(c.op.control_inputs, [a.op, b.op])\n      self.assertEqual(future.calls, 1)\n\n  def testBasicWithConversion(self):\n    g = ops.Graph()\n    a = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    class ConvertibleObj(object):\n\n      def _as_graph_element(self):\n        return a\n\n    with g.control_dependencies([ConvertibleObj()]):\n      c = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    self.assertEqual(c.op.control_inputs, [a.op])\n\n  def testNested(self):\n    g = ops.Graph()\n    a_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    with g.control_dependencies([a_1, a_2, a_3, a_4]):\n      b_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    with g.control_dependencies([a_1]):\n      with g.control_dependencies([a_2]):\n        with g.control_dependencies([a_3]):\n          with g.control_dependencies([a_4]):\n            b_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    self.assertItemsEqual([a_1.op, a_2.op, a_3.op, a_4.op],\n                          b_1.op.control_inputs)\n    self.assertItemsEqual(b_1.op.control_inputs, b_2.op.control_inputs)\n\n  def testClear(self):\n    g = ops.Graph()\n    a_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    with g.control_dependencies([a_1]):\n      with g.control_dependencies([a_2]):\n        with g.control_dependencies(None):\n          with g.control_dependencies([a_3]):\n            with g.control_dependencies([a_4]):\n              # deps [a_3, a_4]\n              b_3_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n            # deps = [a_3]\n            b_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n          # deps back to None\n          b_none = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n        # deps back to [a_1, a_2]\n        b_1_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n      # deps back to [a_1]\n      b_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n      with g.control_dependencies(None):\n        # deps are None again\n        b_none2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    self.assertItemsEqual([a_3.op, a_4.op], b_3_4.op.control_inputs)\n    self.assertItemsEqual([a_3.op], b_3.op.control_inputs)\n    self.assertItemsEqual([], b_none.op.control_inputs)\n    self.assertItemsEqual([a_1.op, a_2.op], b_1_2.op.control_inputs)\n    self.assertItemsEqual([a_1.op], b_1.op.control_inputs)\n    self.assertItemsEqual([], b_none2.op.control_inputs)\n\n  def testComplex(self):\n    g = ops.Graph()\n\n    # Usage pattern:\n    # * Nodes a_i are constants defined at the outermost scope, and are used\n    #   as control inputs for the ith nested scope.\n    # * Nodes b_i are defined as Mul(a_3, a_4) at each scope.\n    # * Nodes c_i are defined as Mul(a_1, b_1) at each scope.\n    # * Nodes d_i are defined as Mul(b_i, c_i) at each scope.\n    # * Nodes e_i are defined as Mul(e_i-1, e_i-1) at each scope i > 1.\n\n    a_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    with g.control_dependencies([a_1]):\n      b_1 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_3, a_4],\n                      [dtypes.float32])\n      c_1 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_1, b_1],\n                      [dtypes.float32])\n      d_1 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [b_1, c_1],\n                      [dtypes.float32])\n      e_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n      with g.control_dependencies([a_2]):\n        b_2 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_3, a_4],\n                        [dtypes.float32])\n        c_2 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_1, b_1],\n                        [dtypes.float32])\n        d_2 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [b_2, c_2],\n                        [dtypes.float32])\n        e_2 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [e_1, e_1],\n                        [dtypes.float32])\n        with g.control_dependencies([a_3]):\n          b_3 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_3, a_4],\n                          [dtypes.float32])\n          c_3 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_1, b_1],\n                          [dtypes.float32])\n          d_3 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [b_3, c_3],\n                          [dtypes.float32])\n          e_3 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [e_2, e_2],\n                          [dtypes.float32])\n          with g.control_dependencies([a_4]):\n            b_4 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_3, a_4],\n                            [dtypes.float32])\n            c_4 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [a_1, b_1],\n                            [dtypes.float32])\n            d_4 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [b_4, c_4],\n                            [dtypes.float32])\n            e_4 = _apply_op(g, ""TwoFloatInputsFloatOutput"", [e_3, e_3],\n                            [dtypes.float32])\n\n    self.assertItemsEqual([a_1.op], b_1.op.control_inputs)\n    self.assertItemsEqual([a_1.op, a_2.op], b_2.op.control_inputs)\n    self.assertItemsEqual([a_1.op, a_2.op], b_3.op.control_inputs)\n    self.assertItemsEqual([a_1.op, a_2.op], b_4.op.control_inputs)\n\n    self.assertItemsEqual([], c_1.op.control_inputs)\n    self.assertItemsEqual([a_2.op], c_2.op.control_inputs)\n    self.assertItemsEqual([a_2.op, a_3.op], c_3.op.control_inputs)\n    self.assertItemsEqual([a_2.op, a_3.op, a_4.op], c_4.op.control_inputs)\n\n    self.assertItemsEqual([], d_1.op.control_inputs)\n    self.assertItemsEqual([], d_2.op.control_inputs)\n    self.assertItemsEqual([], d_3.op.control_inputs)\n    self.assertItemsEqual([], d_4.op.control_inputs)\n\n    self.assertItemsEqual([a_1.op], e_1.op.control_inputs)\n    self.assertItemsEqual([a_2.op], e_2.op.control_inputs)\n    self.assertItemsEqual([a_3.op], e_3.op.control_inputs)\n    self.assertItemsEqual([a_4.op], e_4.op.control_inputs)\n\n  def testRepeatedDependency(self):\n    g = ops.Graph()\n    a = g.create_op(""TwoFloatOutputs"", [], [dtypes.float32, dtypes.float32])\n    a_0, a_1 = a.outputs\n    with g.control_dependencies([a_0]):\n      b = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n      with g.control_dependencies([a_1]):\n        c = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    self.assertEqual(b.op.control_inputs, [a])\n    self.assertEqual(c.op.control_inputs, [a])\n\n  def testNoControlDependencyWithDataDependency(self):\n    g = ops.Graph()\n    a = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    with g.control_dependencies([a]):\n      b = _apply_op(g, ""Identity"", [a], [dtypes.float32])\n\n    self.assertEqual(b.op.control_inputs, [])\n\n\nclass OpScopeTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def testNames(self):\n    with ops.name_scope(""foo"") as foo:\n      self.assertEqual(""foo/"", foo)\n      with ops.name_scope(""foo2"") as foo2:\n        self.assertEqual(""foo/foo2/"", foo2)\n      with ops.name_scope(None) as empty1:\n        self.assertEqual("""", empty1)\n        with ops.name_scope(""foo3"") as foo3:\n          self.assertEqual(""foo3/"", foo3)\n      with ops.name_scope("""") as empty2:\n        self.assertEqual("""", empty2)\n    with ops.name_scope(""foo/"") as outer_foo:\n      self.assertEqual(""foo/"", outer_foo)\n      with ops.name_scope("""") as empty3:\n        self.assertEqual("""", empty3)\n      with ops.name_scope(""foo4"") as foo4:\n        self.assertEqual(""foo/foo4/"", foo4)\n      with ops.name_scope(""foo5//"") as foo5:\n        self.assertEqual(""foo5//"", foo5)\n        with ops.name_scope(""foo6"") as foo6:\n          self.assertEqual(""foo5//foo6/"", foo6)\n      with ops.name_scope(""/"") as foo7:\n        self.assertEqual(""/"", foo7)\n      with ops.name_scope(""//"") as foo8:\n        self.assertEqual(""//"", foo8)\n      with ops.name_scope(""a//b/c"") as foo9:\n        self.assertEqual(""foo/a//b/c/"", foo9)\n    with ops.name_scope(""a//b/c"") as foo10:\n      self.assertEqual(""a//b/c/"", foo10)\n\n  @test_util.run_in_graph_and_eager_modes\n  def testEagerDefaultScopeName(self):\n    with ops.name_scope(None, ""default"") as scope:\n      self.assertEqual(scope, ""default/"")\n      with ops.name_scope(None, ""default2"") as scope2:\n        self.assertEqual(scope2, ""default/default2/"")\n\n  @test_util.run_deprecated_v1\n  def testNoScopeName(self):\n    g0 = ops.Graph()\n    values = [\n        g0.create_op(""A"", [], [dtypes.float32]),\n        g0.create_op(""B"", [], [dtypes.float32])\n    ]\n    with self.assertRaises(ValueError):\n      with ops.name_scope(None, values=values):\n        pass\n    with self.assertRaises(ValueError):\n      with ops.name_scope(None, None, values):\n        pass\n\n  @test_util.run_deprecated_v1\n  def testEmptyScopeName(self):\n    g0 = ops.Graph()\n    a = g0.create_op(""A"", [], [dtypes.float32])\n    b = g0.create_op(""B"", [], [dtypes.float32])\n    with ops.name_scope("""", values=[a, b]) as scope:\n      self.assertEqual("""", scope)\n      self.assertEqual(g0, ops.get_default_graph())\n    with ops.name_scope("""", ""my_default_scope"", [a, b]) as scope:\n      self.assertEqual("""", scope)\n      self.assertEqual(g0, ops.get_default_graph())\n\n  @test_util.run_deprecated_v1\n  def testDefaultScopeName(self):\n    g0 = ops.Graph()\n    a = g0.create_op(""A"", [], [dtypes.float32])\n    b = g0.create_op(""B"", [], [dtypes.float32])\n    scope_name = ""my_scope""\n    default_scope_name = ""my_default_scope""\n    with ops.name_scope(scope_name, default_scope_name, [a, b]) as scope:\n      self.assertEqual(""%s/"" % scope_name, scope)\n      self.assertEqual(g0, ops.get_default_graph())\n    with ops.name_scope(None, default_scope_name, [a, b]) as scope:\n      self.assertEqual(""%s/"" % default_scope_name, scope)\n      self.assertEqual(g0, ops.get_default_graph())\n\n  def _testGraphElements(self, graph_elements):\n    scope_name = ""my_scope""\n    with ops.name_scope(scope_name, values=graph_elements) as scope:\n      self.assertEqual(""%s/"" % scope_name, scope)\n      self.assertEqual(graph_elements[0].graph, ops.get_default_graph())\n    g1 = ops.Graph()\n    a = g1.create_op(""A"", [], [dtypes.float32])\n    with self.assertRaises(ValueError):\n      with ops.name_scope(scope_name, values=graph_elements + [a]):\n        pass\n\n  @test_util.run_deprecated_v1\n  def testTensor(self):\n    g0 = ops.Graph()\n    a = g0.create_op(""A"", [], [dtypes.float32])\n    b = g0.create_op(""B"", [], [dtypes.float32])\n    self._testGraphElements([a, b])\n\n  @test_util.run_deprecated_v1\n  def testSparseTensor(self):\n    g0 = ops.Graph()\n    a = g0.create_op(""A"", [], [dtypes.float32])\n    b = g0.create_op(""B"", [], [dtypes.float32])\n    sparse = sparse_tensor.SparseTensor(\n        _apply_op(g0, ""Int64Output"", [], [dtypes.int64]),\n        _apply_op(g0, ""FloatOutput"", [], [dtypes.float32]),\n        _apply_op(g0, ""Int64Output"", [], [dtypes.int64]))\n    self._testGraphElements([a, sparse, b])\n\n  @test_util.run_deprecated_v1\n  def testVariable(self):\n    g0 = ops.Graph()\n    with g0.as_default():\n      variable = variables.Variable([1.0])\n    a = g0.create_op(""A"", [], [dtypes.float32])\n    b = g0.create_op(""B"", [], [dtypes.float32])\n    self._testGraphElements([a, variable, b])\n\n\nclass InitScopeTest(test_util.TensorFlowTestCase):\n\n  def testClearsControlDependencies(self):\n    g = ops.Graph()\n    a_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    a_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    with g.as_default():\n      with g.control_dependencies([a_1]):\n        with g.control_dependencies([a_2]):\n          with ops.init_scope():\n            with g.control_dependencies([a_3]):\n              with g.control_dependencies([a_4]):\n                # deps [a_3, a_4]\n                b_3_4 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n              # deps = [a_3]\n              b_3 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n            # deps back to None\n            b_none = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n          # deps back to [a_1, a_2]\n          b_1_2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n        # deps back to [a_1]\n        b_1 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n        with ops.init_scope():\n          # deps are None again\n          b_none2 = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n\n    self.assertItemsEqual([a_3.op, a_4.op], b_3_4.op.control_inputs)\n    self.assertItemsEqual([a_3.op], b_3.op.control_inputs)\n    self.assertItemsEqual([], b_none.op.control_inputs)\n    self.assertItemsEqual([a_1.op, a_2.op], b_1_2.op.control_inputs)\n    self.assertItemsEqual([a_1.op], b_1.op.control_inputs)\n    self.assertItemsEqual([], b_none2.op.control_inputs)\n\n  def testLiftsOpsFromFunctions(self):\n    g0 = ops.Graph()\n    g1 = ops.Graph()\n    g1._building_function = True  # pylint: disable=protected-access\n    g2 = ops.Graph()\n    g2._building_function = True  # pylint: disable=protected-access\n\n    with g0.as_default():\n      with g1.as_default():\n        with g2.as_default():\n          with ops.init_scope():\n            _ = constant_op.constant(1.0)\n\n    self.assertEqual(len(g2.get_operations()), 0)\n    self.assertEqual(len(g1.get_operations()), 0)\n    self.assertEqual(len(g0.get_operations()), 1)\n\n  def testPreservesDevices(self):\n    g0 = ops.Graph()\n    with g0.as_default(), ops.device(""CPU:0""):\n      g1 = ops.Graph()\n      g1._building_function = True  # pylint: disable=protected-access\n      with g1.as_default(), ops.device(""GPU:0""):\n        with ops.init_scope():\n          # init_scope should preserve device set under `g1`.\n          on_gpu = constant_op.constant(1.0)\n          self.assertEqual(on_gpu.device, ""/device:GPU:0"")\n        still_on_gpu = constant_op.constant(1.0)\n        self.assertEqual(still_on_gpu.device, ""/device:GPU:0"")\n      on_cpu = constant_op.constant(1.0)\n      self.assertEqual(on_cpu.device, ""/device:CPU:0"")\n\n  def testComposes(self):\n    g0 = ops.Graph()\n    g1 = ops.Graph()\n    g1._building_function = True  # pylint: disable=protected-access\n    g2 = ops.Graph()\n    g2._building_function = True  # pylint: disable=protected-access\n    g3 = ops.Graph()\n    g3._building_function = False  # pylint: disable=protected-access\n\n    with g0.as_default():\n      with g1.as_default():\n        with ops.init_scope():\n          # This op should be lifted into g0.\n          _ = constant_op.constant(1.0)\n          self.assertIs(g0, ops.get_default_graph())\n          self.assertEqual(len(g2.get_operations()), 0)\n          self.assertEqual(len(g1.get_operations()), 0)\n          self.assertEqual(len(g0.get_operations()), 1)\n        with g2.as_default():\n          with ops.init_scope():\n            # This op should be lifted into g0.\n            _ = constant_op.constant(1.0)\n            self.assertIs(g0, ops.get_default_graph())\n            with g3.as_default():\n              with ops.init_scope():\n                # This op should be lifted into g3, because g3 is not building a\n                # function.\n                _ = constant_op.constant(1.0)\n                self.assertIs(g3, ops.get_default_graph())\n\n    self.assertEqual(len(g3.get_operations()), 1)\n    self.assertEqual(len(g2.get_operations()), 0)\n    self.assertEqual(len(g1.get_operations()), 0)\n    self.assertEqual(len(g0.get_operations()), 2)\n\n  def testEscapesToEagerContext(self):\n    g = ops.Graph()\n    g._building_function = True  # pylint: disable=protected-access\n    with context.eager_mode():\n      with context.graph_mode():\n        with g.as_default():\n          with ops.init_scope():\n            # Because g is building a function, init_scope should\n            # escape out to the eager context.\n            self.assertTrue(context.executing_eagerly())\n          # g should be reinstated as the default graph, and the\n          # graph context should be re-entered.\n          self.assertIs(g, ops.get_default_graph())\n          self.assertFalse(context.executing_eagerly())\n\n  def testStaysInEagerWhenOnlyEagerContextActive(self):\n    with context.eager_mode():\n      with ops.init_scope():\n        self.assertTrue(context.eager_mode())\n      self.assertTrue(context.eager_mode())\n\n  def testEscapesDefunWhenInEagerMode(self):\n\n    def function_with_variables():\n      with ops.init_scope():\n        self.v = resource_variable_ops.ResourceVariable(3)\n      return self.v.assign_add(1)\n\n    with context.eager_mode():\n      # Each invocation of function_with_variables recreates a variable.\n      self.assertEqual(4, int(function_with_variables()))\n      self.assertEqual(4, int(function_with_variables()))\n\n      compiled = eager_function.defun(function_with_variables)\n      # The init_scope in function_with_variables lifts the variable out\n      # of the graph function constructed by defun; hence,\n      # compiled now appears to be stateful.\n      self.assertEqual(4, int(compiled()))\n      self.assertEqual(5, int(compiled()))\n\n  def testEscapesDefunWhenInGraphMode(self):\n    def function_with_variables(name):\n      with ops.init_scope():\n        _ = variable_scope.get_variable(name, shape=(1,))\n\n    g = ops.Graph()\n    with g.as_default():\n      with self.cached_session():\n        # First ensure that graphs that are not building functions are\n        # not escaped.\n        function_with_variables(""foo"")\n        with self.assertRaisesRegexp(ValueError,\n                                     r""Variable foo already exists.*""):\n          # This will fail because reuse is not set to True.\n          function_with_variables(""foo"")\n\n        compiled = eager_function.defun(function_with_variables)\n        compiled(""bar"")\n        self.assertEqual(\n            len(ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)), 2)\n\n        # The second call to `compiled` should not create variables: the\n        # init_scope has lifted the variable creation code out of the defun.\n        compiled(""bar"")\n        self.assertEqual(\n            len(ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)), 2)\n\n  def testEscapesNestedDefun(self):\n\n    def inner_function():\n      with ops.init_scope():\n        self.v = resource_variable_ops.ResourceVariable(1)\n      return self.v.assign_add(2)\n\n    def outer_function(inner=None):\n      with ops.init_scope():\n        self.v0 = resource_variable_ops.ResourceVariable(0)\n      return self.v0.assign_add(1) + inner()\n\n    with context.eager_mode():\n      # Each invocation of outer_function recreates variables.\n      self.assertEqual(4, int(outer_function(inner=inner_function)))\n      self.assertEqual(4, int(outer_function(inner=inner_function)))\n\n      compiled_inner = eager_function.defun(inner_function)\n      compiled_outer = eager_function.defun(outer_function)\n      # The init_scope lifts variables out of the graph functions\n      # constructed by defun; hence, compiled_outer should now appear to be\n      # stateful.\n      self.assertEqual(4, int(compiled_outer(inner=compiled_inner)))\n      self.assertEqual(7, int(compiled_outer(inner=compiled_inner)))\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testFallsBackToGlobalGraphWhenAllGraphsAreBuildingFunctions(self):\n    with context.graph_mode():\n      ops.reset_default_graph()\n      # This doesn\'t push anything onto the graph stack, but it does\n      # set the stack\'s global graph.\n      global_graph = ops.get_default_graph()\n      fn_graph = ops.Graph()\n\n      # pylint: disable=protected-access\n      fn_graph._building_function = True\n      self.assertEqual(len(ops._default_graph_stack.stack), 0)\n      with fn_graph.as_default():\n        self.assertEqual(len(ops._default_graph_stack.stack), 1)\n        with ops.init_scope():\n          self.assertGreater(len(ops._default_graph_stack.stack), 1)\n          dummy = constant_op.constant(1.0)\n        self.assertEqual(len(ops._default_graph_stack.stack), 1)\n      # Note that the global graph is _not_ on the graph stack.\n      self.assertEqual(len(ops._default_graph_stack.stack), 0)\n      # Ensure that `dummy` was added to the global graph.\n      self.assertEqual(global_graph, dummy.graph)\n      # pylint: enable=protected-access\n\n  def testInstallsDefaultGraphWhenGraphStackIsEmptyInGraphMode(self):\n    with context.graph_mode():\n      # pylint: disable=protected-access\n      self.assertEqual(len(ops._default_graph_stack.stack), 0)\n      with ops.init_scope():\n        self.assertGreater(len(ops._default_graph_stack.stack), 0)\n      self.assertEqual(len(ops._default_graph_stack.stack), 0)\n      # pylint: enable=protected-access\n\n  def testPreservesNameScopeInGraphConstruction(self):\n    with ops.Graph().as_default():\n      function_graph = ops.Graph()\n      with function_graph.as_default():\n        with ops.name_scope(""inner""), ops.init_scope():\n          self.assertEqual(ops.get_name_scope(), ""inner"")\n      self.assertEqual(ops.get_name_scope(), """")\n\n  def testEnteringGraphFromEagerIsSticky(self):\n    with context.eager_mode():\n      g = ops.Graph()\n      with g.as_default():\n        with ops.init_scope():\n          self.assertFalse(context.executing_eagerly())\n          self.assertEqual(g, ops.get_default_graph())\n\n  def testMixGraphEager(self):\n    with context.eager_mode():\n      c = constant_op.constant(1.0)\n      with ops.Graph().as_default():\n        with self.assertRaisesRegexp(\n            RuntimeError, ""Attempting to capture an EagerTensor""):\n          math_ops.add(c, c)\n        c2 = constant_op.constant(2.0)\n      with self.assertRaisesRegexp(\n          TypeError, ""contains objects other than \'EagerTensor\'""):\n        math_ops.add(c2, c2)\n\n  def testPreservesNameScopeInEagerExecution(self):\n    with context.eager_mode():\n      def foo():\n        with ops.name_scope(""inner""), ops.init_scope():\n          if context.executing_eagerly():\n            # A trailing slash is always appended when eager execution is\n            # enabled.\n            self.assertEqual(context.context().scope_name, ""inner/"")\n          else:\n            self.assertEqual(ops.get_name_scope(), ""inner"")\n\n      foo()\n      self.assertEqual(ops.get_name_scope(), """")\n      foo_compiled = eager_function.defun(foo)\n      foo_compiled()\n      self.assertEqual(ops.get_name_scope(), """")\n\n  def testExecutingEagerlyOutsideFunctions(self):\n\n    @eager_function.defun\n    def f():\n      return ops.executing_eagerly_outside_functions()\n\n    with context.eager_mode():\n      self.assertTrue(ops.executing_eagerly_outside_functions())\n      self.assertTrue(f())\n      g = ops.Graph()\n      with g.as_default():\n        self.assertFalse(ops.executing_eagerly_outside_functions())\n\n\nclass GraphTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    ops.reset_default_graph()\n\n  def _AssertDefault(self, expected):\n    self.assertIs(expected, ops.get_default_graph())\n\n  def testResetDefaultGraphNesting(self):\n    g0 = ops.Graph()\n    with self.assertRaises(AssertionError):\n      with g0.as_default():\n        ops.reset_default_graph()\n\n  def testGraphContextManagerCancelsEager(self):\n    with context.eager_mode():\n      with ops.Graph().as_default():\n        self.assertFalse(context.executing_eagerly())\n\n  def testGraphContextManager(self):\n    g0 = ops.Graph()\n    with g0.as_default() as g1:\n      self.assertIs(g0, g1)\n\n  def testDefaultGraph(self):\n    orig = ops.get_default_graph()\n    self._AssertDefault(orig)\n    g0 = ops.Graph()\n    self._AssertDefault(orig)\n    context_manager_0 = g0.as_default()\n    self._AssertDefault(orig)\n    with context_manager_0 as g0:\n      self._AssertDefault(g0)\n      with ops.Graph().as_default() as g1:\n        self._AssertDefault(g1)\n      self._AssertDefault(g0)\n    self._AssertDefault(orig)\n\n  def testPreventFeeding(self):\n    g = ops.Graph()\n    a = constant_op.constant(2.0)\n    self.assertTrue(g.is_feedable(a))\n    g.prevent_feeding(a)\n    self.assertFalse(g.is_feedable(a))\n\n  @test_util.run_deprecated_v1\n  def testPreventFetching(self):\n    g = ops.Graph()\n    a = constant_op.constant(2.0)\n    self.assertTrue(g.is_fetchable(a))\n    g.prevent_fetching(a.op)\n    self.assertFalse(g.is_fetchable(a))\n\n  def testAsGraphElementConversions(self):\n\n    class ConvertibleObj(object):\n\n      def _as_graph_element(self):\n        return ""FloatOutput:0""\n\n    class NonConvertibleObj(object):\n\n      pass\n\n    g = ops.Graph()\n    a = _apply_op(g, ""FloatOutput"", [], [dtypes.float32])\n    self.assertEqual(a, g.as_graph_element(ConvertibleObj()))\n    with self.assertRaises(TypeError):\n      g.as_graph_element(NonConvertibleObj())\n\n  # Regression test against creating custom __del__ functions in classes\n  # involved in cyclic references, e.g. Graph and Operation. (Python won\'t gc\n  # cycles that require calling a __del__ method, because the __del__ method can\n  # theoretically increase the object\'s refcount to ""save"" it from gc, and any\n  # already-deleted objects in the cycle would have be to restored.)\n  def testGarbageCollected(self):\n    # Create a graph we can delete and a weak reference to monitor if it\'s gc\'d\n    g = ops.Graph()\n    g_ref = weakref.ref(g)\n    # Create some ops\n    with g.as_default():\n      a = constant_op.constant(2.0)\n      b = constant_op.constant(3.0)\n      c = math_ops.add(a, b)\n    # Create a session we can delete\n    with session.Session(graph=g) as sess:\n      self.evaluate(c)\n    # Delete all references and trigger gc\n    del g\n    del a\n    del b\n    del c\n    del sess\n    gc.collect()\n    self.assertIsNone(g_ref())\n\n  def testRunnableAfterInvalidShape(self):\n    with ops.Graph().as_default():\n      with self.assertRaises(ValueError):\n        math_ops.add([1, 2], [1, 2, 3])\n      a = constant_op.constant(1)\n      with session.Session() as sess:\n        self.evaluate(a)\n\n  def testRunnableAfterInvalidShapeWithKernelLabelMap(self):\n    g = ops.Graph()\n    with g.as_default():\n      with g._kernel_label_map({""KernelLabelRequired"": ""overload_1""}):\n        with self.assertRaises(ValueError):\n          test_ops.kernel_label_required(1)\n      a = constant_op.constant(1)\n      with session.Session() as sess:\n        self.evaluate(a)\n\n\nclass AttrScopeTest(test_util.TensorFlowTestCase):\n\n  def _get_test_attrs(self):\n    x = control_flow_ops.no_op()\n    try:\n      a = compat.as_text(x.get_attr(""_A""))\n    except ValueError:\n      a = None\n    try:\n      b = compat.as_text(x.get_attr(""_B""))\n    except ValueError:\n      b = None\n    return (a, b)\n\n  @test_util.run_deprecated_v1\n  def testNoLabel(self):\n    with self.cached_session():\n      self.assertAllEqual((None, None), self._get_test_attrs())\n\n  @test_util.run_deprecated_v1\n  def testLabelMap(self):\n    with self.cached_session() as sess:\n      a1 = self._get_test_attrs()\n      with sess.graph._attr_scope({\n          ""_A"": attr_value_pb2.AttrValue(s=compat.as_bytes(""foo""))\n      }):\n        a2 = self._get_test_attrs()\n        with sess.graph._attr_scope({\n            ""_A"": None,\n            ""_B"": attr_value_pb2.AttrValue(s=compat.as_bytes(""bar""))\n        }):\n          a3 = self._get_test_attrs()\n          with sess.graph._attr_scope({\n              ""_A"": attr_value_pb2.AttrValue(s=compat.as_bytes(""baz""))\n          }):\n            a4 = self._get_test_attrs()\n          a5 = self._get_test_attrs()\n        a6 = self._get_test_attrs()\n      a7 = self._get_test_attrs()\n\n      self.assertAllEqual((None, None), a1)\n      self.assertAllEqual((""foo"", None), a2)\n      self.assertAllEqual((None, ""bar""), a3)\n      self.assertAllEqual((""baz"", ""bar""), a4)\n      self.assertAllEqual((None, ""bar""), a5)\n      self.assertAllEqual((""foo"", None), a6)\n      self.assertAllEqual((None, None), a7)\n\n\nops.RegisterShape(""KernelLabel"")(common_shapes.scalar_shape)\n\n\nclass KernelLabelTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testNoLabel(self):\n    with self.cached_session():\n      self.assertAllEqual(b""My label is: default"",\n                          test_ops.kernel_label().eval())\n\n  @test_util.run_deprecated_v1\n  def testLabelMap(self):\n    with self.cached_session() as sess:\n      default_1 = test_ops.kernel_label()\n      # pylint: disable=protected-access\n      with sess.graph._kernel_label_map({""KernelLabel"": ""overload_1""}):\n        overload_1_1 = test_ops.kernel_label()\n        with sess.graph._kernel_label_map({""KernelLabel"": ""overload_2""}):\n          overload_2 = test_ops.kernel_label()\n          with sess.graph._kernel_label_map({""KernelLabel"": """"}):\n            default_2 = test_ops.kernel_label()\n        overload_1_2 = test_ops.kernel_label()\n      # pylint: enable=protected-access\n      default_3 = test_ops.kernel_label()\n\n      self.assertAllEqual(b""My label is: default"", self.evaluate(default_1))\n      self.assertAllEqual(b""My label is: default"", self.evaluate(default_2))\n      self.assertAllEqual(b""My label is: default"", self.evaluate(default_3))\n      self.assertAllEqual(b""My label is: overload_1"",\n                          self.evaluate(overload_1_1))\n      self.assertAllEqual(b""My label is: overload_1"",\n                          self.evaluate(overload_1_2))\n      self.assertAllEqual(b""My label is: overload_2"", self.evaluate(overload_2))\n\n\nclass AsGraphDefTest(test_util.TensorFlowTestCase):\n\n  def testGraphDefVersion(self):\n    """"""Test that the graphdef version is plumbed through to kernels.""""""\n    with ops.Graph().as_default() as g:\n      version = g.graph_def_versions.producer\n      with self.session(graph=g):\n        v = test_ops.graph_def_version().eval()\n        self.assertEqual(version, v)\n\n  def testAddShapes(self):\n    with ops.Graph().as_default() as g:\n      t1, t2, t3, t4, t5 = _apply_op(g, ""FiveFloatOutputs"", [],\n                                     [dtypes.float32] * 5)\n      t1.set_shape(None)\n      t2.set_shape([])\n      t3.set_shape([None])\n      t4.set_shape([43, 37])\n      t5.set_shape([43, None])\n\n      b = constant_op.constant(1.0)  # pylint: disable=unused-variable\n\n      gd = g.as_graph_def(add_shapes=True)\n      self.assertProtoEqualsVersion(""""""\n      node { name: ""FiveFloatOutputs"" op: ""FiveFloatOutputs""\n        attr {\n          key: ""_output_shapes""\n          value {\n            list {\n              shape { unknown_rank: true }\n              shape { }\n              shape { dim { size: -1 } }\n              shape { dim { size: 43 } dim { size: 37 } }\n              shape { dim { size: 43 } dim { size: -1 } }\n            }\n          }\n        }\n      }\n    node { name: ""Const"" op: ""Const""\n      attr {\n        key: ""_output_shapes""\n        value {\n          list {\n            shape { }\n          }\n        }\n      }\n      attr {\n        key: ""dtype""\n        value { type: DT_FLOAT }\n      }\n      attr {\n        key: ""value""\n        value {\n          tensor {\n            dtype: DT_FLOAT\n            tensor_shape { }\n         float_val: 1.0  } } } }\n      """""", gd)\n\n\n@ops.RegisterStatistics(""a"", ""flops"")\ndef _calc_a_forward_flops(unused_graph, unused_node):\n  return ops.OpStats(""flops"", 20)\n\n\nclass StatisticsTest(test_util.TensorFlowTestCase):\n\n  def testRegisteredNode(self):\n    graph = ops.Graph()\n    node = ops._NodeDef(""a"", ""an_a"")\n    flops = ops.get_stats_for_node_def(graph, node, ""flops"")\n    self.assertEqual(20, flops.value)\n    missing_stat = ops.get_stats_for_node_def(graph, node, ""missing_stat"")\n    self.assertEqual(None, missing_stat.value)\n\n  def testUnregisteredNode(self):\n    graph = ops.Graph()\n    node = ops._NodeDef(""b"", ""a_b"")\n    weight_params = ops.get_stats_for_node_def(graph, node, ""weight_params"")\n    self.assertEqual(None, weight_params.value)\n\n  def testAccumulateStatistics(self):\n    flops_total = ops.OpStats(""flops"")\n    self.assertEqual(None, flops_total.value)\n    second_flops = ops.OpStats(""flops"", 3)\n    flops_total += second_flops\n    self.assertEqual(3, flops_total.value)\n\n\nclass DeviceStackTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testBasicDeviceAssignmentMetadata(self):\n\n    def device_func(unused_op):\n      return ""/cpu:*""\n\n    const_zero = constant_op.constant([0.0], name=""zero"")\n    with ops.device(""/cpu""):\n      const_one = constant_op.constant([1.0], name=""one"")\n      with ops.device(""/cpu:0""):\n        const_two = constant_op.constant([2.0], name=""two"")\n    with ops.device(device_func):\n      const_three = constant_op.constant(3.0, name=""three"")\n\n    self.assertEqual(0, len(const_zero.op._device_assignments))\n\n    one_list = const_one.op._device_assignments\n    self.assertEqual(1, len(one_list))\n    self.assertEqual(""/cpu"", one_list[0].obj)\n    self.assertEqual(""ops_test.py"", os.path.basename(one_list[0].filename))\n\n    two_list = const_two.op._device_assignments\n    self.assertEqual(2, len(two_list))\n    devices = [t.obj for t in two_list]\n    self.assertEqual(set([""/cpu"", ""/cpu:0""]), set(devices))\n\n    three_list = const_three.op._device_assignments\n    self.assertEqual(1, len(three_list))\n    func_description = three_list[0].obj\n    expected_regex = r""device_func<.*ops_test.py, [0-9]+""\n    self.assertRegexpMatches(func_description, expected_regex)\n\n  @test_util.run_deprecated_v1\n  def testDeviceAssignmentMetadataForGraphDeviceAndTfDeviceFunctions(self):\n\n    with ops.device(""/cpu""):\n      const_one = constant_op.constant([1.0], name=""one"")\n    with ops.get_default_graph().device(""/cpu""):\n      const_two = constant_op.constant([2.0], name=""two"")\n\n    one_metadata = const_one.op._device_assignments[0]\n    two_metadata = const_two.op._device_assignments[0]\n\n    # Verify both types of device assignment return the right stack info.\n    self.assertRegexpMatches(""ops_test.py"",\n                             os.path.basename(one_metadata.filename))\n    self.assertEqual(one_metadata.filename, two_metadata.filename)\n    self.assertEqual(one_metadata.lineno + 2, two_metadata.lineno)\n\n\nclass ColocationGroupTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testBasic(self):\n    a = constant_op.constant([2.0], name=""a"")\n    with ops.colocate_with(a.op):\n      b = constant_op.constant(3.0)\n    c = constant_op.constant(4.0)\n    self.assertEqual([b""loc:@a""], a.op.colocation_groups())\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n    with self.assertRaises(ValueError):\n      c.op.get_attr(""_class"")\n\n  @test_util.run_deprecated_v1\n  def testBasicColocationMetadata(self):\n    const_two = constant_op.constant([2.0], name=""two"")\n    with ops.colocate_with(const_two.op):\n      const_three = constant_op.constant(3.0, name=""three"")\n    locations_dict = const_three.op._colocation_dict\n    self.assertIn(""two"", locations_dict)\n    metadata = locations_dict[""two""]\n    self.assertIsNone(metadata.obj)\n    # Check that this test\'s filename is recorded as the file containing the\n    # colocation statement.\n    self.assertEqual(""ops_test.py"", os.path.basename(metadata.filename))\n\n  @test_util.run_deprecated_v1\n  def testColocationDeviceInteraction(self):\n    with ops.device(""/cpu:0""):\n      with ops.device(""/device:GPU:0""):\n        a = constant_op.constant([2.0], name=""a"")\n      with ops.colocate_with(a.op):\n        # \'b\' is created in the scope of /cpu:0, but it is\n        # colocated with \'a\', which is on \'/device:GPU:0\'.  colocate_with\n        # overrides devices because it is a stronger constraint.\n        b = constant_op.constant(3.0)\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n    self.assertEqual(a.op.device, b.op.device)\n\n  @test_util.run_deprecated_v1\n  def testColocationCanonicalization(self):\n    with ops.device(""/device:GPU:0""):\n      _ = constant_op.constant(2.0)\n    with ops.device(lambda op: ""/device:GPU:0""):\n      b = constant_op.constant(3.0)\n    with ops.get_default_graph().colocate_with(b):\n      with ops.device(""/device:GPU:0""):\n        c = constant_op.constant(4.0)\n\n    # A\'s device will be /device:GPU:0\n    # B\'s device will be /device:GPU:0\n    # C\'s device will be /device:GPU:0 because it\n    # inherits B\'s device name, after canonicalizing the names.\n    self.assertEqual(b.op.device, c.op.device)\n\n  @test_util.run_deprecated_v1\n  def testLocationOverrides(self):\n    with ops.device(""/cpu:0""):\n      with ops.device(""/device:GPU:0""):\n        a = constant_op.constant([2.0], name=""a"")\n        # Note that this colocation is ""redundant"", since we are\n        # within the scope of ""/device:GPU:0"".  However, we would like to\n        # preserve in the GraphDef that these two ops should be\n        # colocated in a portable way.\n        with ops.colocate_with(a.op):\n          b = constant_op.constant(3.0)\n        c = constant_op.constant(4.0)\n      d = constant_op.constant(5.0)\n\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n    self.assertEqual(""/device:GPU:0"", a.op.device)\n    self.assertEqual(a.op.device, b.op.device)\n\n    # Test that device function stack is restored.\n    self.assertEqual(""/device:GPU:0"", c.op.device)\n    self.assertEqual(""/device:CPU:0"", d.op.device)\n\n  @test_util.run_deprecated_v1\n  def testNestedColocateWith(self):\n    a = constant_op.constant([2.0], name=""a"")\n    with ops.colocate_with(a.op):\n      b = constant_op.constant(3.0)\n      with ops.colocate_with(b.op):\n        c = constant_op.constant(4.0)\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n    self.assertEqual([b""loc:@a""], c.op.colocation_groups())\n\n  @test_util.run_deprecated_v1\n  def testMultiColocationGroups(self):\n    a = constant_op.constant([2.0], name=""a"")\n    b = constant_op.constant(3.0, name=""b"")\n    with ops.colocate_with(a.op):\n      with ops.colocate_with(b.op):\n        c = constant_op.constant(4.0)\n    self.assertEqual(set([b""loc:@a"", b""loc:@b""]), set(c.op.colocation_groups()))\n\n  @test_util.run_deprecated_v1\n  def testColocationIgnoreStack(self):\n    a = constant_op.constant([2.0], name=""a"")\n    b = constant_op.constant(3.0, name=""b"")\n    with ops.colocate_with(a.op):\n      with ops.colocate_with(b.op, ignore_existing=True):\n        c = constant_op.constant(4.0)\n    self.assertEqual(set([b""loc:@b""]), set(c.op.colocation_groups()))\n\n  @test_util.run_deprecated_v1\n  def testColocateWithReset(self):\n    a = constant_op.constant([2.0], name=""a"")\n    with ops.colocate_with(a.op):\n      b = constant_op.constant(3.0, name=""b"")\n      with ops.colocate_with(None, ignore_existing=True):\n        c = constant_op.constant(4.0, name=""c"")\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n    self.assertEqual([b""loc:@c""], c.op.colocation_groups())\n\n  @test_util.run_deprecated_v1\n  def testColocateWithInitialNoneThenNested(self):\n    a = constant_op.constant([2.0], name=""a"")\n    with ops.colocate_with(a.op):\n      with ops.colocate_with(None, ignore_existing=True):\n        b = constant_op.constant(3.0, name=""b"")\n        with ops.colocate_with(b.op):\n          c = constant_op.constant(4.0, name=""c"")\n    self.assertEqual([b""loc:@b""], b.op.colocation_groups())\n    self.assertEqual([b""loc:@b""], c.op.colocation_groups())\n\n  @test_util.run_deprecated_v1\n  def testColocateVariables(self):\n    a = variables.Variable([2.0], name=""a"")\n    with ops.colocate_with(a.op):\n      b = variables.Variable([3.0], name=""b"")\n    self.assertEqual([b""loc:@a""], b.op.colocation_groups())\n\n\nclass DeprecatedTest(test_util.TensorFlowTestCase):\n\n  def testSuccess(self):\n    with ops.Graph().as_default() as g:\n      test_util.set_producer_version(g, 7)\n      old = test_ops.old()\n      with self.session(graph=g):\n        old.run()\n\n  def _error(self):\n    return ((r""Op Old is not available in GraphDef version %d\\. ""\n             r""It has been removed in version 8\\. For reasons\\."") %\n            versions.GRAPH_DEF_VERSION)\n\n  def testGraphConstructionFail(self):\n    with ops.Graph().as_default():\n      with self.assertRaisesRegexp(NotImplementedError, self._error()):\n        test_ops.old()\n\n\nclass DenseTensorLikeTypeTest(test_util.TensorFlowTestCase):\n\n  def testSuccess(self):\n    op = ops.Operation(\n        ops._NodeDef(""FloatOutput"", ""myop""), ops.Graph(), [], [dtypes.float32])\n    t = op.outputs[0]\n    self.assertTrue(ops.is_dense_tensor_like(t))\n\n    v = variables.Variable([17])\n    self.assertTrue(ops.is_dense_tensor_like(v))\n\n  class BadClassNoName(object):\n    pass\n\n  class BadClassBadName(object):\n\n    def name(self):\n      pass\n\n  class BadClassNoDtype(object):\n\n    @property\n    def name(self):\n      pass\n\n  class BadClassBadDtype(object):\n\n    @property\n    def name(self):\n      pass\n\n    def dtype(self):\n      pass\n\n  def testBadClass(self):\n    with self.assertRaisesRegexp(TypeError, ""`name`""):\n      ops.register_dense_tensor_like_type(\n          DenseTensorLikeTypeTest.BadClassNoName)\n    with self.assertRaisesRegexp(TypeError, ""`name`""):\n      ops.register_dense_tensor_like_type(\n          DenseTensorLikeTypeTest.BadClassBadName)\n    with self.assertRaisesRegexp(TypeError, ""`dtype`""):\n      ops.register_dense_tensor_like_type(\n          DenseTensorLikeTypeTest.BadClassNoDtype)\n    with self.assertRaisesRegexp(TypeError, ""`dtype`""):\n      ops.register_dense_tensor_like_type(\n          DenseTensorLikeTypeTest.BadClassBadDtype)\n\n\nclass NameScopeTest(test_util.TensorFlowTestCase):\n\n  def testStripAndPrependScope(self):\n    strs = [\n        ""hidden1/hidden1/weights"",  # Same prefix. Should strip.\n        ""hidden1///hidden1/weights"",  # Extra ""/"". Should strip.\n        ""^hidden1/hidden1/weights"",  # Same prefix. Should strip.\n        ""loc:@hidden1/hidden1/weights"",  # Same prefix. Should strip.\n        ""hhidden1/hidden1/weights"",  # Different prefix. Should keep.\n        ""hidden1""\n    ]  # Not a prefix. Should keep.\n    expected_striped = [\n        ""hidden1/weights"", ""hidden1/weights"", ""^hidden1/weights"",\n        ""loc:@hidden1/weights"", ""hhidden1/hidden1/weights"", ""hidden1""\n    ]\n    expected_prepended = [\n        ""hidden2/hidden1/weights"", ""hidden2/hidden1/weights"",\n        ""^hidden2/hidden1/weights"", ""loc:@hidden2/hidden1/weights"",\n        ""hidden2/hhidden1/hidden1/weights"", ""hidden2/hidden1""\n    ]\n    name_scope_to_strip = ""hidden1""\n    name_scope_to_add = ""hidden2""\n    for es, ep, s in zip(expected_striped, expected_prepended, strs):\n      striped = ops.strip_name_scope(s, name_scope_to_strip)\n      self.assertEqual(es, striped)\n      self.assertEqual(ep, ops.prepend_name_scope(striped, name_scope_to_add))\n\n  def testGetNameScope(self):\n    with ops.Graph().as_default() as g:\n      with ops.name_scope(""scope1""):\n        with ops.name_scope(""scope2""):\n          with ops.name_scope(""scope3""):\n            self.assertEqual(""scope1/scope2/scope3"", g.get_name_scope())\n          self.assertEqual(""scope1/scope2"", g.get_name_scope())\n        self.assertEqual(""scope1"", g.get_name_scope())\n      self.assertEqual("""", g.get_name_scope())\n\n  def testTwoGraphs(self):\n\n    def f():\n      g1 = ops.Graph()\n      g2 = ops.Graph()\n      with g1.as_default():\n        with g2.as_default():\n          with ops.name_scope(""_""):\n            pass\n\n    self.assertRaisesRegexp(ValueError, ""\'_\' is not a valid scope name"", f)\n\n\nclass TracebackTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testTracebackWithStartLines(self):\n    with self.cached_session() as sess:\n      a = constant_op.constant(2.0)\n      sess.run(\n          a,\n          options=config_pb2.RunOptions(\n              trace_level=config_pb2.RunOptions.FULL_TRACE))\n      self.assertTrue(sess.graph.get_operations())\n\n      # Tests that traceback_with_start_lines is the same as traceback\n      # but includes one more element at the end.\n      for op in sess.graph.get_operations():\n        self.assertEquals(len(op.traceback), len(op.traceback_with_start_lines))\n        for frame, frame_with_start_line in zip(\n            op.traceback, op.traceback_with_start_lines):\n          self.assertEquals(5, len(frame_with_start_line))\n          self.assertEquals(frame, frame_with_start_line[:-1])\n\n\nclass EnableEagerExecutionTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_v1_only(""b/120545219"")\n  def testBadArgumentsToEnableEagerExecution(self):\n    with self.assertRaisesRegexp(TypeError, ""config must be a tf.ConfigProto""):\n      ops.enable_eager_execution(context.DEVICE_PLACEMENT_SILENT)\n    with self.assertRaisesRegexp(ValueError, ""device_policy must be one of""):\n      c = config_pb2.ConfigProto()\n      ops.enable_eager_execution(c, c)\n    with self.assertRaisesRegexp(ValueError, ""execution_mode must be one of""):\n      c = config_pb2.ConfigProto()\n      ops.enable_eager_execution(c, execution_mode=c)\n\n\nif __name__ == ""__main__"":\n  googletest.main()\n'"
test/TensorFlowNET.UnitTest/python/train_saver.py,5,"b'\xef\xbb\xbf\nimport tensorflow as tf\n\n# Create some variables.\nv1 = tf.get_variable(""v1"", shape=[3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(""v2"", shape=[5], initializer = tf.zeros_initializer)\n\ninc_v1 = v1.assign(v1+1)\ndec_v2 = v2.assign(v2-1)\n\n# Add an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, initialize the variables, do some work, and save the\n# variables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  inc_v1.op.run()\n  dec_v2.op.run()\n  # Save the variables to disk.\n  save_path = saver.save(sess, ""/tmp/model.ckpt"")\n  print(""Model saved in path: %s"" % save_path)\n'"
