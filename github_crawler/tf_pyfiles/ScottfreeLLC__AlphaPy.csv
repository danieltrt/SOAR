file_path,api_count,code
setup.py,0,"b' #!/usr/bin/env python\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nDISTNAME = \'alphapy\'\nDESCRIPTION = ""AlphaPy: A Machine Learning Pipeline for Speculators""\nLONG_DESCRIPTION = ""alphapy is a Python library for machine learning using scikit-learn. We have a stock market pipeline and a sports pipeline so that speculators can test predictive models, along with functions for trading systems and portfolio management.""\n\nMAINTAINER = \'ScottFree LLC [Robert D. Scott II, Mark Conway]\'\nMAINTAINER_EMAIL = \'scottfree.analytics@scottfreellc.com\'\nURL = ""https://github.com/ScottFreeLLC/AlphaPy""\nLICENSE = ""Apache License, Version 2""\nVERSION = ""2.4.2""\n\nclassifiers = [\'Development Status :: 4 - Beta\',\n               \'Programming Language :: Python\',\n               \'Programming Language :: Python :: 3\',\n               \'Programming Language :: Python :: 3.6\',\n               \'Programming Language :: Python :: 3.7\',\n               \'License :: OSI Approved :: Apache Software License\',\n               \'Intended Audience :: Science/Research\',\n               \'Topic :: Scientific/Engineering\',\n               \'Topic :: Scientific/Engineering :: Mathematics\',\n               \'Operating System :: OS Independent\']\n\ninstall_reqs = [\n    \'arrow>=0.13\',\n    \'bokeh>=1.3\',\n    \'category_encoders>=2.1\',\n    \'iexfinance>=0.4.3\',\n    \'imbalanced-learn>=0.5\',\n    \'ipython>=7.2\',\n    \'keras>=2.2\',\n    \'matplotlib>=3.0\',\n    \'numpy>=1.17\',\n    \'pandas>=1.0\',\n    \'pandas-datareader>=0.8\',\n    \'pyfolio>=0.9\',\n    \'pyyaml>=5.0\',\n    \'scikit-learn>=0.22\',\n    \'scipy>=1.1\',\n    \'seaborn>=0.9\',\n    \'tensorflow>=1.15\',\n]\n\nif __name__ == ""__main__"":\n    setup(\n        name=DISTNAME,\n        version=VERSION,\n        maintainer=MAINTAINER,\n        maintainer_email=MAINTAINER_EMAIL,\n        description=DESCRIPTION,\n        license=LICENSE,\n        url=URL,\n        long_description=LONG_DESCRIPTION,\n        packages=find_packages(),\n        classifiers=classifiers,\n        install_requires=install_reqs,\n        entry_points={\n            \'console_scripts\': [\n                \'alphapy = alphapy.__main__:main\',\n                \'mflow = alphapy.market_flow:main\',\n                \'sflow = alphapy.sport_flow:main\',\n            ],\n        }\n    )\n'"
alphapy/__init__.py,0,b''
alphapy/__main__.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : __main__\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.data import get_data\nfrom alphapy.data import sample_data\nfrom alphapy.data import shuffle_data\nfrom alphapy.estimators import get_estimators\nfrom alphapy.estimators import scorers\nfrom alphapy.features import apply_transforms\nfrom alphapy.features import create_crosstabs\nfrom alphapy.features import create_features\nfrom alphapy.features import create_interactions\nfrom alphapy.features import drop_features\nfrom alphapy.features import remove_lv_features\nfrom alphapy.features import save_features\nfrom alphapy.features import select_features\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import CSEP, PSEP, SSEP, USEP\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Partition, datasets\nfrom alphapy.globals import WILDCARD\nfrom alphapy.model import first_fit\nfrom alphapy.model import generate_metrics\nfrom alphapy.model import get_model_config\nfrom alphapy.model import load_feature_map\nfrom alphapy.model import load_predictor\nfrom alphapy.model import make_predictions\nfrom alphapy.model import Model\nfrom alphapy.model import predict_best\nfrom alphapy.model import predict_blend\nfrom alphapy.model import save_model\nfrom alphapy.model import save_predictions\nfrom alphapy.optimize import hyper_grid_search\nfrom alphapy.optimize import rfecv_search\nfrom alphapy.plots import generate_plots\nfrom alphapy.utilities import get_datestamp\n\nimport argparse\nfrom datetime import datetime\nimport logging\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport warnings\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function training_pipeline\n#\n\ndef training_pipeline(model):\n    r""""""AlphaPy Training Pipeline\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object for controlling the pipeline.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The final results are stored in the model object.\n\n    Raises\n    ------\n    KeyError\n        If the number of columns of the train and test data do not match,\n        then this exception is raised.\n\n    """"""\n\n    logger.info(""Training Pipeline"")\n\n    # Unpack the model specifications\n\n    calibration = model.specs[\'calibration\']\n    directory = model.specs[\'directory\']\n    drop = model.specs[\'drop\']\n    extension = model.specs[\'extension\']\n    feature_selection = model.specs[\'feature_selection\']\n    grid_search = model.specs[\'grid_search\']\n    model_type = model.specs[\'model_type\']\n    rfe = model.specs[\'rfe\']\n    sampling = model.specs[\'sampling\']\n    scorer = model.specs[\'scorer\']\n    seed = model.specs[\'seed\']\n    separator = model.specs[\'separator\']\n    split = model.specs[\'split\']\n    target = model.specs[\'target\']\n\n    # Get train and test data\n\n    X_train, y_train = get_data(model, Partition.train)\n    X_test, y_test = get_data(model, Partition.test)\n\n    # If there is no test partition, then we will split the train partition\n\n    if X_test.empty:\n        logger.info(""No Test Data Found"")\n        logger.info(""Splitting Training Data"")\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_train, y_train, test_size=split, random_state=seed)\n\n    # Determine if there are any test labels\n\n    if y_test.any():\n        logger.info(""Test Labels Found"")\n        model.test_labels = True\n    model = save_features(model, X_train, X_test, y_train, y_test)\n\n    # Log feature statistics\n\n    logger.info(""Original Feature Statistics"")\n    logger.info(""Number of Training Rows    : %d"", X_train.shape[0])\n    logger.info(""Number of Training Columns : %d"", X_train.shape[1])\n    if model_type == ModelType.classification:\n        uv, uc = np.unique(y_train, return_counts=True)\n        logger.info(""Unique Training Values for %s : %s"", target, uv)\n        logger.info(""Unique Training Counts for %s : %s"", target, uc)\n    logger.info(""Number of Testing Rows     : %d"", X_test.shape[0])\n    logger.info(""Number of Testing Columns  : %d"", X_test.shape[1])\n    if model_type == ModelType.classification and model.test_labels:\n        uv, uc = np.unique(y_test, return_counts=True)\n        logger.info(""Unique Testing Values for %s : %s"", target, uv)\n        logger.info(""Unique Testing Counts for %s : %s"", target, uc)\n\n    # Merge training and test data\n\n    if X_train.shape[1] == X_test.shape[1]:\n        split_point = X_train.shape[0]\n        X_all = pd.concat([X_train, X_test])\n    else:\n        raise IndexError(""The number of training and test columns [%d, %d] must match."" %\n                         (X_train.shape[1], X_test.shape[1]))\n\n    # Apply transforms to the feature matrix\n    X_all = apply_transforms(model, X_all)\n\n    # Drop features\n    X_all = drop_features(X_all, drop)\n\n    # Save the train and test files with extracted and dropped features\n\n    datestamp = get_datestamp()\n    data_dir = SSEP.join([directory, \'input\'])\n    df_train = X_all.iloc[:split_point, :]\n    df_train[target] = y_train\n    output_file = USEP.join([model.train_file, datestamp])\n    write_frame(df_train, data_dir, output_file, extension, separator, index=False)\n    df_test = X_all.iloc[split_point:, :]\n    if y_test.any():\n        df_test[target] = y_test\n    output_file = USEP.join([model.test_file, datestamp])\n    write_frame(df_test, data_dir, output_file, extension, separator, index=False)\n\n    # Create crosstabs for any categorical features\n\n    if model_type == ModelType.classification:\n        create_crosstabs(model)\n\n    # Create initial features\n\n    X_all = create_features(model, X_all, X_train, X_test, y_train)\n    X_train, X_test = np.array_split(X_all, [split_point])\n    model = save_features(model, X_train, X_test)\n\n    # Generate interactions\n\n    X_all = create_interactions(model, X_all)\n    X_train, X_test = np.array_split(X_all, [split_point])\n    model = save_features(model, X_train, X_test)\n\n    # Remove low-variance features\n\n    X_all = remove_lv_features(model, X_all)\n    X_train, X_test = np.array_split(X_all, [split_point])\n    model = save_features(model, X_train, X_test)\n\n    # Shuffle the data [if specified]\n    model = shuffle_data(model)\n\n    # Oversampling or Undersampling [if specified]\n\n    if model_type == ModelType.classification:\n        if sampling:\n            model = sample_data(model)\n        else:\n            logger.info(""Skipping Sampling"")\n\n    # Perform feature selection, independent of algorithm\n\n    if feature_selection:\n        model = select_features(model)\n\n    # Get the available classifiers and regressors \n\n    logger.info(""Getting All Estimators"")\n    estimators = get_estimators(model)\n\n    # Get the available scorers\n\n    if scorer not in scorers:\n        raise KeyError(""Scorer function %s not found"" % scorer)\n\n    # Model Selection\n\n    logger.info(""Selecting Models"")\n\n    for algo in model.algolist:\n        logger.info(""Algorithm: %s"", algo)\n        # select estimator\n        try:\n            estimator = estimators[algo]\n            est = estimator.estimator\n        except KeyError:\n            logger.info(""Algorithm %s not found"", algo)\n        # initial fit\n        model = first_fit(model, algo, est)\n        # copy feature name master into feature names per algorithm\n        model.fnames_algo[algo] = model.feature_names\n        # recursive feature elimination\n        if rfe:\n            has_coef = hasattr(est, ""coef_"")\n            has_fimp = hasattr(est, ""feature_importances_"")\n            if has_coef or has_fimp:\n                model = rfecv_search(model, algo)\n            else:\n                logger.info(""No RFE Available for %s"", algo)\n        # grid search\n        if grid_search:\n            model = hyper_grid_search(model, estimator)\n        # predictions\n        model = make_predictions(model, algo, calibration)\n\n    # Create a blended estimator\n\n    if len(model.algolist) > 1:\n        model = predict_blend(model)\n\n    # Generate metrics\n\n    model = generate_metrics(model, Partition.train)\n    model = generate_metrics(model, Partition.test)\n\n    # Store the best estimator\n    model = predict_best(model)\n\n    # Generate plots\n\n    generate_plots(model, Partition.train)\n    if model.test_labels:\n        generate_plots(model, Partition.test)\n\n    # Save best features and predictions\n    save_model(model, \'BEST\', Partition.test)\n\n    # Return the model\n    return model\n\n\n#\n# Function prediction_pipeline\n#\n\ndef prediction_pipeline(model):\n    r""""""AlphaPy Prediction Pipeline\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object for controlling the pipeline.\n\n    Returns\n    -------\n    None : None\n\n    Notes\n    -----\n    The saved model is loaded from disk, and predictions are made\n    on the new testing data.\n\n    """"""\n\n    logger.info(""Predict Mode"")\n\n    # Unpack the model specifications\n\n    directory = model.specs[\'directory\']\n    drop = model.specs[\'drop\']\n    feature_selection = model.specs[\'feature_selection\']\n    model_type = model.specs[\'model_type\']\n    rfe = model.specs[\'rfe\']\n\n    # Get all data. We need original train and test for encodings.\n\n    X_train, y_train = get_data(model, Partition.train)\n\n    partition = Partition.predict\n    X_predict, _ = get_data(model, partition)\n\n    # Load feature_map\n    model = load_feature_map(model, directory)\n\n    # Log feature statistics\n\n    logger.info(""Feature Statistics"")\n    logger.info(""Number of Prediction Rows    : %d"", X_predict.shape[0])\n    logger.info(""Number of Prediction Columns : %d"", X_predict.shape[1])\n\n    # Apply transforms to the feature matrix\n    X_all = apply_transforms(model, X_predict)\n\n    # Drop features\n    X_all = drop_features(X_all, drop)\n\n    # Create initial features\n    X_all = create_features(model, X_all, X_train, X_predict, y_train)\n\n    # Generate interactions\n    X_all = create_interactions(model, X_all)\n\n    # Remove low-variance features\n    X_all = remove_lv_features(model, X_all)\n\n    # Load the univariate support vector, if any\n\n    if feature_selection:\n        logger.info(""Getting Univariate Support"")\n        try:\n            support = model.feature_map[\'uni_support\']\n            X_all = X_all[:, support]\n            logger.info(""New Feature Count : %d"", X_all.shape[1])\n        except:\n            logger.info(""No Univariate Support"")\n\n    # Load the RFE support vector, if any\n\n    if rfe:\n        logger.info(""Getting RFE Support"")\n        try:\n            support = model.feature_map[\'rfe_support\']\n            X_all = X_all[:, support]\n            logger.info(""New Feature Count : %d"", X_all.shape[1])\n        except:\n            logger.info(""No RFE Support"")\n\n    # Load predictor\n    predictor = load_predictor(directory)\n\n    # Make predictions\n    \n    logger.info(""Making Predictions"")\n    tag = \'BEST\'\n    model.preds[(tag, partition)] = predictor.predict(X_all)\n    if model_type == ModelType.classification:\n        model.probas[(tag, partition)]  = predictor.predict_proba(X_all)[:, 1]\n\n    # Save predictions\n    save_predictions(model, tag, partition)\n\n    # Return the model\n    return model\n\n\n#\n# Function main_pipeline\n#\n\ndef main_pipeline(model):\n    r""""""AlphaPy Main Pipeline\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model specifications for the pipeline.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The final model.\n\n    """"""\n\n    # Extract any model specifications\n    predict_mode = model.specs[\'predict_mode\']\n\n    # Prediction Only or Calibration\n\n    if predict_mode:\n        model = prediction_pipeline(model)\n    else:\n        model = training_pipeline(model)\n\n    # Return the completed model\n    return model\n\n\n#\n# Function main\n#\n\ndef main(args=None):\n    r""""""AlphaPy Main Program\n\n    Notes\n    -----\n    (1) Initialize logging.\n    (2) Parse the command line arguments.\n    (3) Get the model configuration.\n    (4) Create the model object.\n    (5) Call the main AlphaPy pipeline.\n\n    """"""\n\n    # Suppress Warnings\n\n    warnings.simplefilter(action=\'ignore\', category=DeprecationWarning)\n    warnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\n    # Logging\n\n    logging.basicConfig(format=""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                        filename=""alphapy.log"", filemode=\'a\', level=logging.INFO,\n                        datefmt=\'%m/%d/%y %H:%M:%S\')\n    formatter = logging.Formatter(""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                                  datefmt=\'%m/%d/%y %H:%M:%S\')\n    console = logging.StreamHandler()\n    console.setFormatter(formatter)\n    console.setLevel(logging.INFO)\n    logging.getLogger().addHandler(console)\n\n    # Start the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""AlphaPy Start"")\n    logger.info(\'*\'*80)\n\n    # Argument Parsing\n\n    parser = argparse.ArgumentParser(description=""AlphaPy Parser"")\n    parser.add_mutually_exclusive_group(required=False)\n    parser.add_argument(\'--predict\', dest=\'predict_mode\', action=\'store_true\')\n    parser.add_argument(\'--train\', dest=\'predict_mode\', action=\'store_false\')\n    parser.set_defaults(predict_mode=False)\n    args = parser.parse_args()\n\n    # Read configuration file\n\n    specs = get_model_config()\n    specs[\'predict_mode\'] = args.predict_mode\n\n    # Create directories if necessary\n\n    output_dirs = [\'config\', \'data\', \'input\', \'model\', \'output\', \'plots\']\n    for od in output_dirs:\n        output_dir = SSEP.join([specs[\'directory\'], od])\n        if not os.path.exists(output_dir):\n            logger.info(""Creating directory %s"", output_dir)\n            os.makedirs(output_dir)\n\n    # Create a model from the arguments\n\n    logger.info(""Creating Model"")\n    model = Model(specs)\n\n    # Start the pipeline\n\n    logger.info(""Calling Pipeline"")\n    model = main_pipeline(model)\n\n    # Complete the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""AlphaPy End"")\n    logger.info(\'*\'*80)\n\n\n#\n# MAIN PROGRAM\n#\n\nif __name__ == ""__main__"":\n    main()\n'"
alphapy/alias.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : alias\n# Created   : July 11, 2013\n#\n# Copyright 2017 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nimport logging\nimport parser\nimport re\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Class Alias\n#\n\nclass Alias(object):\n    """"""Create a new alias as a key-value pair. All aliases are stored\n    in ``Alias.aliases``. Duplicate keys or values are not allowed,\n    unless the ``replace`` parameter is ``True``.\n\n    Parameters\n    ----------\n    name : str\n        Alias key.\n    expr : str\n        Alias value.\n    replace : bool, optional\n        Replace the current key-value pair if it already exists.\n\n    Attributes\n    ----------\n    Alias.aliases : dict\n        Class variable for storing all known aliases\n\n    Examples\n    --------\n    \n    >>> Alias(\'atr\', \'ma_truerange\')\n    >>> Alias(\'hc\', \'higher_close\')\n\n    """"""\n\n    # class variable to track all aliases\n\n    aliases = {}\n\n    # function __new__\n\n    def __new__(cls,\n                name,\n                expr,\n                replace = False):\n        # code\n        efound = expr in [Alias.aliases[key] for key in Alias.aliases]\n        if efound == True:\n            key = [key for key, aexpr in list(Alias.aliases.items()) if aexpr == expr]\n            logger.info(""Expression %s already exists for key %s"", expr, key)\n            return\n        else:\n            if replace == True or not name in Alias.aliases:\n                identifier = re.compile(r""^[^\\d\\W]\\w*\\Z"", re.UNICODE)\n                result1 = re.match(identifier, name)\n                if result1 is None:\n                    logger.info(""Invalid alias key: %s"", name)\n                    return\n                result2 = re.match(identifier, expr)\n                if result2 is None:\n                    logger.info(""Invalid alias expression: %s"", expr)\n                    return\n                return super(Alias, cls).__new__(cls)\n            else:\n                logger.info(""Key %s already exists"", name)\n\n    # function __init__\n\n    def __init__(self,\n                 name,\n                 expr,\n                 replace = False):\n        # code\n        self.name = name;\n        self.expr = expr;\n        # add key with expression\n        Alias.aliases[name] = expr\n            \n    # function __str__\n\n    def __str__(self):\n        return self.expr\n\n    \n#\n# Function get_alias\n#\n\ndef get_alias(alias):\n    r""""""Find an alias value with the given key.\n\n    Parameters\n    ----------\n    alias : str\n        Key for finding the alias value.\n\n    Returns\n    -------\n    alias_value : str\n        Value for the corresponding key.\n\n    Examples\n    --------\n\n    >>> alias_value = get_alias(\'atr\')\n    >>> alias_value = get_alias(\'hc\')\n\n    """"""\n    if alias in Alias.aliases:\n        return Alias.aliases[alias]\n    else:\n        return None\n'"
alphapy/analysis.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : analysis\n# Created   : July 11, 2013\n#\n# Copyright 2019 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.__main__ import main_pipeline\nfrom alphapy.frame import load_frames\nfrom alphapy.frame import sequence_frame\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import SSEP, TAG_ID, USEP\nfrom alphapy.utilities import subtract_days\n\nfrom datetime import timedelta\nimport logging\nimport pandas as pd\nfrom pandas.tseries.offsets import BDay\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function analysis_name\n#\n\ndef analysis_name(gname, target):\n    r""""""Get the name of the analysis.\n\n    Parameters\n    ----------\n    gname : str\n        Group name.\n    target : str\n        Target of the analysis.\n\n    Returns\n    -------\n    name : str\n        Value for the corresponding key.\n\n    """"""\n    name = USEP.join([gname, target])\n    return name\n\n\n#\n# Class Analysis\n#\n\nclass Analysis(object):\n    """"""Create a new analysis for a group. All analyses are stored\n    in ``Analysis.analyses``. Duplicate keys are not allowed.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object for the analysis.\n    group : alphapy.Group\n        The group of members in the analysis.\n\n    Attributes\n    ----------\n    Analysis.analyses : dict\n        Class variable for storing all known analyses\n\n    """"""\n\n    analyses = {}\n\n    # __new__\n\n    def __new__(cls,\n                model,\n                group):\n        # set analysis name\n        name = model.specs[\'directory\'].split(SSEP)[-1]\n        target = model.specs[\'target\']\n        an = analysis_name(name, target)\n        if not an in Analysis.analyses:\n            return super(Analysis, cls).__new__(cls)\n        else:\n            logger.info(""Analysis %s already exists"", an)\n\n    # function __init__\n\n    def __init__(self,\n                 model,\n                 group):\n        # set analysis name\n        name = model.specs[\'directory\'].split(SSEP)[-1]\n        target = model.specs[\'target\']\n        an = analysis_name(name, target)\n        # initialize analysis\n        self.name = an\n        self.model = model\n        self.group = group\n        # add analysis to analyses list\n        Analysis.analyses[an] = self\n\n    # __str__\n\n    def __str__(self):\n        return self.name\n\n\n#\n# Function run_analysis\n#\n\ndef run_analysis(analysis, lag_period, forecast_period, leaders,\n                 predict_history, splits=True):\n    r""""""Run an analysis for a given model and group.\n\n    First, the data are loaded for each member of the analysis group.\n    Then, the target value is lagged for the ``forecast_period``, and\n    any ``leaders`` are lagged as well. Each frame is split along\n    the ``predict_date`` from the ``analysis``, and finally the\n    train and test files are generated.\n\n    Parameters\n    ----------\n    analysis : alphapy.Analysis\n        The analysis to run.\n    lag_period : int\n        The number of lagged features for the analysis.\n    forecast_period : int\n        The period for forecasting the target of the analysis.\n    leaders : list\n        The features that are contemporaneous with the target.\n    predict_history : int\n        The number of periods required for lookback calculations.\n    splits : bool, optional\n        If ``True``, then the data for each member of the analysis\n        group are in separate files.\n\n    Returns\n    -------\n    analysis : alphapy.Analysis\n        The completed analysis.\n\n    """"""\n\n    # Unpack analysis\n\n    name = analysis.name\n    model = analysis.model\n    group = analysis.group\n\n    # Unpack model data\n\n    predict_file = model.predict_file\n    test_file = model.test_file\n    train_file = model.train_file\n\n    # Unpack model specifications\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    predict_date = model.specs[\'predict_date\']\n    predict_mode = model.specs[\'predict_mode\']\n    separator = model.specs[\'separator\']\n    target = model.specs[\'target\']\n    train_date = model.specs[\'train_date\']\n\n    # Calculate split date\n    logger.info(""Analysis Dates"")\n    split_date = subtract_days(predict_date, predict_history)\n\n    # Load the data frames\n    data_frames = load_frames(group, directory, extension, separator, splits)\n\n    # Create dataframes\n\n    if predict_mode:\n        # create predict frame\n        logger.info(""Split Date for Prediction Mode: %s"", split_date)\n        predict_frame = pd.DataFrame()\n    else:\n        # create train and test frames\n        logger.info(""Split Date for Training Mode: %s"", predict_date)\n        train_frame = pd.DataFrame()\n        test_frame = pd.DataFrame()\n\n    # Subset each individual frame and add to the master frame\n\n    leaders.extend([TAG_ID])\n    for df in data_frames:\n        try:\n            tag = df[TAG_ID].unique()[0]\n        except:\n            tag = \'Unknown\'\n        first_date = df.index[0]\n        last_date = df.index[-1]\n        logger.info(""Analyzing %s from %s to %s"", tag, first_date, last_date)\n        # sequence leaders, laggards, and target(s)\n        df = sequence_frame(df, target, forecast_period, leaders, lag_period)\n        # get frame subsets\n        if predict_mode:\n            new_predict = df.loc[(df.index >= split_date) & (df.index <= last_date)]\n            if len(new_predict) > 0:\n                predict_frame = predict_frame.append(new_predict)\n            else:\n                logger.info(""Prediction frame %s has zero rows. Check prediction date."",\n                            tag)\n        else:\n            # split data into train and test\n            new_train = df.loc[(df.index >= train_date) & (df.index < predict_date)]\n            if len(new_train) > 0:\n                new_train = new_train.dropna()\n                train_frame = train_frame.append(new_train)\n                new_test = df.loc[(df.index >= predict_date) & (df.index <= last_date)]\n                if len(new_test) > 0:\n                    # check if target column has NaN values\n                    nan_count = df[target].isnull().sum()\n                    forecast_check = forecast_period - 1\n                    if nan_count != forecast_check:\n                        logger.info(""%s has %d records with NaN targets"", tag, nan_count)\n                    # drop records with NaN values in target column\n                    new_test = new_test.dropna(subset=[target])\n                    # append selected records to the test frame\n                    test_frame = test_frame.append(new_test)\n                else:\n                    logger.info(""Testing frame %s has zero rows. Check prediction date."",\n                                tag)\n            else:\n                logger.info(""Training frame %s has zero rows. Check data source."", tag)\n\n    # Write out the frames for input into the AlphaPy pipeline\n\n    directory = SSEP.join([directory, \'input\'])\n    if predict_mode:\n        # write out the predict frame\n        write_frame(predict_frame, directory, predict_file, extension, separator,\n                    index=True, index_label=\'date\')\n    else:\n        # write out the train and test frames\n        write_frame(train_frame, directory, train_file, extension, separator,\n                    index=True, index_label=\'date\')\n        write_frame(test_frame, directory, test_file, extension, separator,\n                    index=True, index_label=\'date\')\n\n    # Run the AlphaPy pipeline\n    analysis.model = main_pipeline(model)\n\n    # Return the analysis\n    return analysis\n'"
alphapy/calendrical.py,0,"b'################################################################################\n#\n# Package   : calendrical\n# Created   : July 11, 2017\n# Reference : Calendrical Calculations, Cambridge Press, 2002\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nimport calendar\nimport logging\nimport math\nimport pandas as pd\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function expand_dates\n#\n\ndef expand_dates(date_list):\n    expanded_dates = []\n    for item in date_list:\n        if type(item) == str:\n            expanded_dates.append(item)\n        elif type(item) == list:\n            start_date = item[0]\n            end_date = item[1]\n            dates_dt = pd.date_range(start_date, end_date).tolist()\n            dates_str = [x.strftime(\'%Y-%m-%d\') for x in dates_dt]\n            expanded_dates.extend(dates_str)\n        else:\n            logger.info(""Error in date: %s"" % item)\n    return expanded_dates\n\n\n#\n# Function biz_day_month\n#\n\ndef biz_day_month(rdate):\n    r""""""Calculate the business day of the month.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n\n    Returns\n    -------\n    bdm : int\n        Business day of month.\n    """"""\n\n    gyear, gmonth, _ = rdate_to_gdate(rdate)\n    rdate1 = gdate_to_rdate(gyear, gmonth, 1)\n\n    bdm = 0\n    index_date = rdate1\n    while index_date <= rdate:\n        dw = day_of_week(index_date)\n        week_day = dw >= 1 and dw <= 5\n        if week_day:\n            bdm += 1\n        index_date += 1\n\n    holidays = set_holidays(gyear, True)\n    for h in holidays:\n        holiday = holidays[h]\n        in_period = holiday >= rdate1 and holiday <= rdate\n        dwh = day_of_week(holiday)\n        week_day = dwh >= 1 and dwh <= 5\n        if in_period and week_day:\n            bdm -= 1\n    return bdm\n\n\n#\n# Function biz_day_week\n#\n\ndef biz_day_week(rdate):\n    r""""""Calculate the business day of the week.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n\n    Returns\n    -------\n    bdw : int\n        Business day of week.\n    """"""\n\n    gyear, _, _ = rdate_to_gdate(rdate)\n    dw = day_of_week(rdate)\n    week_day = dw >= 1 and dw <= 5\n\n    bdw = 0\n    if week_day:\n        rdate1 = rdate - dw + 1\n        rdate2 = rdate - 1\n        holidays = set_holidays(gyear, True)\n        for h in holidays:\n            holiday = holidays[h]\n            in_period = holiday >= rdate1 and holiday <= rdate2\n            if in_period:\n                bdw -= 1\n    return bdw\n\n\n#\n# Function day_of_week\n#\n\ndef day_of_week(rdate):\n    r""""""Get the ordinal day of the week.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n\n    Returns\n    -------\n    dw : int\n        Ordinal day of the week.\n    """"""\n    dw = rdate % 7\n    return dw\n\n\n#\n# Function day_of_year\n#\n\ndef day_of_year(gyear, gmonth, gday):\n    r""""""Calculate the day number of the given calendar year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    dy : int\n        Day number of year in RDate format.\n    """"""\n    dy = subtract_dates(gyear - 1, 12, 31, gyear, gmonth, gday)\n    return dy\n\n\n#\n# Function days_left_in_year\n#\n\ndef days_left_in_year(gyear, gmonth, gday):\n    r""""""Calculate the number of days remaining in the calendar year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    days_left : int\n        Calendar days remaining in RDate format.\n    """"""\n    days_left = subtract_dates(gyear, gmonth, gday, gyear, 12, 31)\n    return days_left\n\n\n#\n# Function first_kday\n#\n\ndef first_kday(k, gyear, gmonth, gday):\n    r""""""Calculate the first kday in RDate format.\n\n    Parameters\n    ----------\n    k : int\n        Day of the week.\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    fkd : int\n        first-kday in RDate format.\n    """"""\n    fkd = nth_kday(1, k, gyear, gmonth, gday)\n    return fkd\n\n\n#\n# Function gdate_to_rdate\n#\n\ndef gdate_to_rdate(gyear, gmonth, gday):\n    r""""""Convert Gregorian date to RDate format.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    rdate : int\n        RDate date format.\n    """"""\n\n    if gmonth <= 2:\n        rfactor = 0\n    elif gmonth > 2 and leap_year(gyear):\n        rfactor = -1\n    else:\n        rfactor = -2\n\n    rdate = 365 * (gyear - 1) \\\n            + math.floor((gyear - 1) / 4) \\\n            - math.floor((gyear - 1) / 100) \\\n            + math.floor((gyear - 1) / 400) \\\n            + math.floor(((367 * gmonth) - 362) / 12) \\\n            + gday + rfactor\n    return(rdate)\n\n\n#\n# Function get_nth_kday_of_month\n#\n    \ndef get_nth_kday_of_month(gday, gmonth, gyear):\n    r""""""Convert Gregorian date to RDate format.\n\n    Parameters\n    ----------\n    gday : int\n        Gregorian day.\n    gmonth : int\n        Gregorian month.\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    nth : int\n        Ordinal number of a given day\'s occurrence within the month,\n        for example, the third Friday of the month.\n    """"""\n\n    this_month = calendar.monthcalendar(gyear, gmonth)\n    nth_kday_tuple = next(((i, e.index(gday)) for i, e in enumerate(this_month) if gday in e), None)\n    tuple_row = nth_kday_tuple[0]\n    tuple_pos = nth_kday_tuple[1]\n    nth = tuple_row + 1\n    if tuple_row > 0 and this_month[0][tuple_pos] == 0:\n        nth -= 1\n    return nth\n\n\n#\n# Function get_rdate\n#\n\ndef get_rdate(row):\n    r""""""Extract RDate from a dataframe.\n\n    Parameters\n    ----------\n    row : pandas.DataFrame\n        Row of a dataframe containing year, month, and day.\n\n    Returns\n    -------\n    rdate : int\n        RDate date format.\n    """"""\n    return gdate_to_rdate(row[\'year\'], row[\'month\'], row[\'day\'])\n\n\n#\n# Function kday_after\n#\n\ndef kday_after(rdate, k):\n    r""""""Calculate the day after a given RDate.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    k : int\n        Day of the week.\n\n    Returns\n    -------\n    kda : int\n        kday-after in RDate format.\n    """"""\n    kda = kday_on_before(rdate + 7, k)\n    return kda\n\n\n#\n# Function kday_before\n#\n\ndef kday_before(rdate, k):\n    r""""""Calculate the day before a given RDate.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    k : int\n        Day of the week.\n\n    Returns\n    -------\n    kdb : int\n        kday-before in RDate format.\n    """"""\n    kdb = kday_on_before(rdate - 1, k)\n    return kdb\n\n\n#\n# Function kday_nearest\n#\n\ndef kday_nearest(rdate, k):\n    r""""""Calculate the day nearest a given RDate.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    k : int\n        Day of the week.\n\n    Returns\n    -------\n    kdn : int\n        kday-nearest in RDate format.\n    """"""\n    kdn = kday_on_before(rdate + 3, k)\n    return kdn\n\n\n#\n# Function kday_on_after\n#\n\ndef kday_on_after(rdate, k):\n    r""""""Calculate the day on or after a given RDate.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    k : int\n        Day of the week.\n\n    Returns\n    -------\n    kdoa : int\n        kday-on-or-after in RDate format.\n    """"""\n    kdoa = kday_on_before(rdate + 6, k)\n    return kdoa\n\n\n#\n# Function kday_on_before\n#\n\ndef kday_on_before(rdate, k):\n    r""""""Calculate the day on or before a given RDate.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    k : int\n        Day of the week.\n\n    Returns\n    -------\n    kdob : int\n        kday-on-or-before in RDate format.\n    """"""\n    kdob = rdate - day_of_week(rdate - k)\n    return kdob\n\n\n#\n# Function last_kday\n#\n\ndef last_kday(k, gyear, gmonth, gday):\n    r""""""Calculate the last kday in RDate format.\n\n    Parameters\n    ----------\n    k : int\n        Day of the week.\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    lkd : int\n        last-kday in RDate format.\n    """"""\n    lkd = nth_kday(-1, k, gyear, gmonth, gday)\n    return lkd\n\n\n#\n# Function leap_year\n#\n\ndef leap_year(gyear):\n    r""""""Determine if this is a Gregorian leap year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    leap_year : bool\n        True if a Gregorian leap year, else False.\n    """"""\n\n    mod1 = (gyear % 4 == 0)\n    mod2 = True\n    if gyear % 100 == 0:\n        mod2 = gyear % 400 == 0\n\n    leap_year = False\n    if mod1 and mod2:\n        leap_year = True\n    return leap_year\n\n\n#\n# Function next_event\n#\n\ndef next_event(rdate, events):\n    r""""""Find the next event after a given date.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    events : list of RDate (int)\n        Monthly events in RDate format.\n\n    Returns\n    -------\n    event : RDate (int)\n        Next event in RDate format.\n    """"""\n    try:\n        event = next(e for e in events if e > rdate)\n    except:\n        event = 0\n    return event\n\n\n#\n# Function next_holiday\n#\n\ndef next_holiday(rdate, holidays):\n    r""""""Find the next holiday after a given date.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    holidays : dict of RDate (int)\n        Holidays in RDate format.\n\n    Returns\n    -------\n    holiday : RDate (int)\n        Next holiday in RDate format.\n    """"""\n    try:\n        holiday = next(h for h in sorted(holidays.values()) if h > rdate)\n    except:\n        holiday = 0\n    return holiday\n\n\n#\n# Function nth_bizday\n#\n\ndef nth_bizday(n, gyear, gmonth):\n    r""""""Calculate the nth business day in a month.\n\n    Parameters\n    ----------\n    n : int\n        Number of the business day to get.\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n\n    Returns\n    -------\n    bizday : int\n        Nth business day of a given month in RDate format.\n    """"""\n\n    rdate = gdate_to_rdate(gyear, gmonth, 1)\n    holidays = set_holidays(gyear, True)\n    ibd = 0\n    idate = rdate\n    while (ibd < n):\n        dw = day_of_week(idate)\n        week_day = dw >= 1 and dw <= 5\n        if week_day and idate not in holidays.values():\n            ibd += 1\n            bizday = idate\n        idate += 1\n    return bizday\n\n\n#\n# Function nth_kday\n#\n\ndef nth_kday(n, k, gyear, gmonth, gday):\n    r""""""Calculate the nth-kday in RDate format.\n\n    Parameters\n    ----------\n    n : int\n        Occurrence of a given day counting in either direction.\n    k : int\n        Day of the week.\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n\n    Returns\n    -------\n    nthkday : int\n        nth-kday in RDate format.\n    """"""\n\n    rdate = gdate_to_rdate(gyear, gmonth, gday)\n    if n > 0:\n        nthkday = 7 * n + kday_before(rdate, k)\n    else:\n        nthkday = 7 * n + kday_after(rdate, k)\n    return nthkday\n\n\n#\n# Function previous_event\n#\n\ndef previous_event(rdate, events):\n    r""""""Find the previous event before a given date.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    events : list of RDate (int)\n        Monthly events in RDate format.\n\n    Returns\n    -------\n    event : RDate (int)\n        Previous event in RDate format.\n    """"""\n    try:\n        event = next(e for e in sorted(events, reverse=True) if e < rdate)\n    except:\n        event = 0\n    return event\n\n\n#\n# Function previous_holiday\n#\n\ndef previous_holiday(rdate, holidays):\n    r""""""Find the previous holiday before a given date.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n    holidays : dict of RDate (int)\n        Holidays in RDate format.\n\n    Returns\n    -------\n    holiday : RDate (int)\n        Previous holiday in RDate format.\n    """"""\n    try:\n        holiday = next(h for h in sorted(holidays.values(), reverse=True) if h < rdate)\n    except:\n        holiday = 0\n    return holiday\n\n\n#\n# Function rdate_to_gdate\n#\n\ndef rdate_to_gdate(rdate):\n    r""""""Convert RDate format to Gregorian date format.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n\n    Returns\n    -------\n    gyear : int\n        Gregorian year.\n    gmonth : int\n        Gregorian month.\n    gday : int\n        Gregorian day.\n    """"""\n\n    gyear = rdate_to_gyear(rdate)\n    priordays = rdate - gdate_to_rdate(gyear, 1, 1)\n    value1 = gdate_to_rdate(gyear, 3, 1)\n    if rdate < value1:\n        correction = 0\n    elif rdate >= value1 and leap_year(gyear):\n        correction = 1\n    else:\n        correction = 2\n    gmonth = math.floor((12 * (priordays + correction) + 373) / 367)\n    gday = rdate - gdate_to_rdate(gyear, gmonth, 1) + 1\n    return gyear, gmonth, gday\n\n\n#\n# Function rdate_to_gyear\n#\n\ndef rdate_to_gyear(rdate):\n    r""""""Convert RDate format to Gregorian year.\n\n    Parameters\n    ----------\n    rdate : int\n        RDate date format.\n\n    Returns\n    -------\n    gyear : int\n        Gregorian year.\n    """"""\n\n    d0 = rdate - 1\n    n400 = math.floor(d0 / 146097)\n    d1 = d0 % 146097\n    n100 = math.floor(d1 / 36524)\n    d2 = d1 % 36524\n    n4 = math.floor(d2 / 1461)\n    d3 = d2 % 1461\n    n1 = math.floor(d3 / 365)\n\n    theyear = 400 * n400 + 100 * n100 + 4 * n4 + n1\n    if n100 == 4 or n1 == 4:\n        gyear = theyear\n    else:\n        gyear = theyear + 1\n    return gyear\n\n\n#\n# Function set_events\n#\n\ndef set_events(n, k, gyear, gday):\n    r""""""Define monthly events for a given year.\n\n    Parameters\n    ----------\n    n : int\n        Occurrence of a given day counting in either direction.\n    k : int\n        Day of the week.\n    gyear : int\n        Gregorian year for the events.\n    gday : int\n        Gregorian day representing the first day to consider.\n\n    Returns\n    -------\n    events : list of RDate (int)\n        Monthly events in RDate format.\n\n    Example\n    -------\n    >>> # Options Expiration (Third Friday of every month)\n    >>> set_events(3, 5, 2017, 1)\n    """"""\n\n    events = []\n    month_range = range(1, 13)\n    for m in month_range:\n        rdate = nth_kday(n, k, gyear, m, gday)\n        events.append(rdate)\n    return events\n\n\n#\n# Function subtract_dates\n#\n\ndef subtract_dates(gyear1, gmonth1, gday1, gyear2, gmonth2, gday2):\n    r""""""Calculate the difference between two Gregorian dates.\n\n    Parameters\n    ----------\n    gyear1 : int\n        Gregorian year of first date.\n    gmonth1 : int\n        Gregorian month of first date.\n    gday1 : int\n        Gregorian day of first date.\n    gyear2 : int\n        Gregorian year of successive date.\n    gmonth2 : int\n        Gregorian month of successive date.\n    gday2 : int\n        Gregorian day of successive date.\n\n    Returns\n    -------\n    delta_days : int\n        Difference in days in RDate format.\n    """"""\n    delta_days = gdate_to_rdate(gyear2, gmonth2, gday2) \\\n                 - gdate_to_rdate(gyear1, gmonth1, gday1)\n    return delta_days\n\n\n\n#\n# Holiday Functions in Calendar Order\n#\n\n\n#\n# Function new_years_day\n#\n\ndef new_years_day(gyear, observed):\n    r""""""Get New Year\'s day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    observed : bool\n        False if the exact date, True if the weekday.\n\n    Returns\n    -------\n    nyday : int\n        New Year\'s Day in RDate format.\n    """"""\n    nyday = gdate_to_rdate(gyear, 1, 1)\n    if observed and day_of_week(nyday) == 0:\n        nyday += 1\n    return nyday\n\n\n#\n# Function mlk_day\n#\n\ndef mlk_day(gyear):\n    r""""""Get Martin Luther King Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    mlkday : int\n        Martin Luther King Day in RDate format.\n    """"""\n    mlkday = nth_kday(3, 1, gyear, 1, 1)\n    return mlkday\n\n\n#\n# Function valentines_day\n#\n\ndef valentines_day(gyear):\n    r""""""Get Valentine\'s day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    valentines : int\n        Valentine\'s Day in RDate format.\n    """"""\n    valentines = gdate_to_rdate(gyear, 2, 14)\n    return valentines\n\n\n#\n# Function presidents_day\n#\n\ndef presidents_day(gyear):\n    r""""""Get President\'s Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    prezday : int\n        President\'s Day in RDate format.\n    """"""\n    prezday = nth_kday(3, 1, gyear, 2, 1)\n    return prezday\n\n\n#\n# Function saint_patricks_day\n#\n\ndef saint_patricks_day(gyear):\n    r""""""Get Saint Patrick\'s day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    observed : bool\n        False if the exact date, True if the weekday.\n\n    Returns\n    -------\n    patricks : int\n        Saint Patrick\'s Day in RDate format.\n    """"""\n    patricks = gdate_to_rdate(gyear, 3, 17)\n    return patricks\n\n\n#\n# Function good_friday\n#\n\ndef good_friday(gyear):\n    r""""""Get Good Friday for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    gf : int\n        Good Friday in RDate format.\n    """"""\n    gf = easter_day(gyear) - 2\n    return gf\n\n\n#\n# Function easter_day\n#\n\ndef easter_day(gyear):\n    r""""""Get Easter Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    ed : int\n        Easter Day in RDate format.\n    """"""\n\n    century = math.floor(gyear / 100) + 1\n    epacts = (14 + 11 * (gyear % 19) - math.floor(3 * century / 4) \\\n             + math.floor((5 + 8 * century) / 25)) % 30\n    if epacts == 0 or (epacts == 1 and 10 < (gyear % 19)):\n        epacta = epacts + 1\n    else:\n        epacta = epacts\n    rdate = gdate_to_rdate(gyear, 4, 19) - epacta\n    ed = kday_after(rdate, 0)\n    return ed\n\n\n#\n# Function cinco_de_mayo\n#\n\ndef cinco_de_mayo(gyear):\n    r""""""Get Cinco de Mayo for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    cinco_de_mayo : int\n        Cinco de Mayo in RDate format.\n    """"""\n    cinco = gdate_to_rdate(gyear, 5, 5)\n    return cinco\n\n\n#\n# Function mothers_day\n#\n\ndef mothers_day(gyear):\n    r""""""Get Mother\'s Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    mothers_day : int\n        Mother\'s Day in RDate format.\n    """"""\n    mothers_day = nth_kday(2, 0, gyear, 5, 1)\n    return mothers_day\n\n\n#\n# Function memorial_day\n#\n\ndef memorial_day(gyear):\n    r""""""Get Memorial Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    md : int\n        Memorial Day in RDate format.\n    """"""\n    md = last_kday(1, gyear, 5, 31)\n    return md\n\n\n#\n# Function fathers_day\n#\n\ndef fathers_day(gyear):\n    r""""""Get Father\'s Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    fathers_day : int\n        Father\'s Day in RDate format.\n    """"""\n    fathers_day = nth_kday(3, 0, gyear, 6, 1)\n    return fathers_day\n\n\n#\n# Function independence_day\n#\n\ndef independence_day(gyear, observed):\n    r""""""Get Independence Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    observed : bool\n        False if the exact date, True if the weekday.\n\n    Returns\n    -------\n    d4j : int\n        Independence Day in RDate format.\n    """"""\n    d4j = gdate_to_rdate(gyear, 7, 4)\n    if observed:\n        if day_of_week(d4j) == 6:\n            d4j -= 1\n        if day_of_week(d4j) == 0:\n            d4j += 1\n    return d4j\n\n\n#\n# Function labor_day\n#\n\ndef labor_day(gyear):\n    r""""""Get Labor Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    lday : int\n        Labor Day in RDate format.\n    """"""\n    lday = first_kday(1, gyear, 9, 1)\n    return lday\n\n\n#\n# Function halloween\n#\n\ndef halloween(gyear):\n    r""""""Get Halloween for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    halloween : int\n        Halloween in RDate format.\n    """"""\n    halloween = gdate_to_rdate(gyear, 10, 31)\n    return halloween\n\n\n#\n# Function veterans_day\n#\n\ndef veterans_day(gyear, observed):\n    r""""""Get Veteran\'s day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    observed : bool\n        False if the exact date, True if the weekday.\n\n    Returns\n    -------\n    veterans : int\n        Veteran\'s Day in RDate format.\n    """"""\n    veterans = gdate_to_rdate(gyear, 11, 11)\n    if observed and day_of_week(veterans) == 0:\n        veterans += 1\n    return veterans\n\n\n#\n# Function thanksgiving_day\n#\n\ndef thanksgiving_day(gyear):\n    r""""""Get Thanksgiving Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n\n    Returns\n    -------\n    tday : int\n        Thanksgiving Day in RDate format.\n    """"""\n    tday = nth_kday(4, 4, gyear, 11, 1)\n    return tday\n\n\n#\n# Function christmas_day\n#\n\ndef christmas_day(gyear, observed):\n    r""""""Get Christmas Day for a given year.\n\n    Parameters\n    ----------\n    gyear : int\n        Gregorian year.\n    observed : bool\n        False if the exact date, True if the weekday.\n\n    Returns\n    -------\n    xmas : int\n        Christmas Day in RDate format.\n    """"""\n    xmas = gdate_to_rdate(gyear, 12, 25)\n    if observed:\n        if day_of_week(xmas) == 6:\n            xmas -= 1\n        if day_of_week(xmas) == 0:\n            xmas += 1\n    return xmas\n\n\n#\n# Define holiday map\n#\n\nholiday_map = {""New Year\'s Day""    : (new_years_day, True),\n               ""MLK Day""           : (mlk_day, False),\n               ""Valentine\'s Day""   : (valentines_day, False),\n               ""President\'s Day""   : (presidents_day, False),\n               ""St. Patrick\'s Day"" : (saint_patricks_day, False),\n               ""Good Friday""       : (good_friday, False),\n               ""Easter""            : (easter_day, False),\n               ""Cinco de Mayo""     : (cinco_de_mayo, False),\n               ""Mother\'s Day""      : (mothers_day, False),\n               ""Memorial Day""      : (memorial_day, False),\n               ""Father\'s Day""      : (fathers_day, False),\n               ""Independence Day""  : (independence_day, True),\n               ""Labor Day""         : (labor_day, False),\n               ""Halloween""         : (halloween, False),\n               ""Veteran\'s Day""     : (veterans_day, True),\n               ""Thanksgiving""      : (thanksgiving_day, False),\n               ""Christmas""         : (christmas_day, True)}\n\n\n#\n# Function get_holiday_names\n#\n\ndef get_holiday_names():\n    r""""""Get the list of defined holidays.\n\n    Returns\n    -------\n    holidays : list of str\n        List of holiday names.\n    """"""\n    holidays = [h for h in holiday_map]\n    return holidays\n\n\n#\n# Function set_holidays\n#\n\ndef set_holidays(gyear, observe):\n    r""""""Determine if this is a Gregorian leap year.\n\n    Parameters\n    ----------\n    gyear : int\n        Value for the corresponding key.\n    observe : bool\n        True to get the observed date, otherwise False.\n\n    Returns\n    -------\n    holidays : dict of int\n        Set of holidays in RDate format for a given year.\n    """"""\n\n    holidays = {}\n    for h in holiday_map:\n        hfunc = holiday_map[h][0]\n        observed = holiday_map[h][1]\n        if observed:\n            holidays[h] = hfunc(gyear, observe)\n        else:\n            holidays[h] = hfunc(gyear)\n    return holidays\n'"
alphapy/data.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : data\n# Created   : July 11, 2013\n#\n# Copyright 2019 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.frame import Frame\nfrom alphapy.frame import frame_name\nfrom alphapy.frame import read_frame\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Partition, datasets\nfrom alphapy.globals import PSEP, SSEP, USEP\nfrom alphapy.globals import SamplingMethod\nfrom alphapy.globals import WILDCARD\nfrom alphapy.space import Space\n\nimport arrow\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom iexfinance.stocks import get_historical_data\nfrom iexfinance.stocks import get_historical_intraday\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.ensemble import BalanceCascade\nfrom imblearn.ensemble import EasyEnsemble\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.under_sampling import CondensedNearestNeighbour\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import NeighbourhoodCleaningRule\nfrom imblearn.under_sampling import OneSidedSelection\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import RepeatedEditedNearestNeighbours\nfrom imblearn.under_sampling import TomekLinks\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\npd.core.common.is_list_like = pd.api.types.is_list_like\nimport pandas_datareader.data as web\nimport re\nimport requests\nfrom scipy import sparse\nfrom sklearn.preprocessing import LabelEncoder\nimport sys\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function get_data\n#\n\ndef get_data(model, partition):\n    r""""""Get data for the given partition.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object describing the data.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    X : pandas.DataFrame\n        The feature set.\n    y : pandas.Series\n        The array of target values, if available.\n\n    """"""\n\n    logger.info(""Loading Data"")\n\n    # Extract the model data\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    features = model.specs[\'features\']\n    model_type = model.specs[\'model_type\']\n    separator = model.specs[\'separator\']\n    target = model.specs[\'target\']\n\n    # Initialize X and y\n\n    X = pd.DataFrame()\n    y = np.empty([0, 0])\n\n    # Read in the file\n\n    filename = datasets[partition]\n    input_dir = SSEP.join([directory, \'input\'])\n    df = read_frame(input_dir, filename, extension, separator)\n\n    # Get features and target\n\n    if not df.empty:\n        if target in df.columns:\n            logger.info(""Found target %s in data frame"", target)\n            # check if target column has NaN values\n            nan_count = df[target].isnull().sum()\n            if nan_count > 0:\n                logger.info(""Found %d records with NaN target values"", nan_count)\n                logger.info(""Labels (y) for %s will not be used"", partition)\n            else:\n                # assign the target column to y\n                y = df[target]\n                # encode label only for classification\n                if model_type == ModelType.classification:\n                    y = LabelEncoder().fit_transform(y)\n                logger.info(""Labels (y) found for %s"", partition)\n            # drop the target from the original frame\n            df = df.drop([target], axis=1)\n        else:\n            logger.info(""Target %s not found in %s"", target, partition)\n        # Extract features\n        if features == WILDCARD:\n            X = df\n        else:\n            X = df[features]\n\n    # Labels are returned usually only for training data\n    return X, y\n\n\n#\n# Function shuffle_data\n#\n\ndef shuffle_data(model):\n    r""""""Randomly shuffle the training data.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object describing the data.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the shuffled data.\n\n    """"""\n\n    # Extract model parameters.\n\n    seed = model.specs[\'seed\']\n    shuffle = model.specs[\'shuffle\']\n\n    # Extract model data.\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Shuffle data\n\n    if shuffle:\n        logger.info(""Shuffling Training Data"")\n        np.random.seed(seed)\n        new_indices = np.random.permutation(y_train.size)\n        model.X_train = X_train[new_indices]\n        model.y_train = y_train[new_indices]\n    else:\n        logger.info(""Skipping Shuffling"")\n\n    return model\n\n\n#\n# Function sample_data\n#\n\ndef sample_data(model):\n    r""""""Sample the training data.\n\n    Sampling is configured in the ``model.yml`` file (data:sampling:method)\n    You can learn more about resampling techniques here [IMB]_.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object describing the data.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the sampled data.\n\n    """"""\n\n    logger.info(""Sampling Data"")\n\n    # Extract model parameters.\n\n    sampling_method = model.specs[\'sampling_method\']\n    sampling_ratio = model.specs[\'sampling_ratio\']\n    target = model.specs[\'target\']\n    target_value = model.specs[\'target_value\']\n\n    # Extract model data.\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Calculate the sampling ratio if one is not provided.\n\n    if sampling_ratio > 0.0:\n        ratio = sampling_ratio\n    else:\n        uv, uc = np.unique(y_train, return_counts=True)\n        target_index = np.where(uv == target_value)[0][0]\n        nontarget_index = np.where(uv != target_value)[0][0]\n        ratio = (uc[nontarget_index] / uc[target_index]) - 1.0\n    logger.info(""Sampling Ratio for target %s [%r]: %f"",\n                target, target_value, ratio)\n\n    # Choose the sampling method.\n\n    if sampling_method == SamplingMethod.under_random:\n        sampler = RandomUnderSampler()\n    elif sampling_method == SamplingMethod.under_tomek:\n        sampler = TomekLinks()\n    elif sampling_method == SamplingMethod.under_cluster:\n        sampler = ClusterCentroids()\n    elif sampling_method == SamplingMethod.under_nearmiss:\n        sampler = NearMiss(version=1)\n    elif sampling_method == SamplingMethod.under_ncr:\n        sampler = NeighbourhoodCleaningRule()\n    elif sampling_method == SamplingMethod.over_random:\n        sampler = RandomOverSampler(ratio=ratio)\n    elif sampling_method == SamplingMethod.over_smote:\n        sampler = SMOTE(ratio=ratio, kind=\'regular\')\n    elif sampling_method == SamplingMethod.over_smoteb:\n        sampler = SMOTE(ratio=ratio, kind=\'borderline1\')\n    elif sampling_method == SamplingMethod.over_smotesv:\n        sampler = SMOTE(ratio=ratio, kind=\'svm\')\n    elif sampling_method == SamplingMethod.overunder_smote_tomek:\n        sampler = SMOTETomek(ratio=ratio)\n    elif sampling_method == SamplingMethod.overunder_smote_enn:\n        sampler = SMOTEENN(ratio=ratio)\n    elif sampling_method == SamplingMethod.ensemble_easy:\n        sampler = EasyEnsemble()\n    elif sampling_method == SamplingMethod.ensemble_bc:\n        sampler = BalanceCascade()\n    else:\n        raise ValueError(""Unknown Sampling Method %s"" % sampling_method)\n\n    # Get the newly sampled features.\n\n    X, y = sampler.fit_sample(X_train, y_train)\n\n    logger.info(""Original Samples : %d"", X_train.shape[0])\n    logger.info(""New Samples      : %d"", X.shape[0])\n\n    # Store the new features in the model.\n\n    model.X_train = X\n    model.y_train = y\n\n    return model\n\n\n#\n# Function convert_data\n#\n\ndef convert_data(df, index_column, intraday_data):\n    r""""""Convert the market data frame to canonical format.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The intraday dataframe.\n    index_column : str\n        The name of the index column.\n    intraday_data : bool\n        Flag set to True if the frame contains intraday data.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The canonical dataframe with date/time index.\n\n    """"""\n\n    # Standardize column names\n    df = df.rename(columns = lambda x: x.lower().replace(\' \',\'\'))\n\n    # Create the time/date index if not already done\n\n    if not isinstance(df.index, pd.DatetimeIndex):\n        df.reset_index(inplace=True)\n        if intraday_data:\n            dt_column = df[\'date\'] + \' \' + df[\'time\']\n        else:\n            dt_column = df[\'date\']\n        df[index_column] = pd.to_datetime(dt_column)\n        df.set_index(pd.DatetimeIndex(df[index_column]),\n                     drop=True, inplace=True)\n        del df[\'date\']\n        if intraday_data:\n            del df[\'time\']\n\n    # Make the remaining columns floating point\n\n    cols_float = [\'open\', \'high\', \'low\', \'close\', \'volume\']\n    df[cols_float] = df[cols_float].astype(float)\n\n    # Order the frame by increasing date if necessary\n    df = df.sort_index()\n\n    return df\n\n\n#\n# Function enhance_intraday_data\n#\n\ndef enhance_intraday_data(df):\n    r""""""Add columns to the intraday dataframe.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The intraday dataframe.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe with bar number and end-of-day columns.\n\n    """"""\n\n    # Group by date first\n\n    df[\'date\'] = df.index.strftime(\'%Y-%m-%d\')\n    date_group = df.groupby(\'date\')\n\n    # Number the intraday bars\n    df[\'bar_number\'] = date_group.cumcount()\n\n    # Mark the end of the trading day\n\n    df[\'end_of_day\'] = False\n    df.loc[date_group.tail(1).index, \'end_of_day\'] = True\n\n    # Return the enhanced frame\n\n    del df[\'date\']\n    return df\n\n\n#\n# Function get_google_intraday_data\n#\n\ndef get_google_intraday_data(symbol, lookback_period, fractal):\n    r""""""Get Google Finance intraday data.\n\n    We get intraday data from the Google Finance API, even though\n    it is not officially supported. You can retrieve a maximum of\n    50 days of history, so you may want to build your own database\n    for more extensive backtesting.\n\n    Parameters\n    ----------\n    symbol : str\n        A valid stock symbol.\n    lookback_period : int\n        The number of days of intraday data to retrieve, capped at 50.\n    fractal : str\n        The intraday frequency, e.g., ""5m"" for 5-minute data.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the intraday data.\n\n    """"""\n\n    # Google requires upper-case symbol, otherwise not found\n    symbol = symbol.upper()\n    # Initialize data frame\n    df = pd.DataFrame()\n    # Convert fractal to interval\n    interval = 60 * int(re.findall(\'\\d+\', fractal)[0])\n    # Google has a 50-day limit\n    max_days = 50\n    if lookback_period > max_days:\n        lookback_period = max_days\n    # Set Google data constants\n    toffset = 7\n    line_length = 6\n    # Make the request to Google\n    base_url = \'https://finance.google.com/finance/getprices?q={}&i={}&p={}d&f=d,o,h,l,c,v\'\n    url = base_url.format(symbol, interval, lookback_period)\n    response = requests.get(url)\n    # Process the response\n    text = response.text.split(\'\\n\')\n    records = []\n    for line in text[toffset:]:\n        items = line.split(\',\')\n        if len(items) == line_length:\n            dt_item = items[0]\n            close_item = items[1]\n            high_item = items[2]\n            low_item = items[3]\n            open_item = items[4]\n            volume_item = items[5]\n            if dt_item[0] == \'a\':\n                day_item = float(dt_item[1:])\n                offset = 0\n            else:\n                offset = float(dt_item)\n            dt = datetime.fromtimestamp(day_item + (interval * offset))\n            dt = pd.to_datetime(dt)\n            dt_date = dt.strftime(\'%Y-%m-%d\')\n            dt_time = dt.strftime(\'%H:%M:%S\')\n            record = (dt_date, dt_time, open_item, high_item, low_item, close_item, volume_item)\n            records.append(record)\n    # Create data frame\n    cols = [\'date\', \'time\', \'open\', \'high\', \'low\', \'close\', \'volume\']\n    df = pd.DataFrame.from_records(records, columns=cols)\n    # Return the dataframe\n    return df\n\n\n#\n# Function get_google_data\n#\n\ndef get_google_data(schema, subschema, symbol, intraday_data, data_fractal,\n                    from_date, to_date, lookback_period):\n    r""""""Get data from Google.\n\n    Parameters\n    ----------\n    schema : str\n        The schema (including any subschema) for this data feed.\n    subschema : str\n        Any subschema for this data feed.\n    symbol : str\n        A valid stock symbol.\n    intraday_data : bool\n        If True, then get intraday data.\n    data_fractal : str\n        Pandas offset alias.\n    from_date : str\n        Starting date for symbol retrieval.\n    to_date : str\n        Ending date for symbol retrieval.\n    lookback_period : int\n        The number of periods of data to retrieve.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the market data.\n\n    """"""\n\n    df = pd.DataFrame()\n    if intraday_data:\n        # use internal function\n        # df = get_google_intraday_data(symbol, lookback_period, data_fractal)\n        logger.info(""Google Finance API for intraday data no longer available"")\n    else:\n        # Google Finance API no longer available\n        logger.info(""Google Finance API for daily data no longer available"")\n    return df\n\n\n#\n# Function get_iex_data\n#\n\ndef get_iex_data(schema, subschema, symbol, intraday_data, data_fractal,\n                 from_date, to_date, lookback_period):\n    r""""""Get data from IEX.\n\n    Parameters\n    ----------\n    schema : str\n        The schema (including any subschema) for this data feed.\n    subschema : str\n        Any subschema for this data feed.\n    symbol : str\n        A valid stock symbol.\n    intraday_data : bool\n        If True, then get intraday data.\n    data_fractal : str\n        Pandas offset alias.\n    from_date : str\n        Starting date for symbol retrieval.\n    to_date : str\n        Ending date for symbol retrieval.\n    lookback_period : int\n        The number of periods of data to retrieve.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the market data.\n\n    """"""\n\n    symbol = symbol.upper()\n    df = pd.DataFrame()\n\n    if intraday_data:\n        # use iexfinance function to get intraday data for each date\n        df = pd.DataFrame()\n        for d in pd.date_range(from_date, to_date):\n            dstr = d.strftime(\'%Y-%m-%d\')\n            logger.info(""%s Data for %s"", symbol, dstr)\n            try:\n                df1 = get_historical_intraday(symbol, d, output_format=""pandas"")\n                df1_len = len(df1)\n                if df1_len > 0:\n                    logger.info(""%s: %d rows"", symbol, df1_len)\n                    df = df.append(df1)\n                else:\n                    logger.info(""%s: No Trading Data for %s"", symbol, dstr)\n            except:\n                iex_error = ""*** IEX Intraday Data Error (check Quota) ***""\n                logger.error(iex_error)\n                sys.exit(iex_error)\n    else:\n        # use iexfinance function for historical daily data\n        try:\n            df = get_historical_data(symbol, from_date, to_date, output_format=""pandas"")\n        except:\n            iex_error = ""*** IEX Daily Data Error (check Quota) ***""\n            logger.error(iex_error)\n            sys.exit(iex_error)\n    return df\n\n\n#\n# Function get_pandas_data\n#\n\ndef get_pandas_data(schema, subschema, symbol, intraday_data, data_fractal,\n                    from_date, to_date, lookback_period):\n    r""""""Get Pandas Web Reader data.\n\n    Parameters\n    ----------\n    schema : str\n        The schema (including any subschema) for this data feed.\n    subschema : str\n        Any subschema for this data feed.\n    symbol : str\n        A valid stock symbol.\n    intraday_data : bool\n        If True, then get intraday data.\n    data_fractal : str\n        Pandas offset alias.\n    from_date : str\n        Starting date for symbol retrieval.\n    to_date : str\n        Ending date for symbol retrieval.\n    lookback_period : int\n        The number of periods of data to retrieve.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the market data.\n\n    """"""\n\n    # Call the Pandas Web data reader.\n\n    try:\n        df = web.DataReader(symbol, schema, from_date, to_date)\n    except:\n        df = pd.DataFrame()\n        logger.info(""Could not retrieve %s data with pandas-datareader"", symbol.upper())\n\n    return df\n\n\n#\n# Function get_quandl_data\n#\n\ndef get_quandl_data(schema, subschema, symbol, intraday_data, data_fractal,\n                    from_date, to_date, lookback_period):\n    r""""""Get Quandl data.\n\n    Parameters\n    ----------\n    schema : str\n        The schema for this data feed.\n    subschema : str\n        Any subschema for this data feed.\n    symbol : str\n        A valid stock symbol.\n    intraday_data : bool\n        If True, then get intraday data.\n    data_fractal : str\n        Pandas offset alias.\n    from_date : str\n        Starting date for symbol retrieval.\n    to_date : str\n        Ending date for symbol retrieval.\n    lookback_period : int\n        The number of periods of data to retrieve.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the market data.\n\n    """"""\n\n    # Quandl is a special case with subfeeds.\n\n    symbol = SSEP.join([subschema.upper(), symbol.upper()])\n\n    # Call the Pandas Web data reader.\n\n    df = get_pandas_data(schema, subschema, symbol, intraday_data, data_fractal,\n                         from_date, to_date, lookback_period)\n\n    return df\n\n\n#\n# Function get_yahoo_data\n#\n\ndef get_yahoo_data(schema, subschema, symbol, intraday_data, data_fractal,\n                    from_date, to_date, lookback_period):\n    r""""""Get Yahoo data.\n\n    Parameters\n    ----------\n    schema : str\n        The schema (including any subschema) for this data feed.\n    subschema : str\n        Any subschema for this data feed.\n    symbol : str\n        A valid stock symbol.\n    intraday_data : bool\n        If True, then get intraday data.\n    data_fractal : str\n        Pandas offset alias.\n    from_date : str\n        Starting date for symbol retrieval.\n    to_date : str\n        Ending date for symbol retrieval.\n    lookback_period : int\n        The number of periods of data to retrieve.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The dataframe containing the market data.\n\n    """"""\n\n    df = pd.DataFrame()\n    if intraday_data:\n        url = \'https://query1.finance.yahoo.com/v8/finance/chart/\'\n        data_range = \'\'.join([str(lookback_period), \'d\'])\n        interval = int(\'\'.join(filter(str.isdigit, data_fractal)))\n        fractal = re.sub(r\'\\d+\', \'\', data_fractal)\n        mapper = {\'H\': 60, \'T\': 1, \'min\':1, \'S\': 1./60}\n        interval = math.ceil(interval * mapper[fractal])\n        data_interval = \'\'.join([str(interval), \'m\'])\n        qualifiers = \'{}?range={}&interval={}\'.format(symbol, data_range, data_interval)\n        request = url + qualifiers\n        logger.info(request)\n        response = requests.get(request)\n        response_json = response.json()[\'chart\']\n        if response_json[\'result\']:\n            body = response_json[\'result\'][0]\n            dt = pd.Series(map(lambda x: arrow.get(x).to(\'EST\').datetime.replace(tzinfo=None), body[\'timestamp\']), name=\'dt\')\n            df = pd.DataFrame(body[\'indicators\'][\'quote\'][0], index=dt)\n            df = df.loc[:, (\'open\', \'high\', \'low\', \'close\', \'volume\')]\n        else:\n            logger.info(""Could not get data from %s"", schema)\n            logger.info(response_json[\'error\'][\'code\'])\n            logger.info(response_json[\'error\'][\'description\'])\n    else:\n        # use pandas data reader\n        df = get_pandas_data(schema, subschema, symbol, intraday_data, data_fractal,\n                             from_date, to_date, lookback_period)\n\n    return df\n\n\n#\n# Data Dispatch Tables\n#\n\ndata_dispatch_table = {\'google\' : get_google_data,\n                       \'iex\'    : get_iex_data,\n                       \'pandas\' : get_pandas_data,\n                       \'quandl\' : get_quandl_data,\n                       \'yahoo\'  : get_yahoo_data}\n\n\n#\n# Function get_market_data\n#\n\ndef get_market_data(model, market_specs, group, lookback_period, intraday_data=False):\n    r""""""Get data from an external feed.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object describing the data.\n    market_specs : dict\n        The specifications for controlling the MarketFlow pipeline.\n    group : alphapy.Group\n        The group of symbols.\n    lookback_period : int\n        The number of periods of data to retrieve.\n    intraday_data : bool\n        If True, then get intraday data.\n\n    Returns\n    -------\n    n_periods : int\n        The maximum number of periods actually retrieved.\n\n    """"""\n\n    # Unpack market specifications\n\n    data_fractal = market_specs[\'data_fractal\']\n    subschema = market_specs[\'subschema\']\n\n    # Unpack model specifications\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    separator = model.specs[\'separator\']\n\n    # Unpack group elements\n\n    gspace = group.space\n    schema = gspace.schema\n    fractal = gspace.fractal\n\n    # Determine the feed source\n\n    if intraday_data:\n        # intraday data (date and time)\n        logger.info(""%s Intraday Data [%s] for %d periods"",\n                    schema, data_fractal, lookback_period)\n        index_column = \'datetime\'\n    else:\n        # daily data or higher (date only)\n        logger.info(""%s Daily Data [%s] for %d periods"",\n                    schema, data_fractal, lookback_period)\n        index_column = \'date\'\n\n    # Get the data from the relevant feed\n\n    data_dir = SSEP.join([directory, \'data\'])\n    n_periods = 0\n    resample_data = True if fractal != data_fractal else False\n\n    # Date Arithmetic\n\n    to_date = pd.to_datetime(\'today\')\n    from_date = to_date - pd.to_timedelta(lookback_period, unit=\'d\')\n    to_date = to_date.strftime(\'%Y-%m-%d\')\n    from_date = from_date.strftime(\'%Y-%m-%d\')\n\n    # Get the data from the specified data feed\n\n    df = pd.DataFrame()\n    for symbol in group.members:\n        logger.info(""Getting %s data from %s to %s"",\n                    symbol.upper(), from_date, to_date)\n        # Locate the data source\n        if schema == \'data\':\n            # local intraday or daily\n            dspace = Space(gspace.subject, gspace.schema, data_fractal)\n            fname = frame_name(symbol.lower(), dspace)\n            df = read_frame(data_dir, fname, extension, separator)\n        elif schema in data_dispatch_table.keys():\n            df = data_dispatch_table[schema](schema,\n                                             subschema,\n                                             symbol,\n                                             intraday_data,\n                                             data_fractal,\n                                             from_date,\n                                             to_date,\n                                             lookback_period)\n        else:\n            logger.error(""Unsupported Data Source: %s"", schema)\n        # Now that we have content, standardize the data\n        if not df.empty:\n            logger.info(""Rows: %d [%s]"", len(df), data_fractal)\n            # convert data to canonical form\n            df = convert_data(df, index_column, intraday_data)\n            # resample data and forward fill any NA values\n            if resample_data:\n                df = df.resample(fractal).agg({\'open\'   : \'first\',\n                                               \'high\'   : \'max\',\n                                               \'low\'    : \'min\',\n                                               \'close\'  : \'last\',\n                                               \'volume\' : \'sum\'})\n                df.dropna(axis=0, how=\'any\', inplace=True)\n                logger.info(""Rows after Resampling at %s: %d"",\n                            fractal, len(df))\n            # add intraday columns if necessary\n            if intraday_data:\n                df = enhance_intraday_data(df)\n            # allocate global Frame\n            newf = Frame(symbol.lower(), gspace, df)\n            if newf is None:\n                logger.error(""Could not allocate Frame for: %s"", symbol.upper())\n            # calculate maximum number of periods\n            df_len = len(df)\n            if df_len > n_periods:\n                n_periods = df_len\n        else:\n            logger.info(""No DataFrame for %s"", symbol.upper())\n\n    # The number of periods actually retrieved\n    return n_periods\n'"
alphapy/estimators.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : estimators\n# Created   : July 11, 2013\n#\n# Copyright 2019 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Objective\nfrom alphapy.globals import SSEP\n\nfrom keras.layers import *\nfrom keras.models import Sequential\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.wrappers.scikit_learn import KerasRegressor\nimport logging\nimport numpy as np\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport yaml\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Define scorers\n#\n\nscorers = {\'accuracy\'                   : (ModelType.classification, Objective.maximize),\n           \'average_precision\'          : (ModelType.classification, Objective.maximize),\n           \'balanced_accuracy\'          : (ModelType.classification, Objective.maximize),\n           \'brier_score_loss\'           : (ModelType.classification, Objective.minimize),\n           \'f1\'                         : (ModelType.classification, Objective.maximize),\n           \'f1_macro\'                   : (ModelType.classification, Objective.maximize),\n           \'f1_micro\'                   : (ModelType.classification, Objective.maximize),\n           \'f1_samples\'                 : (ModelType.classification, Objective.maximize),\n           \'f1_weighted\'                : (ModelType.classification, Objective.maximize),\n           \'neg_log_loss\'               : (ModelType.classification, Objective.minimize),\n           \'precision\'                  : (ModelType.classification, Objective.maximize),\n           \'recall\'                     : (ModelType.classification, Objective.maximize),\n           \'roc_auc\'                    : (ModelType.classification, Objective.maximize),\n           \'adjusted_rand_score\'        : (ModelType.clustering,     Objective.maximize),\n           \'explained_variance\'         : (ModelType.regression,     Objective.maximize),\n           \'neg_mean_absolute_error\'    : (ModelType.regression,     Objective.minimize),\n           \'neg_mean_squared_error\'     : (ModelType.regression,     Objective.minimize),\n           \'neg_mean_squared_log_error\' : (ModelType.regression,     Objective.minimize),\n           \'neg_median_absolute_error\'  : (ModelType.regression,     Objective.minimize),\n           \'r2\'                         : (ModelType.regression,     Objective.maximize)}\n\n\n#\n# Define XGB scoring map\n#\n\nxgb_score_map = {\'neg_log_loss\'            : \'logloss\',\n                 \'neg_mean_absolute_error\' : \'mae\',\n                 \'neg_mean_squared_error\'  : \'rmse\',\n                 \'precision\'               : \'map\',\n                 \'roc_auc\'                 : \'auc\'}\n\n\n#\n# Class Estimator\n#\n\nclass Estimator:\n    """"""Store information about each estimator.\n\n    Parameters\n    ----------\n    algorithm : str\n        Abbreviation representing the given algorithm.\n    model_type : enum ModelType\n        The machine learning task for this algorithm.\n    estimator : function\n        A scikit-learn, TensorFlow, or XGBoost function.\n    grid : dict\n        The dictionary of hyperparameters for grid search.\n\n    """"""\n\n    # __new__\n\n    def __new__(cls,\n                algorithm,\n                model_type,\n                estimator,\n                grid):\n        return super(Estimator, cls).__new__(cls)\n\n    # __init__\n\n    def __init__(self,\n                 algorithm,\n                 model_type,\n                 estimator,\n                 grid):\n        self.algorithm = algorithm.upper()\n        self.model_type = model_type\n        self.estimator = estimator\n        self.grid = grid\n\n    # __str__\n\n    def __str__(self):\n        return self.name\n\n\n#\n# Define estimator map\n#\n\nestimator_map = {\'AB\'     : AdaBoostClassifier,\n                 \'GB\'     : GradientBoostingClassifier,\n                 \'GBR\'    : GradientBoostingRegressor,\n                 \'KERASC\' : KerasClassifier,\n                 \'KERASR\' : KerasRegressor,\n                 \'KNN\'    : KNeighborsClassifier,\n                 \'KNR\'    : KNeighborsRegressor,\n                 \'LOGR\'   : LogisticRegression,\n                 \'LR\'     : LinearRegression,\n                 \'LSVC\'   : LinearSVC,\n                 \'LSVM\'   : SVC,\n                 \'NB\'     : MultinomialNB,\n                 \'RBF\'    : SVC,\n                 \'RF\'     : RandomForestClassifier,\n                 \'RFR\'    : RandomForestRegressor,\n                 \'SVM\'    : SVC,\n                 \'XGB\'    : xgb.XGBClassifier,\n                 \'XGBM\'   : xgb.XGBClassifier,\n                 \'XGBR\'   : xgb.XGBRegressor,\n                 \'XT\'     : ExtraTreesClassifier,\n                 \'XTR\'    : ExtraTreesRegressor\n                }\n\n\n#\n# Function get_algos_config\n#\n\ndef get_algos_config(cfg_dir):\n    r""""""Read the algorithms configuration file.\n\n    Parameters\n    ----------\n    cfg_dir : str\n        The directory where the configuration file ``algos.yml``\n        is stored.\n\n    Returns\n    -------\n    specs : dict\n        The specifications for determining which algorithms to run.\n\n    """"""\n\n    logger.info(""Algorithm Configuration"")\n\n    # Read the configuration file\n\n    full_path = SSEP.join([cfg_dir, \'algos.yml\'])\n    with open(full_path, \'r\') as ymlfile:\n        specs = yaml.load(ymlfile, Loader=yaml.FullLoader)\n\n    # Ensure each algorithm has required keys\n\n    minimum_keys = [\'model_type\', \'params\', \'grid\']\n    required_keys_keras = minimum_keys + [\'layers\', \'compiler\']\n    for algo in specs:\n        if \'KERAS\' in algo:\n            required_keys = required_keys_keras\n        else:\n            required_keys = minimum_keys\n        algo_keys = list(specs[algo].keys())\n        if set(algo_keys) != set(required_keys):\n            logger.warning(""Algorithm %s has the wrong keys %s"",\n                           algo, required_keys)\n            logger.warning(""Keys found instead: %s"", algo_keys)\n        else:\n            # determine whether or not model type is valid\n            model_types = {x.name: x.value for x in ModelType}\n            model_type = specs[algo][\'model_type\']\n            if model_type in model_types:\n                specs[algo][\'model_type\'] = ModelType(model_types[model_type])\n            else:\n                raise ValueError(""algos.yml model:type %s unrecognized"" % model_type)\n\n    # Algorithm Specifications\n    return specs\n\n\n#\n# Function create_keras_model\n#\n\ndef create_keras_model(nlayers,\n                       layer1=None,\n                       layer2=None,\n                       layer3=None,\n                       layer4=None,\n                       layer5=None,\n                       layer6=None,\n                       layer7=None,\n                       layer8=None,\n                       layer9=None,\n                       layer10=None,\n                       optimizer=None,\n                       loss=None,\n                       metrics=None):\n    r""""""Create a Keras Sequential model.\n\n    Parameters\n    ----------\n    nlayers : int\n        Number of layers of the Sequential model.\n    layer1...layer10 : str\n        Ordered layers of the Sequential model.\n    optimizer : str\n        Compiler optimizer for the Sequential model.\n    loss : str\n        Compiler loss function for the Sequential model.\n    metrics : str\n        Compiler evaluation metric for the Sequential model.\n\n    Returns\n    -------\n    model : keras.models.Sequential\n        Compiled Keras Sequential Model.\n\n    """"""\n\n    model = Sequential()\n    for i in range(nlayers):\n        lvar = \'layer\' + str(i+1)\n        layer = eval(lvar)\n        model.add(eval(layer))\n    model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n    return model\n\n\n#\n# Function get_estimators\n#\n\ndef get_estimators(model):\n    r""""""Define all the AlphaPy estimators based on the contents\n    of the ``algos.yml`` file.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object containing global AlphaPy parameters.\n\n    Returns\n    -------\n    estimators : dict\n        All of the estimators required for running the pipeline.\n\n    """"""\n\n    # Extract model data\n\n    directory = model.specs[\'directory\']\n    n_estimators = model.specs[\'n_estimators\']\n    n_jobs = model.specs[\'n_jobs\']\n    seed = model.specs[\'seed\']\n    verbosity = model.specs[\'verbosity\']\n\n    # Reference training data for Keras input_dim\n    X_train = model.X_train\n\n    # Initialize estimator dictionary\n    estimators = {}\n\n    # Global parameter substitution fields\n\n    ps_fields = {\'n_estimators\' : \'n_estimators\',\n                 \'n_jobs\'       : \'n_jobs\',\n                 \'nthread\'      : \'n_jobs\',\n                 \'random_state\' : \'seed\',\n                 \'seed\'         : \'seed\',\n                 \'verbose\'      : \'verbosity\'}\n\n    # Get algorithm specifications\n\n    config_dir = SSEP.join([directory, \'config\'])\n    algo_specs = get_algos_config(config_dir)\n\n    # Create estimators for all of the algorithms\n\n    for algo in algo_specs:\n        model_type = algo_specs[algo][\'model_type\']\n        params = algo_specs[algo][\'params\']\n        for param in params:\n            if param in ps_fields and isinstance(param, str):\n                algo_specs[algo][\'params\'][param] = eval(ps_fields[param])\n        func = estimator_map[algo]\n        if \'KERAS\' in algo:\n            params[\'build_fn\'] = create_keras_model\n            layers = algo_specs[algo][\'layers\']\n            params[\'nlayers\'] = len(layers)\n            input_dim_string = \', input_dim={})\'.format(X_train.shape[1])\n            layers[0] = layers[0].replace(\')\', input_dim_string)\n            for i, layer in enumerate(layers):\n                params[\'layer\'+str(i+1)] = layer\n            compiler = algo_specs[algo][\'compiler\']\n            params[\'optimizer\'] = compiler[\'optimizer\']\n            params[\'loss\'] = compiler[\'loss\']\n            try:\n                params[\'metrics\'] = compiler[\'metrics\']\n            except:\n                pass\n        est = func(**params)\n        grid = algo_specs[algo][\'grid\']\n        estimators[algo] = Estimator(algo, model_type, est, grid)\n\n    # return the entire classifier list\n    return estimators\n'"
alphapy/features.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : features\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import BSEP, LOFF, NULLTEXT\nfrom alphapy.globals import PSEP, SSEP, USEP\nfrom alphapy.globals import Encoders\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Scalers\nfrom alphapy.variables import Variable\nfrom alphapy.variables import vparse\n\nimport category_encoders as ce\nfrom importlib import import_module\nimport itertools\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nfrom scipy import sparse\nimport scipy.stats as sps\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import SelectFdr\nfrom sklearn.feature_selection import SelectFpr\nfrom sklearn.feature_selection import SelectFwe\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nimport sys\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Define feature scoring functions\n#\n\nfeature_scorers = {\'f_classif\'    : f_classif,\n                   \'chi2\'         : chi2,\n                   \'f_regression\' : f_regression,\n                   \'SelectKBest\'  : SelectKBest,\n                   \'SelectFpr\'    : SelectFpr,\n                   \'SelectFdr\'    : SelectFdr,\n                   \'SelectFwe\'    : SelectFwe}\n\n\n#\n# Define Encoder map\n#\n\nencoder_map = {Encoders.backdiff     : ce.BackwardDifferenceEncoder,\n               Encoders.basen        : ce.BaseNEncoder,\n               Encoders.binary       : ce.BinaryEncoder,\n               Encoders.catboost     : ce.CatBoostEncoder,\n               Encoders.hashing      : ce.HashingEncoder,\n               Encoders.helmert      : ce.HelmertEncoder,\n               Encoders.jstein       : ce.JamesSteinEncoder,\n               Encoders.leaveone     : ce.LeaveOneOutEncoder,\n               Encoders.mestimate    : ce.MEstimateEncoder,\n               Encoders.onehot       : ce.OneHotEncoder,\n               Encoders.ordinal      : ce.OrdinalEncoder,\n               Encoders.polynomial   : ce.PolynomialEncoder,\n               Encoders.sum          : ce.SumEncoder,\n               Encoders.target       : ce.TargetEncoder,\n               Encoders.woe          : ce.WOEEncoder}\n\n\n#\n# Function apply_transform\n#\n\ndef apply_transform(fname, df, fparams):\n    r""""""Apply a transform function to a column of the dataframe.\n\n    Parameters\n    ----------\n    fname : str\n        Name of the column to be treated in the dataframe ``df``.\n    df : pandas.DataFrame\n        Dataframe containing the column ``fname``.\n    fparams : list\n        The module, function, and parameter list of the transform\n        function\n\n    Returns\n    -------\n    new_features : pandas.DataFrame\n        The set of features after applying a transform function.\n\n    """"""\n    # Extract the transform parameter list\n    module = fparams[0]\n    func_name = fparams[1]\n    plist = fparams[2:]\n    # Append to system path\n    sys.path.append(os.getcwd())\n    # Import the external transform function\n    ext_module = import_module(module)\n    func = getattr(ext_module, func_name)\n    # Prepend the parameter list with the data frame and feature name\n    plist.insert(0, fname)\n    plist.insert(0, df)\n    # Apply the transform\n    logger.info(""Applying function %s from module %s to feature %s"",\n                func_name, module, fname)\n    return func(*plist)\n\n\n#\n# Function apply_transforms\n#\n\ndef apply_transforms(model, X):\n    r""""""Apply special functions to the original features.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model specifications indicating any transforms.\n    X : pandas.DataFrame\n        Combined train and test data, or just prediction data.\n\n    Returns\n    -------\n    all_features : pandas.DataFrame\n        All features, including transforms.\n\n    Raises\n    ------\n    IndexError\n        The number of transform rows must match the number of\n        rows in ``X``.\n\n    """"""\n\n    # Extract model parameters\n    transforms = model.specs[\'transforms\']\n\n    # Log input parameters\n\n    logger.info(""Original Features : %s"", X.columns)\n    logger.info(""Feature Count     : %d"", X.shape[1])\n\n    # Iterate through columns, dispatching and transforming each feature.\n\n    logger.info(""Applying transforms"")\n    all_features = X\n\n    if transforms:\n        for fname in transforms:\n            # find feature series\n            fcols = []\n            for col in X.columns:\n                if col.split(LOFF)[0] == fname:\n                    fcols.append(col)\n            # get lag values\n            lag_values = []\n            for item in fcols:\n                _, _, _, lag = vparse(item)\n                lag_values.append(lag)\n            # apply transform to the most recent value\n            if lag_values:\n                f_latest = fcols[lag_values.index(min(lag_values))]\n                features = apply_transform(f_latest, X, transforms[fname])\n                if features is not None:\n                    if features.shape[0] == X.shape[0]:\n                        all_features = pd.concat([all_features, features], axis=1)\n                    else:\n                        raise IndexError(""The number of transform rows [%d] must match X [%d]"" %\n                                         (features.shape[0], X.shape[0]))\n                else:\n                    logger.info(""Could not apply transform for feature %s"", fname)\n            else:\n                logger.info(""Feature %s is missing for transform"", fname)\n    else:\n        logger.info(""No transforms Specified"")\n\n    logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Return all transformed training and test features\n    return all_features\n\n\n#\n# Function impute_values\n#\n\ndef impute_values(feature, dt, sentinel):\n    r""""""Impute values for a given data type. The *median* strategy\n    is applied for floating point values, and the *most frequent*\n    strategy is applied for integer or Boolean values.\n\n    Parameters\n    ----------\n    feature : pandas.Series or numpy.array\n        The feature for imputation.\n    dt : str\n        The values ``\'float64\'``, ``\'int64\'``, or ``\'bool\'``.\n    sentinel : float\n        The number to be imputed for NaN values.\n\n    Returns\n    -------\n    imputed : numpy.array\n        The feature after imputation.\n\n    Raises\n    ------\n    TypeError\n        Data type ``dt`` is invalid for imputation.\n\n    References\n    ----------\n    You can find more information on feature imputation here [IMP]_.\n\n    .. [IMP] http://scikit-learn.org/stable/modules/preprocessing.html#imputation\n\n    """"""\n\n    try:\n        # for pandas series\n        feature = feature.values.reshape(-1, 1)\n    except:\n        # for numpy array\n        feature = feature.reshape(-1, 1)\n\n    imp = None\n    if dt == \'float64\':\n        logger.info(""    Imputation for Data Type %s: Median Strategy"" % dt)\n        imp = SimpleImputer(missing_values=np.nan, strategy=\'median\')\n    elif dt == \'int64\':\n        logger.info(""    Imputation for Data Type %s: Most Frequent Strategy"" % dt)\n        imp = SimpleImputer(missing_values=np.nan, strategy=\'most_frequent\')\n    else:\n        logger.info(""    Imputation for Data Type %s: Fill Strategy with %d"" % (dt, sentinel))\n\n    if imp:\n        imputed = imp.fit_transform(feature)\n    else:\n        feature[np.isnan(feature)] = sentinel\n        imputed = feature\n    return imputed\n\n\n#\n# Function get_numerical_features\n#\n\ndef get_numerical_features(fnum, fname, df, nvalues, dt,\n                           sentinel, logt, plevel):\n    r""""""Transform numerical features with imputation and possibly\n    log-transformation.\n\n    Parameters\n    ----------\n    fnum : int\n        Feature number, strictly for logging purposes\n    fname : str\n        Name of the numerical column in the dataframe ``df``.\n    df : pandas.DataFrame\n        Dataframe containing the column ``fname``.\n    nvalues : int\n        The number of unique values.\n    dt : str\n        The values ``\'float64\'``, ``\'int64\'``, or ``\'bool\'``.\n    sentinel : float\n        The number to be imputed for NaN values.\n    logt : bool\n        If ``True``, then log-transform numerical values.\n    plevel : float\n        The p-value threshold to test if a feature is normally distributed.\n\n    Returns\n    -------\n    new_values : numpy array\n        The set of imputed and transformed features.\n    new_fnames : list\n        The new feature name(s) for the numerical variable.\n\n    """"""\n    feature = df[fname]\n    if len(feature) == nvalues:\n        logger.info(""Feature %d: %s is a numerical feature of type %s with maximum number of values %d"",\n                    fnum, fname, dt, nvalues)\n    else:\n        logger.info(""Feature %d: %s is a numerical feature of type %s with %d unique values"",\n                    fnum, fname, dt, nvalues)\n    # imputer for float, integer, or boolean data types\n    new_values = impute_values(feature, dt, sentinel)\n    # log-transform any values that do not fit a normal distribution\n    new_fname = fname\n    if logt and np.all(new_values > 0):\n        _, pvalue = sps.normaltest(new_values)\n        if pvalue <= plevel:\n            logger.info(""Feature %d: %s is not normally distributed [p-value: %f]"",\n                        fnum, fname, pvalue)\n            new_values = np.log(new_values)\n        else:\n            new_fname = USEP.join([new_fname, \'log\'])\n    return new_values, [new_fname]\n\n\n#\n# Function get_polynomials\n#\n\ndef get_polynomials(features, poly_degree):\n    r""""""Generate interactions that are products of distinct features.\n\n    Parameters\n    ----------\n    features : pandas.DataFrame\n        Dataframe containing the features for generating interactions.\n    poly_degree : int\n        The degree of the polynomial features.\n\n    Returns\n    -------\n    poly_features : numpy array\n        The interaction features only.\n    poly_fnames : list\n        List of polynomial feature names.\n\n    References\n    ----------\n    You can find more information on polynomial interactions here [POLY]_.\n\n    .. [POLY] http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n\n    """"""\n    polyf = PolynomialFeatures(interaction_only=True,\n                               degree=poly_degree,\n                               include_bias=False)\n    poly_features = polyf.fit_transform(features)\n    poly_fnames = polyf.get_feature_names()\n    return poly_features, poly_fnames\n\n\n#\n# Function get_text_features\n#\n\ndef get_text_features(fnum, fname, df, nvalues, vectorize, ngrams_max):\n    r""""""Transform text features with count vectorization and TF-IDF,\n    or alternatively factorization.\n\n    Parameters\n    ----------\n    fnum : int\n        Feature number, strictly for logging purposes\n    fname : str\n        Name of the text column in the dataframe ``df``.\n    df : pandas.DataFrame\n        Dataframe containing the column ``fname``.\n    nvalues : int\n        The number of unique values.\n    vectorize : bool\n        If ``True``, then attempt count vectorization.\n    ngrams_max : int\n        The maximum number of n-grams for count vectorization.\n\n    Returns\n    -------\n    new_features : numpy array\n        The vectorized or factorized text features.\n    new_fnames : list\n        The new feature name(s) for the numerical variable.\n\n    References\n    ----------\n    To use count vectorization and TF-IDF, you can find more\n    information here [TFE]_.\n\n    """"""\n    feature = df[fname]\n    min_length = int(feature.str.len().min())\n    max_length = int(feature.str.len().max())\n    if len(feature) == nvalues:\n        logger.info(""Feature %d: %s is a text feature [%d:%d] with maximum number of values %d"",\n                    fnum, fname, min_length, max_length, nvalues)\n    else:\n        logger.info(""Feature %d: %s is a text feature [%d:%d] with %d unique values"",\n                    fnum, fname, min_length, max_length, nvalues)\n    # need a null text placeholder for vectorization\n    feature.fillna(value=NULLTEXT, inplace=True)\n    # vectorization creates many columns, otherwise just factorize\n    if vectorize:\n        logger.info(""Feature %d: %s => Attempting Vectorization"", fnum, fname)\n        vectorizer = TfidfVectorizer(ngram_range=[1, ngrams_max])\n        try:\n            new_features = vectorizer.fit_transform(feature)\n            new_fnames = vectorizer.get_feature_names()\n            logger.info(""Feature %d: %s => Vectorization Succeeded"", fnum, fname)\n        except:\n            logger.info(""Feature %d: %s => Vectorization Failed"", fnum, fname)\n            new_features, _ = pd.factorize(feature)\n            new_fnames = [USEP.join([fname, \'factor\'])]\n    else:\n        logger.info(""Feature %d: %s => Factorization"", fnum, fname)\n        new_features, _ = pd.factorize(feature)\n        new_fnames = [USEP.join([fname, \'factor\'])]\n    return new_features, new_fnames\n\n\n#\n# Function float_factor\n#\n\ndef float_factor(x, rounding):\n    r""""""Convert a floating point number to a factor.\n\n    Parameters\n    ----------\n    x : float\n        The value to convert to a factor.\n    rounding : int\n        The number of places to round.\n\n    Returns\n    -------\n    ffactor : int\n        The resulting factor.\n\n    """"""\n    num2str = \'{0:.{1}f}\'.format\n    fstr = re.sub(""[^0-9]"", """", num2str(x, rounding))\n    ffactor = int(fstr) if len(fstr) > 0 else 0\n    return ffactor\n\n\n#\n# Function create_crosstabs\n#\n\ndef create_crosstabs(model):\n    r""""""Create cross-tabulations for categorical variables.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object containing the data.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the updated feature map.\n\n    """"""\n\n    logger.info(""Creating Cross-Tabulations"")\n\n    # Extract model data\n    X = model.X_train\n    y = model.y_train\n\n    # Extract model parameters\n\n    factors = model.specs[\'factors\']\n\n    # Iterate through columns, dispatching and transforming each feature.\n\n    crosstabs = {}\n    for fname in X:\n        if fname in factors:\n            logger.info(""Creating crosstabs for feature %s"", fname)\n            ct = pd.crosstab(X[fname], y).apply(lambda r : r / r.sum(), axis=1)\n            crosstabs[fname] = ct\n\n    # Save crosstabs to the feature map\n\n    model.feature_map[\'crosstabs\'] = crosstabs\n    return model\n\n\n#\n# Function get_factors\n#\n\ndef get_factors(model, X_train, X_test, y_train, fnum, fname,\n                nvalues, dtype, encoder, rounding, sentinel):\n    r""""""Convert the original feature to a factor.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object with the feature specifications.\n    X_train : pandas.DataFrame\n        Training dataframe containing the column ``fname``.\n    X_test : pandas.DataFrame\n        Testing dataframe containing the column ``fname``.\n    y_train : pandas.Series\n        Training series for target variable.\n    fnum : int\n        Feature number, strictly for logging purposes\n    fname : str\n        Name of the text column in the dataframe ``df``.\n    nvalues : int\n        The number of unique values.\n    dtype : str\n        The values ``\'float64\'``, ``\'int64\'``, or ``\'bool\'``.\n    encoder : alphapy.features.Encoders\n        Type of encoder to apply.\n    rounding : int\n        Number of places to round.\n    sentinel : float\n        The number to be imputed for NaN values.\n\n    Returns\n    -------\n    all_features : numpy array\n        The features that have been transformed to factors.\n    all_fnames : list\n        The feature names for the encodings.\n\n    """"""\n\n    logger.info(""Feature %d: %s is a factor of type %s with %d unique values"",\n                fnum, fname, dtype, nvalues)\n    logger.info(""Encoding: %s"", encoder)\n\n    # get feature\n    feature_train = X_train[fname]\n    feature_test = X_test[fname]\n    # convert float to factor\n    if dtype == \'float64\':\n        logger.info(""Rounding: %d"", rounding)\n        feature_train = feature_train.apply(float_factor, args=[rounding])\n        feature_test = feature_test.apply(float_factor, args=[rounding])\n    # create data frames for the feature\n    df_train = pd.DataFrame(feature_train)\n    df_test = pd.DataFrame(feature_test)\n    # encoders\n    enc = None\n    try:\n        enc = encoder_map[encoder](cols=[fname])\n    except:\n        raise ValueError(""Unknown Encoder %s"" % encoder)\n    # Transform the train and test features.\n    if enc is not None:\n        # fit training features\n        logger.info(""Fitting training features for %s"", fname)\n        ftrain = enc.fit_transform(df_train, y_train)\n        # fit testing features\n        logger.info(""Transforming testing features for %s"", fname)\n        ftest = enc.transform(df_test)\n        # get feature names\n        all_fnames = enc.get_feature_names()\n        # concatenate all generated features\n        all_features = np.row_stack((ftrain, ftest))\n    else:\n        all_features = None\n        all_fnames = None\n        logger.info(""Encoding for feature %s failed"" % fname)\n    return all_features, all_fnames\n\n\n#\n# Function create_numpy_features\n#\n\ndef create_numpy_features(base_features, sentinel):\n    r""""""Calculate the sum, mean, standard deviation, and variance\n    of each row.\n\n    Parameters\n    ----------\n    base_features : numpy array\n        The feature dataframe.\n    sentinel : float\n        The number to be imputed for NaN values.\n\n    Returns\n    -------\n    np_features : numpy array\n        The calculated NumPy features.\n    np_fnames : list\n        The NumPy feature names.\n\n    """"""\n\n    logger.info(""Creating NumPy Features"")\n\n    # Calculate the total, mean, standard deviation, and variance.\n\n    np_funcs = {\'sum\'  : np.sum,\n                \'mean\' : np.mean,\n                \'std\'  : np.std,\n                \'var\'  : np.var}\n\n    features = []\n    for k in np_funcs:\n        logger.info(""NumPy Feature: %s"", k)\n        feature = np_funcs[k](base_features, axis=1)\n        feature = impute_values(feature, \'float64\', sentinel)\n        features.append(feature)\n\n    # Stack and scale the new features.\n\n    np_features = np.column_stack(features)\n    np_features = StandardScaler().fit_transform(np_features)\n\n    # Return new NumPy features\n\n    logger.info(""NumPy Feature Count : %d"", np_features.shape[1])\n    return np_features, np_funcs.keys()\n\n\n#\n# Function create_scipy_features\n#\n\ndef create_scipy_features(base_features, sentinel):\n    r""""""Calculate the skew, kurtosis, and other statistical features\n    for each row.\n\n    Parameters\n    ----------\n    base_features : numpy array\n        The feature dataframe.\n    sentinel : float\n        The number to be imputed for NaN values.\n\n    Returns\n    -------\n    sp_features : numpy array\n        The calculated SciPy features.\n    sp_fnames : list\n        The SciPy feature names.\n\n    """"""\n\n    logger.info(""Creating SciPy Features"")\n\n    # Generate scipy features\n\n    logger.info(""SciPy Feature: geometric mean"")\n    row_gmean = sps.gmean(base_features, axis=1)\n    logger.info(""SciPy Feature: kurtosis"")\n    row_kurtosis = sps.kurtosis(base_features, axis=1)\n    logger.info(""SciPy Feature: kurtosis test"")\n    row_ktest, pvalue = sps.kurtosistest(base_features, axis=1)\n    logger.info(""SciPy Feature: normal test"")\n    row_normal, pvalue = sps.normaltest(base_features, axis=1)\n    logger.info(""SciPy Feature: skew"")\n    row_skew = sps.skew(base_features, axis=1)\n    logger.info(""SciPy Feature: skew test"")\n    row_stest, pvalue = sps.skewtest(base_features, axis=1)\n    logger.info(""SciPy Feature: variation"")\n    row_var = sps.variation(base_features, axis=1)\n    logger.info(""SciPy Feature: signal-to-noise ratio"")\n    row_stn = sps.signaltonoise(base_features, axis=1)\n    logger.info(""SciPy Feature: standard error of mean"")\n    row_sem = sps.sem(base_features, axis=1)\n\n    sp_features = np.column_stack((row_gmean, row_kurtosis, row_ktest,\n                                   row_normal, row_skew, row_stest,\n                                   row_var, row_stn, row_sem))\n    sp_features = impute_values(sp_features, \'float64\', sentinel)\n    sp_features = StandardScaler().fit_transform(sp_features)\n\n    # Return new SciPy features\n\n    logger.info(""SciPy Feature Count : %d"", sp_features.shape[1])\n    sp_fnames = [\'sp_geometric_mean\',\n                 \'sp_kurtosis\',\n                 \'sp_kurtosis_test\',\n                 \'sp_normal_test\',\n                 \'sp_skew\',\n                 \'sp_skew_test\',\n                 \'sp_variation\',\n                 \'sp_signal_to_noise\',\n                 \'sp_standard_error_of_mean\']\n    return sp_features, sp_fnames\n\n\n#\n# Function create_clusters\n#\n\ndef create_clusters(features, model):\n    r""""""Cluster the given features.\n\n    Parameters\n    ----------\n    features : numpy array\n        The features to cluster.\n    model : alphapy.Model\n        The model object with the clustering parameters.\n\n    Returns\n    -------\n    cfeatures : numpy array\n        The calculated clusters.\n    cnames : list\n        The cluster feature names.\n\n    References\n    ----------\n    You can find more information on clustering here [CLUS]_.\n\n    .. [CLUS] http://scikit-learn.org/stable/modules/clustering.html\n\n    """"""\n\n    logger.info(""Creating Clustering Features"")\n\n    # Extract model parameters\n\n    cluster_inc = model.specs[\'cluster_inc\']\n    cluster_max = model.specs[\'cluster_max\']\n    cluster_min = model.specs[\'cluster_min\']\n    seed = model.specs[\'seed\']\n\n    # Log model parameters\n\n    logger.info(""Cluster Minimum   : %d"", cluster_min)\n    logger.info(""Cluster Maximum   : %d"", cluster_max)\n    logger.info(""Cluster Increment : %d"", cluster_inc)\n\n    # Generate clustering features\n\n    cfeatures = np.zeros((features.shape[0], 1))\n    cnames = []\n    for i in range(cluster_min, cluster_max+1, cluster_inc):\n        logger.info(""k = %d"", i)\n        km = MiniBatchKMeans(n_clusters=i, random_state=seed)\n        km.fit(features)\n        labels = km.predict(features)\n        labels = labels.reshape(-1, 1)\n        cfeatures = np.column_stack((cfeatures, labels))\n        cnames.append(USEP.join([\'cluster\', str(i)]))\n    cfeatures = np.delete(cfeatures, 0, axis=1)\n\n    # Return new clustering features\n\n    logger.info(""Clustering Feature Count : %d"", cfeatures.shape[1])\n    return cfeatures, cnames\n\n\n#\n# Function create_pca_features\n#\n\ndef create_pca_features(features, model):\n    r""""""Apply Principal Component Analysis (PCA) to the features.\n\n    Parameters\n    ----------\n    features : numpy array\n        The input features.\n    model : alphapy.Model\n        The model object with the PCA parameters.\n\n    Returns\n    -------\n    pfeatures : numpy array\n        The PCA features.\n    pnames : list\n        The PCA feature names.\n\n    References\n    ----------\n    You can find more information on Principal Component Analysis here [PCA]_.\n\n    .. [PCA] http://scikit-learn.org/stable/modules/decomposition.html#pca\n\n    """"""\n\n    logger.info(""Creating PCA Features"")\n\n    # Extract model parameters\n\n    pca_inc = model.specs[\'pca_inc\']\n    pca_max = model.specs[\'pca_max\']\n    pca_min = model.specs[\'pca_min\']\n    pca_whiten = model.specs[\'pca_whiten\']\n\n    # Log model parameters\n\n    logger.info(""PCA Minimum   : %d"", pca_min)\n    logger.info(""PCA Maximum   : %d"", pca_max)\n    logger.info(""PCA Increment : %d"", pca_inc)\n    logger.info(""PCA Whitening : %r"", pca_whiten)\n\n    # Generate clustering features\n\n    pfeatures = np.zeros((features.shape[0], 1))\n    pnames = []\n    for i in range(pca_min, pca_max+1, pca_inc):\n        logger.info(""n_components = %d"", i)\n        X_pca = PCA(n_components=i, whiten=pca_whiten).fit_transform(features)\n        pfeatures = np.column_stack((pfeatures, X_pca))\n        pnames.append(USEP.join([\'pca\', str(i)]))\n    pfeatures = np.delete(pfeatures, 0, axis=1)\n\n    # Return new clustering features\n\n    logger.info(""PCA Feature Count : %d"", pfeatures.shape[1])\n    return pfeatures, pnames\n\n\n#\n# Function create_isomap_features\n#\n\ndef create_isomap_features(features, model):\n    r""""""Create Isomap features.\n\n    Parameters\n    ----------\n    features : numpy array\n        The input features.\n    model : alphapy.Model\n        The model object with the Isomap parameters.\n\n    Returns\n    -------\n    ifeatures : numpy array\n        The Isomap features.\n    inames : list\n        The Isomap feature names.\n\n    Notes\n    -----\n\n    Isomaps are very memory-intensive. Your process will be killed\n    if you run out of memory.\n\n    References\n    ----------\n    You can find more information on Principal Component Analysis here [ISO]_.\n\n    .. [ISO] http://scikit-learn.org/stable/modules/manifold.html#isomap\n\n    """"""\n\n    logger.info(""Creating Isomap Features"")\n\n    # Extract model parameters\n\n    iso_components = model.specs[\'iso_components\']\n    iso_neighbors = model.specs[\'iso_neighbors\']\n    n_jobs = model.specs[\'n_jobs\']\n\n    # Log model parameters\n\n    logger.info(""Isomap Components : %d"", iso_components)\n    logger.info(""Isomap Neighbors  : %d"", iso_neighbors)\n\n    # Generate Isomap features\n\n    model = Isomap(n_neighbors=iso_neighbors, n_components=iso_components,\n                   n_jobs=n_jobs)\n    ifeatures = model.fit_transform(features)\n    inames = [USEP.join([\'isomap\', str(i+1)]) for i in range(iso_components)]\n\n    # Return new Isomap features\n\n    logger.info(""Isomap Feature Count : %d"", ifeatures.shape[1])\n    return ifeatures, inames\n\n\n#\n# Function create_tsne_features\n#\n\ndef create_tsne_features(features, model):\n    r""""""Create t-SNE features.\n\n    Parameters\n    ----------\n    features : numpy array\n        The input features.\n    model : alphapy.Model\n        The model object with the t-SNE parameters.\n\n    Returns\n    -------\n    tfeatures : numpy array\n        The t-SNE features.\n    tnames : list\n        The t-SNE feature names.\n\n    References\n    ----------\n    You can find more information on the t-SNE technique here [TSNE]_.\n\n    .. [TSNE] http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne\n\n    """"""\n\n    logger.info(""Creating T-SNE Features"")\n\n    # Extract model parameters\n\n    seed = model.specs[\'seed\']\n    tsne_components = model.specs[\'tsne_components\']\n    tsne_learn_rate = model.specs[\'tsne_learn_rate\']\n    tsne_perplexity = model.specs[\'tsne_perplexity\']\n\n    # Log model parameters\n\n    logger.info(""T-SNE Components    : %d"", tsne_components)\n    logger.info(""T-SNE Learning Rate : %d"", tsne_learn_rate)\n    logger.info(""T-SNE Perplexity    : %d"", tsne_perplexity)\n\n    # Generate T-SNE features\n\n    model = TSNE(n_components=tsne_components, perplexity=tsne_perplexity,\n                 learning_rate=tsne_learn_rate, random_state=seed)\n    tfeatures = model.fit_transform(features)\n    tnames = [USEP.join([\'tsne\', str(i+1)]) for i in range(tsne_components)]\n\n    # Return new T-SNE features\n\n    logger.info(""T-SNE Feature Count : %d"", tfeatures.shape[1])\n    return tfeatures, tnames\n\n\n#\n# Function create_features\n#\n\ndef create_features(model, X, X_train, X_test, y_train):\n    r""""""Create features for the train and test set.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object with the feature specifications.\n    X : pandas.DataFrame\n        Combined train and test data.\n    X_train : pandas.DataFrame\n        Training data.\n    X_test : pandas.DataFrame\n        Testing data.\n    y_train : pandas.DataFrame\n        Target variable for training data.\n\n    Returns\n    -------\n    all_features : numpy array\n        The new features.\n\n    Raises\n    ------\n    TypeError\n        Unrecognized data type.\n\n    """"""\n\n    # Extract model parameters\n\n    clustering = model.specs[\'clustering\']\n    counts_flag = model.specs[\'counts\']\n    encoder = model.specs[\'encoder\']\n    factors = model.specs[\'factors\']\n    isomap = model.specs[\'isomap\']\n    logtransform = model.specs[\'logtransform\']\n    ngrams_max = model.specs[\'ngrams_max\']\n    numpy_flag = model.specs[\'numpy\']\n    pca = model.specs[\'pca\']\n    pvalue_level = model.specs[\'pvalue_level\']\n    rounding = model.specs[\'rounding\']\n    scaling = model.specs[\'scaler_option\']\n    scaler = model.specs[\'scaler_type\']\n    scipy_flag = model.specs[\'scipy\']\n    sentinel = model.specs[\'sentinel\']\n    tsne = model.specs[\'tsne\']\n    vectorize = model.specs[\'vectorize\']\n\n    # Log input parameters\n\n    logger.info(""Original Features : %s"", X.columns)\n    logger.info(""Feature Count     : %d"", X.shape[1])\n\n    # Count zero and NaN values\n\n    if counts_flag:\n        logger.info(""Creating Count Features"")\n        logger.info(""NA Counts"")\n        X[\'nan_count\'] = X.count(axis=1)\n        logger.info(""Number Counts"")\n        for i in range(10):\n            fc = USEP.join([\'count\', str(i)])\n            X[fc] = (X == i).astype(int).sum(axis=1)\n        logger.info(""New Feature Count : %d"", X.shape[1])\n\n    # Iterate through columns, dispatching and transforming each feature.\n\n    logger.info(""Creating Base Features"")\n    all_features = np.zeros((X.shape[0], 1))\n    model.feature_names = []\n\n    for i, fname in enumerate(X):\n        fnum = i + 1\n        dtype = X[fname].dtypes\n        nunique = len(X[fname].unique())\n        # standard processing of numerical, categorical, and text features\n        if factors and fname in factors:\n            features, fnames = get_factors(model, X_train, X_test, y_train, fnum, fname,\n                                           nunique, dtype, encoder, rounding, sentinel)\n        elif dtype == \'float64\' or dtype == \'int64\' or dtype == \'bool\':\n            features, fnames = get_numerical_features(fnum, fname, X, nunique, dtype,\n                                                      sentinel, logtransform, pvalue_level)\n        elif dtype == \'object\':\n            features, fnames = get_text_features(fnum, fname, X, nunique, vectorize, ngrams_max)\n        else:\n            raise TypeError(""Base Feature Error with unrecognized type %s"" % dtype)\n        if features.shape[0] == all_features.shape[0]:\n            # add features\n            all_features = np.column_stack((all_features, features))\n            # add feature names\n            model.feature_names.extend(fnames)\n        else:\n            logger.info(""Feature %s has the wrong number of rows: %d"",\n                        fname, features.shape[0])\n    all_features = np.delete(all_features, 0, axis=1)\n\n    logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Call standard scaler for all features\n\n    if scaling:\n        logger.info(""Scaling Base Features"")\n        if scaler == Scalers.standard:\n            all_features = StandardScaler().fit_transform(all_features)\n        elif scaler == Scalers.minmax:\n            all_features = MinMaxScaler().fit_transform(all_features)\n        else:\n            logger.info(""Unrecognized scaler: %s"", scaler)\n    else:\n        logger.info(""Skipping Scaling"")\n\n    # Perform dimensionality reduction only on base feature set\n    base_features = all_features\n\n    # Calculate the total, mean, standard deviation, and variance\n\n    if numpy_flag:\n        np_features, fnames = create_numpy_features(base_features, sentinel)\n        all_features = np.column_stack((all_features, np_features))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Generate scipy features\n\n    if scipy_flag:\n        sp_features, fnames = create_scipy_features(base_features, sentinel)\n        all_features = np.column_stack((all_features, sp_features))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Create clustering features\n\n    if clustering:\n        cfeatures, fnames = create_clusters(base_features, model)\n        all_features = np.column_stack((all_features, cfeatures))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Create PCA features\n\n    if pca:\n        pfeatures, fnames = create_pca_features(base_features, model)\n        all_features = np.column_stack((all_features, pfeatures))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Create Isomap features\n\n    if isomap:\n        ifeatures, fnames = create_isomap_features(base_features, model)\n        all_features = np.column_stack((all_features, ifeatures))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Create T-SNE features\n\n    if tsne:\n        tfeatures, fnames = create_tsne_features(base_features, model)\n        all_features = np.column_stack((all_features, tfeatures))\n        model.feature_names.extend(fnames)\n        logger.info(""New Feature Count : %d"", all_features.shape[1])\n\n    # Return all transformed training and test features\n    assert all_features.shape[1] == len(model.feature_names), ""Mismatched Features and Names""\n    return all_features\n\n\n#\n# Function select_features\n#\n\ndef select_features(model):\n    r""""""Select features with univariate selection.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object with the feature selection specifications.\n\n    Returns\n    -------\n    model : alphapy.Model\n        Model object with the revised number of features.\n\n    References\n    ----------\n    You can find more information on univariate feature selection here [UNI]_.\n\n    .. [UNI] http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n\n    """"""\n\n    logger.info(""Feature Selection"")\n\n    # Extract model data.\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Extract model parameters.\n\n    fs_percentage = model.specs[\'fs_percentage\']\n    fs_score_func = model.specs[\'fs_score_func\']\n\n    # Select top features based on percentile.\n\n    fs = SelectPercentile(score_func=fs_score_func,\n                          percentile=fs_percentage)\n\n    # Perform feature selection and get the support mask\n\n    fsfit = fs.fit(X_train, y_train)\n    support = fsfit.get_support()\n\n    # Record the support vector\n\n    logger.info(""Saving Univariate Support"")\n    model.feature_map[\'uni_support\'] = support\n\n    # Record the support vector\n\n    X_train_new = model.X_train[:, support]\n    X_test_new = model.X_test[:, support]\n\n    # Count the number of new features.\n\n    logger.info(""Old Feature Count : %d"", X_train.shape[1])\n    logger.info(""New Feature Count : %d"", X_train_new.shape[1])\n\n    # Store the reduced features in the model.\n\n    model.X_train = X_train_new\n    model.X_test = X_test_new\n\n    # Mask the feature names and test that feature and name lengths are equal\n\n    model.feature_names = list(itertools.compress(model.feature_names, support))\n    assert X_train_new.shape[1] == len(model.feature_names), ""Mismatched Features and Names""\n\n    # Return the modified model\n    return model\n\n\n#\n# Function save_features\n#\n\ndef save_features(model, X_train, X_test, y_train=None, y_test=None):\n    r""""""Save new features to the model.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object with train and test data.\n    X_train : numpy array\n        Training features.\n    X_test : numpy array\n        Testing features.\n    y_train : numpy array\n        Training labels.\n    y_test : numpy array\n        Testing labels.\n\n    Returns\n    -------\n    model : alphapy.Model\n        Model object with new train and test data.\n\n    """"""\n\n    logger.info(""Saving New Features in Model"")\n\n    model.X_train = X_train\n    model.X_test = X_test\n    if y_train is not None:\n        model.y_train = y_train\n    if y_test is not None:\n        model.y_test = y_test\n\n    return model\n\n\n#\n# Function create_interactions\n#\n\ndef create_interactions(model, X):\n    r""""""Create feature interactions based on the model specifications.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model object with train and test data.\n    X : numpy array\n        Feature Matrix.\n\n    Returns\n    -------\n    all_features : numpy array\n        The new interaction features.\n\n    Raises\n    ------\n    TypeError\n        Unknown model type when creating interactions.\n\n    """"""\n\n    logger.info(""Creating Interactions"")\n\n    # Extract model parameters\n\n    interactions = model.specs[\'interactions\']\n    isample_pct = model.specs[\'isample_pct\']\n    model_type = model.specs[\'model_type\']\n    poly_degree = model.specs[\'poly_degree\']\n    predict_mode = model.specs[\'predict_mode\']\n\n    # Extract model data\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Log parameters\n    logger.info(""Initial Feature Count  : %d"", X.shape[1])\n\n    # Initialize all features\n    all_features = X\n\n    # Get polynomial features\n\n    if interactions:\n        if not predict_mode:\n            logger.info(""Generating Polynomial Features"")\n            logger.info(""Interaction Percentage : %d"", isample_pct)\n            logger.info(""Polynomial Degree      : %d"", poly_degree)\n            if model_type == ModelType.regression:\n                selector = SelectPercentile(f_regression, percentile=isample_pct)\n            elif model_type == ModelType.classification:\n                selector = SelectPercentile(f_classif, percentile=isample_pct)\n            else:\n                raise TypeError(""Unknown model type when creating interactions"")\n            selector.fit(X_train, y_train)\n            support = selector.get_support()\n            model.feature_map[\'poly_support\'] = support\n        else:\n            support = model.feature_map[\'poly_support\']\n        pfeatures, pnames = get_polynomials(X[:, support], poly_degree)\n        model.feature_names.extend(pnames)\n        logger.info(""Polynomial Feature Count : %d"", pfeatures.shape[1])\n        pfeatures = StandardScaler().fit_transform(pfeatures)\n        all_features = np.hstack((all_features, pfeatures))\n        logger.info(""New Total Feature Count  : %d"", all_features.shape[1])\n    else:\n        logger.info(""Skipping Interactions"")\n\n    # Return all features\n    assert all_features.shape[1] == len(model.feature_names), ""Mismatched Features and Names""\n    return all_features\n\n\n#\n# Function drop_features\n#\n\ndef drop_features(X, drop):\n    r""""""Drop any specified features.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame\n        The dataframe containing the features.\n    drop : list\n        The list of features to remove from ``X``.\n\n    Returns\n    -------\n    X : pandas.DataFrame\n        The dataframe without the dropped features.\n\n    """"""\n    drop_cols = []\n    if drop:\n        for d in drop:\n            for col in X.columns:\n                if col.split(LOFF)[0] == d:\n                    drop_cols.append(col)\n        logger.info(""Dropping Features: %s"", drop_cols)\n        logger.info(""Original Feature Count : %d"", X.shape[1])\n        X.drop(drop_cols, axis=1, inplace=True, errors=\'ignore\')\n        logger.info(""Reduced Feature Count  : %d"", X.shape[1])\n    return X\n\n\n#\n# Function remove_lv_features\n#\n\ndef remove_lv_features(model, X):\n    r""""""Remove low-variance features.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        Model specifications for removing features.\n    X : numpy array\n        The feature matrix.\n\n    Returns\n    -------\n    X_reduced : numpy array\n        The reduced feature matrix.\n\n    References\n    ----------\n    You can find more information on low-variance feature selection here [LV]_.\n\n    .. [LV] http://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold\n\n    """"""\n\n    logger.info(""Removing Low-Variance Features"")\n\n    # Extract model parameters\n\n    lv_remove = model.specs[\'lv_remove\']\n    lv_threshold = model.specs[\'lv_threshold\']\n    predict_mode = model.specs[\'predict_mode\']\n\n    # Remove low-variance features\n\n    if lv_remove:\n        logger.info(""Low-Variance Threshold  : %.2f"", lv_threshold)\n        logger.info(""Original Feature Count  : %d"", X.shape[1])\n        if not predict_mode:\n            selector = VarianceThreshold(threshold=lv_threshold)\n            selector.fit(X)\n            support = selector.get_support()\n            model.feature_map[\'lv_support\'] = support\n        else:\n            support = model.feature_map[\'lv_support\']\n        X_reduced = X[:, support]\n        model.feature_names = list(itertools.compress(model.feature_names, support))\n        logger.info(""Reduced Feature Count   : %d"", X_reduced.shape[1])\n    else:\n        X_reduced = X\n        logger.info(""Skipping Low-Variance Features"")\n\n    assert X_reduced.shape[1] == len(model.feature_names), ""Mismatched Features and Names""\n    return X_reduced\n'"
alphapy/frame.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : frame\n# Created   : July 11, 2013\n#\n# Copyright 2017 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import PSEP, SSEP, USEP\nfrom alphapy.globals import TAG_ID\n\nimport logging\nimport pandas as pd\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function frame_name\n#\n\ndef frame_name(name, space):\n    r""""""Get the frame name for the given name and space.\n\n    Parameters\n    ----------\n    name : str\n        Group name.\n    space : alphapy.Space\n        Context or namespace for the given group name.\n\n    Returns\n    -------\n    fname : str\n        Frame name.\n\n    Examples\n    --------\n\n    >>> fname = frame_name(\'tech\', Space(\'stock\', \'prices\', \'1d\'))\n    # \'tech_stock_prices_1d\'\n\n    """"""\n    return USEP.join([name, space.subject, space.schema, space.fractal])\n\n\n#\n# Class Frame\n#\n\nclass Frame(object):\n    """"""Create a new Frame that points to a dataframe in memory. All\n    frames are stored in ``Frame.frames``. Names must be unique.\n\n    Parameters\n    ----------\n    name : str\n        Frame key.\n    space : alphapy.Space\n        Namespace of the given frame.\n    df : pandas.DataFrame\n        The contents of the actual dataframe.\n\n    Attributes\n    ----------\n    frames : dict\n        Class variable for storing all known frames\n\n    Examples\n    --------\n    \n    >>> Frame(\'tech\', Space(\'stock\', \'prices\', \'5m\'), df)\n\n    """"""\n\n    # class variable to track all frames\n\n    frames = {}\n\n    # __init__\n\n    def __init__(self,\n                 name,\n                 space,\n                 df):\n        # code\n        if df.__class__.__name__ == \'DataFrame\':\n            fn = frame_name(name, space)\n            if not fn in Frame.frames:\n                self.name = name\n                self.space = space\n                self.df = df\n                # add frame to frames list\n                Frame.frames[fn] = self\n            else:\n                logger.info(""Frame "", fn, "" already exists"")\n        else:\n            logger.info(""df must be of type Pandas DataFrame"")\n        \n    # __str__\n\n    def __str__(self):\n        return frame_name(self.name, self.space)\n\n\n#\n# Function read_frame\n#\n\ndef read_frame(directory, filename, extension, separator,\n               index_col=None, squeeze=False):\n    r""""""Read a delimiter-separated file into a data frame.\n\n    Parameters\n    ----------\n    directory : str\n        Full directory specification.\n    filename : str\n        Name of the file to read, excluding the ``extension``.\n    extension : str\n        File name extension, e.g., ``csv``.\n    separator : str\n        The delimiter between fields in the file.\n    index_col : str, optional\n        Column to use as the row labels in the dataframe.\n    squeeze : bool, optional\n        If the data contains only one column, then return a pandas Series.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The pandas dataframe loaded from the file location. If the file\n        cannot be located, then ``None`` is returned.\n\n    """"""\n    file_only = PSEP.join([filename, extension])\n    file_all = SSEP.join([directory, file_only])\n    logger.info(""Loading data from %s"", file_all)\n    try:\n        df = pd.read_csv(file_all, sep=separator, index_col=index_col,\n                         squeeze=squeeze, low_memory=False)\n    except:\n        df = pd.DataFrame()\n        logger.info(""Could not find or access %s"", file_all)\n    return df\n\n\n#\n# Function write_frame\n#\n\ndef write_frame(df, directory, filename, extension, separator,\n                index=False, index_label=None, columns=None):\n    r""""""Write a dataframe into a delimiter-separated file.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The pandas dataframe to save to a file.\n    directory : str\n        Full directory specification.\n    filename : str\n        Name of the file to write, excluding the ``extension``.\n    extension : str\n        File name extension, e.g., ``csv``.\n    separator : str\n        The delimiter between fields in the file.\n    index : bool, optional\n        If ``True``, write the row names (index).\n    index_label : str, optional\n        A column label for the ``index``.\n    columns : str, optional\n        A list of column names.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n    file_only = PSEP.join([filename, extension])\n    file_all = SSEP.join([directory, file_only])\n    logger.info(""Writing data frame to %s"", file_all)\n    try:\n        df.to_csv(file_all, sep=separator, index=index,\n                  index_label=index_label, columns=columns)\n    except:\n        logger.info(""Could not write data frame to %s"", file_all)\n\n\n#\n# Function load_frames\n#\n\ndef load_frames(group, directory, extension, separator, splits=False):        \n    r""""""Read a group of dataframes into memory.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The collection of frames to be read into memory.\n    directory : str\n        Full directory specification.\n    extension : str\n        File name extension, e.g., ``csv``.\n    separator : str\n        The delimiter between fields in the file.\n    splits : bool, optional\n        If ``True``, then all the members of the group are stored in\n        separate files corresponding with each member. If ``False``,\n        then the data are stored in a single file.\n\n    Returns\n    -------\n    all_frames : list\n        The list of pandas dataframes loaded from the file location. If\n        the files cannot be located, then ``None`` is returned.\n\n    """"""\n    logger.info(""Loading frames from %s"", directory)\n    gname = group.name\n    gspace = group.space\n    # If this is a group analysis, then consolidate the frames.\n    # Otherwise, the frames are already aggregated.\n    all_frames = []\n    if splits:\n        gnames = [item.lower() for item in group.members]\n        for gn in gnames:\n            fname = frame_name(gn, gspace)\n            if fname in Frame.frames:\n                logger.info(""Joining Frame %s"", fname)\n                df = Frame.frames[fname].df\n            else:\n                logger.info(""Data Frame for %s not found"", fname)\n                # read file for corresponding frame\n                logger.info(""Load Data Frame %s from file"", fname)\n                df = read_frame(directory, fname, extension, separator)\n            # add this frame to the consolidated frame list\n            if not df.empty:\n                # set the name\n                df.insert(0, TAG_ID, gn)\n                all_frames.append(df)\n            else:\n                logger.debug(""Empty Data Frame for: %s"", gn)\n    else:\n        # no splits, so use data from consolidated files\n        fname = frame_name(gname, gspace)\n        df = read_frame(directory, fname, extension, separator)\n        if not df.empty:\n            all_frames.append(df)\n    return all_frames\n\n\n#\n# Function dump_frames\n#\n\ndef dump_frames(group, directory, extension, separator):        \n    r""""""Save a group of data frames to disk.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The collection of frames to be saved to the file system.\n    directory : str\n        Full directory specification.\n    extension : str\n        File name extension, e.g., ``csv``.\n    separator : str\n        The delimiter between fields in the file.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n    logger.info(""Dumping frames from %s"", directory)\n    gnames = [item.lower() for item in group.members]\n    gspace = group.space\n    for gn in gnames:\n        fname = frame_name(gn, gspace)\n        if fname in Frame.frames:\n            logger.info(""Writing Data Frame for %s"", fname)\n            df = Frame.frames[fname].df\n            write_frame(df, directory, fname, extension, separator, index=True)\n        else:\n            logger.info(""Data Frame for %s not found"", fname)\n\n\n#\n# Function sequence_frame\n#\n\ndef sequence_frame(df, target, forecast_period=1, leaders=[], lag_period=1):\n    r""""""Create sequences of lagging and leading values.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The original dataframe.\n    target : str\n        The target variable for prediction.\n    forecast_period : int\n        The period for forecasting the target of the analysis.\n    leaders : list\n        The features that are contemporaneous with the target.\n    lag_period : int\n        The number of lagged rows for prediction.\n\n    Returns\n    -------\n    new_frame : pandas.DataFrame\n        The transformed dataframe with variable sequences.\n\n    """"""\n\n    # Set Leaders and Laggards\n    le_cols = sorted(leaders)\n    le_len = len(le_cols)\n    df_cols = sorted(list(set(df.columns) - set(le_cols)))\n    df_len = len(df_cols)\n\n    # Add lagged columns\n    new_cols, new_names = list(), list()\n    for i in range(lag_period, 0, -1):\n        new_cols.append(df[df_cols].shift(i))\n        new_names += [\'%s[%d]\' % (df_cols[j], i) for j in range(df_len)]\n\n    # Preserve leader columns\n    new_cols.append(df[le_cols])\n    new_names += [le_cols[j] for j in range(le_len)]\n\n    # Forecast Target(s)\n    new_cols.append(pd.DataFrame(df[target].shift(1-forecast_period)))\n    new_names.append(target)\n\n    # Collect all columns into new frame\n    new_frame = pd.concat(new_cols, axis=1)\n    new_frame.columns = new_names\n    return new_frame\n'"
alphapy/globals.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : globals\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom enum import Enum, unique\n\n\n#\n# Global Variables\n#\n\n#\n# Delimiters\n#\n\nBSEP = \' \'\nCSEP = \':\'\nPSEP = \'.\'\nSSEP = \'/\'\nUSEP = \'_\'\nLOFF = \'[\'\nROFF = \']\'\n\n#\n# Numerical Constants\n#\n\nQ1 = 0.25\nQ2 = 0.50\nQ3 = 0.75\n\n#\n# String Constants\n#\n\nNULLTEXT = \'NULLTEXT\'\nTAG_ID = \'tag\'\nWILDCARD = \'*\'\n\n#\n# Dictionaries\n#\n\nMULTIPLIERS = {\'crypto\' : 1.0,\n               \'stock\' : 1.0}\n\n#\n# Pandas Time Offset Aliases\n#\n\nPD_INTRADAY_OFFSETS = [\'H\', \'T\', \'min\', \'S\', \'L\', \'ms\', \'U\', \'us\', \'N\']\n\n#\n# Encoder Types\n#\n\n@unique\nclass Encoders(Enum):\n    """"""AlphaPy Encoders.\n\n    These are the encoders used in AlphaPy, as configured in the\n    ``model.yml`` file (features:encoding:type) You can learn more\n    about encoders here [ENC]_.\n\n    .. [ENC] https://github.com/scikit-learn-contrib/categorical-encoding\n\n    """"""\n    backdiff = 1\n    basen = 2\n    binary = 3\n    catboost = 4\n    hashing = 5\n    helmert = 6\n    jstein = 7\n    leaveone = 8\n    mestimate = 9\n    onehot = 10\n    ordinal = 11\n    polynomial = 12\n    sum = 13\n    target = 14\n    woe = 15\n\n\n#\n# Model Types\n#\n\n@unique\nclass ModelType(Enum):\n    """"""AlphaPy Model Types.\n\n    .. note:: One-Class Classification ``oneclass`` is not yet\n       implemented.\n\n    """"""\n    classification = 1\n    clustering = 2\n    multiclass = 3\n    oneclass = 4\n    regression = 5\n\n\n#\n# Objective Functions\n#\n\n@unique\nclass Objective(Enum):\n    """"""Scoring Function Objectives.\n\n    Best model selection is based on the scoring or Objective\n    function, which must be either maximized or minimized. For\n    example, ``roc_auc`` is maximized, while ``neg_log_loss``\n    is minimized.\n\n    """"""\n    maximize = 1\n    minimize = 2\n\n\n#\n# Class Orders\n#\n\nclass Orders:\n    """"""System Order Types.\n\n    Attributes\n    ----------\n    le : str\n        long entry\n    se : str\n        short entry\n    lx : str\n        long exit\n    sx : str\n        short exit\n    lh : str\n        long exit at the end of the holding period\n    sh : str\n        short exit at the end of the holding period\n\n    """"""\n    le = \'le\'\n    se = \'se\'\n    lx = \'lx\'\n    sx = \'sx\'\n    lh = \'lh\'\n    sh = \'sh\'\n\n\n#\n# Partition Types\n#\n\n@unique\nclass Partition(Enum):\n    """"""AlphaPy Partitions.\n\n    """"""\n    predict = 1\n    test = 2\n    train = 3\n\n\n#\n# Sampling Methods\n#\n\n@unique\nclass SamplingMethod(Enum):\n    """"""AlphaPy Sampling Methods.\n\n    These are the data sampling methods used in AlphaPy, as configured\n    in the ``model.yml`` file (data:sampling:method) You can learn more\n    about resampling techniques here [IMB]_.\n\n    .. [IMB] https://github.com/scikit-learn-contrib/imbalanced-learn\n\n    """"""\n    ensemble_bc = 1\n    ensemble_easy = 2\n    over_random = 3\n    over_smote = 4\n    over_smoteb = 5\n    over_smotesv = 6\n    overunder_smote_enn = 7\n    overunder_smote_tomek = 8\n    under_cluster = 9\n    under_ncr = 10\n    under_nearmiss = 11\n    under_random = 12\n    under_tomek = 13\n\n\n#\n# Scaler Types\n#\n\n@unique\nclass Scalers(Enum):\n    """"""AlphaPy Scalers.\n\n    These are the scaling methods used in AlphaPy, as configured in the\n    ``model.yml`` file (features:scaling:type) You can learn more about\n    feature scaling here [SCALE]_.\n\n    .. [SCALE] http://scikit-learn.org/stable/modules/preprocessing.html\n\n    """"""\n    minmax = 1\n    standard = 2\n\n\n#\n# Datasets\n#\n\ndatasets = {Partition.train   : \'train\',\n            Partition.test    : \'test\',\n            Partition.predict : \'predict\'}\n'"
alphapy/group.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : group\n# Created   : July 11, 2013\n#\n# Copyright 2017 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import USEP\nfrom alphapy.space import Space\n\nimport logging\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Class Group\n#\n\nclass Group(object):\n    """"""Create a new Group that contains common members. All\n    defined groups are stored in ``Group.groups``. Group\n    names must be unique.\n\n    Parameters\n    ----------\n    name : str\n        Group name.\n    space : alphapy.Space, optional\n        Namespace for the given group.\n    dynamic : bool, optional, default ``True``\n        Flag for defining whether or not the group membership\n        can change.\n    members : set, optional\n        The initial members of the group, especially if the\n        new group is fixed, e.g., not ``dynamic``.\n\n    Attributes\n    ----------\n    groups : dict\n        Class variable for storing all known groups\n\n    Examples\n    --------\n    \n    >>> Group(\'tech\')\n\n    """"""\n\n    # class variable to track all groups\n\n    groups = {}\n    \n    # function __init__\n    \n    def __init__(self,\n                 name,\n                 space = Space(),\n                 dynamic = True,\n                 members = set()):\n        # code\n        if not name in Group.groups:\n            self.name = name\n            self.space = space\n            self.dynamic =  dynamic\n            self.members = members\n            # add group to groups list\n            Group.groups[name] = self\n        else:\n            logger.info(""Group already %s exists"", name)\n        \n    # function __str__\n\n    def __str__(self):\n        return self.name\n            \n    # function add\n            \n    def add(self,\n            newlist):\n        r""""""Add new members to the group.\n\n        Parameters\n        ----------\n        newlist : list\n            New members or identifiers to add to the group.\n\n        Returns\n        -------\n        None : None\n\n        Notes\n        -----\n\n        New members cannot be added to a fixed or non-dynamic group.\n\n        """"""\n        if all([type(item) is str for item in newlist]):\n            newset = set(newlist)\n            if self.dynamic:\n                if newset.issubset(self.members):\n                    logger.info(""New members already in set"")\n                else:\n                    madd = newset - self.members\n                    self.members = self.members | newset\n                    logger.info(""Added: %s"", madd)\n            else:\n                logger.info(""Cannot add members to a non-dynamic group"")\n        else:\n            logger.info(""All new members must be of type str"")\n            \n    # function member\n            \n    def member(self, item):\n        r""""""Find a member in the group.\n\n        Parameters\n        ----------\n        item : str\n            The member to find the group.\n\n        Returns\n        -------\n        member_exists : bool\n            Flag indicating whether or not the member is in the group.\n\n        """"""\n        return item in self.members\n        \n    # function remove\n    \n    def remove(self, remlist):\n        r""""""Read in data from the given directory in a given format.\n\n        Parameters\n        ----------\n        remlist : list\n            The list of members to remove from the group.\n\n        Returns\n        -------\n        None : None\n\n        Notes\n        -----\n\n        Members cannot be removed from a fixed or non-dynamic group.\n\n        """"""\n        if self.dynamic:\n            nonefound = not any([self.member(item) for item in remlist])\n            if nonefound == True:\n                logger.info(""Members to remove not found"")\n            else:\n                removed = []\n                for item in remlist:\n                    if self.member(item):\n                        self.members.remove(item)\n                        removed += [item]\n                logger.info(""Removed: %s"", removed)\n        else:\n            logger.info(""Cannot remove members from a non-dynamic group"")\n'"
alphapy/market_flow.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : market_flow\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.alias import Alias\nfrom alphapy.analysis import Analysis\nfrom alphapy.analysis import run_analysis\nfrom alphapy.data import get_market_data\nfrom alphapy.globals import PD_INTRADAY_OFFSETS\nfrom alphapy.globals import PSEP, SSEP\nfrom alphapy.group import Group\nfrom alphapy.variables import Variable\nfrom alphapy.variables import vmapply\nfrom alphapy.model import get_model_config\nfrom alphapy.model import Model\nfrom alphapy.portfolio import gen_portfolio\nfrom alphapy.space import Space\nfrom alphapy.system import run_system\nfrom alphapy.system import System\nfrom alphapy.utilities import valid_date\n\nimport argparse\nimport datetime\nimport logging\nimport os\nimport pandas as pd\nimport sys\nimport warnings\nimport yaml\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function get_market_config\n#\n\ndef get_market_config():\n    r""""""Read the configuration file for MarketFlow.\n\n    Parameters\n    ----------\n    None : None\n\n    Returns\n    -------\n    specs : dict\n        The parameters for controlling MarketFlow.\n\n    """"""\n\n    logger.info(""MarketFlow Configuration"")\n\n    # Read the configuration file\n\n    full_path = SSEP.join([PSEP, \'config\', \'market.yml\'])\n    with open(full_path, \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n\n    # Store configuration parameters in dictionary\n\n    specs = {}\n\n    # Section: market [this section must be first]\n\n    specs[\'create_model\'] = cfg[\'market\'][\'create_model\']\n    fractal = cfg[\'market\'][\'data_fractal\']\n    try:\n        _ = pd.to_timedelta(fractal)\n    except:\n        logger.info(""data_fractal [%s] is an invalid pandas offset"",\n                    fractal)\n    specs[\'data_fractal\'] = fractal\n    specs[\'data_history\'] = cfg[\'market\'][\'data_history\']\n    specs[\'forecast_period\'] = cfg[\'market\'][\'forecast_period\']\n    fractal = cfg[\'market\'][\'fractal\']\n    try:\n        test_interval = pd.to_timedelta(fractal)\n    except:\n        logger.info(""fractal [%s] is an invalid pandas offset"",\n                    fractal)\n    specs[\'fractal\'] = fractal\n    specs[\'lag_period\'] = cfg[\'market\'][\'lag_period\']\n    specs[\'leaders\'] = cfg[\'market\'][\'leaders\']\n    specs[\'predict_history\'] = cfg[\'market\'][\'predict_history\']\n    specs[\'schema\'] = cfg[\'market\'][\'schema\']\n    specs[\'subschema\'] = cfg[\'market\'][\'subschema\']\n    specs[\'api_key_name\'] = cfg[\'market\'][\'api_key_name\']\n    specs[\'api_key\'] = cfg[\'market\'][\'api_key\']\n    specs[\'subject\'] = cfg[\'market\'][\'subject\']\n    specs[\'target_group\'] = cfg[\'market\'][\'target_group\']\n\n    # Set API Key environment variable\n    if specs[\'api_key\']:\n        os.environ[specs[\'api_key_name\']] = specs[\'api_key\']\n\n    # Create the subject/schema/fractal namespace\n\n    sspecs = [specs[\'subject\'], specs[\'schema\'], specs[\'fractal\']]\n    space = Space(*sspecs)\n\n    # Section: features\n\n    try:\n        logger.info(""Getting Features"")\n        specs[\'features\'] = cfg[\'features\']\n    except:\n        logger.info(""No Features Found"")\n        specs[\'features\'] = {}\n\n    # Section: groups\n\n    try:\n        logger.info(""Defining Groups"")\n        for g, m in list(cfg[\'groups\'].items()):\n            Group(g, space)\n            Group.groups[g].add(m)\n    except:\n        logger.info(""No Groups Found"")\n\n    # Section: aliases\n\n    try:\n        logger.info(""Defining Aliases"")\n        for k, v in list(cfg[\'aliases\'].items()):\n            Alias(k, v)\n    except:\n        logger.info(""No Aliases Found"")\n\n    # Section: system\n\n    try:\n        logger.info(""Getting System Parameters"")\n        specs[\'system\'] = cfg[\'system\']\n    except:\n        logger.info(""No System Parameters Found"")\n        specs[\'system\'] = {}\n\n    # Section: variables\n\n    logger.info(""Defining AlphaPy Variables [phigh, plow]"")\n\n    Variable(\'phigh\', \'probability >= 0.7\')\n    Variable(\'plow\', \'probability <= 0.3\')\n\n    try:\n        logger.info(""Defining User Variables"")\n        for k, v in list(cfg[\'variables\'].items()):\n            Variable(k, v)\n    except:\n        logger.info(""No Variables Found"")\n\n    # Section: functions\n\n    try:\n        logger.info(""Getting Variable Functions"")\n        specs[\'functions\'] = cfg[\'functions\']\n    except:\n        logger.info(""No Variable Functions Found"")\n        specs[\'functions\'] = {}\n\n    # Log the stock parameters\n\n    logger.info(\'MARKET PARAMETERS:\')\n    logger.info(\'api_key         = %s\', specs[\'api_key\'])\n    logger.info(\'api_key_name    = %s\', specs[\'api_key_name\'])\n    logger.info(\'create_model    = %r\', specs[\'create_model\'])\n    logger.info(\'data_fractal    = %s\', specs[\'data_fractal\'])\n    logger.info(\'data_history    = %d\', specs[\'data_history\'])\n    logger.info(\'features        = %s\', specs[\'features\'])\n    logger.info(\'forecast_period = %d\', specs[\'forecast_period\'])\n    logger.info(\'fractal         = %s\', specs[\'fractal\'])\n    logger.info(\'lag_period      = %d\', specs[\'lag_period\'])\n    logger.info(\'leaders         = %s\', specs[\'leaders\'])\n    logger.info(\'predict_history = %s\', specs[\'predict_history\'])\n    logger.info(\'schema          = %s\', specs[\'schema\'])\n    logger.info(\'subject         = %s\', specs[\'subject\'])\n    logger.info(\'subschema       = %s\', specs[\'subschema\'])\n    logger.info(\'system          = %s\', specs[\'system\'])\n    logger.info(\'target_group    = %s\', specs[\'target_group\'])\n\n    # Market Specifications\n    return specs\n\n\n#\n# Function market_pipeline\n#\n\ndef market_pipeline(model, market_specs):\n    r""""""AlphaPy MarketFlow Pipeline\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object for AlphaPy.\n    market_specs : dict\n        The specifications for controlling the MarketFlow pipeline.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The final results are stored in the model object.\n\n    Notes\n    -----\n    (1) Define a group.\n    (2) Get the market data.\n    (3) Apply system features.\n    (4) Create an analysis.\n    (5) Run the analysis, which calls AlphaPy.\n\n    """"""\n\n    logger.info(""Running MarketFlow Pipeline"")\n\n    # Get model specifications\n\n    predict_mode = model.specs[\'predict_mode\']\n    target = model.specs[\'target\']\n\n    # Get market specifications\n\n    create_model = market_specs[\'create_model\']\n    data_history = market_specs[\'data_history\']\n    features = market_specs[\'features\']\n    forecast_period = market_specs[\'forecast_period\']\n    fractal = market_specs[\'fractal\']\n    functions = market_specs[\'functions\']\n    lag_period = market_specs[\'lag_period\']\n    leaders = market_specs[\'leaders\']\n    predict_history = market_specs[\'predict_history\']\n    target_group = market_specs[\'target_group\']\n\n    # Set the target group\n\n    group = Group.groups[target_group]\n    logger.info(""All Symbols: %s"", group.members)\n\n    # Determine whether or not this is an intraday analysis.\n\n    intraday = any(substring in fractal for substring in PD_INTRADAY_OFFSETS)\n\n    # Get stock data. If we can\'t get all the data, then\n    # predict_history resets to the actual history obtained.\n\n    lookback = predict_history if predict_mode else data_history\n    npoints = get_market_data(model, market_specs, group, lookback, intraday)\n    if npoints > 0:\n        logger.info(""Number of Data Points: %d"", npoints)\n    else:\n        raise ValueError(""Could not get market data from source"")\n\n    # Run an analysis to create the model\n\n    if create_model:\n        logger.info(""Creating Model"")\n        # apply features to all of the frames\n        vmapply(group, features, functions)\n        vmapply(group, [target], functions)\n        # run the analysis, including the model pipeline\n        a = Analysis(model, group)\n        run_analysis(a, lag_period, forecast_period, leaders, predict_history)\n    else:\n        logger.info(""No Model (System Only)"")\n\n    # Run a system\n\n    system_specs = market_specs[\'system\']\n    if system_specs:\n        # get the system specs\n        system_name = system_specs[\'name\']\n        longentry = system_specs[\'longentry\']\n        shortentry = system_specs[\'shortentry\']\n        longexit = system_specs[\'longexit\']\n        shortexit = system_specs[\'shortexit\']\n        holdperiod = system_specs[\'holdperiod\']\n        scale = system_specs[\'scale\']\n        logger.info(""Running System %s"", system_name)\n        logger.info(""Long Entry  : %s"", longentry)\n        logger.info(""Short Entry : %s"", shortentry)\n        logger.info(""Long Exit   : %s"", longexit)\n        logger.info(""Short Exit  : %s"", shortexit)\n        logger.info(""Hold Period : %d"", holdperiod)\n        logger.info(""Scale       : %r"", scale)\n        # create and run the system\n        system = System(system_name, longentry, shortentry,\n                        longexit, shortexit, holdperiod, scale)\n        tfs = run_system(model, system, group, intraday)\n        # generate a portfolio\n        gen_portfolio(model, system_name, group, tfs)\n\n    # Return the completed model\n    return model\n\n\n#\n# Function main\n#\n\ndef main(args=None):\n    r""""""MarketFlow Main Program\n\n    Notes\n    -----\n    (1) Initialize logging.\n    (2) Parse the command line arguments.\n    (3) Get the market configuration.\n    (4) Get the model configuration.\n    (5) Create the model object.\n    (6) Call the main MarketFlow pipeline.\n\n    Raises\n    ------\n    ValueError\n        Training date must be before prediction date.\n\n    """"""\n\n    # Suppress Warnings\n\n    warnings.simplefilter(action=\'ignore\', category=DeprecationWarning)\n    warnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\n    # Logging\n\n    logging.basicConfig(format=""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                        filename=""market_flow.log"", filemode=\'a\', level=logging.DEBUG,\n                        datefmt=\'%m/%d/%y %H:%M:%S\')\n    formatter = logging.Formatter(""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                                  datefmt=\'%m/%d/%y %H:%M:%S\')\n    console = logging.StreamHandler()\n    console.setFormatter(formatter)\n    console.setLevel(logging.INFO)\n    logging.getLogger().addHandler(console)\n\n    # Start the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""MarketFlow Start"")\n    logger.info(\'*\'*80)\n\n    # Argument Parsing\n\n    parser = argparse.ArgumentParser(description=""MarketFlow Parser"")\n    parser.add_argument(\'--pdate\', dest=\'predict_date\',\n                        help=""prediction date is in the format: YYYY-MM-DD"",\n                        required=False, type=valid_date)\n    parser.add_argument(\'--tdate\', dest=\'train_date\',\n                        help=""training date is in the format: YYYY-MM-DD"",\n                        required=False, type=valid_date)\n    parser.add_mutually_exclusive_group(required=False)\n    parser.add_argument(\'--predict\', dest=\'predict_mode\', action=\'store_true\')\n    parser.add_argument(\'--train\', dest=\'predict_mode\', action=\'store_false\')\n    parser.set_defaults(predict_mode=False)\n    args = parser.parse_args()\n\n    # Set train and predict dates\n\n    if args.train_date:\n        train_date = args.train_date\n    else:\n        train_date = pd.datetime(1900, 1, 1).strftime(""%Y-%m-%d"")\n\n    if args.predict_date:\n        predict_date = args.predict_date\n    else:\n        predict_date = datetime.date.today().strftime(""%Y-%m-%d"")\n\n    # Verify that the dates are in sequence.\n\n    if train_date >= predict_date:\n        raise ValueError(""Training date must be before prediction date"")\n    else:\n        logger.info(""Training Date: %s"", train_date)\n        logger.info(""Prediction Date: %s"", predict_date)\n\n    # Read stock configuration file\n    market_specs = get_market_config()\n\n    # Read model configuration file\n\n    model_specs = get_model_config()\n    model_specs[\'predict_mode\'] = args.predict_mode\n    model_specs[\'predict_date\'] = predict_date\n    model_specs[\'train_date\'] = train_date\n\n    # Create directories if necessary\n\n    output_dirs = [\'config\', \'data\', \'input\', \'model\', \'output\', \'plots\', \'systems\']\n    for od in output_dirs:\n        output_dir = SSEP.join([model_specs[\'directory\'], od])\n        if not os.path.exists(output_dir):\n            logger.info(""Creating directory %s"", output_dir)\n            os.makedirs(output_dir)\n\n    # Create a model object from the specifications\n    model = Model(model_specs)\n\n    # Start the pipeline\n    model = market_pipeline(model, market_specs)\n\n    # Complete the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""MarketFlow End"")\n    logger.info(\'*\'*80)\n\n\n#\n# MAIN PROGRAM\n#\n\nif __name__ == ""__main__"":\n    main()\n'"
alphapy/model.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : model\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.estimators import scorers\nfrom alphapy.estimators import xgb_score_map\nfrom alphapy.features import feature_scorers\nfrom alphapy.frame import read_frame\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import Encoders\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Objective\nfrom alphapy.globals import Partition, datasets\nfrom alphapy.globals import PSEP, SSEP, USEP\nfrom alphapy.globals import SamplingMethod\nfrom alphapy.globals import Scalers\nfrom alphapy.utilities import get_datestamp\nfrom alphapy.utilities import most_recent_file\n\nfrom copy import copy\nfrom datetime import datetime\nimport itertools\nimport joblib\nfrom keras.models import load_model\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport yaml\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Class Model\n#\n# model unifies algorithms and we use hasattr to list the available attrs for each\n# algorithm so users can query an algorithm and get the list of attributes\n#\n\nclass Model:\n    """"""Create a new model.\n\n    Parameters\n    ----------\n    specs : dict\n        The model specifications obtained by reading the ``model.yml``\n        file.\n\n    Attributes\n    ----------\n    specs : dict\n        The model specifications.\n    X_train : pandas.DataFrame\n        Training features in matrix format.\n    X_test  : pandas.Series\n        Testing features in matrix format.\n    y_train : pandas.DataFrame\n        Training labels in vector format.\n    y_test  : pandas.Series\n        Testing labels in vector format.\n    algolist : list\n        Algorithms to use in training.\n    estimators : dict\n        Dictionary of estimators (key: algorithm)\n    importances : dict\n        Feature Importances (key: algorithm)\n    coefs : dict\n        Coefficients, if applicable (key: algorithm)\n    support : dict\n        Support Vectors, if applicable (key: algorithm)\n    preds : dict\n        Predictions or labels (keys: algorithm, partition)\n    probas : dict\n        Probabilities from classification (keys: algorithm, partition)\n    metrics : dict\n        Model evaluation metrics (keys: algorith, partition, metric)\n\n    Raises\n    ------\n    KeyError\n        Model specs must include the key *algorithms*, which is\n        stored in ``algolist``.\n\n    """"""\n\n    # __init__\n\n    def __init__(self,\n                 specs):\n        # specifications\n        self.specs = specs\n        # data in memory\n        self.X_train = None\n        self.X_test = None\n        self.y_train = None\n        self.y_test = None\n        # test labels\n        self.test_labels = False\n        # datasets\n        self.train_file = datasets[Partition.train]\n        self.test_file = datasets[Partition.test]\n        self.predict_file = datasets[Partition.predict]\n        # algorithms\n        try:\n            self.algolist = self.specs[\'algorithms\']\n        except:\n            raise KeyError(""Model specs must include the key: algorithms"")\n        self.best_algo = None\n        # feature names\n        self.feature_names = []\n        # feature map\n        self.feature_map = {}\n        # Key: (algorithm)\n        self.estimators = {}\n        self.importances = {}\n        self.coefs = {}\n        self.support = {}\n        self.fnames_algo = {}\n        # Keys: (algorithm, partition)\n        self.preds = {}\n        self.probas = {}\n        # Keys: (algorithm, partition, metric)\n        self.metrics = {}\n\n    # __str__\n\n    def __str__(self):\n        return self.name\n\n    # __getnewargs__\n\n    def __getnewargs__(self):\n        return (self.specs,)\n\n\n#\n# Function get_model_config\n#\n\ndef get_model_config():\n    r""""""Read in the configuration file for AlphaPy.\n\n    Parameters\n    ----------\n    None : None\n\n    Returns\n    -------\n    specs : dict\n        The parameters for controlling AlphaPy.\n\n    Raises\n    ------\n    ValueError\n        Unrecognized value of a ``model.yml`` field.\n\n    """"""\n\n    logger.info(""Model Configuration"")\n\n    # Read the configuration file\n\n    full_path = SSEP.join([PSEP, \'config\', \'model.yml\'])\n    with open(full_path, \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n\n    # Store configuration parameters in dictionary\n\n    specs = {}\n\n    # Section: project [this section must be first]\n\n    specs[\'directory\'] = cfg[\'project\'][\'directory\']\n    specs[\'extension\'] = cfg[\'project\'][\'file_extension\']\n    specs[\'submission_file\'] = cfg[\'project\'][\'submission_file\']\n    specs[\'submit_probas\'] = cfg[\'project\'][\'submit_probas\']\n\n    # Section: data\n\n    specs[\'drop\'] = cfg[\'data\'][\'drop\']\n    specs[\'features\'] = cfg[\'data\'][\'features\']\n    specs[\'sentinel\'] = cfg[\'data\'][\'sentinel\']\n    specs[\'separator\'] = cfg[\'data\'][\'separator\']\n    specs[\'shuffle\'] = cfg[\'data\'][\'shuffle\']\n    specs[\'split\'] = cfg[\'data\'][\'split\']\n    specs[\'target\'] = cfg[\'data\'][\'target\']\n    specs[\'target_value\'] = cfg[\'data\'][\'target_value\']\n    # sampling\n    specs[\'sampling\'] = cfg[\'data\'][\'sampling\'][\'option\']\n    # determine whether or not sampling method is valid\n    samplers = {x.name: x.value for x in SamplingMethod}\n    sampling_method = cfg[\'data\'][\'sampling\'][\'method\']\n    if sampling_method in samplers:\n        specs[\'sampling_method\'] = SamplingMethod(samplers[sampling_method])\n    else:\n        raise ValueError(""model.yml data:sampling:method %s unrecognized"" %\n                         sampling_method)\n    # end of sampling method\n    specs[\'sampling_ratio\'] = cfg[\'data\'][\'sampling\'][\'ratio\']\n\n    # Section: features\n\n    # clustering\n    specs[\'clustering\'] = cfg[\'features\'][\'clustering\'][\'option\']\n    specs[\'cluster_min\'] = cfg[\'features\'][\'clustering\'][\'minimum\']\n    specs[\'cluster_max\'] = cfg[\'features\'][\'clustering\'][\'maximum\']\n    specs[\'cluster_inc\'] = cfg[\'features\'][\'clustering\'][\'increment\']\n    # counts\n    specs[\'counts\'] = cfg[\'features\'][\'counts\'][\'option\']\n    # encoding\n    specs[\'rounding\'] = cfg[\'features\'][\'encoding\'][\'rounding\']\n    # determine whether or not encoder is valid\n    encoders = {x.name: x.value for x in Encoders}\n    encoder = cfg[\'features\'][\'encoding\'][\'type\']\n    if encoder in encoders:\n        specs[\'encoder\'] = Encoders(encoders[encoder])\n    else:\n        raise ValueError(""model.yml features:encoding:type %s unrecognized"" % encoder)\n    # factors\n    specs[\'factors\'] = cfg[\'features\'][\'factors\']\n    # interactions\n    specs[\'interactions\'] = cfg[\'features\'][\'interactions\'][\'option\']\n    specs[\'isample_pct\'] = cfg[\'features\'][\'interactions\'][\'sampling_pct\']\n    specs[\'poly_degree\'] = cfg[\'features\'][\'interactions\'][\'poly_degree\']\n    # isomap\n    specs[\'isomap\'] = cfg[\'features\'][\'isomap\'][\'option\']\n    specs[\'iso_components\'] = cfg[\'features\'][\'isomap\'][\'components\']\n    specs[\'iso_neighbors\'] = cfg[\'features\'][\'isomap\'][\'neighbors\']\n    # log transformation\n    specs[\'logtransform\'] = cfg[\'features\'][\'logtransform\'][\'option\']\n    # low-variance features\n    specs[\'lv_remove\'] = cfg[\'features\'][\'variance\'][\'option\']\n    specs[\'lv_threshold\'] = cfg[\'features\'][\'variance\'][\'threshold\']\n    # NumPy\n    specs[\'numpy\'] = cfg[\'features\'][\'numpy\'][\'option\']\n    # pca\n    specs[\'pca\'] = cfg[\'features\'][\'pca\'][\'option\']\n    specs[\'pca_min\'] = cfg[\'features\'][\'pca\'][\'minimum\']\n    specs[\'pca_max\'] = cfg[\'features\'][\'pca\'][\'maximum\']\n    specs[\'pca_inc\'] = cfg[\'features\'][\'pca\'][\'increment\']\n    specs[\'pca_whiten\'] = cfg[\'features\'][\'pca\'][\'whiten\']\n    # Scaling\n    specs[\'scaler_option\'] = cfg[\'features\'][\'scaling\'][\'option\']\n    # determine whether or not scaling type is valid\n    scaler_types = {x.name: x.value for x in Scalers}\n    scaler_type = cfg[\'features\'][\'scaling\'][\'type\']\n    if scaler_type in scaler_types:\n        specs[\'scaler_type\'] = Scalers(scaler_types[scaler_type])\n    else:\n        raise ValueError(""model.yml features:scaling:type %s unrecognized"" % scaler_type)\n    # SciPy\n    specs[\'scipy\'] = cfg[\'features\'][\'scipy\'][\'option\']\n    # text\n    specs[\'ngrams_max\'] = cfg[\'features\'][\'text\'][\'ngrams\']\n    specs[\'vectorize\'] = cfg[\'features\'][\'text\'][\'vectorize\']\n    # t-sne\n    specs[\'tsne\'] = cfg[\'features\'][\'tsne\'][\'option\']\n    specs[\'tsne_components\'] = cfg[\'features\'][\'tsne\'][\'components\']\n    specs[\'tsne_learn_rate\'] = cfg[\'features\'][\'tsne\'][\'learning_rate\']\n    specs[\'tsne_perplexity\'] = cfg[\'features\'][\'tsne\'][\'perplexity\']\n\n    # Section: model\n\n    specs[\'algorithms\'] = cfg[\'model\'][\'algorithms\']\n    specs[\'cv_folds\'] = cfg[\'model\'][\'cv_folds\']\n    # determine whether or not model type is valid\n    model_types = {x.name: x.value for x in ModelType}\n    model_type = cfg[\'model\'][\'type\']\n    if model_type in model_types:\n        specs[\'model_type\'] = ModelType(model_types[model_type])\n    else:\n        raise ValueError(""model.yml model:type %s unrecognized"" % model_type)\n    # end of model type\n    specs[\'n_estimators\'] = cfg[\'model\'][\'estimators\']\n    specs[\'pvalue_level\'] = cfg[\'model\'][\'pvalue_level\']\n    specs[\'scorer\'] = cfg[\'model\'][\'scoring_function\']\n    # calibration\n    specs[\'calibration\'] = cfg[\'model\'][\'calibration\'][\'option\']\n    specs[\'cal_type\'] = cfg[\'model\'][\'calibration\'][\'type\']\n    # feature selection\n    specs[\'feature_selection\'] = cfg[\'model\'][\'feature_selection\'][\'option\']\n    specs[\'fs_percentage\'] = cfg[\'model\'][\'feature_selection\'][\'percentage\']\n    specs[\'fs_uni_grid\'] = cfg[\'model\'][\'feature_selection\'][\'uni_grid\']\n    score_func = cfg[\'model\'][\'feature_selection\'][\'score_func\']\n    if score_func in feature_scorers:\n        specs[\'fs_score_func\'] = feature_scorers[score_func]\n    else:\n        raise ValueError(""model.yml model:feature_selection:score_func %s unrecognized"" %\n                         score_func)\n    # grid search\n    specs[\'grid_search\'] = cfg[\'model\'][\'grid_search\'][\'option\']\n    specs[\'gs_iters\'] = cfg[\'model\'][\'grid_search\'][\'iterations\']\n    specs[\'gs_random\'] = cfg[\'model\'][\'grid_search\'][\'random\']\n    specs[\'gs_sample\'] = cfg[\'model\'][\'grid_search\'][\'subsample\']\n    specs[\'gs_sample_pct\'] = cfg[\'model\'][\'grid_search\'][\'sampling_pct\']\n    # rfe\n    specs[\'rfe\'] = cfg[\'model\'][\'rfe\'][\'option\']\n    specs[\'rfe_step\'] = cfg[\'model\'][\'rfe\'][\'step\']\n\n    # Section: pipeline\n\n    specs[\'n_jobs\'] = cfg[\'pipeline\'][\'number_jobs\']\n    specs[\'seed\'] = cfg[\'pipeline\'][\'seed\']\n    specs[\'verbosity\'] = cfg[\'pipeline\'][\'verbosity\']\n\n    # Section: plots\n\n    specs[\'calibration_plot\'] = cfg[\'plots\'][\'calibration\']\n    specs[\'confusion_matrix\'] = cfg[\'plots\'][\'confusion_matrix\']\n    specs[\'importances\'] = cfg[\'plots\'][\'importances\']\n    specs[\'learning_curve\'] = cfg[\'plots\'][\'learning_curve\']\n    specs[\'roc_curve\'] = cfg[\'plots\'][\'roc_curve\']\n\n    # Section: transforms\n\n    try:\n        specs[\'transforms\'] = cfg[\'transforms\']\n    except:\n        specs[\'transforms\'] = None\n        logger.info(""No transforms Found"")\n\n    # Section: xgboost\n\n    specs[\'esr\'] = cfg[\'xgboost\'][\'stopping_rounds\']\n\n    # Log the configuration parameters\n\n    logger.info(\'MODEL PARAMETERS:\')\n    logger.info(\'algorithms        = %s\', specs[\'algorithms\'])\n    logger.info(\'calibration       = %r\', specs[\'calibration\'])\n    logger.info(\'cal_type          = %s\', specs[\'cal_type\'])\n    logger.info(\'calibration_plot  = %r\', specs[\'calibration\'])\n    logger.info(\'clustering        = %r\', specs[\'clustering\'])\n    logger.info(\'cluster_inc       = %d\', specs[\'cluster_inc\'])\n    logger.info(\'cluster_max       = %d\', specs[\'cluster_max\'])\n    logger.info(\'cluster_min       = %d\', specs[\'cluster_min\'])\n    logger.info(\'confusion_matrix  = %r\', specs[\'confusion_matrix\'])\n    logger.info(\'counts            = %r\', specs[\'counts\'])\n    logger.info(\'cv_folds          = %d\', specs[\'cv_folds\'])\n    logger.info(\'directory         = %s\', specs[\'directory\'])\n    logger.info(\'extension         = %s\', specs[\'extension\'])\n    logger.info(\'drop              = %s\', specs[\'drop\'])\n    logger.info(\'encoder           = %r\', specs[\'encoder\'])\n    logger.info(\'esr               = %d\', specs[\'esr\'])\n    logger.info(\'factors           = %s\', specs[\'factors\'])\n    logger.info(\'features [X]      = %s\', specs[\'features\'])\n    logger.info(\'feature_selection = %r\', specs[\'feature_selection\'])\n    logger.info(\'fs_percentage     = %d\', specs[\'fs_percentage\'])\n    logger.info(\'fs_score_func     = %s\', specs[\'fs_score_func\'])\n    logger.info(\'fs_uni_grid       = %s\', specs[\'fs_uni_grid\'])\n    logger.info(\'grid_search       = %r\', specs[\'grid_search\'])\n    logger.info(\'gs_iters          = %d\', specs[\'gs_iters\'])\n    logger.info(\'gs_random         = %r\', specs[\'gs_random\'])\n    logger.info(\'gs_sample         = %r\', specs[\'gs_sample\'])\n    logger.info(\'gs_sample_pct     = %f\', specs[\'gs_sample_pct\'])\n    logger.info(\'importances       = %r\', specs[\'importances\'])\n    logger.info(\'interactions      = %r\', specs[\'interactions\'])\n    logger.info(\'isomap            = %r\', specs[\'isomap\'])\n    logger.info(\'iso_components    = %d\', specs[\'iso_components\'])\n    logger.info(\'iso_neighbors     = %d\', specs[\'iso_neighbors\'])\n    logger.info(\'isample_pct       = %d\', specs[\'isample_pct\'])\n    logger.info(\'learning_curve    = %r\', specs[\'learning_curve\'])\n    logger.info(\'logtransform      = %r\', specs[\'logtransform\'])\n    logger.info(\'lv_remove         = %r\', specs[\'lv_remove\'])\n    logger.info(\'lv_threshold      = %f\', specs[\'lv_threshold\'])\n    logger.info(\'model_type        = %r\', specs[\'model_type\'])\n    logger.info(\'n_estimators      = %d\', specs[\'n_estimators\'])\n    logger.info(\'n_jobs            = %d\', specs[\'n_jobs\'])\n    logger.info(\'ngrams_max        = %d\', specs[\'ngrams_max\'])\n    logger.info(\'numpy             = %r\', specs[\'numpy\'])\n    logger.info(\'pca               = %r\', specs[\'pca\'])\n    logger.info(\'pca_inc           = %d\', specs[\'pca_inc\'])\n    logger.info(\'pca_max           = %d\', specs[\'pca_max\'])\n    logger.info(\'pca_min           = %d\', specs[\'pca_min\'])\n    logger.info(\'pca_whiten        = %r\', specs[\'pca_whiten\'])\n    logger.info(\'poly_degree       = %d\', specs[\'poly_degree\'])\n    logger.info(\'pvalue_level      = %f\', specs[\'pvalue_level\'])\n    logger.info(\'rfe               = %r\', specs[\'rfe\'])\n    logger.info(\'rfe_step          = %d\', specs[\'rfe_step\'])\n    logger.info(\'roc_curve         = %r\', specs[\'roc_curve\'])\n    logger.info(\'rounding          = %d\', specs[\'rounding\'])\n    logger.info(\'sampling          = %r\', specs[\'sampling\'])\n    logger.info(\'sampling_method   = %r\', specs[\'sampling_method\'])\n    logger.info(\'sampling_ratio    = %f\', specs[\'sampling_ratio\'])\n    logger.info(\'scaler_option     = %r\', specs[\'scaler_option\'])\n    logger.info(\'scaler_type       = %r\', specs[\'scaler_type\'])\n    logger.info(\'scipy             = %r\', specs[\'scipy\'])\n    logger.info(\'scorer            = %s\', specs[\'scorer\'])\n    logger.info(\'seed              = %d\', specs[\'seed\'])\n    logger.info(\'sentinel          = %d\', specs[\'sentinel\'])\n    logger.info(\'separator         = %s\', specs[\'separator\'])\n    logger.info(\'shuffle           = %r\', specs[\'shuffle\'])\n    logger.info(\'split             = %f\', specs[\'split\'])\n    logger.info(\'submission_file   = %s\', specs[\'submission_file\'])\n    logger.info(\'submit_probas     = %r\', specs[\'submit_probas\'])\n    logger.info(\'target [y]        = %s\', specs[\'target\'])\n    logger.info(\'target_value      = %d\', specs[\'target_value\'])\n    logger.info(\'transforms        = %s\', specs[\'transforms\'])\n    logger.info(\'tsne              = %r\', specs[\'tsne\'])\n    logger.info(\'tsne_components   = %d\', specs[\'tsne_components\'])\n    logger.info(\'tsne_learn_rate   = %f\', specs[\'tsne_learn_rate\'])\n    logger.info(\'tsne_perplexity   = %f\', specs[\'tsne_perplexity\'])\n    logger.info(\'vectorize         = %r\', specs[\'vectorize\'])\n    logger.info(\'verbosity         = %d\', specs[\'verbosity\'])\n\n    # Specifications to create the model\n    return specs\n\n\n#\n# Function load_predictor\n#\n\ndef load_predictor(directory):\n    r""""""Load the model predictor from storage. By default, the\n    most recent model is loaded into memory.\n\n    Parameters\n    ----------\n    directory : str\n        Full directory specification of the predictor\'s location.\n\n    Returns\n    -------\n    predictor : function\n        The scoring function.\n\n    """"""\n\n    # Locate the model Pickle or HD5 file\n\n    search_dir = SSEP.join([directory, \'model\'])\n    file_name = most_recent_file(search_dir, \'model_*.*\')\n\n    # Load the model from the file\n\n    file_ext = file_name.split(PSEP)[-1]\n    if file_ext == \'pkl\' or file_ext == \'h5\':\n        logger.info(""Loading model predictor from %s"", file_name)\n        # load the model predictor\n        if file_ext == \'pkl\':\n            predictor = joblib.load(file_name)\n        elif file_ext == \'h5\':\n            predictor = load_model(file_name)\n    else:\n        logging.error(""Could not find model predictor in %s"", search_path)\n\n    # Return the model predictor\n    return predictor\n\n\n#\n# Function save_predictor\n#\n\ndef save_predictor(model, timestamp):\n    r""""""Save the time-stamped model predictor to disk.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object that contains the best estimator.\n    timestamp : str\n        Date in yyyy-mm-dd format.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n\n    logger.info(""Saving Model Predictor"")\n\n    # Extract model parameters.\n    directory = model.specs[\'directory\']\n\n    # Get the best predictor\n    predictor = model.estimators[\'BEST\']\n\n    # Save model object\n\n    if \'KERAS\' in model.best_algo:\n        filename = \'model_\' + timestamp + \'.h5\'\n        full_path = SSEP.join([directory, \'model\', filename])\n        logger.info(""Writing model predictor to %s"", full_path)\n        predictor.model.save(full_path)\n    else:\n        filename = \'model_\' + timestamp + \'.pkl\'\n        full_path = SSEP.join([directory, \'model\', filename])\n        logger.info(""Writing model predictor to %s"", full_path)\n        joblib.dump(predictor, full_path)\n\n\n#\n# Function load_feature_map\n#\n\ndef load_feature_map(model, directory):\n    r""""""Load the feature map from storage. By default, the\n    most recent feature map is loaded into memory.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object to contain the feature map.\n    directory : str\n        Full directory specification of the feature map\'s location.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object containing the feature map.\n\n    """"""\n\n    # Locate the feature map and load it\n\n    try:\n        search_dir = SSEP.join([directory, \'model\'])\n        file_name = most_recent_file(search_dir, \'feature_map_*.pkl\')\n        logger.info(""Loading feature map from %s"", file_name)\n        # load the feature map\n        feature_map = joblib.load(file_name)\n        model.feature_map = feature_map\n    except:\n        logging.error(""Could not find feature map in %s"", search_dir)\n\n    # Return the model with the feature map\n    return model\n\n\n#\n# Function save_feature_map\n#\n\ndef save_feature_map(model, timestamp):\n    r""""""Save the feature map to disk.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object containing the feature map.\n    timestamp : str\n        Date in yyyy-mm-dd format.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n\n    logger.info(""Saving Feature Map"")\n\n    # Extract model parameters.\n    directory = model.specs[\'directory\']\n\n    # Create full path name.\n\n    filename = \'feature_map_\' + timestamp + \'.pkl\'\n    full_path = SSEP.join([directory, \'model\', filename])\n\n    # Save model object\n\n    logger.info(""Writing feature map to %s"", full_path)\n    joblib.dump(model.feature_map, full_path)\n\n\n#\n# Function first_fit\n#\n\ndef first_fit(model, algo, est):\n    r""""""Fit the model before optimization.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with specifications.\n    algo : str\n        Abbreviation of the algorithm to run.\n    est : alphapy.Estimator\n        The estimator to fit.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the initial estimator.\n\n    Notes\n    -----\n    AlphaPy fits an initial model because the user may choose to get\n    a first score without any additional feature selection or grid\n    search. XGBoost is a special case because it has the advantage\n    of an ``eval_set`` and ``early_stopping_rounds``, which can\n    speed up the estimation phase.\n\n    """"""\n\n    logger.info(""Fitting Initial Model"")\n\n    # Extract model parameters.\n\n    cv_folds = model.specs[\'cv_folds\']\n    esr = model.specs[\'esr\']\n    n_jobs = model.specs[\'n_jobs\']\n    scorer = model.specs[\'scorer\']\n    seed = model.specs[\'seed\']\n    split = model.specs[\'split\']\n    verbosity = model.specs[\'verbosity\']\n\n    # Extract model data.\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Fit the initial model.\n\n    algo_xgb = \'XGB\' in algo\n\n    if algo_xgb and scorer in xgb_score_map:\n        X1, X2, y1, y2 = train_test_split(X_train, y_train, test_size=split,\n                                          random_state=seed)\n        eval_set = [(X1, y1), (X2, y2)]\n        eval_metric = xgb_score_map[scorer]\n        est.fit(X1, y1, eval_set=eval_set, eval_metric=eval_metric,\n                early_stopping_rounds=esr)\n    else:\n        est.fit(X_train, y_train)\n\n    # Get the initial scores\n\n    logger.info(""Cross-Validation"")\n    try:\n        scores = cross_val_score(est, X_train, y_train, scoring=scorer, cv=cv_folds,\n                                 n_jobs=n_jobs, verbose=verbosity)\n        logger.info(""Cross-Validation Scores: %s"", scores)\n    except:\n        logger.info(""Cross-Validation Failed: Try setting number_jobs = 1 in model.yml"")\n\n    # Store the estimator\n    model.estimators[algo] = est\n\n    # Record importances and coefficients if necessary.\n\n    if hasattr(est, ""feature_importances_""):\n        model.importances[algo] = est.feature_importances_\n\n    if hasattr(est, ""coef_""):\n        model.coefs[algo] = est.coef_\n\n    # Save the estimator in the model and return the model\n    return model\n\n\n#\n# Function make_predictions\n#\n\ndef make_predictions(model, algo, calibrate):\n    r""""""Make predictions for the training and testing data.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with specifications.\n    algo : str\n        Abbreviation of the algorithm to make predictions.\n    calibrate : bool\n        If ``True``, calibrate the probabilities for a classifier.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the predictions.\n\n    Notes\n    -----\n    For classification, calibration is a precursor to making the\n    actual predictions. In this case, AlphaPy predicts both labels\n    and probabilities. For regression, real values are predicted.\n\n    """"""\n\n    logger.info(""Final Model Predictions for %s"", algo)\n\n    # Extract model parameters.\n\n    cal_type = model.specs[\'cal_type\']\n    cv_folds = model.specs[\'cv_folds\']\n    model_type = model.specs[\'model_type\']\n\n    # Get the estimator\n\n    est = model.estimators[algo]\n\n    # Extract model data.\n\n    try:\n        support = model.support[algo]\n        X_train = model.X_train[:, support]\n        X_test = model.X_test[:, support]\n    except:\n        X_train = model.X_train\n        X_test = model.X_test\n    y_train = model.y_train\n\n    # Calibration\n\n    if model_type == ModelType.classification:\n        if calibrate:\n            logger.info(""Calibrating Classifier"")\n            est = CalibratedClassifierCV(est, cv=cv_folds, method=cal_type)\n            est.fit(X_train, y_train)\n            model.estimators[algo] = est\n            logger.info(""Calibration Complete"")\n        else:\n            logger.info(""Skipping Calibration"")\n\n    # Make predictions on original training and test data.\n\n    logger.info(""Making Predictions"")\n    model.preds[(algo, Partition.train)] = est.predict(X_train)\n    model.preds[(algo, Partition.test)] = est.predict(X_test)\n    if model_type == ModelType.classification:\n        model.probas[(algo, Partition.train)] = est.predict_proba(X_train)[:, 1]\n        model.probas[(algo, Partition.test)] = est.predict_proba(X_test)[:, 1]\n    logger.info(""Predictions Complete"")\n\n    # Return the model\n    return model\n\n\n#\n# Function predict_best\n#\n\ndef predict_best(model):\n    r""""""Select the best model based on score.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with all of the estimators.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the best estimator.\n\n    Notes\n    -----\n    Best model selection is based on a scoring function. If the\n    objective is to minimize (e.g., negative log loss), then we\n    select the model with the algorithm that has the lowest score.\n    If the objective is to maximize, then we select the algorithm\n    with the highest score (e.g., AUC).\n\n    For multiple algorithms, AlphaPy always creates a blended model.\n    Therefore, the best algorithm that is selected could actually\n    be the blended model itself.\n\n    """"""\n\n    logger.info(\'=\'*80)\n    logger.info(""Selecting Best Model"")\n\n    # Define model tags\n\n    best_tag = \'BEST\'\n    blend_tag = \'BLEND\'\n\n    # Extract model parameters.\n\n    model_type = model.specs[\'model_type\']\n    rfe = model.specs[\'rfe\']\n    scorer = model.specs[\'scorer\']\n    test_labels = model.test_labels\n\n    # Determine the correct partition to select the best model\n\n    partition = Partition.test if test_labels else Partition.train\n    logger.info(""Scoring for: %s"", partition)\n\n    # Initialize best parameters.\n\n    maximize = True if scorers[scorer][1] == Objective.maximize else False\n    if maximize:\n        best_score = -sys.float_info.max\n    else:\n        best_score = sys.float_info.max\n\n    # Initialize the model selection process.\n\n    start_time = datetime.now()\n    logger.info(""Best Model Selection Start: %s"", start_time)\n\n    # Add blended model to the list of algorithms.\n\n    if len(model.algolist) > 1:\n        algolist = copy(model.algolist)\n        algolist.append(blend_tag)\n    else:\n        algolist = model.algolist\n\n    # Iterate through the models, getting the best score for each one.\n\n    for algorithm in algolist:\n        logger.info(""Scoring %s Model"", algorithm)\n        top_score = model.metrics[(algorithm, partition, scorer)]\n        # objective is to either maximize or minimize score\n        if maximize:\n            if top_score > best_score:\n                best_score = top_score\n                best_algo = algorithm\n        else:\n            if top_score < best_score:\n                best_score = top_score\n                best_algo = algorithm\n\n    # Record predictions of best estimator\n\n    logger.info(""Best Model is %s with a %s score of %.4f"", best_algo, scorer, best_score)\n    model.best_algo = best_algo\n    model.estimators[best_tag] = model.estimators[best_algo]\n    model.preds[(best_tag, Partition.train)] = model.preds[(best_algo, Partition.train)]\n    model.preds[(best_tag, Partition.test)] = model.preds[(best_algo, Partition.test)]\n    if model_type == ModelType.classification:\n        model.probas[(best_tag, Partition.train)] = model.probas[(best_algo, Partition.train)]\n        model.probas[(best_tag, Partition.test)] = model.probas[(best_algo, Partition.test)]\n\n    # Record support vector for any recursive feature elimination\n\n    if rfe and \'XGB\' not in best_algo:\n        try:\n            model.feature_map[\'rfe_support\'] = model.support[best_algo]\n        except:\n            # no RFE support for best algorithm\n            pass\n\n    # Return the model with best estimator and predictions.\n\n    end_time = datetime.now()\n    time_taken = end_time - start_time\n    logger.info(""Best Model Selection Complete: %s"", time_taken)\n\n    return model\n\n\n#\n# Function predict_blend\n#\n\ndef predict_blend(model):\n    r""""""Make predictions from a blended model.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with all of the estimators.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the blended estimator.\n\n    Notes\n    -----\n    For classification, AlphaPy uses logistic regression for creating\n    a blended model. For regression, ridge regression is applied.\n\n    """"""\n\n    logger.info(""Blending Models"")\n\n    # Extract model paramters.\n\n    model_type = model.specs[\'model_type\']\n    cv_folds = model.specs[\'cv_folds\']\n\n    # Extract model data.\n\n    X_train = model.X_train\n    X_test = model.X_test\n    y_train = model.y_train\n\n    # Add blended algorithm.\n\n    blend_tag = \'BLEND\'\n\n    # Create blended training and test sets.\n\n    n_models = len(model.algolist)\n    X_blend_train = np.zeros((X_train.shape[0], n_models))\n    X_blend_test = np.zeros((X_test.shape[0], n_models))\n\n    # Iterate through the models, cross-validating for each one.\n\n    start_time = datetime.now()\n    logger.info(""Blending Start: %s"", start_time)\n\n    for i, algorithm in enumerate(model.algolist):\n        # get the best estimator\n        estimator = model.estimators[algorithm]\n        # update coefficients and feature importances\n        if hasattr(estimator, ""coef_""):\n            model.coefs[algorithm] = estimator.coef_\n        if hasattr(estimator, ""feature_importances_""):\n            model.importances[algorithm] = estimator.feature_importances_\n        # store predictions in the blended training set\n        if model_type == ModelType.classification:\n            X_blend_train[:, i] = model.probas[(algorithm, Partition.train)]\n            X_blend_test[:, i] = model.probas[(algorithm, Partition.test)]\n        else:\n            X_blend_train[:, i] = model.preds[(algorithm, Partition.train)]\n            X_blend_test[:, i] = model.preds[(algorithm, Partition.test)]\n\n    # Use the blended estimator to make predictions\n\n    if model_type == ModelType.classification:\n        clf = LogisticRegression()\n        clf.fit(X_blend_train, y_train)\n        model.estimators[blend_tag] = clf\n        model.preds[(blend_tag, Partition.train)] = clf.predict(X_blend_train)\n        model.preds[(blend_tag, Partition.test)] = clf.predict(X_blend_test)\n        model.probas[(blend_tag, Partition.train)] = clf.predict_proba(X_blend_train)[:, 1]\n        model.probas[(blend_tag, Partition.test)] = clf.predict_proba(X_blend_test)[:, 1]\n    else:\n        alphas = [0.0001, 0.005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5,\n                  1.0, 5.0, 10.0, 50.0, 100.0, 500.0, 1000.0]\n        rcvr = RidgeCV(alphas=alphas, normalize=True, cv=cv_folds)\n        rcvr.fit(X_blend_train, y_train)\n        model.estimators[blend_tag] = rcvr\n        model.preds[(blend_tag, Partition.train)] = rcvr.predict(X_blend_train)\n        model.preds[(blend_tag, Partition.test)] = rcvr.predict(X_blend_test)\n\n    # Return the model with blended estimator and predictions.\n\n    end_time = datetime.now()\n    time_taken = end_time - start_time\n    logger.info(""Blending Complete: %s"", time_taken)\n\n    return model\n\n\n#\n# Function generate_metrics\n#\n\ndef generate_metrics(model, partition):\n    r""""""Generate model evaluation metrics for all estimators.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with stored predictions.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the completed metrics.\n\n    Notes\n    -----\n    AlphaPy takes a brute-force approach to calculating each metric.\n    It calls every scikit-learn function without exception. If the\n    calculation fails for any reason, then the evaluation will still\n    continue without error.\n\n    References\n    ----------\n    For more information about model evaluation and the associated metrics,\n    refer to [EVAL]_.\n\n    .. [EVAL] http://scikit-learn.org/stable/modules/model_evaluation.html\n\n    """"""\n\n    logger.info(\'=\'*80)\n    logger.info(""Metrics for: %s"", partition)\n\n    # Extract model paramters.\n\n    model_type = model.specs[\'model_type\']\n\n    # Extract model data.\n\n    if partition == Partition.train:\n        expected = model.y_train\n    else:\n        expected = model.y_test\n\n    # Generate Metrics\n\n    if expected.any():\n        # Add blended model to the list of algorithms.\n        if len(model.algolist) > 1:\n            algolist = copy(model.algolist)\n            algolist.append(\'BLEND\')\n        else:\n            algolist = model.algolist\n\n        # get the metrics for each algorithm\n        for algo in algolist:\n            # get predictions for the given algorithm\n            predicted = model.preds[(algo, partition)]\n            # classification metrics\n            if model_type == ModelType.classification:\n                probas = model.probas[(algo, partition)]\n                try:\n                    model.metrics[(algo, partition, \'accuracy\')] = accuracy_score(expected, predicted)\n                except:\n                    logger.info(""Accuracy Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'average_precision\')] = average_precision_score(expected, probas)\n                except:\n                    logger.info(""Average Precision Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'balanced_accuracy\')] = balanced_accuracy_score(expected, predicted)\n                except:\n                    logger.info(""Accuracy Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'brier_score_loss\')] = brier_score_loss(expected, probas)\n                except:\n                    logger.info(""Brier Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'cohen_kappa\')] = cohen_kappa_score(expected, predicted)\n                except:\n                    logger.info(""Cohen\'s Kappa Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'confusion_matrix\')] = confusion_matrix(expected, predicted)\n                except:\n                    logger.info(""Confusion Matrix not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'f1\')] = f1_score(expected, predicted)\n                except:\n                    logger.info(""F1 Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'neg_log_loss\')] = log_loss(expected, probas)\n                except:\n                    logger.info(""Log Loss not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'precision\')] = precision_score(expected, predicted)\n                except:\n                    logger.info(""Precision Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'recall\')] = recall_score(expected, predicted)\n                except:\n                    logger.info(""Recall Score not calculated"")\n                try:\n                    fpr, tpr, _ = roc_curve(expected, probas)\n                    model.metrics[(algo, partition, \'roc_auc\')] = auc(fpr, tpr)\n                except:\n                    logger.info(""ROC AUC Score not calculated"")\n            # regression metrics\n            elif model_type == ModelType.regression:\n                try:\n                    model.metrics[(algo, partition, \'explained_variance\')] = explained_variance_score(expected, predicted)\n                except:\n                    logger.info(""Explained Variance Score not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'neg_mean_absolute_error\')] = mean_absolute_error(expected, predicted)\n                except:\n                    logger.info(""Mean Absolute Error not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'neg_median_absolute_error\')] = median_absolute_error(expected, predicted)\n                except:\n                    logger.info(""Median Absolute Error not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'neg_mean_squared_error\')] = mean_squared_error(expected, predicted)\n                except:\n                    logger.info(""Mean Squared Error not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'neg_mean_squared_log_error\')] = mean_squared_log_error(expected, predicted)\n                except:\n                    logger.info(""Mean Squared Log Error not calculated"")\n                try:\n                    model.metrics[(algo, partition, \'r2\')] = r2_score(expected, predicted)\n                except:\n                    logger.info(""R-Squared Score not calculated"")\n        # log the metrics for each algorithm\n        for algo in model.algolist:\n            logger.info(\'-\'*80)\n            logger.info(""Algorithm: %s"", algo)\n            metrics = [(k[2], v) for k, v in list(model.metrics.items()) if k[0] == algo and k[1] == partition]\n            for key, value in sorted(metrics):\n                svalue = str(value)\n                svalue.replace(\'\\n\', \' \')\n                logger.info(""%s: %s"", key, svalue)\n    else:\n        logger.info(""No labels for generating %s metrics"", partition)\n\n    return model\n\n\n#\n# Function save_predictions\n#\n\ndef save_predictions(model, tag, partition):\n    r""""""Save the predictions to disk.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object to save.\n    tag : str\n        A unique identifier for the output files, e.g., a date stamp.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    preds : numpy array\n        The prediction vector.\n    probas : numpy array\n        The probability vector.\n\n    """"""\n\n    # Extract model parameters.\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    model_type = model.specs[\'model_type\']\n    separator = model.specs[\'separator\']\n\n    # Get date stamp to record file creation\n    timestamp = get_datestamp()\n\n    # Specify input and output directories\n\n    input_dir = SSEP.join([directory, \'input\'])\n    output_dir = SSEP.join([directory, \'output\'])\n\n    # Read the prediction frame\n    file_spec = \'\'.join([datasets[partition], \'*\'])\n    file_name = most_recent_file(input_dir, file_spec)\n    file_name = file_name.split(SSEP)[-1].split(PSEP)[0]\n    pf = read_frame(input_dir, file_name, extension, separator)\n\n    # Cull records before the prediction date\n\n    try:\n        predict_date = model.specs[\'predict_date\']\n        found_pdate = True\n    except:\n        found_pdate = False\n\n    if found_pdate:\n        pd_indices = pf[pf.date >= predict_date].index.tolist()\n        pf = pf.iloc[pd_indices]\n    else:\n        pd_indices = pf.index.tolist()\n\n    # Save predictions for all projects\n\n    logger.info(""Saving Predictions"")\n    output_file = USEP.join([\'predictions\', timestamp])\n    preds = model.preds[(tag, partition)].squeeze()\n    if found_pdate:\n        preds = np.take(preds, pd_indices)\n    pred_series = pd.Series(preds, index=pd_indices)\n    df_pred = pd.DataFrame(pred_series, columns=[\'prediction\'])\n    write_frame(df_pred, output_dir, output_file, extension, separator)\n\n    # Save probabilities for classification projects\n\n    probas = None\n    if model_type == ModelType.classification:\n        logger.info(""Saving Probabilities"")\n        output_file = USEP.join([\'probabilities\', timestamp])\n        probas = model.probas[(tag, partition)].squeeze()\n        if found_pdate:\n            probas = np.take(probas, pd_indices)\n        prob_series = pd.Series(probas, index=pd_indices)\n        df_prob = pd.DataFrame(prob_series, columns=[\'probability\'])\n        write_frame(df_prob, output_dir, output_file, extension, separator)\n\n    # Save ranked predictions\n\n    logger.info(""Saving Ranked Predictions"")\n    pf[\'prediction\'] = pred_series\n    if model_type == ModelType.classification:\n        pf[\'probability\'] = prob_series\n        pf.sort_values(\'probability\', ascending=False, inplace=True)\n    else:\n        pf.sort_values(\'prediction\', ascending=False, inplace=True)\n    output_file = USEP.join([\'rankings\', timestamp])\n    write_frame(pf, output_dir, output_file, extension, separator)\n\n    # Return predictions and any probabilities\n    return preds, probas\n\n\n#\n# Function save_model\n#\n\ndef save_model(model, tag, partition):\n    r""""""Save the results in the model file.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object to save.\n    tag : str\n        A unique identifier for the output files, e.g., a date stamp.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    Notes\n    -----\n\n    The following components are extracted from the model object\n    and saved to disk:\n\n    * Model predictor (via joblib/pickle)\n    * Predictions\n    * Probabilities (classification only)\n    * Rankings\n    * Submission File (optional)\n\n    """"""\n\n    logger.info(\'=\'*80)\n\n    # Extract model parameters.\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    model_type = model.specs[\'model_type\']\n    submission_file = model.specs[\'submission_file\']\n    submit_probas = model.specs[\'submit_probas\']\n\n    # Get date stamp to record file creation\n\n    d = datetime.now()\n    f = ""%Y%m%d""\n    timestamp = d.strftime(f)\n\n    # Save the model predictor\n    save_predictor(model, timestamp)\n\n    # Save the feature map\n    save_feature_map(model, timestamp)\n\n    # Specify input and output directories\n\n    input_dir = SSEP.join([directory, \'input\'])\n    output_dir = SSEP.join([directory, \'output\'])\n\n    # Save predictions\n    preds, probas = save_predictions(model, tag, partition)\n\n    # Generate submission file\n\n    if submission_file:\n        sample_spec = PSEP.join([submission_file, extension])\n        sample_input = SSEP.join([input_dir, sample_spec])\n        ss = pd.read_csv(sample_input)\n        if submit_probas and model_type == ModelType.classification:\n            ss[ss.columns[1]] = probas\n        else:\n            ss[ss.columns[1]] = preds\n        submission_base = USEP.join([\'submission\', timestamp])\n        submission_spec = PSEP.join([submission_base, extension])\n        submission_output = SSEP.join([output_dir, submission_spec])\n        logger.info(""Saving Submission to %s"", submission_output)\n        ss.to_csv(submission_output, index=False)\n'"
alphapy/optimize.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : optimize\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import ModelType\n\nfrom datetime import datetime\nimport itertools\nimport logging\nimport numpy as np\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom time import time\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function rfecv_search\n#\n\ndef rfecv_search(model, algo):\n    r""""""Return the best feature set using recursive feature elimination\n    with cross-validation.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with RFE parameters.\n    algo : str\n        Abbreviation of the algorithm to run.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the RFE support vector and the best\n        estimator.\n\n    Notes\n    -----\n    If a scoring function is available, then AlphaPy can perform RFE\n    with Cross-Validation (CV), as in this function; otherwise, it just\n    does RFE without CV.\n\n    References\n    ----------\n    For more information about Recursive Feature Elimination,\n    refer to [RFECV]_.\n\n    .. [RFECV] http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html\n\n    """"""\n\n    # Extract model data.\n\n    X_train = model.X_train\n    y_train = model.y_train\n\n    # Extract model parameters.\n\n    cv_folds = model.specs[\'cv_folds\']\n    n_jobs = model.specs[\'n_jobs\']\n    rfe_step = model.specs[\'rfe_step\']\n    scorer = model.specs[\'scorer\']\n    verbosity = model.specs[\'verbosity\']\n    estimator = model.estimators[algo]\n\n    # Perform Recursive Feature Elimination\n\n    logger.info(""Recursive Feature Elimination with CV"")\n    rfecv = RFECV(estimator, step=rfe_step, cv=cv_folds,\n                  scoring=scorer, verbose=verbosity, n_jobs=n_jobs)\n    start = time()\n    selector = rfecv.fit(X_train, y_train)\n    logger.info(""RFECV took %.2f seconds for step %d and %d folds"",\n                (time() - start), rfe_step, cv_folds)\n    logger.info(""Algorithm: %s, Selected Features: %d, Ranking: %s"",\n                algo, selector.n_features_, selector.ranking_)\n\n    # Record the new estimator, support vector, feature names, and importances\n\n    best_estimator = selector.estimator_\n    model.estimators[algo] = best_estimator\n    model.support[algo] = selector.support_\n    model.fnames_algo[algo] = list(itertools.compress(model.fnames_algo[algo], selector.support_))\n    if hasattr(best_estimator, ""feature_importances_""):\n        model.importances[algo] = best_estimator.feature_importances_\n\n    # Return the model with the support vector\n\n    return model\n\n\n#\n# Function grid_report\n#\n\ndef grid_report(results, n_top=3):\n    r""""""Report the top grid search scores.\n\n    Parameters\n    ----------\n    results : dict of numpy arrays\n        Mean test scores for each grid search iteration.\n    n_top : int, optional\n        The number of grid search results to report.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results[\'rank_test_score\'] == i)\n        for candidate in candidates:\n            logger.info(""Model with rank: {0}"".format(i))\n            logger.info(""Mean validation score: {0:.3f} (std: {1:.3f})"".format(\n                        results[\'mean_test_score\'][candidate],\n                        results[\'std_test_score\'][candidate]))\n            logger.info(""Parameters: {0}"".format(results[\'params\'][candidate]))\n\n\n#\n# Function hyper_grid_search\n#\n\ndef hyper_grid_search(model, estimator):\n    r""""""Return the best hyperparameters for a grid search.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with grid search parameters.\n    estimator : alphapy.Estimator\n        The estimator containing the hyperparameter grid.\n\n    Returns\n    -------\n    model : alphapy.Model\n        The model object with the grid search estimator.\n\n    Notes\n    -----\n    To reduce the time required for grid search, use either\n    randomized grid search with a fixed number of iterations\n    or a full grid search with subsampling. AlphaPy uses\n    the scikit-learn Pipeline with feature selection to\n    reduce the feature space.\n\n    References\n    ----------\n    For more information about grid search, refer to [GRID]_.\n\n    .. [GRID] http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n\n    To learn about pipelines, refer to [PIPE]_.\n\n    .. [PIPE] http://scikit-learn.org/stable/modules/pipeline.html#pipeline\n\n    """"""\n\n    # Extract estimator parameters.\n\n    grid = estimator.grid\n    if not grid:\n        logger.info(""No grid is defined for grid search"")\n        return model\n\n    # Get estimator.\n\n    algo = estimator.algorithm\n    est = model.estimators[algo]\n\n    # Extract model data.\n\n    try:\n        support = model.support[algo]\n        X_train = model.X_train[:, support]\n    except:\n        X_train = model.X_train\n    y_train = model.y_train\n\n    # Extract model parameters.\n\n    cv_folds = model.specs[\'cv_folds\']\n    feature_selection = model.specs[\'feature_selection\']\n    fs_percentage = model.specs[\'fs_percentage\']\n    fs_score_func = model.specs[\'fs_score_func\']\n    fs_uni_grid = model.specs[\'fs_uni_grid\']\n    gs_iters = model.specs[\'gs_iters\']\n    gs_random = model.specs[\'gs_random\']\n    gs_sample = model.specs[\'gs_sample\']\n    gs_sample_pct = model.specs[\'gs_sample_pct\']\n    n_jobs = model.specs[\'n_jobs\']\n    scorer = model.specs[\'scorer\']\n    verbosity = model.specs[\'verbosity\']\n\n    # Subsample if necessary to reduce grid search duration.\n\n    if gs_sample:\n        length = len(X_train)\n        subset = int(length * gs_sample_pct)\n        indices = np.random.choice(length, subset, replace=False)\n        X_train = X_train[indices]\n        y_train = y_train[indices]\n\n    # Convert the grid to pipeline format\n\n    grid_new = {}\n    for k, v in list(grid.items()):\n        new_key = \'__\'.join([\'est\', k])\n        grid_new[new_key] = grid[k]\n\n    # Create the pipeline for grid search\n\n    if feature_selection:\n        # Augment the grid for feature selection.\n        fs = SelectPercentile(score_func=fs_score_func,\n                              percentile=fs_percentage)\n        # Combine the feature selection and estimator grids.\n        fs_grid = dict(fs__percentile=fs_uni_grid)\n        grid_new.update(fs_grid)\n        # Create a pipeline with the selected features and estimator.\n        pipeline = Pipeline([(""fs"", fs), (""est"", est)])\n    else:\n        pipeline = Pipeline([(""est"", est)])\n\n    # Create the randomized grid search iterator.\n\n    if gs_random:\n        logger.info(""Randomized Grid Search"")\n        gscv = RandomizedSearchCV(pipeline, param_distributions=grid_new,\n                                  n_iter=gs_iters, scoring=scorer,\n                                  n_jobs=n_jobs, cv=cv_folds, verbose=verbosity)\n    else:\n        logger.info(""Full Grid Search"")\n        gscv = GridSearchCV(pipeline, param_grid=grid_new, scoring=scorer,\n                            n_jobs=n_jobs, cv=cv_folds, verbose=verbosity)\n\n    # Fit the randomized search and time it.\n\n    start = time()\n    gscv.fit(X_train, y_train)\n    if gs_iters > 0:\n        logger.info(""Grid Search took %.2f seconds for %d candidate""\n                    "" parameter settings."" % ((time() - start), gs_iters))\n    else:\n        logger.info(""Grid Search took %.2f seconds for %d candidate parameter""\n                    "" settings."" % (time() - start, len(gscv.cv_results_[\'params\'])))\n\n    # Log the grid search scoring statistics.\n\n    grid_report(gscv.cv_results_)\n    logger.info(""Algorithm: %s, Best Score: %.4f, Best Parameters: %s"",\n                algo, gscv.best_score_, gscv.best_params_)\n\n    # Assign the Grid Search estimator for this algorithm\n\n    model.estimators[algo] = gscv\n\n    # Return the model with Grid Search estimators\n    return model\n'"
alphapy/plots.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : plots\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Model Plots\n#\n#     1. Calibration\n#     2. Feature Importance\n#     3. Learning Curve\n#     4. ROC Curve\n#     5. Confusion Matrix\n#     6. Validation Curve\n#     7. Partial Dependence\n#     8. Decision Boundary\n#\n# EDA Plots\n#\n#     1. Scatter Plot Matrix\n#     2. Facet Grid\n#     3. Distribution Plot\n#     4. Box Plot\n#     5. Swarm Plot\n#\n# Time Series\n#\n#     1. Time Series\n#     2. Candlestick\n#\n\nprint(__doc__)\n\n\n#\n# Imports\n#\n\nfrom alphapy.estimators import get_estimators\nfrom alphapy.globals import BSEP, PSEP, SSEP, USEP\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Partition, datasets\nfrom alphapy.globals import Q1, Q3\nfrom alphapy.utilities import remove_list_items\n\nfrom bokeh.plotting import figure, show, output_file\nimport itertools\nimport logging\nimport math\nimport matplotlib\nmatplotlib.use(\'PS\')\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport pandas as pd\nfrom scipy import interp\nimport seaborn as sns\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.utils.multiclass import unique_labels\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function get_partition_data\n#\n\ndef get_partition_data(model, partition):\n    r""""""Get the X, y pair for a given model and partition\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with partition data.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    X : numpy array\n        The feature matrix.\n    y : numpy array\n        The target vector.\n\n    Raises\n    ------\n    TypeError\n        Partition must be train or test.\n\n    """"""\n\n    if partition == Partition.train:\n        X = model.X_train\n        y = model.y_train\n    elif partition == Partition.test:\n        X = model.X_test\n        y = model.y_test\n    else:\n        raise TypeError(\'Partition must be train or test\')\n\n    return X, y\n\n\n#\n# Function generate_plots\n#\n\ndef generate_plots(model, partition):\n    r""""""Generate plots while running the pipeline.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n\n    logger.info(\'=\'*80)\n    logger.info(""Generating Plots for partition: %s"", datasets[partition])\n\n    # Extract model parameters\n\n    calibration_plot = model.specs[\'calibration_plot\']\n    confusion_matrix = model.specs[\'confusion_matrix\']\n    importances = model.specs[\'importances\']\n    learning_curve = model.specs[\'learning_curve\']\n    roc_curve = model.specs[\'roc_curve\']\n\n    # Generate plots\n\n    if calibration_plot:\n        plot_calibration(model, partition)\n    if confusion_matrix:\n        plot_confusion_matrix(model, partition)\n    if roc_curve:\n        plot_roc_curve(model, partition)\n    if partition == Partition.train:\n        if learning_curve:\n            plot_learning_curve(model, partition)\n        if importances:\n            plot_importance(model, partition)\n\n\n#\n# Function get_plot_directory\n#\n\ndef get_plot_directory(model):\n    r""""""Get the plot output directory of a model.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with directory information.\n\n    Returns\n    -------\n    plot_directory : str\n        The output directory to write the plot.\n\n    """"""\n    directory = model.specs[\'directory\']\n    plot_directory = SSEP.join([directory, \'plots\'])\n    return plot_directory\n\n\n#\n# Function write_plot\n#\n\ndef write_plot(vizlib, plot, plot_type, tag, directory=None):\n    r""""""Save the plot to a file, or display it interactively.\n\n    Parameters\n    ----------\n    vizlib : str\n        The visualization library: ``\'matplotlib\'``, ``\'seaborn\'``,\n        or ``\'bokeh\'``.\n    plot : module\n        Plotting context, e.g., ``plt``.\n    plot_type : str\n        Type of plot to generate.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification for the directory location. if\n        ``directory`` is *None*, then the plot is displayed\n        interactively.\n\n    Returns\n    -------\n    None : None.\n\n    Raises\n    ------\n    ValueError\n        Unrecognized data visualization library.\n\n    References\n    ----------\n\n    Visualization Libraries:\n\n    * Matplotlib : http://matplotlib.org/\n    * Seaborn : https://seaborn.pydata.org/\n    * Bokeh : http://bokeh.pydata.org/en/latest/\n\n    """"""\n\n    # Validate visualization library\n\n    if (vizlib == \'matplotlib\' or\n       vizlib == \'seaborn\' or\n       vizlib == \'bokeh\'):\n        # supported library\n        pass\n    elif vizlib == \'plotly\':\n        raise ValueError(""Unsupported data visualization library: %s"" % vizlib)\n    else:\n        raise ValueError(""Unrecognized data visualization library: %s"" % vizlib)\n\n    # Save or display the plot\n\n    if directory:\n        if vizlib == \'bokeh\':\n            file_only = \'\'.join([plot_type, USEP, tag, \'.html\'])\n        else:\n            file_only = \'\'.join([plot_type, USEP, tag, \'.png\'])\n        file_all = SSEP.join([directory, file_only])\n        logger.info(""Writing plot to %s"", file_all)\n        if vizlib == \'matplotlib\':\n            plot.tight_layout()\n            plot.savefig(file_all)\n        elif vizlib == \'seaborn\':\n            plot.savefig(file_all)\n        else:\n            output_file(file_all, title=tag)\n            show(plot)\n    else:\n        if vizlib == \'bokeh\':\n            show(plot)\n        else:\n            plot.plot()\n\n\n#\n# Function plot_calibration\n#\n\ndef plot_calibration(model, partition):\n    r""""""Display scikit-learn calibration plots.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n    Code excerpts from authors:\n\n    * Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n    * Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n\n    http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py\n\n    """"""\n\n    logger.info(""Generating Calibration Plot"")\n\n    # For classification only\n\n    if model.specs[\'model_type\'] != ModelType.classification:\n        logger.info(\'Calibration plot is for classification only\')\n        return None\n\n    # Get X, Y for correct partition\n\n    X, y = get_partition_data(model, partition)\n\n    plt.style.use(\'classic\')\n    plt.figure(figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly Calibrated"")\n    for algo in model.algolist:\n        logger.info(""Calibration for Algorithm: %s"", algo)\n        clf = model.estimators[algo]\n        if hasattr(clf, ""predict_proba""):\n            prob_pos = model.probas[(algo, partition)]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X)\n            prob_pos = \\\n                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n        fraction_of_positives, mean_predicted_value = \\\n            calibration_curve(y, prob_pos, n_bins=10)\n        ax1.plot(mean_predicted_value, fraction_of_positives, ""s-"",\n                 label=""%s"" % (algo, ))\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=algo,\n                 histtype=""step"", lw=2)\n\n    ax1.set_ylabel(""Fraction of Positives"")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=""lower right"")\n    ax1.set_title(\'Calibration Plots [Reliability Curve]\')\n\n    ax2.set_xlabel(""Mean Predicted Value"")\n    ax2.set_ylabel(""Count"")\n    ax2.legend(loc=""upper center"", ncol=2)\n\n    plot_dir = get_plot_directory(model)\n    pstring = datasets[partition]\n    write_plot(\'matplotlib\', plt, \'calibration\', pstring, plot_dir)\n\n\n#\n# Function plot_importances\n#\n\ndef plot_importance(model, partition):\n    r""""""Display scikit-learn feature importances.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\n    """"""\n\n    logger.info(""Generating Feature Importance Plots"")\n    plot_dir = get_plot_directory(model)\n    pstring = datasets[partition]\n\n    # For each algorithm that has importances, generate the plot.\n\n    n_top = 20\n\n    for algo in model.algolist:\n        logger.info(""Feature Importances for Algorithm: %s"", algo)\n        try:\n            # get feature importances\n            importances = np.array(model.importances[algo])\n            imp_flag = True\n        except:\n            imp_flag = False\n        if imp_flag:\n            # sort the importances by index\n            indices = np.argsort(importances)[::-1]\n            # get feature names\n            feature_names = np.array(model.fnames_algo[algo])\n            n_features = len(feature_names)\n            # log the feature ranking\n            logger.info(""Feature Ranking:"")\n            n_min = min(n_top, n_features)\n            for i in range(n_min):\n                logger.info(""%d. %s (%f)"" % (i + 1,\n                            feature_names[indices[i]],\n                            importances[indices[i]]))\n            # plot the feature importances\n            title = BSEP.join([algo, ""Feature Importances ["", pstring, ""]""])\n            plt.figure()\n            plt.title(title)\n            plt.barh(range(n_min), importances[indices][:n_min][::-1])\n            plt.yticks(range(n_min), feature_names[indices][:n_min][::-1])\n            plt.ylim([-1, n_min])\n            plt.xlabel(\'Relative Importance\')\n            # save the plot\n            tag = USEP.join([pstring, algo])\n            write_plot(\'matplotlib\', plt, \'feature_importance\', tag, plot_dir)\n        else:\n            logger.info(""No Feature Importances for %s"" % algo)\n\n\n#\n# Function plot_learning_curve\n#\n\ndef plot_learning_curve(model, partition):\n    r""""""Generate learning curves for a given partition.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\n    """"""\n\n    logger.info(""Generating Learning Curves"")\n    plot_dir = get_plot_directory(model)\n    pstring = datasets[partition]\n\n    # Extract model parameters.\n\n    cv_folds = model.specs[\'cv_folds\']\n    n_jobs = model.specs[\'n_jobs\']\n    seed = model.specs[\'seed\']\n    shuffle = model.specs[\'shuffle\']\n    verbosity = model.specs[\'verbosity\']\n\n    # Get original estimators\n\n    estimators = get_estimators(model)\n\n    # Get X, Y for correct partition.\n\n    X, y = get_partition_data(model, partition)\n\n    # Set cross-validation parameters to get mean train and test curves.\n\n    cv = StratifiedKFold(n_splits=cv_folds, shuffle=shuffle, random_state=seed)\n\n    # Plot a learning curve for each algorithm.\n\n    ylim = (0.4, 1.01)\n\n    for algo in model.algolist:\n        logger.info(""Learning Curve for Algorithm: %s"", algo)\n        # get estimator\n        est = estimators[algo].estimator\n        # plot learning curve\n        title = BSEP.join([algo, ""Learning Curve ["", pstring, ""]""])\n        # set up plot\n        plt.style.use(\'classic\')\n        plt.figure()\n        plt.title(title)\n        if ylim is not None:\n            plt.ylim(*ylim)\n        plt.xlabel(""Training Examples"")\n        plt.ylabel(""Score"")\n        # call learning curve function\n        train_sizes=np.linspace(0.1, 1.0, cv_folds)\n        train_sizes, train_scores, test_scores = \\\n            learning_curve(est, X, y, train_sizes=train_sizes, cv=cv,\n                           n_jobs=n_jobs, verbose=verbosity)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        plt.grid()\n        # plot data\n        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=""r"")\n        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1, color=""g"")\n        plt.plot(train_sizes, train_scores_mean, \'o-\', color=""r"",\n                 label=""Training Score"")\n        plt.plot(train_sizes, test_scores_mean, \'o-\', color=""g"",\n                 label=""Cross-Validation Score"")\n        plt.legend(loc=""lower right"")\n        # save the plot\n        tag = USEP.join([pstring, algo])\n        write_plot(\'matplotlib\', plt, \'learning_curve\', tag, plot_dir)\n\n\n#\n# Function plot_roc_curve\n#\n\ndef plot_roc_curve(model, partition):\n    r""""""Display ROC Curves with Cross-Validation.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc\n\n    """"""\n\n    logger.info(""Generating ROC Curves"")\n    pstring = datasets[partition]\n\n    # For classification only\n\n    if model.specs[\'model_type\'] != ModelType.classification:\n        logger.info(\'ROC Curves are for classification only\')\n        return None\n\n    # Get X, Y for correct partition.\n\n    X, y = get_partition_data(model, partition)\n\n    # Initialize plot parameters.\n\n    plt.style.use(\'classic\')\n    plt.figure()\n    lw = 2\n\n    # Plot a ROC Curve for each algorithm.\n\n    for algo in model.algolist:\n        logger.info(""ROC Curve for Algorithm: %s"", algo)\n        # compute ROC curve and ROC area for each class\n        probas = model.probas[(algo, partition)]\n        fpr, tpr, _ = roc_curve(y, probas)\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, lw=lw, label=\'%s (area = %0.2f)\' % (algo, roc_auc))\n\n    # draw the luck line\n    plt.plot([0, 1], [0, 1], linestyle=\'--\', color=\'k\', label=\'Luck\')\n    # define plot characteristics\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    title = BSEP.join([algo, ""ROC Curve ["", pstring, ""]""])\n    plt.title(title)\n    plt.legend(loc=""lower right"")\n    # save chart\n    plot_dir = get_plot_directory(model)\n    write_plot(\'matplotlib\', plt, \'roc_curve\', pstring, plot_dir)\n\n\n#\n# Function plot_confusion_matrix\n#\n\ndef plot_confusion_matrix(model, partition):\n    r""""""Draw the confusion matrix.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix\n\n    """"""\n\n    logger.info(""Generating Confusion Matrices"")\n    plot_dir = get_plot_directory(model)\n    pstring = datasets[partition]\n\n    # For classification only\n\n    if model.specs[\'model_type\'] != ModelType.classification:\n        logger.info(\'Confusion Matrix is for classification only\')\n        return None\n\n    # Get X, Y for correct partition.\n    X, y = get_partition_data(model, partition)\n\n    # Plot Parameters\n    np.set_printoptions(precision=2)\n    cmap = plt.cm.Blues\n    fmt = \'.2f\'\n\n    # Generate a Confusion Matrix for each algorithm\n\n    for algo in model.algolist:\n        logger.info(""Confusion Matrix for Algorithm: %s"", algo)\n\n        # get predictions for this partition\n        y_pred = model.preds[(algo, partition)]\n\n        # compute confusion matrix\n        cm = confusion_matrix(y, y_pred)\n        logger.info(\'Confusion Matrix:\')\n        logger.info(\'%s\', cm)\n\n        # normalize confusion matrix\n        cm_pct = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n\n        # initialize plot\n        _, ax = plt.subplots()\n\n        # set the title of the confusion matrix\n        title = algo +  "" Confusion Matrix: "" + pstring + "" ["" + str(np.sum(cm)) + ""]""\n        plt.title(title)\n\n        # only use the labels that appear in the data\n        classes = unique_labels(y, y_pred)\n\n        # show all ticks\n        ax.set(xticks=np.arange(cm.shape[1]),\n            yticks=np.arange(cm.shape[0]),\n            xticklabels=classes, yticklabels=classes,\n            title=title,\n            ylabel=\'True Label\',\n            xlabel=\'Predicted Label\')\n\n        # rotate the tick labels and set their alignment\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",\n                rotation_mode=""anchor"")\n\n        # loop over data dimensions and create text annotations\n        thresh = (cm_pct.max() + cm_pct.min()) / 2.0\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                cm_text = format(cm_pct[i, j], fmt) + ""  ["" + str(cm[i, j]) + ""]""\n                ax.text(j, i, cm_text,\n                        ha=""center"", va=""center"",\n                        color=""white"" if cm_pct[i, j] >= thresh else ""black"")\n\n        # show the color bar\n        im = ax.imshow(cm_pct, interpolation=\'nearest\', cmap=cmap)\n        ax.figure.colorbar(im, ax=ax)\n\n        # save the chart\n        tag = USEP.join([pstring, algo])\n        write_plot(\'matplotlib\', plt, \'confusion\', tag, plot_dir)\n\n\n#\n# Function plot_validation_curve\n#\n\ndef plot_validation_curve(model, partition, pname, prange):\n    r""""""Generate scikit-learn validation curves.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n    pname : str\n        Name of the hyperparameter to test.\n    prange : numpy array\n        The values of the hyperparameter that will be evaluated.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py\n\n    """"""\n\n    logger.info(""Generating Validation Curves"")\n    plot_dir = get_plot_directory(model)\n    pstring = datasets[partition]\n\n    # Extract model parameters.\n\n    cv_folds = model.specs[\'cv_folds\']\n    n_jobs = model.specs[\'n_jobs\']\n    scorer = model.specs[\'scorer\']\n    verbosity = model.specs[\'verbosity\']\n\n    # Get X, Y for correct partition.\n\n    X, y = get_partition_data(model, partition)\n\n    # Define plotting constants.\n\n    spacing = 0.5\n    alpha = 0.2\n\n    # Calculate a validation curve for each algorithm.\n\n    for algo in model.algolist:\n        logger.info(""Algorithm: %s"", algo)\n        # get estimator\n        estimator = model.estimators[algo]\n        # set up plot\n        train_scores, test_scores = validation_curve(\n            estimator, X, y, param_name=pname, param_range=prange,\n            cv=cv_folds, scoring=scorer, n_jobs=n_jobs)\n        train_scores_mean = np.mean(train_scores, axis=1)\n        train_scores_std = np.std(train_scores, axis=1)\n        test_scores_mean = np.mean(test_scores, axis=1)\n        test_scores_std = np.std(test_scores, axis=1)\n        # set up figure\n        plt.style.use(\'classic\')\n        plt.figure()\n        # plot learning curves\n        title = BSEP.join([algo, ""Validation Curve ["", pstring, ""]""])\n        plt.title(title)\n        # x-axis\n        x_min, x_max = min(prange) - spacing, max(prange) + spacing\n        plt.xlabel(pname)\n        plt.xlim(x_min, x_max)\n        # y-axis\n        plt.ylabel(""Score"")\n        plt.ylim(0.0, 1.1)\n        # plot scores\n        plt.plot(prange, train_scores_mean, label=""Training Score"", color=""r"")\n        plt.fill_between(prange, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=alpha, color=""r"")\n        plt.plot(prange, test_scores_mean, label=""Cross-Validation Score"",\n                 color=""g"")\n        plt.fill_between(prange, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=alpha, color=""g"")\n        plt.legend(loc=""best"")        # save the plot\n        tag = USEP.join([pstring, algo])\n        write_plot(\'matplotlib\', plt, \'validation_curve\', tag, plot_dir)\n\n\n#\n# Function plot_boundary\n#\n\ndef plot_boundary(model, partition, f1=0, f2=1):\n    r""""""Display a comparison of classifiers\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with plotting specifications.\n    partition : alphapy.Partition\n        Reference to the dataset.\n    f1 : int\n        Number of the first feature to compare.\n    f2 : int\n        Number of the second feature to compare.\n\n    Returns\n    -------\n    None : None\n\n    References\n    ----------\n    Code excerpts from authors:\n\n    * Gael Varoquaux\n    * Andreas Muller\n\n    http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n\n    """"""\n\n    logger.info(""Generating Boundary Plots"")\n    pstring = datasets[partition]\n\n    # For classification only\n\n    if model.specs[\'model_type\'] != ModelType.classification:\n        logger.info(\'Boundary Plots are for classification only\')\n        return None\n\n    # Get X, Y for correct partition\n\n    X, y = get_partition_data(model, partition)\n\n    # Subset for the two boundary features\n\n    X = X[[f1, f2]]\n\n    # Initialize plot\n\n    n_classifiers = len(model.algolist)\n    plt.figure(figsize=(3 * 2, n_classifiers * 2))\n    plt.subplots_adjust(bottom=.2, top=.95)\n\n    xx = np.linspace(3, 9, 100)\n    yy = np.linspace(1, 5, 100).T\n    xx, yy = np.meshgrid(xx, yy)\n    Xfull = np.c_[xx.ravel(), yy.ravel()]\n\n    # Plot each classification probability\n\n    for index, name in enumerate(model.algolist):\n        # predictions\n        y_pred = model.preds[(algo, partition)]\n        classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100\n        logger.info(""Classification Rate for %s : %f "" % (name, classif_rate))\n        # probabilities\n        probas = model.probas[(algo, partition)]\n        n_classes = np.unique(y_pred).size\n        # plot each class\n        for k in range(n_classes):\n            plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n            plt.title(""Class %d"" % k)\n            if k == 0:\n                plt.ylabel(name)\n            imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                       extent=(3, 9, 1, 5), origin=\'lower\')\n            plt.xticks(())\n            plt.yticks(())\n            idx = (y_pred == k)\n            if idx.any():\n                plt.scatter(X[idx, 0], X[idx, 1], marker=\'o\', c=\'k\')\n\n    # Plot the probability color bar\n\n    ax = plt.axes([0.15, 0.04, 0.7, 0.05])\n    plt.title(""Probability"")\n    plt.colorbar(imshow_handle, cax=ax, orientation=\'horizontal\')\n\n    # Save the plot\n    plot_dir = get_plot_directory(model)\n    write_plot(\'matplotlib\', figure, \'boundary\', pstring, plot_dir)\n\n\n#\n# Function plot_partial_dependence\n#\n\ndef plot_partial_dependence(est, X, features, fnames, tag,\n                            n_jobs=-1, verbosity=0, directory=None):\n    r""""""Display a Partial Dependence Plot.\n\n    Parameters\n    ----------\n    est : estimator\n        The scikit-learn estimator for calculating partial dependence.\n    X : numpy array\n        The data on which the estimator was trained.\n    features : list of int\n        Feature numbers of ``X``.\n    fnames : list of str\n        The feature names to plot.\n    tag : str\n        Unique identifier for the plot\n    n_jobs : int, optional\n        The maximum number of parallel jobs.\n    verbosity : int, optional\n        The amount of logging from 0 (minimum) and higher.\n    directory : str\n        Directory where the plot will be stored.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html#sphx-glr-auto-examples-ensemble-plot-partial-dependence-py\n\n    """"""\n\n    logger.info(""Generating Partial Dependence Plot"")\n\n    # Plot partial dependence\n\n    fig, axs = plot_partial_dependence(est, X, features, feature_names=fnames,\n                                       grid_resolution=50, n_jobs=n_jobs,\n                                       verbose=verbosity)\n    title = ""Partial Dependence Plot""\n    fig.suptitle(title)\n    plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n\n    # Save the plot\n    write_plot(model, \'matplotlib\', plt, \'partial_dependence\', tag, directory)\n\n\n#\n# Function plot_scatter\n#\n\ndef plot_scatter(df, features, target, tag=\'eda\', directory=None):\n    r""""""Plot a scatterplot matrix, also known as a pair plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the features.\n    features: list of str\n        The features to compare in the scatterplot.\n    target : str\n        The target variable for contrast.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    https://seaborn.pydata.org/examples/scatterplot_matrix.html\n\n    """"""\n\n    logger.info(""Generating Scatter Plot"")\n\n    # Get the feature subset\n\n    features.append(target)\n    df = df[features]\n\n    # Generate the pair plot\n\n    sns.set()\n    sns_plot = sns.pairplot(df, hue=target)\n\n    # Save the plot\n    write_plot(\'seaborn\', sns_plot, \'scatter_plot\', tag, directory)\n\n\n#\n# Function plot_facet_grid\n#\n\ndef plot_facet_grid(df, target, frow, fcol, tag=\'eda\', directory=None):\n    r""""""Plot a Seaborn faceted histogram grid.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the features.\n    target : str\n        The target variable for contrast.\n    frow : list of str\n        Feature names for the row elements of the grid.\n    fcol : list of str\n        Feature names for the column elements of the grid.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n\n    """"""\n\n    logger.info(""Generating Facet Grid"")\n\n    # Calculate the number of bins using the Freedman-Diaconis rule.\n\n    tlen = len(df[target])\n    tmax = df[target].max()\n    tmin = df[target].min()\n    trange = tmax - tmin\n    iqr = df[target].quantile(Q3) - df[target].quantile(Q1)\n    h = 2 * iqr * (tlen ** (-1/3))\n    nbins = math.ceil(trange / h)\n\n    # Generate the pair plot\n\n    sns.set(style=""darkgrid"")\n\n    fg = sns.FacetGrid(df, row=frow, col=fcol, margin_titles=True)\n    bins = np.linspace(tmin, tmax, nbins)\n    fg.map(plt.hist, target, color=""steelblue"", bins=bins, lw=0)\n\n    # Save the plot\n    write_plot(\'seaborn\', fg, \'facet_grid\', tag, directory)\n\n\n#\n# Function plot_distribution\n#\n\ndef plot_distribution(df, target, tag=\'eda\', directory=None):\n    r""""""Display a Distribution Plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the ``target`` feature.\n    target : str\n        The target variable for the distribution plot.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://seaborn.pydata.org/generated/seaborn.distplot.html\n\n    """"""\n\n    logger.info(""Generating Distribution Plot"")\n\n    # Generate the distribution plot\n\n    dist_plot = sns.distplot(df[target])\n    dist_fig = dist_plot.get_figure()\n\n    # Save the plot\n    write_plot(\'seaborn\', dist_fig, \'distribution_plot\', tag, directory)\n\n\n#\n# Function plot_box\n#\n\ndef plot_box(df, x, y, hue, tag=\'eda\', directory=None):\n    r""""""Display a Box Plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the ``x`` and ``y`` features.\n    x : str\n        Variable name in ``df`` to display along the x-axis.\n    y : str\n        Variable name in ``df`` to display along the y-axis.\n    hue : str\n        Variable name to be used as hue, i.e., another data dimension.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://seaborn.pydata.org/generated/seaborn.boxplot.html\n\n    """"""\n\n    logger.info(""Generating Box Plot"")\n\n    # Generate the box plot\n\n    box_plot = sns.boxplot(x=x, y=y, hue=hue, data=df)\n    sns.despine(offset=10, trim=True)\n    box_fig = box_plot.get_figure()\n\n    # Save the plot\n    write_plot(\'seaborn\', box_fig, \'box_plot\', tag, directory)\n\n\n#\n# Function plot_swarm\n#\n\ndef plot_swarm(df, x, y, hue, tag=\'eda\', directory=None):\n    r""""""Display a Swarm Plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the ``x`` and ``y`` features.\n    x : str\n        Variable name in ``df`` to display along the x-axis.\n    y : str\n        Variable name in ``df`` to display along the y-axis.\n    hue : str\n        Variable name to be used as hue, i.e., another data dimension.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://seaborn.pydata.org/generated/seaborn.swarmplot.html\n\n    """"""\n\n    logger.info(""Generating Swarm Plot"")\n\n    # Generate the swarm plot\n\n    swarm_plot = sns.swarmplot(x=x, y=y, hue=hue, data=df)\n    swarm_fig = swarm_plot.get_figure()\n\n    # Save the plot\n    write_plot(\'seaborn\', swarm_fig, \'swarm_plot\', tag, directory)\n\n\n#\n# Time Series Plots\n#\n\n\n#\n# Function plot_time_series\n#\n\ndef plot_time_series(df, target, tag=\'eda\', directory=None):\n    r""""""Plot time series data.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the ``target`` feature.\n    target : str\n        The target variable for the time series plot.\n    tag : str\n        Unique identifier for the plot.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    References\n    ----------\n\n    http://seaborn.pydata.org/generated/seaborn.tsplot.html\n\n    """"""\n\n    logger.info(""Generating Time Series Plot"")\n\n    # Generate the time series plot\n\n    ts_plot = sns.tsplot(data=df[target])\n    ts_fig = ts_plot.get_figure()\n\n    # Save the plot\n    write_plot(\'seaborn\', ts_fig, \'time_series_plot\', tag, directory)\n\n\n#\n# Function plot_candlestick\n#\n\ndef plot_candlestick(df, symbol, datecol=\'date\', directory=None):\n    r""""""Plot time series data.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        The dataframe containing the ``target`` feature.\n    symbol : str\n        Unique identifier of the data to plot.\n    datecol : str, optional\n        The name of the date column.\n    directory : str, optional\n        The full specification of the plot location.\n\n    Returns\n    -------\n    None : None.\n\n    Notes\n    -----\n    The dataframe ``df`` must contain these columns:\n\n    * ``open``\n    * ``high``\n    * ``low``\n    * ``close``\n\n    References\n    ----------\n\n    http://bokeh.pydata.org/en/latest/docs/gallery/candlestick.html\n\n    """"""\n\n    df[datecol] = pd.to_datetime(df[datecol])\n\n    mids = (df.open + df.close) / 2\n    spans = abs(df.close - df.open)\n\n    inc = df.close > df.open\n    dec = df.open > df.close\n    w = 12 * 60 * 60 * 1000 # half day in ms\n\n    TOOLS = ""pan, wheel_zoom, box_zoom, reset, save""\n\n    p = figure(x_axis_type=""datetime"", tools=TOOLS, plot_width=1000, toolbar_location=""left"")\n\n    p.title = BSEP.join([symbol.upper(), ""Candlestick""])\n    p.xaxis.major_label_orientation = math.pi / 4\n    p.grid.grid_line_alpha = 0.3\n\n    p.segment(df.date, df.high, df.date, df.low, color=""black"")\n    p.rect(df.date[inc], mids[inc], w, spans[inc], fill_color=""#D5E1DD"", line_color=""black"")\n    p.rect(df.date[dec], mids[dec], w, spans[dec], fill_color=""#F2583E"", line_color=""black"")\n\n    # Save the plot\n    write_plot(\'bokeh\', p, \'candlestick_chart\', symbol, directory)\n'"
alphapy/portfolio.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : portfolio\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.frame import Frame\nfrom alphapy.frame import frame_name\nfrom alphapy.frame import read_frame\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import MULTIPLIERS, SSEP\nfrom alphapy.globals import Orders\nfrom alphapy.space import Space\n\nimport logging\nimport math\nimport numpy as np\nfrom pandas import DataFrame\nfrom pandas import date_range\nfrom pandas import Series\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function portfolio_name\n#\n\ndef portfolio_name(group_name, tag):\n    """"""\n    Return the name of the portfolio.\n\n    Parameters\n    ----------\n    group_name : str\n        The group represented in the portfolio.\n    tag : str\n        A unique identifier.\n\n    Returns\n    -------\n    port_name : str\n        Portfolio name.\n\n    """"""\n    port_name = \'.\'.join([group_name, tag, ""portfolio""])\n    return port_name\n\n\n#\n# Class Portfolio\n#\n\nclass Portfolio():\n    """"""Create a new portfolio with a unique name. All portfolios\n    are stored in ``Portfolio.portfolios``.\n\n    Parameters\n    ----------\n    group_name : str\n        The group represented in the portfolio.\n    tag : str\n        A unique identifier.\n    space : alphapy.Space, optional\n        Namespace for the portfolio.\n    maxpos : int, optional\n        The maximum number of positions.\n    posby : str, optional\n        The denominator for position sizing.\n    kopos : int, optional\n        The number of positions to kick out from the portfolio.\n    koby : str, optional\n        The ""kick out"" criteria. For example, a ``koby`` value\n        of \'-profit\' means the three least profitable positions\n        will be closed.\n    restricted : bool, optional\n        If ``True``, then the portfolio is limited to a maximum\n        number of positions ``maxpos``.\n    weightby : str, optional\n        The weighting variable to balance the portfolio, e.g.,\n        by closing price, by volatility, or by any column.\n    startcap : float, optional\n        The amount of starting capital.\n    margin : float, optional\n        The amount of margin required, expressed as a fraction.\n    mincash : float, optional\n        Minimum amount of cash on hand, expressed as a fraction\n        of the total portfolio value.\n    fixedfrac : float, optional\n        The fixed fraction for any given position.\n    maxloss : float, optional\n        Stop loss for any given position.\n\n    Attributes\n    ----------\n    portfolios : dict\n        Class variable for storing all known portfolios\n    value : float\n        Class variable for storing all known portfolios\n    netprofit : float\n        Net profit ($) since previous valuation.\n    netreturn : float\n        Net return (%) since previous valuation\n    totalprofit : float\n        Total profit ($) since inception.\n    totalreturn : float\n        Total return (%) since inception.\n\n    """"""\n\n    # class variable to track all portfolios\n\n    portfolios = {}\n\n    # __new__\n    \n    def __new__(cls,\n                group_name,\n                tag,\n                space = Space(),\n                maxpos = 10,\n                posby = \'close\',\n                kopos = 0,\n                koby = \'-profit\',\n                restricted = False,\n                weightby = \'quantity\',\n                startcap = 100000,\n                margin = 0.5,\n                mincash = 0.2,\n                fixedfrac = 0.1,\n                maxloss = 0.1):\n        # create portfolio name\n        pn = portfolio_name(group_name, tag)\n        if not pn in Portfolio.portfolios:\n            return super(Portfolio, cls).__new__(cls)\n        else:\n            logger.info(""Portfolio %s already exists"", pn)\n    \n    # __init__\n    \n    def __init__(self,\n                 group_name,\n                 tag,\n                 space = Space(),\n                 maxpos = 10,\n                 posby = \'close\',\n                 kopos = 0,\n                 koby = \'-profit\',\n                 restricted = False,\n                 weightby = \'quantity\',\n                 startcap = 100000,\n                 margin = 0.5,\n                 mincash = 0.2,\n                 fixedfrac = 0.1,\n                 maxloss = 0.1):\n        # initialization\n        self.group_name = group_name\n        self.tag = tag\n        self.space = space\n        self.positions = {}\n        self.startdate = None\n        self.enddate = None\n        self.npos = 0\n        self.maxpos = maxpos\n        self.posby = posby\n        self.kopos = kopos\n        self.koby = koby\n        self.restricted = restricted\n        self.weightby = weightby\n        self.weights = []\n        self.startcap = startcap\n        self.cash = startcap\n        self.margin = margin\n        self.mincash = mincash\n        self.fixedfrac = fixedfrac\n        self.maxloss = maxloss\n        self.value = startcap\n        self.netprofit = 0.0\n        self.netreturn = 0.0\n        self.totalprofit = 0.0\n        self.totalreturn = 0.0\n        # add portfolio to portfolios list\n        pn = portfolio_name(group_name, tag)\n        Portfolio.portfolios[pn] = self\n\n    # __str__\n\n    def __str__(self):\n        return portfolio_name(self.group_name, self.tag)\n\n\n#\n# Class Position\n#\n\nclass Position:\n    """"""Create a new position in the portfolio.\n\n    Parameters\n    ----------\n    portfolio : alphaPy.portfolio\n        The portfolio that will contain the position.\n    name : str\n        A unique identifier such as a stock symbol.\n    opendate : datetime\n        Date the position is opened.\n\n    Attributes\n    ----------\n    date : timedate\n        Current date of the position.\n    name : str\n        A unique identifier.\n    status : str\n        State of the position: ``\'opened\'`` or ``\'closed\'``.\n    mpos : str\n        Market position ``\'long\'`` or ``\'short\'``.\n    quantity : float\n        The net size of the position.\n    price : float\n        The current price of the instrument.\n    value : float\n        The total dollar value of the position.\n    profit : float\n        The net profit of the current position.\n    netreturn : float\n        The Return On Investment (ROI), or net return.\n    opened : datetime\n        Date the position is opened.\n    held : int\n        The holding period since the position was opened.\n    costbasis : float\n        Overall cost basis.\n    trades : list of Trade\n        The executed trades for the position so far.\n    ntrades : int\n        Total number of trades.\n    pdata : pandas DataFrame\n        Price data for the given ``name``.\n    multiplier : float\n        Multiple for instrument type (e.g., 1.0 for stocks).\n\n    """"""\n    \n    # __init__\n    \n    def __init__(self,\n                 portfolio,\n                 name,\n                 opendate):\n        space = portfolio.space\n        self.date = opendate\n        self.name = name\n        self.status = \'opened\'\n        self.mpos = \'flat\'\n        self.quantity = 0\n        self.price = 0.0\n        self.value = 0.0\n        self.profit = 0.0\n        self.netreturn = 0.0\n        self.opened = opendate\n        self.held = 0\n        self.costbasis = 0.0\n        self.trades = []\n        self.ntrades = 0\n        self.pdata = Frame.frames[frame_name(name, space)].df\n        self.multiplier = MULTIPLIERS[space.subject]\n\n    # __str__\n    \n    def __str__(self):\n        return self.name\n\n\n#\n# Class Trade\n#\n\nclass Trade:\n    """"""Initiate a trade.\n\n    Parameters\n    ----------\n    name : str\n        The symbol to trade.\n    order : alphapy.Orders\n        Long or short trade for entry or exit.\n    quantity : int\n        The quantity for the order.\n    price : str\n        The execution price of the trade.\n    tdate : datetime\n        The date and time of the trade.\n\n    Attributes\n    ----------\n    states : list of str\n        Trade state names for a dataframe.\n\n    """"""\n    \n    states = [\'name\', \'order\', \'quantity\', \'price\']\n\n    # __init__\n\n    def __init__(self,\n                 name,\n                 order,\n                 quantity,\n                 price,\n                 tdate):\n        self.name = name\n        self.order = order\n        self.quantity = float(quantity)\n        self.price = float(price)\n        self.tdate = tdate\n\n\n#\n# Function add_position\n#\n\ndef add_position(p, name, pos):\n    r""""""Add a position to a portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio that will hold the position.\n    name : int\n        Unique identifier for the position, e.g., a stock symbol.\n    pos : alphapy.Position\n        New position to add to the portfolio.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the new position.\n\n    """"""\n    if name not in p.positions:\n        p.positions[name] = pos\n    return p\n\n\n#\n# Function remove_position\n#\n\ndef remove_position(p, name):\n    r""""""Remove a position from a portfolio by name.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio with the current position.\n    name : int\n        Unique identifier for the position, e.g., a stock symbol.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the deleted position.\n\n    """"""\n    del p.positions[name]\n    return p\n\n\n#\n# Function valuate_position\n#\n\ndef valuate_position(position, tdate):\n    r""""""Valuate the position for the given date.\n\n    Parameters\n    ----------\n    position : alphapy.Position\n        The position to be valued.\n    tdate : timedate\n        Date to value the position.\n\n    Returns\n    -------\n    position : alphapy.Position\n        New value of the position.\n\n    Notes\n    -----\n\n    An Example of Cost Basis\n\n    ======== ====== ====== ======\n    Date     Shares Price  Amount\n    ======== ====== ====== ======\n    11/09/16  +100   10.0   1,000\n    12/14/16  +200   15.0   3,000\n    04/05/17  -500   20.0  10,000\n    -------- ------ ------ ------\n    All        800         14,000\n    ======== ====== ====== ======\n\n    The cost basis is calculated as the total value of all\n    trades (14,000) divided by the total number of shares\n    traded (800), so 14,000 / 800 = 17.5, and the net position\n    is -200.\n\n    """"""\n    # get current price\n    pdata = position.pdata\n    if tdate in pdata.index:\n        cp = float(pdata.loc[tdate][\'close\'])\n        # start valuation\n        multiplier = position.multiplier\n        netpos = 0\n        tts = 0     # total traded shares\n        ttv = 0     # total traded value\n        totalprofit = 0.0\n        for trade in position.trades:\n            tq = trade.quantity\n            netpos = netpos + tq\n            tts = tts + abs(tq)\n            tp = trade.price\n            pfactor = tq * multiplier\n            cv = pfactor * cp\n            cvabs = abs(cv)\n            ttv = ttv + cvabs\n            ev = pfactor * tp\n            totalprofit = totalprofit + cv - ev\n        position.quantity = netpos\n        position.price = cp\n        position.value = abs(netpos) * multiplier * cp\n        position.profit = totalprofit\n        position.costbasis = ttv / tts\n        position.netreturn = totalprofit / cvabs - 1.0\n    return position\n\n\n#\n# Function update_position\n#\n\ndef update_position(position, trade):\n    r""""""Add the new trade to the position and revalue.\n\n    Parameters\n    ----------\n    position : alphapy.Position\n        The position to be update.\n    trade : alphapy.Trade\n        Trade for updating the position.\n\n    Returns\n    -------\n    position : alphapy.Position\n        New value of the position.\n\n    """"""\n    position.trades.append(trade)\n    position.ntrades = position.ntrades + 1\n    position.date = trade.tdate\n    position.held = trade.tdate - position.opened\n    position = valuate_position(position, trade.tdate)\n    if position.quantity > 0:\n        position.mpos = \'long\'\n    if position.quantity < 0:\n        position.mpos = \'short\'\n    return position\n\n\n#\n# Function close_position\n#\n\ndef close_position(p, position, tdate):\n    r""""""Close the position and remove it from the portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio holding the position.\n    position : alphapy.Position\n        Position to close.\n    tdate : datetime\n        The date for pricing the closed position.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the removed position.\n\n    """"""\n    pq = position.quantity\n    # if necessary, put on an offsetting trade\n    if pq != 0:\n        tradesize = -pq\n        position.date = tdate\n        pdata = position.pdata\n        cp = pdata.loc[tdate][\'close\']\n        newtrade = Trade(position.name, tradesize, cp, tdate)\n        p = update_portfolio(p, position, newtrade)\n        position.quantity = 0\n    position.status = \'closed\'\n    p = remove_position(p, position.name)\n    return p\n\n    \n#\n# Function deposit_portfolio\n#\n\ndef deposit_portfolio(p, cash, tdate):\n    r""""""Deposit cash into a given portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio to accept the deposit.\n    cash : float\n        Cash amount to deposit.\n    tdate : datetime\n        The date of deposit.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the added cash.\n\n    """"""\n    p.cash = p.cash + cash\n    p = valuate_portfolio(p, tdate)\n    return p\n\n\n#\n# Function withdraw_portfolio\n#\n\ndef withdraw_portfolio(p, cash, tdate):\n    r""""""Withdraw cash from a given portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio to accept the withdrawal.\n    cash : float\n        Cash amount to withdraw.\n    tdate : datetime\n        The date of withdrawal.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the withdrawn cash.\n\n    """"""\n    currentcash = p.cash\n    availcash = currentcash - (p.mincash * p.value)\n    if cash > availcash:\n        logger.info(""Withdrawal of %s would exceed reserve amount"", cash)\n    else:\n        p.cash = currentcash - cash\n        p = valuate_portfolio(p, tdate)\n    return p\n\n\n#\n# Function update_portfolio\n#\n\ndef update_portfolio(p, pos, trade):\n    r""""""Update the portfolio positions.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio holding the position.\n    pos : alphapy.Position\n        Position to update.\n    trade : alphapy.Trade\n        Trade for updating the position and portfolio.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the revised position.\n\n    """"""\n    # update position\n    ppq = abs(pos.quantity)\n    pos = update_position(pos, trade)\n    cpq = abs(pos.quantity)\n    npq = cpq - ppq\n    # update portfolio\n    p.date = trade.tdate\n    multiplier = pos.multiplier\n    cv = trade.price * multiplier * npq\n    p.cash -= cv\n    return p\n\n\n#\n# Function delete_portfolio\n#\n\ndef delete_portfolio(p):\n    r""""""Delete the portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio to delete.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n    positions = p.positions\n    for key in positions:\n        p = close_position(p, positions[key])\n    del p\n\n\n#\n# Function balance\n#\n\ndef balance(p, tdate, cashlevel):\n    r""""""Balance the portfolio using a weighting variable.\n\n    Rebalancing is the process of equalizing a portfolio\'s positions\n    using some criterion. For example, if a portfolio is *dollar-weighted*,\n    then one position can increase in proportion to the rest of the\n    portfolio, i.e., its fraction of the overall portfolio is greater\n    than the other positions. To make the portfolio ""equal dollar"",\n    then some positions have to be decreased and others decreased.\n\n    The rebalancing process is periodic (e.g., once per month) and\n    generates a series of trades to balance the positions. Other\n    portfolios are *volatility-weighted* because a more volatile\n    stock has a greater effect on the beta, i.e., the more volatile\n    the instrument, the smaller the position size.\n\n    Technically, any type of weight can be used for rebalancing, so\n    AlphaPy gives the user the ability to specify a ``weightby``\n    column name.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio to rebalance.\n    tdate : datetime\n        The rebalancing date.\n    cashlevel : float\n        The cash level to maintain during rebalancing.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        The rebalanced portfolio.\n\n    Notes\n    -----\n\n    .. warning:: The portfolio management functions ``balance``,\n       ``kick_out``, and ``stop_loss`` are not part of the\n       main **StockStream** pipeline, and thus have not been\n       thoroughly tested. Feel free to exercise the code and\n       report any issues.\n\n    """"""\n    currentcash = p.cash\n    mincash = p.mincash\n    weightby = p.weightby\n    if not weightby:\n        weightby = \'close\'\n    p = valuate_portfolio(p, tdate)\n    pvalue = p.value - cashlevel * p.value\n    positions = p.positions\n    bdata = np.ones(len(positions))\n    # get weighting variable values\n    if weightby[0] == ""-"":\n        invert = True\n        weightby = weightby[1:]\n    else:\n        invert = False\n    attrs = [aname for aname in dir(positions[0]) if not aname.startswith(\'_\')]\n    for i, pos in enumerate(positions):\n        if weightby in attrs:\n            estr = \'.\'.join(\'pos\', weightby)\n            bdata[i] = eval(estr)\n        else:\n            bdata[i] = pos.pdata.loc[tdate][weightby]\n    if invert:\n        bweights = (2 * bdata.mean() - bdata) / sum(bdata)\n    else:\n        bweights = bdata / sum(bdata)\n    # rebalance\n    for i, pos in enumerate(positions):\n        multiplier = pos.multiplier\n        bdelta = bweights[i] * pvalue - pos.value\n        cp = pos.pdata.loc[tdate][\'close\']\n        tradesize = math.trunc(bdelta / cp)\n        ntv = abs(tradesize) * cp * multiplier\n        if tradesize > 0:\n            order = Orders.le\n        if tradesize < 0:\n            order = Orders.se\n        exec_trade(p, pos.name, order, tradesize, cp, tdate)\n        p.cash = currentcash + bdelta - ntv\n    return p\n\n\n#\n# Function kick_out\n#\n\ndef kick_out(p, tdate):\n    r""""""Trim the portfolio based on filter criteria.\n\n    To reduce a portfolio\'s positions, AlphaPy can rank the\n    positions on some criterion, such as open profit or net\n    return. On a periodic basis, the worst performers can be\n    culled from the portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        The portfolio for reducing positions.\n    tdate : datetime\n        The date to trim the portfolio positions.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        The reduced portfolio.\n\n    Notes\n    -----\n\n    .. warning:: The portfolio management functions ``kick_out``,\n       ``balance``, and ``stop_loss`` are not part of the\n       main **StockStream** pipeline, and thus have not been\n       thoroughly tested. Feel free to exercise the code and\n       report any issues.\n\n    """"""\n    positions = p.positions\n    maxpos = p.maxpos\n    numpos = len(positions)\n    kovalue = np.zeros(numpos)\n    koby = p.koby\n    if not koby:\n        koby = \'profit\'\n    if koby[0] == ""-"":\n        descending = True\n        koby = koby[1:]\n    else:\n        descending = False\n    attrs = [aname for aname in dir(positions[0]) if not aname.startswith(\'_\')]\n    for i, pos in enumerate(positions):\n        if koby in attrs:\n            estr = \'.\'.join(\'pos\', koby)\n            kovalue[i] = eval(estr)\n        else:\n            kovalue[i] = pos.pdata.loc[tdate][koby]\n    koorder = np.argsort(np.argsort(kovalues))\n    if descending:\n        koorder = [i for i in reversed(koorder)]\n    if numpos >= maxpos:\n        freepos = numpos - maxpos + p.kopos\n        # close the top freepos positions\n        if freepos > 0:\n            for i in range(freepos):\n                p = close_position(p, positions[koorder[i]], tdate)\n    return p\n\n\n#\n# Function stop_loss\n#\n\ndef stop_loss(p, tdate):\n    r""""""Trim the portfolio based on stop-loss criteria.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        The portfolio for reducing positions based on ``maxloss``.\n    tdate : datetime\n        The date to trim any underperforming positions.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        The reduced portfolio.\n\n    Notes\n    -----\n\n    .. warning:: The portfolio management functions ``stop_loss``,\n       ``balance``, and ``kick_out`` are not part of the\n       main **StockStream** pipeline, and thus have not been\n       thoroughly tested. Feel free to exercise the code and\n       report any issues.\n\n    """"""\n    positions = p.positions\n    maxloss = p.maxloss\n    for key in positions:\n        pos = positions[key]\n        nr = pos.netreturn\n        if nr <= -maxloss:\n            p = close_position(p, pos, tdate)\n    return p\n\n\n#\n# Function valuate_portfolio\n#\n\ndef valuate_portfolio(p, tdate):\n    r""""""Value the portfolio based on the current positions.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio for calculating profit and return.\n    tdate : datetime\n        The date of valuation.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        Portfolio with the new valuation.\n\n    """"""\n    positions = p.positions\n    poslen = len(positions)\n    vpos = [0] * poslen\n    p.weights = [0] * poslen\n    posenum = enumerate(positions)\n    # save the current portfolio value\n    prev_value = p.value\n    # compute the total portfolio value\n    value = p.cash\n    for i, key in posenum:\n        pos = positions[key]\n        pos = valuate_position(pos, tdate)\n        vpos[i] = pos.value\n        value = value + vpos[i]\n    p.value = value\n    # now compute the weights\n    for i, key in posenum:\n        p.weights[i] = vpos[i] / p.value\n    # update portfolio stats\n    p.netprofit = p.value - prev_value\n    p.netreturn = p.value / prev_value - 1.0\n    p.totalprofit = p.value - p.startcap\n    p.totalreturn = p.value / p.startcap - 1.0\n    return p\n\n\n#\n# Function allocate_trade\n#\n\ndef allocate_trade(p, pos, trade):\n    r""""""Determine the trade allocation for a given portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio that will hold the new position.\n    pos : alphapy.Position\n        Position to update.\n    trade : alphapy.Trade\n        The proposed trade.\n\n    Returns\n    -------\n    allocation : float\n        The trade size that can be allocated for the portfolio.\n\n    """"""\n    cash = p.cash\n    margin = p.margin\n    mincash = p.mincash\n    restricted = p.restricted\n    if restricted:\n        kick_out(p, trade.tdate)\n        stop_loss(p, trade.tdate)\n    multiplier = pos.multiplier\n    qpold = pos.quantity\n    qtrade = trade.quantity\n    qpnew = qpold + qtrade\n    allocation = abs(qpnew) - abs(qpold)\n    addedvalue = trade.price * multiplier * abs(allocation)\n    if restricted:\n        cashreserve = mincash * cash\n        freemargin = (cash - cashreserve) / margin\n        if addedvalue > freemargin:\n            logger.info(""Required free margin: %d < added value: %d"",\n                        freemargin, addedvalue)\n            allocation = 0\n        else:\n            freecash = cash - addedvalue\n            if freecash < 0:\n                p.cash = cash + freecash\n    return allocation\n\n\n#\n# Function exec_trade\n#\n\ndef exec_trade(p, name, order, quantity, price, tdate):\n    r""""""Execute a trade for a portfolio.\n\n    Parameters\n    ----------\n    p : alphapy.Portfolio\n        Portfolio in which to trade.\n    name : str\n        The symbol to trade.\n    order : alphapy.Orders\n        Long or short trade for entry or exit.\n    quantity : int\n        The quantity for the order.\n    price : str\n        The execution price of the trade.\n    tdate : datetime\n        The date and time of the trade.\n\n    Returns\n    -------\n    tsize : float\n        The executed trade size.\n\n    Other Parameters\n    ----------------\n    Frame.frames : dict\n        Dataframe for the price data.\n\n    """"""\n    # see if the position already exists\n    if name in p.positions:\n        pos = p.positions[name]\n        newpos = False\n    else:\n        pos = Position(p, name, tdate)\n        newpos = True\n    # check the dynamic position sizing variable\n    if not p.posby:\n        tsize = quantity\n    else:\n        if order == Orders.le or order == Orders.se:\n            pf = Frame.frames[frame_name(name, p.space)].df\n            cv = float(pf.loc[tdate][p.posby])\n            tsize = math.trunc((p.value * p.fixedfrac) / cv)\n            if quantity < 0:\n                tsize = -tsize\n        else:\n            tsize = -pos.quantity\n    # instantiate and allocate the trade\n    newtrade = Trade(name, order, tsize, price, tdate)\n    allocation = allocate_trade(p, pos, newtrade)\n    if allocation != 0:\n        # create a new position if necessary\n        if newpos:\n            p = add_position(p, name, pos)\n            p.npos += 1        \n        # update the portfolio\n        p = update_portfolio(p, pos, newtrade)\n        # if net position is zero, then close the position\n        pflat = pos.quantity == 0\n        if pflat:\n            p = close_position(p, pos, tdate)\n            p.npos -= 1\n    else:\n        logger.info(""Trade Allocation for %s is 0"", name)\n    # return trade size\n    return tsize\n\n\n#\n# Function gen_portfolio\n#\n\ndef gen_portfolio(model, system, group, tframe,\n                  startcap=100000, posby=\'close\'):\n    r""""""Create a portfolio from a trades frame.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model with specifications.\n    system : str\n        Name of the system.\n    group : alphapy.Group\n        The group of instruments in the portfolio.\n    tframe : pandas.DataFrame\n        The input trade list from running the system.\n    startcap : float\n        Starting capital.\n    posby : str\n        The position sizing column in the price dataframe.\n\n    Returns\n    -------\n    p : alphapy.Portfolio\n        The generated portfolio.\n\n    Raises\n    ------\n    MemoryError\n        Could not allocate Portfolio.\n\n    Notes\n    -----\n\n    This function also generates the files required for analysis\n    by the *pyfolio* package:\n\n    * Returns File\n    * Positions File\n    * Transactions File\n\n    """"""\n\n    logger.info(""Creating Portfolio for System %s"", system)\n\n    # Unpack the model data.\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    separator = model.specs[\'separator\']\n\n    # Create the portfolio.\n\n    gname = group.name\n    gspace = group.space\n    gmembers = group.members\n    ff = 1.0 / len(gmembers)\n\n    p = Portfolio(gname,\n                  system,\n                  gspace,\n                  startcap = startcap,\n                  posby = posby,\n                  restricted = False,\n                  fixedfrac = ff)\n    if not p:\n        raise MemoryError(""Could not allocate Portfolio"")\n\n    # Build pyfolio data from the trades frame.\n\n    start = tframe.index[0]\n    end = tframe.index[-1]\n    trange = np.unique(tframe.index.map(lambda x: x.date().strftime(\'%Y-%m-%d\'))).tolist()\n    drange = date_range(start, end).map(lambda x: x.date().strftime(\'%Y-%m-%d\'))\n\n    # Initialize return, position, and transaction data.\n\n    rs = []\n    pcols = list(gmembers)\n    pcols.extend([\'cash\'])\n    pf = DataFrame(index=drange, columns=pcols).fillna(0.0)\n    ts = []\n\n    # Iterate through the date range, updating the portfolio.\n    for d in drange:\n        # process today\'s trades\n        if d in trange:\n            trades = tframe.loc[d]\n            if isinstance(trades, Series):\n                trades = DataFrame(trades).transpose()\n            for t in trades.iterrows():\n                tdate = t[0]\n                row = t[1]\n                tsize = exec_trade(p, row[\'name\'], row[\'order\'], row[\'quantity\'], row[\'price\'], tdate)\n                if tsize != 0:\n                    ts.append((d, [tsize, row[\'price\'], row[\'name\']]))\n                else:\n                    logger.info(""Trade could not be executed for %s"", row[\'name\'])\n        # iterate through current positions\n        positions = p.positions\n        pfrow = pf.loc[d]\n        for key in positions:\n            pos = positions[key]\n            if pos.quantity > 0:\n                value = pos.value\n            else:\n                value = -pos.value\n            pfrow[pos.name] = value\n        pfrow[\'cash\'] = p.cash\n        # update the portfolio returns\n        p = valuate_portfolio(p, d)\n        rs.append((d, [p.netreturn]))\n\n    # Create systems directory path\n\n    system_dir = SSEP.join([directory, \'systems\'])\n\n    # Create and record the returns frame for this system.\n\n    logger.info(""Recording Returns Frame"")\n    rspace = Space(system, \'returns\', gspace.fractal)\n    rf = DataFrame.from_dict(dict(rs), orient=\'index\', columns=[\'return\'])\n    rfname = frame_name(gname, rspace)\n    write_frame(rf, system_dir, rfname, extension, separator,\n                index=True, index_label=\'date\')\n    del rspace\n\n    # Record the positions frame for this system.\n\n    logger.info(""Recording Positions Frame"")\n    pspace = Space(system, \'positions\', gspace.fractal)\n    pfname = frame_name(gname, pspace)\n    write_frame(pf, system_dir, pfname, extension, separator,\n                index=True, index_label=\'date\')\n    del pspace\n\n    # Create and record the transactions frame for this system.\n\n    logger.info(""Recording Transactions Frame"")\n    tspace = Space(system, \'transactions\', gspace.fractal)\n    tf = DataFrame.from_dict(dict(ts), orient=\'index\', columns=[\'amount\', \'price\', \'symbol\'])\n    tfname = frame_name(gname, tspace)\n    write_frame(tf, system_dir, tfname, extension, separator,\n                index=True, index_label=\'date\')\n    del tspace\n\n    # Return the portfolio.\n    return p\n'"
alphapy/space.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : space\n# Created   : July 11, 2013\n#\n# Copyright 2017 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import USEP\n\n\n#\n# Function space_name\n#\n\ndef space_name(subject, schema, fractal):\n    r""""""Get the namespace string.\n\n    Parameters\n    ----------\n    subject : str\n        An identifier for a group of related items.\n    schema : str\n        The data related to the ``subject``.\n    fractal : str\n        The time fractal of the data, e.g., ""5m"" or ""1d"".\n\n    Returns\n    -------\n    name : str\n        The joined namespace string.\n\n    """"""\n    name = USEP.join([subject, schema, fractal])\n    return name\n    \n\n#\n# Class Space\n#\n\nclass Space:\n    """"""Create a new namespace.\n\n    Parameters\n    ----------\n    subject : str\n        An identifier for a group of related items.\n    schema : str\n        The data related to the ``subject``.\n    fractal : str\n        The time fractal of the data, e.g., ""5m"" or ""1d"".\n\n    """"""\n    \n    # __init__\n    \n    def __init__(self,\n                 subject = ""stock"",\n                 schema = ""prices"",\n                 fractal = ""1d""):\n        # code\n        self.subject = subject\n        self.schema = schema\n        self.fractal = fractal\n        \n    # __str__\n\n    def __str__(self):\n        return space_name(self.subject, self.schema, self.fractal)\n'"
alphapy/sport_flow.py,5,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : sport_flow\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nprint(__doc__)\n\nfrom alphapy.__main__ import main_pipeline\nfrom alphapy.frame import read_frame\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import ModelType\nfrom alphapy.globals import Partition, datasets\nfrom alphapy.globals import PSEP, SSEP, USEP\nfrom alphapy.globals import WILDCARD\nfrom alphapy.model import get_model_config\nfrom alphapy.model import Model\nfrom alphapy.space import Space\nfrom alphapy.utilities import valid_date\n\nimport argparse\nimport datetime\nfrom itertools import groupby\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport warnings\nimport yaml\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Sports Fields\n#\n# The following fields are repeated for:\n#     1. \'home\'\n#     2. \'away\'\n#     3. \'delta\'\n#\n# Note that [Target]s will not be merged into the Game table;\n# these targets will be predictors in the Game table that are\n# generated after each game result. All of the fields below\n# are predictors and are generated a priori, i.e., we calculate\n# deltas from the last previously played game for each team and\n# these data go into the row for the next game to be played.\n#\n\nsports_dict = {\'wins\' : int,\n               \'losses\' : int,\n               \'ties\' : int,\n               \'days_since_first_game\' : int,\n               \'days_since_previous_game\' : int,\n               \'won_on_points\' : bool,\n               \'lost_on_points\' : bool,\n               \'won_on_spread\' : bool,\n               \'lost_on_spread\' : bool,\n               \'point_win_streak\' : int,\n               \'point_loss_streak\' : int,\n               \'point_margin_game\' : int,\n               \'point_margin_season\' : int,\n               \'point_margin_season_avg\' : float,\n               \'point_margin_streak\' : int,\n               \'point_margin_streak_avg\' : float,\n               \'point_margin_ngames\' : int,\n               \'point_margin_ngames_avg\' : float,\n               \'cover_win_streak\' : int,\n               \'cover_loss_streak\' : int,\n               \'cover_margin_game\' : float,\n               \'cover_margin_season\' : float, \n               \'cover_margin_season_avg\' : float,\n               \'cover_margin_streak\' : float,\n               \'cover_margin_streak_avg\' : float,\n               \'cover_margin_ngames\' : float,\n               \'cover_margin_ngames_avg\' : float,\n               \'total_points\' : int,\n               \'overunder_margin\' : float,\n               \'over\' : bool,\n               \'under\' : bool,\n               \'over_streak\' : int,\n               \'under_streak\' : int,\n               \'overunder_season\' : float,\n               \'overunder_season_avg\' : float,\n               \'overunder_streak\' : float,\n               \'overunder_streak_avg\' : float,\n               \'overunder_ngames\' : float,\n               \'overunder_ngames_avg\' : float}\n\n\n#\n# These are the leaders. Generally, we try to predict one of these\n# variables as the target and lag the remaining ones.\n#\n\ngame_dict = {\'point_margin_game\' : int,\n             \'won_on_points\' : bool,\n             \'lost_on_points\' : bool,\n             \'cover_margin_game\' : float,\n             \'won_on_spread\' : bool,\n             \'lost_on_spread\' : bool,\n             \'overunder_margin\' : float,\n             \'over\' : bool,\n             \'under\' : bool}\n\n\n#\n# Function get_sport_config\n#\n\ndef get_sport_config():\n    r""""""Read the configuration file for SportFlow.\n\n    Parameters\n    ----------\n    None : None\n\n    Returns\n    -------\n    specs : dict\n        The parameters for controlling SportFlow.\n\n    """"""\n\n    # Read the configuration file\n\n    full_path = SSEP.join([\'.\', \'config\', \'sport.yml\'])\n    with open(full_path, \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile, Loader=yaml.FullLoader)\n\n    # Store configuration parameters in dictionary\n\n    specs = {}\n\n    # Section: sport\n\n    specs[\'league\'] = cfg[\'sport\'][\'league\']\n    specs[\'points_max\'] = cfg[\'sport\'][\'points_max\']\n    specs[\'points_min\'] = cfg[\'sport\'][\'points_min\']\n    specs[\'random_scoring\'] = cfg[\'sport\'][\'random_scoring\']\n    specs[\'rolling_window\'] = cfg[\'sport\'][\'rolling_window\']   \n    specs[\'seasons\'] = cfg[\'sport\'][\'seasons\']\n\n    # Log the sports parameters\n\n    logger.info(\'SPORT PARAMETERS:\')\n    logger.info(\'league           = %s\', specs[\'league\'])\n    logger.info(\'points_max       = %d\', specs[\'points_max\'])\n    logger.info(\'points_min       = %d\', specs[\'points_min\'])\n    logger.info(\'random_scoring   = %r\', specs[\'random_scoring\'])\n    logger.info(\'rolling_window   = %d\', specs[\'rolling_window\'])\n    logger.info(\'seasons          = %s\', specs[\'seasons\'])\n\n    # Game Specifications\n    return specs\n\n\n#\n# Function get_point_margin\n#\n\ndef get_point_margin(row, score, opponent_score):\n    r""""""Get the point margin for a game.\n\n    Parameters\n    ----------\n    row : pandas.Series\n        The row of a game.\n    score : int\n        The score for one team.\n    opponent_score : int\n        The score for the other team.\n\n    Returns\n    -------\n    point_margin : int\n        The resulting point margin (0 if NaN).\n\n    """"""\n    point_margin = 0\n    nans = math.isnan(row[score]) or math.isnan(row[opponent_score])\n    if not nans:\n        point_margin = row[score] - row[opponent_score]\n    return point_margin\n\n\n#\n# Function get_wins\n#\n\ndef get_wins(point_margin):\n    r""""""Determine a win based on the point margin.\n\n    Parameters\n    ----------\n    point_margin : int\n        The point margin can be positive, zero, or negative.\n\n    Returns\n    -------\n    won : int\n        If the point margin is greater than 0, return 1, else 0.\n\n    """"""\n    won = 1 if point_margin > 0 else 0\n    return won\n\n\n#\n# Function get_losses\n#\n\ndef get_losses(point_margin):\n    r""""""Determine a loss based on the point margin.\n\n    Parameters\n    ----------\n    point_margin : int\n        The point margin can be positive, zero, or negative.\n\n    Returns\n    -------\n    lost : int\n        If the point margin is less than 0, return 1, else 0.\n\n    """"""\n    lost = 1 if point_margin < 0 else 0\n    return lost\n\n\n#\n# Function get_ties\n#\n\ndef get_ties(point_margin):\n    r""""""Determine a tie based on the point margin.\n\n    Parameters\n    ----------\n    point_margin : int\n        The point margin can be positive, zero, or negative.\n\n    Returns\n    -------\n    tied : int\n        If the point margin is equal to 0, return 1, else 0.\n\n    """"""\n    tied = 1 if point_margin == 0 else 0\n    return tied\n\n\n#\n# Function get_day_offset\n#\n\ndef get_day_offset(date_vector):\n    r""""""Compute the day offsets between games.\n\n    Parameters\n    ----------\n    date_vector : pandas.Series\n        The date column.\n\n    Returns\n    -------\n    day_offset : pandas.Series\n        A vector of day offsets between adjacent dates.\n\n    """"""\n    dv = pd.to_datetime(date_vector)\n    offsets = pd.to_datetime(dv) - pd.to_datetime(dv[0])\n    day_offset = offsets.astype(\'timedelta64[D]\').astype(int)\n    return day_offset\n\n\n#\n# Function get_series_diff\n#\n\ndef get_series_diff(series):\n    r""""""Perform the difference operation on a series.\n\n    Parameters\n    ----------\n    series : pandas.Series\n        The series for the ``diff`` operation.\n\n    Returns\n    -------\n    new_series : pandas.Series\n        The differenced series.\n\n    """"""\n    new_series = pd.Series(len(series))\n    new_series = series.diff()\n    new_series[0] = 0\n    return new_series\n\n\n#\n# Function get_streak\n#\n\ndef get_streak(series, start_index, window):\n    r""""""Calculate the current streak.\n\n    Parameters\n    ----------\n    series : pandas.Series\n        A Boolean series for calculating streaks.\n    start_index : int\n        The offset of the series to start counting.\n    window : int\n        The period over which to count.\n\n    Returns\n    -------\n    streak : int\n        The count value for the current streak.\n\n    """"""\n    if window <= 0:\n        window = len(series)\n    i = start_index\n    streak = 0\n    while i >= 0 and (start_index-i+1) < window and series[i]:\n        streak += 1\n        i -= 1\n    return streak\n\n\n#\n# Function add_features\n#\n\ndef add_features(frame, fdict, flen, prefix=\'\'):\n    r""""""Add new features to a dataframe with the specified dictionary.\n\n    Parameters\n    ----------\n    frame : pandas.DataFrame\n        The dataframe to extend with new features defined by ``fdict``.\n    fdict : dict\n        A dictionary of column names (key) and data types (value).\n    flen : int\n        Length of ``frame``.\n    prefix : str, optional\n        Prepend all columns with a prefix.\n\n    Returns\n    -------\n    frame : pandas.DataFrame\n        The dataframe with the added features.\n\n    """"""\n    # generate sequences\n    seqint = [0] * flen\n    seqfloat = [0.0] * flen\n    seqbool = [False] * flen\n    # initialize new fields in frame\n    for key, value in list(fdict.items()):\n        newkey = key\n        if prefix:\n            newkey = PSEP.join([prefix, newkey])\n        if value == int:\n            frame[newkey] = pd.Series(seqint)\n        elif value == float:\n            frame[newkey] = pd.Series(seqfloat)\n        elif value == bool:\n            frame[newkey] = pd.Series(seqbool)\n        else:\n            raise ValueError(""Type to generate feature series not found"")\n    return frame\n\n\n#\n# Function generate_team_frame\n#\n\ndef generate_team_frame(team, tf, home_team, away_team, window):\n    r""""""Calculate statistics for each team.\n\n    Parameters\n    ----------\n    team : str\n        The abbreviation for the team.\n    tf : pandas.DataFrame\n        The initial team frame.\n    home_team : str\n        Label for the home team.\n    away_team : str\n        Label for the away team.\n    window : int\n        The value for the rolling window to calculate means and sums.\n\n    Returns\n    -------\n    tf : pandas.DataFrame\n        The completed team frame.\n\n    """"""\n    # Initialize new features\n    tf = add_features(tf, sports_dict, len(tf))\n    # Daily Offsets\n    tf[\'days_since_first_game\'] = get_day_offset(tf[\'date\'])\n    tf[\'days_since_previous_game\'] = get_series_diff(tf[\'days_since_first_game\'])\n    # Team Loop\n    for index, row in tf.iterrows():\n        if team == row[home_team]:\n            tf[\'point_margin_game\'].at[index] = get_point_margin(row, \'home.score\', \'away.score\')\n            line = row[\'line\']\n        elif team == row[away_team]:\n            tf[\'point_margin_game\'].at[index] = get_point_margin(row, \'away.score\', \'home.score\')\n            line = -row[\'line\']\n        else:\n            raise KeyError(""Team not found in Team Frame"")\n        if index == 0:\n            tf[\'wins\'].at[index] = get_wins(tf[\'point_margin_game\'].at[index])\n            tf[\'losses\'].at[index] = get_losses(tf[\'point_margin_game\'].at[index])\n            tf[\'ties\'].at[index] = get_ties(tf[\'point_margin_game\'].at[index])\n        else:\n            tf[\'wins\'].at[index] = tf[\'wins\'].at[index-1] + get_wins(tf[\'point_margin_game\'].at[index])\n            tf[\'losses\'].at[index] = tf[\'losses\'].at[index-1] + get_losses(tf[\'point_margin_game\'].at[index])\n            tf[\'ties\'].at[index] = tf[\'ties\'].at[index-1] + get_ties(tf[\'point_margin_game\'].at[index])\n        tf[\'won_on_points\'].at[index] = True if tf[\'point_margin_game\'].at[index] > 0 else False\n        tf[\'lost_on_points\'].at[index] = True if tf[\'point_margin_game\'].at[index] < 0 else False\n        tf[\'cover_margin_game\'].at[index] = tf[\'point_margin_game\'].at[index] + line\n        tf[\'won_on_spread\'].at[index] = True if tf[\'cover_margin_game\'].at[index] > 0 else False\n        tf[\'lost_on_spread\'].at[index] = True if tf[\'cover_margin_game\'].at[index] <= 0 else False\n        nans = math.isnan(row[\'home.score\']) or math.isnan(row[\'away.score\'])\n        if not nans:\n            tf[\'total_points\'].at[index] = row[\'home.score\'] + row[\'away.score\']\n        nans = math.isnan(row[\'over_under\'])\n        if not nans:\n            tf[\'overunder_margin\'].at[index] = tf[\'total_points\'].at[index] - row[\'over_under\']\n        tf[\'over\'].at[index] = True if tf[\'overunder_margin\'].at[index] > 0 else False\n        tf[\'under\'].at[index] = True if tf[\'overunder_margin\'].at[index] < 0 else False\n        tf[\'point_win_streak\'].at[index] = get_streak(tf[\'won_on_points\'], index, 0)\n        tf[\'point_loss_streak\'].at[index] = get_streak(tf[\'lost_on_points\'], index, 0)\n        tf[\'cover_win_streak\'].at[index] = get_streak(tf[\'won_on_spread\'], index, 0)\n        tf[\'cover_loss_streak\'].at[index] = get_streak(tf[\'lost_on_spread\'], index, 0)\n        tf[\'over_streak\'].at[index] = get_streak(tf[\'over\'], index, 0)\n        tf[\'under_streak\'].at[index] = get_streak(tf[\'under\'], index, 0)\n        # Handle the streaks\n        if tf[\'point_win_streak\'].at[index] > 0:\n            streak = tf[\'point_win_streak\'].at[index]\n        elif tf[\'point_loss_streak\'].at[index] > 0:\n            streak = tf[\'point_loss_streak\'].at[index]\n        else:\n            streak = 1\n        tf[\'point_margin_streak\'].at[index] = tf[\'point_margin_game\'][index-streak+1:index+1].sum()\n        tf[\'point_margin_streak_avg\'].at[index] = tf[\'point_margin_game\'][index-streak+1:index+1].mean()\n        if tf[\'cover_win_streak\'].at[index] > 0:\n            streak = tf[\'cover_win_streak\'].at[index]\n        elif tf[\'cover_loss_streak\'].at[index] > 0:\n            streak = tf[\'cover_loss_streak\'].at[index]\n        else:\n            streak = 1\n        tf[\'cover_margin_streak\'].at[index] = tf[\'cover_margin_game\'][index-streak+1:index+1].sum()\n        tf[\'cover_margin_streak_avg\'].at[index] = tf[\'cover_margin_game\'][index-streak+1:index+1].mean()\n        if tf[\'over_streak\'].at[index] > 0:\n            streak = tf[\'over_streak\'].at[index]\n        elif tf[\'under_streak\'].at[index] > 0:\n            streak = tf[\'under_streak\'].at[index]\n        else:\n            streak = 1\n        tf[\'overunder_streak\'].at[index] = tf[\'overunder_margin\'][index-streak+1:index+1].sum()\n        tf[\'overunder_streak_avg\'].at[index] = tf[\'overunder_margin\'][index-streak+1:index+1].mean()\n    # Rolling and Expanding Variables\n    tf[\'point_margin_season\'] = tf[\'point_margin_game\'].cumsum()\n    tf[\'point_margin_season_avg\'] = tf[\'point_margin_game\'].expanding().mean()\n    tf[\'point_margin_ngames\'] = tf[\'point_margin_game\'].rolling(window=window, min_periods=1).sum()\n    tf[\'point_margin_ngames_avg\'] = tf[\'point_margin_game\'].rolling(window=window, min_periods=1).mean()\n    tf[\'cover_margin_season\'] = tf[\'cover_margin_game\'].cumsum()\n    tf[\'cover_margin_season_avg\'] = tf[\'cover_margin_game\'].expanding().mean()\n    tf[\'cover_margin_ngames\'] = tf[\'cover_margin_game\'].rolling(window=window, min_periods=1).sum()\n    tf[\'cover_margin_ngames_avg\'] = tf[\'cover_margin_game\'].rolling(window=window, min_periods=1).mean()\n    tf[\'overunder_season\'] = tf[\'overunder_margin\'].cumsum()\n    tf[\'overunder_season_avg\'] = tf[\'overunder_margin\'].expanding().mean()\n    tf[\'overunder_ngames\'] = tf[\'overunder_margin\'].rolling(window=window, min_periods=1).sum()\n    tf[\'overunder_ngames_avg\'] = tf[\'overunder_margin\'].rolling(window=window, min_periods=1).mean()\n    return tf\n\n\n#\n# Function get_team_frame\n#\n\ndef get_team_frame(game_frame, team, home, away):\n    r""""""Calculate statistics for each team.\n\n    Parameters\n    ----------\n    game_frame : pandas.DataFrame\n        The game frame for a given season.\n    team : str\n        The team abbreviation.\n    home : str\n        The label of the home team column.\n    away : int\n        The label of the away team column.\n\n    Returns\n    -------\n    team_frame : pandas.DataFrame\n        The extracted team frame.\n\n    """"""\n    team_frame = game_frame[(game_frame[home] == team) | (game_frame[away] == team)]\n    return team_frame\n\n\n#\n# Function insert_model_data\n#\n\ndef insert_model_data(mf, mpos, mdict, tf, tpos, prefix):\n    r""""""Insert a row from the team frame into the model frame.\n\n    Parameters\n    ----------\n    mf : pandas.DataFrame\n        The model frame for a single season.\n    mpos : int\n        The position in the model frame where to insert the row.\n    mdict : dict\n        A dictionary of column names (key) and data types (value).\n    tf : pandas.DataFrame\n        The team frame for a season.\n    tpos : int\n        The position of the row in the team frame.\n    prefix : str\n        The prefix to join with the ``mdict`` key.\n\n    Returns\n    -------\n    mf : pandas.DataFrame\n        The .\n\n    """"""\n    team_row = tf.iloc[tpos]\n    for key, value in list(mdict.items()):\n        newkey = key\n        if prefix:\n            newkey = PSEP.join([prefix, newkey])\n        mf.at[mpos, newkey] = team_row[key]\n    return mf\n\n\n#\n# Function generate_delta_data\n#\n\ndef generate_delta_data(frame, fdict, prefix1, prefix2):\n    r""""""Subtract two similar columns to get the delta value.\n\n    Parameters\n    ----------\n    frame : pandas.DataFrame\n        The input model frame.\n    fdict : dict\n        A dictionary of column names (key) and data types (value).\n    prefix1 : str\n        The prefix of the first team.\n    prefix2 : str\n        The prefix of the second team.\n\n    Returns\n    -------\n    frame : pandas.DataFrame\n        The completed dataframe with the delta data.\n\n    """"""\n    for key, value in list(fdict.items()):\n        newkey = PSEP.join([\'delta\', key])\n        key1 = PSEP.join([prefix1, key])\n        key2 = PSEP.join([prefix2, key])\n        frame[newkey] = frame[key1] - frame[key2]\n    return frame\n\n\n#\n# Function main\n#\n\ndef main(args=None):\n    r""""""The main program for SportFlow.\n\n    Notes\n    -----\n    (1) Initialize logging.\n    (2) Parse the command line arguments.\n    (3) Get the game configuration.\n    (4) Get the model configuration.\n    (5) Generate game frames for each season.\n    (6) Create statistics for each team.\n    (7) Merge the team frames into the final model frame.\n    (8) Run the AlphaPy pipeline.\n\n    Raises\n    ------\n    ValueError\n        Training date must be before prediction date.\n\n    """"""\n\n    # Suppress Warnings\n\n    warnings.simplefilter(action=\'ignore\', category=DeprecationWarning)\n    warnings.simplefilter(action=\'ignore\', category=FutureWarning)\n\n    # Logging\n\n    logging.basicConfig(format=""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                        filename=""sport_flow.log"", filemode=\'a\', level=logging.DEBUG,\n                        datefmt=\'%m/%d/%y %H:%M:%S\')\n    formatter = logging.Formatter(""[%(asctime)s] %(levelname)s\\t%(message)s"",\n                                  datefmt=\'%m/%d/%y %H:%M:%S\')\n    console = logging.StreamHandler()\n    console.setFormatter(formatter)\n    console.setLevel(logging.INFO)\n    logging.getLogger().addHandler(console)\n\n    logger = logging.getLogger(__name__)\n\n    # Start the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""SportFlow Start"")\n    logger.info(\'*\'*80)\n\n    # Argument Parsing\n\n    parser = argparse.ArgumentParser(description=""SportFlow Parser"")\n    parser.add_argument(\'--pdate\', dest=\'predict_date\',\n                        help=""prediction date is in the format: YYYY-MM-DD"",\n                        required=False, type=valid_date)\n    parser.add_argument(\'--tdate\', dest=\'train_date\',\n                        help=""training date is in the format: YYYY-MM-DD"",\n                        required=False, type=valid_date)\n    parser.add_mutually_exclusive_group(required=False)\n    parser.add_argument(\'--predict\', dest=\'predict_mode\', action=\'store_true\')\n    parser.add_argument(\'--train\', dest=\'predict_mode\', action=\'store_false\')\n    parser.set_defaults(predict_mode=False)\n    args = parser.parse_args()\n\n    # Set train and predict dates\n\n    if args.train_date:\n        train_date = args.train_date\n    else:\n        train_date = pd.datetime(1900, 1, 1).strftime(""%Y-%m-%d"")\n\n    if args.predict_date:\n        predict_date = args.predict_date\n    else:\n        predict_date = datetime.date.today().strftime(""%Y-%m-%d"")\n\n    # Verify that the dates are in sequence.\n\n    if train_date >= predict_date:\n        raise ValueError(""Training date must be before prediction date"")\n    else:\n        logger.info(""Training Date: %s"", train_date)\n        logger.info(""Prediction Date: %s"", predict_date)\n\n    # Read game configuration file\n\n    sport_specs = get_sport_config()\n\n    # Section: game\n\n    league = sport_specs[\'league\']\n    points_max = sport_specs[\'points_max\']\n    points_min = sport_specs[\'points_min\']\n    random_scoring = sport_specs[\'random_scoring\']\n    seasons = sport_specs[\'seasons\']\n    window = sport_specs[\'rolling_window\']   \n\n    # Read model configuration file\n\n    specs = get_model_config()\n\n    # Add command line arguments to model specifications\n\n    specs[\'predict_mode\'] = args.predict_mode\n    specs[\'predict_date\'] = args.predict_date\n    specs[\'train_date\'] = args.train_date\n\n    # Unpack model arguments\n\n    directory = specs[\'directory\']\n    target = specs[\'target\']\n\n    # Create directories if necessary\n\n    output_dirs = [\'config\', \'data\', \'input\', \'model\', \'output\', \'plots\']\n    for od in output_dirs:\n        output_dir = SSEP.join([directory, od])\n        if not os.path.exists(output_dir):\n            logger.info(""Creating directory %s"", output_dir)\n            os.makedirs(output_dir)\n\n    # Create the game scores space\n    space = Space(\'game\', \'scores\', \'1g\')\n\n    #\n    # Derived Variables\n    #\n\n    series = space.schema\n    team1_prefix = \'home\'\n    team2_prefix = \'away\'\n    home_team = PSEP.join([team1_prefix, \'team\'])\n    away_team = PSEP.join([team2_prefix, \'team\'])\n\n    #\n    # Read in the game frame. This is the feature generation phase.\n    #\n\n    logger.info(""Reading Game Data"")\n\n    data_dir = SSEP.join([directory, \'data\'])\n    file_base = USEP.join([league, space.subject, space.schema, space.fractal])\n    df = read_frame(data_dir, file_base, specs[\'extension\'], specs[\'separator\'])\n    logger.info(""Total Game Records: %d"", df.shape[0])\n\n    #\n    # Locate any rows with null values\n    #\n\n    null_rows = df.isnull().any(axis=1)\n    null_indices = [i for i, val in enumerate(null_rows.tolist()) if val == True]\n    for i in null_indices:\n        logger.info(""Null Record: %d on Date: %s"", i, df.date[i])\n\n    #\n    # Run the game pipeline on a seasonal loop\n    #\n\n    if not seasons:\n        # run model on all seasons\n        seasons = df[\'season\'].unique().tolist()\n\n    #\n    # Initialize the final frame\n    #\n\n    ff = pd.DataFrame()\n\n    #\n    # Iterate through each season of the game frame\n    #\n\n    for season in seasons:\n\n        # Generate a frame for each season\n\n        gf = df[df[\'season\'] == season]\n        gf = gf.reset_index()\n\n        # Generate derived variables for the game frame\n\n        total_games = gf.shape[0]\n        if random_scoring:\n            gf[\'home.score\'] = np.random.randint(points_min, points_max, total_games)\n            gf[\'away.score\'] = np.random.randint(points_min, points_max, total_games)\n        gf[\'total_points\'] = gf[\'home.score\'] + gf[\'away.score\']\n\n        # gf[\'line_delta\'] = gf[\'line\'] - gf[\'line_open\']\n        # gf[\'over_under_delta\'] = gf[\'over_under\'] - gf[\'over_under_open\']\n\n        gf = add_features(gf, game_dict, gf.shape[0])\n        for index, row in gf.iterrows():\n            gf[\'point_margin_game\'].at[index] = get_point_margin(row, \'home.score\', \'away.score\')\n            gf[\'won_on_points\'].at[index] = True if gf[\'point_margin_game\'].at[index] > 0 else False\n            gf[\'lost_on_points\'].at[index] = True if gf[\'point_margin_game\'].at[index] < 0 else False\n            gf[\'cover_margin_game\'].at[index] = gf[\'point_margin_game\'].at[index] + row[\'line\']\n            gf[\'won_on_spread\'].at[index] = True if gf[\'cover_margin_game\'].at[index] > 0 else False\n            gf[\'lost_on_spread\'].at[index] = True if gf[\'cover_margin_game\'].at[index] <= 0 else False\n            gf[\'overunder_margin\'].at[index] = gf[\'total_points\'].at[index] - row[\'over_under\']\n            gf[\'over\'].at[index] = True if gf[\'overunder_margin\'].at[index] > 0 else False\n            gf[\'under\'].at[index] = True if gf[\'overunder_margin\'].at[index] < 0 else False\n\n        # Generate each team frame\n\n        team_frames = {}\n        teams = gf.groupby([home_team])\n        for team, data in teams:\n            team_frame = USEP.join([league, team.lower(), series, str(season)])\n            logger.info(""Generating team frame: %s"", team_frame)\n            tf = get_team_frame(gf, team, home_team, away_team)\n            tf = tf.reset_index()\n            tf = generate_team_frame(team, tf, home_team, away_team, window)\n            team_frames[team_frame] = tf\n\n        # Create the model frame, initializing the home and away frames\n\n        mdict = {k:v for (k,v) in list(sports_dict.items()) if v != bool}\n        team1_frame = pd.DataFrame()\n        team1_frame = add_features(team1_frame, mdict, gf.shape[0], prefix=team1_prefix)\n        team2_frame = pd.DataFrame()\n        team2_frame = add_features(team2_frame, mdict, gf.shape[0], prefix=team2_prefix)\n        frames = [gf, team1_frame, team2_frame]\n        mf = pd.concat(frames, axis=1)\n\n        # Loop through each team frame, inserting data into the model frame row\n        #     get index+1 [if valid]\n        #     determine if team is home or away to get prefix\n        #     try: np.where((gf[home_team] == \'PHI\') & (gf[\'date\'] == \'09/07/14\'))[0][0]\n        #     Assign team frame fields to respective model frame fields: set gf.at(pos, field)\n\n        for team, data in teams:\n            team_frame = USEP.join([league, team.lower(), series, str(season)])\n            logger.info(""Merging team frame %s into model frame"", team_frame)\n            tf = team_frames[team_frame]\n            for index in range(0, tf.shape[0]-1):\n                gindex = index + 1\n                model_row = tf.iloc[gindex]\n                key_date = model_row[\'date\']\n                at_home = False\n                if team == model_row[home_team]:\n                    at_home = True\n                    key_team = model_row[home_team]\n                elif team == model_row[away_team]:\n                    key_team = model_row[away_team]\n                else:\n                    raise KeyError(""Team %s not found in Team Frame"" % team)            \n                try:\n                    if at_home:\n                        mpos = np.where((mf[home_team] == key_team) & (mf[\'date\'] == key_date))[0][0]\n                    else:\n                        mpos = np.where((mf[away_team] == key_team) & (mf[\'date\'] == key_date))[0][0]\n                except:\n                    raise IndexError(""Team/Date Key not found in Model Frame"")\n                # insert team data into model row\n                mf = insert_model_data(mf, mpos, mdict, tf, index, team1_prefix if at_home else team2_prefix)\n\n        # Compute delta data \'home\' - \'away\'\n        mf = generate_delta_data(mf, mdict, team1_prefix, team2_prefix)\n\n        # Append this to final frame\n        frames = [ff, mf]\n        ff = pd.concat(frames)\n\n    # Write out dataframes\n\n    input_dir = SSEP.join([directory, \'input\'])\n    if args.predict_mode:\n        new_predict_frame = ff.loc[ff.date >= predict_date]\n        if len(new_predict_frame) <= 1:\n            raise ValueError(""Prediction frame has length 1 or less"")\n        # rewrite with all the features to the train and test files\n        logger.info(""Saving prediction frame"")\n        write_frame(new_predict_frame, input_dir, datasets[Partition.predict],\n                    specs[\'extension\'], specs[\'separator\'])\n    else:\n        # split data into training and test data\n        new_train_frame = ff.loc[(ff.date >= train_date) & (ff.date < predict_date)]\n        if len(new_train_frame) <= 1:\n            raise ValueError(""Training frame has length 1 or less"")\n        new_test_frame = ff.loc[ff.date >= predict_date]\n        if len(new_test_frame) <= 1:\n            raise ValueError(""Testing frame has length 1 or less"")\n        # rewrite with all the features to the train and test files\n        logger.info(""Saving training frame"")\n        write_frame(new_train_frame, input_dir, datasets[Partition.train],\n                    specs[\'extension\'], specs[\'separator\'])\n        logger.info(""Saving testing frame"")\n        write_frame(new_test_frame, input_dir, datasets[Partition.test],\n                    specs[\'extension\'], specs[\'separator\'])\n\n    # Create the model from specs\n\n    logger.info(""Running Model"")\n    model = Model(specs)\n\n    # Run the pipeline\n    model = main_pipeline(model)\n\n    # Complete the pipeline\n\n    logger.info(\'*\'*80)\n    logger.info(""SportFlow End"")\n    logger.info(\'*\'*80)\n\n\n#\n# MAIN PROGRAM\n#\n\nif __name__ == ""__main__"":\n    main()\n'"
alphapy/system.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : system\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.frame import Frame\nfrom alphapy.frame import frame_name\nfrom alphapy.frame import read_frame\nfrom alphapy.frame import write_frame\nfrom alphapy.globals import Orders\nfrom alphapy.globals import BSEP, SSEP\nfrom alphapy.variables import vexec\nfrom alphapy.space import Space\nfrom alphapy.portfolio import Trade\nfrom alphapy.utilities import most_recent_file\n\nimport logging\nimport numbers\nimport pandas as pd\nfrom pandas import DataFrame\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Class System\n#\n\nclass System(object):\n    """"""Create a new system. All systems are stored in\n    ``System.systems``. Duplicate names are not allowed.\n\n    Parameters\n    ----------\n    name : str\n        The system name.\n    longentry : str\n        Name of the conditional feature for a long entry.\n    shortentry : str, optional\n        Name of the conditional feature for a short entry.\n    longexit : str, optional\n        Name of the conditional feature for a long exit.\n    shortexit : str, optional\n        Name of the conditional feature for a short exit.\n    holdperiod : int, optional\n        Holding period of a position.\n    scale : bool, optional\n        Add to a position for a signal in the same direction.\n\n    Attributes\n    ----------\n    systems : dict\n        Class variable for storing all known systems\n\n    Examples\n    --------\n    \n    >>> System(\'closer\', hc, lc)\n\n    """"""\n\n    # class variable to track all systems\n\n    systems = {}\n\n    # __new__\n    \n    def __new__(cls,\n                name,\n                longentry,\n                shortentry = None,\n                longexit = None,\n                shortexit = None,\n                holdperiod = 0,\n                scale = False):\n        # create system name\n        if name not in System.systems:\n            return super(System, cls).__new__(cls)\n        else:\n            logger.info(""System %s already exists"", name)\n    \n    # __init__\n    \n    def __init__(self,\n                 name,\n                 longentry,\n                 shortentry = None,\n                 longexit = None,\n                 shortexit = None,\n                 holdperiod = 0,\n                 scale = False):\n        # initialization\n        self.name = name\n        self.longentry = longentry\n        self.shortentry = shortentry\n        self.longexit = longexit\n        self.shortexit = shortexit\n        self.holdperiod = holdperiod\n        self.scale = scale\n        # add system to systems list\n        System.systems[name] = self\n        \n    # __str__\n\n    def __str__(self):\n        return self.name\n\n\n#\n# Function trade_system\n#\n\ndef trade_system(model, system, space, intraday, name, quantity):\n    r""""""Trade the given system.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with specifications.\n    system : alphapy.System\n        The long/short system to run.\n    space : alphapy.Space\n        Namespace of instrument prices.\n    intraday : bool\n        If True, then run an intraday system.\n    name : str\n        The symbol to trade.\n    quantity : float\n        The amount of the ``name`` to trade, e.g., number of shares\n\n    Returns\n    -------\n    tradelist : list\n        List of trade entries and exits.\n\n    Other Parameters\n    ----------------\n    Frame.frames : dict\n        All of the data frames containing price data.\n\n    """"""\n\n    # Unpack the model data.\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    separator = model.specs[\'separator\']\n\n    # Unpack the system parameters.\n\n    longentry = system.longentry\n    shortentry = system.shortentry\n    longexit = system.longexit\n    shortexit = system.shortexit\n    holdperiod = system.holdperiod\n    scale = system.scale\n\n    # Determine whether or not this is a model-driven system.\n\n    entries_and_exits = [longentry, shortentry, longexit, shortexit]\n    active_signals = [x for x in entries_and_exits if x is not None]\n    use_model = False\n    for signal in active_signals:\n        if any(x in signal for x in [\'phigh\', \'plow\']):\n            use_model = True\n\n    # Read in the price frame\n    pf = Frame.frames[frame_name(name, space)].df\n\n    # Use model output probabilities as input to the system\n\n    if use_model:\n        # get latest probabilities file\n        probs_dir = SSEP.join([directory, \'output\'])\n        file_path = most_recent_file(probs_dir, \'probabilities*\')\n        file_name = file_path.split(SSEP)[-1].split(\'.\')[0]\n        # read the probabilities frame and trim the price frame\n        probs_frame = read_frame(probs_dir, file_name, extension, separator)\n        pf = pf[-probs_frame.shape[0]:]\n        probs_frame.index = pf.index\n        probs_frame.columns = [\'probability\']\n        # add probability column to price frame\n        pf = pd.concat([pf, probs_frame], axis=1)\n\n    # Evaluate the long and short events in the price frame\n\n    for signal in active_signals:\n        vexec(pf, signal)\n\n    # Initialize trading state variables\n\n    inlong = False\n    inshort = False\n    h = 0\n    p = 0\n    q = quantity\n    tradelist = []\n\n    # Loop through prices and generate trades\n\n    for dt, row in pf.iterrows():\n        # get closing price\n        c = row[\'close\']\n        if intraday:\n            bar_number = row[\'bar_number\']\n            end_of_day = row[\'end_of_day\']            \n        # evaluate entry and exit conditions\n        lerow = row[longentry] if longentry else None\n        serow = row[shortentry] if shortentry else None\n        lxrow = row[longexit] if longexit else None\n        sxrow = row[shortexit] if shortexit else None\n        # process the long and short events\n        if lerow:\n            if p < 0:\n                # short active, so exit short\n                tradelist.append((dt, [name, Orders.sx, -p, c]))\n                inshort = False\n                h = 0\n                p = 0\n            if p == 0 or scale:\n                # go long (again)\n                tradelist.append((dt, [name, Orders.le, q, c]))\n                inlong = True\n                p = p + q\n        elif serow:\n            if p > 0:\n                # long active, so exit long\n                tradelist.append((dt, [name, Orders.lx, -p, c]))\n                inlong = False\n                h = 0\n                p = 0\n            if p == 0 or scale:\n                # go short (again)\n                tradelist.append((dt, [name, Orders.se, -q, c]))\n                inshort = True\n                p = p - q\n        # check exit conditions\n        if inlong and h > 0 and lxrow:\n            # long active, so exit long\n            tradelist.append((dt, [name, Orders.lx, -p, c]))\n            inlong = False\n            h = 0\n            p = 0\n        if inshort and h > 0 and sxrow:\n            # short active, so exit short\n            tradelist.append((dt, [name, Orders.sx, -p, c]))\n            inshort = False\n            h = 0\n            p = 0\n        # if a holding period was given, then check for exit\n        if holdperiod and h >= holdperiod:\n            if inlong:\n                tradelist.append((dt, [name, Orders.lh, -p, c]))\n                inlong = False\n            if inshort:\n                tradelist.append((dt, [name, Orders.sh, -p, c]))\n                inshort = False\n            h = 0\n            p = 0\n        # increment the hold counter\n        if inlong or inshort:\n            h += 1\n            if intraday and end_of_day:\n                if inlong:\n                    # long active, so exit long\n                    tradelist.append((dt, [name, Orders.lx, -p, c]))\n                    inlong = False\n                if inshort:\n                    # short active, so exit short\n                    tradelist.append((dt, [name, Orders.sx, -p, c]))\n                    inshort = False\n                h = 0\n                p = 0\n    return tradelist\n\n\n#\n# Function run_system\n#\n\ndef run_system(model,\n               system,\n               group,\n               intraday = False,\n               quantity = 1):\n    r""""""Run a system for a given group, creating a trades frame.\n\n    Parameters\n    ----------\n    model : alphapy.Model\n        The model object with specifications.\n    system : alphapy.System\n        The system to run.\n    group : alphapy.Group\n        The group of symbols to trade.\n    intraday : bool, optional\n        If true, this is an intraday system.\n    quantity : float, optional\n        The amount to trade for each symbol, e.g., number of shares\n\n    Returns\n    -------\n    tf : pandas.DataFrame\n        All of the trades for this ``group``.\n\n    """"""\n\n    system_name = system.name\n    logger.info(""Generating Trades for System %s"", system_name)\n\n    # Unpack the model data.\n\n    directory = model.specs[\'directory\']\n    extension = model.specs[\'extension\']\n    separator = model.specs[\'separator\']\n\n    # Extract the group information.\n\n    gname = group.name\n    gmembers = group.members\n    gspace = group.space\n\n    # Run the system for each member of the group\n\n    gtlist = []\n    for symbol in gmembers:\n        # generate the trades for this member\n        tlist = trade_system(model, system, gspace, intraday, symbol, quantity)\n        if tlist:\n            # add trades to global trade list\n            for item in tlist:\n                gtlist.append(item)\n        else:\n            logger.info(""No trades for symbol %s"", symbol)\n\n    # Create group trades frame\n\n    tf = None\n    if gtlist:\n        tspace = Space(system_name, ""trades"", group.space.fractal)\n        gtlist = sorted(gtlist, key=lambda x: x[0])\n        tf = DataFrame.from_dict(dict(gtlist), orient=\'index\', columns=Trade.states)\n        tfname = frame_name(gname, tspace)\n        system_dir = SSEP.join([directory, \'systems\'])\n        labels = [\'date\']\n        if intraday:\n            labels.append(\'time\')\n        write_frame(tf, system_dir, tfname, extension, separator,\n                    index=True, index_label=labels)\n        del tspace\n    else:\n        logger.info(""No trades were found"")\n\n    # Return trades frame\n    return tf\n'"
alphapy/transforms.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : transforms\n# Created   : March 14, 2020\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.calendrical import biz_day_month\nfrom alphapy.calendrical import biz_day_week\nfrom alphapy.globals import NULLTEXT\nfrom alphapy.globals import BSEP, PSEP, USEP\nfrom alphapy.variables import vexec\n\nimport itertools\nimport logging\nimport math\nimport numpy as np\nimport pandas as pd\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function abovema\n#\n\ndef abovema(f, c, p = 50):\n    r""""""Determine those values of the dataframe that are above the\n    moving average.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period of the moving average.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] > ma(f, c, p)\n    return new_column\n\n\n#\n# Function adx\n#\n\ndef adx(f, p = 14):\n    r""""""Calculate the Average Directional Index (ADX).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with all columns required for calculation. If you\n        are applying ADX through ``vapply``, then these columns are\n        calculated automatically.\n    p : int\n        The period over which to calculate the ADX.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    The Average Directional Movement Index (ADX) was invented by J. Welles\n    Wilder in 1978 [WIKI_ADX]_.  Its value reflects the strength of trend in any\n    given instrument.\n\n    .. [WIKI_ADX] https://en.wikipedia.org/wiki/Average_directional_movement_index\n\n    """"""\n    c1 = \'diplus\'\n    vexec(f, c1)\n    c2 = \'diminus\'\n    vexec(f, c2)\n    # calculations\n    dip = f[c1]\n    dim = f[c2]\n    didiff = abs(dip - dim)\n    disum = dip + dim\n    new_column = 100 * didiff.ewm(span=p).mean() / disum\n    return new_column\n\n\n#\n# Function belowma\n#\n\ndef belowma(f, c, p = 50):\n    r""""""Determine those values of the dataframe that are below the\n    moving average.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period of the moving average.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] < ma(f, c, p)\n    return new_column\n\n\n#\n# Function c2max\n#\n    \ndef c2max(f, c1, c2):\n    r""""""Take the maximum value between two columns in a dataframe.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the two columns ``c1`` and ``c2``.\n    c1 : str\n        Name of the first column in the dataframe ``f``.\n    c2 : str\n        Name of the second column in the dataframe ``f``.\n\n    Returns\n    -------\n    max_val : float\n        The maximum value of the two columns.\n\n    """"""\n    max_val = max(f[c1], f[c2])\n    return max_val\n\n\n#\n# Function c2min\n#\n    \ndef c2min(f, c1, c2):\n    r""""""Take the minimum value between two columns in a dataframe.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the two columns ``c1`` and ``c2``.\n    c1 : str\n        Name of the first column in the dataframe ``f``.\n    c2 : str\n        Name of the second column in the dataframe ``f``.\n\n    Returns\n    -------\n    min_val : float\n        The minimum value of the two columns.\n\n    """"""\n    min_val = min(f[c1], f[c2])\n    return min_val\n\n\n#\n# Function diff\n#\n\ndef diff(f, c, n = 1):\n    r""""""Calculate the n-th order difference for the given variable.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    n : int\n        The number of times that the values are differenced.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = np.diff(f[c], n)\n    return new_column\n\n\n#\n# Function diminus\n#\n\ndef diminus(f, p = 14):\n    r""""""Calculate the Minus Directional Indicator (-DI).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n    p : int\n        The period over which to calculate the -DI.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A component of the average directional index (ADX) that is used to\n    measure the presence of a downtrend. When the -DI is sloping downward,\n    it is a signal that the downtrend is getting stronger* [IP_NDI]_.\n\n    .. [IP_NDI] http://www.investopedia.com/terms/n/negativedirectionalindicator.asp\n\n    """"""\n    tr = \'truerange\'\n    vexec(f, tr)\n    atr = USEP.join([\'atr\', str(p)])\n    vexec(f, atr)\n    dmm = \'dmminus\'\n    f[dmm] = dminus(f)\n    new_column = 100 * dminus(f).ewm(span=p).mean() / f[atr]\n    return new_column\n\n\n#\n# Function diplus\n#\n\ndef diplus(f, p = 14):\n    r""""""Calculate the Plus Directional Indicator (+DI).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n    p : int\n        The period over which to calculate the +DI.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A component of the average directional index (ADX) that is used to\n    measure the presence of an uptrend. When the +DI is sloping upward,\n    it is a signal that the uptrend is getting stronger* [IP_PDI]_.\n\n    .. [IP_PDI] http://www.investopedia.com/terms/p/positivedirectionalindicator.asp\n\n    """"""\n    tr = \'truerange\'\n    vexec(f, tr)\n    atr = USEP.join([\'atr\', str(p)])\n    vexec(f, atr)\n    dmp = \'dmplus\'\n    vexec(f, dmp)\n    new_column = 100 * f[dmp].ewm(span=p).mean() / f[atr]\n    return new_column\n\n\n#\n# Function dminus\n#\n\ndef dminus(f):\n    r""""""Calculate the Minus Directional Movement (-DM).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Directional movement is negative (minus) when the prior low minus\n    the current low is greater than the current high minus the prior high.\n    This so-called Minus Directional Movement (-DM) equals the prior low\n    minus the current low, provided it is positive. A negative value\n    would simply be entered as zero* [SC_ADX]_.\n\n    """"""\n    c1 = \'downmove\'\n    f[c1] = -net(f, \'low\')\n    c2 = \'upmove\'\n    f[c2] = net(f, \'high\')\n    new_column = f.apply(gtval0, axis=1, args=[c1, c2])\n    return new_column\n\n\n#\n# Function dmplus\n#\n\ndef dmplus(f):\n    r""""""Calculate the Plus Directional Movement (+DM).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Directional movement is positive (plus) when the current high minus\n    the prior high is greater than the prior low minus the current low.\n    This so-called Plus Directional Movement (+DM) then equals the current\n    high minus the prior high, provided it is positive. A negative value\n    would simply be entered as zero* [SC_ADX]_.\n\n    .. [SC_ADX] http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_directional_index_adx\n\n    """"""\n    c1 = \'upmove\'\n    f[c1] = net(f, \'high\')\n    c2 = \'downmove\'\n    f[c2] = -net(f, \'low\')\n    new_column = f.apply(gtval0, axis=1, args=[c1, c2])\n    return new_column\n\n\n#\n# Function down\n#\n\ndef down(f, c):\n    r""""""Find the negative values in the series.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] < 0\n    return new_column\n\n\n#\n# Function dpc\n#\n\ndef dpc(f, c):\n    r""""""Get the negative values, with positive values zeroed.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with column ``c``.\n    c : str\n        Name of the column.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = f.apply(mval, axis=1, args=[c])\n    return new_column\n\n\n#\n# Function ema\n#\n\ndef ema(f, c, p = 20):\n    r""""""Calculate the mean on a rolling basis.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period over which to calculate the rolling mean.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *An exponential moving average (EMA) is a type of moving average\n    that is similar to a simple moving average, except that more weight\n    is given to the latest data* [IP_EMA]_.\n\n    .. [IP_EMA] http://www.investopedia.com/terms/e/ema.asp\n\n    """"""\n    new_column = pd.ewma(f[c], span=p)\n    return new_column\n\n\n#\n# Function extract_bizday\n#\n\ndef extract_bizday(f, c):\n    r""""""Extract business day of month and week.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the date column ``c``.\n    c : str\n        Name of the date column in the dataframe ``f``.\n\n    Returns\n    -------\n    date_features : pandas.DataFrame\n        The dataframe containing the date features.\n    """"""\n\n    date_features = pd.DataFrame()\n    try:\n        date_features = extract_date(f, c)\n        rdate = date_features.apply(get_rdate, axis=1)\n        bdm = pd.Series(rdate.apply(biz_day_month), name=\'bizday_month\')\n        bdw = pd.Series(rdate.apply(biz_day_week), name=\'bizday_week\')\n        frames = [date_features, bdm, bdw]\n        date_features = pd.concat(frames, axis=1)\n    except:\n        logger.info(""Could not extract business date information from %s column"", c)\n    return date_features\n\n\n#\n# Function extract_date\n#\n\ndef extract_date(f, c):\n    r""""""Extract date into its components: year, month, day.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the date column ``c``.\n    c : str\n        Name of the date column in the dataframe ``f``.\n\n    Returns\n    -------\n    date_features : pandas.DataFrame\n        The dataframe containing the date features.\n    """"""\n\n    fc = pd.to_datetime(f[c])\n    date_features = pd.DataFrame()\n    try:\n        fyear = pd.Series(fc.dt.year, name=\'year\')\n        fmonth = pd.Series(fc.dt.month, name=\'month\')\n        fday = pd.Series(fc.dt.day, name=\'day\')\n        frames = [fyear, fmonth, fday]\n        date_features = pd.concat(frames, axis=1)\n    except:\n        logger.info(""Could not extract date information from %s column"", c)\n    return date_features\n\n\n#\n# Function extract_time\n#\n\ndef extract_time(f, c):\n    r""""""Extract time into its components: hour, minute, second.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the time column ``c``.\n    c : str\n        Name of the time column in the dataframe ``f``.\n\n    Returns\n    -------\n    time_features : pandas.DataFrame\n        The dataframe containing the time features.\n    """"""\n\n    fc = pd.to_datetime(f[c])\n    time_features = pd.DataFrame()\n    try:\n        fhour = pd.Series(fc.dt.hour, name=\'year\')\n        fminute = pd.Series(fc.dt.minute, name=\'month\')\n        fsecond = pd.Series(fc.dt.second, name=\'day\')\n        frames = [fhour, fminute, fsecond]\n        time_features = pd.concat(frames, axis=1)\n    except:\n        logger.info(""Could not extract time information from %s column"", c)\n    return time_features\n\n\n#\n# Function gap\n#\n\ndef gap(f):\n    r""""""Calculate the gap percentage between the current open and\n    the previous close.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``open`` and ``close``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A gap is a break between prices on a chart that occurs when the\n    price of a stock makes a sharp move up or down with no trading\n    occurring in between* [IP_GAP]_.\n\n    .. [IP_GAP] http://www.investopedia.com/terms/g/gap.asp\n\n    """"""\n    c1 = \'open\'\n    c2 = \'close[1]\'\n    vexec(f, c2)\n    new_column = 100 * pchange2(f, c1, c2)\n    return new_column\n\n\n#\n# Function gapbadown\n#\n\ndef gapbadown(f):\n    r""""""Determine whether or not there has been a breakaway gap down.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``open`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A breakaway gap represents a gap in the movement of a stock price\n    supported by levels of high volume* [IP_BAGAP]_.\n\n    .. [IP_BAGAP] http://www.investopedia.com/terms/b/breakawaygap.asp\n\n    """"""\n    new_column = f[\'open\'] < f[\'low\'].shift(1)\n    return new_column\n\n\n#\n# Function gapbaup\n#\n\ndef gapbaup(f):\n    r""""""Determine whether or not there has been a breakaway gap up.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``open`` and ``high``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A breakaway gap represents a gap in the movement of a stock price\n    supported by levels of high volume* [IP_BAGAP]_.\n\n    """"""\n    new_column = f[\'open\'] > f[\'high\'].shift(1)\n    return new_column\n\n\n#\n# Function gapdown\n#\n\ndef gapdown(f):\n    r""""""Determine whether or not there has been a gap down.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``open`` and ``close``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A gap is a break between prices on a chart that occurs when the\n    price of a stock makes a sharp move up or down with no trading\n    occurring in between* [IP_GAP]_.\n\n    """"""\n    new_column = f[\'open\'] < f[\'close\'].shift(1)\n    return new_column\n\n\n#\n# Function gapup\n#\n\ndef gapup(f):\n    r""""""Determine whether or not there has been a gap up.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``open`` and ``close``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *A gap is a break between prices on a chart that occurs when the\n    price of a stock makes a sharp move up or down with no trading\n    occurring in between* [IP_GAP]_.\n\n    """"""\n    new_column = f[\'open\'] > f[\'close\'].shift(1)\n    return new_column\n\n\n#\n# Function gtval\n#\n\ndef gtval(f, c1, c2):\n    r""""""Determine whether or not the first column of a dataframe\n    is greater than the second.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the two columns ``c1`` and ``c2``.\n    c1 : str\n        Name of the first column in the dataframe ``f``.\n    c2 : str\n        Name of the second column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c1] > f[c2]\n    return new_column\n\n\n#\n# Function gtval0\n#\n\ndef gtval0(f, c1, c2):\n    r""""""For positive values in the first column of the dataframe\n    that are greater than the second column, get the value in\n    the first column, otherwise return zero. \n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the two columns ``c1`` and ``c2``.\n    c1 : str\n        Name of the first column in the dataframe ``f``.\n    c2 : str\n        Name of the second column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_val : float\n        A positive value or zero.\n\n    """"""\n    if f[c1] > f[c2] and f[c1] > 0:\n        new_val = f[c1]\n    else:\n        new_val = 0\n    return new_val\n\n\n#\n# Function higher\n#\n\ndef higher(f, c, o = 1):\n    r""""""Determine whether or not a series value is higher than\n    the value ``o`` periods back.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    o : int, optional\n        Offset value for shifting the series.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] > f[c].shift(o)\n    return new_column\n\n\n#\n# Function highest\n#\n\ndef highest(f, c, p = 20):\n    r""""""Calculate the highest value on a rolling basis.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period over which to calculate the rolling maximum.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c].rolling(p).max()\n    return new_column\n\n\n#\n# Function hlrange\n#\n\ndef hlrange(f, p = 1):\n    r""""""Calculate the Range, the difference between High and Low.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n    p : int\n        The period over which the range is calculated.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = highest(f, \'high\', p) - lowest(f, \'low\', p)\n    return new_column\n\n\n#\n# Function lower\n#\n\ndef lower(f, c, o = 1):\n    r""""""Determine whether or not a series value is lower than\n    the value ``o`` periods back.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    o : int, optional\n        Offset value for shifting the series.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] < f[c].shift(o)\n    return new_column\n\n\n#\n# Function lowest\n#\n\ndef lowest(f, c, p = 20):\n    r""""""Calculate the lowest value on a rolling basis.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period over which to calculate the rolling minimum.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    return f[c].rolling(p).min()\n\n\n#\n# Function ma\n#\n\ndef ma(f, c, p = 20):\n    r""""""Calculate the mean on a rolling basis.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period over which to calculate the rolling mean.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *In statistics, a moving average (rolling average or running average)\n    is a calculation to analyze data points by creating series of averages\n    of different subsets of the full data set* [WIKI_MA]_.\n\n    .. [WIKI_MA] https://en.wikipedia.org/wiki/Moving_average\n\n    """"""\n    new_column = f[c].rolling(p).mean()\n    return new_column\n\n\n#\n# Function maratio\n#\n\ndef maratio(f, c, p1 = 1, p2 = 10):\n    r""""""Calculate the ratio of two moving averages.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p1 : int\n        The period of the first moving average.\n    p2 : int\n        The period of the second moving average.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = ma(f, c, p1) / ma(f, c, p2)\n    return new_column\n\n\n#\n# Function mval\n#\n   \ndef mval(f, c):\n    r""""""Get the negative value, otherwise zero.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_val : float\n        Negative value or zero.\n\n    """"""\n    new_val = -f[c] if f[c] < 0 else 0\n    return new_val\n\n\n#\n# Function net\n#\n\ndef net(f, c=\'close\', o = 1):\n    r""""""Calculate the net change of a given column.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    o : int, optional\n        Offset value for shifting the series.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Net change is the difference between the closing price of a security\n    on the day\'s trading and the previous day\'s closing price. Net change\n    can be positive or negative and is quoted in terms of dollars* [IP_NET]_.\n\n    .. [IP_NET] http://www.investopedia.com/terms/n/netchange.asp\n\n    """"""\n    new_column = f[c] - f[c].shift(o)\n    return new_column\n\n\n#\n# Function netreturn\n#\n\ndef netreturn(f, c, o = 1):\n    r""""""Calculate the net return, or Return On Invesment (ROI)\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    o : int, optional\n        Offset value for shifting the series.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *ROI measures the amount of return on an investment relative to the\n    original cost. To calculate ROI, the benefit (or return) of an\n    investment is divided by the cost of the investment, and the result\n    is expressed as a percentage or a ratio* [IP_ROI]_.\n\n    .. [IP_ROI] http://www.investopedia.com/terms/r/returnoninvestment.asp\n\n    """"""\n    new_column = 100 * pchange1(f, c, o)\n    return new_column\n\n\n#\n# Function pchange1\n#\n    \ndef pchange1(f, c, o = 1):\n    r""""""Calculate the percentage change within the same variable.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    o : int\n        Offset to the previous value.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] / f[c].shift(o) - 1.0\n    return new_column\n\n\n#\n# Function pchange2\n#\n\ndef pchange2(f, c1, c2):\n    r""""""Calculate the percentage change between two variables.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the two columns ``c1`` and ``c2``.\n    c1 : str\n        Name of the first column in the dataframe ``f``.\n    c2 : str\n        Name of the second column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c1] / f[c2] - 1.0\n    return new_column\n\n\n#\n# Function pval\n#\n  \ndef pval(f, c):\n    r""""""Get the positive value, otherwise zero.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_val : float\n        Positive value or zero.\n\n    """"""\n    new_val = f[c] if f[c] > 0 else 0\n    return new_val\n\n\n#\n# Function rindex\n#\n\ndef rindex(f, ci, ch, cl, p = 1):\n    r""""""Calculate the *range index* spanning a given period ``p``.\n\n    The **range index** is a number between 0 and 100 that\n    relates the value of the index column ``ci`` to the\n    high column ``ch`` and the low column ``cl``. For example,\n    if the low value of the range is 10 and the high value\n    is 20, then the range index for a value of 15 would be 50%.\n    The range index for 18 would be 80%.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the columns ``ci``, ``ch``, and ``cl``.\n    ci : str\n        Name of the index column in the dataframe ``f``.\n    ch : str\n        Name of the high column in the dataframe ``f``.\n    cl : str\n        Name of the low column in the dataframe ``f``.\n    p : int\n        The period over which the range index of column ``ci``\n        is calculated.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    o = p-1 if f[ci].name == \'open\' else 0\n    hh = highest(f, ch, p)\n    ll = lowest(f, cl, p)\n    fn = f[ci].shift(o) - ll\n    fd = hh - ll\n    new_column = 100 * fn / fd\n    return new_column\n\n\n#\n# Function rsi\n#\n\ndef rsi(f, c, p = 14):\n    r""""""Calculate the Relative Strength Index (RSI).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``net``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    p : int\n        The period over which to calculate the RSI.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Developed by J. Welles Wilder, the Relative Strength Index (RSI) is a momentum\n    oscillator that measures the speed and change of price movements* [SC_RSI]_.\n\n    .. [SC_RSI] http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi\n\n    """"""\n    cdiff = \'net\'\n    vexec(f, cdiff)\n    f[\'pval\'] = upc(f, cdiff)\n    f[\'mval\'] = dpc(f, cdiff)\n    upcs = ma(f, \'pval\', p)\n    dpcs = ma(f, \'mval\', p)\n    new_column = 100 - (100 / (1 + (upcs / dpcs)))\n    return new_column\n\n\n#\n# Function rtotal\n#\n\ndef rtotal(vec):\n    r""""""Calculate the running total.\n\n    Parameters\n    ----------\n    vec : pandas.Series\n        The input array for calculating the running total.\n\n    Returns\n    -------\n    running_total : int\n        The final running total.\n\n    Example\n    -------\n\n    >>> vec.rolling(window=20).apply(rtotal)\n\n    """"""\n    tcount = np.count_nonzero(vec)\n    fcount = len(vec) - tcount\n    running_total = tcount - fcount\n    return running_total\n\n\n#\n# Function runs\n#\n\ndef runs(vec):\n    r""""""Calculate the total number of runs.\n\n    Parameters\n    ----------\n    vec : pandas.Series\n        The input array for calculating the number of runs.\n\n    Returns\n    -------\n    runs_value : int\n        The total number of runs.\n\n    Example\n    -------\n\n    >>> vec.rolling(window=20).apply(runs)\n\n    """"""\n    runs_value = len(list(itertools.groupby(vec)))\n    return runs_value\n\n\n#\n# Function runs_test\n#\n\ndef runs_test(f, c, wfuncs, window):\n    r""""""Perform a runs test on binary series.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n    wfuncs : list\n        The set of runs test functions to apply to the column:\n\n        ``\'all\'``:\n            Run all of the functions below.\n        ``\'rtotal\'``:\n            The running total over the ``window`` period.\n        ``\'runs\'``:\n            Total number of runs in ``window``.\n        ``\'streak\'``:\n            The length of the latest streak.\n        ``\'zscore\'``:\n            The Z-Score over the ``window`` period.\n    window : int\n        The rolling period.\n\n    Returns\n    -------\n    new_features : pandas.DataFrame\n        The dataframe containing the runs test features.\n\n    References\n    ----------\n    For more information about runs tests for detecting non-randomness,\n    refer to [RUNS]_.\n\n    .. [RUNS] http://www.itl.nist.gov/div898/handbook/eda/section3/eda35d.htm\n\n    """"""\n\n    fc = f[c]\n    all_funcs = {\'runs\'   : runs,\n                 \'streak\' : streak,\n                 \'rtotal\' : rtotal,\n                 \'zscore\' : zscore}\n    # use all functions\n    if \'all\' in wfuncs:\n        wfuncs = list(all_funcs.keys())\n    # apply each of the runs functions\n    new_features = pd.DataFrame()\n    for w in wfuncs:\n        if w in all_funcs:\n            new_feature = fc.rolling(window=window).apply(all_funcs[w])\n            new_feature.fillna(0, inplace=True)\n            new_column_name = PSEP.join([c, w])\n            new_feature = new_feature.rename(new_column_name)\n            frames = [new_features, new_feature]\n            new_features = pd.concat(frames, axis=1)\n        else:\n            logger.info(""Runs Function %s not found"", w)\n    return new_features\n\n\n#\n# Function split_to_letters\n#\n\ndef split_to_letters(f, c):\n    r""""""Separate text into distinct characters.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the text column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_feature : pandas.Series\n        The array containing the new feature.\n\n    Example\n    -------\n    The value \'abc\' becomes \'a b c\'.\n\n    """"""\n    fc = f[c]\n    new_feature = None\n    dtype = fc.dtypes\n    if dtype == \'object\':\n        fc.fillna(NULLTEXT, inplace=True)\n        maxlen = fc.str.len().max()\n        if maxlen > 1:\n            new_feature = fc.apply(lambda x: BSEP.join(list(x)))\n    return new_feature\n\n\n#\n# Function streak\n#\n\ndef streak(vec):\n    r""""""Determine the length of the latest streak.\n\n    Parameters\n    ----------\n    vec : pandas.Series\n        The input array for calculating the latest streak.\n\n    Returns\n    -------\n    latest_streak : int\n        The length of the latest streak.\n\n    Example\n    -------\n\n    >>> vec.rolling(window=20).apply(streak)\n\n    """"""\n    latest_streak = [len(list(g)) for k, g in itertools.groupby(vec)][-1]\n    return latest_streak\n\n\n#\n# Function texplode\n#\n\ndef texplode(f, c):\n    r""""""Get dummy values for a text column.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the text column in the dataframe ``f``.\n\n    Returns\n    -------\n    dummies : pandas.DataFrame\n        The dataframe containing the dummy variables.\n\n    Example\n    -------\n\n    This function is useful for columns that appear to\n    have separate character codes but are consolidated\n    into a single column. Here, the column ``c`` is\n    transformed into five dummy variables.\n\n    === === === === === ===\n     c  0_a 1_x 1_b 2_x 2_z\n    === === === === === ===\n    abz   1   0   1   0   1\n    abz   1   0   1   0   1\n    axx   1   1   0   1   0\n    abz   1   0   1   0   1\n    axz   1   1   0   0   1\n    === === === === === ===\n\n    """"""\n    fc = f[c]\n    maxlen = fc.str.len().max()\n    fc.fillna(maxlen * BSEP, inplace=True)\n    fpad = str().join([\'{:\', BSEP, \'>\', str(maxlen), \'}\'])\n    fcpad = fc.apply(fpad.format)\n    fcex = fcpad.apply(lambda x: pd.Series(list(x)))\n    dummies = pd.get_dummies(fcex)\n    return dummies\n\n\n#\n# Function truehigh\n#\n\ndef truehigh(f):\n    r""""""Calculate the *True High* value.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Today\'s high, or the previous close, whichever is higher* [TS_TR]_.\n\n    .. [TS_TR] http://help.tradestation.com/09_01/tradestationhelp/charting_definitions/true_range.htm\n\n    """"""\n    c1 = \'low[1]\'\n    vexec(f, c1)\n    c2 = \'high\'\n    new_column = f.apply(c2max, axis=1, args=[c1, c2])\n    return new_column\n\n\n#\n# Function truelow\n#\n\ndef truelow(f):\n    r""""""Calculate the *True Low* value.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *Today\'s low, or the previous close, whichever is lower* [TS_TR]_.\n\n    """"""\n    c1 = \'high[1]\'\n    vexec(f, c1)\n    c2 = \'low\'\n    new_column = f.apply(c2min, axis=1, args=[c1, c2])\n    return new_column\n\n\n#\n# Function truerange\n#\n\ndef truerange(f):\n    r""""""Calculate the *True Range* value.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with columns ``high`` and ``low``.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    References\n    ----------\n    *True High - True Low* [TS_TR]_.\n\n    """"""\n    new_column = truehigh(f) - truelow(f)\n    return new_column\n\n\n#\n# Function up\n#\n\ndef up(f, c):\n    r""""""Find the positive values in the series.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str\n        Name of the column in the dataframe ``f``.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    """"""\n    new_column = f[c] > 0\n    return new_column\n\n\n#\n# Function upc\n#\n\ndef upc(f, c):\n    r""""""Get the positive values, with negative values zeroed.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe with column ``c``.\n    c : str\n        Name of the column.\n\n    Returns\n    -------\n    new_column : pandas.Series (float)\n        The array containing the new feature.\n\n    """"""\n    new_column = f.apply(pval, axis=1, args=[c])\n    return new_column\n\n\n#\n# Function xmadown\n#\n\ndef xmadown(f, c=\'close\', pfast = 20, pslow = 50):\n    r""""""Determine those values of the dataframe that are below the\n    moving average.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str, optional\n        Name of the column in the dataframe ``f``.\n    pfast : int, optional\n        The period of the fast moving average.\n    pslow : int, optional\n        The period of the slow moving average.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *In the statistics of time series, and in particular the analysis\n    of financial time series for stock trading purposes, a moving-average\n    crossover occurs when, on plotting two moving averages each based\n    on different degrees of smoothing, the traces of these moving averages\n    cross* [WIKI_XMA]_.\n\n    .. [WIKI_XMA] https://en.wikipedia.org/wiki/Moving_average_crossover\n\n    """"""\n    sma = ma(f, c, pfast)\n    sma_prev = sma.shift(1)\n    lma = ma(f, c, pslow)\n    lma_prev = lma.shift(1)\n    new_column = (sma < lma) & (sma_prev > lma_prev)\n    return new_column\n\n\n#\n# Function xmaup\n#\n\ndef xmaup(f, c=\'close\', pfast = 20, pslow = 50):\n    r""""""Determine those values of the dataframe that are below the\n    moving average.\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe containing the column ``c``.\n    c : str, optional\n        Name of the column in the dataframe ``f``.\n    pfast : int, optional\n        The period of the fast moving average.\n    pslow : int, optional\n        The period of the slow moving average.\n\n    Returns\n    -------\n    new_column : pandas.Series (bool)\n        The array containing the new feature.\n\n    References\n    ----------\n    *In the statistics of time series, and in particular the analysis\n    of financial time series for stock trading purposes, a moving-average\n    crossover occurs when, on plotting two moving averages each based\n    on different degrees of smoothing, the traces of these moving averages\n    cross* [WIKI_XMA]_.\n\n    """"""\n    sma = ma(f, c, pfast)\n    sma_prev = sma.shift(1)\n    lma = ma(f, c, pslow)\n    lma_prev = lma.shift(1)\n    new_column = (sma > lma) & (sma_prev < lma_prev)\n    return new_column\n\n\n#\n# Function zscore\n#\n\ndef zscore(vec):\n    r""""""Calculate the Z-Score.\n\n    Parameters\n    ----------\n    vec : pandas.Series\n        The input array for calculating the Z-Score.\n\n    Returns\n    -------\n    zscore : float\n        The value of the Z-Score.\n\n    References\n    ----------\n    To calculate the Z-Score, you can find more information here [ZSCORE]_.\n\n    .. [ZSCORE] https://en.wikipedia.org/wiki/Standard_score\n\n    Example\n    -------\n\n    >>> vec.rolling(window=20).apply(zscore)\n\n    """"""\n    n1 = np.count_nonzero(vec)\n    n2 = len(vec) - n1\n    fac1 = float(2 * n1 * n2)\n    fac2 = float(n1 + n2)\n    rbar = fac1 / fac2 + 1\n    sr2num = fac1 * (fac1 - n1 - n2)\n    sr2den = math.pow(fac2, 2) * (fac2 - 1)\n    sr = math.sqrt(sr2num / sr2den)\n    if sr2den and sr:\n        zscore = (runs(vec) - rbar) / sr\n    else:\n        zscore = 0\n    return zscore\n'"
alphapy/utilities.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : utilities\n# Created   : July 11, 2013\n#\n# Copyright 2017 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Imports\n#\n\nfrom alphapy.globals import PSEP, SSEP, USEP\n\nimport argparse\nfrom datetime import datetime, timedelta\nimport glob\nimport inspect\nfrom itertools import groupby\nimport logging\nimport numpy as np\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport re\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Function get_datestamp\n#\n\ndef get_datestamp():\n    r""""""Returns today\'s datestamp.\n\n    Returns\n    -------\n    datestamp : str\n        The valid date string in YYYY-mm-dd format.\n\n    """"""\n    d = datetime.now()\n    f = ""%Y%m%d""\n    datestamp = d.strftime(f)\n    return datestamp\n\n\n#\n# Function most_recent_file\n#\n\ndef most_recent_file(directory, file_spec):\n    r""""""Find the most recent file in a directory.\n\n    Parameters\n    ----------\n    directory : str\n        Full directory specification.\n    file_spec : str\n        Wildcard search string for the file to locate.\n\n    Returns\n    -------\n    file_name : str\n        Name of the file to read, excluding the ``extension``.\n\n    """"""\n    # Create search path\n    search_path = SSEP.join([directory, file_spec])\n    # find the latest file\n    file_name = max(glob.iglob(search_path), key=os.path.getctime)\n    # load the model predictor\n    return file_name\n\n\n#\n# Function np_store_data\n#\n\ndef np_store_data(data, dir_name, file_name, extension, separator):\n    r""""""Store NumPy data in a file.\n\n    Parameters\n    ----------\n    data : numpy array\n        The model component to store\n    dir_name : str\n        Full directory specification.\n    file_name : str\n        Name of the file to read, excluding the ``extension``.\n    extension : str\n        File name extension, e.g., ``csv``.\n    separator : str\n        The delimiter between fields in the file.\n\n    Returns\n    -------\n    None : None\n\n    """"""\n    output_file = PSEP.join([file_name, extension])\n    output = SSEP.join([dir_name, output_file])\n    logger.info(""Storing output to %s"", output)\n    np.savetxt(output, data, delimiter=separator)\n\n\n#\n# Function remove_list_items\n#\n\ndef remove_list_items(elements, alist):\n    r""""""Remove one or more items from the given list.\n\n    Parameters\n    ----------\n    elements : list\n        The items to remove from the list ``alist``.\n    alist : list\n        Any object of any type can be a list item.\n\n    Returns\n    -------\n    sublist : list\n        The subset of items after removal.\n\n    Examples\n    --------\n\n    >>> test_list = [\'a\', \'b\', \'c\', test_func]\n    >>> remove_list_items([test_func], test_list)  # [\'a\', \'b\', \'c\']\n\n    """"""\n    sublist = [x for x in alist if x not in elements]\n    return sublist\n\n\n#\n# Function subtract_days\n#\n\ndef subtract_days(date_string, ndays):\n    r""""""Subtract a number of days from a given date.\n\n    Parameters\n    ----------\n    date_string : str\n        An alphanumeric string in the format %Y-%m-%d.\n    ndays : int\n        Number of days to subtract.\n\n    Returns\n    -------\n    new_date_string : str\n        The adjusted date string in the format %Y-%m-%d.\n\n    Examples\n    --------\n\n    >>> subtract_days(\'2017-11-10\', 31)   # \'2017-10-10\'\n\n    """"""\n    new_date_string = None\n    valid = valid_date(date_string)\n    if valid:\n        date_dt = datetime.strptime(date_string, ""%Y-%m-%d"")\n        new_date = date_dt - timedelta(days=ndays)\n        new_date_string = new_date.strftime(""%Y-%m-%d"")\n    return new_date_string\n\n\n#\n# Function valid_date\n#\n\ndef valid_date(date_string):\n    r""""""Determine whether or not the given string is a valid date.\n\n    Parameters\n    ----------\n    date_string : str\n        An alphanumeric string in the format %Y-%m-%d.\n\n    Returns\n    -------\n    date_string : str\n        The valid date string.\n\n    Raises\n    ------\n    ValueError\n        Not a valid date.\n\n    Examples\n    --------\n\n    >>> valid_date(\'2016-7-1\')   # datetime.datetime(2016, 7, 1, 0, 0)\n    >>> valid_date(\'345\')        # ValueError: Not a valid date\n\n    """"""\n    try:\n        date_time = datetime.strptime(date_string, ""%Y-%m-%d"")\n        return date_string\n    except:\n        message = ""Not a valid date: \'{0}\'."".format(date_string)\n        raise argparse.ArgumentTypeError(message)\n\n\n#\n# Function valid_name\n#\n\ndef valid_name(name):\n    r""""""Determine whether or not the given string is a valid\n    alphanumeric string.\n\n    Parameters\n    ----------\n    name : str\n        An alphanumeric identifier.\n\n    Returns\n    -------\n    result : bool\n        ``True`` if the name is valid, else ``False``.\n\n    Examples\n    --------\n\n    >>> valid_name(\'alpha\')   # True\n    >>> valid_name(\'!alpha\')  # False\n\n    """"""\n    identifier = re.compile(r""^[^\\d\\W]\\w*\\Z"", re.UNICODE)\n    result = re.match(identifier, name)\n    return result is not None\n'"
alphapy/variables.py,0,"b'################################################################################\n#\n# Package   : AlphaPy\n# Module    : variables\n# Created   : July 11, 2013\n#\n# Copyright 2020 ScottFree Analytics LLC\n# Mark Conway & Robert D. Scott II\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n\n\n#\n# Variables\n# ---------\n#\n# Numeric substitution is allowed for any number in the expression.\n# Offsets are allowed in event expressions but cannot be substituted.\n#\n# Examples\n# --------\n#\n# Variable(\'rrunder\', \'rr_3_20 <= 0.9\')\n#\n# \'rrunder_2_10_0.7\'\n# \'rrunder_2_10_0.9\'\n# \'xmaup_20_50_20_200\'\n# \'xmaup_10_50_20_50\'\n#\n\n\n#\n# Imports\n#\n\nfrom alphapy.alias import get_alias\nfrom alphapy.frame import Frame\nfrom alphapy.frame import frame_name\nfrom alphapy.globals import BSEP, LOFF, ROFF, USEP\nfrom alphapy.utilities import valid_name\n\nimport builtins\nfrom collections import OrderedDict\nfrom importlib import import_module\nimport logging\nimport numpy as np\nimport pandas as pd\nimport parser\nimport re\nimport sys\n\n\n#\n# Initialize logger\n#\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Class Variable\n#\n\nclass Variable(object):\n    """"""Create a new variable as a key-value pair. All variables are stored\n    in ``Variable.variables``. Duplicate keys or values are not allowed,\n    unless the ``replace`` parameter is ``True``.\n\n    Parameters\n    ----------\n    name : str\n        Variable key.\n    expr : str\n        Variable value.\n    replace : bool, optional\n        Replace the current key-value pair if it already exists.\n\n    Attributes\n    ----------\n    variables : dict\n        Class variable for storing all known variables\n\n    Examples\n    --------\n    \n    >>> Variable(\'rrunder\', \'rr_3_20 <= 0.9\')\n    >>> Variable(\'hc\', \'higher_close\')\n\n    """"""\n\n    # class variable to track all variables\n\n    variables = {}\n\n    # function __new__\n\n    def __new__(cls,\n                name,\n                expr,\n                replace = False):\n        # code\n        efound = expr in [Variable.variables[key].expr for key in Variable.variables]\n        if efound:\n            key = [key for key in Variable.variables if expr in Variable.variables[key].expr]\n            logger.info(""Expression \'%s\' already exists for key %s"", expr, key)\n            return\n        else:\n            if replace or not name in Variable.variables:\n                if not valid_name(name):\n                    logger.info(""Invalid variable key: %s"", name)\n                    return\n                try:\n                    result = parser.expr(expr)\n                except:\n                    logger.info(""Invalid expression: %s"", expr)\n                    return\n                return super(Variable, cls).__new__(cls)\n            else:\n                logger.info(""Key %s already exists"", name)\n\n    # function __init__\n\n    def __init__(self,\n                 name,\n                 expr,\n                 replace = False):\n        # code\n        self.name = name;\n        self.expr = expr;\n        # add key with expression\n        Variable.variables[name] = self\n            \n    # function __str__\n\n    def __str__(self):\n        return self.expr\n\n\n#\n# Function vparse\n#\n\ndef vparse(vname):\n    r""""""Parse a variable name into its respective components.\n\n    Parameters\n    ----------\n    vname : str\n        The name of the variable.\n\n    Returns\n    -------\n    vxlag : str\n        Variable name without the ``lag`` component.\n    root : str\n        The base variable name without the parameters.\n    plist : list\n        The parameter list.\n    lag : int\n        The offset starting with the current value [0]\n        and counting back, e.g., an offset [1] means the\n        previous value of the variable.\n\n    Notes\n    -----\n\n    **AlphaPy** makes feature creation easy. The syntax\n    of a variable name maps to a function call:\n\n    xma_20_50 => xma(20, 50)\n\n    Examples\n    --------\n\n    >>> vparse(\'xma_20_50[1]\')\n    # (\'xma_20_50\', \'xma\', [\'20\', \'50\'], 1)\n\n    """"""\n\n    # split along lag first\n    lsplit = vname.split(LOFF)\n    vxlag = lsplit[0]\n    # if necessary, substitute any alias\n    root = vxlag.split(USEP)[0]\n    alias = get_alias(root)\n    if alias:\n        vxlag = vxlag.replace(root, alias)\n    vsplit = vxlag.split(USEP)\n    root = vsplit[0]\n    plist = vsplit[1:]\n    # extract lag\n    lag = 0\n    if len(lsplit) > 1:\n        # lag is present\n        slag = lsplit[1].replace(ROFF, \'\')\n        if len(slag) > 0:\n            lpat = r\'(^-?[0-9]+$)\'\n            lre = re.compile(lpat)\n            if lre.match(slag):\n                lag = int(slag)\n    # return all components\n    return vxlag, root, plist, lag\n\n\n#\n# Function allvars\n#\n\ndef allvars(expr):\n    r""""""Get the list of valid names in the expression.\n\n    Parameters\n    ----------\n    expr : str\n        A valid expression conforming to the Variable Definition Language.\n\n    Returns\n    -------\n    vlist : list\n        List of valid variable names.\n\n    """"""\n    regex = re.compile(\'\\w+\')\n    items = regex.findall(expr)\n    vlist = []\n    for item in items:\n        if valid_name(item):\n            vlist.append(item)\n    return vlist\n\n\n#\n# Function vtree\n#\n\ndef vtree(vname):\n    r""""""Get all of the antecedent variables. \n\n    Before applying a variable to a dataframe, we have to recursively\n    get all of the child variables, beginning with the starting variable\'s\n    expression. Then, we have to extract the variables from all the\n    subsequent expressions. This process continues until all antecedent\n    variables are obtained.\n\n    Parameters\n    ----------\n    vname : str\n        A valid variable stored in ``Variable.variables``.\n\n    Returns\n    -------\n    all_variables : list\n        The variables that need to be applied before ``vname``.\n\n    Other Parameters\n    ----------------\n    Variable.variables : dict\n        Global dictionary of variables\n\n    """"""\n    allv = []\n    def vwalk(allv, vname):\n        vxlag, root, plist, lag = vparse(vname)\n        if root in Variable.variables:\n            root_expr = Variable.variables[root].expr\n            expr = vsub(vname, root_expr)\n            av = allvars(expr)\n            for v in av:\n                vwalk(allv, v)\n        else:\n            for p in plist:\n                if valid_name(p):\n                    vwalk(allv, p)\n        allv.append(vname)\n        return allv\n    allv = vwalk(allv, vname)\n    all_variables = list(OrderedDict.fromkeys(allv))\n    return all_variables\n\n\n#\n# Function vsub\n#\n\ndef vsub(v, expr):\n    r""""""Substitute the variable parameters into the expression.\n\n    This function performs the parameter substitution when\n    applying features to a dataframe. It is a mechanism for\n    the user to override the default values in any given\n    expression when defining a feature, instead of having\n    to programmatically call a function with new values.  \n\n    Parameters\n    ----------\n    v : str\n        Variable name.\n    expr : str\n        The expression for substitution.\n\n    Returns\n    -------\n    newexpr\n        The expression with the new, substituted values.\n\n    """"""\n    # numbers pattern\n    npat = \'[-+]?[0-9]*\\.?[0-9]+\'\n    nreg = re.compile(npat)\n    # find all number locations in variable name\n    vnums = nreg.findall(v)\n    viter = nreg.finditer(v)\n    vlocs = []\n    for match in viter:\n        vlocs.append(match.span())\n    # find all number locations in expression\n    # find all non-number locations as well\n    elen = len(expr)\n    enums = nreg.findall(expr)\n    eiter = nreg.finditer(expr)\n    elocs = []\n    enlocs = []\n    index = 0\n    for match in eiter:\n        eloc = match.span()\n        elocs.append(eloc)\n        enlocs.append((index, eloc[0]))\n        index = eloc[1]\n    # build new expression\n    newexpr = str()\n    for i, enloc in enumerate(enlocs):\n        if i < len(vlocs):\n            newexpr += expr[enloc[0]:enloc[1]] + v[vlocs[i][0]:vlocs[i][1]]\n        else:\n            newexpr += expr[enloc[0]:enloc[1]] + expr[elocs[i][0]:elocs[i][1]]\n    if elocs:\n        estart = elocs[len(elocs)-1][1]\n    else:\n        estart = 0\n    newexpr += expr[estart:elen]\n    return newexpr\n\n    \n#\n# Function vexec\n#\n\ndef vexec(f, v, vfuncs=None):\n    r""""""Add a variable to the given dataframe.\n\n    This is the core function for adding a variable to a dataframe.\n    The default variable functions are already defined locally\n    in ``alphapy.transforms``; however, you may want to define your\n    own variable functions. If so, then the ``vfuncs`` parameter\n    will contain the list of modules and functions to be imported\n    and applied by the ``vexec`` function.\n\n    To write your own variable function, your function must have\n    a pandas *DataFrame* as an input parameter and must return\n    a pandas *DataFrame* with the new variable(s).\n\n    Parameters\n    ----------\n    f : pandas.DataFrame\n        Dataframe to contain the new variable.\n    v : str\n        Variable to add to the dataframe.\n    vfuncs : dict, optional\n        Dictionary of external modules and functions.\n\n    Returns\n    -------\n    f : pandas.DataFrame\n        Dataframe with the new variable.\n\n    Other Parameters\n    ----------------\n    Variable.variables : dict\n        Global dictionary of variables\n\n    """"""\n    vxlag, root, plist, lag = vparse(v)\n    logger.debug(""vexec : %s"", v)\n    logger.debug(""vxlag : %s"", vxlag)\n    logger.debug(""root  : %s"", root)\n    logger.debug(""plist : %s"", plist)\n    logger.debug(""lag   : %s"", lag)\n    if vxlag not in f.columns:\n        if root in Variable.variables:\n            logger.debug(""Found variable %s: "", root)\n            vroot = Variable.variables[root]\n            expr = vroot.expr\n            expr_new = vsub(vxlag, expr)\n            estr = ""%s"" % expr_new\n            logger.debug(""Expression: %s"", estr)\n            # pandas eval\n            f[vxlag] = f.eval(estr)\n        else:\n            logger.debug(""Did not find variable: %s"", root)\n            # Must be a function call\n            func_name = root\n            # Convert the parameter list and prepend the data frame\n            newlist = []\n            for p in plist:\n                try:\n                    newlist.append(int(p))\n                except:\n                    try:\n                        newlist.append(float(p))\n                    except:\n                        newlist.append(p)\n            newlist.insert(0, f)\n            # Find the module and function\n            module = None\n            if vfuncs:\n                for m in vfuncs:\n                    funcs = vfuncs[m]\n                    if func_name in funcs:\n                        module = m\n                        break\n            # If the module was found, import the external transform function,\n            # else search the local namespace and AlphaPy.\n            if module:\n                ext_module = import_module(module)\n                func = getattr(ext_module, func_name)\n            else:\n                modname = globals()[\'__name__\']\n                module = sys.modules[modname]\n                if func_name in dir(module):\n                    func = getattr(module, func_name)\n                else:\n                    try:\n                        ap_module = import_module(\'alphapy.transforms\')\n                        func = getattr(ap_module, func_name)\n                    except:\n                        func = None\n            if func:\n                # Create the variable by calling the function\n                f[v] = func(*newlist)\n            elif func_name not in dir(builtins):\n                module_error = ""*** Could not find module to execute function {} ***"".format(func_name)\n                logger.error(module_error)\n                sys.exit(module_error)\n    # if necessary, add the lagged variable\n    if lag > 0 and vxlag in f.columns:\n        f[v] = f[vxlag].shift(lag)\n    # output frame\n    return f\n\n\n#\n# Function vapply\n#\n\ndef vapply(group, vname, vfuncs=None):\n    r""""""Apply a variable to multiple dataframes.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The input group.\n    vname : str\n        The variable to apply to the ``group``.\n    vfuncs : dict, optional\n        Dictionary of external modules and functions.\n\n    Returns\n    -------\n    None : None\n\n    Other Parameters\n    ----------------\n    Frame.frames : dict\n        Global dictionary of dataframes\n\n    See Also\n    --------\n    vunapply\n\n    """"""\n    # get all frame names to apply variables\n    gnames = [item.lower() for item in group.members]\n    # get all the precedent variables\n    allv = vtree(vname)\n    # apply the variables to each frame\n    for g in gnames:\n        fname = frame_name(g, group.space)\n        if fname in Frame.frames:\n            f = Frame.frames[fname].df\n            if not f.empty:\n                for v in allv:\n                    logger.debug(""Applying variable %s to %s"", v, g)\n                    f = vexec(f, v, vfuncs)\n            else:\n                logger.debug(""Frame for %s is empty"", g)\n        else:\n            logger.debug(""Frame not found: %s"", fname)\n                \n\n#\n# Function vmapply\n#\n\ndef vmapply(group, vs, vfuncs=None):\n    r""""""Apply multiple variables to multiple dataframes.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The input group.\n    vs : list\n        The list of variables to apply to the ``group``.\n    vfuncs : dict, optional\n        Dictionary of external modules and functions.\n\n    Returns\n    -------\n    None : None\n\n    See Also\n    --------\n    vmunapply\n\n    """"""\n    for v in vs:\n        logger.info(""Applying variable: %s"", v)\n        vapply(group, v, vfuncs)\n\n        \n#\n# Function vunapply\n#\n\ndef vunapply(group, vname):\n    r""""""Remove a variable from multiple dataframes.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The input group.\n    vname : str\n        The variable to remove from the ``group``.\n\n    Returns\n    -------\n    None : None\n\n    Other Parameters\n    ----------------\n    Frame.frames : dict\n        Global dictionary of dataframes\n\n    See Also\n    --------\n    vapply\n\n    """"""\n    # get all frame names to apply variables\n    gnames = [item.lower() for item in group.all_members()]\n    # apply the variables to each frame\n    for g in gnames:\n        fname = frame_name(g, group.space)\n        if fname in Frame.frames:\n            f = Frame.frames[fname].df\n            logger.info(""Unapplying variable %s from %s"", vname, g)\n            if vname not in f.columns:\n                logger.info(""Variable %s not in %s frame"", vname, g)\n            else:\n                estr = ""Frame.frames[\'%s\'].df = f.df.drop(\'%s\', axis=1)"" \\\n                        % (fname, vname)\n                exec(estr)\n        else:\n            logger.info(""Frame not found: %s"", fname)\n            \n\n#\n# Function vmunapply\n#\n\ndef vmunapply(group, vs):\n    r""""""Remove a list of variables from multiple dataframes.\n\n    Parameters\n    ----------\n    group : alphapy.Group\n        The input group.\n    vs : list\n        The list of variables to remove from the ``group``.\n\n    Returns\n    -------\n    None : None\n\n    See Also\n    --------\n    vmapply\n\n    """"""\n    for v in vs:\n        vunapply(group, v)\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# AlphaPy documentation build configuration file, created by\n# sphinx-quickstart on Sat Mar 11 20:20:30 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n#\nimport os\nimport sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n              \'sphinx.ext.napoleon\',\n              \'sphinx.ext.mathjax\']\n\nnapoleon_google_docstring = False\nnapoleon_use_param = False\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'AlphaPy\'\ncopyright = \'2020, ScottFree Analytics LLC\'\nauthor = \'Robert D. Scott II, Mark Conway\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'2.4.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'2.4.2\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'AlphaPydoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'AlphaPy.tex\', \'AlphaPy Documentation\',\n     \'Mark Conway, Robert D. Scott II\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'alphapy\', \'AlphaPy Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'AlphaPy\', \'AlphaPy Documentation\',\n     author, \'AlphaPy\', \'AutoML for Stocks and Sports\',\n     \'Miscellaneous\'),\n]\n'"
