file_path,api_count,code
python/setup.py,0,"b'#!/usr/bin/env python\nimport os\nfrom setuptools import setup, find_packages\nfrom seldon import __version__\n\nsetup(name=\'seldon\',\n      version=__version__,\n      description=\'Seldon Python Utilities\',\n      author=\'Clive Cox\',\n      author_email=\'support@seldon.io\',\n      url=\'https://github.com/SeldonIO/seldon-server\',\n      license=\'Apache 2 License\',\n      install_requires=[\'wabbit_wappa==3.0.2\',\'xgboost\',\'kazoo\',\'scikit-learn>=0.17\',\'scipy\',\'numpy\',\'boto\',\'filechunkio\',\'keras\',\'pylibmc\', \'annoy\', \'gensim\', ""Flask"", ""pandas>=0.17"", ""cmd2"", ""MySQL-python"", ""smart_open"", ""luigi""],\n      dependency_links = [\n          \'https://github.com/SeldonIO/wabbit_wappa/archive/3.0.2.zip#egg=wabbit-wappa-3.0.2\'\n      ],\n      packages=[\'seldon\', \'seldon.pipeline\', \'seldon.microservice\', \'seldon.text\', \'seldon.shell\', \'seldon.cli\',\'seldon.misc\',\'seldon.luigi\',\'seldon.rpc\',\'seldon.anomaly\'],\n      package_dir={\'seldon.shell\': \'seldon/shell\', \'seldon.cli\': \'seldon/cli\'},\n      package_data={\'seldon.shell\': [\'dbschema/mysql/api.sql\', \'dbschema/mysql/client.sql\'], \'seldon.cli\': [\'dbschema/mysql/api.sql\', \'dbschema/mysql/client.sql\', \'grafana/client-dashboard.json\']},\n      scripts=[\'bin/seldon-shell\',\'bin/seldon-cli\'],\n      )\n'"
scripts/add_attr_schema.py,0,"b'#!/usr/bin/env python\nimport json\nimport MySQLdb\nimport getopt, argparse\n\nfrom pprint import pprint\n\n\nvalid_value_types = set([\'double\', \'string\', \'date\', \'text\', \'int\',\'boolean\'])\nvalue_types_to_db_map = dict(double=\'DOUBLE\', string=\'VARCHAR\', date=\'DATETIME\', int=\'INT\', boolean=\'BOOLEAN\',\n\ttext=\'TEXT\', enum=\'ENUM\')\n\ndef addAttrsToDb(attrs, item_type):\n\tattrs.append({""name"":""content_type"", ""value_type"":[""article""]})\n\tfor attr in attrs:\n\t\tattrValType = attr[\'value_type\']\n\t\tif type(attrValType) is list:\n\t\t\tattrValType = \'enum\'\n\t\tcur = db.cursor()\n\t\tcur.execute(""INSERT INTO ITEM_ATTR (name, type, item_type) ""\n\t\t\t+ "" VALUES (%s, %s, %s)"", (attr[\'name\'], value_types_to_db_map[attrValType], item_type))\n\t\tif attrValType is \'enum\':\n\t\t\tfor index,enum in enumerate(attr[\'value_type\'], start=1):\n\t\t\t\tcur = db.cursor()\n\t\t\t\tcur.execute(""SELECT attr_id FROM ITEM_ATTR WHERE NAME = %s and ITEM_TYPE = %s"", (attr[\'name\'],item_type))\n\t\t\t\trows = cur.fetchall()\n\t\t\t\tattrId = rows[0][0]\n\t\t\t\tcur = db.cursor()\n\t\t\t\tcur.execute(""INSERT INTO ITEM_ATTR_ENUM (attr_id, value_id, value_name) VALUES (%s, %s, %s)"",(attrId, index, enum))\n\tcur = db.cursor()\n\n\tcur.execute(""SELECT attr_id, value_id, value_name FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tfor row in rows:\n\t\tenum_attr_id = row[0]\n\t\tenum_value_id = row[1]\n\t\tenum_value_name = row[2]\n\t\tcur = db.cursor()\n\t\tcur.execute(""INSERT INTO DIMENSION (item_type, attr_id, value_id) VALUES""\n\t\t\t+ "" (%s, %s, %s)"", (item_type, enum_attr_id, enum_value_id))\n\ndef doDbChecks():\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_TYPE"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_TYPE table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_ATTR table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""ITEM_ATTR_ENUM table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM DIMENSION"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""DIMENSION table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\n\ndef doExitBecauseDbNotEmpty():\n\tprint ""To run this script, the relevant DB tables must be empty. Please rerun this script with the -clean option to delete these entries.""\n\texit(1)\n\ndef addToDb(types):\n\twith db:\n\t\tdoDbChecks()\n\t\tfor theType in types:\n\t\t\tcur= db.cursor()\n\t\t\tcur.execute(""INSERT INTO ITEM_TYPE (type_id, name)""+\n\t\t\t\t"" values (%s, %s)"",(theType[\'type_id\'],theType[\'type_name\']))\n\t\t\taddAttrsToDb(theType[\'type_attrs\'], theType[\'type_id\'])\n\n\ndef validateValueType(valType):\n    theType = type(valType)\n    if theType is list:\n    \tfor enum in valType:\n    \t\ttheEnumType = type(enum)\n    \t\tif theEnumType is not unicode and theEnumType is not str:\n    \t\t\tprint ""enum objects must be strings:"", theEnumType\n    \t\t\texit(1)\n    elif theType is unicode:\n    \t\tif valType not in valid_value_types:\n    \t\t\t\tprint ""the value type must be one of \'double\', \'string\', \'date\' or an object""\n    \t\t\t\texit(1)\n    else:\n    \tprint ""the type of the field value_type must be a string or a list where as it was"",theType\n    \texit(1)\n\ndef validateAttr(theAttr):\n    if \'name\' not in theAttr or \'value_type\' not in theAttr:\n        print ""couldn\'t find one of (name, value_type) for attr ""\n        pprint(theAttr)\n        exit(1)\n    else:\n    \tvalidateValueType(theAttr[\'value_type\']);\n\ndef validateType(theType):\n    if \'type_id\' not in theType or \'type_name\' not in theType or \'type_attrs\' not in theType:\n        print ""couldn\'t find one of (type_id, type_name, type_attrs) for object""\n        pprint(theType)\n        exit(1)\n    for theAttr in theType[\'type_attrs\']:\n        validateAttr(theAttr)\n\ndef validateNumbering(types):\n    ids = set()\n    for theType in types:\n        if isinstance(theType[\'type_id\'], int):\n        \tif theType[\'type_id\'] in ids:\n        \t\tprint ""found a repeated type_id"", theType[\'type_id\']\n        \t\texit(1)\n        \telse:\n        \t\tids.add(theType[\'type_id\'])\n        else:\n        \tprint ""type_id s must be integers but one was"",""\\"""",theType[\'type_id\'],""\\""""\n        \texit(1)\n\ndef outputDimensionsToFile(file, db):\n\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""SELECT d.dim_id, e.value_name from DIMENSION d, ITEM_ATTR_ENUM e where d.attr_id = e.attr_id and d.value_id = e.value_id and e.value_name != \\\'article\\\'"")\n\t\trows = cur.fetchall()\n\t\tjson.dump(rows, file)\n\ndef readTypes(types):\n    for theType in types:\n        validateType(theType)\n    validateNumbering(types)\n    return types\n\ndef clearUp(db):\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""TRUNCATE TABLE ITEMS"")\n\t\tcur.execute(""TRUNCATE TABLE DIMENSION"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR_ENUM"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_TYPE"")\n\t\tcur.execute(\'truncate table users\')\n\t\tcur.execute(\'truncate table items\')\n\t\tcur.execute(\'truncate table item_map_varchar\')\n\t\tcur.execute(\'truncate table item_map_double\')\n\t\tcur.execute(\'truncate table item_map_datetime\')\n\t\tcur.execute(\'truncate table item_map_int\')\n\t\tcur.execute(\'truncate table item_map_boolean\')\n\t\tcur.execute(\'truncate table item_map_enum\')\n\t\tcur.execute(\'truncate table item_map_text\')\n\nparser = argparse.ArgumentParser(description=""Adds the data schema defined via a file to the DB. Outputs a file (dimensions.json) which contains shortcuts that can be used to narrow the scope of the recommendation output (see API docs for further information)."", prog=\'add_attr_schema.py\')\nparser.add_argument(\'-db-host\', help=\'database host\', required=False, default=""localhost"")\nparser.add_argument(\'-db-user\', help=\'database username\', required=False, default=""root"")\nparser.add_argument(\'-db-pass\', help=\'database password\', required=False, default=""root"")\nparser.add_argument(\'-client\', help=\'client/database name\', required=False, default=""testclient"")\nparser.add_argument(\'-schema-file\', help=""json file with data schema"", required=True)\nparser.add_argument(\'-clean\',action=\'store_true\', help=""instead of adding a schema, clean the current one up (delete the entries in the DB)"", default=False)\n\nopts = vars(parser.parse_args())\n\nif opts[\'clean\']:\n\tprint ""Cleaning the database...""\nelse:\n\n\tjson_data=open(opts[\'schema_file\'])\n\n\tdata = json.load(json_data)\n\tif \'types\' not in data:\n\t    print ""couldn\'t find types object in json""\n\t    exit(1)\n\telse:\n\t    types = readTypes(data[\'types\'])\n\ndb = MySQLdb.connect(user=opts[\'db_user\'],db=opts[\'client\'],passwd=opts[\'db_pass\'], host=opts[\'db_host\'])\nif opts[\'clean\']:\n\tclearUp(db)\n\tprint ""Finished successfully""\nelse:\n\taddToDb(types)\n\tf = open(\'dimensions.json\',\'w\')\n\toutputDimensionsToFile(f,db)\n\n\tprint \'Finished successfully\'\n\n\tjson_data.close()\n\n'"
scripts/add_client.py,0,"b'#!/usr/bin/env python\nimport argparse, os, json\nimport seldon_utils as seldon\nfrom kazoo.client import KazooClient\ndir = os.path.dirname(os.path.abspath(__file__))\nparser = argparse.ArgumentParser(prog=""add_client"",\n\tdescription=""Adds a new client to the Seldon Server"")\nparser.add_argument(""client"",nargs=1,help=""The client to add"")\nparser.add_argument(""--json"",help=""extra JSON configuration for the client"")\nparser.add_argument(""--props"",help=""Relative path to the file with the props"", default=\'../server_config.json\')\nparser.add_argument(""--db"",help=""The name of the DB to use (from the config file), default is to use the first one mentioned."")\nparser.add_argument(""--zookeeper"",help=""Location of zookeeper (hosts)"", default=""localhost"")\n\nargs = parser.parse_args()\nfilename = os.path.join(dir, args.props)\nzk = KazooClient(hosts=args.zookeeper)\nzk.start()\nif not os.path.exists(filename):\n    print ""Properties file doesn\'t exist at"", filename, "", please create it before running this script""\n    exit(1)\nwith open(filename) as data_file:\n    data = json.load(data_file)\nseldon.clientSetup(zk,[{""name"":args.client[0],""db"":args.db}],data[""db""],""/all_clients"")\n\nprint ""Finished successfully""'"
scripts/add_items.py,0,"b'#!/usr/bin/env python\nimport time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nITEM_MAP_VARCHAR_INSERT = ""insert into item_map_varchar (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DOUBLE_INSERT = ""insert into item_map_double (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DATETIME_INSERT = ""insert into item_map_datetime (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_ENUM_INSERT = ""insert into item_map_enum (item_id, attr_id, value_id) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), (select e.value_id from ITEM_ATTR_ENUM e, item_attr a where a.name = %(attr_name)s and e.value_name = %(value)s and a.attr_id = e.attr_id) )""\nITEM_MAP_TEXT_INSERT = ""insert into item_map_text (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_INT_INSERT = ""insert into item_map_int (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_BOOLEAN_INSERT = ""insert into item_map_boolean (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\n\nITEM_INSERT = ""INSERT INTO ITEMS (name, first_op, last_op, client_item_id, type) VALUES (%(name)s, NOW(), NOW(), %(id)s, 1)""\nDB_BATCH_SIZE = 1000\nattr_insert_map = {\n\t\'ENUM\': ITEM_MAP_ENUM_INSERT,\n\t\'BOOLEAN\': ITEM_MAP_BOOLEAN_INSERT,\n\t\'VARCHAR\': ITEM_MAP_VARCHAR_INSERT,\n\t\'TEXT\': ITEM_MAP_TEXT_INSERT,\n\t\'DATETIME\': ITEM_MAP_DATETIME_INSERT,\n\t\'INT\': ITEM_MAP_INT_INSERT,\n\t\'DOUBLE\': ITEM_MAP_DOUBLE_INSERT\n}\n\n\navailable_attrs = dict()\navailable_enums = dict()\n\ndef retrieveDbAttrs(db):\n\tcur = db.cursor()\n\tcur.execute(""SELECT ATTR_ID, NAME, TYPE FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tattrs = dict()\n\tfor row in rows:\n\t\tattrs[row[1]]= (row[0], row[2])\n\n\treturn attrs\n\ndef retrieveDbEnums(db):\n\tcur = db.cursor()\n\t# enum structure:\n\t#    attr_id1:\n\t#\t\t\t\tvalue_name1 : value_id1\n\t#\t\t\t\tvalue_name2 :value_id2\n\tcur.execute(""SELECT ATTR_ID, VALUE_NAME, VALUE_ID FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tenums = defaultdict(dict)\n\tfor row in rows:\n\t\tenums[row[0]][row[1]] = row[2]\n\n\treturn enums\n\ndef validateCSVAgainstDb(csv_file, db):\n\tglobal available_attrs, available_enums\n\tfailed = False\n\tattrs = retrieveDbAttrs(db)\n\tavailable_attrs = attrs\n\tenums = retrieveDbEnums(db)\n\tavailable_enums = enums\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tnoOfFields = 0\n\t\tfor index, line in enumerate(reader, start=1):\n\t\t\tif index is 1:\n\t\t\t\tnoOfFields = len(line)\n\t\t\t\tif not validateFieldsAgainstDbFields(set(line), attrs, db):\n\t\t\t\t\texit(1)\n\t\t\tvalidateLine(line, noOfFields, attrs, enums)\n\t\t\tif len(line) != noOfFields:\n\t\t\t\tfailLine(index, line)\n\t\t\t\tfailed = True\n\n\n\tif failed:\n\t\texit(1)\n\ndef validateLine(line, noOfFields, attrs, enums):\n\tif len(line) != noOfFields:\n\t\tfailLine(index, line)\n\t\tfailed = True\n\telse:\n\t\tfor word in line:\n\t\t\tif str(word) == \'id\':\n\t\t\t\tcontinue\n\t\t\tif str(word) == \'name\':\n\t\t\t\tcontinue\n\t\t\tvalue = line[word]\n\t\t\tif str(attrs[word][1]) == \'ENUM\':\n\t\t\t\tif value not in enums[attrs[word][0]]:\n\t\t\t\t\tprint \'couldn\\\'t find enum value\', value\n\t\t\t\t\texit(1)\n\n\ndef validateFieldsAgainstDbFields(fields,attrs,  db):\n\tfailed = False\n\tfor field in fields:\n\t\tif field not in attrs and field != \'id\' and field != \'name\':\n\t\t\tfailed = True\n\t\t\tprint \'Field \\\'\',field,\'\\\'not an attribute in the DB\'\n\n\treturn not failed\n\ndef doItemInserts(csv_file, db):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n\t\t\tinserts.append({\'name\':name,\'id\':client_id})\n\t\tcur = db.cursor()\n\t\tprint ""inserting items into the db""\n\t\tcur.executemany(ITEM_INSERT, inserts)\n\t\tdb.commit()\n\t\tprint \'finished item inserts\'\n\ndef doAttrInserts(csv_file, db):\n\tinserts = defaultdict(list)\n\tinsertNum = 0\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tfor line in reader:\n\t\t\tfor field_name in line:\n\t\t\t\tif field_name == \'id\' or field_name== \'name\':\n\t\t\t\t\tcontinue\n\t\t\t\tattr_type = available_attrs[str(field_name)][1]\n\t\t\t\tinserts[attr_type].append({\'attr_name\': field_name, \'value\': line[field_name], \'id\': line[\'id\']})\n\t\t\t\tif len(inserts[attr_type]) > DB_BATCH_SIZE:\n\t\t\t\t\tinsertNum+=1\n\t\t\t\t\treallyDoInserts(inserts[attr_type], attr_insert_map[attr_type], insertNum, db)\n\t\t\t\t\tdel inserts[attr_type]\n\tfor index, insert_label in enumerate(inserts, start=1):\n\t\tinsertNum+=1\n\t\treallyDoInserts(inserts[insert_label], attr_insert_map[insert_label], insertNum, db)\n\tdb.commit()\n\tprint \'finished attribute inserts\'\n\ndef reallyDoInserts(params, insertStatement, insertNum, db):\n\t\tcur = db.cursor()\n\t\tprint ""inserting attribute batch"", insertNum,\'into the db\'\n\t\tcur.executemany(insertStatement, params)\n\ndef failLine(lineNum, line):\n\tprint ""line"",lineNum,""failed as it only had"",len(line),""fields""\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table items\')\n\tdbc.execute(\'truncate table item_map_varchar\')\n\tdbc.execute(\'truncate table item_map_double\')\n\tdbc.execute(\'truncate table item_map_datetime\')\n\tdbc.execute(\'truncate table item_map_int\')\n\tdbc.execute(\'truncate table item_map_boolean\')\n\tdbc.execute(\'truncate table item_map_enum\')\n\tdbc.execute(\'truncate table item_map_text\')\n\tdb.commit()\n\nparser = argparse.ArgumentParser(prog=""add_items.py"")\nparser.add_argument(\'-items\', help=\'items csv file\', required=True)\nparser.add_argument(\'-db-host\', help=\'database host\', required=False, default=""localhost"")\nparser.add_argument(\'-db-user\', help=\'database username\', required=False, default=""root"")\nparser.add_argument(\'-db-pass\', help=\'database password\', required=False, default=""root"")\nparser.add_argument(\'-client\', help=\'client/database name\', required=False, default=""testclient"")\n\nopts = vars(parser.parse_args())\n\ndb = MySQLdb.connect(user=opts[\'db_user\'],db=opts[\'client\'],passwd=opts[\'db_pass\'], host=opts[\'db_host\'])\ndb.set_character_set(\'utf8\')\ndbc = db.cursor()\ndbc.execute(\'SET NAMES utf8;\')\ndbc.execute(\'SET CHARACTER SET utf8;\')\ndbc.execute(\'SET character_set_connection=utf8;\')\ndbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\ntry:\n\tvalidateCSVAgainstDb(opts[\'items\'], db)\n\tdoItemInserts(opts[\'items\'], db)\n\tdoAttrInserts(opts[\'items\'],db)\nexcept:\n\tprint \'Unexpected error ...\', sys.exc_info()[0]\n\tprint \'Clearing DB of items and attributes\'\n\ttry:\n\t\tcleanUpDb(db)\n\texcept:\n\t\tprint \'couldn\\\'t clean up db\'\n\traise\nprint ""Successfully ran all inserts""'"
scripts/add_users.py,0,"b'#!/usr/bin/env python\nimport time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nUSER_INSERT = ""insert into users (client_user_id, username, first_op, last_op,type,num_op, active) values (%(id)s, %(name)s, now(), now(), 1,1,1)""\n\n\ndef validateCSV(csv_file):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tline = reader.next()\n\t\tfor field_name in line:\n\t\t\tif not field_name == \'id\' and not field_name == \'username\':\n\t\t\t\tprint \'only id or username fields allowed\'\n\t\t\t\texit(1)\n\ndef doUserInserts(csv_file, db):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tinsertNum = 0\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n\t\t\tinserts.append({\'name\':name,\'id\':client_id})\n\t\t\tif len(inserts) > 1000:\n\t\t\t\tinsertNum+=1\n\t\t\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\t\t\tinserts = []\n\n\t\tinsertNum+=1\n\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\tdb.commit()\n\t\tprint \'finished user inserts\'\n\ndef reallyDoInserts(insertStatement, params, num, db):\n\tcur = db.cursor()\n\tprint ""inserting user batch"", num,\'into the db\'\n\tcur.executemany(insertStatement, params)\n\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table users\')\n\nparser = argparse.ArgumentParser(prog=""add_users.py"")\nparser.add_argument(\'-users\', help=\'user csv file\', required=True)\nparser.add_argument(\'-db-host\', help=\'database host\', required=False, default=""localhost"")\nparser.add_argument(\'-db-user\', help=\'database username\', required=False, default=""root"")\nparser.add_argument(\'-db-pass\', help=\'database password\', required=False, default=""root"")\nparser.add_argument(\'-client\', help=\'client/database name\', required=False, default=""testclient"")\n\nopts = vars(parser.parse_args())\n\ndb = MySQLdb.connect(user=opts[\'db_user\'],db=opts[\'client\'],passwd=opts[\'db_pass\'], host=opts[\'db_host\'])\ndb.set_character_set(\'utf8\')\ndbc = db.cursor()\ndbc.execute(\'SET NAMES utf8;\')\ndbc.execute(\'SET CHARACTER SET utf8;\')\ndbc.execute(\'SET character_set_connection=utf8;\')\ndbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\ntry:\n\tvalidateCSV(opts[\'users\'])\n\tdoUserInserts(opts[\'users\'], db)\nexcept:\n\tprint \'Unexpected error ...\', sys.exc_info()[0]\n\tprint \'Clearing DB of users\'\n\ttry:\n\t\tcleanUpDb(db)\n\texcept:\n\t\tprint \'couldn\\\'t clean up db\'\n\traise\nprint ""Successfully ran all inserts""'"
scripts/create_actions_json.py,0,"b'#!/usr/bin/env python\nimport time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\n\nparser = argparse.ArgumentParser(prog=\'monitorClientsDb.py\')\nparser.add_argument(\'-actions\', help=\'actions csv file\', required=True)\nparser.add_argument(\'-db-host\', help=\'database host\', required=True)\nparser.add_argument(\'-db-user\', help=\'database username\', required=True)\nparser.add_argument(\'-db-pass\', help=\'database password\', required=False)\nparser.add_argument(\'-client\', help=\'client/database name\', required=False)\nparser.add_argument(\'-out\', help=\'json output file\', required=False)\n\nopts = vars(parser.parse_args())\nclient = opts[\'client\']\nif opts[\'db_pass\']:\n    db = MySQLdb.connect(user=opts[\'db_user\'], passwd=opts[\'db_pass\'],db=client, host=opts[\'db_host\'])\nelse:\n    db = MySQLdb.connect(user=opts[\'db_user\'],db=client, host=opts[\'db_host\'])\n\ndef getItemId(db,cache,client_item_id):\n    if client_item_id in cache:\n        return cache[client_item_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select item_id, client_item_id from items"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            itemId = long(row[0])\n            client_item_id_from_db = row[1]\n            cache[client_item_id_from_db] = itemId\n        cursor.close()\n        return cache[client_item_id]\n\ndef getUserId(db,cache,client_user_id):\n    if client_user_id in cache:\n        return cache[client_user_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select user_id,client_user_id from users"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            userId = long(row[0])\n            client_user_id_from_db = row[1]\n            cache[client_user_id_from_db] = userId\n\n        cursor.close()\n        return cache[client_user_id]\n\nuserCache = {}\nitemCache = {}\ncount = 0\nwith open(opts[\'actions\']) as csvfile, open(opts[\'out\'],\'w\') as outfile:\n    reader = unicodecsv.DictReader(csvfile,encoding=\'utf-8\')\n    for f in reader:\n        item = getItemId(db,itemCache,f[""item_id""])\n        user = getUserId(db,userCache,f[""user_id""])\n        action_type = 1\n        action = {}\n        action[""userid""] = int(user)\n        action[""client_userid""] = f[""item_id""]\n        action[""itemid""] = int(item)\n        action[""client_itemid""] = f[""user_id""]\n        action[""value""] = float(f[""value""])\n        utc = datetime.datetime.fromtimestamp(int(f[""time""])).strftime(\'%Y-%m-%dT%H:%M:%SZ\')\n        action[""timestamp_utc""] = utc\n        action[""rectag""] = ""default""\n        action[""type""] = action_type\n        action[""client""] = client\n        s = json.dumps(action,sort_keys=True)\n        outfile.write(s+""\\n"")\n        count += 1\n        if count % 50000 == 0:\n            print ""Processed ""+str(count)+"" actions""\n'"
scripts/initial_setup.py,0,"b'#!/usr/bin/env python\nimport argparse, traceback\nimport os\nfrom kazoo.client import KazooClient\nimport json, random\nimport seldon_utils as seldon\n\ndef lowercase_clients(data):\n    clients = data[\'clients\']\n    for client in clients:\n        client[\'name\'] = client[\'name\'].lower()\n\nrequiredSections = [""db"",""memcached""]\npropertyToZkNode = dict()\npropertyToZkNode[""db""] = ""/config/dbcp""\npropertyToZkNode[""memcached""] = ""/config/memcached""\npropertyToZkNode[""statsd""] = ""/config/statsd""\npropertyToZkNode[""clients""] = ""/all_clients""\n\ndir = os.path.dirname(os.path.abspath(__file__))\nparser = argparse.ArgumentParser(prog=""PROG"",description=""Sets up the Seldon Server"")\nparser.add_argument(""--props"",help=""Relative path to the file with the props"", default=\'../server_config.json\')\nparser.add_argument(""--zookeeper"",help=""Location of zookeeper (hosts)"", default=""localhost"")\nparser.add_argument(""--js-consumer-key"",help=""Client js consumer key else a random one will be used"", default=None)\nparser.add_argument(""--all-consumer-key"",help=""Client all consumer key else a random one will be used"", default=None)\nparser.add_argument(""--all-consumer-secret"",help=""Client all consumer secret else a random one will be used"", default=None)\nargs = parser.parse_args()\n\nconsumer_details={}\nconsumer_details[\'js_consumer_key\']     = args.js_consumer_key      if args.js_consumer_key     != None else None\nconsumer_details[\'all_consumer_key\']    = args.all_consumer_key     if args.all_consumer_key    != None else None\nconsumer_details[\'all_consumer_secret\'] = args.all_consumer_secret  if args.all_consumer_secret != None else None\n\nfilename = os.path.join(dir, args.props)\nif not os.path.exists(filename):\n    print ""Properties file doesn\'t exist at"", filename, "", please create it before running this script""\n    exit(1)\nwith open(filename) as data_file:\n    data = json.load(data_file)\n    lowercase_clients(data)\n\nfor section in requiredSections:\n    if not section in data:\n        print ""Must have section"",section,""in config JSON""\n        exit(1)\n\nzk = KazooClient(hosts=args.zookeeper)\nzk.start()\n\ntry:\n\tseldon.dbSetup(zk,data[""db""],propertyToZkNode[""db""])\n\tseldon.memcachedSetup(zk,data[""memcached""],propertyToZkNode[""memcached""])\n\tseldon.clientSetup(zk,data[""clients""],data[""db""],propertyToZkNode[""clients""], consumer_details)\n\tprint ""Finished succesfully""\nexcept KeyError as e:\n\tprint ""Property missing from config json:"",e\n\ttraceback.print_exc()\n\n'"
scripts/seldon_utils.py,0,"b'from kazoo.client import KazooClient\nimport json, os, random, string\nimport MySQLdb\nimport sys\n\ndef retrieveDbSettings(data):\n\tdbs = {}\n\tfor db in data[""servers""]:\n\t\tdbs[db[""name""]] = {""host"":db[\'host\'], ""port"":db[\'port\'], ""user"":db[\'user\'], ""password"":db[\'password\']}\n\treturn dbs\n\ndef dbSetup(zk,data,zkNode):\n\tdbsProps = retrieveDbSettings(data)\n\tdbs = []\n\tfor dbName in dbsProps:\n\t\tprint ""Setting up DB \\\'""+ dbName+""\\\'""\n\t\tdb = dbsProps[dbName]\n\t\taddApiDb(dbName, db)\n\t\tjdbcString = ""jdbc:mysql:replication://HOST:PORT,HOST:PORT/?characterEncoding=utf8&useServerPrepStmts=true&logger=com.mysql.jdbc.log.StandardLogger&roundRobinLoadBalance=true&transformedBitIsBoolean=true&rewriteBatchedStatements=true""\n\t\tjdbcString = jdbcString.replace(""HOST"", db[""host""]).replace(""PORT"",str(db[""port""]));\n\t\tdel(db[""host""])\n\t\tdel(db[""port""])\n\t\tdb[\'jdbc\'] = jdbcString\n\t\tdb[\'name\'] = dbName\n\t\tdbs.append(db)\n\tdbcpObj = {""dbs"": dbs}\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,json.dumps(dbcpObj))\n\n\ndef memcachedSetup(zk, data, zkNode):\n\tservers = []\n\tprint ""Setting up memcache servers""\n\tfor server in data[""servers""]:\n\t\thost = server[\'host\']\n\t\tport = server[\'port\']\n\t\tserverStr = host+"":""+str(port)\n\t\tservers.append(serverStr)\n        server_list=str("","".join(servers))\n        zkNodeValueBuilder = {}\n        zkNodeValueBuilder[""servers""] = server_list\n        zkNodeValueBuilder[""numClients""] = 1\n        zkNodeValue = json.dumps(zkNodeValueBuilder)\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,zkNodeValue)\n\ndef addClientDb(clientName, dbSettings, consumer_details=None):\n\n\tjs_consumer_key     = consumer_details[\'js_consumer_key\']       if consumer_details != None and consumer_details.has_key(\'js_consumer_key\')     else None\n\tall_consumer_key    = consumer_details[\'all_consumer_key\']      if consumer_details != None and consumer_details.has_key(\'all_consumer_key\')    else None\n\tall_consumer_secret = consumer_details[\'all_consumer_secret\']   if consumer_details != None and consumer_details.has_key(\'all_consumer_secret\') else None\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""../db-schema/mysql/client.sql"")\n\tf = open(filename, \'r\')\n\tquery = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'""+clientName+""\\\'"")\n\tif numrows < 1:\n\t\tcur.execute(""CREATE DATABASE ""+clientName)\n\t\tcur.execute(""USE ""+clientName)\n\t\tcur.execute(query)\n\t\tmore = True\n\t\twhile more:\n\t\t\tmore = cur.nextset()\n\telse:\n\t\tprint(""Client \\\'""+clientName+""\\\' has already been added to the DB"")\n\tcur.execute(""USE API"")\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'js\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key = js_consumer_key if js_consumer_key != None else generateRandomString()\n\t\tprint ""Adding JS consumer key for client \\\'""+clientName +""\\\' : \\\'""+consumer_key+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\', \'\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'js\')"")\n\telse:\n\t\tprint ""JS Consumer key already added for client \\\'""+clientName+""\\\'""\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'all\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key    = all_consumer_key      if all_consumer_key != None     else generateRandomString()\n\t\tconsumer_secret = all_consumer_secret   if all_consumer_secret != None  else generateRandomString()\n\t\tprint ""Adding REST API key for client \\\'""+clientName +""\\\' : consumer_key=\\\'""+consumer_key+""\\\' consumer_secret=\\\'""+consumer_secret+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\',\\\'""+consumer_secret+""\\\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'all\')"")\n\telse:\n\t\tprint ""REST API key already added for client \\\'""+clientName+""\\\'""\n\ndef addApiDb(dbName, dbSettings):\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""../db-schema/mysql/api.sql"")\n\tf = open(filename, \'r\')\n\tquery = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'api\\\'"")\n\tif numrows < 1:\n\t\tprint ""Adding api DB to MySQL DB \\\'""+dbName+""\\\'""\n\t\tcur.execute(query)\n\telse:\n\t\tprint ""API DB has already been added to the MySQL DB \\\'""+dbName+""\\\'""\n\ndef clientSetup(zk, client_data, db_data, zkNode, consumer_details=None):\n\tdbs= retrieveDbSettings(db_data)\n\tfor client in client_data:\n\n\t\tprint ""Adding client \\\'""+client[\'name\']+""\\\'""\n\t\tdbname = client[\'db\']\n\t\tif client[\'db\'] is None:\n\t\t\tdbname = dbs.keys()[0]\n\t\taddClientDb(client[\'name\'],dbs[dbname], consumer_details)\n\t\tclientNode = zkNode + ""/"" + client[\'name\']\n\t\tzk.ensure_path(clientNode)\n\t\tclientNodeValue = {""DB_JNDI_NAME"":dbname}\n\t\tzk.set(clientNode,json.dumps(clientNodeValue))\n\t\tfor setting in client:\n\t\t\tif setting != ""name"" and setting != ""db"":\n\t\t\t\tzk.ensure_path(clientNode + ""/"" + setting)\n\t\t\t\tzk.set(clientNode + ""/"" + setting, str(client[setting]))\n\ndef generateRandomString():\n\treturn \'\'.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(20))\n'"
docker/iago/create_prediction_replay.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nfrom random import randint,random,uniform\nimport json\nimport urllib\n\nPREDICT_TEMPLATE = \'http://%ENDPOINT%/js/predict?json=%JSON%&consumer_key=%KEY%&jsonpCallback=j\'\n\nclass ReplayCreate(object):\n\n    def __init__(self):\n        self.features = []\n\n    def get_key(self,filename):\n        with open(filename) as f:\n            for line in f:\n                line = line.rstrip()\n                j = json.loads(line)\n                self.key = j[0][""key""]\n\n    def parse_features(self,features):\n        for f in features:\n            j = json.loads(f)\n            self.features.append(j)\n\n    def construct_json(self):\n        j = {}\n        for f in self.features:\n            \n            if f[""type""] == ""numeric"":\n                fval = uniform(f[""min""],f[""max""])\n                j[f[""name""]] = fval\n        return json.dumps(j)\n\n    def create_replay(self,endpoint,filename,num):\n        with open(filename,""w"") as f:\n            for i in range (0,num):\n                jStr = self.construct_json()\n                jEncoded = urllib.quote_plus(jStr)\n                url = PREDICT_TEMPLATE.replace(""%ENDPOINT%"",endpoint).replace(""%KEY%"",self.key).replace(""%JSON%"",jEncoded)+""\\n""\n                f.write(url)\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--endpoint\', help=\'endpoint for seldon server\', default=""seldon-server"")\n    parser.add_argument(\'--key\', help=\'file containing output of seldon-cli keys call\', required=True)\n    parser.add_argument(\'--replay\', help=\'replay file to create\', required=True)\n    parser.add_argument(\'--num\', help=\'number of actions and recommendation pair calls to create\', required=False, type=int, default=1000)\n    parser.add_argument(\'--feature\', help=\'feature to add \', required=True, action=\'append\')\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    rc = ReplayCreate()\n    rc.get_key(args.key)\n    rc.parse_features(args.feature)\n    rc.create_replay(args.endpoint,args.replay,args.num)\n'"
docker/iago/create_recommendation_replay.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nfrom random import randint,random\nimport json\n\nRECOMMENDATION_TEMPLATE = \'http://%ENDPOINT%/js/recommendations?consumer_key=%KEY%&user=%USER%&item=%ITEM%&limit=10&jsonpCallback=j\'\nACTION_TEMPLATE = \'http://%ENDPOINT%/js/action/new?consumer_key=%KEY%&user=%USER%&item=%ITEM%&type=1&jsonpCallback=j\'\nACTION_CLICK_TEMPLATE = \'http://%ENDPOINT%/js/action/new?consumer_key=%KEY%&user=%USER%&item=%ITEM%&type=1&rlabs=1&jsonpCallback=j\'\n\nclass ReplayCreate(object):\n\n    def __init__(self,click_percent=0.1):\n        self.click_percent=click_percent\n\n    def get_key(self,filename):\n        with open(filename) as f:\n            for line in f:\n                line = line.rstrip()\n                j = json.loads(line)\n                self.key = j[0][""key""]\n\n    def get_items(self,filename,item_type):\n        self.items = {}\n        i = 0\n        with open(filename) as f:\n            for line in f:\n                line = line.rstrip()\n                j = json.loads(line)\n                items = j[""list""]\n                for item in items:\n                    idx = item[\'id\']\n                    itype = int(item[\'type\'])\n                    if itype == item_type:\n                        self.items[i] = idx\n                        i += 1\n\n\n    def create_replay(self,endpoint,filename,num_actions,num_users):\n        with open(filename,""w"") as f:\n            for i in range (0,num_actions):\n                user = str(randint(1,num_users))\n                item_idx = randint(0,len(self.items)-1)\n                item = self.items[item_idx]\n                url = RECOMMENDATION_TEMPLATE.replace(""%ENDPOINT%"",endpoint).replace(""%KEY%"",self.key).replace(""%USER%"",user).replace(""%ITEM%"",item)+""\\n""\n                f.write(url)\n                p = random()\n                if p < self.click_percent:\n                    url = ACTION_CLICK_TEMPLATE\n                else:\n                    url = ACTION_TEMPLATE\n                url = url.replace(""%ENDPOINT%"",endpoint).replace(""%KEY%"",self.key).replace(""%USER%"",user).replace(""%ITEM%"",item)+""\\n""        \n                f.write(url)\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--endpoint\', help=\'endpoint for seldon server\', default=""seldon-server"")\n    parser.add_argument(\'--key\', help=\'file containing output of seldon-cli keys call\', required=True)\n    parser.add_argument(\'--items\', help=\'file containing output of seldon-cli api /items call\', required=True)\n    parser.add_argument(\'--item-type\', help=\'limit to a certain iytem type\', type=int, default=1)\n    parser.add_argument(\'--replay\', help=\'replay file to create\', required=True)\n    parser.add_argument(\'--num-actions\', help=\'number of actions and recommendation pair calls to create\', required=False, type=int, default=1000)\n    parser.add_argument(\'--num-users\', help=\'number of users to create recommendation calls for\', required=False, type=int, default=10000)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    rc = ReplayCreate()\n    rc.get_key(args.key)\n    rc.get_items(args.items,args.item_type)\n    rc.create_replay(args.endpoint,args.replay,args.num_actions,args.num_users)\n'"
python/docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# seldon documentation build configuration file, created by\n# sphinx-quickstart on Fri Sep  4 15:20:44 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nimport shlex\nimport sys, os\nimport sphinx_rtd_theme\n\nsys.path.append(os.path.abspath(\'exts\'))\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'../seldon\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.todo\',\n    \'ghpages\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'seldon\'\ncopyright = u\'2015, Seldon\'\nauthor = u\'Seldon\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'2.2.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'2.2.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = \'en\'\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'_build\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = ""sphinx_rtd_theme""\n#html_theme = \'alabaster\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'seldondoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (master_doc, \'seldon.tex\', u\'seldon Documentation\',\n   u\'Clive Cox\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'seldon\', u\'seldon Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (master_doc, \'seldon\', u\'seldon Documentation\',\n   author, \'seldon\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The basename for the epub file. It defaults to the project name.\n#epub_basename = project\n\n# The HTML theme for the epub output. Since the default themes are not optimized\n# for small screen space, using the same theme for HTML and epub output is\n# usually not wise. This defaults to \'epub\', a theme designed to save visual\n# space.\n#epub_theme = \'epub\'\n\n# The language of the text. It defaults to the language option\n# or \'en\' if the language is not set.\n#epub_language = \'\'\n\n# The scheme of the identifier. Typical schemes are ISBN or URL.\n#epub_scheme = \'\'\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#epub_identifier = \'\'\n\n# A unique identification for the text.\n#epub_uid = \'\'\n\n# A tuple containing the cover image and cover page html template filenames.\n#epub_cover = ()\n\n# A sequence of (type, uri, title) tuples for the guide element of content.opf.\n#epub_guide = ()\n\n# HTML files that should be inserted before the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_pre_files = []\n\n# HTML files shat should be inserted after the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n#epub_post_files = []\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# The depth of the table of contents in toc.ncx.\n#epub_tocdepth = 3\n\n# Allow duplicate toc entries.\n#epub_tocdup = True\n\n# Choose between \'default\' and \'includehidden\'.\n#epub_tocscope = \'default\'\n\n# Fix unsupported image types using the Pillow.\n#epub_fix_images = False\n\n# Scale large images.\n#epub_max_image_width = 0\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#epub_show_urls = \'inline\'\n\n# If false, no index is generated.\n#epub_use_index = True\n'"
python/examples/auto_transform.py,0,"b'import seldon.pipeline.auto_transforms as auto\nimport pandas as pd\n\ndf = pd.DataFrame([{""a"":10,""b"":1,""c"":""cat""},{""a"":5,""b"":2,""c"":""dog"",""d"":""Nov 13 08:36:29 2015""},{""a"":10,""b"":3,""d"":""Oct 13 10:50:12 2015""}])\nt = auto.Auto_transform(max_values_numeric_categorical=2,date_cols=[""d""])\nt.fit(df)\ndf2 = t.transform(df)\nprint df2\n'"
python/examples/sklearn_scaler.py,0,"b'import seldon.pipeline.sklearn_transform as ssk\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndf = pd.DataFrame.from_dict([{""a"":1.0,""b"":2.0},{""a"":2.0,""b"":3.0}])\nt = ssk.sklearn_transform(input_features=[""a""],output_features=[""a_scaled""],transformer=StandardScaler())\nt.fit(df)\ndf_2 = t.transform(df)\nprint df_2\n'"
python/seldon/__init__.py,0,"b""import logging\n\nfrom .util import RecommenderWrapper,Recommender,Extension,ExtensionWrapper,Recommender_wrapper,Extension_wrapper\n\n__version__ = '2.2.6'\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\nlogger = logging.getLogger(__name__)\nif len(logger.handlers) == 0:# To ensure reload() doesn't add another one\n    logger.addHandler(NullHandler())\n"""
python/seldon/anomaly_wrapper.py,0,"b'from sklearn.feature_extraction import DictVectorizer\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom collections import OrderedDict\nimport io\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator,ClassifierMixin\nimport pandas as pd\n\nclass AnomalyWrapper(BasePandasEstimator,BaseEstimator,ClassifierMixin):\n\n    """"""\n    Wrapper for XGBoost classifier with pandas support\n    XGBoost specific arguments follow https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py\n\n    clf : sklearn estimator\n       sklearn estimator to run\n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    sk_args : str, optional\n       extra args for sklearn classifier\n    """"""\n    def __init__(self, clf=None,target=None, target_readable=None,included=None,excluded=None,id_map={0:\'Anomaly_score\',1:\'Complementary_score\'},vectorizer=None,**sk_args):\n        super(AnomalyWrapper, self).__init__(target,target_readable,included,excluded,id_map)\n        self.vectorizer = vectorizer\n        self.clf = clf\n        self.sk_args = sk_args\n\n    def fit(self,X,y=None):\n        """"""\n        Fit an sklearn classifier to data\n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,y,self.vectorizer) = self.convert_numpy(df)\n        else:\n            check_X_y(X,y)\n\n        self.clf.fit(X,y)\n        return self\n\n    def fit_transform(self,X,y=None):\n        """"""\n        Fit an sklearn classifier to data\n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,y,self.vectorizer) = self.convert_numpy(df)\n        else:\n            check_X_y(X,y)\n\n        self.clf.fit(X,y)\n        return self\n\n    \n    def predict_proba(self,X):\n        """"""\n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        return self.clf.get_score(X)\n\n\n    def predict(self,X):\n        """"""\n        Returns class predictions\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class predictions\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        return self.clf.get_score(X)\n'"
python/seldon/fileutil.py,0,"b'__authour__ = \'Clive Cox\'\nimport sys\nimport zlib\nimport boto\nfrom boto.s3.connection import S3Connection\nfrom boto.s3.key import Key\nimport glob\nfrom shutil import copyfile\nimport os\nimport math\nfrom filechunkio import FileChunkIO\nimport logging\nimport smart_open\n\nlogger = logging.getLogger(__name__)\n\nclass FileUtil:\n    """"""utilities to input and output files. Locally or from AWS S3.\n\n    Args:\n        key [Optional(str)]: aws key\n\n        secret [Optional(str)]: aws secret\n    """"""\n    def __init__(self, aws_key = None, aws_secret = None):\n        self.aws_key = aws_key\n        self.aws_secret = aws_secret\n\n\n    #\n    # Streaming\n    #\n\n    def stream_decompress(self,stream):\n        """"""decompress a stream\n        """"""\n        dec = zlib.decompressobj(16+zlib.MAX_WBITS)  # same as gzip module\n        for chunk in stream:\n            rv = dec.decompress(chunk)\n            if rv:\n                yield rv\n\n    def stream_gzip(self,k,fn):\n        """"""stream from gzip file and call function for each line\n        \n        Args:\n            k (file): input gziped file\n            fn (function): function to call for each line\n        """"""\n        unfinished = """"\n        for data in self.stream_decompress(k):\n            data = unfinished + data\n            lines = data.split(""\\n"");\n            unfinished = lines.pop()\n            for line in lines:\n                fn(line)\n\n    def stream_other(self,path,fn):\n        """"""stream from local folders call a function\n\n        Args:\n            folders (list): list of folders\n            fn (function): function to call\n        """"""\n        parsed_uri = smart_open.ParseUri(path)\n        if parsed_uri.scheme in (""file"", ) and os.path.isdir(path): # stream each file from directory\n            for f in glob.glob(path+""/*""):\n                for line in smart_open.smart_open(f):\n                    fn(line)\n        else: #let smart_open handle streaming of file from location \n            for line in smart_open.smart_open(path):\n                fn(line)\n\n    def stream_s3(self,bucket,prefix,fn):\n        """"""stream from an AWS S3 bucket all files under a prefix and call a function\n\n        Args:\n            bucket (str): name of S3 bucket\n            prefix (str): prefix in bucket\n            fn (function): function to call for each line\n        """"""\n        if self.aws_key:\n            self.conn = boto.connect_s3(self.aws_key,self.aws_secret)\n        else:\n            self.conn = boto.connect_s3()\n        b = self.conn.get_bucket(bucket)\n        for k in b.list(prefix=prefix):\n            if k.name.endswith("".gz""): #smart_open can\'t currently handle gzip on s3...\n                self.stream_gzip(k,fn)\n            else:\n                for line in smart_open.smart_open(k):\n                    fn(line)\n\n    def stream_multi(self,inputPaths,fn):\n        """""" stream multilple paths calling a function on each line\n\n        Args:\n            inputPaths (list): list of input folders\n            fn (function): function to call\n        """"""\n        for path in inputPaths:\n            self.stream(path,fn)\n\n    def stream(self,inputPath,fn):\n        """"""stream from an inputpath calling function\n\n        Args:\n            inputPath (str): input path to stream from\n            fn (function): function to call\n        """"""\n        if inputPath.startswith(""s3n://""):\n            isS3 = True\n            inputPath = inputPath[6:]\n        elif inputPath.startswith(""s3://""):\n            isS3 = True\n            inputPath = inputPath[5:]\n        else:\n            isS3 = False\n        if isS3:\n            logger.info(""AWS S3 input path %s"",inputPath)\n            parts = inputPath.split(\'/\')\n            bucket = parts[0]\n            prefix = inputPath[len(bucket)+1:]\n            self.stream_s3(bucket,prefix,fn)\n        else:\n            self.stream_other(inputPath,fn)\n\n\n    #\n    # Copying\n    #\n\n    def copy_local(self,fromPath,toPath):\n        """"""copy local folders\n\n        Args:\n            fromPath (str): local from path to copy all files under\n            toPath (str): local destination folder (will be created if does not exist)\n        """"""\n        logger.info(""copy %s to %s"",fromPath,toPath)\n        if os.path.isfile(fromPath):\n            dir = os.path.dirname(toPath)\n            if len(dir) > 0 and not os.path.exists(dir):\n                os.makedirs(dir)\n            copyfile(fromPath,toPath)\n        elif os.path.isdir(fromPath):\n            if not os.path.exists(toPath):\n                os.makedirs(toPath)\n            for f in glob.glob(fromPath+""/*""):\n                basename = os.path.basename(f)\n                fnew = toPath+""/""+basename\n                logger.info(""copying %s to %s"",f,fnew)\n                copyfile(f,fnew)\n\n            \n    def copy_s3_file(self,fromPath,bucket,path):\n        """"""copy from local file to S3 \n\n        Args:\n            fromPath (str): local file\n            bucket (str): S3 bucket\n            path (str): S3 prefix to add to files\n        """"""\n        if self.aws_key:\n            self.conn = boto.connect_s3(self.aws_key,self.aws_secret)\n        else:\n            self.conn = boto.connect_s3()\n        b = self.conn.get_bucket(bucket)\n        source_size = os.stat(fromPath).st_size\n        # Create a multipart upload request\n        uploadPath = path\n        logger.info(""uploading to bucket %s path %s"",bucket,uploadPath)\n        mp = b.initiate_multipart_upload(uploadPath)\n        chunk_size = 10485760\n        chunk_count = int(math.ceil(source_size / float(chunk_size)))\n        for i in range(chunk_count):\n            offset = chunk_size * i\n            bytes = min(chunk_size, source_size - offset)\n            with FileChunkIO(fromPath, \'r\', offset=offset,bytes=bytes) as fp:\n                logger.info(""uploading to s3 chunk %d/%d"",(i+1),chunk_count)\n                mp.upload_part_from_file(fp, part_num=i + 1)\n        # Finish the upload\n        logger.info(""completing transfer to s3"")\n        mp.complete_upload()\n\n    \n\n    def upload_s3(self,fromPath,toPath):\n        """"""upload from local path to S3\n\n        Args:\n            fromPath (str): folder to copy from\n            toPath (str): S3 URL\n        """"""\n        if toPath.startswith(""s3n://""):\n            noSchemePath = toPath[6:]\n        elif toPath.startswith(""s3://""):\n            noSchemePath = toPath[5:]\n        parts = noSchemePath.split(\'/\')\n        bucket = parts[0]\n        opath = noSchemePath[len(bucket)+1:]\n        if os.path.isfile(fromPath):\n            self.copy_s3_file(fromPath,bucket,opath)\n        elif os.path.isdir(fromPath):\n            for f in glob.glob(fromPath+""/*""):\n                basename = os.path.basename(f)\n                fnew = opath+""/""+basename\n                logger.info(""copying %s to %s"",f,fnew)\n                self.copy_s3_file(f,bucket,fnew)\n\n    def download_s3(self,fromPath,toPath):\n        """"""download from S3 to local folder\n\n        Args:\n            fromPath (str): S3 URL\n            toPath (str): local folder\n        """"""\n        if fromPath.startswith(""s3n://""):\n            noSchemePath = fromPath[6:]\n        elif fromPath.startswith(""s3://""):\n            noSchemePath = fromPath[5:]\n        parts = noSchemePath.split(\'/\')\n        bucket = parts[0]\n        s3path = noSchemePath[len(bucket)+1:]\n        if self.aws_key:\n            self.conn = boto.connect_s3(self.aws_key,self.aws_secret)\n        else:\n            self.conn = boto.connect_s3()\n        b = self.conn.get_bucket(bucket)\n        for k in b.list(prefix=s3path):\n            if not k.name.endswith(""/""):\n                basename = os.path.basename(k.name)\n                fnew = toPath+""/""+basename\n                logger.info(""copying %s to %s"",k.name,fnew)\n                k.get_contents_to_filename(fnew)\n\n\n    def copy(self,fromPath,toPath):\n        """"""copy files. local->local, S3->local, local->S3 (S3->S3 not supported)\n        \n        Args:\n            fromPath (str): local or S3 URL\n            toPath (str): local or S3 URL\n        """"""\n        if fromPath.startswith(""s3n://"") or fromPath.startswith(""s3://""):\n            fromS3 = True\n        else:\n            fromS3 = False\n        if toPath.startswith(""s3n://"") or toPath.startswith(""s3://""):\n            toS3 = True\n        else:\n            toS3 = False\n        if not fromS3 and not toS3:\n            self.copy_local(fromPath,toPath)\n        elif not fromS3 and toS3:\n            self.upload_s3(fromPath,toPath)\n        elif fromS3 and not toS3:\n            if os.path.isdir(toPath):\n                self.download_s3(fromPath,toPath)\n            else:\n                logger.error(""Local destination folder must exist :%s"",toPath)\n        else:\n            logger.warn(""can\'t copy from s3 to s3"")\n            \n\n        \n'"
python/seldon/keras.py,0,"b'from __future__ import absolute_import\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.models import model_from_json\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom sklearn.base import BaseEstimator,ClassifierMixin\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import model_from_json\nimport copy\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef default_classification_model(input_width,num_classes):\n    """"""Default classification model\n    """"""\n    model = Sequential()                         \n    logger.info(""input width=%d"",input_width)\n    model.add(Dense(5, init=\'uniform\',input_dim=input_width))\n    model.add(Activation(\'tanh\'))\n\n    model.add(Dense(num_classes))\n    model.add(Activation(\'softmax\'))\n\n    return model\n\n\nclass KerasClassifier(BasePandasEstimator,BaseEstimator,ClassifierMixin):\n    def __init__(self,model_create=default_classification_model,tmp_model=""/tmp/model"",target=None, target_readable=None,included=None,excluded=None,id_map={},optimizer=\'adam\', loss=\'categorical_crossentropy\', train_batch_size=128, test_batch_size=128, nb_epoch=100, shuffle=True, validation_split=0, validation_data=None, callbacks=None,verbose=0):\n        """"""\n        Wrapper for keras with pandas support\n        Derived from https://github.com/fchollet/keras/blob/master/keras/wrappers/scikit_learn.py\n    \n        Parameters\n        ----------\n           \n        target : str\n           Target column\n        target_readable : str\n           More descriptive version of target variable\n        included : list str, optional\n           columns to include\n        excluded : list str, optional\n           columns to exclude\n        id_map : dict (int,str), optional\n           map of class ids to high level names\n        optimizer : str, optional\n           Optimizer to use in training\n        loss : str, optional\n           loss to appy\n        train_batch_size : int, optional\n           Number of training samples evaluated at a time.\n        test_batch_size : int, optional\n           Number of test samples evaluated at a time.\n        nb_epochs : int, optional\n           Number of training epochs.\n        shuffle : boolean, optional\n           Wheter to shuffle the samples at each epoch.\n        validation_split : float [0, 1], optional\n           Fraction of the data to use as held-out validation data.\n        validation_data : tuple (X, y), optional\n           Data to be used as held-out validation data. Will override validation_split.\n        callbacks : list, optional\n           List of callbacks to apply during training.\n        verbose : int, optional\n           Verbosity level.\n    """"""\n        super(KerasClassifier, self).__init__(target,target_readable,included,excluded,id_map)\n        self.target = target\n        self.target_readable = target_readable\n        self.id_map=id_map\n        self.included = included\n        self.excluded = excluded\n        if not self.target_readable is None:\n            if self.excluded is None:\n                self.excluded = [self.target_readable]\n            else:\n                self.excluded.append(self.target_readable)\n        self.vectorizer=None\n        self.model_create=model_create\n        self.optimizer=optimizer \n        self.loss=loss \n        self.train_batch_size=train_batch_size \n        self.test_batch_size=test_batch_size \n        self.nb_epoch=nb_epoch\n        self.shuffle=shuffle\n        self.validation_split=validation_split\n        self.validation_data=validation_data\n        self.callbacks = [] if callbacks is None else callbacks\n        self.verbose=verbose\n        self.tmp_model=tmp_model\n        self.compiled_model_ = None\n\n    def __getstate__(self):\n        """"""Remove parts of class that cause issue in pickling. Can recreate them in setstate\n        """"""\n        result = self.__dict__.copy()\n        del result[\'compiled_model_\']\n        del result[\'model_create\']\n        return result\n\n    def __setstate__(self, dict):\n        """"""Create compiled model from parameters. saving model to file and loading it back in\n        """"""\n        self.__dict__ = dict\n        self.model_create=None\n        with open(self.tmp_model, mode=\'wb\') as modelfile: # b is important -> binary\n            modelfile.write(self.model_saved)\n        self.compiled_model_ = model_from_json(self.config_)\n        self.compiled_model_.load_weights(self.tmp_model)\n        self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n\n    def fit(self,X,y=None):\n        """"""Derived from https://github.com/fchollet/keras/blob/master/keras/wrappers/scikit_learn.py\n        Adds:\n        Handling pandas inputs\n        Saving of model into the class to allow for easy pickling\n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,y,self.vectorizer) = self.convert_numpy(df)\n        else:\n            check_X_y(X,y)\n\n        input_width = X.shape[1]\n        num_classes = len(y.unique())\n        logger.info(""input_width %d"",input_width)\n        logger.info(""num_classes %d"",num_classes)\n        train_y = np_utils.to_categorical(y, num_classes)\n        self.model = self.model_create(input_width,num_classes)\n\n        if len(y.shape) == 1:\n            self.classes_ = list(np.unique(y))\n            if self.loss == \'categorical_crossentropy\':\n                y = to_categorical(y)\n        else:\n            self.classes_ = np.arange(0, y.shape[1])\n        \n        if self.compiled_model_ is None:\n            self.compiled_model_ = copy.deepcopy(self.model)\n            self.compiled_model_.compile(optimizer=self.optimizer, loss=self.loss)\n        history = self.compiled_model_.fit(\n            X, y, batch_size=self.train_batch_size, nb_epoch=self.nb_epoch, verbose=self.verbose,\n            shuffle=self.shuffle,\n            validation_split=self.validation_split, validation_data=self.validation_data,\n            callbacks=self.callbacks)\n\n        self.config_ = self.model.to_json()\n        self.compiled_model_.save_weights(self.tmp_model)\n        with open(self.tmp_model, mode=\'rb\') as file: # b is important -> binary\n            self.model_saved = file.read()\n        return self\n\n    def predict_proba(self,X):\n        """"""\n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n        return self.compiled_model_.predict_proba(X, batch_size=self.test_batch_size, verbose=self.verbose)\n\n'"
python/seldon/sklearn_estimator.py,0,"b'from sklearn.feature_extraction import DictVectorizer\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom collections import OrderedDict\nimport io\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator,ClassifierMixin\nimport pandas as pd\n\nclass SKLearnClassifier(BasePandasEstimator,BaseEstimator,ClassifierMixin):\n\n    """"""\n    Wrapper for XGBoost classifier with pandas support\n    XGBoost specific arguments follow https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py\n\n    clf : sklearn estimator\n       sklearn estimator to run\n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    sk_args : str, optional\n       extra args for sklearn classifier\n    """"""\n    def __init__(self, clf=None,target=None, target_readable=None,included=None,excluded=None,id_map={},vectorizer=None,**sk_args):\n        super(SKLearnClassifier, self).__init__(target,target_readable,included,excluded,id_map)\n        self.vectorizer = vectorizer\n        self.clf = clf\n        self.sk_args = sk_args\n\n    def fit(self,X,y=None):\n        """"""\n        Fit an sklearn classifier to data\n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,y,self.vectorizer) = self.convert_numpy(df)\n        else:\n            check_X_y(X,y)\n\n        self.clf.fit(X,y)\n        return self\n\n    def predict_proba(self,X):\n        """"""\n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        return self.clf.predict_proba(X)\n\n\n    def predict(self,X):\n        """"""\n        Returns class predictions\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class predictions\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        return self.clf.predict(X)\n'"
python/seldon/tensorflow_wrapper.py,6,"b'from seldon.pipeline.pandas_pipelines import BasePandasEstimator\nfrom sklearn.base import BaseEstimator\nimport pandas as pd\nfrom sklearn.utils import check_array\nimport tensorflow as tf\n\nclass TensorFlowWrapper(BasePandasEstimator,BaseEstimator):\n    """"""\n    Wrapper for tensorflow with pandas support\n\n    Parameters\n    ----------\n    \n    session : tensorflow session\n        Contains the tensorflow model\n    tf_input : tensorflow variable\n        Variable used as the model input\n    tf_output : tensorflow variable\n        Variable used as the model output\n    tf_constant : iterable\n        List of tuples (tensorflow variable, value) to be used as constant for the model when doing predictions\n    tmp_model : string\n        url to folder where the model will be temporarily saved\n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    """"""\n    def __init__(self,\n                 session,\n                 tf_input,\n                 tf_output,\n                 tf_constants=(),\n                 tmp_model=""/tmp"",\n                 target=None,\n                 target_readable=None,\n                 included=None,\n                 excluded=None,\n                 id_map={}):\n        super(TensorFlowWrapper, self).__init__(target,target_readable,included,excluded,id_map)\n        self.target = target\n        self.target_readable = target_readable\n        self.id_map=id_map\n        self.included = included\n        self.excluded = excluded\n        if not self.target_readable is None:\n            if self.excluded is None:\n                self.excluded = [self.target_readable]\n            else:\n                self.excluded.append(self.target_readable)\n        self.tmp_model = tmp_model\n        self.sess = session\n        self.tf_input = tf_input\n        self.tf_output = tf_output\n        self.tf_constants_vars = [(\'constant_%s\'%i,var) for i,(var,value) in enumerate(tf_constants)]\n        self.tf_constants_values = [(\'constant_%s\'%i,value) for i,(var,value) in enumerate(tf_constants)]\n        self.vectorizer = None\n        \n    def __getstate__(self):\n        result = self.__dict__.copy()\n        del result[\'sess\']\n        del result[\'tf_input\']\n        del result[\'tf_output\']\n        del result[\'tf_constants_vars\']\n        self.save(self.tmp_model)\n        with open(\'%s/tensorflow-model.meta\'%self.tmp_model, mode=\'rb\') as metafile:\n            result[\'_meta\'] = metafile.read()\n        with open(\'%s/tensorflow-model\'%self.tmp_model, mode=\'rb\') as modelfile:\n            result[\'_model\'] = modelfile.read()\n        return result\n    \n    def __setstate__(self,dict):\n        self.__dict__ = dict\n        with open(\'%s/tensorflow-model.meta\'%self.tmp_model, mode=\'wb\') as metafile:\n            metafile.write(dict[\'_meta\'])\n        with open(\'%s/tensorflow-model\'%self.tmp_model, mode=\'wb\') as modelfile:\n            modelfile.write(dict[\'_model\'])\n        del self.__dict__[\'_meta\']\n        del self.__dict__[\'_model\']\n        self.load(self.tmp_model)\n        \n    def save(self,folder):\n        """"""Exports tensorflow model\n        """"""\n        tf.add_to_collection(\'tf_input\',self.tf_input)\n        tf.add_to_collection(\'tf_output\',self.tf_output)\n        for var_name,var in self.tf_constants_vars:\n            tf.add_to_collection(var_name,var)\n        saver = tf.train.Saver()\n        saver.save(self.sess,\'%s/tensorflow-model\'%folder)\n        \n    def load(self,folder):\n        self.sess = tf.Session()\n        new_saver = tf.train.import_meta_graph(\'%s/tensorflow-model.meta\'%folder)\n        new_saver.restore(self.sess,\'%s/tensorflow-model\'%folder)\n        self.tf_input = self.sess.graph.get_collection(\'tf_input\')[0]\n        self.tf_output = self.sess.graph.get_collection(\'tf_output\')[0]\n        self.tf_constants_vars = []\n        for var_name,value in self.tf_constants_values:\n            self.tf_constants_vars.append((var_name,self.sess.graph.get_collection(var_name)[0]))\n\n    def fit(self,X,y=None):\n        pass\n        \n    def predict_proba(self,X):\n        """"""\n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n\n            (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        all_inputs = [(self.tf_input,X)] + [(var,value) for (_,var),(_,value) in zip(self.tf_constants_vars,self.tf_constants_values)]\n\n        return self.sess.run(self.tf_output,feed_dict={var:value for var,value in all_inputs})\n\n\n\n'"
python/seldon/util.py,0,"b'import seldon.fileutil as fu\nimport os.path\nimport logging\nimport shutil \nfrom sklearn.externals import joblib\nimport logging\nimport random\nfrom sklearn.base import BaseEstimator\n\nlogger = logging.getLogger(__name__)\n\n\nclass DeprecationHelper(object):\n    def __init__(self, new_target):\n        self.new_target = new_target\n\n    def _warn(self):\n        from warnings import warn\n        warn(""This class is deprecated. Please use {} instead"".format(str(self.new_target)))\n\n    def __call__(self, *args, **kwargs):\n        self._warn()\n        return self.new_target(*args,**kwargs)\n\n    def __getattr__(self,attr):\n        self._warn()\n        return getattr(self.new_target, attr)\n\n\nclass Recommender(BaseEstimator):\n    """"""\n    General recommendation interface\n    """"""\n\n    def recommend(self,user,ids,recent_interactions,client,limit):\n        """"""\n        Recommend items\n\n        Parameters\n        ----------\n\n        user : long\n           user id\n        ids : list(long)\n           item ids to score\n        recent_interactions : list(long)\n           recent items the user has interacted with\n        client : str\n           name of client to recommend for (business group, company, product..)\n        limit : int\n           number of recommendations to return\n\n\n        Returns\n        -------\n        list of pairs of (item_id,score)\n        """"""\n        return []\n\n    def save(self,folder):\n        """"""\n        Save the recommender model. Allows more fine grained control over model state saving than pickling would allow. The method should save objects that only can\'t be pickled.\n\n        Parameters\n        ----------\n        \n        folder : str\n           local folder to save model\n        """"""\n        pass\n\n    def load(self,folder):\n        """"""\n        Load the model into the recommender. Allows more complex models than can easily handled via pickling.\n\n        Parameters\n        ----------\n\n        folder : str\n           local folder to load model\n        """"""\n        return self\n\nclass RecommenderWrapper(object):\n    """"""\n    Wrapper to allow recommenders to be easily saved and loaded\n    """"""\n    def __init__(self,work_folder=""/tmp"",aws_key=None,aws_secret=None):\n        self.work_folder=work_folder\n        self.aws_key=aws_key\n        self.aws_secret=aws_secret\n\n    def get_work_folder(self):\n        return self.work_folder\n\n    def create_work_folder(self):\n        if not os.path.exists(self.work_folder):\n            logger.info(""creating %s"",self.work_folder)\n            os.makedirs(self.work_folder)\n\n    def save_recommender(self,recommender,location):\n        """"""\n        Save recommender to external location\n\n        Parameters\n        ----------\n\n        recommender : Recommender \n           recommender to be saved\n        location : str\n           external folder to save recommender\n        """"""\n        self.create_work_folder()\n        rint = random.randint(1,999999)\n        recommender_folder = self.work_folder+""/recommender_tmp""+str(rint)\n        if not os.path.exists(recommender_folder):\n            logger.info(""creating folder %s"",recommender_folder)\n            os.makedirs(recommender_folder)\n        tmp_file = recommender_folder+""/rec""\n        joblib.dump(recommender,tmp_file)\n        recommender.save(recommender_folder)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(recommender_folder,location)\n\n\n    def load_recommender(self,recommender_folder):\n        """"""\n        Load scikit learn recommender from external folder\n        \n        Parameters\n        ----------\n\n        recommender_folder : str\n           external folder holding recommender\n        """"""\n        self.create_work_folder()\n        rint = random.randint(1,999999)\n        local_recommender_folder = self.work_folder+""/recommender_tmp""+str(rint)\n        if not os.path.exists(local_recommender_folder):\n            logger.info(""creating folder %s"",local_recommender_folder)\n            os.makedirs(local_recommender_folder)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(recommender_folder,local_recommender_folder)\n        recommender =  joblib.load(local_recommender_folder+""/rec"")\n        recommender.load(local_recommender_folder)\n        return recommender\n\nRecommender_wrapper = DeprecationHelper(RecommenderWrapper)\n\nclass Extension(object):\n\n    """"""\n    Generic function that takes dict input and return JSON\n    """"""\n    def predict(self,input={}):\n        return {}\n\n\n    def save(self,folder):\n        """"""\n        Save the extension model. Allows more fine grained control over model state saving than pickling would allow. The method should save objects that only can\'t be pickled.\n\n        Parameters\n        ----------\n        \n        folder : str\n           local folder to save model\n        """"""\n        pass\n\n    def load(self,folder):\n        """"""\n        Load the model into the extension. Allows more complex models than can easily handled via pickling.\n\n        Parameters\n        ----------\n\n        folder : str\n           local folder to load model\n        """"""\n        return self\n\n\nclass ExtensionWrapper(object):\n\n    def __init__(self,work_folder=""/tmp"",aws_key=None,aws_secret=None):\n        self.work_folder=work_folder\n        self.aws_key=aws_key\n        self.aws_secret=aws_secret\n\n    def get_work_folder(self):\n        return self.work_folder\n\n    def create_work_folder(self):\n        if not os.path.exists(self.work_folder):\n            logger.info(""creating %s"",self.work_folder)\n            os.makedirs(self.work_folder)\n\n    def load_extension(self,extension_folder):\n        self.create_work_folder()\n        rint = random.randint(1,999999)\n        local_extension_folder = self.work_folder+""/extension_tmp""+str(rint)\n        if not os.path.exists(local_extension_folder):\n            logger.info(""creating folder %s"",local_extension_folder)\n            os.makedirs(local_extension_folder)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(extension_folder,local_extension_folder)\n        extension =  joblib.load(local_extension_folder+""/ext"")\n        extension.load(local_extension_folder)\n        return extension\n\n    def save_extension(self,extension,location):\n        self.create_work_folder()\n        rint = random.randint(1,999999)\n        extension_folder = self.work_folder+""/extension_tmp""+str(rint)\n        if not os.path.exists(extension_folder):\n            logger.info(""creating folder %s"",extension_folder)\n            os.makedirs(extension_folder)\n        tmp_file = extension_folder+""/ext""\n        joblib.dump(extension,tmp_file)\n        extension.save(extension_folder)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(extension_folder,location)\n\nExtension_wrapper = DeprecationHelper(ExtensionWrapper)'"
python/seldon/vw.py,0,"b'import sys\nfrom fileutil import *\nimport json\nfrom wabbit_wappa import *\nfrom subprocess import call\nimport numpy as np\nimport random\nfrom socket import *\nimport threading, Queue, subprocess\nimport time\nimport psutil\nimport pandas as pd\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator,ClassifierMixin\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass VWClassifier(BasePandasEstimator,BaseEstimator,ClassifierMixin):\n    """"""\n    Wrapper for Vowpall Wabbit classifier with pandas support\n\n    Parameters\n    ----------\n           \n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    num_iterations : int\n       number of iterations over data to run vw\n    raw_predictions_file : str\n       file to push raw predictions from vw to\n    model_file : str\n       model filename\n    pid_file : str\n       file to store pid of vw server so we can terminate it\n    vw_args : optional dict \n       extra args to pass to vw\n    """"""\n    def __init__(self, target=None, target_readable=None,included=None,excluded=None,id_map={},num_iterations=1, raw_predictions_file=""/tmp/raw_predictions"",model_file=""/tmp/model"",pid_file=\'/tmp/vw_pid_file\',**vw_args):\n        super(VWClassifier, self).__init__(target,target_readable,included,excluded,id_map)\n        self.clf = None\n        self.num_iterations = num_iterations\n        self.model_file=""/tmp/model""\n        self.param_suffix=""_params""\n        self.model_suffix=""_model""\n        self.raw_predictions_file=raw_predictions_file\n        self.raw_predictions_thread_running = False\n        self.tailq = Queue.Queue(maxsize=1000)         \n        self.vw = None\n        self.vw_mode = None\n        self.pid_file = pid_file\n        self.vw_args = vw_args\n        self.model = None\n        self.model_saved = False\n\n    def __getstate__(self):\n        """"""\n        Remove things that should not be pickled\n        """"""\n        result = self.__dict__.copy()\n        del result[\'model_saved\']\n        del result[\'vw\']\n        del result[\'tailq\']\n        del result[\'raw_predictions_thread_running\']\n        return result\n\n    def __setstate__(self, dict):\n        """"""\n        Add thread based variables when creating\n        """"""\n        self.__dict__ = dict\n        self.model_saved = False\n        self.vw = None\n        self.tailq = Queue.Queue(maxsize=1000)      \n        self.raw_predictions_thread_running=False\n        if not self.model is None:\n            self._start_vw_if_needed(""test"")\n            \n    def _wait_model_saved(self,fname):\n        """"""\n        Hack to wait for vw model to finish saving. It creates a file <model>.writing during this process\n        """"""\n        logger.info(""waiting for %s"",fname)\n        time.sleep(1)\n        while os.path.isfile(fname):\n            logger.info(""sleeping until model is saved"")\n            time.sleep(1)\n\n    def _save_model(self,fname):\n        """"""\n        Save vw model from running vw instance\n        """"""\n        self.vw.save_model(fname)\n        self._wait_model_saved(fname+"".writing"")\n        with open(fname, mode=\'rb\') as file: # b is important -> binary\n            self.model = file.read()\n\n    def _write_model(self):\n        """"""\n        Write the vw model to file\n        """"""\n        with open(self.model_file, mode=\'wb\') as modelfile: # b is important -> binary\n            modelfile.write(self.model)\n            self.model_saved = True\n\n\n    @staticmethod\n    def _is_number(s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n\n    def _get_feature(self,name,val):\n        """"""\n        Create a vw feature from name and value\n        """"""\n        if isinstance(val, basestring):\n            if len(val) > 0:\n                if self._is_number(val):\n                    return (name,float(val))\n                else:\n                    if len(name) > 0:\n                        return (name+""_""+val)\n                    else:\n                        return (val)\n        else:\n            if not np.isnan(val):\n                return (name,float(val))\n\n    def _convert_row(self,row,tag=None):\n        """"""Convert a dataframe row into a vw line\n        """"""\n        ns = {}\n        ns[""def""] = []\n        for col in row.index.values:\n            if not col == self.target:\n                val = row[col]\n                feature = None\n                if isinstance(val,basestring):\n                    feature = self._get_feature(col,val)\n                    if not feature is None:\n                        ns[""def""].append(feature)\n                elif isinstance(val,dict):\n                    for key in val:\n                        feature = self._get_feature(key,val[key])\n                        if not feature is None:\n                            if not col in ns:\n                                ns[col] = []\n                            ns[col].append(feature)\n                elif isinstance(val,list):\n                    for v in val:\n                        feature = self._get_feature("""",v)\n                        if not feature is None:\n                            if not col in ns:\n                                ns[col] = []\n                            ns[col].append(feature)\n        if self.target in row:\n            target = row[self.target]\n            target = int(target)\n            if self.zero_based:\n                target += 1\n        else:\n            target = None\n        namespaces = []\n        for k in ns:\n            if not k == \'def\':\n                namespaces.append(Namespace(name=k,features=ns[k]))\n        return self.vw.make_line(response=target,features=ns[\'def\'],namespaces=namespaces)\n    \n    @staticmethod\n    def _sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n\n    @staticmethod        \n    def _normalize( predictions ):\n        s = sum( predictions )\n        normalized = []\n        for p in predictions:\n            normalized.append( p / s )\n        return normalized  \n\n    def _start_raw_predictions(self):\n        """"""Start a thread to tail the raw predictions file\n        """"""\n        if not self.raw_predictions_thread_running:\n            thread = threading.Thread(target=self._tail_forever, args=(self.raw_predictions_file,))\n            thread.setDaemon(True)\n            thread.start()\n            self.raw_predictions_thread_running = True\n\n    def close(self):\n        """"""Shutdown the vw process\n        """"""\n        if not self.vw is None:\n            f=open(self.pid_file)\n            for line in f:\n                logger.info(""terminating pid %s"",line)\n                p = psutil.Process(int(line))\n                p.terminate()\n            self.vw.close()\n            self.vw = None\n\n\n    def _tail_forever(self,fn):\n        """"""Tail the raw predictions file so we can get class probabilities when doing predictions\n        """"""\n        p = subprocess.Popen([""tail"", ""-f"", fn], stdout=subprocess.PIPE)\n        while 1:\n            line = p.stdout.readline()\n            self.tailq.put(line)\n            if not line:\n                break\n\n    def _get_full_scores(self):\n        """"""Get the predictions from the vw raw predictions and normalise them\n        """"""\n        rawLine = self.tailq.get()\n        parts = rawLine.split(\' \')\n        tagScores = parts[len(parts)-1].rstrip() \n        scores = []\n        for score in parts:\n            (classId,score) = score.split(\':\')\n            scores.append(self._sigmoid(float(score)))\n        nscores = self._normalize(scores)\n        fscores = []\n        c = 1\n        for nscore in nscores:\n            fscores.append(nscore)\n            c = c + 1\n        return np.array(fscores)\n\n    def _exclude_include_features(self,df):\n        if not self.included is None:\n            df = df[list(set(self.included+[self.target]).intersection(df.columns))]\n        if not self.excluded is None:\n            df = df.drop(set(self.excluded).intersection(df.columns), axis=1)\n        return df\n\n\n    def fit(self,X,y=None):\n        """"""Convert data to vw lines and then train for required iterations\n           \n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n        Caveats : \n        1. A seldon specific fork of wabbit_wappa is needed to allow vw to run in server mode without save_resume. Save_resume seems to cause issues with the scores returned. Maybe connected to https://github.com/JohnLangford/vowpal_wabbit/issues/262\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            df_base = self._exclude_include_features(df)\n            df_base = df_base.fillna(0)\n        else:\n            check_X_y(X,y)\n            df = pd.DataFrame(X)\n            df_y = pd.DataFrame(y,columns=list(\'y\'))\n            self.target=\'y\'\n            df_base = pd.concat([df,df_y],axis=1)\n\n        min_target = df_base[self.target].astype(float).min()\n        if min_target == 0:\n            self.zero_based = True\n        else:\n            self.zero_based = False\n        if not self.target_readable is None:\n            self.create_class_id_map(df,self.target,self.target_readable,zero_based=self.zero_based)\n\n        self.num_classes = len(df_base[self.target].unique())\n        self._start_vw_if_needed(""train"")\n        df_vw = df_base.apply(self._convert_row,axis=1)\n        for i in range(0,self.num_iterations):\n            for (index,val) in df_vw.iteritems():\n                self.vw.send_line(val,parse_result=False)\n        self._save_model(self.model_file)        \n        return self\n\n    def _start_vw_if_needed(self,mode):\n        if self.vw is None or self.vw_mode != mode:\n            logger.info(""Creating vw in mode %s"",mode)\n            if not self.vw is None:\n                self.close()\n            if mode == ""test"":\n                if not self.model_saved:\n                    self._write_model()\n                self.vw =  VW(server_mode=True,pid_file=self.pid_file,port=29743,num_children=1,i=self.model_file,raw_predictions=self.raw_predictions_file,t=True)\n            else:\n                self.vw =  VW(server_mode=True,pid_file=self.pid_file,port=29742,num_children=1,oaa=self.num_classes,**self.vw_args)\n            logger.info(self.vw.command)\n            self.vw_mode = mode\n            logger.info(""Created vw in mode %s"",mode)\n\n\n    def predict_proba(self,X):\n        """"""Create predictions. Start a vw process. Convert data to vw format and send. \n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        Caveats : \n        1. A seldon specific fork of wabbit_wappa is needed to allow vw to run in server mode without save_resume. Save_resume seems to cause issues with the scores returned. Maybe connected to https://github.com/JohnLangford/vowpal_wabb#it/issues/262\n        """"""\n        self._start_vw_if_needed(""test"")\n        if isinstance(X,pd.DataFrame):\n            df = X\n            df_base = self._exclude_include_features(df)\n            df_base = df_base.fillna(0)\n        else:\n            check_array(X)\n            df_base = pd.DataFrame(X)\n        df_vw = df_base.apply(self._convert_row,axis=1)\n        predictions = None\n        for (index,val) in df_vw.iteritems():\n            prediction = self.vw.send_line(val,parse_result=True)\n            self._start_raw_predictions()\n            scores = self._get_full_scores()\n            if predictions is None:\n                predictions = np.array([scores])\n            else:\n                predictions = np.vstack([predictions,scores])\n        return predictions\n'"
python/seldon/xgb.py,0,"b'import sys\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.datasets import load_svmlight_file\nimport scipy.sparse\nimport math\nimport pandas as pd\nfrom sklearn.feature_extraction import DictVectorizer\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom collections import OrderedDict\nimport io\nfrom sklearn.utils import check_X_y\nfrom sklearn.utils import check_array\nfrom sklearn.base import BaseEstimator,ClassifierMixin\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass XGBoostClassifier(BasePandasEstimator,BaseEstimator,ClassifierMixin):\n    """"""\n    Wrapper for XGBoost classifier with pandas support\n    XGBoost specific arguments follow https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py\n\n    Parameters\n    ----------\n           \n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    num_iterations : int\n       number of iterations over data to run vw\n    raw_predictions : str\n       file to push raw predictions from vw to\n    max_depth : int\n        Maximum tree depth for base learners.\n    learning_rate : float\n        Boosting learning rate (xgb\'s ""eta"")\n    n_estimators : int\n        Number of boosted trees to fit.\n    silent : boolean\n        Whether to print messages while running boosting.\n    objective : string\n        Specify the learning task and the corresponding learning objective.\n    nthread : int\n        Number of parallel threads used to run xgboost.\n    gamma : float\n        Minimum loss reduction required to make a further partition on a leaf node of the tree.\n    min_child_weight : int\n        Minimum sum of instance weight(hessian) needed in a child.\n    max_delta_step : int\n        Maximum delta step we allow each tree\'s weight estimation to be.\n    subsample : float\n        Subsample ratio of the training instance.\n    colsample_bytree : float\n        Subsample ratio of columns when constructing each tree.\n    colsample_bylevel : float\n        Subsample ratio of columns for each split, in each level.\n    reg_alpha : float (xgb\'s alpha)\n        L2 regularization term on weights\n    reg_lambda : float (xgb\'s lambda)\n        L1 regularization term on weights\n    scale_pos_weight : float\n        Balancing of positive and negative weights.\n    base_score:\n        The initial prediction score of all instances, global bias.\n    seed : int\n        Random number seed.\n    missing : float, optional\n        Value in the data which needs to be present as a missing value. If\n        None, defaults to np.nan.\n    """"""\n    def __init__(self, target=None, target_readable=None,included=None,excluded=None,clf=None,\n                 id_map={},vectorizer=None,svmlight_feature=None, \n                 max_depth=3, learning_rate=0.1, n_estimators=100,\n                 silent=True, objective=""reg:linear"",\n                 nthread=-1, gamma=0, min_child_weight=1, max_delta_step=0,\n                 subsample=1, colsample_bytree=1, colsample_bylevel=1,\n                 reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n                 base_score=0.5, seed=0, missing=None):\n        super(XGBoostClassifier, self).__init__(target,target_readable,included,excluded,id_map)\n        self.vectorizer = vectorizer\n        self.clf = clf\n        self.max_depth=max_depth \n        self.learning_rate=learning_rate\n        self.n_estimators=n_estimators\n        self.silent=silent\n        self.objective=objective\n        self.nthread=nthread\n        self.gamma=gamma \n        self.min_child_weight=min_child_weight\n        self.max_delta_step=max_delta_step\n        self.subsample=subsample \n        self.colsample_bytree=colsample_bytree\n        self.colsample_bylevel=colsample_bylevel\n        self.reg_alpha=reg_alpha\n        self.reg_lambda=reg_lambda\n        self.scale_pos_weight=scale_pos_weight\n        self.base_score=base_score\n        self.seed=seed\n        self.missing=missing\n        #self.params = { ""max_depth"":max_depth,""learning_rate"":learning_rate,""n_estimators"":n_estimators,\n        #               ""silent"":silent, ""objective"":objective,\n        #               ""nthread"":nthread, ""gamma"":gamma, ""min_child_weight"":min_child_weight, ""max_delta_step"":max_delta_step,\n        #               ""subsample"":subsample, ""colsample_bytree"":colsample_bytree, ""colsample_bylevel"":colsample_bylevel,\n        #               ""reg_alpha"":reg_alpha, ""reg_lambda"":reg_lambda, ""scale_pos_weight"":scale_pos_weight,\n        #               ""base_score"":base_score, ""seed"":seed, ""missing"":missing }\n        self.svmlight_feature = svmlight_feature\n        \n\n    def _to_svmlight(self,row):\n        """"""Convert a dataframe row containing a dict of id:val to svmlight line\n        """"""\n        if self.target in row:\n            line = str(row[self.target])\n        else:\n            line = ""1""\n        d = row[self.svmlight_feature]\n        for (k,v) in d:\n            line += ("" ""+str(k)+"":""+str(v))\n        return line\n        \n    def _load_from_svmlight(self,df):\n        """"""Load data from dataframe with dict of id:val into numpy matrix\n        """"""\n        logger.info(""loading from dictionary feature"")\n        df_svm = df.apply(self._to_svmlight,axis=1)\n        output = io.BytesIO()\n        df_svm.to_csv(output,index=False,header=False)\n        output.seek(0)\n        (X,y) = load_svmlight_file(output,zero_based=False)\n        output.close()\n        return (X,y)\n\n\n    def fit(self,X,y=None):\n        """"""Fit a model: \n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples. If pandas dataframe can handle dict of feature in one column or cnvert a set of columns\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n\n\n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            if not self.svmlight_feature is None:\n                if not self.target_readable is None:\n                    self.create_class_id_map(df,self.target,self.target_readable)\n                (X,y) = self._load_from_svmlight(df)\n                num_class = len(np.unique(y))\n            else:\n                (X,y,self.vectorizer) = self.convert_numpy(df)\n                num_class = len(y.unique())\n        else:\n            check_X_y(X,y)\n            num_class = len(np.unique(y))\n\n        self.clf = xgb.XGBClassifier(max_depth=self.max_depth, learning_rate=self.learning_rate, \n                                     n_estimators=self.n_estimators,\n                                     silent=self.silent, objective=self.objective,\n                                     nthread=self.nthread, gamma=self.gamma, \n                                     min_child_weight=self.min_child_weight, \n                                     max_delta_step=self.max_delta_step,\n                                     subsample=self.subsample, colsample_bytree=self.colsample_bytree, \n                                     colsample_bylevel=self.colsample_bylevel,\n                                     reg_alpha=self.reg_alpha, reg_lambda=self.reg_lambda, \n                                     scale_pos_weight=self.scale_pos_weight,\n                                     base_score=self.base_score, seed=self.seed, missing=self.missing)\n        logger.info(self.clf.get_params(deep=True))\n        self.clf.fit(X,y,verbose=True)\n        return self\n\n    def predict_proba(self, X):\n        """"""\n        Returns class probability estimates for the given test data.\n\n        X : pandas dataframe or array-like\n            Test samples \n        \n        Returns\n        -------\n        proba : array-like, shape = (n_samples, n_outputs)\n            Class probability estimates.\n  \n        """"""\n        if isinstance(X,pd.DataFrame):\n            df = X\n            if not self.svmlight_feature is None:\n                (X,_) = self._load_from_svmlight(df)\n            else:\n                (X,_,_) = self.convert_numpy(df)\n        else:\n            check_array(X)\n\n        return self.clf.predict_proba(X)\n\n\n\n'"
scripts/vw/vw_hash.py,0,"b'#!/usr/bin/env python\nimport sys, getopt, argparse\nimport mmh3\nimport math\n\ndef is_number(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\ndef get_hash(label,namespace,feature,stride,mask):\n    if namespace:\n        namespace_hash = mmh3.hash(namespace,0)\n    else:\n        namespace_hash = 0\n    if is_number(feature):\n        feature_hash = int(feature) + namespace_hash\n    else:\n        feature_hash = mmh3.hash(feature,namespace_hash)\n    feature_hash_oaa = feature_hash * stride\n    return (feature_hash_oaa + label - 1) & mask\n\ndef get_constant_hash(label,stride,mask):\n    feature_hash_oaa = 11650396 * stride\n    return (feature_hash_oaa + label - 1) & mask\n    \n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'vw_hash\')\n    parser.add_argument(\'--oaa\', help=\'oaa value - will assume binary if not specified\', required=False, default=1, type=int)\n    parser.add_argument(\'-b\', help=\'bits - the hash bit sized used for training - defaults to 18\', required=False, default=18, type=int)\n\n    args = parser.parse_args()\n    opts = vars(args)\n    mask = (2**opts[\'b\'])-1\n    stride =  int(2**math.ceil(math.log(opts[\'oaa\'],2)))\n\n    for line in sys.stdin:\n        line = line.rstrip()\n        parts = line.split()\n        label = int(parts[0])\n        if label < 0:\n            label = 1\n        namespace = None\n        initialNameSpaceSeen = False\n        for token in parts:\n            if token.find(\'|\') > -1:\n                namespace = token.split(""|"")[1]\n                initialNameSpaceSeen = True\n            elif initialNameSpaceSeen:\n                if token.find(\':\') > 0:\n                    (feature,value) = token.split(\':\')\n                    print feature+"":""+str(get_hash(label,namespace,feature,stride,mask))+"":""+value,\n                else:\n                    print token+"":""+str(get_hash(label,namespace,token,stride,mask)),\n        print ""Constant:""+str(get_constant_hash(label,stride,mask))\n'"
scripts/zookeeper/set-client-config.py,0,"b'#!/usr/bin/env python\nimport sys, getopt, argparse\nfrom kazoo.client import KazooClient\nimport json\n\ndef loadZookeeperOptions(opts,zk):\n    node = ""/all_clients/""+opts[\'client\']+""/offline/semvec""\n    if zk.exists(node):\n        data, stat = zk.get(node)\n        jStr = data.decode(""utf-8"")\n        print ""Found zookeeper configuration:"",jStr\n        j = json.loads(jStr)\n        for key in j:\n            opts[key] = j[key]\n\ndef activateModel(args,folder,zk):\n    node = ""/all_clients/""+args.client+""/svtext""\n    print ""Activating model in zookeper at node "",node,"" with data "",folder\n    if zk.exists(node):\n        zk.set(node,folder)\n    else:\n        zk.create(node,folder,makepath=True)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'set-client-config\')\n    parser.add_argument(\'-z\', \'--zookeeper\', help=\'zookeeper hosts\', required=True)\n    parser.add_argument(\'--clientVariable\', help=\'client variable name\', default=""$CLIENT"")\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    zk = KazooClient(hosts=args.zookeeper)\n    zk.start()\n\n    for line in sys.stdin:\n        line = line.rstrip()\n        parts = line.split()\n        if not line.startswith(""#""):\n            clients = parts[0].split(\',\')\n            node = parts[1]\n            value = "" "".join(parts[2:])\n            print ""--------------------------""\n            print parts[0],node,""->"",value\n            for client in clients:\n                nodeClient = node.replace(args.clientVariable,client)\n                valueClient = value.replace(args.clientVariable,client)\n                print ""----""\n                print nodeClient\n                print valueClient\n                if zk.exists(nodeClient):\n                    zk.set(nodeClient,valueClient)\n                else:\n                    zk.create(nodeClient,valueClient,makepath=True)\n                    \n    zk.stop()\n\n\n'"
vm/bin/zkcmd.py,0,"b'#!/usr/bin/env python\n\n__author__ = ""Gurminder Sunner""\n\nimport pprint\nimport sys\nimport argparse\nfrom kazoo.client import KazooClient\n\ndef doCmdUsingHosts(zk_hosts, cmd, cmd_args):\n    zk_client = getKazooClient(zk_hosts)\n    zk_client.start()\n    doCmd(zk_client, cmd, cmd_args)\n    zk_client.stop()\n\ndef doCmd(zk_client, cmd, cmd_args):\n    if cmd == \'set\':\n        thePath = cmd_args[0]\n        theValue = cmd_args[1]\n        retVal = None\n        if zk_client.exists(thePath):\n            retVal = zk_client.set(thePath,theValue)\n        else:\n            retVal = zk_client.create(thePath,theValue,makepath=True)\n        print ""[{cmd}][{thePath}][{theValue}]"".format(cmd=cmd,thePath=thePath,theValue=theValue)\n    elif cmd == \'get\':\n        thePath = cmd_args[0]\n        retVal = None\n        theValue = None\n        if zk_client.exists(thePath):\n            retVal = zk_client.get(thePath)\n            theValue = retVal[0]\n        print ""[{cmd}][{thePath}][{theValue}]"".format(cmd=cmd,thePath=thePath,theValue=theValue)\n\ndef getOpts():\n    parser = argparse.ArgumentParser(description=\'Some Description\')\n    parser.add_argument(\'--zk-hosts\', help=""the zookeeper hosts"", required=True)\n    parser.add_argument(\'--cmd\', help=""the cmd to use"", required=True)\n    parser.add_argument(\'--cmd-args\', help=""the cmd args to use"", nargs=\'+\')\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = vars(parser.parse_args())\n    return opts\n\ndef getKazooClient(zk_hosts):\n    zk_client = KazooClient(hosts=zk_hosts)\n    return zk_client\n\ndef main():\n    opts = getOpts()\n    #print opts\n    ##doCmd(opts[\'zk_hosts\'], opts[\'cmd\'], opts[\'cmd_args\'])\n    doCmdUsingHosts(opts[\'zk_hosts\'], opts[\'cmd\'], opts[\'cmd_args\'])\n\nif __name__ == ""__main__"":\n    main()\n\n\n\n'"
vm/bin/zklines.py,0,"b'#!/usr/bin/env python\n\n__author__ = ""Gurminder Sunner""\n\nimport sys\nimport os\nimport argparse\n\nscript_dir = os.path.dirname( os.path.realpath(__file__) )\nsys.path.append( script_dir )\nimport zkcmd\n\ndef process_line(zk_client, line):\n    parts =line.split(None,2)\n    cmd=parts[0]\n    cmd_args=parts[1:]\n    zkcmd.doCmd(zk_client, cmd, cmd_args)\n\ndef process_file(zk_client, f):\n    for line_raw in f:\n        line = line_raw.strip() # remove whiespace and nl\n        if len(line) > 0:\n            process_line(zk_client, line)\n\ndef getOpts():\n    parser = argparse.ArgumentParser(description=\'Some Description\')\n    parser.add_argument(\'--zk-hosts\', help=""the zookeeper hosts"", required=True)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = vars(parser.parse_args())\n    return opts\n\ndef main():\n    opts = getOpts()\n    #print opts\n    zk_client = zkcmd.getKazooClient(opts[\'zk_hosts\'])\n    zk_client.start()\n    filenames = opts[\'args\']\n    if len(filenames) > 0:\n        for filename in filenames:\n            f = open(filename)\n            process_file(zk_client, f)\n            f.close()\n    else:\n        process_file(zk_client, sys.stdin)\n    zk_client.stop()\n\nif __name__ == ""__main__"":\n    main()\n\n'"
docker/examples/US_stocks_fund/create_pipeline.py,96,"b'import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import preprocessing\nfrom seldon.tensorflow_wrapper import TensorFlowWrapper\nfrom sklearn.pipeline import Pipeline\nimport seldon.pipeline.util as sutl\nimport argparse\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\ndf_Xy = pd.read_csv(\'home/seldon/data/indicators_nan_replaced.csv\')\n#df_Xy = pd.read_csv(\'home/seldon/data/indicators_nan_replacedv2.csv\')\n#df_Xy = pd.read_csv(\'home/seldon/data/indicators_nan_replacedv3.csv\')\n\nlogger.info(\'tf version: %s \' % tf.__version__)\n\ndef get_data(split_train_test=False):\n    \n    cols_tokeep = df_Xy.columns.tolist()\n    logger.debug(\'columns to keep: \')\n    logger.debug(cols_tokeep)\n    \n    if \'company_id\' in cols_tokeep:\n        cols_tokeep.remove(\'company_id\')\n    Xy = df_Xy.as_matrix(columns=cols_tokeep)\n    Xy_shuffled = np.random.permutation(Xy)\n\n    (means,stds) = (np.mean(Xy_shuffled[:,:-1],axis=0).reshape((1,Xy_shuffled[:,:-1].shape[1])),\n                    np.std(Xy_shuffled[:,:-1],axis=0).reshape((1,Xy_shuffled[:,:-1].shape[1])))\n            \n    for i,v in enumerate(means[0]):\n        if i%2!=0:\n            means[0,i]=0\n            stds[0,i]=1\n\n    if split_train_test:\n        #split train-test\n        split_ratio = int(0.7*(len(Xy)))\n        Xy_train = Xy_shuffled[:split_ratio,:]\n        Xy_test = Xy_shuffled[split_ratio:,:]\n                        \n        dataset = {\'train\':Xy_train, \'test\':Xy_test, \'means\':means, \'stds\':stds}\n\n    else:\n        #no splitting                \n        dataset = {\'train\':Xy_shuffled, \'test\':Xy_shuffled, \'means\':means, \'stds\':stds}\n\n    return dataset\n\ndef get_data_v3(split_train_test=False):\n    \n    cols_tokeep = df_Xy.columns.tolist()\n    logger.debug(\'columns to keep\')\n    logger.debug(cols_tokeep)\n\n    if \'company_id\' in cols_tokeep:\n        cols_tokeep.remove(\'company_id\')\n    Xy = df_Xy.as_matrix(columns=cols_tokeep)\n    Xy_shuffled = np.random.permutation(Xy)\n\n    (means,stds) = (np.mean(Xy_shuffled[:,:-1],axis=0).reshape((1,Xy_shuffled[:,:-1].shape[1])),\n                    np.std(Xy_shuffled[:,:-1],axis=0).reshape((1,Xy_shuffled[:,:-1].shape[1])))\n            \n\n    if split_train_test:\n        #split train-test\n        split_ratio = int(0.7*(len(Xy)))\n        Xy_train = Xy_shuffled[:split_ratio,:]\n        Xy_test = Xy_shuffled[split_ratio:,:]\n                        \n        dataset = {\'train\':Xy_train, \'test\':Xy_test, \'means\':means, \'stds\':stds}\n\n    else:\n        #no splitting                \n        dataset = {\'train\':Xy_shuffled, \'test\':Xy_shuffled, \'means\':means, \'stds\':stds}\n\n    return dataset\n\n\ndef fill_feed_dict_train(in_pl,\n                         y_pl,\n                         dataset,\n                         iterator,\n                         batch_size=128):\n\n    train = dataset[\'train\']\n\n    if batch_size==\'all\':\n        feed_dict_train = {in_pl : train[:,:-1],\n                           y_pl : train[:,-1].reshape((len(train),1))}\n\n    else:\n        nb_batches = int(dataset[\'train\'].shape[0]/batch_size)\n        j = iterator % nb_batches\n    \n        feed_dict_train = {in_pl : train[j*batch_size:(j+1)*batch_size,:-1],\n                           y_pl : train[j*batch_size:(j+1)*batch_size,-1].reshape((batch_size,1))}\n    \n    return feed_dict_train\n\ndef fill_feed_dict_test(in_pl,\n                        y_pl,\n                        dataset):\n    test = dataset[\'test\']\n\n    feed_dict_test = {in_pl : test[:,:-1],\n                      y_pl : test[:,-1].reshape((len(test),1))} \n\n    return feed_dict_test\n\ndataset = get_data()\n#dataset = get_data_v3()\n\n# model v1\ndef create_pipeline_v1(load=None):\n    \n    nb_features = 58\n    nb_hidden1 = 116\n    nb_hidden2 = 29\n\n    batch_size = 64\n    nb_iter = 30001\n    lamb = 0.0001\n    \n    in_pl = tf.placeholder(dtype=tf.float32,\n                           shape=(None,nb_features),\n                           name=\'input_placeholder\')\n\n    means = tf.constant(dataset[\'means\'],\n                           dtype=tf.float32,\n                           shape=(1,nb_features),\n                           name=\'features_means\')\n    stds = tf.constant(dataset[\'stds\'],\n                       dtype=tf.float32,\n                       shape=(1,nb_features),\n                       name=\'features_stds_placeholder\')\n    means_tiled = tf.tile(means,[tf.shape(in_pl)[0],1])\n    stds_tiled = tf.tile(stds,[tf.shape(in_pl)[0],1])    \n\n    #scaled inputs\n    inp = (in_pl - means_tiled)/(stds_tiled+1e-10)\n\n    y_pl = tf.placeholder(dtype=tf.float32,\n                          shape=(None,1),\n                          name=\'target_placeholder\')\n\n    \n    W1 = tf.Variable(tf.truncated_normal([nb_features,nb_hidden1]),\n                     dtype=tf.float32,\n                     name=\'first_layer_weights\')\n    W1_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W1))\n    b1 = tf.Variable(tf.zeros(shape=[nb_hidden1]))\n\n    #first hidden layer\n    h1 = tf.nn.relu(tf.matmul(inp,W1) + b1,\n                    name=\'first_hidden_layer\')\n\n    W2 = tf.Variable(tf.truncated_normal([nb_hidden1,nb_hidden2]),\n                     dtype=tf.float32,\n                     name=\'second_layer_weights\')\n    W2_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W2))\n    b2 = tf.Variable(tf.zeros(shape=[nb_hidden2]))\n\n    #second hidden layer\n    h2 = tf.sigmoid(tf.matmul(h1,W2) + b2,\n                    name=\'second_hidden_layer\')\n\n    W3 = tf.Variable(tf.truncated_normal([nb_hidden2,1]),\n                     dtype=tf.float32,\n                     name=\'last_layer_weights\')\n    W3_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W3))\n    b3 = tf.Variable(tf.zeros(shape=[1]))\n\n    #out layer\n    out = tf.sigmoid(tf.matmul(h2,W3) + b3,\n                     name=\'output_layer\')\n    proba = tf.squeeze(tf.pack([1-out,out],2),\n                       squeeze_dims=[1])\n\n    L2reg = lamb*(W1_L2reg + W2_L2reg + W3_L2reg)\n                  \n    cross_entropy = -(1/float(2))*tf.reduce_mean(y_pl * tf.log(out+1e-10) + (1-y_pl) * tf.log(1-out+1e-10),\n                                                 name=\'cost_function\')\n    cost = cross_entropy + L2reg\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session()\n    \n    logger.info(\'Training model...\')\n    logger.info(\'model version: %i\' % 1)\n    sess.run(init)\n    \n    for i in range(nb_iter):\n\n        if i % 1000 == 0:\n            logger.info(\'iteration %i of %i\' % (i,nb_iter))\n            \n        feed_dict_train = fill_feed_dict_train(in_pl,\n                                               y_pl,\n                                               dataset,\n                                               i,\n                                               batch_size=batch_size)\n        (_,\n         W3_value,\n         cost_value,\n         out_value) = sess.run([train_step,\n                                W3,\n                                cost,\n                                out],\n                               feed_dict=feed_dict_train)\n\n        if i % 10000 == 0:\n#            feed_dict_test = fill_feed_dict_test(in_pl,\n#                                                 y_pl,\n#                                                 dataset)\n            \n            inp_values,proba_values = sess.run([inp,proba],feed_dict=feed_dict_train)\n            logger.debug(\'scaled inputs:\')\n            logger.debug(inp_values)\n            logger.debug(\'probabilities:\')\n            logger.debug(proba_values)\n            logger.debug(\'proba out shape:\')\n            logger.debug(proba_values.shape)\n            logger.debug(\'cost: %f\' % cost_value)\n\n    tfw = TensorFlowWrapper(sess,tf_input=in_pl,tf_output=proba,\n                            target=""y"",target_readable=""class"",excluded=[\'class\'])\n\n    return Pipeline([(\'deep_classifier\',tfw)])\n\n#model v2\ndef create_pipeline_v2(load=None):\n    \n    nb_features = 58\n    nb_hidden1 = 400\n    nb_hidden2 = 200\n    nb_hidden3 = 100\n    \n    batch_size = 64\n    nb_iter = 30001\n    lamb = 0.0001\n    \n    in_pl = tf.placeholder(dtype=tf.float32,\n                           shape=(None,nb_features),\n                           name=\'input_placeholder\')\n\n    means = tf.constant(dataset[\'means\'],\n                           dtype=tf.float32,\n                           shape=(1,nb_features),\n                           name=\'features_means\')\n    stds = tf.constant(dataset[\'stds\'],\n                       dtype=tf.float32,\n                       shape=(1,nb_features),\n                       name=\'features_stds_placeholder\')\n    means_tiled = tf.tile(means,[tf.shape(in_pl)[0],1])\n    stds_tiled = tf.tile(stds,[tf.shape(in_pl)[0],1])    \n\n    #scaled inputs\n    inp = (in_pl - means_tiled)/(stds_tiled+1e-10)\n\n    y_pl = tf.placeholder(dtype=tf.float32,\n                          shape=(None,1),\n                          name=\'target_placeholder\')\n\n    #first hidden layer    \n    W1 = tf.Variable(tf.truncated_normal([nb_features,nb_hidden1]),\n                     dtype=tf.float32,\n                     name=\'first_layer_weights\')\n    W1_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W1))\n    b1 = tf.Variable(tf.zeros(shape=[nb_hidden1]))\n    \n    h1 = tf.sigmoid(tf.matmul(inp,W1) + b1,\n                    name=\'first_hidden_layer\')\n\n    #second hidden layer\n    W2 = tf.Variable(tf.truncated_normal([nb_hidden1,nb_hidden2]),\n                     dtype=tf.float32,\n                     name=\'second_layer_weights\')\n    W2_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W2))\n    b2 = tf.Variable(tf.zeros(shape=[nb_hidden2]))\n\n    h2 = tf.sigmoid(tf.matmul(h1,W2) + b2,\n                    name=\'second_hidden_layer\')\n\n    #third hidden layer\n    W3 = tf.Variable(tf.truncated_normal([nb_hidden2,nb_hidden3]),\n                     dtype=tf.float32,\n                     name=\'third_layer_weights\')\n    W3_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W3))\n    b3 = tf.Variable(tf.zeros(shape=[nb_hidden3]))\n\n    h3 = tf.sigmoid(tf.matmul(h2,W3) + b3,\n                    name=\'third_hidden_layer\')\n\n    #out layer\n    W4 = tf.Variable(tf.truncated_normal([nb_hidden3,1]),\n                     dtype=tf.float32,\n                     name=\'last_layer_weights\')\n    W4_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W4))\n    b4 = tf.Variable(tf.zeros(shape=[1]))\n\n    out = tf.sigmoid(tf.matmul(h3,W4) + b4,\n                     name=\'output_layer\')\n    proba = tf.squeeze(tf.pack([1-out,out],2),\n                       squeeze_dims=[1])\n\n    L2reg = lamb*(W1_L2reg + W2_L2reg + W3_L2reg + W4_L2reg)\n                  \n    cross_entropy = -(1/float(2))*tf.reduce_mean(y_pl * tf.log(out+1e-10) + (1-y_pl) * tf.log(1-out+1e-10),\n                                                 name=\'cost_function\')\n    cost = cross_entropy + L2reg\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session()\n    \n    logger.info(\'Training model...\')\n    logger.info(\'model version: %i\' % 2)\n\n    sess.run(init)\n    \n    for i in range(nb_iter):\n\n        if i % 1000 == 0:\n            logger.info(\'iteration %i of %i\' % (i,nb_iter))\n            \n        feed_dict_train = fill_feed_dict_train(in_pl,\n                                               y_pl,\n                                               dataset,\n                                               i,\n                                               batch_size=batch_size)\n        (_,\n         W3_value,\n         cost_value,\n         out_value) = sess.run([train_step,\n                                W3,\n                                cost,\n                                out],\n                               feed_dict=feed_dict_train)\n\n        if i % 10000 == 0:\n#            feed_dict_test = fill_feed_dict_test(in_pl,\n#                                                 y_pl,\n#                                                 dataset)\n            \n            inp_values,proba_values = sess.run([inp,proba],feed_dict=feed_dict_train)\n            logger.debug(\'scaled inputs\')\n            logger.debug(inp_values)\n            logger.debug(\'probabilities\')\n            logger.debug(proba_values)\n            logger.debug(\'proba out shape\')\n            logger.debug(proba_values.shape)\n            logger.debug(\'cost\')\n            logger.debug(cost_value)\n\n    tfw = TensorFlowWrapper(sess,tf_input=in_pl,tf_output=proba,\n                            target=""y"",target_readable=""class"",excluded=[\'class\'])\n\n    return Pipeline([(\'deep_classifier\',tfw)])\n\n# model v3\ndef create_pipeline_v3(load=None):\n    \n    nb_features = 29\n    nb_hidden1 = 400\n    nb_hidden2 = 200\n    nb_hidden3 = 100\n    \n    batch_size = 64\n    nb_iter = 30001\n    lamb = 0.0001\n    \n    in_pl = tf.placeholder(dtype=tf.float32,\n                           shape=(None,nb_features),\n                           name=\'input_placeholder\')\n\n    means = tf.constant(dataset[\'means\'],\n                           dtype=tf.float32,\n                           shape=(1,nb_features),\n                           name=\'features_means\')\n    stds = tf.constant(dataset[\'stds\'],\n                       dtype=tf.float32,\n                       shape=(1,nb_features),\n                       name=\'features_stds_placeholder\')\n    means_tiled = tf.tile(means,[tf.shape(in_pl)[0],1])\n    stds_tiled = tf.tile(stds,[tf.shape(in_pl)[0],1])    \n\n    #scaled inputs\n    inp = (in_pl - means_tiled)/(stds_tiled+1e-10)\n\n    y_pl = tf.placeholder(dtype=tf.float32,\n                          shape=(None,1),\n                          name=\'target_placeholder\')\n\n    #first hidden layer    \n    W1 = tf.Variable(tf.truncated_normal([nb_features,nb_hidden1]),\n                     dtype=tf.float32,\n                     name=\'first_layer_weights\')\n    W1_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W1))\n    b1 = tf.Variable(tf.zeros(shape=[nb_hidden1]))\n    \n    h1 = tf.sigmoid(tf.matmul(inp,W1) + b1,\n                    name=\'first_hidden_layer\')\n\n    #second hidden layer\n    W2 = tf.Variable(tf.truncated_normal([nb_hidden1,nb_hidden2]),\n                     dtype=tf.float32,\n                     name=\'second_layer_weights\')\n    W2_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W2))\n    b2 = tf.Variable(tf.zeros(shape=[nb_hidden2]))\n\n    h2 = tf.sigmoid(tf.matmul(h1,W2) + b2,\n                    name=\'second_hidden_layer\')\n\n    #third hidden layer\n    W3 = tf.Variable(tf.truncated_normal([nb_hidden2,nb_hidden3]),\n                     dtype=tf.float32,\n                     name=\'third_layer_weights\')\n    W3_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W3))\n    b3 = tf.Variable(tf.zeros(shape=[nb_hidden3]))\n\n    h3 = tf.sigmoid(tf.matmul(h2,W3) + b3,\n                    name=\'third_hidden_layer\')\n\n    #out layer\n    W4 = tf.Variable(tf.truncated_normal([nb_hidden3,1]),\n                     dtype=tf.float32,\n                     name=\'last_layer_weights\')\n    W4_L2reg = (1/2*batch_size)*tf.reduce_sum(tf.square(W4))\n    b4 = tf.Variable(tf.zeros(shape=[1]))\n\n    out = tf.sigmoid(tf.matmul(h3,W4) + b4,\n                     name=\'output_layer\')\n    proba = tf.squeeze(tf.pack([1-out,out],2),\n                       squeeze_dims=[1])\n\n    L2reg = lamb*(W1_L2reg + W2_L2reg + W3_L2reg + W4_L2reg)\n                  \n    cross_entropy = -(1/float(2))*tf.reduce_mean(y_pl * tf.log(out+1e-10) + (1-y_pl) * tf.log(1-out+1e-10),\n                                                 name=\'cost_function\')\n    cost = cross_entropy + L2reg\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session()\n    \n    logger.info(\'Training model...\')\n    logger.info(\'model version %i\' % 3)\n\n    sess.run(init)\n    \n    for i in range(nb_iter):\n\n        if i % 1000 == 0:\n            logger.info(\'iteration %i of %i\' % (i,nb_iter))\n            \n        feed_dict_train = fill_feed_dict_train(in_pl,\n                                               y_pl,\n                                               dataset,\n                                               i,\n                                               batch_size=batch_size)\n        (_,\n         W3_value,\n         cost_value,\n         out_value) = sess.run([train_step,\n                                W3,\n                                cost,\n                                out],\n                               feed_dict=feed_dict_train)\n\n        if i % 10000 == 0:\n#            feed_dict_test = fill_feed_dict_test(in_pl,\n#                                                 y_pl,\n#                                                 dataset)\n            \n            inp_values,proba_values = sess.run([inp,proba],feed_dict=feed_dict_train)\n            logger.debug(\'scaled inputs\')\n            logger.debug(inp_values)\n            logger.debug(\'probabilities\')\n            logger.debug(proba_values)\n            logger.debug(\'proba out shape\')\n            logger.debug(proba_values.shape)\n            logger.debug(\'cost\')\n            logger.debug(cost_value)\n\n    tfw = TensorFlowWrapper(sess,tf_input=in_pl,tf_output=proba,\n                            target=""y"",target_readable=""class"",excluded=[\'class\'])\n\n    return Pipeline([(\'deep_classifier\',tfw)])\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(prog=\'pipeline_example\')\n    parser.add_argument(\'-m\',\'--model\', help=\'model output folder\', required=True)\n    parser.add_argument(\'-l\',\'--load\',help=\'Load pretrained model from file\')\n    \n    args = parser.parse_args()\n    \n    p = create_pipeline_v1(args.load)\n#    p = create_pipeline_v2(args.load)\n#    p = create_pipeline_v3(args.load)\n\n    pw = sutl.PipelineWrapper()\n\n    pw.save_pipeline(p,args.model)\n\n    logger.info(\'tf version: %s\' % tf.__version__)\n    logger.info(\'pipeline saved in %s\' % args.model)\n'"
docker/examples/tensorflow_deep_mnist/create_pipeline.py,23,"b'from tensorflow.examples.tutorials.mnist import input_data\n#mnist = input_data.read_data_sets(""MNIST_data/"", one_hot = True)\nimport tensorflow as tf\nfrom seldon.tensorflow_wrapper import TensorFlowWrapper\nfrom sklearn.pipeline import Pipeline\nimport seldon.pipeline.util as sutl\nimport argparse\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding=\'SAME\')\n\ndef create_pipeline(load=None):\n\n    x = tf.placeholder(tf.float32, [None,784])\n\n    W_conv1 = weight_variable([5, 5, 1, 32])\n    b_conv1 = bias_variable([32])\n\n    x_image = tf.reshape(x, [-1,28,28,1])\n\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n    W_conv2 = weight_variable([5, 5, 32, 64])\n    b_conv2 = bias_variable([64])\n\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n    h_pool2 = max_pool_2x2(h_conv2)\n\n    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n    b_fc1 = bias_variable([1024])\n\n    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    keep_prob = tf.placeholder(tf.float32)\n    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    W_fc2 = weight_variable([1024, 10])\n    b_fc2 = bias_variable([10])\n\n    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\n    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    init = tf.initialize_all_variables()\n\n    sess = tf.Session()\n    \n\n    if not load:\n        mnist = input_data.read_data_sets(""MNIST_data/"", one_hot = True)\n        print \'Training model\'\n\n        sess.run(init)\n        for i in range(20000):\n            batch_xs, batch_ys = mnist.train.next_batch(50)\n            if i%100 == 0:\n\n                train_accuracy = accuracy.eval(session=sess,feed_dict={x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n                print(""step %d, training accuracy %.3f""%(i, train_accuracy))\n            sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n\n        print(""test accuracy %g""%accuracy.eval(session=sess,feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\n    else:\n\n        print \'Loading pre-trained model\'\n        saver = tf.train.Saver()\n        saver.restore(sess,load)\n\n#    print(""test accuracy %g""%accuracy.eval(session=sess,feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n\n    # print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y_:mnist.test.labels}))\n\n    tfw = TensorFlowWrapper(sess,tf_input=x,tf_output=y_conv,tf_constants=[(keep_prob,1.0)],target=""y"",target_readable=""class"",excluded=[\'class\'])\n\n    return Pipeline([(\'deep_classifier\',tfw)])\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(prog=\'pipeline_example\')\n    parser.add_argument(\'-m\',\'--model\', help=\'model output folder\', required=True)\n    parser.add_argument(\'-l\',\'--load\',help=\'Load pretrained model from file\')\n    \n    args = parser.parse_args()\n\n    p = create_pipeline(args.load)\n\n    pw = sutl.PipelineWrapper()\n\n    pw.save_pipeline(p,args.model)\n\n\n'"
docker/grpc-util/python/grpcLatencyTest.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nimport json\nfrom google.protobuf import json_format\nimport seldon.rpc.seldon_pb2 as seldon_pb2\nfrom google.protobuf import any_pb2\nimport requests\nimport grpc\nimport time\n\nclass LatencyTest(object):\n\n    def __init__(self):\n        self.latency_sum = 0\n\n    def getToken(self,url,key,secret):\n        params = {}\n        params[""consumer_key""] = key\n        params[""consumer_secret""] = secret\n        r = requests.get(url+""/token"",params=params)\n        if r.status_code == requests.codes.ok:\n            print r.text\n            j = json.loads(r.text)\n            return j[""access_token""]\n        else:\n            print ""failed call to get token""\n            return None\n\n    def callRpc(self,token,data,stub):\n        dataAny = any_pb2.Any()\n        dataAny.Pack(data)\n        meta = seldon_pb2.ClassificationRequestMeta(puid=""12345"")\n        request = seldon_pb2.ClassificationRequest(meta=meta,data=dataAny)\n        metadata = [(b\'oauth_token\', token)]\n        time1 = time.time()\n        reply = stub.Predict(request,999,metadata=metadata)\n        time2 = time.time()\n        self.latency_sum += ((time2-time1)*1000.0)\n\n    def run(self,json_filename,klass,stub,num_requests):\n        reqs = 0\n        while True:\n            with open(json_filename) as f:\n                for line in f:\n                    line = line.rstrip();\n                    message = json_format.Parse(line, klass())                  \n                    self.callRpc(token,message,stub)\n                    reqs += 1\n                    if reqs % 10 == 0:\n                        print reqs\n                    if reqs >= num_requests:\n                        return\n            \n    def print_stats(self,num_requests):\n\n        print ""%d calls, avg %0.3f ms"" % (num_requests,self.latency_sum/num_requests)\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--proto-python-path\', help=\'path to python generated protobuf\', required=True)\n    parser.add_argument(\'--request-class\', help=\'request class as x.y.z\', required=True)\n    parser.add_argument(\'--json\', help=\'json file\', required=True)\n    parser.add_argument(\'--oauth-url\', help=\'oauth url\', required=True)\n    parser.add_argument(\'--grpc-endpoint\', help=\'grpc endpoint as host:port\', required=True)\n    parser.add_argument(\'--key\', help=\'oauth consumer key\',required=True)\n    parser.add_argument(\'--secret\', help=\'oauth consumer secret\',required=True)\n    parser.add_argument(\'--num-requests\', help=\'number of requests to run\',type=int,default=1)\n\n    args = parser.parse_args()\n    opts = vars(args)\n    lt = LatencyTest()\n\n    # create request class\n    sys.path.append(args.proto_python_path)\n    parts = args.request_class.split(""."")\n    className = parts[-1]\n    mod = __import__(""."".join(parts[0:-1]), fromlist=[className])\n    klass = getattr(mod, className)\n\n    #get token from oauth server\n    token = lt.getToken(args.oauth_url,args.key,args.secret)\n\n    if not token is None:\n        #create channel to rpc server\n        channel = grpc.insecure_channel(args.grpc_endpoint)\n        stub = seldon_pb2.ClassifierStub(channel)    \n\n        lt.run(args.json,klass,stub,args.num_requests)\n        lt.print_stats(args.num_requests)\n    else:\n        print ""failed to get oauth token""\n'"
docker/grpc-util/python/jsonLatencyTest.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nimport json\nimport time\nimport requests\n\nclass BadCallError(Exception):\n    def __init__(self, value):\n        self.value = value\n    def __str__(self):\n        return repr(self.value)\n\n\nclass LatencyTest(object):\n\n    def __init__(self):\n        self.latency_sum = 0\n\n    def callRest(self,json,url,key):\n        params = {}\n        params[""consumer_key""] = key\n        params[""json""] = json\n        params[""jsonpCallback""] = ""unused""\n        time1 = time.time()\n        r = requests.get(url+""/js/predict"",params=params)\n        time2 = time.time()\n        self.latency_sum += ((time2-time1)*1000.0)\n        if r.status_code == requests.codes.ok:\n            return\n        else:\n            raise BadCallError(""bad http reponse ""+str(r.status_code))\n\n\n    def run(self,json_filename,url,key,num_requests):\n        reqs = 0\n        while True:\n            with open(json_filename) as f:\n                for line in f:\n                    line = line.rstrip();\n                    data = json.loads(line)\n                    callJson = {""data"":data}\n                    callJsonStr = json.dumps(callJson)\n                    self.callRest(callJsonStr,url,key)\n                    reqs += 1\n                    if reqs >= num_requests:\n                        return\n            \n    def print_stats(self,num_requests):\n        print ""%d calls, avg %0.3f ms"" % (num_requests,self.latency_sum/num_requests)\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--json\', help=\'json file\', required=True)\n    parser.add_argument(\'--server-url\', help=\'server url\', required=True)\n    parser.add_argument(\'--key\', help=\'js key\', required=True)\n    parser.add_argument(\'--num-requests\', help=\'number of requests to run\',type=int,default=1)\n\n    args = parser.parse_args()\n    opts = vars(args)\n    lt = LatencyTest()\n\n    lt.run(args.json,args.server_url,args.key,args.num_requests)\n    lt.print_stats(args.num_requests)\n'"
docker/locust/scripts/seldon_grpc_predict_locust.py,0,"b'from locust.stats import RequestStats\nfrom locust import Locust, TaskSet, task, events\nimport os\nimport sys, getopt, argparse\nfrom random import randint,random\nimport json\nfrom locust.events import EventHook\nimport requests\nimport re\nimport grpc\nfrom seldon.rpc import seldon_pb2\nfrom google.protobuf import any_pb2\nimport time\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(prog=\'locust\')\n    parser.add_argument(\'--host\')\n    parser.add_argument(\'--clients\')\n    parser.add_argument(\'--hatch-rate\')\n    parser.add_argument(\'--master\', action=\'store_true\')\n    args, unknown = parser.parse_known_args() \n    #args = parser.parse_args()\n    opts = vars(args)\n    print args\n    return args.host, int(args.clients), int(args.hatch_rate)\n\nHOST, MAX_USERS_NUMBER, USERS_PER_SECOND = parse_arguments()\n\n\nslaves_connect = []\nslave_report = EventHook()\nALL_SLAVES_CONNECTED = False\nSLAVES_NUMBER = 1\ndef on_my_event(client_id,data):\n    """"""\n    Waits for all slaves to be connected and launches the swarm\n    :param client_id:\n    :param data:\n    :return:\n    """"""\n    global ALL_SLAVES_CONNECTED\n    if not ALL_SLAVES_CONNECTED:\n        print ""Event was fired with arguments""\n        if client_id not in slaves_connect:\n            slaves_connect.append(client_id)\n        if len(slaves_connect) == SLAVES_NUMBER:\n            print ""All Slaves Connected""\n            ALL_SLAVES_CONNECTED = True\n            print events.slave_report._handlers\n            header = {\'Content-Type\': \'application/x-www-form-urlencoded\'}\n            r = requests.post(\'http://127.0.0.1:8089/swarm\',data={\'hatch_rate\':USERS_PER_SECOND,\'locust_count\':MAX_USERS_NUMBER},headers=header)\nimport resource\n\nrsrc = resource.RLIMIT_NOFILE\nsoft, hard = resource.getrlimit(rsrc)\nprint \'RLIMIT_NOFILE soft limit starts as  :\', soft\n\n#resource.setrlimit(rsrc, (65535, hard)) #limit to one kilobyte\n\nsoft, hard = resource.getrlimit(rsrc)\nprint \'RLIMIT_NOFILE soft limit changed to :\', soft\n\nevents.slave_report += on_my_event # Register method in slaves report event\n\n\n\nclass GrpcLocust(Locust):\n    def __init__(self, *args, **kwargs):\n        super(GrpcLocust, self).__init__(*args, **kwargs)\n\nclass ApiUser(GrpcLocust):\n    \n    min_wait=900    # Min time between requests of each user\n    max_wait=1100    # Max time between requests of each user\n    stop_timeout= 1000000  # Stopping time\n    \n\n    class task_set(TaskSet):\n\n        def getEnviron(self,key,default):\n            if key in os.environ:\n                return os.environ[key]\n            else:\n                return default\n\n        def getToken(self):\n            consumer_key = self.getEnviron(\'SELDON_OAUTH_KEY\',""oauthkey"")\n            consumer_secret = self.getEnviron(\'SELDON_OAUTH_SECRET\',""oauthsecret"")\n\n            params = {}\n            params[""consumer_key""] = consumer_key\n            params[""consumer_secret""] = consumer_secret\n            url = self.oauth_endpoint+""/token""\n            r = requests.get(url,params=params)\n            if r.status_code == requests.codes.ok:\n                j = json.loads(r.text)\n                print j\n                return j[""access_token""]\n            else:\n                print ""failed call to get token""\n                return None\n\n\n        def on_start(self):\n            """"""\n            get token\n            :return:\n            """"""\n            print ""on start""\n            self.oauth_endpoint = self.getEnviron(\'SELDON_OAUTH_ENDPOINT\',""http://127.0.0.1:30015"")\n            self.token = self.getToken()\n            self.grpc_endpoint = self.getEnviron(\'SELDON_GRPC_ENDPOINT\',""127.0.0.1:30017"")\n            self.data_size = int(self.getEnviron(\'SELDON_DEFAULT_DATA_SIZE\',""784""))            \n\n        @task\n        def get_prediction(self):\n            channel = grpc.insecure_channel(self.grpc_endpoint)\n            stub = seldon_pb2.SeldonStub(channel)\n            fake_data = [random() for i in range(0,self.data_size)]\n            data = seldon_pb2.DefaultCustomPredictRequest(values=fake_data)\n            dataAny = any_pb2.Any()\n            dataAny.Pack(data)\n            meta = seldon_pb2.ClassificationRequestMeta(puid=str(randint(0,99999999)))\n            metadata = [(b\'oauth_token\', self.token)]\n            request = seldon_pb2.ClassificationRequest(meta=meta,data=dataAny)\n            start_time = time.time()\n            try:\n                reply = stub.Classify(request,999,metadata=metadata)\n            except xmlrpclib.Fault as e:\n                total_time = int((time.time() - start_time) * 1000)\n                events.request_failure.fire(request_type=""grpc"", name=HOST, response_time=total_time, exception=e)\n            else:\n                total_time = int((time.time() - start_time) * 1000)\n                events.request_success.fire(request_type=""grpc"", name=HOST, response_time=total_time, response_length=0)\n\n\n        \n\n\n\n\n'"
docker/locust/scripts/seldon_js_predict_locust.py,0,"b'from locust.stats import RequestStats\nfrom locust import HttpLocust, TaskSet, task, events\nimport os\nimport sys, getopt, argparse\nfrom random import randint,random\nimport json\nfrom locust.events import EventHook\nimport requests\nimport re\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(prog=\'locust\')\n    parser.add_argument(\'--host\')\n    parser.add_argument(\'--clients\')\n    parser.add_argument(\'--hatch-rate\')\n    parser.add_argument(\'--master\', action=\'store_true\')\n    args, unknown = parser.parse_known_args() \n    #args = parser.parse_args()\n    opts = vars(args)\n    print args\n    return args.host, int(args.clients), int(args.hatch_rate)\n\nHOST, MAX_USERS_NUMBER, USERS_PER_SECOND = parse_arguments()\n\nslaves_connect = []\nslave_report = EventHook()\nALL_SLAVES_CONNECTED = False\nSLAVES_NUMBER = 1\ndef on_my_event(client_id,data):\n    """"""\n    Waits for all slaves to be connected and launches the swarm\n    :param client_id:\n    :param data:\n    :return:\n    """"""\n    global ALL_SLAVES_CONNECTED\n    if not ALL_SLAVES_CONNECTED:\n        print ""Event was fired with arguments""\n        if client_id not in slaves_connect:\n            slaves_connect.append(client_id)\n        if len(slaves_connect) == SLAVES_NUMBER:\n            print ""All Slaves Connected""\n            ALL_SLAVES_CONNECTED = True\n            print events.slave_report._handlers\n            header = {\'Content-Type\': \'application/x-www-form-urlencoded\'}\n            r = requests.post(\'http://127.0.0.1:8089/swarm\',data={\'hatch_rate\':USERS_PER_SECOND,\'locust_count\':MAX_USERS_NUMBER},headers=header)\nimport resource\n\nrsrc = resource.RLIMIT_NOFILE\nsoft, hard = resource.getrlimit(rsrc)\nprint \'RLIMIT_NOFILE soft limit starts as  :\', soft\n\n#resource.setrlimit(rsrc, (65535, hard)) #limit to one kilobyte\n\nsoft, hard = resource.getrlimit(rsrc)\nprint \'RLIMIT_NOFILE soft limit changed to :\', soft\n\nevents.slave_report += on_my_event # Register method in slaves report event\n\n\nclass SeldonJsLocust(TaskSet):\n\n    def getEnviron(self,key,default):\n        if key in os.environ:\n            return os.environ[key]\n        else:\n            return default\n\n    def on_start(self):\n        """"""\n        Executes for each user at the start\n        :return:\n        """"""\n        self.consumer_secret = self.getEnviron(\'SELDON_JS_KEY\',""jssecret"")\n        self.data_size = int(self.getEnviron(\'SELDON_DEFAULT_DATA_SIZE\',""784""))            \n\n    @task\n    def getPrediction(self):\n        fake_data = [round(random(),2) for i in range(0,self.data_size)]\n        j = {""data"":fake_data}\n        jStr = json.dumps(j)\n        r = self.client.get(""/js/predict?consumer_key={}&json={}&jsonpCallback=j"".format(self.consumer_secret, jStr),name=""/js/predict"")\n\n\nclass WebsiteUser(HttpLocust):\n    task_set = SeldonJsLocust\n    min_wait=900    # Min time between requests of each user\n    max_wait=1100    # Max time between requests of each user\n    stop_timeout= 1000000  # Stopping time\n\n\n\n\n'"
docker/ngram/ngram_scripts/build_recommender.py,0,"b'import sys, getopt, argparse\nfrom  seldon.text.ngram_recommend import NgramModel\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'upload model\')\n    parser.add_argument(\'--arpa\', help=\'arpa file\', required=True)\n    parser.add_argument(\'--dst\', help=\'dst folder\', required=True)\n    parser.add_argument(\'--aws_key\', help=\'aws key - needed if input or output is on AWS and no IAM\')\n    parser.add_argument(\'--aws_secret\', help=\'aws secret - needed if input or output on AWS  and no IAM\')\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    recommender = NgramModel()\n    recommender.fit(args.arpa)\n\n    import seldon\n    if ""aws_key"" in opts:\n        rw = seldon.Recommender_wrapper(aws_key=args.aws_key,aws_secret=args.aws_secret)\n    else:\n        rw = seldon.Recommender_wrapper()\n    rw.save_recommender(recommender,args.dst)\n\n\n\n\n'"
docker/ngram/ngram_scripts/create_corpus.py,0,"b'import sys, getopt, argparse\nfrom seldon.fileutil import *\nimport json\nimport collections\nfrom dateutil.parser import parse\nimport datetime\nimport nltk\nfrom sklearn.externals import joblib\n\nclass NgramModel:\n    \n    def __init__(self,key=None,secret=None):\n        self.userItems = {}\n        self.count = 0\n        self.fu = FileUtil(aws_key=key,aws_secret=secret)\n        self.epoch = datetime.datetime.utcfromtimestamp(0)\n\n    def unix_time(self,dt):\n        delta = dt - self.epoch\n        return delta.total_seconds()\n\n    def collect_sessions(self,line):\n        if len(line) > 0:\n            j = json.loads(line)\n            userId = j[\'userid\']\n            itemId = j[\'itemid\']\n            itemId = int(itemId)\n            if itemId > 0:\n                if not userId in self.userItems:\n                    self.userItems[userId] = []\n                dt = parse(j[\'timestamp_utc\'])\n                naiveDt = dt.replace(tzinfo=None)\n                epoch = self.unix_time(naiveDt)\n                self.userItems[userId].append((itemId,epoch))\n                self.count += 1\n                if self.count % 10000 == 0:\n                    print ""Processed "",self.count\n                    sys.stdout.flush()\n\n    def stream(self,bucket,day):\n        filePath = ""s3://""+bucket+""/""+args.client+""/actions/""+str(day)+""/part""\n        print filePath\n        print args.bucket\n        self.fu.stream_multi([filePath],self.collect_sessions)\n        print ""total users so far "",len(self.userItems)\n\n    def save_sessions(self,filename):\n        with open(filename,""w"") as f:\n            processed = 0\n            for user in self.userItems:\n                sessions = self.userItems[user]\n                if len(sessions) > 1:\n                    processed += 1\n                    sessions = sorted(sessions,key=lambda x: x[1])\n                    items = [item for (item,_) in sessions]\n                    flist = []\n                    last = None\n                    for item in items:\n                        if last is None:\n                            flist.append(item)\n                        elif not item == last:\n                            flist.append(item)\n                        last = item\n                    if len(flist) > 1:\n                        f.write("" "".join(str(x) for x in flist))\n                        f.write(""\\n"")\n            f.close()\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'create corpus\')\n    parser.add_argument(\'--client\', help=\'client\', required=True)\n    parser.add_argument(\'--corpus\', help=\'corpus file to create\', required=True)\n    parser.add_argument(\'--bucket\', help=\'bucket\', required=True)\n    parser.add_argument(\'--startDay\', help=\'day to start\' , type=int, required=True)\n    parser.add_argument(\'--numDays\', help=\'number of days to get data for\' , type=int, default=1)\n    parser.add_argument(\'--aws_key\', help=\'aws key - needed if input or output is on AWS and no IAM\')\n    parser.add_argument(\'--aws_secret\', help=\'aws secret - needed if input or output on AWS  and no IAM\')\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    if \'aws_key\' in opts:\n        model = NgramModel(key=args.aws_key,secret=args.aws_secret)\n    else:\n        model = NgramModel()\n    for day in range(args.startDay-args.numDays+1,args.startDay+1):\n        print ""streaming day "",day\n        model.stream(args.bucket,day)\n        model.save_sessions(args.corpus)\n'"
docker/pyseldon/scripts/start_prediction_default_rpc_microservice.py,0,"b'from concurrent import futures\nimport time\nimport sys, getopt, argparse\nimport seldon.pipeline.util as sutl\nimport random\nimport grpc\nimport google.protobuf\nfrom google.protobuf import any_pb2\nimport pandas as pd \nfrom seldon.microservice.rpc import CustomDataHandler\nfrom seldon.microservice import Microservices\n\nif __name__ == ""__main__"":\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n\n    parser = argparse.ArgumentParser(prog=\'microservice\')\n    parser.add_argument(\'--model-name\', help=\'name of model\', required=True)\n    parser.add_argument(\'--pipeline\', help=\'location of prediction pipeline\', required=True)\n    parser.add_argument(\'--aws-key\', help=\'aws key\', required=False)\n    parser.add_argument(\'--aws-secret\', help=\'aws secret\', required=False)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    m = Microservices(aws_key=args.aws_key,aws_secret=args.aws_secret)\n    m.create_prediction_rpc_microservice(args.pipeline,args.model_name)\n\n\n'"
docker/pyseldon/scripts/start_prediction_microservice.py,0,"b'import sys, getopt, argparse\nfrom seldon.microservice import Microservices\n\nif __name__ == ""__main__"":\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n\n    parser = argparse.ArgumentParser(prog=\'microservice\')\n    parser.add_argument(\'--model_name\', help=\'name of model\', required=True)\n    parser.add_argument(\'--pipeline\', help=\'location of prediction pipeline\', required=True)\n    parser.add_argument(\'--aws_key\', help=\'aws key\', required=False)\n    parser.add_argument(\'--aws_secret\', help=\'aws secret\', required=False)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    m = Microservices(aws_key=args.aws_key,aws_secret=args.aws_secret)\n    app = m.create_prediction_microservice(args.pipeline,args.model_name)\n\n    app.run(host=""0.0.0.0"", debug=False)\n\n'"
docker/pyseldon/scripts/start_recommendation_microservice.py,0,"b'import sys, getopt, argparse\nfrom seldon.microservice import Microservices\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(prog=\'microservice\')\n    parser.add_argument(\'--recommender\', help=\'location of recommender\', required=True)\n    parser.add_argument(\'--aws_key\', help=\'aws key\', required=False)\n    parser.add_argument(\'--aws_secret\', help=\'aws secret\', required=False)\n    parser.add_argument(\'--memcache_servers\', help=\'memcache servers\', required=False)\n    parser.add_argument(\'--memcache_pool_size\', help=\'memcache servers pool size\', required=False, default=2, type=int)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    m = Microservices(aws_key=args.aws_key,aws_secret=args.aws_secret)\n    app = m.create_recommendation_microservice(args.recommender,memcache_servers=args.memcache_servers,memcache_pool_size=args.memcache_pool_size)\n\n    app.run(host=""0.0.0.0"", debug=False)\n\n'"
docker/stream-itemsim/scripts/itemsim-kafka-to-mysql.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nimport json\nfrom kafka import KafkaConsumer\nimport MySQLdb\n\nINSERT_SQL = ""insert into item_similarity_new (item_id,item_id2,score) values (%(item_id)s,%(item_id2)s,%(score)s)""\nADD_PREV_RESULTS_SQL = ""insert ignore into item_similarity_new select item_id,item_id2,score-0.01 from item_similarity""\nREMOVE_OLD_RESULTS_SQL = ""delete from item_similarity_new where score<0""\nRENAME_TABLE_SQL = ""rename table item_similarity to item_similarity_tmp,item_similarity_new to item_similarity,item_similarity_tmp to item_similarity_new""\nBATCH_SIZE = 3000\n\nclass KafkaToMysql(object):\n\n    def __init__(self,db_settings,client_name):\n        self.db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name)\n\n    def begin_insert(self):\n        self.dbc = self.db.cursor()\n        self.dbc.execute(""truncate item_similarity_new"")\n        self.inserts = []\n        self.batch = 0\n\n    def add_similarity(self,item1,item2,score,time):\n        self.inserts.append({""item_id"":item1,""item_id2"":item2,""score"":score})\n        if len(self.inserts) > BATCH_SIZE:\n            self.batch += 1\n            self.run_inserts(self.inserts,self.batch)\n            self.inserts = []\n\n    def end_insert(self):\n        if len(self.inserts) > 0:\n            self.batch += 1\n            self.run_inserts(self.inserts,self.batch)\n        self.dbc.execute(ADD_PREV_RESULTS_SQL)\n        self.dbc.execute(REMOVE_OLD_RESULTS_SQL)\n        self.dbc.execute(RENAME_TABLE_SQL)\n        self.db.commit()\n        self.dbc.close()\n\n    def run_inserts(self,inserts,batch):\n        logger.info(""inserting batch %d into the db"",batch)\n        self.dbc.executemany(INSERT_SQL, inserts)\n\n    def run(self,kafka_server,topic):\n        state = ""WAITING""\n        consumer = KafkaConsumer(topic,bootstrap_servers=kafka_server)\n        for msg in consumer:\n            logger.info(msg)\n            (time,item1,item2,score) = msg.value.split("","")\n            time = int(time)\n            if time == 0:\n                if state == ""WAITING"":\n                    self.begin_insert()\n                    state = ""INSERTING""\n                elif state == ""INSERTING"":\n                    self.end_insert()\n                    state = ""WAITING""\n                else:\n                    logger.info(""bad state! found start/end marking but state is %s"",state)\n            elif state == ""INSERTING"":\n                item1 = int(item1)\n                item2 = int(item2)\n                score = float(score)\n                self.add_similarity(item1,item2,score,time)\n            else:\n                logger.info(""bad state! found something to insert but state is %s"",state)\n            logger.info(""STATE is now %s"",state)\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--kafka\', help=\'kafka server\', default=""localhost:9092"")\n    parser.add_argument(\'--topic\', help=\'topic\', required=True)\n    parser.add_argument(\'--mysql-host\', help=\'mysql host\', required=True)\n    parser.add_argument(\'--mysql-user\', help=\'mysql user\', required=True)\n    parser.add_argument(\'--mysql-password\', help=\'mysql password\', required=True)\n    parser.add_argument(\'--client\', help=\'seldon client\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    db_settings = {}\n    db_settings[\'host\'] = args.mysql_host\n    db_settings[\'user\'] = args.mysql_user\n    db_settings[\'password\'] = args.mysql_password\n\n    k = KafkaToMysql(db_settings,args.client)\n    k.run(args.kafka,args.topic)\n\n'"
python/docs/exts/ghpages.py,0,"b'""""""\n:Description: Sphinx extension to remove leading under-scores from directories names in the html build output directory.\n""""""\nimport os\nimport shutil\n\n\ndef setup(app):\n    """"""\n    Add a html-page-context  and a build-finished event handlers\n    """"""\n    app.connect(\'html-page-context\', change_pathto)\n    app.connect(\'build-finished\', move_private_folders)\n                \ndef change_pathto(app, pagename, templatename, context, doctree):\n    """"""\n    Replace pathto helper to change paths to folders with a leading underscore.\n    """"""\n    pathto = context.get(\'pathto\')\n    def gh_pathto(otheruri, *args, **kw):\n        if otheruri.startswith(\'_\'):\n            otheruri = otheruri[1:]\n        return pathto(otheruri, *args, **kw)\n    context[\'pathto\'] = gh_pathto\n    \ndef move_private_folders(app, e):\n    """"""\n    remove leading underscore from folders in in the output folder.\n    \n    :todo: should only affect html built\n    """"""\n    def join(dir):\n        return os.path.join(app.builder.outdir, dir)\n    \n    for item in os.listdir(app.builder.outdir):\n        if item.startswith(\'_\') and os.path.isdir(join(item)):\n            shutil.move(join(item), join(item[1:]))\n'"
python/seldon/anomaly/AnomalyDetection.py,0,"b'import numpy as np\nimport pandas as pd\nimport scipy.spatial.distance as ssd\nfrom sklearn.utils import check_array\nimport logging\nfrom time import time\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.INFO)\nformatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\nlogger.propagate = False\n\nclass iNNEDetector(object):\n    """"""\n    Create an ensemble classifier for anomaly detection based on iNNE method (cite iNNE paper)\n    \n    Parameters\n    ----------\n    ensemble_size : int\n        Number of ensembles for the classifier\n    sample_size : int\n        Number of samples on each ensemble\n    metric : str\n        Metric used by iNNE. Default \'euclid\'\n    verbose : bool\n        default True\n    """"""\n    def __init__(self,ensemble_size=100,sample_size=32,metric=\'euclid\',verbose=True):\n        self.ensemble_size = ensemble_size\n        self.sample_size = sample_size\n        self.metric = metric\n        self.verbose = verbose\n        \n    def _D(self,x,y,metric):\n        """""" \n        Calculates the distance between x and y according to metric \'metric\'\n        \n        Parameters\n        ----------\n        \n        x : numpy array \n            1-d vector of dimension d\n        y : numpy array\n            1-d vector of dimension d\n        metric: str \n            specify the metric used (default euclidian metric)\n        \n        Returns\n        -------\n        \n        D(x | y) : Distance between x and y according to metric \n        """"""\n        \n        if metric == \'euclid\' or metric == \'Euclid\':\n            return np.linalg.norm(x-y)\n        if metric == \'kolmogorov\' or metric == \'Kolmogorov\':\n            #check normalization\n            norm_x = np.around(np.linalg.norm(x),decimals=10)\n            norm_y = np.around(np.linalg.norm(y),decimals=10)\n            if norm_x == 1 and norm_y == 1:\n                return np.sqrt(1 - np.around(np.absolute(np.dot(x,y))),decimals=10)\n            else:\n                raise NameError(\'%s metric supports only normalized vectors\'\n                                % metric)\n        if metric == \'chebyshev\' or metric == \'Chebyshev\':\n            return ssd.chebyshev(x,y)\n\n        else:\n            raise NameError(\'%s metric not supported\'\n                            % metric)\n\n    def _generate_spheres(self,X_s):\n        """""" \n        Generates set of hyperspheres from sample X_s\n\n        Parameters\n        ----------\n\n        X_s : numpy array \n            dimensions: sample_size X nb_features\n\n        Returns\n        -------\n        \n        spheres : list \n            list of tuples storing sphere\'s center, radius and nearest neighbour index\n        \n        """"""\n        spheres = []\n        for i in range(X_s.shape[0]):\n            k = int(np.random.randint(X_s.shape[0],size=1))\n            while k==i:\n                k = int(np.random.randint(X_s.shape[0],size=1))\n            radius = self._D(X_s[i],X_s[k],self.metric)\n            nn_index = k\n            for j in range(X_s.shape[0]):\n                if self._D(X_s[i],X_s[j],self.metric) < radius and j!=i:\n                    radius = self._D(X_s[i],X_s[j],self.metric)\n                    nn_index = j\n            spheres.append((X_s[i], radius, nn_index))\n        \n        return spheres\n\n    def _score(self,y,spheres):\n        """"""\n        Returns the anomaly score for vector y based on the given  set of spheres\n          \n        Parameters\n        ----------\n\n        y : numpy array\n            1-d vector of dimension d to score\n        spheres : list\n            list of 3-d tuples where each tuple contain sphere center, radius and nearest neighbour index\n\n        Returns\n        -------\n\n        score : float\n            anomaly score\n        """"""\n        spheres_in=[]\n        for sphere in spheres:\n            if self._D(y,sphere[0],self.metric) <= sphere[1]:\n                spheres_in.append(sphere)\n        if len(spheres_in) == 0:\n            B = ()\n        elif len(spheres_in) != 0:\n            B = spheres_in[int(np.random.randint(len(spheres_in),size=1))]\n            for sphere_in in spheres_in:\n                if sphere_in[1] < B[1]:\n                  B = sphere_in\n                        \n        if B == ():\n            score = 1\n        else:\n            score = 1 - (float(spheres[B[2]][1])/float(B[1]))\n    \n        return score\n  \n    def fit(self,X,y=None):\n        """""" \n        Generates sets of hyper-spheres for anomaly scores \n        \n        Parameters\n        ----------\n        \n        X : numpy array (nb_samples, nb_features)\n            data set\n    \n        Returns\n        -------\n        \n        self\n        """"""\n        t_0 = time()\n        \n        check_array(X)\n                 \n        self._sets_of_spheres = []\n        if self.verbose:\n            logger.info(\'generating sets of spheres...\')\n        for j in range(self.ensemble_size):\n            X_s = np.random.permutation(X)[:self.sample_size,:]\n            spheres = self._generate_spheres(X_s)\n            self._sets_of_spheres.append(spheres)\n        t_f = time() - t_0\n        m,s = divmod(t_f, 60)\n        h,m = divmod(m, 60)\n        if self.verbose:\n            logger.info(\'Total run time: %i:%i:%i\'\n                        % (h,m,s))\n\n        return self\n\n\n    def fit_transform(self,X,y=None):\n        """""" \n        Generates sets of hyper-spheres for anomaly scores \n        \n        Parameters\n        ----------\n        \n        X : numpy array (nb_samples, nb_features)\n            data set\n    \n        Returns\n        -------\n        \n        self\n        """"""\n        t_0 = time()\n        \n        check_array(X)\n                 \n        self._sets_of_spheres = []\n        if self.verbose:\n            logger.info(\'generating sets of spheres...\')\n        for j in range(self.ensemble_size):\n            X_s = np.random.permutation(X)[:self.sample_size,:]\n            spheres = self._generate_spheres(X_s)\n            self._sets_of_spheres.append(spheres)\n        t_f = time() - t_0\n        m,s = divmod(t_f, 60)\n        h,m = divmod(m, 60)\n        if self.verbose:\n            logger.info(\'Total run time: %i:%i:%i\'\n                        % (h,m,s))\n\n        return self\n      \n    def fit_score(self,X,y=None):\n        """"""\n        Generate set of hyper-sphere and return anomaly score for all points in dataset\n\n        Parameters\n        ----------\n\n        X : numpy array\n            data set\n\n        Return\n        ------\n\n        scores : numpy array\n            1-d vector with the anomaly scores for all data points\n        """"""\n        t_0 = time()\n\n        check_array(X)\n   \n        self._sets_of_spheres = []\n        if self.verbose:\n            logger.info(\'generating sets of spheres...\')\n        for j in range(self.ensemble_size):\n            X_s = np.random.permutation(X)[:self.sample_size,:]\n            spheres = self._generate_spheres(X_s)\n            self._sets_of_spheres.append(spheres)\n    \n        scores = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            if i % 1000 == 0 and self.verbose:\n                logger.info(\'Getting anomaly score for data point %i\'\n                            % i)\n                logger.info(\'X shape: %i X %i\'\n                            % X.shape)\n            scores_i = []\n            j=0\n            for spheres in self._sets_of_spheres:\n                score = self._score(X[i],spheres)\n                if i % 1000 == 0 and j % 10 ==0 and self.verbose:\n                    logger.info(\'Anomaly score for data point %i from estimator %i: %f\'\n                                % (i,j,score))\n                scores_i.append(score)\n                j+=1\n            scores[i] = np.mean(scores_i)\n\n        if \'X_scored\' not in dir(self):\n            self.X_scored = np.column_stack((X,scores))\n        \n        t_f = time() - t_0\n        m,s = divmod(t_f, 60)\n        h,m = divmod(m, 60)\n        if self.verbose:\n            logger.info(\'Total run time: %i:%i:%i\'\n                        % (h,m,s))\n\n        return scores\n    \n    def get_all_scores(self):\n        """""" \n        Returns the dataset with the anomaly scores stored in the last column\n    \n        Parameters\n        ----------\n        \n        None\n        \n        Returns\n        -------\n\n        X_scored : numpy array \n            the dataset with anomaly scores stored in the last column\n        """"""\n        if \'X_scored\' in dir(self):\n            return self.X_scored\n        else:\n            raise NameError(\'method get_all_scores returns scores only if method fit_score has been previously called\')\n            return self\n\n\n    def get_score(self,X):\n        """""" \n        Calculates the anomaly score for a new data point X\n    \n        Parameters\n        ----------\n\n        y : numpy array\n            1-d vector to score\n\n        Returns\n        -------\n        \n        score : tuple\n            tuple where first element is the anomaly score and the second element is True if the point is lab            elled as anomalous and False if is labelled as non-anomalous based on the decision threshold\n        """"""\n        if X.ndim == 1:\n            s = np.zeros(2)\n            scores = []\n            for spheres in self._sets_of_spheres:\n                score_s = self._score(X,spheres)\n                scores.append(score_s)\n            score_mean = np.mean(scores)\n            s[0]=score_mean\n            s[1]=1-score_mean\n            return s\n        elif X.ndim == 2:\n            s = np.zeros((X.shape[0],2))\n            for i in range(X.shape[0]):\n                scores = []\n                for spheres in self._sets_of_spheres:\n                    score_s = self._score(X,spheres)\n                    scores.append(score_s)\n                score_mean = np.mean(scores)\n                s[i,0] = score_mean\n                s[i,1] = 1-score_mean\n            return s\n        \n    def get_anomalies(self,decision_threshold=1):\n        """"""\n        Returns the data points whose anomaly score is above the decision_threshold\n\n        Parameters\n        ----------\n\n        decition_threshold : float\n            anomaly decision threshold. Default 0.5\n\n        Returns\n        -------\n\n        X_anom: numpy array (nb_anomalies, nb_features + 1)\n            anomalous data points with anomaly scores stored in the last column\n        """"""\n        if \'X_scored\' in dir(self):\n            X_tmp = self.X_scored[:,:-1]\n            scores_tmp = self.X_scored[:,-1]\n            X_an = X_tmp[scores_tmp>=decision_threshold]\n            anom_scores = scores_tmp[scores_tmp>=decision_threshold]\n            self.X_anom = np.column_stack((X_an,anom_scores))\n            return self.X_anom\n        else:    \n            raise NameError(\'method get_anomalies returns scores only if method fit_score has been previously called\')\n            return self\n\n\n'"
python/seldon/anomaly/__init__.py,0,b''
python/seldon/cli/__init__.py,0,b'def start_seldoncli():\n    import cli_main\n    cli_main.main()\n\n'
python/seldon/cli/attr_schema_utils.py,0,"b'import json\nimport MySQLdb\nimport getopt, argparse\n\nimport pprint\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\nvalid_value_types = set([\'double\', \'string\', \'date\', \'text\', \'int\',\'boolean\'])\nvalue_types_to_db_map = dict(double=\'DOUBLE\', string=\'VARCHAR\', date=\'DATETIME\', int=\'INT\', boolean=\'BOOLEAN\',\n\ttext=\'TEXT\', enum=\'ENUM\')\n\ndef addAttrsToDb(db, attrs, item_type):\n\tattrs.append({""name"":""content_type"", ""value_type"":[""article""]})\n\tfor attr in attrs:\n\t\tattrValType = attr[\'value_type\']\n\t\tif type(attrValType) is list:\n\t\t\tattrValType = \'enum\'\n\t\tcur = db.cursor()\n\t\tcur.execute(""INSERT INTO ITEM_ATTR (name, type, item_type) ""\n\t\t\t+ "" VALUES (%s, %s, %s)"", (attr[\'name\'], value_types_to_db_map[attrValType], item_type))\n\t\tif attrValType is \'enum\':\n\t\t\tfor index,enum in enumerate(attr[\'value_type\'], start=1):\n\t\t\t\tcur = db.cursor()\n\t\t\t\tcur.execute(""SELECT attr_id FROM ITEM_ATTR WHERE NAME = %s and ITEM_TYPE = %s"", (attr[\'name\'],item_type))\n\t\t\t\trows = cur.fetchall()\n\t\t\t\tattrId = rows[0][0]\n\t\t\t\tcur = db.cursor()\n\t\t\t\tcur.execute(""INSERT INTO ITEM_ATTR_ENUM (attr_id, value_id, value_name) VALUES (%s, %s, %s)"",(attrId, index, enum))\n\tcur = db.cursor()\n\n\tcur.execute(""SELECT attr_id, value_id, value_name FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tfor row in rows:\n\t\tenum_attr_id = row[0]\n\t\tenum_value_id = row[1]\n\t\tenum_value_name = row[2]\n\t\tcur = db.cursor()\n\t\tcur.execute(""INSERT INTO DIMENSION (item_type, attr_id, value_id) VALUES""\n\t\t\t+ "" (%s, %s, %s)"", (item_type, enum_attr_id, enum_value_id))\n\ndef doDbChecks(db):\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_TYPE"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_TYPE table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_ATTR table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""ITEM_ATTR_ENUM table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM DIMENSION"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""DIMENSION table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\n\ndef doExitBecauseDbNotEmpty():\n\tprint ""To run this script, the relevant DB tables must be empty. Please rerun this script with the -clean option to delete these entries.""\n\texit(1)\n\ndef addToDb(db, types):\n\twith db:\n\t\tdoDbChecks(db)\n\t\tfor theType in types:\n\t\t\tcur= db.cursor()\n\t\t\tcur.execute(""INSERT INTO ITEM_TYPE (type_id, name)""+\n\t\t\t\t"" values (%s, %s)"",(theType[\'type_id\'],theType[\'type_name\']))\n\t\t\taddAttrsToDb(db, theType[\'type_attrs\'], theType[\'type_id\'])\n\n\ndef validateValueType(valType):\n    theType = type(valType)\n    if theType is list:\n        for enum in valType:\n            theEnumType = type(enum)\n            if theEnumType is not unicode and theEnumType is not str:\n                print ""enum objects must be strings:"", theEnumType\n                exit(1)\n    elif theType is unicode:\n        if valType not in valid_value_types:\n            print ""the value type must be one of \'double\', \'string\', \'date\' or an object""\n            exit(1)\n    else:\n        print ""the type of the field value_type must be a string or a list where as it was"",theType\n        exit(1)\n\ndef validateAttr(theAttr):\n    if \'name\' not in theAttr or \'value_type\' not in theAttr:\n        print ""couldn\'t find one of (name, value_type) for attr ""\n        pp(theAttr)\n        exit(1)\n    else:\n        validateValueType(theAttr[\'value_type\']);\n\ndef validateType(theType):\n    if \'type_id\' not in theType or \'type_name\' not in theType or \'type_attrs\' not in theType:\n        print ""couldn\'t find one of (type_id, type_name, type_attrs) for object""\n        pp(theType)\n        exit(1)\n    for theAttr in theType[\'type_attrs\']:\n        validateAttr(theAttr)\n\ndef validateNumbering(types):\n    ids = set()\n    for theType in types:\n        if isinstance(theType[\'type_id\'], int):\n            if theType[\'type_id\'] in ids:\n                print ""found a repeated type_id"", theType[\'type_id\']\n                exit(1)\n            else:\n                ids.add(theType[\'type_id\'])\n        else:\n            print ""type_id s must be integers but one was"",""\\"""",theType[\'type_id\'],""\\""""\n            exit(1)\n\ndef outputDimensionsToFile(file, db):\n\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""SELECT d.dim_id, e.value_name from DIMENSION d, ITEM_ATTR_ENUM e where d.attr_id = e.attr_id and d.value_id = e.value_id and e.value_name != \\\'article\\\'"")\n\t\trows = cur.fetchall()\n\t\tjson.dump(rows, file)\n\ndef outputDimensions(db):\n\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""SELECT d.dim_id, e.value_name from DIMENSION d, ITEM_ATTR_ENUM e where d.attr_id = e.attr_id and d.value_id = e.value_id and e.value_name != \\\'article\\\'"")\n\t\trows = cur.fetchall()\n                print ""{h_dim_id:>8} {h_value_name}"".format(h_dim_id=""dim_id"",h_value_name=""value_name"")\n                for row in rows:\n                    print ""{v_dim_id:>8} {v_value_name}"".format(v_dim_id=row[0], v_value_name=row[1])\n\ndef readTypes(types):\n    for theType in types:\n        validateType(theType)\n    validateNumbering(types)\n    return types\n\ndef clearUp(db):\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""TRUNCATE TABLE ITEMS"")\n\t\tcur.execute(""TRUNCATE TABLE DIMENSION"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR_ENUM"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_TYPE"")\n\t\tcur.execute(\'truncate table users\')\n\t\tcur.execute(\'truncate table items\')\n\t\tcur.execute(\'truncate table item_map_varchar\')\n\t\tcur.execute(\'truncate table item_map_double\')\n\t\tcur.execute(\'truncate table item_map_datetime\')\n\t\tcur.execute(\'truncate table item_map_int\')\n\t\tcur.execute(\'truncate table item_map_boolean\')\n\t\tcur.execute(\'truncate table item_map_enum\')\n\t\tcur.execute(\'truncate table item_map_text\')\n\ndef create_schema(client_name, dbSettings, scheme_file_path, clean=False):\n\n    if clean != True:\n        json_data=open(scheme_file_path)\n        data = json.load(json_data)\n        if \'types\' not in data:\n            print ""couldn\'t find types object in json""\n            return\n        else:\n            types = readTypes(data[\'types\'])\n\n    db = MySQLdb.connect(user=dbSettings[""user""],db=client_name,passwd=dbSettings[""password""], host=dbSettings[""host""])\n    if clean == True:\n        clearUp(db)\n        print ""Finished cleaning attributes successfully""\n    else:\n        addToDb(db, types)\n        #f = open(\'dimensions.json\',\'w\')\n        #outputDimensionsToFile(f,db)\n        outputDimensions(db)\n\n        print \'Finished applying attributes successfully\'\n\n        json_data.close()\n\ndef show_dimensions(client_name, dbSettings):\n    db = MySQLdb.connect(user=dbSettings[""user""],db=client_name,passwd=dbSettings[""password""], host=dbSettings[""host""])\n    outputDimensions(db)\n\n'"
python/seldon/cli/cli_main.py,0,"b'import sys\nimport argparse\nimport pprint\nimport os\nimport json\nfrom kazoo.client import KazooClient\nimport errno\nimport logging\n\nimport cmd_memcached\nimport cmd_db\nimport cmd_client\nimport cmd_attr\nimport cmd_import\nimport cmd_alg\nimport cmd_model\nimport cmd_pred\nimport cmd_keys\nimport cmd_api\nimport cmd_rpc\nimport cmd_rec_exp\n\ngdata = {\n    \'zk_client\': None,\n    \'conf_data\': None,\n    \'conf_path\': os.path.expanduser(""~/.seldon/seldon.conf""),\n    \'cmds\': {\n        \'memcached\' : cmd_memcached.cmd_memcached,\n        \'db\' : cmd_db.cmd_db,\n        \'client\' : cmd_client.cmd_client,\n        \'attr\' : cmd_attr.cmd_attr,\n        \'import\' : cmd_import.cmd_import,\n        \'rec_alg\' : cmd_alg.cmd_alg,\n        \'predict_alg\' : cmd_pred.cmd_pred,\n        \'keys\' : cmd_keys.cmd_keys,\n        \'model\': cmd_model.cmd_model,\n        \'api\': cmd_api.cmd_api,\n        \'rpc\': cmd_rpc.cmd_rpc,\n        \'rec_exp\' : cmd_rec_exp.cmd_rec_exp,\n    }\n}\n\ndef get_default_conf():\n    return \'\'\'\\\n{\n    ""default_predictors"": {\n        ""externalPredictionServer"": {\n            ""config"": []\n        }\n    },\n\n    ""default_algorithms"": {\n        ""assocRuleRecommender"": {\n            ""zk_activate_node"" : ""/config/assocrules"",\n            ""config"": []\n        },\n        ""dynamicClusterCountsRecommender"": {\n            ""zk_activate_node"" : ""/config/userclusters"",\n            ""config"": []\n        },\n        ""externalItemRecommendationAlgorithm"": {\n            ""config"": [\n                {\n                    ""name"": ""io.seldon.algorithm.inclusion.itemsperincluder"",\n                    ""value"": 1000\n                }\n            ],\n            ""includers"": [\n                ""recentItemsIncluder""\n            ]\n        },\n        ""globalClusterCountsRecommender"": {\n            ""zk_activate_node"" : ""/config/userclusters"",\n            ""config"": []\n        },\n        ""itemCategoryClusterCountsRecommender"": {\n            ""zk_activate_node"" : ""/config/userclusters"",\n            ""config"": []\n        },\n        ""itemClusterCountsRecommender"": {\n            ""zk_activate_node"" : ""/config/userclusters"",\n            ""config"": []\n        },\n        ""itemSimilarityRecommender"": {\n            ""config"": []\n        },\n        ""mfRecommender"": {\n            ""zk_activate_node"" : ""/config/mf"",\n            ""config"": []\n        },\n        ""mostPopularRecommender"": {\n            ""config"": []\n        },\n        ""recentItemsRecommender"": {\n            ""config"": []\n        },\n        ""recentMfRecommender"": {\n            ""zk_activate_node"" : ""/config/mf"",\n            ""config"": [\n                {\n                    ""name"": ""io.seldon.algorithm.general.numrecentactionstouse"",\n                    ""value"": ""1""\n                }\n            ]\n        },\n        ""semanticVectorsRecommender"": {\n            ""zk_activate_node"" : ""/config/svtext"",\n            ""config"": []\n        },\n        ""userTagAffinityRecommender"": {\n            ""zk_activate_node"" : ""/config/tagaffinity"",\n            ""config"": []\n        }\n    },\n    ""default_models"": {\n        ""cluster-by-dimension"": {},\n        ""matrix-factorization"": {\n            ""config"": {\n                ""activate"": true,\n                ""alpha"": 1,\n                ""days"": 1,\n                ""inputPath"": ""%SELDON_MODELS%"",\n                ""iterations"": 5,\n                ""lambda"": 0.01,\n                ""outputPath"": ""%SELDON_MODELS%"",\n                ""rank"": 30,\n                ""startDay"": 1\n            },\n            ""training"": {\n                ""job_info"": {\n                    ""cmd"": ""%SPARK_HOME%/bin/spark-submit"",\n                    ""cmd_args"": [\n                        ""--class"",\n                        ""io.seldon.spark.mllib.MfModelCreation"",\n                        ""--executor-memory"",\n                        ""%SPARK_EXECUTOR_MEMORY%"",\n                        ""--driver-memory"",\n                        ""%SPARK_DRIVER_MEMORY%"",\n                        ""--master"",\n                        ""spark://spark-master:7077"",\n                        ""%SELDON_SPARK_HOME%/seldon-spark-%SELDON_VERSION%-jar-with-dependencies.jar"",\n                        ""--client"",\n                        ""%CLIENT_NAME%"",\n                        ""--zookeeper"",\n                        ""%ZK_HOSTS%""\n                    ]\n                },\n                ""job_type"": ""spark""\n            }\n        },\n        ""semvec"": {},\n        ""similar-items"": {\n            ""config"": {\n                ""activate"": true,\n                ""days"": 1,\n                ""inputPath"": ""%SELDON_MODELS%"",\n                ""outputPath"": ""%SELDON_MODELS%"",\n                ""startDay"": 1,\n\t\t""itemType"" : -1,\n\t\t""limit"" : 100,\n\t\t""minItemsPerUser"" : 0,\n\t\t""minUsersPerItem"" : 0,\n\t\t""maxUsersPerItem"" : 2000000,\n\t\t""dimsumThreshold"" : 0.1,\n\t\t""sample"" : 1.0\n            },\n            ""training"": {\n                ""job_info"": {\n                    ""cmd"": ""%SPARK_HOME%/bin/spark-submit"",\n                    ""cmd_args"": [\n                        ""--class"",\n                        ""io.seldon.spark.mllib.SimilarItems"",\n                        ""--executor-memory"",\n                        ""%SPARK_EXECUTOR_MEMORY%"",\n                        ""--driver-memory"",\n                        ""%SPARK_DRIVER_MEMORY%"",\n                        ""--master"",\n                        ""spark://spark-master:7077"",\n                        ""%SELDON_SPARK_HOME%/seldon-spark-%SELDON_VERSION%-jar-with-dependencies.jar"",\n                        ""--client"",\n                        ""%CLIENT_NAME%"",\n                        ""--zookeeper"",\n                        ""%ZK_HOSTS%""\n                    ]\n                },\n                ""job_type"": ""spark""\n\t\t}\n\t},\n        ""tagaffinity"": {},\n        ""tagcluster"": {}\n    },\n    ""processactions"": {\n        ""job_info"": {\n            ""cmd"": ""%SPARK_HOME%/bin/spark-submit"",\n            ""cmd_args"": [\n                ""--class"",\n                ""io.seldon.spark.actions.GroupActionsJob"",\n                        ""--executor-memory"",\n                        ""%SPARK_EXECUTOR_MEMORY%"",\n                        ""--driver-memory"",\n                        ""%SPARK_DRIVER_MEMORY%"",\n                ""--master"",\n                ""spark://spark-master:7077"",\n                ""%SELDON_SPARK_HOME%/seldon-spark-%SELDON_VERSION%-jar-with-dependencies.jar"",\n                ""--input-path-pattern"",\n                ""%SELDON_LOGS%/actions.%y/%m%d/*/*"",\n                ""--output-path-dir"",\n                ""%SELDON_MODELS%"",\n                ""--input-date-string"",\n                ""%INPUT_DATE_STRING%"",\n                ""--gzip-output""\n            ]\n        },\n        ""job_type"": ""spark""\n    },\n    ""processevents"": {\n        ""job_info"": {\n            ""cmd"": ""%SPARK_HOME%/bin/spark-submit"",\n            ""cmd_args"": [\n                ""--class"",\n                ""io.seldon.spark.events.ProcessEventsJob"",\n                        ""--executor-memory"",\n                        ""%SPARK_EXECUTOR_MEMORY%"",\n                        ""--driver-memory"",\n                        ""%SPARK_DRIVER_MEMORY%"",\n                ""--master"",\n                ""spark://spark-master:7077"",\n                ""%SELDON_SPARK_HOME%/seldon-spark-%SELDON_VERSION%-jar-with-dependencies.jar"",\n                ""--input-path-pattern"",\n                ""%SELDON_LOGS%/events.%y/%m%d/*/*"",\n                ""--output-path-dir"",\n                ""%SELDON_MODELS%"",\n                ""--input-date-string"",\n                ""%INPUT_DATE_STRING%"",\n                ""--gzip-output""\n            ]\n        },\n        ""job_type"": ""spark""\n    },\n    ""seldon_logs"": ""/seldon-data/logs"",\n    ""seldon_models"": ""/seldon-data/seldon-models"",\n    ""seldon_spark_home"": ""/home/seldon/libs"",\n    ""seldon_version"": ""__SELDON_VERSION__"",\n    ""server_endpoint"": ""http://seldon-server"",\n    ""spark_home"": ""/opt/spark"",\n    ""zk_hosts"": ""zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181"",\n    ""zkroot"": ""/seldon-data/conf/zkroot"",\n    ""grafana_endpoint"" : ""http://monitoring-grafana"",\n    ""spark_executor_memory"" : ""1g"",\n    ""spark_driver_memory"" : ""1g""\n}\n\'\'\'\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef getOpts():\n    parser = argparse.ArgumentParser(prog=\'seldon-cli\', description=\'Seldon Cli\')\n    parser.add_argument(\'--version\', action=\'store_true\', help=""print the version"", required=False)\n    parser.add_argument(\'--debug\', action=\'store_true\', help=""debugging flag"", required=False)\n    parser.add_argument(\'--zk-hosts\', help=""the zookeeper hosts"", required=False)\n    parser.add_argument(\'--setup-config\', action=\'store_true\', help=""setup the config and exit"", required=False)\n    parser.add_argument(\'--print-default-config\', action=\'store_true\', help=""print the default config and exit"", required=False)\n    parser.add_argument(\'-q\', ""--quiet"", action=\'store_true\', help=""only display important messages, useful in non-interactive mode"")\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args()\n    return opts\n\ndef expand_conf():\n    conf_data = gdata[\'conf_data\']\n\n    if conf_data != None:\n        expansion_list = [""zkroot"",""seldon_models"",""spark_home"",""seldon_spark_home""]\n        for expansion_item in expansion_list:\n            conf_data[expansion_item] = os.path.expanduser(conf_data[expansion_item])\n\ndef create_default_conf():\n    fpath = gdata[\'conf_path\']\n    if not os.path.isfile(fpath):\n        default_conf = get_default_conf()\n        mkdir_p(os.path.dirname(fpath))\n        d = json_to_dict(default_conf)\n        zkroot = os.path.expanduser(d[""zkroot""])\n        mkdir_p(zkroot)\n        seldon_models = os.path.expanduser(d[""seldon_models""])\n        mkdir_p(seldon_models)\n        default_conf = json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \'))\n        f = open(fpath, \'w\')\n        f.write(default_conf)\n        f.write(\'\\n\')\n        f.close()\n        print ""Created conf file [{fpath}] with default settings."".format(**locals())\n        sys.exit(0)\n    else:\n        print ""Existing conf file [{fpath}] found."".format(**locals())\n\ndef check_conf():\n    fpath = gdata[\'conf_path\']\n    if os.path.isfile(fpath):\n        with open(fpath) as data_file:\n            gdata[""conf_data""] = json.load(data_file)\n\ndef start_zk_client(opts):\n    zk_hosts = opts.zk_hosts\n    if not opts.quiet:\n        sys.stdout.write(""connecting to ""+zk_hosts)\n    gdata[""zk_client""] = KazooClient(hosts=zk_hosts)\n    gdata[""zk_client""].start()\n    res = ""SUCCEEDED"" if gdata[""zk_client""].connected else ""FAILED""\n    if not opts.quiet:\n        print "" [{res}]"".format(**locals())\n\ndef stop_zk_client():\n    if gdata[""zk_client""].connected:\n        gdata[""zk_client""].stop()\n\ndef main():\n    logging.basicConfig()\n    check_conf()\n    expand_conf()\n    opts = getOpts()\n    if opts.setup_config:\n        create_default_conf()\n        sys.exit(0)\n    if opts.print_default_config:\n        print dict_to_json(json_to_dict(get_default_conf()), True)\n        sys.exit(0)\n\n    # is the conf still not setup\n    if gdata[\'conf_data\'] == None:\n        print ""No config found, run with \'--setup-config\' to set it up.""\n        sys.exit(1)\n\n    if opts.version == True:\n        from seldon import __version__\n        print __version__\n        sys.exit(0)\n\n    if len(opts.args) < 1:\n        print ""Need command""\n        sys.exit(1)\n\n    if opts.zk_hosts != None:\n        gdata[""conf_data""][""zk_hosts""] = opts.zk_hosts\n    else:\n        opts.zk_hosts = gdata[""conf_data""][""zk_hosts""]\n\n    cmd = opts.args[0]\n    command_args = opts.args[1:]\n\n    cmds = gdata[\'cmds\']\n    if cmds.has_key(cmd):\n        start_zk_client(opts)\n        command_data = {\n                \'conf_data\' : gdata[\'conf_data\'],\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n        }\n        cmds[cmd](opts,command_data, command_args)\n        stop_zk_client()\n    else:\n        print ""Invalid command[{}]"".format(cmd)\n        sys.exit(1)\n\nif __name__ == \'__main__\':\n    main()\n\n'"
python/seldon/cli/cmd_alg.py,0,"b'import pprint\nimport argparse\nimport sys\nimport os\nimport json\nimport errno\n\nimport zk_utils\n\nimport cmd_pred\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\nCONFIG_MICROSERVICE_URL=""io.seldon.algorithm.external.url""\nCONFIG_MICROSERVICE_NAME=""io.seldon.algorithm.external.name""\nEXTERNAL_RECOMMENDER=""externalItemRecommendationAlgorithm""\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli rec_alg\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=True, choices=[\'list\',\'add\',\'delete\',\'show\',\'commit\',\'create\'])\n    parser.add_argument(\'--alg-type\', help=""type of algorithm"", required=False, choices=[\'recommendation\',\'prediction\'], default=\'recommendation\')\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--recommender-name\', help=""the name of recommender"", required=False)\n    parser.add_argument(\'--config\', help=""algorithm specific config in the form x=y"", required=False, action=\'append\')\n    parser.add_argument(\'-f\',\'--json-file\', help=""the json file to use for creating algs or \'-\' for stdin"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\n\ndef ensure_client_has_algs(zk_client):\n\n\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/algs""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_alg_json = \'{""algorithms"":[{""config"":[],""filters"":[],""includers"":[],""name"":""recentItemsRecommender""}],""combiner"":""firstSuccessfulCombiner""}\'\n            data = json_to_dict(default_alg_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, dict_to_json(data))\n\n\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef get_data_from_file(data_fpath):\n    if os.path.isfile(data_fpath):\n        f = open(data_fpath)\n        data = f.read()\n        f.close()\n        return data\n    else:\n        return """"\n\n\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\n\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\n\n\ndef add_model_activate(zkroot,client_name,activate_path):\n    node_fpath = zkroot + activate_path + ""/_data_""\n    data = get_data_from_file(node_fpath).rstrip()\n    if len(data) > 0:\n        clients = data.split(\',\')\n        for client in clients:\n            if client == client_name:\n                return\n    else:\n        clients = []\n    clients.append(client_name)\n    data = "","".join(clients)\n    write_data_to_file(node_fpath,data)\n\n\ndef remove_model_activate(zkroot,client_name,activate_path):\n    node_fpath = zkroot + activate_path + ""/_data_""\n    data = get_data_from_file(node_fpath).rstrip()\n    print ""data is "",data\n    if len(data)>0:\n        clients = data.split(\',\')\n        for client in clients:\n            print ""looking at "",client\n            if client == client_name:\n                clients.remove(client)\n                data = "","".join(clients)\n                write_data_to_file(node_fpath,data)\n\ndef show_algs(data):\n    json = dict_to_json(data, True)\n    print json\n\ndef ensure_client_has_algs(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/algs/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/algs""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_alg_json = \'{""algorithms"":[{""config"":[],""filters"":[],""includers"":[],""name"":""recentItemsRecommender""}],""combiner"":""firstSuccessfulCombiner""}\'\n            data = json_to_dict(default_alg_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, dict_to_json(data))\n\ndef action_show(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the algs for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/alg_rectags/_data_""\n    if os.path.isfile(data_fpath):\n        f = open(data_fpath)\n        json = f.read()\n        f.close()\n        data = json_to_dict(json)\n        show_algs(data)\n    else:\n        print ""Unable to show recommenders definition for client[{client_name}]"".format(**locals())\n\ndef has_config(opts,name):\n    if not opts.config is None:\n        for nv in opts.config:\n            if nv.split(\'=\')[0] == name:\n                return True\n    return False\n\n\ndef action_add(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to add algs for""\n        sys.exit(1)\n\n    recommender_name = opts.recommender_name\n    if recommender_name == None:\n        print ""Need recommender name""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    default_algorithms = command_data[""conf_data""][""default_algorithms""]\n    recommenders = default_algorithms.keys()\n\n    if recommender_name not in recommenders:\n        print ""Invalid recommender[{recommender_name}]"".format(**locals())\n        sys.exit(1)\n\n\n    if recommender_name == EXTERNAL_RECOMMENDER:\n        if not (has_config(opts,CONFIG_MICROSERVICE_URL) and has_config(opts,CONFIG_MICROSERVICE_NAME)):\n            print ""You must supply ""+CONFIG_MICROSERVICE_URL+"" and ""+CONFIG_MICROSERVICE_NAME+"" for ""+EXTERNAL_RECOMMENDER\n            sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n\n    algorithms = data[""algorithms""]\n    includers = default_algorithms[recommender_name][""includers""] if default_algorithms[recommender_name].has_key(""includers"") else []\n    recommender_data = {\n            \'filters\':[],\n            \'includers\': includers,\n            \'name\': recommender_name,\n            \'config\': default_algorithms[recommender_name][""config""]\n    }\n\n\n    if not opts.config is None:\n        for nv in opts.config:\n            (name,value) = nv.split(\'=\')\n            recommender_data[\'config\'].append({""name"":name,""value"":value})\n\n    algorithms.append(recommender_data)\n    write_data_to_file(data_fpath, data)\n\n    if default_algorithms[recommender_name].has_key(""zk_activate_node""):\n        add_model_activate(zkroot,client_name,default_algorithms[recommender_name][""zk_activate_node""])\n\n    print ""Added [{recommender_name}]"".format(**locals())\n    show_algs(data)\n\ndef action_delete(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to add algs for""\n        sys.exit(1)\n\n    recommender_name = opts.recommender_name\n    if recommender_name == None:\n        print ""Need recommender name""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n\n    default_algorithms = command_data[""conf_data""][""default_algorithms""]\n    recommenders = default_algorithms.keys()\n\n    if recommender_name not in recommenders:\n        print ""Invalid recommender[{recommender_name}]"".format(**locals())\n        sys.exit(1)\n\n    algorithms = data[""algorithms""]\n\n    length_before_removal = len(algorithms)\n    def recommender_filter(item):\n        if item[""name""] == recommender_name:\n            return False\n        else:\n            return True\n    filtered_algorithms = filter(recommender_filter, algorithms)\n    length_after_removal = len(filtered_algorithms)\n    data[""algorithms""] = filtered_algorithms\n    if length_after_removal < length_before_removal:\n        write_data_to_file(data_fpath, data)\n\n        if default_algorithms[recommender_name].has_key(""zk_activate_node""):\n            remove_model_activate(zkroot,client_name,default_algorithms[recommender_name][""zk_activate_node""])\n\n        print ""Removed [{recommender_name}]"".format(**locals())\n\ndef action_list(command_data, opts):\n    print ""Default recommenders:""\n    default_algorithms = command_data[""conf_data""][""default_algorithms""]\n    for recommender in default_algorithms:\n        print ""    {recommender}"".format(**locals())\n\ndef action_commit(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to commit data for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/alg_rectags/_data_""\n    if os.path.isfile(data_fpath):\n        f = open(data_fpath)\n        data_json = f.read()\n        f.close()\n\n        zk_client = command_data[""zkdetails""][""zk_client""]\n        node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/alg_rectags""\n        zk_utils.node_set(zk_client, node_path, data_json)\n\n        # activate any required models\n        data = json_to_dict(data_json)\n        recommender_set = set()\n        if data[""defaultStrategy""].has_key(""algorithms""):\n            for alg in data[""defaultStrategy""][""algorithms""]:\n                recommender_set.add(alg[""name""])\n        elif data[""defaultStrategy""].has_key(""variations""):\n            for variation in data[""defaultStrategy""][""variations""]:\n                for alg in variation[""config""][""algorithms""]:\n                    recommender_set.add(alg[""name""])\n        default_algorithms = command_data[""conf_data""][""default_algorithms""]\n        for recommender_name in recommender_set:\n            if default_algorithms[recommender_name].has_key(""zk_activate_node""):\n                print ""activate"",recommender_name\n                node_path = default_algorithms[recommender_name][""zk_activate_node""]\n                node_fpath = zkroot + node_path + ""/_data_""\n                data_models = get_data_from_file(node_fpath)\n                zk_utils.node_set(zk_client, node_path, data_models)\n\n        return\n\n    #TODO remove the following once only using alg_rectags\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    if not os.path.isfile(data_fpath):\n        print ""Data to commit not found!!""\n        sys.exit(1)\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs""\n    zk_utils.node_set(zk_client, node_path, data_json)\n\n    # activate any required models\n\n    default_algorithms = command_data[""conf_data""][""default_algorithms""]\n    data = json_to_dict(data_json)\n    algorithms = data[""algorithms""]\n    print ""algorithms:""\n    for alg in algorithms:\n        alg_name=alg[""name""]\n        if default_algorithms[alg_name].has_key(""zk_activate_node""):\n            node_path = default_algorithms[alg_name][""zk_activate_node""]\n            node_fpath = zkroot + node_path + ""/_data_""\n            data_models = get_data_from_file(node_fpath)\n            zk_utils.node_set(zk_client, node_path, data_models)\n\ndef action_create(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    #check_valid_client_name\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to create algs for""\n        sys.exit(1)\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    #check_valid_json_file\n    json_file_contents = """"\n    json_file = opts.json_file\n    if json_file == None:\n        print ""Need json-file to use for creating algs""\n        sys.exit(1)\n    if json_file == ""-"":\n        json_file_contents = sys.stdin.read()\n    else:\n        if not os.path.isfile(json_file):\n            print ""Unable find file[{json_file}]"".format(**locals())\n            sys.exit(1)\n        f = open(json_file)\n        json_file_contents = f.read()\n        f.close()\n\n    # ensure valid data\n    data = json_to_dict(json_file_contents)\n\n    #do the model activate\n    recommender_set = set()\n    if data[""defaultStrategy""].has_key(""algorithms""):\n        for alg in data[""defaultStrategy""][""algorithms""]:\n            recommender_set.add(alg[""name""])\n    elif data[""defaultStrategy""].has_key(""variations""):\n        for variation in data[""defaultStrategy""][""variations""]:\n            for alg in variation[""config""][""algorithms""]:\n                recommender_set.add(alg[""name""])\n    default_algorithms = command_data[""conf_data""][""default_algorithms""]\n    for recommender_name in recommender_set:\n        if default_algorithms[recommender_name].has_key(""zk_activate_node""):\n            add_model_activate(zkroot,client_name,default_algorithms[recommender_name][""zk_activate_node""])\n\n    #save to zkoot\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/alg_rectags/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef cmd_alg(gopts,command_data, command_args):\n    actions = {\n        ""list"" : action_list,\n        ""show"" : action_show,\n        ""add"" : action_add,\n        ""delete"" : action_delete,\n        ""commit"" : action_commit,\n        ""create"" : action_create,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        print ""Running default list action""\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_api.py,0,"b'import pprint\nimport argparse\nimport os\nimport sys\nimport re\nimport json\nimport MySQLdb\nimport requests\n\nimport db_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli api\', description=\'Seldon CLI\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'call\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=True)\n    parser.add_argument(\'--endpoint\', help=""api to use"", required=True, choices=[\'/js/action/new\',\'/js/recommendations\',\'/js/predict\',\'/js/event/new\',\'/actions\',\'/users/recommendations\',\'/predict\',\'/events\',""/items""])\n    parser.add_argument(\'--method\', help=""http method"", required=False, default=\'GET\', choices=[\'GET\',\'POST\'])\n    parser.add_argument(\'--user\', help=""user"", required=False, default=""1"")\n    parser.add_argument(\'--item\', help=""item"", required=False)\n    parser.add_argument(\'--type\', help=""type"", required=False, default=""1"")\n    parser.add_argument(\'--limit\', help=""limit"", required=False, default=""5"")\n    parser.add_argument(\'--json\', help=""json"", required=False, default=""{}"")\n    parser.add_argument(\'--dimensions\', help=""dimensions"", required=False, action=\'append\')\n    parser.add_argument(\'--full\', help=""whether to return full attributes true|false"", required=False)\n    parser.add_argument(\'--attributes\', help=""attributes"", required=False, action=\'append\')\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\n\ndef get_auth(command_data,opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + ""/config/dbcp/_data_""\n    f = open(data_fpath)\n    jsonStr = f.read()\n    data = json_to_dict(jsonStr)\n    f.close()\n\n    if opts.endpoint.startswith(""/js/""):\n        scope = ""js""\n    else:\n        scope = ""all""\n\n    db_info = None\n    for db_info in data[\'dbs\']:\n        dbSettings = {}\n        dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n        dbSettings[""user""]=db_info[""user""]\n        dbSettings[""password""]=db_info[""password""]\n        dbSettings[""name""] = db_info[""name""]\n        res = db_utils.get_keys(dbSettings,opts.client_name,scope)\n        if len(res)>0:\n            break\n\n    if len(res) == 1:\n        return res[0]\n    else:\n        print ""Failed to find single auth key""\n        print json.dumps(res)\n        sys.exit(1)\n\ndef get_action_params(opts,params):\n    params[""user""] = opts.user\n    if opts.item is None:\n        params[""item""] = 1\n    else:\n        params[""item""] = opts.item\n    params[""type""] = opts.type\n    return params\n\ndef get_js_recommend_params(opts,params):\n    params[""user""] = opts.user\n    if not opts.item is None:\n        params[""item""] = opts.item\n    if not opts.dimensions is None:\n        params[""dimensions""] = opts.dimensions\n    params[""type""] = opts.type\n    params[""limit""] = opts.limit\n    if not opts.attributes is None:\n        params[""attributes""] = opts.attributes\n    return params\n\ndef get_oauth_recommend_params(opts,params):\n    if not opts.item is None:\n        params[""item""] = opts.item\n    if not opts.dimensions is None:\n        params[""dimensions""] = opts.dimensions\n    params[""type""] = opts.type\n    params[""limit""] = opts.limit\n    return params\n\ndef get_js_predict_params(opts,params):\n    params[""json""] = opts.json\n    return params\n\ndef get_oauth_items_params(opts,params):\n    if not opts.full is None:\n        params[""full""] = opts.full\n    params[""type""] = opts.type\n    params[""limit""] = opts.limit\n    return params\n\ndef call_js(gopts,command_data,opts,auth):\n    params = {}\n    params[""consumer_key""] = auth[\'key\']\n    params[""jsonpCallback""]= ""j""\n    if opts.endpoint == ""/js/action/new"":\n        params = get_action_params(opts,params)\n    elif opts.endpoint == ""/js/recommendations"":\n        params = get_js_recommend_params(opts,params)\n    elif opts.endpoint == ""/js/predict"":\n        params = get_js_predict_params(opts,params)\n    elif opts.endpoint == \'/js/event/new\':\n        params = get_js_predict_params(opts,params)\n    url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n    r = requests.get(url,params=params)\n    if not gopts.quiet:\n        print ""response code"",r.status_code\n    if r.status_code == requests.codes.ok:\n        res = re.sub(r\'^j\\(\',"""",r.text)\n        res = re.sub(r\'\\)$\',"""",res)\n        print res\n\ndef call_oauth(gopts,command_data,opts,token):\n    data = {}\n    params = {}\n    params[\'oauth_token\'] = token\n    headers = {}\n    headers[""content-type""] = ""application/json""\n    if opts.endpoint == ""/actions"":\n        data = get_action_params(opts,data)\n        url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n        r = requests.post(url,data=json.dumps(data),params=params,headers=headers)\n    elif opts.endpoint == ""/users/recommendations"":\n        params = get_oauth_recommend_params(opts,params)\n        url = command_data[\'conf_data\'][""server_endpoint""] + ""/users/""+opts.user+""/recommendations""\n        r = requests.get(url,params=params)\n    elif opts.endpoint == ""/predict"":\n        url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n        r = requests.post(url,data=opts.json,params=params,headers=headers)\n    elif opts.endpoint == ""/events"":\n        url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n        r = requests.post(url,data=opts.json,params=params,headers=headers)\n    elif opts.endpoint == ""/items"" and opts.method == \'GET\':\n        params = get_oauth_items_params(opts,params)\n        url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n        r = requests.get(url,params=params)\n    elif opts.endpoint == ""/items"" and opts.method == \'POST\':\n        url = command_data[\'conf_data\'][""server_endpoint""] + opts.endpoint\n        r = requests.post(url,data=opts.json,params=params,headers=headers)\n    else:\n        print ""unknown endpoint and method""\n        sys.exit(1)\n\n    if not gopts.quiet:\n        print ""response code"",r.status_code\n    if r.status_code == requests.codes.ok:\n        res = re.sub(r\'^j\\(\',"""",r.text)\n        res = re.sub(r\'\\)$\',"""",res)\n        print res\n    \n\ndef get_token(gopts,command_data,opts,auth):\n    params = {}\n    params[""consumer_key""] = auth[""key""]\n    params[""consumer_secret""] = auth[""secret""]\n    url = command_data[\'conf_data\'][""server_endpoint""] + ""/token""\n    r = requests.get(url,params=params)\n    if not gopts.quiet:\n        print ""response code"",r.status_code\n    if r.status_code == requests.codes.ok:\n        res = re.sub(r\'^j\\(\',"""",r.text)\n        res = re.sub(r\'\\)$\',"""",res)\n        return json_to_dict(res)[\'access_token\']\n        \n\ndef action_call(gopts,command_data,opts):\n    auth = get_auth(command_data,opts)\n    if opts.endpoint.startswith(""/js""):\n        call_js(gopts,command_data,opts,auth)\n    else:\n        token = get_token(gopts,command_data,opts,auth)\n        call_oauth(gopts,command_data,opts,token)\n\ndef cmd_api(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_call,\n        ""call"" : action_call\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](gopts,command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](gopts,command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n'"
python/seldon/cli/cmd_attr.py,0,"b'import pprint\nimport argparse\nimport sys\nimport os\nimport re\n\nimport seldon_utils\nimport zk_utils\nimport attr_schema_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli attr\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'edit\',\'show\',\'apply\',\'dimensions\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--json\', help=""the file containing attr json"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef ensure_client_has_attr(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/attr/_data_"".format(\n            zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/attr""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_attr_json = \'{""types"":[{""type_attrs"":[{""name"":""title"",""value_type"":""string""}],""type_id"":1,""type_name"":""defaulttype""}]}\'\n            data = seldon_utils.json_to_dict(default_attr_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef write_data_to_file(data_fpath, data):\n    json = seldon_utils.dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    seldon_utils.mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef show_attr(data):\n    attr_types = data[""types""]\n    print ""types:""\n    for attr_type in attr_types:\n        attr_type_name = attr_type[""type_name""]\n        attr_type_id = attr_type[""type_id""]\n        attr_type_attrs = attr_type[""type_attrs""]\n        print ""   [{attr_type_name}]"".format(**locals())\n        print ""       type_id: {attr_type_id}"".format(**locals())\n        print ""       type_attrs:""\n        for attr_type_attr in attr_type_attrs:\n            attrib_name = attr_type_attr[""name""]\n            attrib_value = attr_type_attr[""value_type""]\n            attrib_value_str = ""enum[""+"","".join(attrib_value)+""]"" if isinstance(attrib_value,list) else attrib_value\n            print ""           {attrib_name}: {attrib_value_str}"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = seldon_utils.json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef action_show(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the attr for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n    show_attr(data)\n\n\ndef store_json(command_data,opts):\n    f = open(opts.json)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + opts.client_name + ""/attr/_data_""\n    if data is None:\n        print ""Invalid attr json!""\n        sys.exit(1)\n    else:\n        write_data_to_file(data_fpath, data)\n        node_path = gdata[""all_clients_node_path""]+""/""+opts.client_name+""/attr""\n        zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef get_db_settings(zkroot, client_name):\n\n    def get_db_jndi_name():\n        data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n        DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n        return DB_JNDI_NAME\n\n    def get_db_info(db_name):\n        data_fpath = zkroot + ""/config/dbcp/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n\n        db_info = None\n        for db_info_entry in data[\'dbs\']:\n            if db_info_entry[\'name\'] == db_name:\n                db_info = db_info_entry\n                break\n        return db_info\n\n    db_name = get_db_jndi_name()\n    db_info = get_db_info(db_name)\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        sys.exit(1)\n\n    dbSettings = {}\n    dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n    dbSettings[""user""]=db_info[""user""]\n    dbSettings[""password""]=db_info[""password""]\n    return dbSettings\n\ndef action_edit(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the attr for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    #do the edit\n    from subprocess import call\n    editor=seldon_utils.get_editor()\n    call([editor, data_fpath])\n\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    if data is None:\n        print ""Invalid attr json!""\n    else:\n        write_data_to_file(data_fpath, data)\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/attr""\n        zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n        show_attr(data)\n\ndef action_apply(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the attr for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n\n    if not opts.json is None:\n        store_json(command_data,opts)\n\n    dbSettings = get_db_settings(zkroot, client_name)\n\n    scheme_file_path = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    clean = True\n    attr_schema_utils.create_schema(client_name, dbSettings, scheme_file_path, clean)\n    clean = False\n    attr_schema_utils.create_schema(client_name, dbSettings, scheme_file_path, clean)\n\ndef action_dimensions(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the attr for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    dbSettings = get_db_settings(zkroot,client_name)\n    attr_schema_utils.show_dimensions(client_name, dbSettings)\n\ndef cmd_attr(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_show,\n        ""show"" : action_show,\n        ""edit"" : action_edit,\n        ""apply"" : action_apply,\n        ""dimensions"": action_dimensions,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_client.py,0,"b'import pprint\nimport argparse\nimport os\nimport json\nimport sys\nimport re\n\nimport zk_utils\nimport seldon_utils\nimport spark_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli client\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'list\',\'setup\',\'processactions\',\'processevents\',\'zk_push\',""zk_pull""])\n    parser.add_argument(\'--db-name\', help=""the name of the db"", required=False)\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--input-date-string\', help=""The date to process in YYYYMMDD format"", required=False)\n    parser.add_argument(\'--set-js-key\', help=""the key to use for the js scope"", required=False)\n    parser.add_argument(\'--set-all-key\', help=""the key to use for the all scope"", required=False)\n    parser.add_argument(\'--set-all-secret\', help=""the secret to use for the all scope"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef get_data_fpath(zkroot, client):\n    return zkroot + gdata[""all_clients_node_path""] + ""/"" + client + ""/_data_""\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef add_client(gopts,command_data,zk_client, zkroot, client_name, db_name, consumer_details=None):\n    data_fpath = zkroot + ""/config/dbcp/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    db_info = None\n    for db_info_entry in data[\'dbs\']:\n        if db_info_entry[\'name\'] == db_name:\n            db_info = db_info_entry\n            break\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        sys.exit(1)\n\n    dbSettings = {}\n    dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n    dbSettings[""user""]=db_info[""user""]\n    dbSettings[""password""]=db_info[""password""]\n    seldon_utils.addApiDb(db_name, dbSettings)\n    seldon_utils.addClientDb(client_name, dbSettings, consumer_details)\n    # write to local file\n    data_fpath = get_data_fpath(zkroot, client_name)\n    data = {\'DB_JNDI_NAME\': db_name}\n    write_data_to_file(data_fpath, data)\n    # write to zookeeper\n    node_path=gdata[""all_clients_node_path""]+""/""+client_name\n    data_json = dict_to_json(data)\n    zk_utils.node_set(zk_client, node_path, data_json)\n\n\ndef add_client_dashboard(gopts,command_data,client_name):\n    if ""grafana_endpoint"" in command_data[""conf_data""] and (\'GRAFANA_ADMIN_PASSWORD\' in os.environ):\n        admin_password = os.environ[\'GRAFANA_ADMIN_PASSWORD\']\n        grafana = command_data[""conf_data""][""grafana_endpoint""]\n        if ""grafana_dashboard_template"" in command_data[""conf_data""]:\n            dashboard_template = command_data[""conf_data""][""grafana_dashboard_template""]\n        else:\n            dashboard_template = None\n        if not (grafana is None or grafana == """"):\n            seldon_utils.add_grafana_dashboard(grafana,client_name,gopts.quiet,dashboard_template,admin_password)\n\ndef action_list(gopts,command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n\n    all_clients_fpath = zkroot + gdata[""all_clients_node_path""]\n    if not os.path.isdir(all_clients_fpath):\n        # the dir for all_clients doesnt exist\n        if zk_client.exists(gdata[""all_clients_node_path""]):\n            client_nodes = zk_client.get_children(gdata[""all_clients_node_path""])\n            def write_node_value_to_file(node_path):\n                node_value = zk_utils.node_get(zk_client, node_path)\n                node_value = node_value.strip()\n                if zk_utils.is_json_data(node_value):\n                    data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n                else:\n                    data = str(node_value)\n                data_fpath = zkroot + node_path + ""/_data_""\n                write_data_to_file(data_fpath, data)\n            for client_node in client_nodes:\n                node_path=gdata[""all_clients_node_path""]+""/""+client_node\n                write_node_value_to_file(node_path)\n                client_child_nodes = zk_client.get_children(gdata[""all_clients_node_path""]+""/""+client_node)\n                for client_child_node in client_child_nodes:\n                    node_path=gdata[""all_clients_node_path""]+""/""+client_node+""/""+client_child_node\n                    write_node_value_to_file(node_path)\n\n    if not os.path.isdir(all_clients_fpath):\n        print ""No clients found!""\n    else:\n        for client in os.listdir(zkroot + gdata[""all_clients_node_path""]):\n            data_fpath = get_data_fpath(zkroot, client)\n            f = open(data_fpath)\n            json = f.read()\n            data = json_to_dict(json)\n            f.close()\n            print ""client[{client}]:"".format(**locals())\n            DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n            print ""    DB_JNDI_NAME: ""+DB_JNDI_NAME\n\ndef action_setup(gopts,command_data, opts):\n    db_name_to_use = opts.db_name\n    client_name_to_setup = opts.client_name\n    if db_name_to_use == None:\n        print ""Need db name to use""\n        sys.exit(1)\n    if client_name_to_setup == None:\n        print ""Need client name to setup""\n        sys.exit(1)\n\n    consumer_details = {\n        \'js_consumer_key\': opts.set_js_key,\n        \'all_consumer_key\': opts.set_all_key,\n        \'all_consumer_secret\': opts.set_all_secret,\n    }\n\n    # check if this client exists\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    data_fpath = get_data_fpath(zkroot, client_name_to_setup)\n    if not os.path.isfile(data_fpath):\n        print ""Trying to create the client""\n        add_client(gopts,command_data,zk_client, zkroot, client_name_to_setup, db_name_to_use, consumer_details)\n        add_client_dashboard(gopts,command_data,client_name_to_setup)\n    else:\n        add_client_dashboard(gopts,command_data,client_name_to_setup)\n        print ""Client already exists!""\n\ndef action_zk_push(gopts,command_data,opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zk_utils.push_all_nodes(zk_client,zkroot)\n\ndef action_zk_pull(gopts,command_data,opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zk_utils.pull_all_nodes(zk_client,zkroot)\n\n\ndef action_processactions(gopts,command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    def get_valid_client():\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            sys.exit(1)\n        return client_name\n\n    def get_valid_input_date_string():\n        input_date_string = opts.input_date_string\n        if input_date_string == None:\n            print ""Need input date string!""\n            sys.exit(1)\n        return input_date_string\n\n    client_name = opts.client_name\n    if client_name != None:\n        client_name = get_valid_client()\n\n    job_info = command_data[""conf_data""][""processactions""][""job_info""]\n    if client_name != None:\n        job_info[""cmd_args""].append(""--single-client"")\n        job_info[""cmd_args""].append(""%CLIENT_NAME%"")\n\n    conf_data = command_data[""conf_data""]\n    spark_executor_memory = conf_data[""spark_executor_memory""]\n    spark_driver_memory = conf_data[""spark_driver_memory""]\n    input_date_string = get_valid_input_date_string()\n    replacements = [\n        (""%INPUT_DATE_STRING%"", input_date_string),\n        (""%SPARK_EXECUTOR_MEMORY%"", spark_executor_memory),\n        (""%SPARK_DRIVER_MEMORY%"", spark_driver_memory)\n    ]\n\n    def appy_replacements(item):\n        for rpair in replacements:\n            if rpair[1] != None:\n                item = item.replace(rpair[0],rpair[1])\n        return item\n\n\n    cmd_args = job_info[""cmd_args""]\n    job_info[""cmd_args""] = map(appy_replacements, cmd_args)\n\n    spark_utils.run_spark_job(command_data, job_info, client_name)\n\ndef action_processevents(gopts,command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    def get_valid_client():\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            sys.exit(1)\n        return client_name\n\n    def get_valid_input_date_string():\n        input_date_string = opts.input_date_string\n        if input_date_string == None:\n            print ""Need input date string!""\n            sys.exit(1)\n        return input_date_string\n\n    client_name = opts.client_name\n    if client_name != None:\n        client_name = get_valid_client()\n\n    job_info = command_data[""conf_data""][""processevents""][""job_info""]\n    if client_name != None:\n        job_info[""cmd_args""].append(""--single-client"")\n        job_info[""cmd_args""].append(""%CLIENT_NAME%"")\n\n    input_date_string = get_valid_input_date_string()\n    replacements = [\n        (""%INPUT_DATE_STRING%"", input_date_string),\n    ]\n\n    def appy_replacements(item):\n        for rpair in replacements:\n            if rpair[1] != None:\n                item = item.replace(rpair[0],rpair[1])\n        return item\n\n\n    cmd_args = job_info[""cmd_args""]\n    job_info[""cmd_args""] = map(appy_replacements, cmd_args)\n\n    spark_utils.run_spark_job(command_data, job_info, client_name)\n\ndef cmd_client(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_list,\n        ""list"" : action_list,\n        ""setup"" : action_setup,\n        ""processactions"" : action_processactions,\n        ""processevents"" : action_processevents,\n        ""zk_push"" : action_zk_push,\n        ""zk_pull"" : action_zk_pull,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](gopts,command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](gopts,command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_db.py,0,"b'import pprint\nimport argparse\nimport os\nimport json\nimport sys\nimport errno\nimport copy\n\nimport zk_utils\n\ngdata = {\n    \'data_path\': ""/config/dbcp/_data_"",\n    \'node_path\': ""/config/dbcp"",\n    \'default_data\': {\n        \'dbs\': [\n            {\n                \'jdbc\': ""jdbc:mysql:replication://127.0.0.1:3306,127.0.0.1:3306/?characterEncoding=utf8&useServerPrepStmts=true&logger=com.mysql.jdbc.log.StandardLogger&roundRobinLoadBalance=true&transformedBitIsBoolean=true&rewriteBatchedStatements=true"",\n                \'password\': ""mypass"",\n                ""name"": ""ClientDB"",\n                \'user\': ""root"",\n                ""maxIdle"" : 50,\n                ""minIdle"" : 20\n            }\n         ]\n    },\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli db\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'show\',\'list\',\'setup\',\'commit\'])\n    parser.add_argument(\'--db-name\', help=""the name of the db"", required=False)\n    parser.add_argument(\'--db-user\', help=""the user"", required=False)\n    parser.add_argument(\'--db-password\', help=""the password for the user"", required=False)\n    parser.add_argument(\'--db-jdbc\', help=""the jdbc string"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef ensure_local_data_file_exists(zk_client, zkroot):\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        node_path=gdata[""node_path""]\n        node_value = zk_utils.node_get(zk_client, node_path)\n        data = json_to_dict(node_value) if node_value != None else gdata[""default_data""]\n        write_data_to_file(data_fpath, data)\n\ndef action_list(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    db_list = [str(db[\'name\']) for db in data[\'dbs\']]\n    list_str = "", "".join(db_list)\n    print list_str\n\ndef action_setup(command_data, opts):\n    db_to_setup = opts.db_name\n    if db_to_setup == None:\n        print ""Need db-name to setup!""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n    print ""Setting up Databases""\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    has_any_value_changed = False\n\n    db_entries_found = [db_entry for db_entry in data[""dbs""] if db_entry[""name""]==db_to_setup]\n    db_entry = db_entries_found[0] if len(db_entries_found)>0 else None\n\n    if db_entry == None:\n        db_entry = copy.deepcopy(gdata[""default_data""][""dbs""][0])\n        db_entry[""name""] = db_to_setup\n        data[""dbs""].append(db_entry)\n        has_any_value_changed = True\n\n    if (opts.db_user != None) and (db_entry[""user""] != opts.db_user):\n        db_entry[""user""] = opts.db_user\n        has_any_value_changed = True\n\n    if (opts.db_password != None) and (db_entry[""password""] != opts.db_password):\n        db_entry[""password""] = opts.db_password\n        has_any_value_changed = True\n\n    if (opts.db_jdbc != None) and (db_entry[""jdbc""] != opts.db_jdbc):\n        db_entry[""jdbc""] = opts.db_jdbc\n        has_any_value_changed = True\n\n    if has_any_value_changed:\n        write_data_to_file(data_fpath, data)\n\ndef action_show(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n    dbs = data[\'dbs\']\n    for db in dbs:\n        name = db[""name""]\n        jdbc = db[""jdbc""]\n        user = db[""user""]\n        password = db[""password""]\n        print ""db[{name}]:"".format(**locals())\n        print ""    jdbc: {jdbc}"".format(**locals())\n        print ""    user: {user}"".format(**locals())\n        print ""    password: {password}"".format(**locals())\n\ndef action_commit(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        write_data_to_file(data_fpath, gdata[""default_data""])\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    node_path=gdata[""node_path""]\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef cmd_db(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_show,\n        ""show"" : action_show,\n        ""list"" : action_list,\n        ""setup"" : action_setup,\n        ""commit"" : action_commit,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_import.py,0,"b'import pprint\nimport argparse\nimport os\nimport sys\nimport re\n\nimport seldon_utils\nimport import_items_utils\nimport import_users_utils\nimport import_actions_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli import\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=True, choices=[\'items\',\'users\',\'actions\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=True)\n    parser.add_argument(\'--file-path\', help=""path to the data file"", required=True)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef get_db_settings(zkroot, client_name):\n    def get_db_jndi_name():\n        data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n        DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n        return DB_JNDI_NAME\n\n    def get_db_info(db_name):\n        data_fpath = zkroot + ""/config/dbcp/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n\n        db_info = None\n        for db_info_entry in data[\'dbs\']:\n            if db_info_entry[\'name\'] == db_name:\n                db_info = db_info_entry\n                break\n        return db_info\n\n    db_name = get_db_jndi_name()\n    db_info = get_db_info(db_name)\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        return None\n\n    dbSettings = {}\n    dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n    dbSettings[""user""]=db_info[""user""]\n    dbSettings[""password""]=db_info[""password""]\n    return dbSettings\n\ndef action_items(command_data, opts):\n    client_name = opts.client_name\n    data_file_fpath = opts.file_path\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        sys.exit(1)\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    import_items_utils.import_items(client_name, db_settings, data_file_fpath)\n\ndef action_users(command_data, opts):\n    client_name = opts.client_name\n    data_file_fpath = opts.file_path\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        sys.exit(1)\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    import_users_utils.import_users(client_name, db_settings, data_file_fpath)\n\ndef action_actions(command_data, opts):\n    client_name = opts.client_name\n    data_file_fpath = opts.file_path\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        sys.exit(1)\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    out_file_dir = command_data[""conf_data""][""seldon_models""] + ""/"" + client_name + ""/actions/1""\n    out_file_fpath = out_file_dir + ""/actions.json""\n\n    seldon_utils.mkdir_p(out_file_dir)\n\n    import_actions_utils.import_actions(client_name, db_settings, data_file_fpath, out_file_fpath)\n\ndef cmd_import(gopts,command_data, command_args):\n    actions = {\n        ""items"" : action_items,\n        ""users"" : action_users,\n        ""actions"" : action_actions,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n'"
python/seldon/cli/cmd_keys.py,0,"b'import pprint\nimport argparse\nimport os\nimport sys\nimport re\nimport json\nimport MySQLdb\n\nimport db_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli keys\', description=\'Seldon CLI\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'list\',\'update\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--scope\', help=""the key scope"", required=False, choices=[\'js\',\'all\'])\n    parser.add_argument(\'--key\', help=""the key for the update"", required=False)\n    parser.add_argument(\'--secret\', help=""the secret for the update"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\nKEYS_SQL_CLIENT = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where short_name=%(client_name)s""\nKEYS_SQL_CLIENT_SCOPE = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where short_name=%(client_name)s and scope=%(scope)s""\nKEYS_SQL_SCOPE = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where scope=%(scope)s""\nKEYS_SQL = ""select short_name,consumer_key,consumer_secret,scope from api.consumer""\n\n\ndef action_list(command_data,opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + ""/config/dbcp/_data_""\n    f = open(data_fpath)\n    jsonStr = f.read()\n    data = json_to_dict(jsonStr)\n    f.close()\n\n    db_info = None\n    for db_info in data[\'dbs\']:\n        dbSettings = {}\n        dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n        dbSettings[""user""]=db_info[""user""]\n        dbSettings[""password""]=db_info[""password""]\n        dbSettings[""name""] = db_info[""name""]\n        res = db_utils.get_keys(dbSettings,opts.client_name,opts.scope)\n    print json.dumps(res)\n\ndef action_update(command_data,opts):\n    client_name=opts.client_name\n    scope=opts.scope\n    consumer_key=opts.key\n    consumer_secret=opts.secret\n\n    if client_name == None:\n        print ""Need client name to update""\n        sys.exit(1)\n    if consumer_key == None:\n        print ""Need key to update""\n        sys.exit(1)\n    if (scope == ""all"") and (consumer_secret == None):\n        print ""Need secret to update""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + ""/config/dbcp/_data_""\n    f = open(data_fpath)\n    jsonStr = f.read()\n    data = json_to_dict(jsonStr)\n    f.close()\n\n    db_info = None\n    for db_info in data[\'dbs\']:\n        dbSettings = {}\n        dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n        dbSettings[""user""]=db_info[""user""]\n        dbSettings[""password""]=db_info[""password""]\n        dbSettings[""name""] = db_info[""name""]\n        db_utils.set_keys(dbSettings,client_name,scope,consumer_key,consumer_secret)\n    action_list(command_data,opts)\n\ndef cmd_keys(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_list,\n        ""list"" : action_list,\n        ""update"": action_update,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n'"
python/seldon/cli/cmd_memcached.py,0,"b'import pprint\nimport argparse\nimport os\nimport sys\nimport json\nimport errno\n\nimport zk_utils\n\ngdata = {\n    \'data_path\': ""/config/memcached/_data_"",\n    \'node_path\': ""/config/memcached"",\n    \'default_data\': {\n        \'numClients\': 1,\n        \'servers\': ""localhost:11211""\n    },\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli memcached\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'setup\', \'commit\'])\n    parser.add_argument(\'--numClients\', help=""number of clients"", required=False)\n    parser.add_argument(\'--servers\', help=""the server list"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef action_setup(command_data, opts):\n    print ""Setting up memcached""\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    has_any_value_changed = False\n\n    if opts.numClients != None:\n        data[""numClients""] = opts.numClients\n        has_any_value_changed = True\n\n    if opts.servers != None:\n        data[""servers""] = opts.servers\n        has_any_value_changed = True\n\n    if has_any_value_changed:\n        write_data_to_file(data_fpath, data)\n\ndef action_commit(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        write_data_to_file(data_fpath, gdata[""default_data""])\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    node_path=gdata[""node_path""]\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef ensure_local_data_file_exists(zk_client, zkroot):\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        node_path=gdata[""node_path""]\n        node_value = zk_utils.node_get(zk_client, node_path)\n        data = json_to_dict(node_value) if node_value != None else gdata[""default_data""]\n        write_data_to_file(data_fpath, data)\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef action_default(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n    print ""memcached:""\n    print ""    numClients: {numClients}"".format(numClients=data[""numClients""])\n    print ""    servers: {servers}"".format(servers=data[""servers""])\n\ndef cmd_memcached(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_default,\n        ""setup"" : action_setup,\n        ""commit"" : action_commit,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_model.py,0,"b'import pprint\nimport argparse\nimport sys\nimport os\nimport json\nfrom subprocess import call\n\nimport zk_utils\nimport seldon_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli model\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'list\',\'add\',\'show\',\'edit\',\'train\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--model-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--spark-executor-memory\', help=""spark executor memory"", required=False)\n    parser.add_argument(\'--spark-driver-memory\', help=""spark driver memory"", required=False)\n    opts = parser.parse_known_args(args)\n    return opts\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef write_data_to_file(data_fpath, data):\n    json = seldon_utils.dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    seldon_utils.mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = seldon_utils.json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef run_spark_job(command_data, job_info, client_name,opts):\n    conf_data = command_data[""conf_data""]\n    spark_home = conf_data[""spark_home""]\n    seldon_spark_home = conf_data[""seldon_spark_home""]\n    seldon_version = conf_data[""seldon_version""]\n    zk_hosts = conf_data[""zk_hosts""]\n\n    \n    spark_executor_memory = conf_data[""spark_executor_memory""]\n    if not opts.spark_executor_memory is None:\n        spark_executor_memory = opts.spark_executor_memory\n    spark_driver_memory = conf_data[""spark_driver_memory""]\n    if not opts.spark_driver_memory is None:\n        spark_driver_memory = opts.spark_driver_memory\n\n\n    cmd = job_info[""cmd""].replace(""%SPARK_HOME%"", spark_home)\n\n    cmd_args = job_info[""cmd_args""]\n\n    replacements = [\n        (""%CLIENT_NAME%"", client_name),\n        (""%SPARK_HOME%"", spark_home),\n        (""%SELDON_SPARK_HOME%"", seldon_spark_home),\n        (""%SELDON_VERSION%"", seldon_version),\n        (""%ZK_HOSTS%"", zk_hosts),\n        (""%SPARK_EXECUTOR_MEMORY%"", spark_executor_memory),\n        (""%SPARK_DRIVER_MEMORY%"", spark_driver_memory)\n    ]\n\n    def appy_replacements(item):\n        for rpair in replacements:\n            item = item.replace(rpair[0],rpair[1])\n        return item\n\n    cmd_args = map(appy_replacements, cmd_args)\n\n    print ""Run Spark Job  "",cmd,cmd_args\n\n    call([cmd]+cmd_args)\n\ndef get_config_args(extra_args):\n    extra_args = [x.replace(""--"","""") for x in extra_args]\n    d = dict(zip(extra_args[0::2], extra_args[1::2]))\n    for k in d:\n        try:\n            d[k] = int(d[k])\n        except:\n            try:\n                d[k] = float(d[k])\n            except:\n                pass\n    return d\n\ndef action_add(command_data, opts, extra_args):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to add model for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    model_name = opts.model_name\n    if model_name == None:\n        print ""Need model name to use""\n        sys.exit(1)\n\n    default_models = command_data[""conf_data""][""default_models""]\n    if model_name not in default_models.keys():\n        print ""Invalid model name: {model_name}"".format(**locals())\n        sys.exit(1)\n\n    config_args = get_config_args(extra_args)\n\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline/{model_name}/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    node_path = ""{all_clients_node_path}/{client_name}/offline/{model_name}"".format(all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n    if not os.path.isfile(data_fpath):\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n            f = open(data_fpath)\n            json = f.read()\n            f.close()\n            data = seldon_utils.json_to_dict(json)\n        else:\n            default_model_data = default_models[model_name][""config""]\n            if default_model_data.has_key(""inputPath""):\n                default_model_data[""inputPath""]=command_data[""conf_data""][""seldon_models""]\n            if default_model_data.has_key(""outputPath""):\n                default_model_data[""outputPath""]=command_data[""conf_data""][""seldon_models""]\n            data = default_model_data\n    else:\n        f = open(data_fpath)\n        json = f.read()\n        f.close()\n        data = seldon_utils.json_to_dict(json)\n        print ""Model [{model_name}] already added"".format(**locals())\n\n    for k in config_args:\n        print ""adding config "",k,"":"",config_args[k]\n        data[k] = config_args[k]\n    write_data_to_file(data_fpath, data)\n    zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef action_list(command_data, opts, extra_args):\n    default_models = command_data[""conf_data""][""default_models""]\n    models = default_models.keys()\n    print ""models:""\n    for idx,model in enumerate(models):\n        print ""    {model}"".format(**locals())\n\ndef action_show(command_data, opts, extra_args):\n    def get_valid_client():\n        client_name = opts.client_name\n        if client_name == None:\n            print ""Need client name to show models for""\n            sys.exit(1)\n\n        zkroot = command_data[""zkdetails""][""zkroot""]\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            sys.exit(1)\n        return client_name\n    def show_models(models_for_client_fpath):\n        models = os.listdir(models_for_client_fpath)\n        for idx,model in enumerate(models):\n            print ""    {model}"".format(**locals())\n\n    client_name = get_valid_client()\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    models_for_client_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n\n    show_models(models_for_client_fpath)\n\ndef action_edit(command_data, opts, extra_args):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    def get_valid_client():\n        client_name = opts.client_name\n        if client_name == None:\n            print ""Need client name to show models for""\n            sys.exit(1)\n\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            sys.exit(1)\n        return client_name\n\n    def get_valid_model():\n        model_name = opts.model_name\n        if model_name == None:\n            print ""Need model name to use""\n            sys.exit(1)\n\n        default_models = command_data[""conf_data""][""default_models""]\n        if model_name not in default_models.keys():\n            print ""Invalid model name: {model_name}"".format(**locals())\n            sys.exit(1)\n        return model_name\n\n    client_name = get_valid_client()\n    model_name = get_valid_model()\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline/{model_name}/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n\n    #do the edit\n    editor=seldon_utils.get_editor()\n    call([editor, data_fpath])\n\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    if data is None:\n        print ""Invalid model json!""\n    else:\n        write_data_to_file(data_fpath, data)\n        node_path = ""{all_clients_node_path}/{client_name}/offline/{model_name}"".format(all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n        pp(node_path)\n        zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef action_train(command_data, opts,extra_args):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    def get_valid_client():\n        client_name = opts.client_name\n        if client_name == None:\n            print ""Need client name to show models for""\n            sys.exit(1)\n\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            sys.exit(1)\n        return client_name\n\n    def get_valid_model():\n        model_name = opts.model_name\n        if model_name == None:\n            print ""Need model name to use""\n            sys.exit(1)\n\n        default_models = command_data[""conf_data""][""default_models""]\n        if model_name not in default_models.keys():\n            print ""Invalid model name: {model_name}"".format(**locals())\n            sys.exit(1)\n        return model_name\n\n    client_name = get_valid_client()\n    model_name = get_valid_model()\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    default_models = command_data[""conf_data""][""default_models""]\n    model_training = default_models[model_name][""training""]\n    job_type = model_training[""job_type""]\n    job_info = model_training[""job_info""]\n\n    job_handlers = {\n            \'spark\' : run_spark_job\n    }\n\n    if job_handlers.has_key(job_type):\n        job_handlers[job_type](command_data, job_info, client_name,opts)\n    else:\n        print ""No handler found for job_type[{job_type}]"".format(**locals())\n\n\ndef cmd_model(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_list,\n        ""list"" : action_list,\n        ""add"" : action_add,\n        ""show"" : action_show,\n        ""edit"" : action_edit,\n        ""train"" : action_train,\n    }\n\n    (opts,extra_args) = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts, extra_args)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts, extra_args)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/cmd_pred.py,0,"b'import pprint\nimport argparse\nimport sys\nimport os\nimport json\nimport errno\n\nimport zk_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli predict_alg\', description=\'Seldon CLI\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=True, choices=[\'list\',\'show\', \'commit\',\'create\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=False)\n    parser.add_argument(\'--predictor-name\', help=""the name of predictor"", required=False)\n    parser.add_argument(\'--config\', help=""algorithm specific config in the form x=y"", required=False, action=\'append\')\n    parser.add_argument(\'-f\',\'--json-file\', help=""the json file to use for creating algs or \'-\' for stdin"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef show_algs(data):\n    algorithms = data[""algorithms""]\n    print ""algorithms:""\n    for alg in algorithms:\n        print ""    {alg_name}"".format(alg_name=alg[""name""])\n        for config_item in alg[""config""]:\n            print ""        {n}={v}"".format(n=config_item[""name""],v=config_item[""value""])\n\n\ndef ensure_client_has_algs(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/predict_algs/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/predict_algs""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_alg_json = \'{""algorithms"":[]}\'\n            data = json_to_dict(default_alg_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, dict_to_json(data))\n\ndef action_show(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the algs for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/predict_algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n    show_algs(data)\n\ndef has_config(opts,name):\n    if not opts.config is None:\n        for nv in opts.config:\n            if nv.split(\'=\')[0] == name:\n                return True\n    return False\n\ndef action_list(command_data, opts):\n    print ""Default predictors:""\n    default_algorithms = command_data[""conf_data""][""default_predictors""]\n    for predictor in default_algorithms:\n        print ""    {predictor}"".format(**locals())\n\ndef action_commit(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to commit algs for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/predict_algs/_data_""\n    if not os.path.isfile(data_fpath):\n        ""Data to commit not found!!""\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/predict_algs""\n    zk_utils.node_set(zk_client, node_path, data_json)\n\n\ndef action_create(command_data, opts):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    #check_valid_client_name\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to create algs for""\n        sys.exit(1)\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    #check_valid_json_file\n    json_file_contents = """"\n    json_file = opts.json_file\n    if json_file == None:\n        print ""Need json-file to use for creating algs""\n        sys.exit(1)\n    if json_file == ""-"":\n        json_file_contents = sys.stdin.read()\n    else:\n        if not os.path.isfile(json_file):\n            print ""Unable find file[{json_file}]"".format(**locals())\n            sys.exit(1)\n        f = open(json_file)\n        json_file_contents = f.read()\n        f.close()\n\n    # ensure valid data\n    data = json_to_dict(json_file_contents)\n\n    #save to zkoot\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/predict_algs/_data_""\n    write_data_to_file(data_fpath, data)\n\n    print ""Added prediction algs for {client_name}"".format(**locals())\n\n\ndef cmd_pred(gopts,command_data, command_args):\n    actions = {\n        ""list"" : action_list,\n        ""show"" : action_show,\n        ""commit"" : action_commit,\n        ""create"" : action_create,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n'"
python/seldon/cli/cmd_rec_exp.py,0,"b'import pprint\nimport argparse\nimport sys\nimport os\n\nimport zk_utils\nimport seldon_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli rec_alg\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=True, choices=[\'show\',\'configure\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=True)\n\n    parser.add_argument(\'--cache-enabled\', help=""enable cache or not"", required=False, choices=[\'true\',\'false\'])\n    parser.add_argument(\'--default-locale\', help=""set the deafult locale to use eg \'us-en\'"", required=False)\n    parser.add_argument(\'--explanations-enabled\', help=""enable the explanaions or not"", required=False, choices=[\'true\',\'false\'])\n\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef write_data_to_file(data_fpath, data):\n    json = seldon_utils.dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    seldon_utils.mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = seldon_utils.json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef ensure_client_has_recommendation_explanation(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/recommendation_explanation/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/recommendation_explanation""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_rec_exp = \'{""cache_enabled"":true,""default_locale"":""us-en"",""explanations_enabled"":false}\'\n            data = seldon_utils.json_to_dict(default_rec_exp)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef action_show(command_data, opts):\n    client_name = opts.client_name\n    if (client_name == None) or (len(client_name) == 0):\n        print ""Need client name to comfigure explanations for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/recommendation_explanation/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        print ""Explanations not configured for this client""\n        sys.exit(1)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/recommendation_explanation/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    print ""Explanations configuration, for client: ""+client_name\n    data_keys = data.keys()\n    data_keys.sort()\n    for data_key in data_keys:\n        data_value = data[data_key]\n        data_value = ""true"" if data_value == True else data_value\n        data_value = ""false"" if data_value == False else data_value\n        line = ""    {data_key}: {data_value}"".format(**locals())\n        print line\n\ndef action_configure(command_data, opts):\n    client_name = opts.client_name\n    if (client_name == None) or (len(client_name) == 0):\n        print ""Need client name to comfigure explanations for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_recommendation_explanation(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/recommendation_explanation/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    if opts.cache_enabled != None:\n        data[""cache_enabled""] = True if opts.cache_enabled == ""true"" else False\n    if opts.default_locale!= None:\n        data[""default_locale""] = opts.default_locale\n    if opts.explanations_enabled != None:\n        data[""explanations_enabled""] = True if opts.explanations_enabled == ""true"" else False\n\n    write_data_to_file(data_fpath, data)\n    node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/recommendation_explanation""\n    zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n    action_show(command_data, opts)\n\ndef cmd_rec_exp(gopts,command_data, command_args):\n    actions = {\n        ""show"" : action_show,\n        ""configure"" : action_configure,\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        print ""Need action""\n        sys.exit(1)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n'"
python/seldon/cli/cmd_rpc.py,0,"b'import pprint\nimport argparse\nimport os\nimport json\nimport sys\nimport errno\nimport copy\nimport shutil\nfrom subprocess import call\n\nimport zk_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n}\n\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getOpts(args):\n    parser = argparse.ArgumentParser(prog=\'seldon-cli rpc\', description=\'Seldon Cli\')\n    parser.add_argument(\'--action\', help=""the action to use"", required=False, choices=[\'show\',\'set\', \'remove\'])\n    parser.add_argument(\'--client-name\', help=""the name of the client"", required=True)\n    parser.add_argument(\'--proto\', help=""the proto buffer file with request and optional reply class"", required=False)\n    parser.add_argument(\'--request-class\', help=""the request class"", required=False)\n    parser.add_argument(\'--reply-class\', help=""the reply class"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args(args)\n    return opts\n\ndef remove_path(path):\n    shutil.rmtree(path)\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\n\ndef show_rpc(data):\n    json = dict_to_json(data, True)\n    print json\n\ndef action_show(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the rpc settings for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        sys.exit(1)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/rpc/_data_""\n    if os.path.isfile(data_fpath):\n        f = open(data_fpath)\n        json = f.read()\n        f.close()\n        data = json_to_dict(json)\n        show_rpc(data)\n    else:\n        print ""Unable to show rpc definition for client[{client_name}]"".format(**locals())\n\n\ndef create_jar(grpc_home,proto_file,jar_file):\n    cmd = grpc_home+""/create-proto-jar.sh ""+proto_file+"" ""+jar_file\n    print ""Creating jar file from proto file. This may take some time...""\n    sys.stdout.flush()\n    return call(cmd, shell=True)\n\ndef action_set(command_data, opts):\n    if ""grpc_util_home"" in command_data[""conf_data""]:\n        grpc_home = command_data[""conf_data""][""grpc_util_home""]\n    else:\n        print ""Need the location for grpc_util_home to create proto jar via maven""\n        sys.exit(1)\n\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the rpc settings for""\n        sys.exit(1)\n\n    proto_file = opts.proto\n    if proto_file == None:\n        print ""Need --proto argument""\n        sys.exit(1)\n\n    request_class = opts.request_class\n    if request_class == None:\n        print ""Need request class to setup!""\n        sys.exit(1)\n\n    jar_file = ""/seldon-data/rpc/jar/""+client_name+"".jar""\n    if not create_jar(grpc_home,opts.proto,jar_file) == 0:\n        print ""Failed to create jar file from proto""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n\n    if opts.reply_class == None:\n        data = {""jarFilename"":jar_file,""requestClassName"":opts.request_class}\n    else:\n        data = {""jarFilename"":jar_file,""requestClassName"":opts.request_class,""replyClassName"":opts.reply_class}\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/rpc/_data_""\n    node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/rpc""\n\n    write_data_to_file(data_fpath, data)\n\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    zk_utils.node_set(zk_client, node_path, data_json)\n\n\n\ndef action_remove(command_data, opts):\n    client_name = opts.client_name\n    if client_name == None:\n        print ""Need client name to show the rpc settings for""\n        sys.exit(1)\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/rpc""\n    node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/rpc""\n\n    zk_utils.node_delete(zk_client,node_path)\n    remove_path(data_fpath)\n    \n\ndef cmd_rpc(gopts,command_data, command_args):\n    actions = {\n        ""default"" : action_show,\n        ""show"" : action_show,\n        ""set"" : action_set,\n        ""remove"" : action_remove\n    }\n\n    opts = getOpts(command_args)\n\n    action = opts.action\n    if action == None:\n        actions[""default""](command_data, opts)\n    else:\n        if actions.has_key(action):\n            actions[action](command_data, opts)\n        else:\n            print ""Invalid action[{}]"".format(action)\n\n'"
python/seldon/cli/db_utils.py,0,"b'import pprint\nimport argparse\nimport os\nimport sys\nimport re\nimport json\nimport MySQLdb\n\n\nKEYS_SQL_CLIENT = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where short_name=%(client_name)s""\nKEYS_SQL_CLIENT_SCOPE = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where short_name=%(client_name)s and scope=%(scope)s""\nKEYS_SQL_SCOPE = ""select short_name,consumer_key,consumer_secret,scope from api.consumer where scope=%(scope)s""\nKEYS_SQL = ""select short_name,consumer_key,consumer_secret,scope from api.consumer""\n\ndef get_keys(dbSettings,client_name,scope):\n    db = MySQLdb.connect(host=dbSettings[""host""],\n                         user=dbSettings[""user""],\n                         passwd=dbSettings[""password""])\n    cur = db.cursor()\n    if client_name is None and scope is None:\n        sql = KEYS_SQL\n    elif client_name is None and not scope is None:\n        sql = KEYS_SQL_SCOPE\n    elif scope is None and not client_name is None:\n        sql = KEYS_SQL_CLIENT\n    else:\n        sql = KEYS_SQL_CLIENT_SCOPE\n    cur.execute(sql,{""client_name"":client_name,""scope"":scope})\n    rows = cur.fetchall()\n    res = []\n    for row in rows:\n        res.append({""db"":dbSettings[""name""],""client"":row[0],""key"":row[1],""secret"":row[2],""scope"":row[3]})\n    cur.close()\n    return res\n\ndef set_keys(dbSettings,client_name,scope,consumer_key, consumer_secret):\n    db = MySQLdb.connect(host=dbSettings[""host""],\n                         user=dbSettings[""user""],\n                         passwd=dbSettings[""password""])\n    cur = db.cursor()\n    UPDATE_KEYS_SQL_SCOPE_ALL=""update api.consumer set consumer_key=%(consumer_key)s,consumer_secret=%(consumer_secret)s where short_name=%(client_name)s and scope=%(scope)s""\n    UPDATE_KEYS_SQL_SCOPE_JS=""update api.consumer set consumer_key=%(consumer_key)s where short_name=%(client_name)s and scope=%(scope)s""\n    sql = UPDATE_KEYS_SQL_SCOPE_JS if scope == \'js\' else UPDATE_KEYS_SQL_SCOPE_ALL\n    cur.execute(sql,{\n        ""client_name"":client_name,\n        ""scope"":scope,\n        ""consumer_key"":consumer_key,\n        ""consumer_secret"":consumer_secret,\n    })\n    cur.fetchall()\n\n'"
python/seldon/cli/import_actions_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getItemId(db,cache,client_item_id):\n    if client_item_id in cache:\n        return cache[client_item_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select item_id, client_item_id from items"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            itemId = long(row[0])\n            client_item_id_from_db = row[1]\n            cache[client_item_id_from_db] = itemId\n        cursor.close()\n        return cache[client_item_id]\n\ndef getUserId(db,cache,client_user_id):\n    if client_user_id in cache:\n        return cache[client_user_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select user_id,client_user_id from users"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            userId = long(row[0])\n            client_user_id_from_db = row[1]\n            cache[client_user_id_from_db] = userId\n\n        cursor.close()\n        return cache[client_user_id]\n\ndef import_actions(client_name, db_settings, data_file_fpath, out_file_fpath):\n\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    userCache = {}\n    itemCache = {}\n    count = 0\n    with open(data_file_fpath) as csvfile, open(out_file_fpath,\'w\') as outfile:\n        reader = unicodecsv.DictReader(csvfile,encoding=\'utf-8\')\n        for f in reader:\n            item = getItemId(db,itemCache,f[""item_id""])\n            user = getUserId(db,userCache,f[""user_id""])\n            action_type = 1\n            action = {}\n            action[""userid""] = int(user)\n            action[""client_userid""] = f[""item_id""]\n            action[""itemid""] = int(item)\n            action[""client_itemid""] = f[""user_id""]\n            action[""value""] = float(f[""value""])\n            utc = datetime.datetime.fromtimestamp(int(f[""time""])).strftime(\'%Y-%m-%dT%H:%M:%SZ\')\n            action[""timestamp_utc""] = utc\n            action[""rectag""] = ""default""\n            action[""type""] = action_type\n            action[""client""] = client_name\n            s = json.dumps(action,sort_keys=True)\n            outfile.write(s+""\\n"")\n            count += 1\n            if count % 50000 == 0:\n                print ""Processed ""+str(count)+"" actions""\n\n'"
python/seldon/cli/import_items_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\nITEM_MAP_VARCHAR_INSERT = ""insert into item_map_varchar (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DOUBLE_INSERT = ""insert into item_map_double (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DATETIME_INSERT = ""insert into item_map_datetime (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_ENUM_INSERT = ""insert into item_map_enum (item_id, attr_id, value_id) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), (select e.value_id from ITEM_ATTR_ENUM e, item_attr a where a.name = %(attr_name)s and e.value_name = %(value)s and a.attr_id = e.attr_id) )""\nITEM_MAP_TEXT_INSERT = ""insert into item_map_text (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_INT_INSERT = ""insert into item_map_int (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_BOOLEAN_INSERT = ""insert into item_map_boolean (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\n\nITEM_INSERT = ""INSERT INTO ITEMS (name, first_op, last_op, client_item_id, type) VALUES (%(name)s, NOW(), NOW(), %(id)s, 1)""\nITEM_INSERT_NO_AUTO_INCREMENT = ""INSERT INTO ITEMS (item_id, name, first_op, last_op, client_item_id, type) VALUES (%(item_id)s, %(name)s, NOW(), NOW(), %(id)s, 1)""\nDB_BATCH_SIZE = 1000\nattr_insert_map = {\n\t\'ENUM\': ITEM_MAP_ENUM_INSERT,\n\t\'BOOLEAN\': ITEM_MAP_BOOLEAN_INSERT,\n\t\'VARCHAR\': ITEM_MAP_VARCHAR_INSERT,\n\t\'TEXT\': ITEM_MAP_TEXT_INSERT,\n\t\'DATETIME\': ITEM_MAP_DATETIME_INSERT,\n\t\'INT\': ITEM_MAP_INT_INSERT,\n\t\'DOUBLE\': ITEM_MAP_DOUBLE_INSERT\n}\n\n\navailable_attrs = dict()\navailable_enums = dict()\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef retrieveDbAttrs(db):\n\tcur = db.cursor()\n\tcur.execute(""SELECT ATTR_ID, NAME, TYPE FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tattrs = dict()\n\tfor row in rows:\n\t\tattrs[row[1]]= (row[0], row[2])\n\n\treturn attrs\n\ndef retrieveDbEnums(db):\n\tcur = db.cursor()\n\t# enum structure:\n\t#    attr_id1:\n\t#\t\t\t\tvalue_name1 : value_id1\n\t#\t\t\t\tvalue_name2 :value_id2\n\tcur.execute(""SELECT ATTR_ID, VALUE_NAME, VALUE_ID FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tenums = defaultdict(dict)\n\tfor row in rows:\n\t\tenums[row[0]][row[1]] = row[2]\n\n\treturn enums\n\ndef validateCSVAgainstDb(csv_file, db):\n\tglobal available_attrs, available_enums\n\tfailed = False\n\tattrs = retrieveDbAttrs(db)\n\tavailable_attrs = attrs\n\tenums = retrieveDbEnums(db)\n\tavailable_enums = enums\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tnoOfFields = 0\n\t\tfor index, line in enumerate(reader, start=1):\n\t\t\tif index is 1:\n\t\t\t\tnoOfFields = len(line)\n\t\t\t\tif not validateFieldsAgainstDbFields(set(line), attrs, db):\n\t\t\t\t\texit(1)\n\t\t\tvalidateLine(index,line, noOfFields, attrs, enums)\n\t\t\tif len(line) != noOfFields:\n\t\t\t\tfailLine(index, line)\n\t\t\t\tfailed = True\n\n\n\tif failed:\n\t\texit(1)\n\ndef validateLine(index,line, noOfFields, attrs, enums):\n\tif len(line) != noOfFields:\n\t\tfailLine(index, line)\n\t\tfailed = True\n\telse:\n\t\tfor word in line:\n\t\t\tif str(word) == \'id\':\n\t\t\t\tcontinue\n\t\t\tif str(word) == \'name\':\n\t\t\t\tcontinue\n\t\t\tvalue = line[word]\n\t\t\tif str(attrs[word][1]) == \'ENUM\':\n\t\t\t\tif value not in enums[attrs[word][0]]:\n\t\t\t\t\tprint \'couldn\\\'t find enum value\', value\n\t\t\t\t\texit(1)\n\n\ndef validateFieldsAgainstDbFields(fields,attrs,  db):\n\tfailed = False\n\tfor field in fields:\n\t\tif field not in attrs and field != \'id\' and field != \'name\':\n\t\t\tfailed = True\n\t\t\tprint \'Field \\\'\',field,\'\\\'not an attribute in the DB\'\n\n\treturn not failed\n\ndef doItemInserts(csv_file, db):\n\tinsertNum = 0\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n                        inserts.append({\'name\':name,\'id\':client_id, \'item_id\':client_id})\n                        insertNum+=1\n                        if insertNum >= DB_BATCH_SIZE:\n                            cur = db.cursor()\n                            print ""inserting batch items into the db""\n                            cur.executemany(ITEM_INSERT_NO_AUTO_INCREMENT, inserts)\n                            insertNum = 0\n                            inserts = []\n                if insertNum > 0:\n                    cur = db.cursor()\n                    print ""inserting final batch items into the db""\n                    cur.executemany(ITEM_INSERT_NO_AUTO_INCREMENT, inserts)\n\t\tdb.commit()\n\t\tprint \'finished item inserts\'\n\ndef doAttrInserts(csv_file, db):\n\tinserts = defaultdict(list)\n\tinsertNum = 0\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tfor line in reader:\n\t\t\tfor field_name in line:\n\t\t\t\tif field_name == \'id\' or field_name== \'name\':\n\t\t\t\t\tcontinue\n\t\t\t\tattr_type = available_attrs[str(field_name)][1]\n\t\t\t\tinserts[attr_type].append({\'attr_name\': field_name, \'value\': line[field_name], \'id\': line[\'id\']})\n\t\t\t\tif len(inserts[attr_type]) > DB_BATCH_SIZE:\n\t\t\t\t\tinsertNum+=1\n\t\t\t\t\treallyDoInserts(inserts[attr_type], attr_insert_map[attr_type], insertNum, db)\n\t\t\t\t\tdel inserts[attr_type]\n\tfor index, insert_label in enumerate(inserts, start=1):\n\t\tinsertNum+=1\n\t\treallyDoInserts(inserts[insert_label], attr_insert_map[insert_label], insertNum, db)\n\tdb.commit()\n\tprint \'finished attribute inserts\'\n\ndef reallyDoInserts(params, insertStatement, insertNum, db):\n\t\tcur = db.cursor()\n\t\tprint ""inserting attribute batch"", insertNum,\'into the db\'\n\t\tcur.executemany(insertStatement, params)\n\ndef failLine(lineNum, line):\n\tprint ""line"",lineNum,""failed as it only had"",len(line),""fields""\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table items\')\n\tdbc.execute(\'truncate table item_map_varchar\')\n\tdbc.execute(\'truncate table item_map_double\')\n\tdbc.execute(\'truncate table item_map_datetime\')\n\tdbc.execute(\'truncate table item_map_int\')\n\tdbc.execute(\'truncate table item_map_boolean\')\n\tdbc.execute(\'truncate table item_map_enum\')\n\tdbc.execute(\'truncate table item_map_text\')\n\tdb.commit()\n\ndef import_items(client_name, db_settings, data_file_fpath):\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    db.set_character_set(\'utf8\')\n    dbc = db.cursor()\n    dbc.execute(\'SET NAMES utf8;\')\n    dbc.execute(\'SET CHARACTER SET utf8;\')\n    dbc.execute(\'SET character_set_connection=utf8;\')\n    #dbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\n    try:\n            validateCSVAgainstDb(data_file_fpath, db)\n            doItemInserts(data_file_fpath, db)\n            doAttrInserts(data_file_fpath,db)\n    except:\n            print \'Unexpected error ...\', sys.exc_info()[0]\n            print \'Clearing DB of items and attributes\'\n            try:\n                    cleanUpDb(db)\n            except:\n                    print \'couldn\\\'t clean up db\'\n            raise\n    print ""Successfully ran all inserts""\n\n'"
python/seldon/cli/import_users_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\nUSER_INSERT = ""insert into users (client_user_id, username, first_op, last_op,type,num_op, active) values (%(id)s, %(name)s, now(), now(), 1,1,1)""\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef validateCSV(csv_file):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tline = reader.next()\n\t\tfor field_name in line:\n\t\t\tif not field_name == \'id\' and not field_name == \'username\':\n\t\t\t\tprint \'only id or username fields allowed\'\n\t\t\t\texit(1)\n\ndef doUserInserts(csv_file, db):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tinsertNum = 0\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n\t\t\tinserts.append({\'name\':name,\'id\':client_id})\n\t\t\tif len(inserts) > 1000:\n\t\t\t\tinsertNum+=1\n\t\t\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\t\t\tinserts = []\n\n\t\tinsertNum+=1\n\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\tdb.commit()\n\t\tprint \'finished user inserts\'\n\ndef reallyDoInserts(insertStatement, params, num, db):\n\tcur = db.cursor()\n\tprint ""inserting user batch"", num,\'into the db\'\n\tcur.executemany(insertStatement, params)\n\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table users\')\n\ndef import_users(client_name, db_settings, data_file_fpath):\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    db.set_character_set(\'utf8\')\n    dbc = db.cursor()\n    dbc.execute(\'SET NAMES utf8;\')\n    dbc.execute(\'SET CHARACTER SET utf8;\')\n    dbc.execute(\'SET character_set_connection=utf8;\')\n    #dbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\n    try:\n            validateCSV(data_file_fpath)\n            doUserInserts(data_file_fpath, db)\n    except:\n            print \'Unexpected error ...\', sys.exc_info()[0]\n            print \'Clearing DB of users\'\n            try:\n                    cleanUpDb(db)\n            except:\n                    print \'couldn\\\'t clean up db\'\n            raise\n    print ""Successfully ran all inserts""\n\n'"
python/seldon/cli/seldon_utils.py,0,"b'from kazoo.client import KazooClient\nimport json, os, random, string\nimport MySQLdb\nimport sys\nimport errno\nimport requests\n\ndef retrieveDbSettings(data):\n\tdbs = {}\n        for db in data[""servers""]:\n\t\tdbs[db[""name""]] = {""host"":db[\'host\'], ""port"":db[\'port\'], ""user"":db[\'user\'], ""password"":db[\'password\']}\n\treturn dbs\n\ndef dbSetup(zk,data,zkNode):\n\tdbsProps = retrieveDbSettings(data)\n\tdbs = []\n\tfor dbName in dbsProps:\n\t\tprint ""Setting up DB \\\'""+ dbName+""\\\'""\n\t\tdb = dbsProps[dbName]\n\t\taddApiDb(dbName, db)\n\t\tjdbcString = ""jdbc:mysql:replication://HOST:PORT,HOST:PORT/?characterEncoding=utf8&useServerPrepStmts=true&logger=com.mysql.jdbc.log.StandardLogger&roundRobinLoadBalance=true&transformedBitIsBoolean=true&rewriteBatchedStatements=true""\n\t\tjdbcString = jdbcString.replace(""HOST"", db[""host""]).replace(""PORT"",str(db[""port""]));\n\t\tdel(db[""host""])\n\t\tdel(db[""port""])\n\t\tdb[\'jdbc\'] = jdbcString\n\t\tdb[\'name\'] = dbName\n\t\tdbs.append(db)\n\tdbcpObj = {""dbs"": dbs}\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,json.dumps(dbcpObj))\n\n\ndef memcachedSetup(zk, data, zkNode):\n\tservers = []\n\tprint ""Setting up memcache servers""\n\tfor server in data[""servers""]:\n\t\thost = server[\'host\']\n\t\tport = server[\'port\']\n\t\tserverStr = host+"":""+str(port)\n\t\tservers.append(serverStr)\n        server_list=str("","".join(servers))\n        zkNodeValueBuilder = {}\n        zkNodeValueBuilder[""servers""] = server_list\n        zkNodeValueBuilder[""numClients""] = 1\n        zkNodeValue = json.dumps(zkNodeValueBuilder)\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,zkNodeValue)\n\ndef addClientDb(clientName, dbSettings, consumer_details=None):\n\n\tjs_consumer_key     = consumer_details[\'js_consumer_key\']       if consumer_details != None and consumer_details.has_key(\'js_consumer_key\')     else None\n\tall_consumer_key    = consumer_details[\'all_consumer_key\']      if consumer_details != None and consumer_details.has_key(\'all_consumer_key\')    else None\n\tall_consumer_secret = consumer_details[\'all_consumer_secret\']   if consumer_details != None and consumer_details.has_key(\'all_consumer_secret\') else None\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n        db.autocommit(True)\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""dbschema/mysql/client.sql"")\n\tf = open(filename, \'r\')\n\tqueries = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'""+clientName+""\\\'"")\n\tif numrows < 1:\n\t\tcur.execute(""CREATE DATABASE ""+clientName)\n\t\tcur.execute(""USE ""+clientName)\n                for query in queries.split("";""):\n                        if len(query.strip()) > 0:\n                                cur.execute(query+"";"")\n\telse:\n\t\tprint(""Client \\\'""+clientName+""\\\' has already been added to the DB"")\n\tcur.execute(""USE API"")\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'js\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key = js_consumer_key if js_consumer_key != None else generateRandomString()\n\t\tprint ""Adding JS consumer key for client \\\'""+clientName +""\\\' : \\\'""+consumer_key+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\', \'\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'js\')"")\n\telse:\n\t\tprint ""JS Consumer key already added for client \\\'""+clientName+""\\\'""\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'all\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key    = all_consumer_key      if all_consumer_key != None     else generateRandomString()\n\t\tconsumer_secret = all_consumer_secret   if all_consumer_secret != None  else generateRandomString()\n\t\tprint ""Adding REST API key for client \\\'""+clientName +""\\\' : consumer_key=\\\'""+consumer_key+""\\\' consumer_secret=\\\'""+consumer_secret+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\',\\\'""+consumer_secret+""\\\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'all\')"")\n\telse:\n\t\tprint ""REST API key already added for client \\\'""+clientName+""\\\'""\n\ndef addApiDb(dbName, dbSettings):\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n        db.autocommit(True)\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""dbschema/mysql/api.sql"")\n\tf = open(filename, \'r\')\n\tqueries = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'api\\\'"")\n\tif numrows < 1:\n\t\tprint ""Adding api DB to MySQL DB \\\'""+dbName+""\\\'""\n                for query in queries.split("";""):\n                        if len(query.strip()) > 0:\n                                cur.execute(query+"";"")\n\telse:\n\t\tprint ""API DB has already been added to the MySQL DB \\\'""+dbName+""\\\'""\n\ndef clientSetup(zk, client_data, db_data, zkNode, consumer_details=None):\n\tdbs= retrieveDbSettings(db_data)\n\tfor client in client_data:\n\n\t\tprint ""Adding client \\\'""+client[\'name\']+""\\\'""\n\t\tdbname = client[\'db\']\n\t\tif client[\'db\'] is None:\n\t\t\tdbname = dbs.keys()[0]\n\t\taddClientDb(client[\'name\'],dbs[dbname], consumer_details)\n\t\tclientNode = zkNode + ""/"" + client[\'name\']\n\t\tzk.ensure_path(clientNode)\n\t\tclientNodeValue = {""DB_JNDI_NAME"":dbname}\n\t\tzk.set(clientNode,json.dumps(clientNodeValue))\n\t\tfor setting in client:\n\t\t\tif setting != ""name"" and setting != ""db"":\n\t\t\t\tzk.ensure_path(clientNode + ""/"" + setting)\n\t\t\t\tzk.set(clientNode + ""/"" + setting, str(client[setting]))\n\ndef generateRandomString():\n\treturn \'\'.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(20))\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    retVal = None\n    try:\n      retVal = json.loads(json_data)\n    except ValueError:\n        pass\n    return retVal\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef get_editor():\n    return os.environ[""EDITOR""] if os.environ.has_key(""EDITOR"") else ""vim""\n\n\ndef add_grafana_dashboard(grafana_endpoint,client,quiet,template,admin_password):\n        if template is None:\n                dir = os.path.dirname(os.path.abspath(__file__))\n                filename = os.path.join(dir, ""grafana/client-dashboard.json"")\n        else:\n                filename = template\n\tf = open(filename, \'r\')\n\tjStr = "" "".join(f.readlines())\n        jStr = jStr.replace(""%CLIENT%"",client)\n        headers = {}\n        headers[""content-type""] = ""application/json""\n        r = requests.post(grafana_endpoint+""/api/dashboards/db"",data=jStr,headers=headers,auth=(\'admin\', admin_password))\n        if not quiet:\n                print ""Adding grafana dashboard, response code"",r.status_code\n'"
python/seldon/cli/spark_utils.py,0,"b'import pprint\nfrom subprocess import call\nimport sys\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef run_spark_job(command_data, job_info, client_name):\n    conf_data = command_data[""conf_data""]\n    spark_home = conf_data[""spark_home""]\n    seldon_spark_home = conf_data[""seldon_spark_home""]\n    seldon_version = conf_data[""seldon_version""]\n    zk_hosts = conf_data[""zk_hosts""]\n    seldon_models = conf_data[""seldon_models""]\n    seldon_logs = conf_data[""seldon_logs""]\n\n    cmd = job_info[""cmd""].replace(""%SPARK_HOME%"", spark_home)\n\n    cmd_args = job_info[""cmd_args""]\n\n    replacements = [\n        (""%CLIENT_NAME%"", client_name),\n        (""%SPARK_HOME%"", spark_home),\n        (""%SELDON_SPARK_HOME%"", seldon_spark_home),\n        (""%SELDON_VERSION%"", seldon_version),\n        (""%ZK_HOSTS%"", zk_hosts),\n        (""%SELDON_MODELS%"", seldon_models),\n        (""%SELDON_LOGS%"", seldon_logs),\n    ]\n\n    def appy_replacements(item):\n        for rpair in replacements:\n            if rpair[1] != None:\n                item = item.replace(rpair[0],rpair[1])\n        return item\n\n    cmd_args = map(appy_replacements, cmd_args)\n\n    print ""Running spark job""\n    pp([cmd]+cmd_args)\n    sys.stdout.flush()\n    call([cmd]+cmd_args)\n\n'"
python/seldon/cli/zk_utils.py,0,"b'import json\nfrom os import walk\nimport os \nimport sys\nimport errno\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\n\ndef is_json_data(data):\n    if (data != None) and (len(data)>0):\n        return data[0] == \'{\' or data[0] == \'[\'\n    else:\n        return False\n\ndef push_all_nodes(zk_client,zkroot):\n    for (dirpath, dirnames, filenames) in walk(zkroot):\n        for filename in filenames:\n            file_path = dirpath + ""/"" + filename\n            f = open(file_path)\n            data = f.read()\n            f.close()\n            node_path = file_path.replace(zkroot,"""").replace(""/_data_"","""")\n            node_set(zk_client,node_path,data)\n\ndef get_all_nodes_list(zk_client, start_node, all_nodes_list):\n    #print ""processing: {}"".format(start_node)\n    try:\n        children = zk_client.get_children(start_node)\n        for child in children:\n            child = str(child)\n            node_path = start_node+""/""+child if start_node != \'/\' else ""/""+child\n            all_nodes_list.add(node_path)\n            get_all_nodes_list(zk_client, node_path, all_nodes_list)\n    except kazoo.exceptions.NoNodeError:\n        pass\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef pull_all_nodes(zk_client,zkroot):\n    all_nodes_list = set()\n    nodes = [""/config"",""/all_clients""]\n    for node in nodes:\n        start_node = node\n        get_all_nodes_list(zk_client, start_node, all_nodes_list)\n    all_nodes_list = list(all_nodes_list)\n    for node_path in all_nodes_list:\n        if node_path == ""/config/topics"" or node_path == ""/config/clients"" or node_path == ""/config/changes"" or node_path == ""/config/users"":\n            print ""Ignoring kafka data node "",node_path\n        else:\n            print ""trying to sync "",node_path\n            node_value = node_get(zk_client,node_path)\n            if not node_value is None:\n                node_value = node_value.strip()\n                if is_json_data(node_value):\n                    data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n                else:\n                    data = str(node_value)\n                data_fpath = zkroot + node_path + ""/_data_""\n                write_data_to_file(data_fpath, data)\n\ndef json_compress(json_data):\n    d = json.loads(json_data)\n    return json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef node_set(zk_client, node_path, node_value):\n    if is_json_data(node_value):\n        node_value = json_compress(node_value)\n    node_value = node_value.strip() if node_value != None else node_value\n\n    if zk_client.exists(node_path):\n        retVal = zk_client.set(node_path,node_value)\n    else:\n        retVal = zk_client.create(node_path,node_value,makepath=True)\n    print ""updated zk node[{node_path}]"".format(node_path=node_path)\n\ndef node_get(zk_client, node_path):\n    theValue = None\n    if zk_client.exists(node_path):\n        theValue = zk_client.get(node_path)\n        theValue = theValue[0]\n    return theValue.strip() if theValue != None else theValue\n\ndef node_delete(zk_client, node_path):\n    if zk_client.exists(node_path):\n        retVal = zk_client.delete(node_path)\n        print ""deleted zk node[{node_path}]"".format(node_path=node_path)\n'"
python/seldon/luigi/__init__.py,0,b''
python/seldon/luigi/spark.py,0,"b'import luigi\nfrom subprocess import call\nimport logging\nfrom seldon.misc.item_similarity import *\nfrom seldon.misc.most_popular import *\nfrom luigi.contrib.spark import SparkSubmitTask\n\n#\n# Item Similarity\n#\n\nclass ItemSimilaritySparkJob(luigi.Task):\n    """"""\n    Spark job for running item similarity model\n    """"""\n    inputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    sparkDriverMemory = luigi.Parameter(default=""1g"")\n    sparkExecutorMemory = luigi.Parameter(default=""1g"")\n    startDay = luigi.IntParameter(default=1)\n    days = luigi.IntParameter(default=1)\n    itemType = luigi.IntParameter(-1)\n    limit = luigi.IntParameter(default=100)\n    minItemsPerUser = luigi.IntParameter(default=0)\n    minUsersPerItem = luigi.IntParameter(default=0)    \n    maxUsersPerItem = luigi.IntParameter(default=2000000)\n    dimsumThreshold =luigi.FloatParameter(default=0.1)\n    sample = luigi.FloatParameter(default=1.0)\n\n    def output(self):\n        return luigi.LocalTarget(""{}/{}/item-similarity/{}"".format(self.outputPath,self.client,self.startDay))\n\n    def run(self):\n        params = [""seldon-cli"",""model"",""--action"",""add"",""--client-name"",self.client,""--model-name"",""similar-items"",""--inputPath"",self.inputPath,""--outputPath"",self.outputPath,""--startDay"",str(self.startDay),""--days"",str(self.days),""--sample"",str(self.sample),""--itemType"",str(self.itemType),""--limit"",str(self.limit),""--minItemsPerUser"",str(self.minItemsPerUser),""--minUsersPerItem"",str(self.minUsersPerItem),""--maxUsersPerItem"",str(self.maxUsersPerItem),""--dimsumThreshold"",str(self.dimsumThreshold)]\n        res = call(params)\n        params = [""seldon-cli"",""model"",""--action"",""train"",""--client-name"",self.client,""--model-name"",""similar-items"",""--spark-executor-memory"",self.sparkExecutorMemory,""--spark-driver-memory"",self.sparkDriverMemory]\n        res = call(params)\n        return res\n\n\nclass SeldonItemSimilarity(luigi.Task):\n    """"""\n    Item similarity model. Depends on spark job. Writes results to mysql db.\n    """"""\n    startDay = luigi.IntParameter(default=1)\n    client = luigi.Parameter(default=""test"")\n    db_host = luigi.Parameter(default=""mysql"")\n    db_user = luigi.Parameter(default=""root"")\n    db_pass = luigi.Parameter(default=""mypass"")\n\n    def requires(self):\n        return ItemSimilaritySparkJob(client=self.client,startDay=self.startDay)\n    \n    def run(self):\n        u = ItemSimilarityUploadMysql(self.client,self.db_host,self.db_user,self.db_pass)\n        u.stream_and_upload(self.input().path)\n\n\n#\n# MF\n#\n\nclass SeldonMatrixFactorization(luigi.Task):\n    """"""\n    Matrix factorization using Spark\n    """"""\n    inputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    sparkDriverMemory = luigi.Parameter(default=""1g"")\n    sparkExecutorMemory = luigi.Parameter(default=""1g"")\n    startDay = luigi.IntParameter(default=1)\n    days = luigi.IntParameter(default=1)\n\n    rank = luigi.IntParameter(default=30)\n    mf_lambda = luigi.FloatParameter(default=0.01)\n    alpha = luigi.FloatParameter(default=1)\n    iterations = luigi.IntParameter(default=5)\n\n    def output(self):\n        return luigi.LocalTarget(""{}/{}/matrix-factorization/{}"".format(self.outputPath,self.client,self.startDay))\n\n    def run(self):\n        params = [""seldon-cli"",""model"",""--action"",""add"",""--client-name"",self.client,""--model-name"",""matrix-factorization"",""--inputPath"",self.inputPath,""--outputPath"",self.outputPath,""--startDay"",str(self.startDay),""--days"",str(self.days),""--rank"",str(self.rank),""--lambda"",str(self.mf_lambda),""--alpha"",str(self.alpha),""--iterations"",str(self.iterations)]\n        res = call(params)\n        params = [""seldon-cli"",""model"",""--action"",""train"",""--client-name"",self.client,""--model-name"",""matrix-factorization"",""--spark-executor-memory"",self.sparkExecutorMemory,""--spark-driver-memory"",self.sparkDriverMemory]\n        res = call(params)\n        return res\n\n\nclass SeldonMatrixFactorizationClusters(luigi.Task):\n    """"""\n    User Clustered Matrix factorization using Spark\n    """"""\n    inputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    sparkDriverMemory = luigi.Parameter(default=""1g"")\n    sparkExecutorMemory = luigi.Parameter(default=""1g"")\n    startDay = luigi.IntParameter(default=1)\n    days = luigi.IntParameter(default=1)\n\n    rank = luigi.IntParameter(default=30)\n    mf_lambda = luigi.FloatParameter(default=0.01)\n    alpha = luigi.FloatParameter(default=1)\n    iterations = luigi.IntParameter(default=5)\n\n    def output(self):\n        return luigi.LocalTarget(""{}/{}/matrix-factorization-clusters/{}"".format(self.outputPath,self.client,self.startDay))\n\n    def run(self):\n        params = [""seldon-cli"",""model"",""--action"",""add"",""--client-name"",self.client,""--model-name"",""matrix-factorization-clusters"",""--inputPath"",self.inputPath,""--outputPath"",self.outputPath,""--startDay"",str(self.startDay),""--days"",str(self.days),""--rank"",str(self.rank),""--lambda"",str(self.mf_lambda),""--alpha"",str(self.alpha),""--iterations"",str(self.iterations)]\n        res = call(params)\n        params = [""seldon-cli"",""model"",""--action"",""train"",""--client-name"",self.client,""--model-name"",""matrix-factorization-clusters"",""--spark-executor-memory"",self.sparkExecutorMemory,""--spark-driver-memory"",self.sparkDriverMemory]\n        res = call(params)\n        return res\n\n\nclass SeldonMostPopularDim(luigi.Task):\n    """"""\n    Most Popular by Dimension using Spark\n    """"""\n    inputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    sparkDriverMemory = luigi.Parameter(default=""1g"")\n    sparkExecutorMemory = luigi.Parameter(default=""1g"")\n    startDay = luigi.IntParameter(default=1)\n    days = luigi.IntParameter(default=1)\n\n    k = luigi.IntParameter(default=28)\n    db_host = luigi.Parameter(default=""mysql"")\n    db_port = luigi.IntParameter(default=3306)\n    db_user = luigi.Parameter(default=""root"")\n    db_pass = luigi.Parameter(default=""mypass"")\n    \n    def output(self):\n        return luigi.LocalTarget(""{}/{}/mostpopulardim/{}"".format(self.outputPath,self.client,self.startDay))\n\n    def run(self):\n        jdbc = ""jdbc:mysql://""+self.db_host+"":""+str(self.db_port)+""/""+self.client+""?characterEncoding=utf8&user=""+self.db_user+""&password=""+self.db_pass\n        params = [""seldon-cli"",""model"",""--action"",""add"",""--client-name"",self.client,""--model-name"",""mostpopulardim"",""--inputPath"",self.inputPath,""--outputPath"",self.outputPath,""--startDay"",str(self.startDay),""--days"",str(self.days),""--jdbc"",jdbc,""--k"",str(self.k)]\n        res = call(params)\n        params = [""seldon-cli"",""model"",""--action"",""train"",""--client-name"",self.client,""--model-name"",""mostpopulardim"",""--spark-executor-memory"",self.sparkExecutorMemory,""--spark-driver-memory"",self.sparkDriverMemory]\n        res = call(params)\n        return res\n\nclass MostPopularSparkJob(luigi.Task):\n    """"""\n    Most Popular using Spark\n    """"""\n    inputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    sparkDriverMemory = luigi.Parameter(default=""1g"")\n    sparkExecutorMemory = luigi.Parameter(default=""1g"")\n    startDay = luigi.IntParameter(default=1)\n    days = luigi.IntParameter(default=1)\n\n    def output(self):\n        return luigi.LocalTarget(""{}/{}/mostpopular/{}"".format(self.outputPath,self.client,self.startDay))\n\n    def run(self):\n        params = [""seldon-cli"",""model"",""--action"",""add"",""--client-name"",self.client,""--model-name"",""mostpopular"",""--inputPath"",self.inputPath,""--outputPath"",self.outputPath,""--startDay"",str(self.startDay),""--days"",str(self.days)]\n        res = call(params)\n        params = [""seldon-cli"",""model"",""--action"",""train"",""--client-name"",self.client,""--model-name"",""mostpopular"",""--spark-executor-memory"",self.sparkExecutorMemory,""--spark-driver-memory"",self.sparkDriverMemory]\n        res = call(params)\n        return res\n\n\nclass SeldonMostPopular(luigi.Task):\n    """"""\n    Most Popular. Depends on spark job. Writes results to mysql db.\n    """"""\n    startDay = luigi.IntParameter(default=1)\n    client = luigi.Parameter(default=""test"")\n    db_host = luigi.Parameter(default=""mysql"")\n    db_user = luigi.Parameter(default=""root"")\n    db_pass = luigi.Parameter(default=""mypass"")\n\n    def requires(self):\n        return MostPopularSparkJob(client=self.client,startDay=self.startDay)\n    \n    def run(self):\n        u = MostPopularUploadMysql(self.client,self.db_host,self.db_user,self.db_pass)\n        u.stream_and_upload(self.input().path)\n\n\nclass SeldonSparkJob(SparkSubmitTask):\n    """"""\n    Template for running a Spark Job\n    """"""\n\n    app = ""/home/seldon/libs/seldon-spark.jar""\n    entry_class = ""io.seldon.spark.mllib.SimilarItems""\n    master = ""spark://spark-master:7077""\n    \n    outputPath = luigi.Parameter(default=""/seldon-data/seldon-models/"")\n    client = luigi.Parameter(default=""test"")\n    startDay = luigi.IntParameter(default=17278)\n                    \n    def app_options(self):\n        return [""--client"",self.client,""--zookeeper"",""zookeeper-1""]\n\n    def output(self):\n        return luigi.LocalTarget(""{}/{}/item-similarity/{}"".format(self.outputPath,self.client,self.startDay))\n\n\n\n'"
python/seldon/microservice/__init__.py,0,b'\nfrom .microservice import Microservices\n'
python/seldon/microservice/extension.py,0,"b'from flask import Blueprint, render_template, jsonify, current_app\nfrom flask import request\nimport json\n\n\nextension_blueprint = Blueprint(\'extension\', __name__)\n\ndef extract_input():\n    if not request.args.get(\'json\') is None:\n        jStr = request.args.get(\'json\')\n    else:\n        jStr = request.form.get(\'json\')\n    j = json.loads(jStr)\n    return j\n\n@extension_blueprint.route(\'/extension\',methods=[\'GET\',\'POST\'])\ndef do_extension():\n    """"""\n    extension endpoint\n\n    """"""\n    input = extract_input()\n    print ""input is "",input\n    extension = current_app.config[""seldon_extension""]\n    preds = extension.predict(input=input)\n    print ""returned preds"",preds\n    json = jsonify(preds)\n    return json\n'"
python/seldon/microservice/microservice.py,0,"b'from flask import Flask\nfrom concurrent import futures\nimport time\nfrom seldon.microservice.predict import predict_blueprint\nfrom seldon.microservice.recommend import recommend_blueprint\nfrom seldon.microservice.extension import extension_blueprint\nfrom seldon.microservice.rpc import RpcClassifier\nfrom seldon.microservice.rpc import DefaultCustomDataHandler\nimport seldon\nfrom sklearn.pipeline import Pipeline\nimport seldon.pipeline.util as sutl\nimport random\nimport pylibmc\nfrom seldon.rpc import seldon_pb2\nimport grpc\n\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\nclass Microservices(object):\n    """"""\n    Allow creation of predict and recommender microservices\n\n    aws_key : str, optional\n       aws key\n    aws_secret : str, optional\n       aws secret\n    """"""\n    def __init__(self,aws_key=None,aws_secret=None):\n        self.aws_key = aws_key\n        self.aws_secret = aws_secret\n\n    def create_prediction_microservice(self,pipeline_folder,model_name):\n        """"""\n        Create a prediction Flask microservice app\n\n        Parameters\n        ----------\n\n        pipeline_folder : str\n           location of pipeline\n        model_name : str\n           model name to use for this pipeline\n        """"""\n        app = Flask(__name__)\n                   \n        rint = random.randint(1,999999)\n        pw = sutl.PipelineWrapper(work_folder=\'/tmp/pl_\'+str(rint),aws_key=self.aws_key,aws_secret=self.aws_secret)\n        pipeline = pw.load_pipeline(pipeline_folder)\n        \n        app.config[""seldon_pipeline_wrapper""] = pw\n        app.config[""seldon_pipeline""] = pipeline\n        app.config[""seldon_model_name""] = model_name\n \n        app.register_blueprint(predict_blueprint)\n\n        # other setup tasks\n        return app\n\n    def create_prediction_rpc_microservice(self,pipeline_folder,model_name,custom_data_handler=DefaultCustomDataHandler()):\n        rint = random.randint(1,999999)\n        pw = sutl.PipelineWrapper(work_folder=\'/tmp/pl_\'+str(rint),aws_key=self.aws_key,aws_secret=self.aws_secret)\n        pipeline = pw.load_pipeline(pipeline_folder)\n        server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n        seldon_pb2.add_SeldonServicer_to_server(RpcClassifier(pipeline,model_name,custom_data_handler), server)\n        server.add_insecure_port(\'[::]:5000\')\n        server.start()\n        try:\n            while True:\n                time.sleep(_ONE_DAY_IN_SECONDS)\n        except KeyboardInterrupt:\n            server.stop(0)\n\n\n\n    def create_recommendation_microservice(self,recommender_folder,memcache_servers=None,memcache_pool_size=2):\n        """"""\n        create recommedation Flask microservice app\n\n        Parameters\n        ----------\n\n        recommender_folder : str\n           location of recommender model files\n        memcache_servers : comma separated string, optional\n           memcache server locations, e.g., 127.0.0.1:11211 \n        memcache_pool_size : int, optional\n           size of memcache pool\n        """"""\n        app = Flask(__name__)\n\n        if not memcache_servers is None:\n            mc = pylibmc.Client(memcache_servers)\n            _mc_pool = pylibmc.ClientPool(mc, memcache_pool_size)\n            app.config[""seldon_memcache""] = _mc_pool\n            \n        if self.aws_key:\n            rw = seldon.RecommenderWrapper(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        else:\n            rw = seldon.RecommenderWrapper()\n        recommender = rw.load_recommender(recommender_folder)\n        app.config[""seldon_recommender""] = recommender\n \n        app.register_blueprint(recommend_blueprint)\n\n        # other setup tasks\n        return app\n\n\n    def create_extension_microservice(self,extension_folder):\n        """"""\n        Create a prediction Flask microservice app\n\n        Parameters\n        ----------\n\n        extension_folder : str\n           location of extension\n        """"""\n        app = Flask(__name__)\n                   \n        rint = random.randint(1,999999)\n        ew = seldon.ExtensionWrapper(work_folder=\'/tmp/pl_\'+str(rint),aws_key=self.aws_key,aws_secret=self.aws_secret)\n        extension = ew.load_extension(extension_folder)\n\n        app.config[""seldon_extension_wrapper""] = ew\n        app.config[""seldon_extension""] = extension\n \n        app.register_blueprint(extension_blueprint)\n\n        # other setup tasks\n        return app\n\n\n'"
python/seldon/microservice/predict.py,0,"b'from flask import Blueprint, render_template, jsonify, current_app\nfrom flask import request\nimport json\n\npredict_blueprint = Blueprint(\'predict\', __name__)\n\ndef extract_input():\n    client = request.args.get(\'client\')\n    if not request.args.get(\'json\') is None:\n        jStr = request.args.get(\'json\')\n    else:\n        jStr = request.form.get(\'json\')\n    j = json.loads(jStr)\n    input = {\n        ""client"" : client,\n        ""json"" : j\n    }\n    return input\n\n@predict_blueprint.route(\'/predict\',methods=[\'GET\',\'POST\'])\ndef do_predict():\n    """"""\n    prediction endpoint\n\n    - get recommeder from Flask app config\n    - create dataframe from JSON in call\n    - call prediction pipeline \n    - get class id mapping\n    - construct result\n    """"""\n    input = extract_input()\n    print input\n    pw = current_app.config[""seldon_pipeline_wrapper""]\n    pipeline = current_app.config[""seldon_pipeline""]\n    df = pw.create_dataframe(input[""json""][""data""])\n    preds = pipeline.predict_proba(df)\n    idMap = pipeline._final_estimator.get_class_id_map()\n    formatted_recs_list=[]\n    for index, proba in enumerate(preds[0]):\n        if index in idMap:\n            indexName = idMap[index]\n        else:\n            indexName = str(index)\n        formatted_recs_list.append({\n            ""prediction"": str(proba),\n            ""predictedClass"": indexName,\n            ""confidence"" : str(proba)\n        })\n    ret = { ""predictions"": formatted_recs_list , ""meta"": {""modelName"" : current_app.config[\'seldon_model_name\']}}\n    json = jsonify(ret)\n    return json\n'"
python/seldon/microservice/recommend.py,0,"b'from flask import Blueprint, render_template, jsonify, current_app\nfrom flask import request\nimport json\n\nrecommend_blueprint = Blueprint(\'recommend\', __name__)\n\ndef extract_input():\n    user_id = long(request.args.get(\'user_id\'))\n    client = request.args.get(\'client\')\n    limit = int(request.args.get(\'limit\'))\n    exclusion_items = request.args.get(\'exclusion_items\')\n    if not exclusion_items is None and len(exclusion_items) > 0:\n        exclusion_items_list = map(lambda x: long(x), exclusion_items.split("",""))\n    else:\n        exclusion_items_list = []\n    recent_interactions = request.args.get(\'recent_interactions\')\n    if not recent_interactions is None and len(recent_interactions) > 0:\n        recent_interactions_list = map(lambda x: long(x), recent_interactions.split("",""))\n    else:\n        recent_interactions_list = []\n    data_keys = request.args.get(\'data_key\')\n    if not data_keys is None:\n        data_keys_list = map(lambda x: str(x), data_keys.split("",""))\n    else:\n        data_keys_list = []\n    input = {\n        ""user_id"" : user_id,\n        ""client"" : client,\n        ""limit"" : limit,\n        ""exclusion_items_list"" : exclusion_items_list,\n        ""recent_interactions_list"": recent_interactions_list,\n        ""data_keys_list"": data_keys_list\n    }\n    return input\n\ndef format_recs(recs):\n    formatted_recs_list=[]\n    for (item,score) in recs:\n        formatted_recs_list.append({\n            ""item"": item,\n            ""score"": score\n        })\n    return { ""recommended"": formatted_recs_list }\n\ndef get_data_set(raw_data):\n    return set(json.loads(raw_data))\n\ndef memcache_get(key):\n    key=str(key)\n    value=None\n    _mc_pool = current_app.config.get(""seldon_memcache"")\n    if not _mc_pool is None:\n        with _mc_pool.reserve(block=True) as mc:\n            value = mc.get(key)\n        return value\n    else:\n        return None\n\n\n@recommend_blueprint.route(\'/recommend\',methods=[\'GET\',\'POST\'])\ndef do_recommend():\n    """"""\n    recommendation endpoint\n\n    - extract parameters from call\n    - extract items to score from any data keys provided\n    - get recommender from Flask app config\n    - call recommender\n    - construct JSON response\n    """"""\n    input = extract_input()\n\n    data_set = set()\n    for data_key in input[\'data_keys_list\']:\n        raw_data = memcache_get(data_key)\n        raw_data = raw_data if raw_data != None else \'[]\'\n        data_set |= get_data_set(raw_data)\n\n    data_set -= set(input[\'exclusion_items_list\'])\n\n    recommender = current_app.config[""seldon_recommender""]\n    recs = recommender.recommend(input[\'user_id\'],ids=data_set,recent_interactions=input[\'recent_interactions_list\'],limit=input[\'limit\'],client=input[\'client\'])\n\n    f=format_recs(recs)\n    json = jsonify(f)\n    return json\n\n\n'"
python/seldon/microservice/rpc.py,0,"b""from concurrent import futures\nimport time\nimport sys, getopt, argparse\nimport seldon.pipeline.util as sutl\nimport random\nimport seldon.rpc.seldon_pb2 as seldon_pb2\nimport grpc\nimport google.protobuf\nfrom google.protobuf import any_pb2\nimport numpy as np\n\n_ONE_DAY_IN_SECONDS = 60 * 60 * 24\n\nclass CustomDataHandler():\n\n    def getData(request):\n        return pd.DataFrame()\n\n\nclass BadDataError(Exception):\n    def __init__(self, value):\n        self.value = value\n    def __str__(self):\n        return repr(self.value)\n\nclass DefaultCustomDataHandler(CustomDataHandler):\n\n    def getData(self, request):\n        anyMsg = request.data\n        dc = seldon_pb2.DefaultCustomPredictRequest()\n        success = anyMsg.Unpack(dc)\n        if success:\n            x = np.array(dc.values)\n            x = x.reshape(1, -1)\n            return x\n        else:\n            context.set_code(grpc.StatusCode.INTERNAL)\n            context.set_details('Invalid data')\n            raise BadDataError('Invalid data')\n\n\nclass RpcClassifier(seldon_pb2.SeldonServicer):\n\n    def __init__(self,pipeline,model_name,custom_data_handler=DefaultCustomDataHandler()):\n        self.pipeline = pipeline\n        self.model_name = model_name\n        self.custom_data_handler = custom_data_handler\n\n    def Classify(self, request, context):\n        print request # custom prediction data\n        df = self.custom_data_handler.getData(request)\n        preds = self.pipeline.predict_proba(df)\n        idMap = self.pipeline._final_estimator.get_class_id_map()\n        recs_list=[]\n        for index, proba in enumerate(preds[0]):\n            if index in idMap:\n                indexName = idMap[index]\n            else:\n                indexName = str(index)\n            recs_list.append(seldon_pb2.ClassificationResult(prediction=float(proba),predictedClass=indexName,confidence=float(proba)))\n        meta = seldon_pb2.ClassificationReplyMeta(modelName=self.model_name)\n        predictions = seldon_pb2.ClassificationReply(meta=meta,predictions=recs_list)\n        return predictions\n\n"""
python/seldon/misc/__init__.py,0,b''
python/seldon/misc/item_similarity.py,0,"b'#!/usr/bin/python\nimport zlib\nimport boto\nimport getopt, argparse\nimport json\nimport sys \nimport MySQLdb\nfrom seldon import fileutil as fu\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass ItemSimilarityUploadMysql(object):\n    """"""\n    Upload results of item similarity training to mysql database\n    """"""\n    def __init__(self,client,db_host,db_user,db_pass):\n        self.db = MySQLdb.connect(\n            host=db_host,\n            user=db_user,\n            passwd=db_pass,\n            db=client\n        )\n\n        self.db.set_character_set(\'utf8\')\n        self.count = 0\n        self.rows = 0\n        self.inserts = []\n        self.DB_BATCH_SIZE = 5000\n\n    def truncate_table(self):\n        logger.info(""truncate table"")\n        dbc = self.db.cursor()\n        dbc.execute(\'truncate item_similarity_new\')\n        dbc.close()\n\n    def rename_table(self):\n        logger.info(""renaming table"")\n        dbc = self.db.cursor()\n        dbc.execute(\'rename table item_similarity to item_similarity_old,item_similarity_new to item_similarity,item_similarity_old to item_similarity_new\')\n        dbc.close()\n\n    def reallyDoInserts(self,params):\n        cur = self.db.cursor()\n        cur.executemany(""insert into item_similarity_new values (%(item1)s,%(item2)s,%(sim)s)"", params)\n        cur.close()\n\n    def upload(self,line):\n        line = line.rstrip()\n        j = json.loads(line)\n        item1 = int(j[\'item1\'])\n        item2 = int(j[\'item2\'])\n        sim = float(j[\'sim\'])\n        if item1>0 and item2>0:\n            self.inserts.append({\'item1\': item1, \'item2\': item2, \'sim\': sim})\n        if len(self.inserts) > self.DB_BATCH_SIZE:\n            self.count += 1\n            self.rows += self.DB_BATCH_SIZE\n            logger.info(""Running batch %d rows inserted %d"",self.count,self.rows)\n            self.reallyDoInserts(self.inserts)\n            self.inserts = []\n\n    def stream_and_upload(self,folder):\n        self.truncate_table()\n        futl = fu.FileUtil()\n        futl.stream(folder,self.upload)\n        if len(self.inserts) > 0:\n            logger.info(""Running final batch with rows inserted %d"",self.rows)\n            self.reallyDoInserts(self.inserts)\n        self.rename_table()\n        self.db.commit()\n\n\n'"
python/seldon/misc/most_popular.py,0,"b'#!/usr/bin/python\nimport zlib\nimport boto\nimport getopt, argparse\nimport json\nimport sys \nimport MySQLdb\nfrom seldon import fileutil as fu\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass MostPopularUploadMysql(object):\n    """"""\n    Upload results of most popular counts to mysql database for serving\n    """"""\n    def __init__(self,client,db_host,db_user,db_pass):\n        self.db = MySQLdb.connect(\n            host=db_host,\n            user=db_user,\n            passwd=db_pass,\n            db=client\n        )\n\n        self.db.set_character_set(\'utf8\')\n        self.count = 0\n        self.rows = 0\n        self.inserts = []\n        self.DB_BATCH_SIZE = 5000\n        self.SQL_INSERT = ""insert into items_recent_popularity_new (item_id,score) values (%(item)s,%(count)s)""\n\n    def truncate_table(self):\n        logger.info(""truncate table"")\n        dbc = self.db.cursor()\n        dbc.execute(\'truncate items_recent_popularity_new\')\n        dbc.close()\n\n    def rename_table(self):\n        dbc = self.db.cursor()\n        dbc.execute(\'rename table items_recent_popularity to items_recent_popularity_old,items_recent_popularity_new to items_recent_popularity,items_recent_popularity_old to items_recent_popularity_new\')\n        dbc.close()\n\n    def add_popular_assets(self):\n        logger.info(""adding assets to recent popularity"")\n        dbc = self.db.cursor()\n        dbc.execute(\'insert ignore into items_recent_popularity select * from items_recent_popularity_assets\')\n        dbc.close()\n\n    def reallyDoInserts(self,params):\n        cur = self.db.cursor()\n        cur.executemany(self.SQL_INSERT, params)\n        cur.close()\n\n    def upload(self,line):\n        line = line.rstrip()\n        j = json.loads(line)\n        item = int(j[\'item\'])\n        count = float(j[\'count\'])\n        self.inserts.append({\'item\': item, \'count\': count})\n        if len(self.inserts) > self.DB_BATCH_SIZE:\n            self.count += 1\n            self.rows += self.DB_BATCH_SIZE\n            logger.info(""Running batch %d rows inserted %d"",self.count,self.rows)\n            self.reallyDoInserts(self.inserts)\n            self.inserts = []\n\n    def stream_and_upload(self,folder):\n        self.truncate_table()\n        futl = fu.FileUtil()\n        futl.stream(folder,self.upload)\n        if len(self.inserts) > 0:\n            logger.info(""Running final batch with rows inserted %d"",self.rows)\n            self.reallyDoInserts(self.inserts)\n        self.rename_table()\n        self.add_popular_assets()\n        self.db.commit()\n\n\n'"
python/seldon/pipeline/__init__.py,0,"b'from .cross_validation import SeldonKFold, Seldon_KFold\n'"
python/seldon/pipeline/auto_transforms.py,0,"b'from sklearn import preprocessing\nfrom dateutil.parser import parse\nimport datetime\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport math\nimport itertools\nfrom sklearn.base import BaseEstimator,TransformerMixin\nimport logging\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\nclass AutoTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Automatically transform a set of features into normalzied numeric or categorical features or dates\n\n    Parameters\n    ----------\n\n    exclude : list str, optional\n       list of features to not include\n    include : list str, optional\n       features to include if None then all unless exclude used\n    max_values_numeric_categorical : int, optional\n       max number of unique values for numeric feature to treat as categorical\n    custom_date_formats : list str, optional\n       list of custom date formats to try\n    ignore_vals : list str, optional\n       list of feature values to treat as NA/ignored values\n    force_categorical : list str, optional\n       features to force to be categorical\n    min_cat_percent : list str, optional\n       min percentage for a categorical value to be kept\n    max_cat_percent : float, optional\n       max percentage for a categorical value to be kept\n    bool_map : dict, optional\n       set of string values to be treated as boolean\n    cat_missing_value : str, optional\n       string to use for missing categorical values\n    date_transforms : list bool, optional\n       which date transforms to apply [hour,month,day_of_week,year], default is all\n    create_date_differences : bool, optional\n       whether to create differences between all date variables\n    nan_threshold : float, optional\n       featurs to drop if too many nan, threshold is between 0-1 as percent\n    drop_constant_features : bool, optional\n       drop a column if its value is constant\n    drop duplicate columns : bool, optional\n       drop duplicate columns\n    min_max_limit : bool, optional\n       limit numeric cols to min and max seen in fit\n    """"""\n    def __init__(self,exclude=[],include=None,max_values_numeric_categorical=0,date_cols=[],custom_date_formats=None,ignore_vals=None,force_categorical=[],min_cat_percent=0.0,max_cat_percent=1.0,bool_map={""true"":1,""false"":0,""1"":1,""0"":0,""yes"":1,""no"":0,""1.0"":1,""0.0"":0},cat_missing_val=""UKN"",date_transforms=[True,True,True,True],create_date_differences=False,nan_threshold=None,drop_constant_features=True,drop_duplicate_cols=True,min_max_limit=False):\n        super(AutoTransform, self).__init__()\n        self.exclude = exclude\n        self.include = include\n        self.max_values_numeric_categorical = max_values_numeric_categorical\n        self.scalers = {}\n        self.date_diff_scalers = {}\n        self.custom_date_formats = custom_date_formats\n        self.ignore_vals  = ignore_vals\n        self.force_categorical = force_categorical\n        self.catValueCount = {}\n        self.convert_categorical = []\n        self.convert_date = []\n        self.date_cols = date_cols\n        self.min_cat_percent = min_cat_percent\n        self.max_cat_percent = max_cat_percent\n        self.cat_percent = {}\n        self.bool_map = bool_map\n        self.convert_bool = []\n        self.cat_missing_val = cat_missing_val\n        self.date_transforms=date_transforms\n        self.create_date_differences = create_date_differences\n        self.nan_threshold=nan_threshold\n        self.drop_cols = []\n        self.drop_constant_features=drop_constant_features\n        self.drop_duplicate_cols=drop_duplicate_cols\n        self.min_max_limit=min_max_limit\n        self.min_max = {}\n\n    def _scale(self,v,col):\n        if np.isnan(v):\n            return 0.0\n        else:\n            return self.scalers[col].transform([[float(v)]])[0,0]\n\n    def _scale_date_diff(self,v,col):\n        if np.isnan(v):\n            return 0.0\n        else:\n            return self.date_diff_scalers[col].transform([[float(v)]])[0,0]\n\n    @staticmethod\n    def _is_number(s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n\n    def _make_cat(self,v,col):\n        if not isinstance(v,basestring) and np.isnan(v):\n            return self.cat_missing_val\n        else:\n            if col in self.cat_percent and v in self.cat_percent[col] and self.cat_percent[col][v] >= self.min_cat_percent and self.cat_percent[col][v] <= self.max_cat_percent:\n                val = unicode(str(v), ""utf-8"")\n                if self._is_number(v):\n                    val = col + ""_"" + val.replace("" "",""_"").lower()\n                else:\n                    val = val.replace("" "",""_"").lower()\n                return val\n            else:\n                return np.nan\n\n    def _create_hour_features(self,v,col):\n        val = (v.hour/24.0) * 2*math.pi\n        v1 = math.sin(val)\n        v2 = math.cos(val)\n        return pd.Series({col+""_hour"":""h""+str(v.hour),col+""_""+\'h1\':v1, col+""_""+\'h2\':v2})\n\n    def _create_month_features(self,v,col):\n        val = (v.month/12.0) * 2*math.pi\n        v1 = math.sin(val)\n        v2 = math.cos(val)\n        return pd.Series({col+""_month"":""m""+str(v.month),col+""_""+\'m1\':v1, col+""_""+\'m2\':v2})\n\n    def _create_dayofweek_features(self,v,col):\n        val = (v.dayofweek/7.0) * 2*math.pi\n        v1 = math.sin(val)\n        v2 = math.cos(val)\n        return pd.Series({col+""_w"":""w""+str(v.dayofweek),col+""_""+\'w1\':v1, col+""_""+\'w2\':v2})\n\n    def _create_year_features(self,v,col):\n        return pd.Series({col+""_year"":""y""+str(v.year)})\n\n\n    def _convert_to_date(self,df,col):\n        if not df[col].dtype == \'datetime64[ns]\':\n            try:\n                return pd.to_datetime(df[col])\n            except:\n                logger.info(""failed default conversion "")\n                pass\n            for f in self.custom_date_formats:\n                try:\n                    return pd.to_datetime(df[col],format=f)\n                except:\n                    logger.info(""failed custom conversion %s"",f)\n                    pass\n            return None\n        else:\n            return df[col]\n\n    def _duplicate_columns(self,frame):\n        groups = frame.columns.to_series().groupby(frame.dtypes).groups\n        dups = []\n        for t, v in groups.items():\n            dcols = frame[v].to_dict(orient=""list"")\n\n            vs = dcols.values()\n            ks = dcols.keys()\n            lvs = len(vs)\n\n            for i in range(lvs):\n                for j in range(i+1,lvs):\n                    if vs[i] == vs[j]: \n                        dups.append(ks[i])\n                        break\n\n        return dups       \n\n\n\n    def fit(self,df):\n        """"""\n        Fit models against an input pandas dataframe\n\n        Parameters\n        ----------\n\n        X : pandas dataframe \n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if not self.nan_threshold is None:\n            max_nan = float(len(df)) * self.nan_threshold\n        numerics = [\'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n        numeric_cols = set(df.select_dtypes(include=numerics).columns)\n        categorical_cols = set(df.select_dtypes(exclude=numerics).columns)\n        if self.drop_duplicate_cols:\n            self.drop_cols = self._duplicate_columns(df)\n            logger.info(""Adding duplicate cols to be dropped %s"",self.drop_cols)\n        for col in df.columns:\n            if col in self.exclude:\n                continue\n            if col in self.drop_cols:\n                continue\n            elif not self.include or col in self.include:\n                if not self.nan_threshold is None:\n                    num_nan = len(df) - df[col].count()\n                    if num_nan > max_nan:\n                        if not col in self.drop_cols:\n                            logger.info(""adding %s to drop columns %d %d"",col,num_nan,max_nan)\n                            self.drop_cols.append(col)\n                            continue\n                if not self.ignore_vals is None:\n                    df[col].replace(self.ignore_vals,np.nan,inplace=True)\n                df[col] = df[col].apply(lambda x: np.nan if isinstance(x, basestring) and len(x)==0 else x)\n                cat_counts = df[col].value_counts(normalize=True,dropna=False)\n                if len(cat_counts) == 1 and self.drop_constant_features:\n                    if not col in self.drop_cols:\n                        logger.info(""adding %s to drop columns as is constant"",col)\n                        self.drop_cols.append(col)\n                        continue\n                is_bool = True\n                for val in cat_counts.index:\n                    if not str(val).lower() in self.bool_map.keys():\n                        is_bool = False\n                        break\n                if is_bool:\n                    self.convert_bool.append(col)\n                elif df[col].dtype in numerics:\n                    if len(cat_counts) > self.max_values_numeric_categorical and not col in self.force_categorical:\n                        logger.info(""fitting scaler for col %s"",col)\n                        dfs = df[col].dropna()\n                        if dfs.shape[0] > 0:\n                            arr = dfs.astype(float).values.reshape(-1,1)\n                            self.scalers[col] = preprocessing.StandardScaler(with_mean=True, with_std=True).fit(arr)\n                            self.min_max[col] = (dfs.min(),dfs.max())\n                    else:\n                        self.convert_categorical.append(col)\n                        self.cat_percent[col] = cat_counts\n                else:\n                    if df[col].dtype == \'datetime64[ns]\':\n                        self.convert_date.append(col)\n                    elif col in self.date_cols:\n                        self.convert_date.append(col)\n                    else:\n                        self.convert_categorical.append(col)\n                        self.cat_percent[col] = cat_counts\n        if self.create_date_differences:\n            dates_converted = pd.DataFrame([])\n            for col in self.convert_date:\n                date_converted = self._convert_to_date(df,col)\n                if not date_converted is None:\n                    dates_converted[col] = date_converted\n            if len(dates_converted.columns)>1:\n                for (col1,col2) in itertools.combinations(dates_converted.columns, 2):\n                    logger.info(""training date diff scaler for %s %s"",col1,col2)\n                    d_diff = dates_converted[col1] - dates_converted[col2]\n                    d_diff = (d_diff / np.timedelta64(1, \'D\')).astype(float)\n                    self.date_diff_scalers[col1+""_""+col2] = preprocessing.StandardScaler(with_mean=True, with_std=True).fit(arr)\n        logger.info(""num columns to drop %d"",len(self.drop_cols))\n        logger.info(""num scalers %d"",len(self.scalers))\n        logger.info(""num categorical %d"",len(self.convert_categorical))\n        logger.info(""num dates %d"",len(self.convert_date))\n        logger.info(""num date diffs %d"",len(self.date_diff_scalers))\n        logger.info(""num bool %d"",len(self.convert_bool))\n        return self\n\n    def transform(self,df):\n        """"""\n        transform a datframe with fitted models\n\n        Parameters\n        ----------\n\n        X : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n\n        """"""\n        df = df.drop(self.drop_cols,axis=1)\n        c = 0\n        num_bools  = len(self.convert_bool)\n        for col in self.convert_bool:\n            c += 1\n            logger.info(""convert bool %s %d/%d"",col,c,num_bools)\n            df[col] = df[col].apply(str).apply(str.lower)\n            df[col] = df[col].map(self.bool_map)\n        c = 0\n        num_dates  = len(self.convert_date)\n        dates_converted = []\n        for col in self.convert_date:\n            c += 1\n            logger.info(""convert date %s %d/%d %s"",col,c,num_dates,df[col].dtype)\n            date_converted = self._convert_to_date(df,col)\n            if not date_converted is None:\n                logger.info(""successfully converted %s to date"",col)\n                df[col] = date_converted\n                dates_converted.append(col)\n            if df[col].dtype == \'datetime64[ns]\':\n                if self.date_transforms[0]:\n                    logger.info(""creating hour features"")\n                    df = pd.concat([df,df[col].apply(self._create_hour_features,col=col)],axis=1)\n                if self.date_transforms[1]:\n                    logger.info(""creating month features"")\n                    df = pd.concat([df,df[col].apply(self._create_month_features,col=col)],axis=1)\n                if self.date_transforms[2]:                    \n                    logger.info(""creating day of week features"")\n                    df = pd.concat([df,df[col].apply(self._create_dayofweek_features,col=col)],axis=1)\n                if self.date_transforms[3]:                    \n                    logger.info(""creating year features"")\n                    df = pd.concat([df,df[col].apply(self._create_year_features,col=col)],axis=1)\n            else:\n                logger.info(""warning - failed to convert to date col %s"",col)\n        if self.create_date_differences and len(dates_converted) > 1:\n            for (col1,col2) in itertools.combinations(dates_converted, 2):\n                logger.info(""diff scaler for %s %s"",col1,col2)\n                col_name = col1+""_""+col2\n                df[col_name] = df[col1] - df[col2]\n                df[col_name] = (df[col_name] / np.timedelta64(1, \'D\')).astype(float)\n                if not self.ignore_vals is None:\n                    df[col_name].replace(self.ignore_vals,np.nan,inplace=True)\n                df[col_name] = df[col_name].apply(self._scale_date_diff,col=col_name)\n        c = 0\n        num_cats = len(self.convert_categorical)\n        for col in self.convert_categorical:\n            if not self.ignore_vals is None:\n                df[col].replace(self.ignore_vals,np.nan,inplace=True)\n            c += 1\n            logger.info(""convert categorical %s %d/%d "",col,c,num_cats)\n            df[col] = df[col].apply(self._make_cat,col=col)\n        num_scalers = len(self.scalers)\n        c = 0\n        for col in self.scalers:\n            if not self.ignore_vals is None:\n                df[col].replace(self.ignore_vals,np.nan,inplace=True)\n            if self.min_max_limit:\n                df[col] = df[col].apply(lambda x : self.min_max[col][0] if x < self.min_max[col][0] else x)\n                df[col] = df[col].apply(lambda x : self.min_max[col][1] if x > self.min_max[col][1] else x)\n            c += 1\n            logger.info(""scaling col %s %d/%d"",col,c,num_scalers)\n            df[col] = df[col].apply(self._scale,col=col)\n        return df\n\nAuto_transform = DeprecationHelper(AutoTransform)'"
python/seldon/pipeline/basic_transforms.py,0,"b'from collections import defaultdict\nfrom  collections import OrderedDict\nimport logging\nimport operator\nimport re\nimport pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator,TransformerMixin\nimport time\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\nclass BinaryTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Create a binary feature based on existence of another feature\n    \n    Parameters\n    ----------\n    input_feature : str\n       input feature to transform\n    output_feature : str\n       output feature to place transformation\n    """"""\n    def __init__(self,input_feature=None,output_feature=None):\n        self.input_feature=input_feature\n        self.output_feature=output_feature\n\n    def fit(self,X):\n        """"""nothing to do in fit\n        """"""\n        return self\n\n    def transform(self,df):\n        logger.info(""Binary transform"")\n        """"""\n        Transform a dataframe creating a new binary feature\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n\n        """"""\n        df[self.output_feature] = df.apply(lambda row: 1 if (not pd.isnull(row[self.input_feature])) and (not row[self.input_feature] == ""0"") and (not row[self.input_feature] == 0) and (not row[self.input_feature] == """") else 0,axis=1)\n        return df\n\nBinary_transform = DeprecationHelper(BinaryTransform)\n\n################\n\nclass IncludeFeaturesTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Filter a dataset and include only specided set of features\n\n\n    Parameters\n    ----------\n    \n    input_features : list str\n       input features to include\n    """"""\n    def __init__(self,included=[]):\n        self.included = included\n\n    def fit(self,X):\n        """"""nothing to do in fit\n        """"""\n        return self\n\n    def transform(self,df):\n        """"""\n        transform a dataframe to include given features\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        df =  df[list(set(self.included).intersection(df.columns))]\n        return df\n\nInclude_features_transform = DeprecationHelper(IncludeFeaturesTransform)\n\n################\n\nclass ExcludeFeaturesTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Filter a dataset and exclude specided set of features\n\n    Parameters\n    ----------\n\n    excluded : list str\n       list of features to be excluded\n    """"""\n    def __init__(self,excluded=[]):\n        self.excluded = excluded\n\n    def fit(self,X):\n        """"""nothing to do in fit\n        """"""\n        return self\n\n    def transform(self,df):\n        """"""\n        Trasform dataframe to include specified features only\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        df = df.drop(self.excluded, axis=1,errors=\'ignore\')\n        return df\n\nExclude_features_transform = DeprecationHelper(ExcludeFeaturesTransform)\n\n#############\n\nclass SplitTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Split a set of string input features on an expression and create a new feature which has a list of values\n\n    Parameters\n    ----------\n\n    split_expression : str\n       regular expression to split feature on\n    ignore_numbers : bool\n       whether to ignore any resulting strings that represent numbers\n    input_features : list str\n       list of feature names to split - should all have text values\n    output_feature : str\n       output feature \n    """"""\n    def __init__(self,split_expression="" "",ignore_numbers=False,input_features=[],output_feature=None):\n        super(SplitTransform, self).__init__()\n        self.split_expression=split_expression\n        self.ignore_numbers=ignore_numbers\n        self.input_features=input_features\n        self.output_feature = output_feature\n\n    def _is_number(self,s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n    def fit(self,X):\n        return self\n\n    def _split(self,row):\n        ftokens = []\n        for col in self.input_features:\n            if isinstance(row[col],basestring):\n                tokens = re.split(self.split_expression,row[col])\n                for token in tokens:\n                    token = token.rstrip().lower()\n                    if not self.ignore_numbers or (self.ignore_numbers and not self._is_number(token)):\n                        ftokens.append(token)\n        return pd.Series({\'val\': ftokens})\n\n    def transform(self,df):\n        """"""\n        Transform text features by splitting them and creating a list of feature as result\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        df[self.output_feature] = df.apply(self._split,axis=1)\n        return df\n\nSplit_transform = DeprecationHelper(SplitTransform)\n\n#############\n\nclass ExistFeaturesTransform(BaseEstimator,TransformerMixin):\n    """"""Filter rows based on whether a specified set of features exists\n\n    Parameters\n    ----------\n    included : list str\n       list of features that need to exist\n    """"""\n    def __init__(self,included=None):\n        super(ExistFeaturesTransform, self).__init__()\n        self.included = included\n\n    def fit(self,objs):\n        return self\n\n    def transform(self,df):\n        """"""\n        Transform by returning input feature set if required features exist in it\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        df.dropna(subset=self.included,inplace=True)\n        return df\n\nExist_features_transform = DeprecationHelper(ExistFeaturesTransform)\n\n#############\n\nclass SvmlightTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Take a set of features and transform into a sorted dictionary of numeric id:value features\n\n    Parameters\n    ----------\n\n    included : list str\n       set of feature to use as input\n    zero_based : zero_based, optional\n       whether to start first id at 0\n    excluded : list str\n       set of features to exclude\n    """"""\n    def __init__(self,included=None,zero_based=False,excluded=[],id_map={},output_feature=None,id_map_file=None):\n        super(SvmlightTransform, self).__init__()\n        self.included = included\n        self.excluded = excluded\n        self.id_map = id_map\n        self.zero_based = zero_based\n        self.output_feature=output_feature\n        self.id_map_file = id_map_file\n\n    @staticmethod\n    def _is_number(s):\n        try:\n            float(s)\n            return True\n        except ValueError:\n            return False\n\n    def _map(self,v,col):\n        if isinstance(v,list):\n            return set([col+""_""+lval for lval in v])\n        elif isinstance(v,dict):\n            return set([col+""_""+k if self._is_number(v) else col+""_""+k+""_""+str(v) for k,v in v.items()])\n        else:\n            if self._is_number(v):\n                return set(col)\n            elif isinstance(v, basestring):\n                return set([col+""_""+v])\n            else:\n                return set([col+""_""+str(v)])\n\n    def _set_id(self,row):\n        lvals = []\n        for col in row.index.values:\n            if (not self.included or col in self.included) and (not col in self.excluded):\n                v = row[col]\n                if isinstance(v,list):\n                    lvals += [(self.id_map[col+""_""+lval],1) for lval in v]\n                elif isinstance(v,dict):\n                    lvals += [(self.id_map[col+""_""+k],v) if self._is_number(v) else (self.id_map[col+""_""+k+""_""+str(v)],1) for k,v in v.items()]\n                else:\n                    if self._is_number(v):\n                        if not pd.isnull(v):\n                            lvals += [(self.id_map[col],v)]\n                    else:\n                        var_name = col+""_""+v\n                        if var_name in self.id_map:\n                            lvals += [(self.id_map[var_name],1)]\n        self.progress += 1\n        if self.progress % 100 == 0:\n            logger.info(""processed %d/%d"",self.progress,self.size)\n        return pd.Series([sorted(lvals)])\n\n\n    def _union(self,vals):\n        s = set()\n        for v in vals:\n            s = s.union(v)\n        return s\n\n    def _save_id_map(self):\n        import unicodecsv\n        writer = unicodecsv.writer(open(self.id_map_file, \'wb\'))\n        for key, value in self.id_map.items():\n            writer.writerow([value, key])\n\n\n\n    def fit(self,df):\n        """"""\n        create ids for each feature to be included\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        self: object\n        """"""\n        numerics = [\'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n        features = set()\n        df_numeric = df.select_dtypes(include=numerics)\n        df_categorical = df.select_dtypes(exclude=numerics)\n        for col in df_categorical.columns:\n            if (not self.included or col in self.included) and (not col in self.excluded):\n                logger.info(""SVM transform - Fitting categorical feature %s"" % col)\n                res = df[col].apply(self._map,col=col)\n                s = res.groupby(lambda x : ""all"").aggregate(self._union)\n                features = features.union(s[""all""])\n        for col in df_numeric.columns:\n            if (not self.included or col in self.included) and (not col in self.excluded):\n                logger.info(""SVM transform - Fitting numerical feature %s"" % col)\n                features.add(col)\n        inc = 1\n        if self.zero_based:\n            inc = 0\n        self.id_map = dict([(v,i+inc) for i,v in enumerate(features)])\n        if not self.id_map_file is None:\n            self._save_id_map()\n        return self\n\n    def transform(self,df):\n        """"""\n        Transform features by getting id and numeric value\n\n        Parameters\n        ----------\n\n        X : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        self.progress = 0\n        self.size = df.shape[0]\n        df[self.output_feature] = df.apply(self._set_id,axis=1,reduce=True)\n        return df\n\nSvmlight_transform = DeprecationHelper(SvmlightTransform)\n\n#############\n\n\nclass FeatureIdTransform(BaseEstimator,TransformerMixin):\n    """"""create a numeric feature id\n\n    Parameters\n    ----------\n    \n    input_feature : str\n       input feature to create ids from\n    output_feature : str\n       output feature to place ids\n    min_size : int, optional\n       minimum number of examples of each feature value for feature to be included in transform as new id\n    exclude_missing : bool, optional\n       exclude rows that do not have the input feature\n    """"""\n    def __init__(self,input_feature=None,output_feature=None,min_size=0,max_classes=1000,exclude_missing=False,zero_based=False,id_map={}):\n        self.input_feature=input_feature\n        self.output_feature=output_feature\n        self.min_size = min_size\n        self.exclude_missing = exclude_missing\n        self.id_map = id_map\n        self.zero_based = zero_based\n        self.max_classes=max_classes\n\n    def fit(self,df):\n        """"""\n        Create map of ids for each feature value\n\n        create counts of occurrences of each feature value. Exclude features with not enough counds. Create id map.\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        self: object\n\n        """"""\n        if self.input_feature in df:\n            counts = df[self.input_feature].value_counts()\n            sorted_counts = sorted(counts.iteritems(), key=operator.itemgetter(1),reverse=True)\n            self.id_map = {}\n            if self.zero_based:\n                idx = 0\n            else:\n                idx = 1\n            for c,v in sorted_counts:\n                if v >= self.min_size and len(self.id_map)<self.max_classes:\n                    self.id_map[c] = idx\n                    idx += 1\n                else:\n                    break\n        return self\n\n    def _map(self,v):\n        if v in self.id_map:\n            return self.id_map[v]\n        else:\n            return np.nan\n\n    def transform(self,df):\n        """"""\n        Transform features creating a new id and exluding rows if needed\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n\n        """"""\n        if self.input_feature in df:\n            df[self.output_feature] = df[self.input_feature].apply(self._map)\n            if self.exclude_missing:\n                df = df[pd.notnull(df[self.output_feature])]\n                df[self.output_feature] = df[self.output_feature].astype(int)\n            else:\n                df[self.output_feature] = df[self.output_feature].fillna(-1).astype(int)\n        return df\n\nFeature_id_transform = DeprecationHelper(FeatureIdTransform)\n\n\n'"
python/seldon/pipeline/bayes_optimize.py,0,"b'import pandas as pd\nimport numpy as np\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.base import BaseEstimator\nimport seldon.pipeline.cross_validation as cf\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass BayesOptimizer(BaseEstimator):\n\n    """"""\n    sklearn wrapper for BayesianOptimization module\n\n    Parameters\n    ----------\n    \n    clf : object\n       sklearn compatible estimator\n    param_ranges : dict\n       dict of parameters to optimize with ranges in form \'name\':(min,max)\n    param_int : list\n       list of parameters that need conversion to int before calling estimator\n    cv_folds : int\n       number of cross validation folds to run\n\n    """"""\n    def __init__(self,clf=None,param_ranges={},param_int=[],cv_folds=5):\n        self.clf = clf\n        self.param_ranges=param_ranges\n        self.param_int = param_int\n        self.best_score = 0.0\n        self.cv_folds=5\n        self.X = None\n        self.y = None\n\n    def __getstate__(self):\n        """"""\n        Remove things that should not be pickled\n        """"""\n        result = self.__dict__.copy()\n        del result[\'X\']\n        del result[\'y\']\n        return result\n\n\n    def get_best_score(self):\n        return self.best_score\n\n    def score(self,**params):\n        for v in self.param_int:\n            params[v] = int(params[v])\n        self.clf.set_params(**params)\n        cv = cf.SeldonKFold(self.clf,self.cv_folds)\n        cv.fit(self.X,self.y)\n        return cv.get_score()\n\n    def fit(self,X,y=None):\n        """"""Fit a model: \n\n        Parameters\n        ----------\n\n        X : pandas dataframe or array-like\n           training samples. If pandas dataframe can handle dict of feature in one column or convert a set of columns\n        y : array like, required for array-like X and not used presently for pandas dataframe\n           class labels\n\n        Returns\n        -------\n        self: object\n        """"""\n        self.X = X\n        self.y = y\n        bopt = BayesianOptimization(self.score,self.param_ranges)\n        bopt.maximize()\n        logger.info(bopt.res)\n        self.best_score = bopt.res[\'max\'][\'max_val\']\n        params = bopt.res[\'max\'][\'max_params\']\n        for v in self.param_int:\n            params[v] = int(params[v])\n        self.clf.set_params(**params)\n        self.clf.fit(X,y)\n        return self\n        \n    def transform(self,X):\n        """"""\n        Do nothing and pass input back\n        """"""\n        return X\n\n    def predict_proba(self, X):\n        return self.clf.predict_proba(X)\n\n    def get_class_id_map(self):\n        return self.clf.get_class_id_map()\n'"
python/seldon/pipeline/cross_validation.py,0,"b'import pandas as pd\nfrom sklearn.cross_validation import KFold\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator\nimport logging\nimport numpy as np\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\nclass SeldonKFold(BaseEstimator):\n    """"""\n    Simple wrapper to provide cross validation test using estimator with input from pandas dataframe\n\n    Parameters\n    ----------\n\n    clf : object\n       Pandas compatible scikit learn Estimator to apply to data splits\n    k : int, optional\n       number of folds\n    save_folder_folder : str, optional\n       a folder to save prediction results from each fold\n    """"""\n    def __init__(self,clf=None,k=5,save_folds_folder=None,metric=\'accuracy\',random_state=1):\n        self.clf = clf\n        self.k = k\n        self.scores = []\n        self.save_folds_folder=save_folds_folder\n        self.metric = metric\n        self.random_state = random_state\n\n    def get_scores(self):\n        return self.scores\n\n    def get_score(self):\n        if len(self.scores) > 0:\n            return sum(self.scores) / float(len(self.scores))\n        else:\n            return 0.0\n\n\n    def fit(self,X,y=None):\n        """"""\n        Split dataframe into k folds and train test classifier on each. Finally train classifier on all data.\n        Parameters\n        ----------\n\n        X : pandas dataframe \n\n        Returns\n        -------\n        self: object\n        """"""\n        df_len = X.shape[0]\n        kf = KFold(df_len, n_folds=self.k,shuffle=True,random_state=self.random_state)\n        self.scores = []\n        idx = 1\n        for train_index, test_index in kf:\n            if isinstance(X,pd.DataFrame):\n                X_train = X.iloc[train_index]\n                y_train = None\n                X_test = X.iloc[test_index]\n                y_test = X_test[self.clf.get_target()]\n            else:\n                X_train, X_test = X[train_index], X[test_index]\n                y_train, y_test = y[train_index], y[test_index]\n            self.clf.fit(X_train,y_train)\n            y_pred = self.clf.predict(X_test)\n            y_pred_proba = self.clf.predict_proba(X_test)\n            if self.metric == \'accuracy\':\n                self.scores.append(metrics.accuracy_score(y_test, y_pred))\n            elif self.metric == \'auc\':\n                fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba[:, 1])\n                self.scores.append(metrics.auc(fpr, tpr))\n            logger.info(""Running scores %s"",self.scores)\n            if not self.save_folds_folder is None:\n                np.savetxt(self.save_folds_folder+""/""+str(idx)+""_correct.txt"",y_test,fmt=\'%1.3f\')\n                np.savetxt(self.save_folds_folder+""/""+str(idx)+""_predictions.txt"",y_pred,fmt=\'%1.3f\')\n                np.savetxt(self.save_folds_folder+""/""+str(idx)+""_predictions_proba.txt"",y_pred_proba,fmt=\'%1.3f\')\n            idx += 1\n        logger.info(""accuracy scores %s"",self.scores)\n        self.clf.fit(X,y)\n        return self\n\n    def transform(self,X):\n        """"""\n        Do nothing and pass input back\n        """"""\n        return X\n\n    def predict_proba(self, X):\n        return self.clf.predict_proba(X)\n\n    def predict(self,X):\n        return self.clf.predict(X)\n\n    def get_class_id_map(self):\n        return self.clf.get_class_id_map()\n\n    def set_params(self,**params):\n        self.clf.set_params(params)\n\n\nSeldon_KFold = DeprecationHelper(SeldonKFold)'"
python/seldon/pipeline/pandas_pipelines.py,0,"b'import seldon.fileutil as fu\nimport json\nfrom sklearn.externals import joblib\nfrom sklearn.feature_extraction import DictVectorizer\nimport os.path\nimport logging\nimport shutil \nimport unicodecsv\nimport numpy as np\nimport pandas as pd\nimport random\nimport string\n\nclass BasePandasEstimator(object):\n    """"""\n    Tools to help with Pandas based estimators.\n\n    Parameters\n    ----------\n    \n    target : str\n       Target column\n    target_readable : str\n       More descriptive version of target variable\n    included : list str, optional\n       columns to include\n    excluded : list str, optional\n       columns to exclude\n    id_map : dict (int,str), optional\n       map of class ids to high level names\n    """"""\n    def __init__(self, target=None, target_readable=None,included=None,excluded=None,id_map={}):\n        self.target = target\n        self.target_readable = target_readable\n        self.id_map=id_map\n        self.included = included\n        self.excluded = excluded\n        if self.excluded is None:\n            self.excluded = []\n        if not self.target_readable is None:\n            self.excluded.append(self.target_readable)\n\n    def get_target(self):\n        return self.target\n\n    def set_class_id_map(self,id_map):\n        self.id_map = id_map\n\n    def get_class_id_map(self):\n        return self.id_map\n\n    def create_class_id_map(self,df,target,target_readable,zero_based=True):\n        """"""\n        Create a map of classification ids to readable values \n        """"""\n        ids = df.drop_duplicates([target,target_readable]).to_dict(orient=\'records\')\n        m = {}\n        for d in ids:\n            if zero_based:\n                m[d[target]] = d[target_readable]\n            else:\n                m[d[target]-1] = d[target_readable]\n        self.set_class_id_map(m)\n\n    def encode_onehot(self,df, cols, vec, op):\n        """"""\n        One hot encode categorical values from a data frame using a vectorizer passed in\n        """"""\n        if op == ""fit"":\n            vec_data = pd.DataFrame(vec.fit_transform(df[cols].to_dict(outtype=\'records\')).toarray())\n        else:\n            vec_data = pd.DataFrame(vec.transform(df[cols].to_dict(outtype=\'records\')).toarray())\n        vec_data.columns = vec.get_feature_names()\n        vec_data.index = df.index\n        \n        df = df.drop(cols, axis=1)\n        df = df.join(vec_data)\n        return df\n\n    def convert_dataframe(self,df_base,vectorizer):\n        """"""\n        Convert a dataframe into one for use with ml algorithms\n        One hot encode the categorical variable\n        Ignore date values\n        concatenate with numeric values\n        """"""\n        if vectorizer is None:\n            vectorizer = DictVectorizer()\n            op = ""fit""\n        else:\n            op = ""transform""\n        numerics = [\'int16\', \'int32\', \'int64\', \'float16\', \'float32\', \'float64\']\n        # will ignore all date columns\n        df_numeric = df_base.select_dtypes(include=numerics)\n        df_categorical = df_base.select_dtypes(exclude=numerics+[\'datetime64[ns]\'])\n        cat_cols = []\n        if len(df_categorical.columns) > 0:\n            df_categorical = self.encode_onehot(df_categorical, cols=df_categorical.columns,vec=vectorizer,op=op)\n            df_X = pd.concat([df_numeric, df_categorical], axis=1)\n        else:\n            df_X = df_numeric\n        return (df_X,vectorizer)\n\n    def _exclude_include_features(self,df):\n        """"""\n        Utility function to include and exclude features from a data frame to create a new one\n        """"""\n        if not self.included is None:\n            df = df[list(set(self.included+[self.target]).intersection(df.columns))]\n        if not self.excluded is None:\n            df = df.drop(set(self.excluded).intersection(df.columns), axis=1)\n        return df\n\n    def convert_numpy(self,df):\n        """"""\n        Convert a dataframe into a numpy data matrix for training and an array of target values\n        Uses a vectorizer for one hot encoding which is returned\n        NaNs are filled with zeros\n\n        Parameters\n        ----------\n\n        df : pandas dataframe\n\n        Returns\n        -------\n\n        X : array like - data as array\n        y : array - target labels\n        vectorizer : vectorizer used for one hot encoding\n        """"""\n        if self.target in df:\n            if not self.target_readable is None:\n                self.create_class_id_map(df,self.target,self.target_readable)\n            df_y = df[self.target]\n            df_base = df.drop([self.target], axis=1)\n        else:\n            df_y = None\n            df_base = df\n        df_base = self._exclude_include_features(df_base)\n        df_base = df_base.fillna(0)\n\n        (df_X,self.vectorizer) = self.convert_dataframe(df_base,self.vectorizer)\n        return (df_X.as_matrix(),df_y,self.vectorizer)\n\n\n    def close(self):\n        pass\n\n\n    def predict(self,X):\n        proba = self.predict_proba(X)\n        return np.argmax(proba, axis=1)\n'"
python/seldon/pipeline/sklearn_transform.py,0,"b'from collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom seldon.util import DeprecationHelper\n\nclass SklearnTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Allow sklearn transformers to be run on Pandas dataframes.\n\n    Parameters\n    ----------\n\n    input_features : list str\n       input columns to use\n    output_features : list str, optional\n       names of output columns\n    transformer : scikit learn Transformer\n       transformer to run on data\n    """"""\n    def __init__(self,input_features=None,output_features=None,output_features_prefix=None,transformer=None):\n        self.input_features=input_features\n        self.output_features=output_features\n        self.transformer=transformer\n        self.output_features_prefix=output_features_prefix\n\n\n    def fit(self,df):\n        """"""\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        self: object\n        """"""\n        self.transformer.fit(df[self.input_features].values)\n        return self\n        \n    def transform(self,df):\n        """"""\n        transform the input columns and merge result into input dataframe using column names if provided\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        Y = self.transformer.transform(df[self.input_features].values)\n        df_Y = pd.DataFrame(Y)\n        if not self.output_features_prefix is None:\n            cols = [self.output_features_prefix+""_""+str(c) for c in df_Y.columns]\n            df_Y.columns = cols\n        elif not self.output_features is None and len(df_Y.columns) == len(self.output_features):\n            df_Y.columns = self.output_features\n        df_2 = pd.concat([df,df_Y],axis=1)\n        return df_2\n\nsklearn_transform = DeprecationHelper(SklearnTransform)'"
python/seldon/pipeline/tfidf_transform.py,0,"b'from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.base import BaseEstimator,TransformerMixin\nimport logging \nimport time\nimport logging\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\nclass TfidfTransform(BaseEstimator,TransformerMixin):\n    """"""\n    Create TF-IDF (term frequency - inverse document frequency) features. \n\n    can use chi-squared test to limit features. Assumes string based input feature that can be split.\n    Uses scikit-learn based transformers internally\n\n    Parameters\n    ----------\n\n    min_df : int, optinal\n       min document frequency (for sklearn vectorizer)\n    max_df : float, optional\n       max document frequency (for sklearn vectorizer)\n    select_features : bool, optional\n       use chi-squared test to select features\n    topn_features : int, optional\n       keep top features from chi-squared test\n    stop_words : str, optional\n       stop words (for sklearn vectorizer)\n    target_feature : str, optional\n       target feature for chi-squared test\n    """"""\n    def __init__(self,input_feature=None,output_feature=None,min_df=0,max_df=1.0,select_features=False,topn_features=50000,stop_words=None,target_feature=None,vectorizer=None,TfidfTransformer=None,ch2=None,fnames=None,feature_names_support=[],ngram_range=[1,1]):\n        self.input_feature=input_feature\n        self.output_feature=output_feature\n        self.min_df=min_df\n        self.max_df=max_df\n        self.select_features = select_features\n        self.topn_features=topn_features\n        self.stop_words = stop_words\n        self.target_feature = target_feature\n        self.vectorizer = vectorizer\n        self.TfidfTransformer = TfidfTransformer\n        self.ch2 = ch2\n        self.fnames = fnames\n        self.feature_names_support = feature_names_support\n        self.ngram_range = ngram_range\n\n\n    def get_tokens(self,v):\n        """"""basic method to get ""document"" string from feature\n        """"""\n        if isinstance(v, list):\n            return "" "".join([i if isinstance(i, basestring) else str(i) for i in v])\n        elif isinstance(v,basestring):\n            return v\n        else:\n            return str(v)\n\n    \n    def fit(self,df):\n        """"""\n        Fit tfidf transform\n\n        Parameters\n        ----------\n\n        df : pandas dataframe \n\n        Returns\n        -------\n        self: object\n        """"""\n        self.vectorizer = CountVectorizer(min_df=self.min_df,max_df=self.max_df,stop_words=self.stop_words,ngram_range=self.ngram_range)\n        self.TfidfTransformer = TfidfTransformer()\n        logger.info(""getting docs"")\n        docs = df[self.input_feature].apply(self.get_tokens)\n        logger.info(""running vectorizer"")\n        counts = self.vectorizer.fit_transform(docs.as_matrix())\n        logger.info(""run tfidf transform"")\n        self.tfidf = self.TfidfTransformer.fit_transform(counts)\n        self.fnames = self.vectorizer.get_feature_names()\n        logger.info(""base tfidf features %d"",len(self.fnames))\n        if self.select_features:\n            self.ch2 = SelectKBest(chi2, k=self.topn_features)\n            self.ch2.fit_transform(self.tfidf, df[self.target_feature])\n            self.feature_names_support = set([self.fnames[i] for i in self.ch2.get_support(indices=True)])\n            logger.info(""selected tfidf features %d"",len(self.feature_names_support))\n        return self\n\n    def _create_tfidf(self,v):\n        s = [self.get_tokens(v)]\n        counts = self.vectorizer.transform(s)\n        self.tfidf = self.TfidfTransformer.transform(counts)\n        doc_tfidf = {}\n        for (col,val) in zip(self.tfidf[0].indices,self.tfidf[0].data):\n            fname = self.fnames[col]\n            if self.select_features:\n                if fname in self.feature_names_support:\n                    doc_tfidf[fname] = val\n            else:\n                doc_tfidf[fname] = val\n        self.progress += 1\n        if self.progress % 100 == 0:\n            logger.info(""processed %d/%d"",self.progress,self.size)\n        return doc_tfidf\n        \n\n    def transform(self,df):\n        """"""\n        transform features with tfidf transform\n\n        Parameters\n        ----------\n\n        X : pandas dataframe \n\n        Returns\n        -------\n        \n        Transformed pandas dataframe\n        """"""\n        self.progress = 0\n        self.size = df.shape[0]\n        df[self.output_feature] = df[self.input_feature].apply(self._create_tfidf)\n        return df\n\nTfidf_transform = DeprecationHelper(TfidfTransform)\n\n'"
python/seldon/pipeline/util.py,0,"b'import seldon.fileutil as fu\nimport json\nimport os.path\nimport logging\nimport shutil \nimport unicodecsv\nimport numpy as np\nimport pandas as pd\nimport random\nimport string\nfrom sklearn.externals import joblib\nimport logging\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\nclass PipelineWrapper(object):\n    """"""\n    Wrapper to allow dataframes to be created and saved from external data sources.\n    Data sources:AWS s3 and file system\n    Formats: JSON and CSV\n\n    Parameters\n    ----------\n\n    work_folder : str\n       load work folder to stage files\n    aws_key : str, optional\n       AWS key\n    aws_secret : str, optional\n       AWS secret\n    """"""\n    def __init__(self,work_folder=""/tmp"",aws_key=None,aws_secret=None):\n        self.work_folder=work_folder\n        self.lines_read = 0\n        self.df_format=\'json\'\n        self.active_file=None\n        self.aws_key=aws_key\n        self.aws_secret=aws_secret\n\n    def get_work_folder(self):\n        return self.work_folder\n\n    def create_work_folder(self):\n        if not os.path.exists(self.work_folder):\n            logger.info(""creating %s"",self.work_folder)\n            os.makedirs(self.work_folder)\n\n\n    #\n    # save dataframe to location\n    #\n\n    def save_dataframe(self,df,location,df_format=""json"",csv_index=True):\n        """"""Save dataframe\n\n        Parameters\n        ----------\n\n        df : pandas daraframe\n           dataframe to save\n        location : str\n           external filesystem location to save to\n        df_format : str\n           format to use : json or csv\n        csv_index : bool\n           whether to save index when outputing to csv\n        """"""\n        self.create_work_folder()\n        tmp_file = self.work_folder+""/df_tmp""\n        if df_format == \'csv\':\n            logger.info(""saving dataframe as csv"")\n            df.to_csv(tmp_file,index=csv_index)\n        else:\n            logger.info(""saving dataframe as json"")\n            f = open(tmp_file,""w"")\n            for i in range(0, df.shape[0]):\n                row = df.irow(i).dropna()\n                jNew = row.to_dict()\n                jStr =  json.dumps(jNew,sort_keys=True)\n                f.write(jStr+""\\n"")\n            f.close()\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(tmp_file,location)\n\n    #\n    # download data and convert to dataframe\n    #\n    \n\n    def _save_features_local(self,line):\n        """"""save data line to local features file\n        """"""\n        if not self.df_format == \'csv\' and self.lines_read > 0:\n            self.active_file.write("","")\n        self.active_file.write(line+""\\n"")\n        self.lines_read += 1\n\n\n    def _copy_features_locally(self,locations,local_file,df_format):\n        self.df_format=df_format\n        self.create_work_folder()\n        logger.info(""streaming features %s to %s"",locations,local_file)\n        logger.info(""input type is %s"",self.df_format)\n        self.lines_read = 0\n        self.active_file = open(local_file,""w"")\n        if not self.df_format == \'csv\':\n            self.active_file.write(""["")\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.stream_multi(locations,self._save_features_local)\n        if not self.df_format == \'csv\':\n            self.active_file.write(""]"")\n        self.active_file.close()\n        logger.info(""finished stream of features"")\n\n    def _convert_dataframe(self,local_file,df_format,csv_dates=None,index_col=None):\n        """"""load csv or json into pandas dataframe\n        """"""\n        logger.info(""loading data into pandas dataframe"")\n        if df_format == \'csv\':\n            logger.info(""loading csv %s index:%s"",csv_dates,index_col)\n            return pd.read_csv(local_file,parse_dates=csv_dates,index_col=index_col)\n        else:\n            logger.info(""loading json"")\n            return pd.read_json(local_file,orient=\'records\')\n\n\n    def create_dataframe_from_files(self,locations,df_format=""json"",csv_dates=None,index_col=None):\n        local_file= self.work_folder+""/data""\n        self._copy_features_locally(locations,local_file,df_format)\n        return self._convert_dataframe(local_file,df_format,csv_dates,index_col)\n\n    def create_dataframe(self,data=None,df_format=""json"",csv_dates=None,index_col=None):\n        """"""\n        Create Pandas dataframe from external source\n\n        Parameters\n        ----------\n\n        data : object, list, dict or str\n           object : pandas dataframe - will be returned as is\n           list : list of folders to load data frame\n           str : filename to load data frome\n           dict : data in dict\n        """"""\n        if data is not None:\n            if isinstance(data, pd.DataFrame):\n                return data\n            elif isinstance(data,dict):\n                return pd.DataFrame([data])\n            elif isinstance(data,basestring):\n                local_file= self.work_folder+""/data""\n                futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n                futil.copy(data,local_file)\n                return self._convert_dataframe(local_file,df_format,csv_dates,index_col)\n            elif isinstance(data,list):\n                return np.array(data).reshape(1, -1)\n            else:\n                raise ValueError(""unknown argument type for data"")\n        \n\n\n    #\n    # Upload pipeline\n    #\n\n    def save_pipeline(self,pipeline,location):\n        """"""\n        Save scikit learn pipeline to external location\n\n        Parameters\n        ----------\n\n        pipelines : sklearn pipeline\n           pipeline to be saved\n        location : str\n           external folder to save pipeline\n        """"""\n        self.create_work_folder()\n        pipeline_folder = self.work_folder+""/pipeline""\n        if not os.path.exists(pipeline_folder):\n            logger.info(""creating folder %s"",pipeline_folder)\n            os.makedirs(pipeline_folder)\n        tmp_file = pipeline_folder+""/p""\n        joblib.dump(pipeline,tmp_file)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(pipeline_folder,location)\n\n\n    def load_pipeline(self,pipeline_folder):\n        """"""\n        Load scikit learn pipeline from external folder\n        \n        Parameters\n        ----------\n\n        pipeline_folder : str\n           external folder holding pipeline\n        """"""\n        self.create_work_folder()\n        local_pipeline_folder = self.work_folder+""/pipeline""\n        if not os.path.exists(local_pipeline_folder):\n            logger.info(""creating folder %s"",local_pipeline_folder)\n            os.makedirs(local_pipeline_folder)\n        futil = fu.FileUtil(aws_key=self.aws_key,aws_secret=self.aws_secret)\n        futil.copy(pipeline_folder,local_pipeline_folder)\n        return joblib.load(local_pipeline_folder+""/p"")\n\nPipeline_wrapper = DeprecationHelper(PipelineWrapper)\n\n\n            \n'"
python/seldon/rpc/__init__.py,0,b''
python/seldon/rpc/seldon_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: seldon.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'seldon.proto\',\n  package=\'io.seldon.api.rpc\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x0cseldon.proto\\x12\\x11io.seldon.api.rpc\\x1a\\x19google/protobuf/any.proto\\""w\\n\\x15\\x43lassificationRequest\\x12:\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32,.io.seldon.api.rpc.ClassificationRequestMeta\\x12\\""\\n\\x04\\x64\\x61ta\\x18\\x02 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\"")\\n\\x19\\x43lassificationRequestMeta\\x12\\x0c\\n\\x04puid\\x18\\x01 \\x01(\\t\\""\\xb3\\x01\\n\\x13\\x43lassificationReply\\x12\\x38\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32*.io.seldon.api.rpc.ClassificationReplyMeta\\x12<\\n\\x0bpredictions\\x18\\x02 \\x03(\\x0b\\x32\\\'.io.seldon.api.rpc.ClassificationResult\\x12$\\n\\x06\\x63ustom\\x18\\x03 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\""M\\n\\x17\\x43lassificationReplyMeta\\x12\\x0c\\n\\x04puid\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tmodelName\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tvariation\\x18\\x03 \\x01(\\t\\""V\\n\\x14\\x43lassificationResult\\x12\\x12\\n\\nprediction\\x18\\x01 \\x01(\\x01\\x12\\x16\\n\\x0epredictedClass\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nconfidence\\x18\\x03 \\x01(\\x01\\""-\\n\\x1b\\x44\\x65\\x66\\x61ultCustomPredictRequest\\x12\\x0e\\n\\x06values\\x18\\x01 \\x03(\\x02\\x32h\\n\\x06Seldon\\x12^\\n\\x08\\x43lassify\\x12(.io.seldon.api.rpc.ClassificationRequest\\x1a&.io.seldon.api.rpc.ClassificationReply\\""\\x00\\x42$\\n\\x11io.seldon.api.rpcB\\rPredictionAPIP\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_CLASSIFICATIONREQUEST = _descriptor.Descriptor(\n  name=\'ClassificationRequest\',\n  full_name=\'io.seldon.api.rpc.ClassificationRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'io.seldon.api.rpc.ClassificationRequest.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'io.seldon.api.rpc.ClassificationRequest.data\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=62,\n  serialized_end=181,\n)\n\n\n_CLASSIFICATIONREQUESTMETA = _descriptor.Descriptor(\n  name=\'ClassificationRequestMeta\',\n  full_name=\'io.seldon.api.rpc.ClassificationRequestMeta\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'puid\', full_name=\'io.seldon.api.rpc.ClassificationRequestMeta.puid\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=183,\n  serialized_end=224,\n)\n\n\n_CLASSIFICATIONREPLY = _descriptor.Descriptor(\n  name=\'ClassificationReply\',\n  full_name=\'io.seldon.api.rpc.ClassificationReply\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'io.seldon.api.rpc.ClassificationReply.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predictions\', full_name=\'io.seldon.api.rpc.ClassificationReply.predictions\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom\', full_name=\'io.seldon.api.rpc.ClassificationReply.custom\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=227,\n  serialized_end=406,\n)\n\n\n_CLASSIFICATIONREPLYMETA = _descriptor.Descriptor(\n  name=\'ClassificationReplyMeta\',\n  full_name=\'io.seldon.api.rpc.ClassificationReplyMeta\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'puid\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.puid\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'modelName\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.modelName\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variation\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.variation\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=408,\n  serialized_end=485,\n)\n\n\n_CLASSIFICATIONRESULT = _descriptor.Descriptor(\n  name=\'ClassificationResult\',\n  full_name=\'io.seldon.api.rpc.ClassificationResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'prediction\', full_name=\'io.seldon.api.rpc.ClassificationResult.prediction\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predictedClass\', full_name=\'io.seldon.api.rpc.ClassificationResult.predictedClass\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'confidence\', full_name=\'io.seldon.api.rpc.ClassificationResult.confidence\', index=2,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=487,\n  serialized_end=573,\n)\n\n\n_DEFAULTCUSTOMPREDICTREQUEST = _descriptor.Descriptor(\n  name=\'DefaultCustomPredictRequest\',\n  full_name=\'io.seldon.api.rpc.DefaultCustomPredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'values\', full_name=\'io.seldon.api.rpc.DefaultCustomPredictRequest.values\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=575,\n  serialized_end=620,\n)\n\n_CLASSIFICATIONREQUEST.fields_by_name[\'meta\'].message_type = _CLASSIFICATIONREQUESTMETA\n_CLASSIFICATIONREQUEST.fields_by_name[\'data\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_CLASSIFICATIONREPLY.fields_by_name[\'meta\'].message_type = _CLASSIFICATIONREPLYMETA\n_CLASSIFICATIONREPLY.fields_by_name[\'predictions\'].message_type = _CLASSIFICATIONRESULT\n_CLASSIFICATIONREPLY.fields_by_name[\'custom\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\nDESCRIPTOR.message_types_by_name[\'ClassificationRequest\'] = _CLASSIFICATIONREQUEST\nDESCRIPTOR.message_types_by_name[\'ClassificationRequestMeta\'] = _CLASSIFICATIONREQUESTMETA\nDESCRIPTOR.message_types_by_name[\'ClassificationReply\'] = _CLASSIFICATIONREPLY\nDESCRIPTOR.message_types_by_name[\'ClassificationReplyMeta\'] = _CLASSIFICATIONREPLYMETA\nDESCRIPTOR.message_types_by_name[\'ClassificationResult\'] = _CLASSIFICATIONRESULT\nDESCRIPTOR.message_types_by_name[\'DefaultCustomPredictRequest\'] = _DEFAULTCUSTOMPREDICTREQUEST\n\nClassificationRequest = _reflection.GeneratedProtocolMessageType(\'ClassificationRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUEST,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationRequest)\n  ))\n_sym_db.RegisterMessage(ClassificationRequest)\n\nClassificationRequestMeta = _reflection.GeneratedProtocolMessageType(\'ClassificationRequestMeta\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUESTMETA,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationRequestMeta)\n  ))\n_sym_db.RegisterMessage(ClassificationRequestMeta)\n\nClassificationReply = _reflection.GeneratedProtocolMessageType(\'ClassificationReply\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREPLY,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationReply)\n  ))\n_sym_db.RegisterMessage(ClassificationReply)\n\nClassificationReplyMeta = _reflection.GeneratedProtocolMessageType(\'ClassificationReplyMeta\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREPLYMETA,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationReplyMeta)\n  ))\n_sym_db.RegisterMessage(ClassificationReplyMeta)\n\nClassificationResult = _reflection.GeneratedProtocolMessageType(\'ClassificationResult\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESULT,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationResult)\n  ))\n_sym_db.RegisterMessage(ClassificationResult)\n\nDefaultCustomPredictRequest = _reflection.GeneratedProtocolMessageType(\'DefaultCustomPredictRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _DEFAULTCUSTOMPREDICTREQUEST,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.DefaultCustomPredictRequest)\n  ))\n_sym_db.RegisterMessage(DefaultCustomPredictRequest)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\021io.seldon.api.rpcB\\rPredictionAPIP\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n\n\nclass SeldonStub(object):\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Classify = channel.unary_unary(\n        \'/io.seldon.api.rpc.Seldon/Classify\',\n        request_serializer=ClassificationRequest.SerializeToString,\n        response_deserializer=ClassificationReply.FromString,\n        )\n\n\nclass SeldonServicer(object):\n\n  def Classify(self, request, context):\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_SeldonServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Classify\': grpc.unary_unary_rpc_method_handler(\n          servicer.Classify,\n          request_deserializer=ClassificationRequest.FromString,\n          response_serializer=ClassificationReply.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'io.seldon.api.rpc.Seldon\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n\n\nclass BetaSeldonServicer(object):\n  def Classify(self, request, context):\n    context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\nclass BetaSeldonStub(object):\n  def Classify(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n    raise NotImplementedError()\n  Classify.future = None\n\n\ndef beta_create_Seldon_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n  request_deserializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationRequest.FromString,\n  }\n  response_serializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationReply.SerializeToString,\n  }\n  method_implementations = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): face_utilities.unary_unary_inline(servicer.Classify),\n  }\n  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n  return beta_implementations.server(method_implementations, options=server_options)\n\n\ndef beta_create_Seldon_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n  request_serializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationRequest.SerializeToString,\n  }\n  response_deserializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationReply.FromString,\n  }\n  cardinalities = {\n    \'Classify\': cardinality.Cardinality.UNARY_UNARY,\n  }\n  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n  return beta_implementations.dynamic_stub(channel, \'io.seldon.api.rpc.Seldon\', cardinalities, options=stub_options)\n# @@protoc_insertion_point(module_scope)\n'"
python/seldon/shell/__init__.py,0,b'def start_seldonshell():\n    import shell_main\n    shell_main.main()\n\n'
python/seldon/shell/attr_schema_utils.py,0,"b'import json\nimport MySQLdb\nimport getopt, argparse\n\nimport pprint\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\nvalid_value_types = set([\'double\', \'string\', \'date\', \'text\', \'int\',\'boolean\'])\nvalue_types_to_db_map = dict(double=\'DOUBLE\', string=\'VARCHAR\', date=\'DATETIME\', int=\'INT\', boolean=\'BOOLEAN\',\n\ttext=\'TEXT\', enum=\'ENUM\')\n\ndef hasAttr(attrs,name):\n    for attr in attrs:\n        if attr[""name""] == name:\n            return True\n    return False\n\ndef addAttrsToDb(db, attrs, item_type):\n    if not hasAttr(attrs,""content_type""):\n        attrs.append({""name"":""content_type"", ""value_type"":[""article""]})\n    for attr in attrs:\n        print ""adding item type"",item_type,""attribute "",attr[\'name\']\n        attrValType = attr[\'value_type\']\n        if type(attrValType) is list:\n            attrValType = \'enum\'\n        cur = db.cursor()\n        cur.execute(""INSERT INTO ITEM_ATTR (name, type, item_type) ""\n                    + "" VALUES (%s, %s, %s)"", (attr[\'name\'], value_types_to_db_map[attrValType], item_type))\n        if attrValType is \'enum\':\n            for index,enum in enumerate(attr[\'value_type\'], start=1):\n                cur = db.cursor()\n                cur.execute(""SELECT attr_id FROM ITEM_ATTR WHERE NAME = %s and ITEM_TYPE = %s"", (attr[\'name\'],item_type))\n                rows = cur.fetchall()\n                attrId = rows[0][0]\n                cur = db.cursor()\n                cur.execute(""INSERT INTO ITEM_ATTR_ENUM (attr_id, value_id, value_name) VALUES (%s, %s, %s)"",(attrId, index, enum))\n    cur = db.cursor()\n    cur.execute(""SELECT e.attr_id, e.value_id, e.value_name FROM ITEM_ATTR_ENUM e join item_attr a on (a.attr_id=e.attr_id and a.item_type=%s)"",(item_type,))\n    rows = cur.fetchall()\n    for row in rows:\n            enum_attr_id = row[0]\n            enum_value_id = row[1]\n            enum_value_name = row[2]\n            cur = db.cursor()\n            cur.execute(""INSERT INTO DIMENSION (item_type, attr_id, value_id) VALUES""\n                    + "" (%s, %s, %s)"", (item_type, enum_attr_id, enum_value_id))\n\ndef doDbChecks(db):\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_TYPE"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_TYPE table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tif rows[0][0] != 0:\n\t\tprint ""ITEM_ATTR table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""ITEM_ATTR_ENUM table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\tcur = db.cursor()\n\tcur.execute(""SELECT COUNT(*) FROM DIMENSION"")\n\trows = cur.fetchall()\n\tif rows[0][0] !=0:\n\t\tprint ""DIMENSION table was not empty, it had"", rows[0][0], \'rows\'\n\t\tdoExitBecauseDbNotEmpty()\n\n\ndef doExitBecauseDbNotEmpty():\n\tprint ""To run this script, the relevant DB tables must be empty. Please rerun this script with the -clean option to delete these entries.""\n\texit(1)\n\ndef addToDb(db, types):\n\twith db:\n\t\tdoDbChecks(db)\n\t\tfor theType in types:\n\t\t\tcur= db.cursor()\n\t\t\tcur.execute(""INSERT INTO ITEM_TYPE (type_id, name)""+\n\t\t\t\t"" values (%s, %s)"",(theType[\'type_id\'],theType[\'type_name\']))\n\t\t\taddAttrsToDb(db, theType[\'type_attrs\'], theType[\'type_id\'])\n\n\ndef validateValueType(valType):\n    theType = type(valType)\n    if theType is list:\n        for enum in valType:\n            theEnumType = type(enum)\n            if theEnumType is not unicode and theEnumType is not str:\n                print ""enum objects must be strings:"", theEnumType\n                exit(1)\n    elif theType is unicode:\n        if valType not in valid_value_types:\n            print ""the value type must be one of \'double\', \'string\', \'date\' or an object""\n            exit(1)\n    else:\n        print ""the type of the field value_type must be a string or a list where as it was"",theType\n        exit(1)\n\ndef validateAttr(theAttr):\n    if \'name\' not in theAttr or \'value_type\' not in theAttr:\n        print ""couldn\'t find one of (name, value_type) for attr ""\n        pp(theAttr)\n        exit(1)\n    else:\n        validateValueType(theAttr[\'value_type\']);\n\ndef validateType(theType):\n    if \'type_id\' not in theType or \'type_name\' not in theType or \'type_attrs\' not in theType:\n        print ""couldn\'t find one of (type_id, type_name, type_attrs) for object""\n        pp(theType)\n        exit(1)\n    for theAttr in theType[\'type_attrs\']:\n        validateAttr(theAttr)\n\ndef validateNumbering(types):\n    ids = set()\n    for theType in types:\n        if isinstance(theType[\'type_id\'], int):\n            if theType[\'type_id\'] in ids:\n                print ""found a repeated type_id"", theType[\'type_id\']\n                exit(1)\n            else:\n                ids.add(theType[\'type_id\'])\n        else:\n            print ""type_id s must be integers but one was"",""\\"""",theType[\'type_id\'],""\\""""\n            exit(1)\n\ndef outputDimensionsToFile(file, db):\n\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""SELECT d.dim_id, e.value_name from DIMENSION d, ITEM_ATTR_ENUM e where d.attr_id = e.attr_id and d.value_id = e.value_id and e.value_name != \\\'article\\\'"")\n\t\trows = cur.fetchall()\n\t\tjson.dump(rows, file)\n\ndef readTypes(types):\n    for theType in types:\n        validateType(theType)\n    validateNumbering(types)\n    return types\n\ndef clearUp(db):\n\twith db:\n\t\tcur = db.cursor()\n\t\tcur.execute(""TRUNCATE TABLE ITEMS"")\n\t\tcur.execute(""TRUNCATE TABLE DIMENSION"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR_ENUM"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_ATTR"")\n\t\tcur.execute(""TRUNCATE TABLE ITEM_TYPE"")\n\t\t#cur.execute(\'truncate table users\')\n\t\tcur.execute(\'truncate table items\')\n\t\tcur.execute(\'truncate table item_map_varchar\')\n\t\tcur.execute(\'truncate table item_map_double\')\n\t\tcur.execute(\'truncate table item_map_datetime\')\n\t\tcur.execute(\'truncate table item_map_int\')\n\t\tcur.execute(\'truncate table item_map_boolean\')\n\t\tcur.execute(\'truncate table item_map_enum\')\n\t\tcur.execute(\'truncate table item_map_text\')\n\ndef create_schema(client_name, dbSettings, scheme_file_path, clean=False):\n\n    if clean != True:\n        json_data=open(scheme_file_path)\n        data = json.load(json_data)\n        if \'types\' not in data:\n            print ""couldn\'t find types object in json""\n            return\n        else:\n            types = readTypes(data[\'types\'])\n\n    db = MySQLdb.connect(user=dbSettings[""user""],db=client_name,passwd=dbSettings[""password""], host=dbSettings[""host""])\n    if clean == True:\n        clearUp(db)\n        print ""Finished cleaning attributes successfully""\n    else:\n        addToDb(db, types)\n        f = open(\'dimensions.json\',\'w\')\n        outputDimensionsToFile(f,db)\n\n        print \'Finished applying attributes successfully\'\n\n        json_data.close()\n\n'"
python/seldon/shell/cmd_alg.py,0,"b'import os\nimport pprint\nimport json\nimport errno\n\nimport zk_utils\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n    \'help_cmd_strs_list\' : [\n        (""alg"", ""shows the Default recommenders""),\n        (""alg show <clientName>"", ""show algs for client""),\n        (""alg add <clientName>"", ""add algs for client""),\n        (""alg delete <clientName>"", ""pick alg to delete for client""),\n        (""alg promote <clientName>"", ""pick alg to promote for client""),\n        (""alg commit <clientName>"", ""commit chnanges"")\n    ],\n}\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef ensure_client_has_algs(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/algs/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/algs""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_alg_json = \'{""algorithms"":[{""config"":[],""filters"":[],""includers"":[],""name"":""recentItemsRecommender""}],""combiner"":""firstSuccessfulCombiner""}\'\n            data = json_to_dict(default_alg_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, dict_to_json(data))\n\ndef show_algs(data):\n    combiner = data[""combiner""]\n    algorithms = data[""algorithms""]\n    print ""algorithms:""\n    for alg in algorithms:\n        print ""    {alg_name}"".format(alg_name=alg[""name""])\n        for config_item in alg[""config""]:\n            print ""        {n}={v}"".format(n=config_item[""name""],v=config_item[""value""])\n    print ""combiner: ""+combiner\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef subcmd_show(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to show the algs for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n    show_algs(data)\n\ndef subcmd_add(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to add algs for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    default_algorithms = command_data[""default_algorithms""]\n    recommenders = default_algorithms.keys()\n    for idx,recommender in enumerate(recommenders):\n        print ""    [{idx}] {recommender}"".format(**locals())\n    q=""Choose recommender: ""\n    recommender_idx = None\n    try:\n        recommender_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    recommender_name = recommenders[recommender_idx] if recommender_idx != None and recommender_idx>=0 and recommender_idx<len(recommenders) else None\n\n    if recommender_name == None:\n        print ""Invaild recommender""\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n\n    algorithms = data[""algorithms""]\n    includers = default_algorithms[recommender_name][""includers""] if default_algorithms[recommender_name].has_key(""includers"") else []\n    recommender_data = {\n            \'filters\':[],\n            \'includers\': includers,\n            \'name\': recommender_name,\n            \'config\': default_algorithms[recommender_name][""config""]\n    }\n    algorithms.append(recommender_data)\n    write_data_to_file(data_fpath, data)\n    print ""Added [{recommender_name}]"".format(**locals())\n    show_algs(data)\n\ndef subcmd_delete(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to delete algs for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n\n    algorithms = data[""algorithms""]\n    print ""algorithms:""\n    for idx, alg in enumerate(algorithms):\n        print ""    [{idx}] {alg_name}"".format(idx=idx, alg_name=alg[""name""])\n\n    q=""Choose recommender to delete: ""\n    recommender_idx = None\n    try:\n        recommender_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    if recommender_idx == None or (not (recommender_idx>=0 and recommender_idx<len(algorithms))):\n        print ""Invalid choice!""\n        return\n\n    recommender_name=algorithms[recommender_idx][""name""]\n    del(algorithms[recommender_idx])\n    write_data_to_file(data_fpath, data)\n    print ""Removed [{recommender_name}]"".format(**locals())\n\ndef subcmd_promote(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to promote algs for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_algs(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = json_to_dict(json)\n\n    algorithms = data[""algorithms""]\n    print ""algorithms:""\n    for idx, alg in enumerate(algorithms):\n        print ""    [{idx}] {alg_name}"".format(idx=idx, alg_name=alg[""name""])\n\n    q=""Choose recommender to promote: ""\n    recommender_idx = None\n    try:\n        recommender_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    if recommender_idx == None or (not (recommender_idx>=1 and recommender_idx<len(algorithms))):\n        print ""Invalid choice!""\n        return\n\n    recommender_name=algorithms[recommender_idx][""name""]\n    algorithms[recommender_idx], algorithms[recommender_idx-1]  = algorithms[recommender_idx-1], algorithms[recommender_idx]\n    write_data_to_file(data_fpath, data)\n    print ""Promoted [{recommender_name}]"".format(**locals())\n\ndef subcmd_commit(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to delete algs for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs/_data_""\n    if not os.path.isfile(data_fpath):\n        ""Data to commit not found!!""\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    node_path = gdata[""all_clients_node_path""] + ""/"" + client_name + ""/algs""\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef subcmd_default(command_data):\n    print ""Default recommenders:""\n    default_algorithms = command_data[""default_algorithms""]\n    for recommender in default_algorithms:\n        print ""    {recommender}"".format(**locals())\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_alg(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""show"" : subcmd_show,\n    ""add"" : subcmd_add,\n    ""delete"" : subcmd_delete,\n    ""promote"" : subcmd_promote,\n    ""commit"" : subcmd_commit,\n}\n\n'"
python/seldon/shell/cmd_attr.py,0,"b'import os\nimport json\nimport pprint\nimport errno\nimport re\n\nimport zk_utils\nimport seldon_utils\nimport attr_schema_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n    \'help_cmd_strs_list\' : [\n        (""attr"", ""todo""),\n        (""attr edit <clientName>"", ""edit the attributes for client""),\n        (""attr show <clientName>"", ""show the attributes for client""),\n        (""attr apply <clientName>"", ""todo"")\n    ],\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef write_data_to_file(data_fpath, data):\n    json = seldon_utils.dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    seldon_utils.mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = seldon_utils.json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef ensure_client_has_attr(zkroot, zk_client, client_name):\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/attr/_data_"".format(\n            zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n    if not os.path.isfile(data_fpath):\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/attr""\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_attr_json = \'{""types"":[{""type_attrs"":[{""name"":""title"",""value_type"":""string""}],""type_id"":1,""type_name"":""defaulttype""}]}\'\n            data = seldon_utils.json_to_dict(default_attr_json)\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef show_attr(data):\n    attr_types = data[""types""]\n    print ""types:""\n    for attr_type in attr_types:\n        attr_type_name = attr_type[""type_name""]\n        attr_type_id = attr_type[""type_id""]\n        attr_type_attrs = attr_type[""type_attrs""]\n        print ""   [{attr_type_name}]"".format(**locals())\n        print ""       type_id: {attr_type_id}"".format(**locals())\n        print ""       type_attrs:""\n        for attr_type_attr in attr_type_attrs:\n            attrib_name = attr_type_attr[""name""]\n            attrib_value = attr_type_attr[""value_type""]\n            attrib_value_str = ""enum[""+"","".join(attrib_value)+""]"" if isinstance(attrib_value,list) else attrib_value\n            print ""           {attrib_name}: {attrib_value_str}"".format(**locals())\n\ndef subcmd_show(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to show the attr for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n    show_attr(data)\n\ndef subcmd_edit(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to edit the attr for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n    data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    #do the edit\n    from subprocess import call\n    editor=seldon_utils.get_editor()\n    call([editor, data_fpath])\n\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    if data is None:\n        print ""Invalid attr json!""\n    else:\n        write_data_to_file(data_fpath, data)\n        node_path = gdata[""all_clients_node_path""]+""/""+client_name+""/attr""\n        zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n        show_attr(data)\n\ndef subcmd_apply(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to apply the attr for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_client_has_attr(zkroot, zk_client, client_name)\n\n    def get_db_jndi_name():\n        data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n        DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n        return DB_JNDI_NAME\n\n    def get_db_info(db_name):\n        data_fpath = zkroot + ""/config/dbcp/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n\n        db_info = None\n        for db_info_entry in data[\'dbs\']:\n            if db_info_entry[\'name\'] == db_name:\n                db_info = db_info_entry\n                break\n        return db_info\n\n    def get_db_settings():\n        dbSettings = {}\n        dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n        dbSettings[""user""]=db_info[""user""]\n        dbSettings[""password""]=db_info[""password""]\n        return dbSettings\n\n    db_name = get_db_jndi_name()\n    db_info = get_db_info(db_name)\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        return\n\n    dbSettings = get_db_settings()\n\n    scheme_file_path = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/attr/_data_""\n    clean = True\n    attr_schema_utils.create_schema(client_name, dbSettings, scheme_file_path, clean)\n    clean = False\n    attr_schema_utils.create_schema(client_name, dbSettings, scheme_file_path, clean)\n\ndef subcmd_default(command_data):\n    print ""todo default!""\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_attr(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""show"" : subcmd_show,\n    ""edit"" : subcmd_edit,\n    ""apply"" : subcmd_apply,\n}\n\n'"
python/seldon/shell/cmd_client.py,0,"b'import os\nimport json\nimport pprint\nimport errno\nimport re\n\nimport zk_utils\nimport seldon_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n    \'help_cmd_strs_list\' : [\n        (""client"", ""shows list of clients already setup""),\n        (""client setup <clientName> <dbName>"", ""to create a new client/TODO deal with existing"")\n    ],\n}\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef subcmd_list(command_data):\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    client_nodes = zk_client.get_children(\'/all_clients\')\n    for client_node in client_nodes:\n        print client_node\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef get_data_fpath(zkroot, client):\n    return zkroot + gdata[""all_clients_node_path""] + ""/"" + client + ""/_data_""\n\ndef subcmd_default(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n\n    all_clients_fpath = zkroot + gdata[""all_clients_node_path""]\n    if not os.path.isdir(all_clients_fpath):\n        # the dir for all_clients doesnt exist\n        if zk_client.exists(gdata[""all_clients_node_path""]):\n            client_nodes = zk_client.get_children(gdata[""all_clients_node_path""])\n            def write_node_value_to_file(node_path):\n                node_value = zk_utils.node_get(zk_client, node_path)\n                node_value = node_value.strip()\n                if zk_utils.is_json_data(node_value):\n                    data = json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n                else:\n                    data = str(node_value)\n                data_fpath = zkroot + node_path + ""/_data_""\n                write_data_to_file(data_fpath, data)\n            for client_node in client_nodes:\n                node_path=gdata[""all_clients_node_path""]+""/""+client_node\n                write_node_value_to_file(node_path)\n                client_child_nodes = zk_client.get_children(gdata[""all_clients_node_path""]+""/""+client_node)\n                for client_child_node in client_child_nodes:\n                    node_path=gdata[""all_clients_node_path""]+""/""+client_node+""/""+client_child_node\n                    write_node_value_to_file(node_path)\n\n    if not os.path.isdir(all_clients_fpath):\n        print ""No clients found!""\n    else:\n        for client in os.listdir(zkroot + gdata[""all_clients_node_path""]):\n            print client\n            data_fpath = get_data_fpath(zkroot, client)\n            f = open(data_fpath)\n            json = f.read()\n            print json\n            data = json_to_dict(json)\n            f.close()\n            print ""client[{client}]:"".format(**locals())\n            DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n            print ""    DB_JNDI_NAME: ""+DB_JNDI_NAME\n\ndef add_client(zk_client, zkroot, client_name, db_name, consumer_details=None):\n    data_fpath = zkroot + ""/config/dbcp/_data_""\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    db_info = None\n    for db_info_entry in data[\'dbs\']:\n        if db_info_entry[\'name\'] == db_name:\n            db_info = db_info_entry\n            break\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        return\n\n    dbSettings = {}\n    dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n    dbSettings[""user""]=db_info[""user""]\n    dbSettings[""password""]=db_info[""password""]\n    seldon_utils.addApiDb(db_name, dbSettings)\n    seldon_utils.addClientDb(client_name, dbSettings, consumer_details=None)\n    # write to local file\n    data_fpath = get_data_fpath(zkroot, client_name)\n    data = {\'DB_JNDI_NAME\': db_name}\n    write_data_to_file(data_fpath, data)\n    # write to zookeeper\n    node_path=gdata[""all_clients_node_path""]+""/""+client_name\n    data_json = dict_to_json(data)\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef subcmd_setup(command_data):\n    client_name_to_setup = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    db_name_to_use = command_data[""subcmd_args""][1] if len(command_data[""subcmd_args""])>1 else None\n    if client_name_to_setup == None:\n        print ""Need client name to setup""\n        return\n\n    # check if this client exists\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    data_fpath = get_data_fpath(zkroot, client_name_to_setup)\n    if not os.path.isfile(data_fpath):\n        print ""Trying to create the client""\n        if db_name_to_use == None:\n            print ""Need a db name""\n            return\n        add_client(zk_client, zkroot, client_name_to_setup, db_name_to_use)\n\ndef subcmd_alg(command_data):\n    client_name_to_setup = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name_to_setup == None:\n        print ""Need client name to setup""\n        return\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name_to_setup}/algs/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name_to_setup=client_name_to_setup)\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    combiner = data[""combiner""]\n    algorithms = data[""algorithms""]\n    print ""combiner: ""+combiner\n    print ""algorithms:""\n    for alg in algorithms:\n        print ""    {alg_name}"".format(alg_name=alg[""name""])\n\ndef subcmd_algsetup(command_data):\n    pp(command_data)\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_client(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""list"" : subcmd_list,\n    ""setup"" : subcmd_setup,\n    ""alg"" : subcmd_alg,\n    ""algsetup"" : subcmd_algsetup,\n}\n\n'"
python/seldon/shell/cmd_db.py,0,"b'import pprint\nimport cmdutils\nimport json\nimport re\nimport os\nimport errno\nimport copy\n\nimport zk_utils\n\ngdata = {\n    \'data_path\': ""/config/dbcp/_data_"",\n    \'node_path\': ""/config/dbcp"",\n    \'default_data\': {\n        \'dbs\': [\n            {\n                \'jdbc\': ""jdbc:mysql:replication://127.0.0.1:3306,127.0.0.1:3306/?characterEncoding=utf8&useServerPrepStmts=true&logger=com.mysql.jdbc.log.StandardLogger&roundRobinLoadBalance=true&transformedBitIsBoolean=true&rewriteBatchedStatements=true"",\n                \'password\': ""mypass"",\n                ""name"": ""ClientDB"",\n                \'user\': ""root""\n            }\n         ]\n    },\n    \'help_cmd_strs_list\' : [\n        (""db"", ""show the currently setup databases""),\n        (""db setup <SomeDb>"", ""setup the db given""),\n        (""db commit"", ""commit changes in file to zookeeper"")\n    ],\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef ensure_local_data_file_exists(zk_client, zkroot):\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        node_path=gdata[""node_path""]\n        node_value = zk_utils.node_get(zk_client, node_path)\n        data = json_to_dict(node_value) if node_value != None else gdata[""default_data""]\n        write_data_to_file(data_fpath, data)\n\ndef subcmd_setup(command_data):\n    def get_input_var(var_name, var_current_value):\n        var_current_value_to_display = (var_current_value[:45] + \'...\') if len(var_current_value) > 45 else var_current_value\n        q=""{var_name}[{var_current_value}]:"".format(var_name=var_name,var_current_value=var_current_value_to_display)\n        var_val = raw_input(q)\n        var_val = var_val.strip()\n        is_changed = len(var_val) > 0\n        var_val = var_val if is_changed else var_current_value\n        return var_val,is_changed\n\n    db_to_setup = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if db_to_setup == None:\n        print ""Need db name to setup""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n    print ""Setting up Databases""\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    has_any_value_changed = False\n\n    db_entries_found = [db_entry for db_entry in data[""dbs""] if db_entry[""name""]==db_to_setup]\n    db_entry = db_entries_found[0] if len(db_entries_found)>0 else None\n\n    if db_entry == None:\n        db_entry = copy.deepcopy(gdata[""default_data""][""dbs""][0])\n        db_entry[""name""] = db_to_setup\n        data[""dbs""].append(db_entry)\n        has_any_value_changed = True\n\n    name,is_changed = get_input_var(""name"",db_entry[""name""]); has_any_value_changed |= is_changed\n    jdbc,is_changed = get_input_var(""jdbc"",db_entry[""jdbc""]); has_any_value_changed |= is_changed\n    user,is_changed = get_input_var(""user"",db_entry[""user""]); has_any_value_changed |= is_changed\n    password,is_changed = get_input_var(""password"",db_entry[""password""]); has_any_value_changed |= is_changed\n\n    db_entry[""name""] = name\n    db_entry[""jdbc""] = jdbc\n    db_entry[""user""] = user\n    db_entry[""password""] = password\n\n    if has_any_value_changed:\n        write_data_to_file(data_fpath, data)\n\ndef subcmd_delete(command_data):\n    db_to_delete = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if db_to_delete == None:\n        print ""Need db name to delete""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    index_found_to_delete = [idx for idx,db_entry in enumerate(data[""dbs""]) if db_entry[""name""]==db_to_delete]\n    if len(index_found_to_delete) >0:\n        idx_to_delete = index_found_to_delete[0]\n        del data[""dbs""][idx_to_delete]\n        print ""Removed the entry for {db_to_delete}"".format(**locals())\n        write_data_to_file(data_fpath, data)\n    else:\n        print ""No entry for {db_to_delete} found!"".format(**locals())\n\ndef subcmd_list(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    db_list = [str(db[\'name\']) for db in data[\'dbs\']]\n    list_str = "", "".join(db_list)\n    print list_str\n\ndef subcmd_default(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n    dbs = data[\'dbs\']\n    for db in dbs:\n        name = db[""name""]\n        jdbc = db[""jdbc""]\n        user = db[""user""]\n        password = db[""password""]\n        print ""db[{name}]:"".format(**locals())\n        print ""    jdbc: {jdbc}"".format(**locals())\n        print ""    user: {user}"".format(**locals())\n        print ""    password: {password}"".format(**locals())\n\ndef subcmd_commit(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    data_fpath = zkroot + gdata[\'data_path\']\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    node_path=gdata[""node_path""]\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef cmd_db(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""setup"" : subcmd_setup,\n    ""list"" : subcmd_list,\n    ""commit"" : subcmd_commit,\n    ""delete"" : subcmd_delete\n}\n\n'"
python/seldon/shell/cmd_import.py,0,"b'import os\nimport pprint\nimport seldon_utils\nimport re\n\nimport import_items_utils\nimport import_users_utils\nimport import_actions_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n    \'help_cmd_strs_list\' : [\n        (""import"", ""todo""),\n        (""import items <clientName> </path/to/items.csv>"", ""import items for client""),\n        (""import users <clientName> </path/to/users.csv>"", ""import users for client""),\n        (""import actions <clientName> </path/to/actions.csv>"", ""import actions for client"")\n    ],\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef get_db_settings(zkroot, client_name):\n    def get_db_jndi_name():\n        data_fpath = zkroot + gdata[""all_clients_node_path""] + ""/"" + client_name + ""/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n        DB_JNDI_NAME = data[""DB_JNDI_NAME""] if isinstance(data, dict) and data.has_key(""DB_JNDI_NAME"") else """"\n        return DB_JNDI_NAME\n\n    def get_db_info(db_name):\n        data_fpath = zkroot + ""/config/dbcp/_data_""\n        f = open(data_fpath)\n        json = f.read()\n        data = seldon_utils.json_to_dict(json)\n        f.close()\n\n        db_info = None\n        for db_info_entry in data[\'dbs\']:\n            if db_info_entry[\'name\'] == db_name:\n                db_info = db_info_entry\n                break\n        return db_info\n\n    db_name = get_db_jndi_name()\n    db_info = get_db_info(db_name)\n\n    if db_info == None:\n        print ""Invalid db name[{db_name}]"".format(**locals())\n        return None\n\n    dbSettings = {}\n    dbSettings[""host""]=re.search(\'://(.*?):(.*?),\',db_info[""jdbc""]).groups()[0]\n    dbSettings[""user""]=db_info[""user""]\n    dbSettings[""password""]=db_info[""password""]\n    return dbSettings\n\ndef subcmd_items(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to add model for""\n        return\n\n    data_file_fpath = command_data[""subcmd_args""][1] if len(command_data[""subcmd_args""])>1 else None\n    if data_file_fpath == None:\n        print ""Need data file path to import""\n        return\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        return\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    import_items_utils.import_items(client_name, db_settings, data_file_fpath)\n\ndef subcmd_users(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to add model for""\n        return\n\n    data_file_fpath = command_data[""subcmd_args""][1] if len(command_data[""subcmd_args""])>1 else None\n    if data_file_fpath == None:\n        print ""Need data file path to import""\n        return\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        return\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    import_users_utils.import_users(client_name, db_settings, data_file_fpath)\n\ndef subcmd_actions(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to add model for""\n        return\n\n    data_file_fpath = command_data[""subcmd_args""][1] if len(command_data[""subcmd_args""])>1 else None\n    if data_file_fpath == None:\n        print ""Need data file path to import""\n        return\n\n    zkroot = command_data[""conf_data""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    if not os.path.isfile(data_file_fpath):\n        print ""Invalid file[{data_file_fpath}]"".format(**locals())\n        return\n\n    db_settings = get_db_settings(zkroot, client_name)\n\n    out_file_dir = command_data[""conf_data""][""seldon_models""] + ""/"" + client_name + ""/actions/1""\n    out_file_fpath = out_file_dir + ""/actions.json""\n\n    seldon_utils.mkdir_p(out_file_dir)\n\n    import_actions_utils.import_actions(client_name, db_settings, data_file_fpath, out_file_fpath)\n\ndef subcmd_default(command_data):\n    print ""todo default!""\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]+10\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_import(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""items"" : subcmd_items,\n    ""users"" : subcmd_users,\n    ""actions"" : subcmd_actions,\n}\n\n'"
python/seldon/shell/cmd_memcached.py,0,"b'import pprint\nimport cmdutils\nimport json\nimport re\nimport os\nimport errno\nimport zk_utils\n\ngdata = {\n    \'data_path\': ""/config/memcached/_data_"",\n    \'node_path\': ""/config/memcached"",\n    \'default_data\': {\n        \'numClients\': 1,\n        \'servers\': ""localhost:11211""\n    },\n    \'help_cmd_strs_list\' : [\n        (""memcached"", ""shows the current setup""),\n        (""memcached setup"", ""change the current setup - write chnages to file""),\n        (""memcached commit"", ""commit changes in file to zookeeper"")\n    ],\n}\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef write_data_to_file(data_fpath, data):\n    json = dict_to_json(data, True)\n    mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef subcmd_commit(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        write_data_to_file(data_fpath, gdata[""default_data""])\n    f = open(data_fpath)\n    data_json = f.read()\n    f.close()\n\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    node_path=gdata[""node_path""]\n    zk_utils.node_set(zk_client, node_path, data_json)\n\ndef subcmd_setup(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    def get_input_var(var_name, var_current_value):\n        q=""{var_name}[{var_current_value}]:"".format(var_name=var_name,var_current_value=var_current_value)\n        var_val = raw_input(q)\n        var_val = var_val.strip()\n        is_changed = len(var_val) > 0\n        var_val = var_val if is_changed else var_current_value\n        return var_val,is_changed\n\n    print ""Setting up memcached""\n    data_fpath = zkroot + gdata[\'data_path\']\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n\n    has_any_value_changed = False\n\n    numClients,is_changed = get_input_var(""numClients"",data[""numClients""])\n    numClients = int(numClients)\n    has_any_value_changed = has_any_value_changed | is_changed\n\n    servers,is_changed = get_input_var(""servers"",data[""servers""])\n    has_any_value_changed = has_any_value_changed | is_changed\n\n    data = gdata[""default_data""]\n    data[""numClients""] = numClients\n    data[""servers""] = servers\n    if has_any_value_changed:\n        write_data_to_file(data_fpath, data)\n\ndef ensure_local_data_file_exists(zk_client, zkroot):\n    data_fpath = zkroot + gdata[\'data_path\']\n    if not os.path.isfile(data_fpath):\n        node_path=gdata[""node_path""]\n        node_value = zk_utils.node_get(zk_client, node_path)\n        data = json_to_dict(node_value) if node_value != None else gdata[""default_data""]\n        write_data_to_file(data_fpath, data)\n\ndef subcmd_default(command_data):\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    ensure_local_data_file_exists(zk_client, zkroot)\n\n    data_fpath = zkroot + gdata[\'data_path\']\n    f = open(data_fpath)\n    json = f.read()\n    data = json_to_dict(json)\n    f.close()\n    print ""memcached:""\n    print ""    numClients: {numClients}"".format(numClients=data[""numClients""])\n    print ""    servers: {servers}"".format(servers=data[""servers""])\n\ndef subcmd_test(command_data):\n    print ""-- Testing --""\n    zk_client=command_data[""zkdetails""][""zk_client""]\n    node_path=gdata[""node_path""]+\'x\'\n    node_value = zk_utils.node_get(zk_client, node_path)\n    pp(node_value)\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_memcached(subcmd, command_data):\n    if subcmd == """":\n        subcmd_default(command_data)\n    else:\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""setup"" : subcmd_setup,\n    ""commit"" : subcmd_commit,\n    ""test"" : subcmd_test\n}\n\n'"
python/seldon/shell/cmd_model.py,0,"b'import os\nimport pprint\nfrom subprocess import call\n\nimport zk_utils\nimport seldon_utils\n\ngdata = {\n    \'all_clients_node_path\': ""/all_clients"",\n    \'help_cmd_strs_list\' : [\n        (""model add <clientName>"", ""add a model for client""),\n        (""model edit <clientName>"", ""edit a model for client""),\n        (""model show <clientName>"", ""show the added models for client""),\n        (""model train <clientName>"", ""choose model and run offline training"")\n    ],\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef write_data_to_file(data_fpath, data):\n    json = seldon_utils.dict_to_json(data, True) if isinstance(data,dict) else str(data)\n    seldon_utils.mkdir_p(os.path.dirname(data_fpath))\n    f = open(data_fpath,\'w\')\n    f.write(json)\n    f.write(\'\\n\')\n    f.close()\n    print ""Writing data to file[{data_fpath}]"".format(**locals())\n\ndef write_node_value_to_file(zk_client, zkroot, node_path):\n    node_value = zk_utils.node_get(zk_client, node_path)\n    node_value = node_value.strip()\n    if zk_utils.is_json_data(node_value):\n        data = seldon_utils.json_to_dict(node_value) if node_value != None and len(node_value)>0 else """"\n    else:\n        data = str(node_value)\n    data_fpath = zkroot + node_path + ""/_data_""\n    write_data_to_file(data_fpath, data)\n\ndef is_existing_client(zkroot, client_name):\n    client_names = os.listdir(zkroot + gdata[""all_clients_node_path""])\n    if client_name in client_names:\n        return True\n    else:\n        return False\n\ndef subcmd_add(command_data):\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if client_name == None:\n        print ""Need client name to add model for""\n        return\n\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not is_existing_client(zkroot, client_name):\n        print ""Invalid client[{client_name}]"".format(**locals())\n        return\n\n    default_models = command_data[""default_models""]\n    models = default_models.keys()\n    for idx,model in enumerate(models):\n        print ""    [{idx}] {model}"".format(**locals())\n    q=""Choose model: ""\n    model_idx = None\n    try:\n        model_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    model_name = models[model_idx] if model_idx != None and model_idx>=0 and model_idx<len(models) else None\n\n    if model_name == None:\n        print ""Invaild model""\n        return\n\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline/{model_name}/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    if not os.path.isfile(data_fpath):\n        node_path = ""{all_clients_node_path}/{client_name}/offline/{model_name}"".format(all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n        if zk_client.exists(node_path):\n            write_node_value_to_file(zk_client, zkroot, node_path)\n        else:\n            default_model_data = default_models[model_name][""config""]\n            if default_model_data.has_key(""inputPath""):\n                default_model_data[""inputPath""]=command_data[""seldon_models""]\n            if default_model_data.has_key(""outputPath""):\n                default_model_data[""outputPath""]=command_data[""seldon_models""]\n            data = default_model_data\n            write_data_to_file(data_fpath, data)\n            zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n    else:\n        print ""Model [{model_name}] already added"".format(**locals())\n\ndef subcmd_default(command_data):\n    pass\n\ndef subcmd_edit(command_data):\n    def have_client_name():\n        if client_name == None:\n            print ""Need client name to edit model for""\n            return False\n        else:\n            return True\n\n    def have_valid_client_name():\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            return False\n        else:\n            return True\n\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if not have_client_name(): return\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not have_valid_client_name(): return\n\n    models_for_client_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n\n    models = os.listdir(models_for_client_fpath)\n\n    for idx,model in enumerate(models):\n        print ""    [{idx}] {model}"".format(**locals())\n    q=""Choose model: ""\n    model_idx = None\n    try:\n        model_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    model_name = models[model_idx] if model_idx != None and model_idx>=0 and model_idx<len(models) else None\n\n    if model_name == None:\n        print ""Invaild model""\n        return\n\n    data_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline/{model_name}/_data_"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n\n    #do the edit\n    editor=seldon_utils.get_editor()\n    call([editor, data_fpath])\n\n    f = open(data_fpath)\n    json = f.read()\n    f.close()\n    data = seldon_utils.json_to_dict(json)\n\n    if data is None:\n        print ""Invalid model json!""\n    else:\n        write_data_to_file(data_fpath, data)\n        node_path = ""{all_clients_node_path}/{client_name}/offline/{model_name}"".format(all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name,model_name=model_name)\n        pp(node_path)\n        zk_utils.node_set(zk_client, node_path, seldon_utils.dict_to_json(data))\n\ndef subcmd_show(command_data):\n    def have_client_name():\n        if client_name == None:\n            print ""Need client name to edit model for""\n            return False\n        else:\n            return True\n\n    def have_valid_client_name():\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            return False\n        else:\n            return True\n\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if not have_client_name(): return\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not have_valid_client_name(): return\n\n    models_for_client_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n\n    models = os.listdir(models_for_client_fpath)\n\n    for idx,model in enumerate(models):\n        print ""    {model}"".format(**locals())\n\ndef run_spark_job(command_data, job_info, client_name):\n    conf_data = command_data[""conf_data""]\n    spark_home = conf_data[""spark_home""]\n    seldon_spark_home = conf_data[""seldon_spark_home""]\n    seldon_version = conf_data[""seldon_version""]\n    zk_hosts = conf_data[""zk_hosts""]\n\n    cmd = job_info[""cmd""].replace(""%SPARK_HOME%"", spark_home)\n\n    cmd_args = job_info[""cmd_args""]\n\n    replacements = [\n        (""%CLIENT_NAME%"", client_name),\n        (""%SPARK_HOME%"", spark_home),\n        (""%SELDON_SPARK_HOME%"", seldon_spark_home),\n        (""%SELDON_VERSION%"", seldon_version),\n        (""%ZK_HOSTS%"", zk_hosts),\n    ]\n\n    def appy_replacements(item):\n        for rpair in replacements:\n            item = item.replace(rpair[0],rpair[1])\n        return item\n\n    cmd_args = map(appy_replacements, cmd_args)\n\n    call([cmd]+cmd_args)\n\ndef subcmd_train(command_data):\n    def have_client_name():\n        if client_name == None:\n            print ""Need client name to edit model for""\n            return False\n        else:\n            return True\n\n    def have_valid_client_name():\n        if not is_existing_client(zkroot, client_name):\n            print ""Invalid client[{client_name}]"".format(**locals())\n            return False\n        else:\n            return True\n\n    client_name = command_data[""subcmd_args""][0] if len(command_data[""subcmd_args""])>0 else None\n    if not have_client_name(): return\n    zk_client = command_data[""zkdetails""][""zk_client""]\n    zkroot = command_data[""zkdetails""][""zkroot""]\n    if not have_valid_client_name(): return\n\n    models_for_client_fpath = ""{zkroot}{all_clients_node_path}/{client_name}/offline"".format(zkroot=zkroot,all_clients_node_path=gdata[""all_clients_node_path""],client_name=client_name)\n\n    models = os.listdir(models_for_client_fpath)\n\n    for idx,model in enumerate(models):\n        print ""    [{idx}] {model}"".format(**locals())\n    q=""Choose model: ""\n    model_idx = None\n    try:\n        model_idx = int(raw_input(q))\n    except ValueError:\n        pass\n\n    model_name = models[model_idx] if model_idx != None and model_idx>=0 and model_idx<len(models) else None\n\n    if model_name == None:\n        print ""Invaild model""\n        return\n\n    default_models = command_data[""default_models""]\n    model_training = default_models[model_name][""training""]\n    job_type = model_training[""job_type""]\n    job_info = model_training[""job_info""]\n\n    job_handlers = {\n            \'spark\' : run_spark_job\n    }\n\n    if job_handlers.has_key(job_type):\n        job_handlers[job_type](command_data, job_info, client_name)\n    else:\n        print ""No handler found for job_type[{job_type}]"".format(**locals())\n\ndef subcmd_help(command_data):\n    lmargin_size=command_data[""help_formatting""][""lmargin_size""]\n    cmd_size=command_data[""help_formatting""][""cmd_size""]\n    lmargin_pad="" ""\n    for help_strs in gdata[""help_cmd_strs_list""]:\n        cmd_str = help_strs[0]\n        cmd_help = help_strs[1]\n        print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\ndef cmd_model(args, command_data):\n    if args == """":\n        subcmd_default(command_data)\n    else:\n        subcmd = args.split()[0]\n        subcmd_args = args.split()[1:]\n        command_data[""subcmd""] = subcmd\n        command_data[""subcmd_args""] = subcmd_args\n        if subcmds.has_key(subcmd):\n            subcmds[subcmd](command_data)\n        else:\n            print ""unkown subcmd[%s]"" % subcmd\n\nsubcmds = {\n    ""help"" : subcmd_help,\n    ""add"" : subcmd_add,\n    ""edit"" : subcmd_edit,\n    ""train"" : subcmd_train,\n    ""show"" : subcmd_show,\n}\n\n'"
python/seldon/shell/cmdutils.py,0,"b'import MySQLdb as mdb\nimport json\nimport re\n\ndef get_conn(user, passwd, db, host, port):\n    conn = mdb.connect(user=user, passwd=passwd,db=db, host=host, port=port)\n    return conn;\n\ndef get_db_details(zk_client):\n    db_details = {}\n    dbcp_node =  zk_client.get(\'/config/dbcp\')\n    dbcp_details_json = dbcp_node[0]\n    dbcp_details = json.loads(dbcp_details_json)\n    for db_info in dbcp_details[\'dbs\']:\n        name = db_info[\'name\']\n        user = db_info[\'user\']\n        passwd = db_info[\'password\']\n        db = \'mysql\'\n        jdbc = db_info[\'jdbc\']\n        m=re.search(\':\\/\\/(.*)[:](.*),\', jdbc)\n        host = m.group(1)\n        port = int(m.group(2))\n        db_details[ name ] = {\n            ""host"" : host,\n            ""port"" : port,\n            ""status"" : """",\n            ""error_msg"" : """"\n        }\n        try:\n            conn = get_conn(user, passwd, db, host, port)\n            try:\n                cur = conn.cursor(mdb.cursors.DictCursor)\n                sql=""show status like \'Threads_connected%\';""\n                cur.execute(sql)\n                rows = cur.fetchall()\n                cur.close()\n                db_details[ name ][\'status\'] = ""OK""\n            except Exception as e:\n                error_msg=str(e)\n                db_details[ name ][\'status\'] = ""FAILED""\n                db_details[ name ][\'error_msg\'] = error_msg\n            finally:\n                conn.close()\n        except Exception as e:\n            error_msg=str(e)\n            db_details[ name ][\'status\'] = ""FAILED""\n            db_details[ name ][\'error_msg\'] = error_msg\n    return db_details\n'"
python/seldon/shell/import_actions_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef getItemId(db,cache,client_item_id):\n    if client_item_id in cache:\n        return cache[client_item_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select item_id, client_item_id from items"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            itemId = long(row[0])\n            client_item_id_from_db = row[1]\n            cache[client_item_id_from_db] = itemId\n        cursor.close()\n        return cache[client_item_id]\n\ndef getUserId(db,cache,client_user_id):\n    if client_user_id in cache:\n        return cache[client_user_id]\n    else:\n        cursor = db.cursor()\n        cursor.execute(""""""select user_id,client_user_id from users"""""")\n        rows = cursor.fetchall()\n        for row in rows:\n            userId = long(row[0])\n            client_user_id_from_db = row[1]\n            cache[client_user_id_from_db] = userId\n\n        cursor.close()\n        return cache[client_user_id]\n\ndef import_actions(client_name, db_settings, data_file_fpath, out_file_fpath):\n\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    userCache = {}\n    itemCache = {}\n    count = 0\n    print out_file_fpath\n    with open(data_file_fpath) as csvfile, open(out_file_fpath,\'w\') as outfile:\n        reader = unicodecsv.DictReader(csvfile,encoding=\'utf-8\')\n        for f in reader:\n            item = getItemId(db,itemCache,f[""item_id""])\n            user = getUserId(db,userCache,f[""user_id""])\n            action_type = 1\n            action = {}\n            action[""userid""] = int(user)\n            action[""client_userid""] = f[""user_id""]\n            action[""itemid""] = int(item)\n            action[""client_itemid""] = f[""item_id""]\n            action[""value""] = float(f[""value""])\n            utc = datetime.datetime.fromtimestamp(int(f[""time""])).strftime(\'%Y-%m-%dT%H:%M:%SZ\')\n            action[""timestamp_utc""] = utc\n            action[""rectag""] = ""default""\n            action[""type""] = action_type\n            action[""client""] = client_name\n            s = json.dumps(action,sort_keys=True)\n            outfile.write(s+""\\n"")\n            count += 1\n            if count % 50000 == 0:\n                print ""Processed ""+str(count)+"" actions""\n\n'"
python/seldon/shell/import_items_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\nITEM_MAP_VARCHAR_INSERT = ""insert into item_map_varchar (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DOUBLE_INSERT = ""insert into item_map_double (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_DATETIME_INSERT = ""insert into item_map_datetime (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_ENUM_INSERT = ""insert into item_map_enum (item_id, attr_id, value_id) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), (select e.value_id from ITEM_ATTR_ENUM e, item_attr a where a.name = %(attr_name)s and e.value_name = %(value)s and a.attr_id = e.attr_id) )""\nITEM_MAP_TEXT_INSERT = ""insert into item_map_text (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_INT_INSERT = ""insert into item_map_int (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\nITEM_MAP_BOOLEAN_INSERT = ""insert into item_map_boolean (item_id, attr_id, value) values ((select item_id from items where client_item_id = %(id)s),(select attr_id from item_attr where name = %(attr_name)s), %(value)s )""\n\nITEM_INSERT = ""INSERT INTO ITEMS (name, first_op, last_op, client_item_id, type) VALUES (%(name)s, NOW(), NOW(), %(id)s, 1)""\nITEM_INSERT_NO_AUTO_INCREMENT = ""INSERT INTO ITEMS (item_id, name, first_op, last_op, client_item_id, type) VALUES (%(item_id)s, %(name)s, NOW(), NOW(), %(id)s, 1)""\nDB_BATCH_SIZE = 1000\nattr_insert_map = {\n\t\'ENUM\': ITEM_MAP_ENUM_INSERT,\n\t\'BOOLEAN\': ITEM_MAP_BOOLEAN_INSERT,\n\t\'VARCHAR\': ITEM_MAP_VARCHAR_INSERT,\n\t\'TEXT\': ITEM_MAP_TEXT_INSERT,\n\t\'DATETIME\': ITEM_MAP_DATETIME_INSERT,\n\t\'INT\': ITEM_MAP_INT_INSERT,\n\t\'DOUBLE\': ITEM_MAP_DOUBLE_INSERT\n}\n\n\navailable_attrs = dict()\navailable_enums = dict()\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef retrieveDbAttrs(db):\n\tcur = db.cursor()\n\tcur.execute(""SELECT ATTR_ID, NAME, TYPE FROM ITEM_ATTR"")\n\trows = cur.fetchall()\n\tattrs = dict()\n\tfor row in rows:\n\t\tattrs[row[1]]= (row[0], row[2])\n\n\treturn attrs\n\ndef retrieveDbEnums(db):\n\tcur = db.cursor()\n\t# enum structure:\n\t#    attr_id1:\n\t#\t\t\t\tvalue_name1 : value_id1\n\t#\t\t\t\tvalue_name2 :value_id2\n\tcur.execute(""SELECT ATTR_ID, VALUE_NAME, VALUE_ID FROM ITEM_ATTR_ENUM"")\n\trows = cur.fetchall()\n\tenums = defaultdict(dict)\n\tfor row in rows:\n\t\tenums[row[0]][row[1]] = row[2]\n\n\treturn enums\n\ndef validateCSVAgainstDb(csv_file, db):\n\tglobal available_attrs, available_enums\n\tfailed = False\n\tattrs = retrieveDbAttrs(db)\n\tavailable_attrs = attrs\n\tenums = retrieveDbEnums(db)\n\tavailable_enums = enums\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tnoOfFields = 0\n\t\tfor index, line in enumerate(reader, start=1):\n\t\t\tif index is 1:\n\t\t\t\tnoOfFields = len(line)\n\t\t\t\tif not validateFieldsAgainstDbFields(set(line), attrs, db):\n\t\t\t\t\texit(1)\n\t\t\tvalidateLine(line, noOfFields, attrs, enums)\n\t\t\tif len(line) != noOfFields:\n\t\t\t\tfailLine(index, line)\n\t\t\t\tfailed = True\n\n\n\tif failed:\n\t\texit(1)\n\ndef validateLine(line, noOfFields, attrs, enums):\n\tif len(line) != noOfFields:\n\t\tfailLine(index, line)\n\t\tfailed = True\n\telse:\n\t\tfor word in line:\n\t\t\tif str(word) == \'id\':\n\t\t\t\tcontinue\n\t\t\tif str(word) == \'name\':\n\t\t\t\tcontinue\n\t\t\tvalue = line[word]\n\t\t\tif str(attrs[word][1]) == \'ENUM\':\n\t\t\t\tif value not in enums[attrs[word][0]]:\n\t\t\t\t\tprint \'couldn\\\'t find enum value\', value\n\t\t\t\t\texit(1)\n\n\ndef validateFieldsAgainstDbFields(fields,attrs,  db):\n\tfailed = False\n\tfor field in fields:\n\t\tif field not in attrs and field != \'id\' and field != \'name\':\n\t\t\tfailed = True\n\t\t\tprint \'Field \\\'\',field,\'\\\'not an attribute in the DB\'\n\n\treturn not failed\n\ndef doItemInserts(csv_file, db):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n                        inserts.append({\'name\':name,\'id\':client_id, \'item_id\':client_id})\n\t\tcur = db.cursor()\n\t\tprint ""inserting items into the db""\n\t\t###cur.executemany(ITEM_INSERT, inserts)\n\t\tcur.executemany(ITEM_INSERT_NO_AUTO_INCREMENT, inserts)\n\t\tdb.commit()\n\t\tprint \'finished item inserts\'\n\ndef doAttrInserts(csv_file, db):\n\tinserts = defaultdict(list)\n\tinsertNum = 0\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tfor line in reader:\n\t\t\tfor field_name in line:\n\t\t\t\tif field_name == \'id\' or field_name== \'name\':\n\t\t\t\t\tcontinue\n\t\t\t\tattr_type = available_attrs[str(field_name)][1]\n\t\t\t\tinserts[attr_type].append({\'attr_name\': field_name, \'value\': line[field_name], \'id\': line[\'id\']})\n\t\t\t\tif len(inserts[attr_type]) > DB_BATCH_SIZE:\n\t\t\t\t\tinsertNum+=1\n\t\t\t\t\treallyDoInserts(inserts[attr_type], attr_insert_map[attr_type], insertNum, db)\n\t\t\t\t\tdel inserts[attr_type]\n\tfor index, insert_label in enumerate(inserts, start=1):\n\t\tinsertNum+=1\n\t\treallyDoInserts(inserts[insert_label], attr_insert_map[insert_label], insertNum, db)\n\tdb.commit()\n\tprint \'finished attribute inserts\'\n\ndef reallyDoInserts(params, insertStatement, insertNum, db):\n\t\tcur = db.cursor()\n\t\tprint ""inserting attribute batch"", insertNum,\'into the db\'\n\t\tcur.executemany(insertStatement, params)\n\ndef failLine(lineNum, line):\n\tprint ""line"",lineNum,""failed as it only had"",len(line),""fields""\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table items\')\n\tdbc.execute(\'truncate table item_map_varchar\')\n\tdbc.execute(\'truncate table item_map_double\')\n\tdbc.execute(\'truncate table item_map_datetime\')\n\tdbc.execute(\'truncate table item_map_int\')\n\tdbc.execute(\'truncate table item_map_boolean\')\n\tdbc.execute(\'truncate table item_map_enum\')\n\tdbc.execute(\'truncate table item_map_text\')\n\tdb.commit()\n\ndef import_items(client_name, db_settings, data_file_fpath):\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    db.set_character_set(\'utf8\')\n    dbc = db.cursor()\n    dbc.execute(\'SET NAMES utf8;\')\n    dbc.execute(\'SET CHARACTER SET utf8;\')\n    dbc.execute(\'SET character_set_connection=utf8;\')\n# requires SUPER privs so remove\n#    dbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\n    try:\n            validateCSVAgainstDb(data_file_fpath, db)\n            doItemInserts(data_file_fpath, db)\n            doAttrInserts(data_file_fpath,db)\n    except:\n            print \'Unexpected error ...\', sys.exc_info()[0]\n            print \'Clearing DB of items and attributes\'\n            try:\n                    cleanUpDb(db)\n            except:\n                    print \'couldn\\\'t clean up db\'\n            raise\n    print ""Successfully ran all inserts""\n\n'"
python/seldon/shell/import_users_utils.py,0,"b'import time\nimport datetime\nimport sys\nimport getopt, argparse\nfrom collections import defaultdict\nimport json\nimport MySQLdb\nimport unicodecsv\nimport pprint\n\nUSER_INSERT = ""insert into users (client_user_id, username, first_op, last_op,type,num_op, active) values (%(id)s, %(name)s, now(), now(), 1,1,1)""\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef validateCSV(csv_file):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tline = reader.next()\n\t\tfor field_name in line:\n\t\t\tif not field_name == \'id\' and not field_name == \'username\':\n\t\t\t\tprint \'only id or username fields allowed\'\n\t\t\t\texit(1)\n\ndef doUserInserts(csv_file, db):\n\twith open(csv_file) as csvFile:\n\t\treader = unicodecsv.DictReader(csvFile,encoding=\'utf-8\')\n\t\tinserts = []\n\t\tinsertNum = 0\n\t\tfor line in reader:\n\t\t\tclient_id = line[\'id\']\n\t\t\tname = \'\'\n\t\t\tif \'name\' in line:\n\t\t\t\tname = line[\'name\']\n\t\t\tinserts.append({\'name\':name,\'id\':client_id})\n\t\t\tif len(inserts) > 1000:\n\t\t\t\tinsertNum+=1\n\t\t\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\t\t\tinserts = []\n\n\t\tinsertNum+=1\n\t\treallyDoInserts(USER_INSERT, inserts, insertNum, db)\n\t\tdb.commit()\n\t\tprint \'finished user inserts\'\n\ndef reallyDoInserts(insertStatement, params, num, db):\n\tcur = db.cursor()\n\tprint ""inserting user batch"", num,\'into the db\'\n\tcur.executemany(insertStatement, params)\n\n\ndef cleanUpDb(db):\n\tdbc = db.cursor()\n\tdbc.execute(\'truncate table users\')\n\ndef import_users(client_name, db_settings, data_file_fpath):\n    db = MySQLdb.connect(\n            host=db_settings[\'host\'],\n            user=db_settings[\'user\'],\n            passwd=db_settings[\'password\'],\n            db=client_name\n    )\n\n    db.set_character_set(\'utf8\')\n    dbc = db.cursor()\n    dbc.execute(\'SET NAMES utf8;\')\n    dbc.execute(\'SET CHARACTER SET utf8;\')\n    dbc.execute(\'SET character_set_connection=utf8;\')\n    dbc.execute(""SET GLOBAL max_allowed_packet=1073741824"")\n    try:\n            validateCSV(data_file_fpath)\n            doUserInserts(data_file_fpath, db)\n    except:\n            print \'Unexpected error ...\', sys.exc_info()[0]\n            print \'Clearing DB of users\'\n            try:\n                    cleanUpDb(db)\n            except:\n                    print \'couldn\\\'t clean up db\'\n            raise\n    print ""Successfully ran all inserts""\n\n'"
python/seldon/shell/seldon_utils.py,0,"b'from kazoo.client import KazooClient\nimport json, os, random, string\nimport MySQLdb\nimport sys\nimport errno\n\ndef retrieveDbSettings(data):\n\tdbs = {}\n\tfor db in data[""servers""]:\n\t\tdbs[db[""name""]] = {""host"":db[\'host\'], ""port"":db[\'port\'], ""user"":db[\'user\'], ""password"":db[\'password\']}\n\treturn dbs\n\ndef dbSetup(zk,data,zkNode):\n\tdbsProps = retrieveDbSettings(data)\n\tdbs = []\n\tfor dbName in dbsProps:\n\t\tprint ""Setting up DB \\\'""+ dbName+""\\\'""\n\t\tdb = dbsProps[dbName]\n\t\taddApiDb(dbName, db)\n\t\tjdbcString = ""jdbc:mysql:replication://HOST:PORT,HOST:PORT/?characterEncoding=utf8&useServerPrepStmts=true&logger=com.mysql.jdbc.log.StandardLogger&roundRobinLoadBalance=true&transformedBitIsBoolean=true&rewriteBatchedStatements=true""\n\t\tjdbcString = jdbcString.replace(""HOST"", db[""host""]).replace(""PORT"",str(db[""port""]));\n\t\tdel(db[""host""])\n\t\tdel(db[""port""])\n\t\tdb[\'jdbc\'] = jdbcString\n\t\tdb[\'name\'] = dbName\n\t\tdbs.append(db)\n\tdbcpObj = {""dbs"": dbs}\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,json.dumps(dbcpObj))\n\n\ndef memcachedSetup(zk, data, zkNode):\n\tservers = []\n\tprint ""Setting up memcache servers""\n\tfor server in data[""servers""]:\n\t\thost = server[\'host\']\n\t\tport = server[\'port\']\n\t\tserverStr = host+"":""+str(port)\n\t\tservers.append(serverStr)\n        server_list=str("","".join(servers))\n        zkNodeValueBuilder = {}\n        zkNodeValueBuilder[""servers""] = server_list\n        zkNodeValueBuilder[""numClients""] = 1\n        zkNodeValue = json.dumps(zkNodeValueBuilder)\n\tzk.ensure_path(zkNode)\n\tzk.set(zkNode,zkNodeValue)\n\ndef addClientDb(clientName, dbSettings, consumer_details=None):\n\n\tjs_consumer_key     = consumer_details[\'js_consumer_key\']       if consumer_details != None and consumer_details.has_key(\'js_consumer_key\')     else None\n\tall_consumer_key    = consumer_details[\'all_consumer_key\']      if consumer_details != None and consumer_details.has_key(\'all_consumer_key\')    else None\n\tall_consumer_secret = consumer_details[\'all_consumer_secret\']   if consumer_details != None and consumer_details.has_key(\'all_consumer_secret\') else None\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""dbschema/mysql/client.sql"")\n\tf = open(filename, \'r\')\n\tquery = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'""+clientName+""\\\'"")\n\tif numrows < 1:\n\t\tcur.execute(""CREATE DATABASE ""+clientName)\n\t\tcur.execute(""USE ""+clientName)\n\t\tcur.execute(query)\n\t\tmore = True\n\t\twhile more:\n\t\t\tmore = cur.nextset()\n\telse:\n\t\tprint(""Client \\\'""+clientName+""\\\' has already been added to the DB"")\n\tcur.execute(""USE API"")\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'js\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key = js_consumer_key if js_consumer_key != None else generateRandomString()\n\t\tprint ""Adding JS consumer key for client \\\'""+clientName +""\\\' : \\\'""+consumer_key+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\', \'\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'js\')"")\n\telse:\n\t\tprint ""JS Consumer key already added for client \\\'""+clientName+""\\\'""\n\tnumrows = cur.execute(""SELECT * FROM CONSUMER WHERE SHORT_NAME=\\\'""+clientName+""\\\' and SCOPE=\\\'all\\\'"")\n\tif numrows < 1:\n\t\tconsumer_key    = all_consumer_key      if all_consumer_key != None     else generateRandomString()\n\t\tconsumer_secret = all_consumer_secret   if all_consumer_secret != None  else generateRandomString()\n\t\tprint ""Adding REST API key for client \\\'""+clientName +""\\\' : consumer_key=\\\'""+consumer_key+""\\\' consumer_secret=\\\'""+consumer_secret+""\\\'""\n\t\tcur.execute(""INSERT INTO `CONSUMER` (`consumer_key`, `consumer_secret`, `name`, `short_name`, `time`, `active`, `secure`, `scope`) VALUES (\\\'""+consumer_key+""\\\',\\\'""+consumer_secret+""\\\',\\\'""+clientName+""\\\',\\\'""+ clientName+""\\\',CURRENT_TIMESTAMP(), 1, 0, \'all\')"")\n\telse:\n\t\tprint ""REST API key already added for client \\\'""+clientName+""\\\'""\n\ndef addApiDb(dbName, dbSettings):\n\n\tdb = MySQLdb.connect(host=dbSettings[""host""],\n                     \tuser=dbSettings[""user""],\n                      passwd=dbSettings[""password""])\n\tcur = db.cursor()\n\tdir = os.path.dirname(os.path.abspath(__file__))\n\tfilename = os.path.join(dir, ""dbschema/mysql/api.sql"")\n\tf = open(filename, \'r\')\n\tquery = "" "".join(f.readlines())\n\tnumrows = cur.execute(""SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = \\\'api\\\'"")\n\tif numrows < 1:\n\t\tprint ""Adding api DB to MySQL DB \\\'""+dbName+""\\\'""\n\t\tcur.execute(query)\n\telse:\n\t\tprint ""API DB has already been added to the MySQL DB \\\'""+dbName+""\\\'""\n\ndef clientSetup(zk, client_data, db_data, zkNode, consumer_details=None):\n\tdbs= retrieveDbSettings(db_data)\n\tfor client in client_data:\n\n\t\tprint ""Adding client \\\'""+client[\'name\']+""\\\'""\n\t\tdbname = client[\'db\']\n\t\tif client[\'db\'] is None:\n\t\t\tdbname = dbs.keys()[0]\n\t\taddClientDb(client[\'name\'],dbs[dbname], consumer_details)\n\t\tclientNode = zkNode + ""/"" + client[\'name\']\n\t\tzk.ensure_path(clientNode)\n\t\tclientNodeValue = {""DB_JNDI_NAME"":dbname}\n\t\tzk.set(clientNode,json.dumps(clientNodeValue))\n\t\tfor setting in client:\n\t\t\tif setting != ""name"" and setting != ""db"":\n\t\t\t\tzk.ensure_path(clientNode + ""/"" + setting)\n\t\t\t\tzk.set(clientNode + ""/"" + setting, str(client[setting]))\n\ndef generateRandomString():\n\treturn \'\'.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(20))\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef json_to_dict(json_data):\n    retVal = None\n    try:\n      retVal = json.loads(json_data)\n    except ValueError:\n        pass\n    return retVal\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef get_editor():\n    return os.environ[""EDITOR""] if os.environ.has_key(""EDITOR"") else ""vim""\n\n'"
python/seldon/shell/shell_main.py,0,"b'from cmd2 import Cmd, make_option, options\nimport os,sys\nimport argparse\nfrom kazoo.client import KazooClient\nimport json\nimport os.path\nimport errno\n\nimport cmd_client\nimport cmd_attr\nimport cmd_model\nimport cmd_alg\nimport cmd_db\nimport cmd_memcached\nimport cmd_import\nimport pprint\n\ngdata = {\n    ""zk_client"": None,\n    ""conf_data"": None,\n    ""conf_path"": os.path.expanduser(""~/.seldon/seldon.conf""),\n    \'help_cmd_strs_list\' : [\n        (""attr"", ""configure attributes for client""),\n        (""alg"", ""configure algs for client""),\n        (""client"", ""configure client""),\n        (""model"", ""configure model for client""),\n        (""db"", ""configure databases""),\n        (""memcached"", ""configure memcached""),\n        (""import"", ""static import of data"")\n    ],\n    \'help_formatting\' : { \'lmargin_size\' : 4, \'cmd_size\' : 40 },\n}\n\ndef pp(o):\n    p = pprint.PrettyPrinter(indent=4)\n    p.pprint(o)\n\ndef get_seldon_banner():\n    return \'\'\'\\\n   _____      __    __\n  / ___/___  / /___/ /___  ____\n  \\__ \\/ _ \\/ / __  / __ \\/ __ \\\\\n ___/ /  __/ / /_/ / /_/ / / / /\n/____/\\___/_/\\__,_/\\____/_/ /_/ \\\n\'\'\'\n\ndef get_default_conf():\n    return \'\'\'\\\n{\n    ""default_algorithms"": {\n        ""assocRuleRecommender"": {\n            ""config"": []\n        },\n        ""dynamicClusterCountsRecommender"": {\n            ""config"": []\n        },\n        ""externalItemRecommendationAlgorithm"": {\n            ""config"": [\n                {\n                    ""name"": ""io.seldon.algorithm.inclusion.itemsperincluder"",\n                    ""value"": 100000\n                },\n                {\n                    ""name"": ""io.seldon.algorithm.external.url"",\n                    ""value"": ""http://127.0.0.1:5000/recommend""\n                },\n                {\n                    ""name"": ""io.seldon.algorithm.external.name"",\n                    ""value"": ""example_alg""\n                }\n            ],\n            ""includers"": [\n                ""recentItemsIncluder""\n            ]\n        },\n        ""globalClusterCountsRecommender"": {\n            ""config"": []\n        },\n        ""itemCategoryClusterCountsRecommender"": {\n            ""config"": []\n        },\n        ""itemClusterCountsRecommender"": {\n            ""config"": []\n        },\n        ""itemSimilarityRecommender"": {\n            ""config"": []\n        },\n        ""mfRecommender"": {\n            ""config"": []\n        },\n        ""mostPopularRecommender"": {\n            ""config"": []\n        },\n        ""recentItemsRecommender"": {\n            ""config"": []\n        },\n        ""recentMfRecommender"": {\n            ""config"": [\n                {\n                    ""name"": ""io.seldon.algorithm.general.numrecentactionstouse"",\n                    ""value"": ""1""\n                }\n            ]\n        },\n        ""semanticVectorsRecommender"": {\n            ""config"": []\n        },\n        ""userTagAffinityRecommender"": {\n            ""config"": []\n        }\n    },\n    ""default_models"": {\n        ""cluster-by-dimension"": {},\n        ""matrix-factorization"": {\n            ""config"": {\n                ""activate"": true,\n                ""alpha"": 1,\n                ""days"": 1,\n                ""inputPath"": ""%SELDON_MODELS%"",\n                ""iterations"": 5,\n                ""lambda"": 0.01,\n                ""outputPath"": ""%SELDON_MODELS%"",\n                ""rank"": 30,\n                ""startDay"": 1\n            },\n            ""training"": {\n                ""job_info"": {\n                    ""cmd"": ""%SPARK_HOME%/bin/spark-submit"",\n                    ""cmd_args"": [\n                        ""--class"",\n                        ""io.seldon.spark.mllib.MfModelCreation"",\n                        ""--master"",\n                        ""local[1]"",\n                        ""%SELDON_SPARK_HOME%/target/seldon-spark-%SELDON_VERSION%-jar-with-dependencies.jar"",\n                        ""--client"",\n                        ""%CLIENT_NAME%"",\n                        ""--zookeeper"",\n                        ""%ZK_HOSTS%""\n                    ]\n                },\n                ""job_type"": ""spark""\n            }\n        },\n        ""semvec"": {},\n        ""similar-items"": {},\n        ""tagaffinity"": {},\n        ""tagcluster"": {}\n    },\n    ""seldon_models"": ""~/.seldon/seldon-models"",\n    ""seldon_spark_home"": ""~/seldon-server/offline-jobs/spark"",\n    ""seldon_version"": ""0.93"",\n    ""spark_home"": ""~/apps/spark"",\n    ""zk_hosts"": ""localhost:2181"",\n    ""zkroot"": ""~/.seldon/zkroot""\n}\n\'\'\'\n\ndef completions_helper(help_cmd_strs_list, text, line):\n    completions = [x[0].partition(\' \')[2].partition(\' \')[0] for x in help_cmd_strs_list if len(x[0].split(\' \'))>1]\n    mline = line.partition(\' \')[2]\n    offs = len(mline) - len(text)\n    return [s[offs:] for s in completions if s.startswith(mline)]\n\ndef json_to_dict(json_data):\n    return json.loads(json_data)\n\ndef dict_to_json(d, expand=False):\n    return json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \')) if expand else json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc: # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else: raise\n\ndef getOpts():\n    parser = argparse.ArgumentParser(description=\'Seldon Shell\')\n    parser.add_argument(\'-d\', ""--debug"", action=\'store_true\', help=""turn on debugging"")\n    parser.add_argument(\'-q\', ""--quiet"", action=\'store_true\', help=""only display important messages, useful in non-interactive mode"")\n    parser.add_argument(\'--zk-hosts\', help=""the zookeeper hosts"", required=False)\n    parser.add_argument(\'--version\', action=\'store_true\', help=""print the version"", required=False)\n    parser.add_argument(\'args\', nargs=argparse.REMAINDER) # catch rest (non-options) as args\n    opts = parser.parse_args()\n    return opts\n\nclass CmdLineApp(Cmd):\n\n    def do_ping(self, arg, opts=None):\n        print ""pong""\n\n    def do_conf(self, arg, opts=None):\n        pp(gdata[""conf_data""])\n\n    def do_gdata(self, arg, opts=None):\n        pp(gdata)\n\n    def do_db(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_db.cmd_db(arg, command_data)\n\n    def complete_db(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_db.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_memcached(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_memcached.cmd_memcached(arg, command_data)\n\n    def complete_memcached(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_memcached.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_client(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_client.cmd_client(arg, command_data)\n\n    def complete_client(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_client.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_alg(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'default_algorithms\' : gdata[\'conf_data\'][\'default_algorithms\'],\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_alg.cmd_alg(arg, command_data)\n\n    def complete_alg(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_alg.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_attr(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_attr.cmd_attr(arg, command_data)\n\n    def complete_attr(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_attr.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_model(self, arg, opts=None):\n        command_data = {\n                \'zkdetails\' : {\'zkroot\': gdata[\'conf_data\'][\'zkroot\'], \'zk_client\': gdata[\'zk_client\']},\n                \'default_models\' : gdata[\'conf_data\'][\'default_models\'],\n                \'seldon_models\' : gdata[\'conf_data\'][\'seldon_models\'],\n                \'conf_data\' : gdata[\'conf_data\'],\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_model.cmd_model(arg, command_data)\n\n    def complete_model(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_model.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_import(self, arg, opts=None):\n        command_data = {\n                \'conf_data\' : gdata[\'conf_data\'],\n                \'help_formatting\' : gdata[""help_formatting""],\n        }\n        cmd_import.cmd_import(arg, command_data)\n\n    def complete_import(self, text, line, start_index, end_index):\n        help_cmd_strs_list = cmd_import.gdata[""help_cmd_strs_list""]\n        return completions_helper(help_cmd_strs_list, text, line)\n\n    def do_help(self, arg, opts=None):\n        lmargin_size=4\n        cmd_size=40\n        lmargin_pad="" ""\n        for help_strs in gdata[""help_cmd_strs_list""]:\n            cmd_str = help_strs[0]\n            cmd_help = help_strs[1]\n            print ""{lmargin_pad:<{lmargin_size}}{cmd_str:<{cmd_size}} - {cmd_help}"".format(**locals())\n\n    def do_test(self, arg, opts=None):\n        conf_data = gdata[\'conf_data\']\n        print dict_to_json(conf_data, False)\n\n    do_m = do_memcached # an alias\n    do_c = do_client # an alias\n    do_a = do_alg # an alias\n    do_t = do_test # an alias\n\ndef expand_conf():\n    conf_data = gdata[\'conf_data\']\n\n    expansion_list = [""zkroot"",""seldon_models"",""spark_home"",""seldon_spark_home""]\n    for expansion_item in expansion_list:\n        conf_data[expansion_item] = os.path.expanduser(conf_data[expansion_item])\n\ndef check_conf():\n    def create_default_conf(fpath):\n        default_conf = get_default_conf()\n        mkdir_p(os.path.dirname(fpath))\n        d = json_to_dict(default_conf)\n        zkroot = os.path.expanduser(d[""zkroot""])\n        mkdir_p(zkroot)\n        seldon_models = os.path.expanduser(d[""seldon_models""])\n        mkdir_p(seldon_models)\n        default_conf = json.dumps(d, sort_keys=True, indent=4, separators=(\',\', \': \'))\n        f = open(fpath, \'w\')\n        f.write(default_conf)\n        f.write(\'\\n\')\n        f.close()\n        print ""Created conf file [{fpath}] with default settings."".format(**locals())\n        print ""Edit this file and re-run.""\n        sys.exit(0)\n    fpath = gdata[\'conf_path\']\n    if not os.path.isfile(fpath):\n        create_default_conf(fpath)\n    else:\n        with open(fpath) as data_file:\n            gdata[""conf_data""] = json.load(data_file)\n\ndef start_zk_client(opts):\n    zk_hosts = opts.zk_hosts\n    if not opts.quiet:\n        sys.stdout.write(""connecting to ""+zk_hosts)\n    gdata[""zk_client""] = KazooClient(hosts=zk_hosts)\n    gdata[""zk_client""].start()\n    res = ""SUCCEEDED"" if gdata[""zk_client""].connected else ""FAILED""\n    if not opts.quiet:\n        print "" [{res}]"".format(**locals())\n\ndef stop_zk_client():\n    if gdata[""zk_client""].connected:\n        gdata[""zk_client""].stop()\n\ndef main():\n    check_conf()\n    expand_conf()\n    opts = getOpts()\n    if opts.version == True:\n        from seldon import __version__\n        print __version__\n        sys.exit(0)\n\n    if not opts.quiet:\n        print get_seldon_banner(); print """"\n\n    if opts.zk_hosts != None:\n        gdata[""conf_data""][""zk_hosts""] = opts.zk_hosts\n    else:\n        opts.zk_hosts = gdata[""conf_data""][""zk_hosts""]\n\n    if opts.debug:\n        pp(opts)\n        pp(gdata)\n\n    start_zk_client(opts)\n    Cmd.prompt = ""seldon> "" if not opts.quiet else """"\n    sys.argv = [sys.argv[0]] # the --zk-hosts argument upsets app\n    app = CmdLineApp()\n    app.cmdloop()\n    stop_zk_client()\n\nif __name__ == \'__main__\':\n    main()\n\n'"
python/seldon/shell/zk_utils.py,0,"b'import json\n\ndef is_json_data(data):\n    if (data != None) and (len(data)>0):\n        return data[0] == \'{\' or data[0] == \'[\'\n    else:\n        return False\n\ndef json_compress(json_data):\n    d = json.loads(json_data)\n    return json.dumps(d, sort_keys=True, separators=(\',\',\':\'))\n\ndef node_set(zk_client, node_path, node_value):\n    if is_json_data(node_value):\n        node_value = json_compress(node_value)\n    node_value = node_value.strip() if node_value != None else node_value\n\n    if zk_client.exists(node_path):\n        retVal = zk_client.set(node_path,node_value)\n    else:\n        retVal = zk_client.create(node_path,node_value,makepath=True)\n    print ""updated zk node[{node_path}]"".format(node_path=node_path)\n\ndef node_get(zk_client, node_path):\n    theValue = None\n    if zk_client.exists(node_path):\n        theValue = zk_client.get(node_path)\n        theValue = theValue[0]\n    return theValue.strip() if theValue != None else theValue\n\n'"
python/seldon/tests/__init__.py,0,b''
python/seldon/tests/test_keras.py,0,"b'from __future__ import absolute_import\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.normalization import BatchNormalization\nimport unittest\nimport pandas as pd\nfrom seldon import keras\nimport numpy as np\nimport sys\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport logging\n\n\nclass Test_KerasClassifier(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        t = keras.KerasClassifier(target=""target"",verbose=1)\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        estimators = [(""keras"",t)]\n        p = Pipeline(estimators)\n        p.fit(df)\n        preds = p.predict_proba(df)\n        print preds\n        print ""-------------------""\n        joblib.dump(p,""/tmp/p"")\n        p2 = joblib.load(""/tmp/p"")\n        df3 = p2.predict_proba(df)\n        print ""df3""\n        print df3\n\n\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n'"
python/seldon/tests/test_sklearn.py,0,"b'import unittest\nimport pandas as pd\nfrom .. import sklearn_estimator as ske\nimport numpy as np\nimport sys\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nclass Test_SKLearnClassifier(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        t = ske.SKLearnClassifier(clf=LogisticRegression(),target=""target"")\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        df = pd.DataFrame.from_dict(fs)\n        estimators = [(""lr"",t)]\n        p = Pipeline(estimators)\n        p.fit(df)\n        preds = p.predict_proba(df)\n        preds2 = p.predict(df)\n        print preds2\n        print preds\n        print ""-------------------""\n        joblib.dump(p,""/tmp/p"")\n        p2 = joblib.load(""/tmp/p"")\n        df3 = p2.predict_proba(df)\n        print ""df3""\n        print df3\n\n\n\n        \nif __name__ == \'__main__\':\n    unittest.main()\n'"
python/seldon/tests/test_util.py,0,"b'import unittest\nfrom seldon import Recommender,RecommenderWrapper\nimport sys\nimport logging\n\nclass SimpleRecommender(Recommender):\n    \n    def __init__(self):\n        self.res = [(1,0.8),(2,0.7)]\n    \n    def recommend(self,user,ids,recent_interactions,client,limit):\n        return self.res\n\n    \nclass Test_RecommenderWrapper(unittest.TestCase):\n\n    def test_save_load(self):\n        sr1 = SimpleRecommender()\n        res1 = sr1.recommend(1,None,None,""test"",2)\n        self.assertEqual(len(res1),2)\n        \n        rr = RecommenderWrapper()\n        rr.save_recommender(sr1,""/tmp/simplerec"")\n\n        sr2 = rr.load_recommender(""/tmp/simplerec"")\n        res2 = sr2.recommend(1,None,None,""test"",2)\n        self.assertEqual(len(res2),2)\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/tests/test_vw.py,0,"b'import unittest\nimport pandas as pd\nfrom seldon import vw\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\nclass Test_VWClassifier(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        t = vw.VWClassifier(target=""target"")\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        estimators = [(""vw"",t)]\n        p = Pipeline(estimators)\n        print ""fitting""\n        p.fit(df)\n        print ""get preds 1 ""\n        preds = p.predict_proba(df)\n        print preds\n        print ""-------------------""\n        t.close()\n        # vw sometimes need some more time between invocations. Need to look into this.\n        import time\n        time.sleep(5)\n        joblib.dump(p,""/tmp/pipeline/p"")\n        p2 = joblib.load(""/tmp/pipeline/p"")\n        print ""get preds 2""\n        df3 = p2.predict_proba(df)\n        print df3\n        vw2 = p2._final_estimator\n        vw2.close()\n\n\n    def test_zero_based_target(self):\n        t = vw.VWClassifier(target=""target"",target_readable=""name"")\n        try:\n            df = pd.DataFrame.from_dict([{""target"":0,""b"":""c d"",""c"":3,""name"":""zeroTarget""},{""target"":1,""b"":""word2"",""name"":""oneTarget""}])\n            t.fit(df)\n            scores = t.predict_proba(df)\n            print ""scores->"",scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertEquals(scores.shape[1],2)\n            idMap = t.get_class_id_map()\n            print idMap\n            formatted_recs_list=[]\n            for index, proba in enumerate(scores[0]):\n                print index,proba\n                if index in idMap:\n                    indexName = idMap[index]\n                else:\n                    indexName = str(index)\n                formatted_recs_list.append({\n                        ""prediction"": str(proba),\n                        ""predictedClass"": indexName,\n                        ""confidence"" : str(proba)\n        })\n            print formatted_recs_list\n        finally:\n            t.close()\n\n\n    def test_create_features(self):\n        t = vw.VWClassifier(target=""target"")\n        try:\n            df = pd.DataFrame.from_dict([{""target"":""1"",""b"":""c d"",""c"":3},{""target"":""2"",""b"":""word2""}])\n            t.fit(df)\n            scores = t.predict_proba(df)\n            print scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertEquals(scores.shape[1],2)\n        finally:\n            t.close()\n\n    def test_predictions(self):\n        t = vw.VWClassifier(target=""target"")\n        try:\n            df = pd.DataFrame.from_dict([{""target"":""1"",""b"":""c d"",""c"":3},{""target"":""2"",""b"":""word2""}])\n            t.fit(df)\n            preds = t.predict(df)\n            print preds\n            self.assertEquals(preds[0],0)\n            self.assertEquals(preds[1],1)\n        finally:\n            t.close()\n\n\n    def test_dict_feature(self):\n        t = vw.VWClassifier(target=""target"")\n        try:\n            df = pd.DataFrame.from_dict([{""target"":""1"",""df"":{""1"":0.234,""2"":0.1}},{""target"":""2"",""df"":{""1"":0.5}}])\n            t.fit(df)\n            scores = t.predict_proba(df)\n            print scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertEquals(scores.shape[1],2)\n        finally:\n            t.close()\n\n    def test_list_feature(self):\n        t = vw.VWClassifier(target=""target"",num_iterations=10)\n        try:\n            df = pd.DataFrame.from_dict([{""target"":""1"",""df"":[""a"",""b"",""c"",""d""]},{""target"":""2"",""df"":[""x"",""y"",""z""]}])\n            t.fit(df)\n            df2 = pd.DataFrame.from_dict([{""df"":[""a"",""b"",""c"",""d""]},{""df"":[""x"",""y"",""z""]}])\n            scores = t.predict_proba(df2)\n            if not scores is  None:\n                print scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertTrue(scores[0][0]>scores[0][1])\n            self.assertEquals(scores.shape[1],2)\n            self.assertTrue(scores[1][0]<scores[1][1])\n        finally:\n            t.close()\n\n\n    def test_vw_same_score_bug(self):\n        t = vw.VWClassifier(target=""target"",num_iterations=10)\n        try:\n            df = pd.DataFrame.from_dict([{""target"":""1"",""df"":[""a"",""b"",""c"",""d""]},{""target"":""2"",""df"":[""x"",""y"",""z""]}])\n            t.fit(df)\n            df2 = pd.DataFrame.from_dict([{""df"":[""a"",""b"",""c"",""d""]},{""df"":[""x"",""y"",""z""]}])\n            scores = t.predict_proba(df2)\n            score_00 = scores[0][0]\n            score_10 = scores[1][0]\n            for i in range(1,4):\n                scores = t.predict_proba(df2)\n                self.assertEquals(scores[0][0],score_00)\n                self.assertEquals(scores[1][0],score_10)\n        finally:\n            t.close()\n\n    def test_large_number_features(self):\n        t = vw.VWClassifier(target=""target"")\n        try:\n            f = {}\n            f2 = {}\n            for i in range(1,5000):\n                f[str(i)] = 1\n                f2[str(i)] = 0.1\n            df = pd.DataFrame.from_dict([{""target"":""1"",""df"":f},{""target"":""2"",""df"":f2}])\n            t.fit(df)\n            scores = t.predict_proba(df)\n            print scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertEquals(scores.shape[1],2)\n        finally:\n            t.close()\n\n    def test_vw_args(self):\n        t = vw.VWClassifier(target=""target"",b=18)\n        try:\n            f = {}\n            f2 = {}\n            for i in range(1,5000):\n                f[str(i)] = 1\n                f2[str(i)] = 0.1\n            df = pd.DataFrame.from_dict([{""target"":""1"",""df"":f},{""target"":""2"",""df"":f2}])\n            t.fit(df)\n            scores = t.predict_proba(df)\n            print scores\n            self.assertEquals(scores.shape[0],2)\n            self.assertEquals(scores.shape[1],2)\n        finally:\n            t.close()\n\n\n    def test_numpy_input(self):\n        t = vw.VWClassifier()\n        try:\n            X = np.random.randn(6,4)\n            y = np.array([1,2,1,1,2,2])\n            t.fit(X,y)\n            scores = t.predict_proba(X)\n            print scores\n        finally:\n            t.close()\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/tests/test_xgb.py,0,"b'import unittest\nimport pandas as pd\nfrom seldon import xgb \nimport numpy as np\nimport sys\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\nclass Test_XGBoostClassifier(unittest.TestCase):\n\n    def test_set_params(self):\n        t = xgb.XGBoostClassifier(target=""target"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        t.set_params(learning_rate=0.9,gamma=0.1)\n        self.assertEquals(t.get_params()[\'learning_rate\'],0.9)\n\n    def test_sklearn_pipeline(self):\n        t = xgb.XGBoostClassifier(target=""target"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        estimators = [(""xgb"",t)]\n        p = Pipeline(estimators)\n        p.fit(df)\n        preds = p.predict_proba(df)\n        print preds\n        print ""-------------------""\n        joblib.dump(p,""/tmp/pipeline/p"")\n        p2 = joblib.load(""/tmp/pipeline/p"")\n        df3 = p2.predict_proba(df)\n        print df3\n\n    def test_create_features(self):\n        t = xgb.XGBoostClassifier(target=""target"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        t.fit(df)\n        scores = t.predict_proba(df)\n        print scores.shape\n        print ""scores->"",scores[0]\n        preds = t.predict(df)\n        print ""predictions->"",preds[0],preds[1]\n        self.assertEquals(preds[0],0)\n        self.assertEquals(preds[1],1)\n\n\n    def test_numpy_input(self):\n        t = xgb.XGBoostClassifier(n_estimators=10,learning_rate=0.1,silent=0)\n        X = np.random.randn(6,4)\n        y = np.array([0,1,1,0,0,1])\n        t.fit(X,y)\n        scores = t.predict_proba(X)\n        print scores\n\n\n    def test_svmlight_features(self):\n        t = xgb.XGBoostClassifier(target=""target"",svmlight_feature=""svm"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        df = pd.DataFrame([{""svm"":[(1,2.0),(2,3.0)],""target"":1}])\n        t.fit(df)\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/text/__init__.py,0,"b'from .docsim import DocumentSimilarity,DefaultJsonCorpus\nfrom .tagrecommend import TagRecommender\n\n\n'"
python/seldon/text/docsim.py,0,"b'from gensim.models.doc2vec import LabeledSentence\nimport gensim\nfrom seldon import Recommender\nfrom gensim.corpora.textcorpus import TextCorpus\nfrom gensim import interfaces, utils\nfrom six import string_types\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim import corpora, models, similarities\nimport annoy\nimport json\nimport codecs \nfrom nltk.corpus import stopwords\nimport logging\nimport scipy\nimport time\nimport glob\nimport shutil\nimport operator\nfrom collections import defaultdict\nimport copy\n\nlogger = logging.getLogger(__name__)\n\ndef jaccard(s1,s2):\n    return len(s1.intersection(s2))/float(len(s1.union(s2)))\n\ncurrent_milli_time = lambda: int(round(time.time() * 1000))\n\nclass DefaultJsonCorpus(object):\n    """"""\n    A default JSON corpus based on gensim TextCorpus. It assumes a file or list of JSON as input.\n    The methods provided by gensim TextCorpus are needed for the GenSim training.\n    Any corpus provided to DocumentSimilarity should provide the methods given in this class.\n    """"""\n    def __init__(self, input=None,create_dictionary=True):\n        super(DefaultJsonCorpus, self).__init__()\n        self.input = input\n        self.dictionary = Dictionary()\n        self.metadata = False\n        if create_dictionary:\n            self.dictionary.add_documents(self.get_texts())\n\n\n    def __iter__(self):\n        for text in self.get_texts():\n            yield self.dictionary.doc2bow(text, allow_update=False)\n\n    def getstream(self):\n        return utils.file_or_filename(self.input)\n\n    def __len__(self):\n        if not hasattr(self, \'length\'):\n            # cache the corpus length\n            self.length = sum(1 for _ in self.get_texts())\n        return self.length\n\n    def get_json(self):\n        if isinstance(self.input,list):\n            for j in self.input:\n                yield j\n        else:\n            with self.getstream() as lines:\n                for line in lines:\n                    line = line.rstrip()\n                    j = json.loads(line)\n                    yield j\n\n    def get_texts(self,raw=False):\n        """"""\n        yield raw text or tokenized text\n        """"""\n        for j in self.get_json():\n            text = j[""text""]\n            if raw:\n                yield text\n            else:\n                yield utils.tokenize(text, deacc=True, lowercase=True)\n\n    def get_meta(self):\n        """"""\n        return a json object with meta data for the documents. It must return:\n        id - id for this document\n        optional title and tags. Tags will be used as base truth used to score document similarity results.\n        """"""\n        doc_id = 0\n        for j in self.get_json():\n            m = copy.deepcopy(j)\n            m[\'id\'] = long(m[\'id\'])\n            m[\'corpus_seq_id\'] = doc_id\n            doc_id += 1\n            yield m\n\n    def get_dictionary(self):\n        return self.dictionary\n\n\n\nclass DocumentSimilarity(Recommender):\n\n    """"""\n\n    Parameters\n    ----------\n\n    model_type : string\n       gensim_lsi,gensim_lda,gensim_rp,sklearn_nmf\n    vec_size : int\n       vector size of model\n    annoy_trees : int\n       number of trees to create for Annoy approx nearest neighbour\n    work_folder : str\n       folder for tmp files\n    sklearn_tfidf_args : dict, optional\n       args to pass to sklearn TfidfVectorizer\n    sklear_nmf_args : dict, optional\n       args to pass to sklearn NMF model\n    """"""\n    def __init__(self,model_type=\'gensim_lsi\',vec_size=100,annoy_trees=100,work_folder=""/tmp"",sklearn_tfidf_args={\'stop_words\':""english""},sklearn_nmf_args={""random_state"":1,""alpha"":.1,""l1_ratio"":.5}):\n        if not (model_type == \'gensim_lsi\' or model_type == \'gensim_lda\' or model_type == \'gensim_rp\' or model_type == \'sklearn_nmf\'):\n            raise ValueError(""Unknown model type"")\n        self.model_type=model_type\n        self.vec_size=vec_size\n        self.annoy_trees=annoy_trees\n        self.gensim_output_prefix = ""gensim_index""\n        self.annoy_output_prefix = ""annoy_index""\n        self.meta_output_prefix = ""meta""\n        self.sklearn_tfidf_args = sklearn_tfidf_args\n        self.sklearn_nmf_args = sklearn_nmf_args\n        self.work_folder=work_folder\n\n    def __getstate__(self):\n        """"""\n        Remove things that should not be pickled as they are handled in save/load\n        """"""\n        result = self.__dict__.copy()\n        del result[\'index\']\n        del result[\'index_annoy\']\n        del result[\'seq2meta\']\n        del result[\'id2meta\']\n        return result\n\n    def __setstate__(self, dict):\n        self.__dict__ = dict\n        \n\n\n    def create_gensim_model(self,corpus):\n        """"""\n        Create a gensim model\n\n        Parameters\n        ----------\n\n        corpus : an object that satisfies a gensim TextCorpus\n\n        Returns\n        -------\n        \n        gensim corpus model\n        """"""\n        dictionary = corpus.get_dictionary()\n        tfidf = models.TfidfModel(corpus)\n        corpus_tfidf = tfidf[corpus]\n        if self.model_type==\'gensim_lsi\':\n            logger.info(""Building gensim lsi model"")\n            model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=self.vec_size)\n        elif self.model_type==\'gensim_lda\':\n            logger.info(""Building gensim lda model"")\n            model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=self.vec_size,passes=10)\n        else:\n            logger.info(""Building gensim random projection model"")\n            model = models.RpModel(corpus_tfidf, id2word=dictionary, num_topics=self.vec_size)\n        return model[corpus_tfidf]\n\n    def create_sklearn_model(self,corpus):\n        """"""\n        Create a sklearn model\n\n        Parameters\n        ----------\n\n        corpus : object \n           a corpus object that has get_text(raw=True) method\n\n        Returns\n        -------\n        \n        gensim corpus model\n        """"""\n        from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n        from sklearn.decomposition import NMF\n        #make tfidf and NMF args configurable\n        tfidf_vectorizer = TfidfVectorizer(**self.sklearn_tfidf_args)\n        tfidf = tfidf_vectorizer.fit_transform(corpus.get_texts(raw=True))\n        logger.info(""Building sklearn NMF model"")\n        W = NMF(n_components=self.vec_size, **self.sklearn_nmf_args).fit_transform(tfidf)\n        return gensim.matutils.Dense2Corpus(W.T)\n\n\n    def fit(self,corpus):\n        """"""\n        Fit a document similarity model\n\n        Parameters\n        ----------\n\n        corpus : object\n           a corpus object that follows DefaultJsonCorpus\n\n        Returns\n        -------\n        \n        trained DocumentSimilarity object\n        """"""\n        if self.model_type == \'sklearn_nmf\':\n            model = self.create_sklearn_model(corpus)\n        else:\n            model = self.create_gensim_model(corpus)\n\n        self.index = similarities.Similarity(self.work_folder+""/gensim_index"",model,self.vec_size)\n        self.index_annoy = annoy.AnnoyIndex(self.vec_size, metric=\'angular\')\n        for i, vec in enumerate(model):\n            self.index_annoy.add_item(i, list(gensim.matutils.sparse2full(vec, self.vec_size).astype(float)))\n        self.index_annoy.build(self.annoy_trees)\n        self.seq2meta = {}\n        self.id2meta = {}\n        for j in corpus.get_meta():\n            self.seq2meta[j[\'corpus_seq_id\']] = j\n            self.id2meta[j[\'id\']] = j\n        return self\n\n    def save(self,folder):\n        """"""\n        save models to folder\n\n        Parameters\n        ----------\n\n        folder : str\n           saved location folder\n        """"""\n        self.index.close_shard()\n        for f in glob.glob(self.work_folder+""/gensim_index*""):\n            shutil.move(f, folder)\n        self.index.output_prefix=folder+""/gensim_index""\n        self.index.check_moved()\n        self.index.save(folder+""/""+self.gensim_output_prefix)\n        self.index_annoy.save(folder+""/""+self.annoy_output_prefix)\n        fout = codecs.open(folder+""/""+self.meta_output_prefix, ""w"", ""utf-8"")\n        for k in self.seq2meta:\n            jStr = json.dumps(self.seq2meta[k],sort_keys=True)\n            fout.write(jStr+""\\n"")\n        fout.close()\n\n    def load(self,folder):\n        """"""\n        load models from folder\n\n        Parameters\n        ----------\n\n        folder : str\n           location of models\n        """"""\n        self.index =  similarities.Similarity.load(folder+""/""+self.gensim_output_prefix)\n        self.index.output_prefix=folder+""/gensim_index""\n        self.index.check_moved()\n        self.index_annoy = annoy.AnnoyIndex(self.vec_size)\n        self.index_annoy.load(folder+""/""+self.annoy_output_prefix)\n        self.seq2meta = {}\n        self.id2meta = {}\n        with open(folder+""/""+self.meta_output_prefix) as f:\n            for line in f:\n                line = line.rstrip()\n                j = json.loads(line)\n                self.seq2meta[j[\'corpus_seq_id\']] = j\n                self.id2meta[j[\'id\']] = j\n\n    def _remove_query_doc(self,query_id,results):\n        transformed = []\n        for (doc_id,score) in results:\n            if not doc_id == query_id:\n                transformed.append((doc_id,score))\n        return transformed\n\n    def recommend(self,user=None,ids=[],recent_interactions=[],client=None,limit=1):\n        if ids is None or len(ids) == 0:\n            scores = defaultdict(float)\n            for doc_id in recent_interactions:\n                doc_scores = self.nn(doc_id,k=limit*10,translate_id=True,approx=True)\n                for (doc_id,score) in doc_scores:\n                    scores[doc_id] += score\n            sorted_x = sorted(scores.items(), key=operator.itemgetter(1))\n            sorted_x = sorted_x[::-1]\n            return sorted_x[:limit]\n        else:\n            return []\n\n    def nn(self,doc_id,k=1,translate_id=False,approx=False):\n        """"""\n        nearest neighbour query\n\n        Parameters\n        ----------\n\n        doc_id : long\n           internal or external document id\n        k : int\n           number of neighbours to return\n        translate_id : bool\n           translate doc_id into internal id\n        approx : bool\n           run approx nearest neighbour search using Annoy\n\n        Returns\n        -------\n\n        list of pairs of document id (internal or external) and similarity metric in range (0,1)\n        """"""        \n        if translate_id:\n            doc_id_internal = self.id2meta[doc_id][\'corpus_seq_id\']\n        else:\n            doc_id_internal = doc_id\n        k += 1\n        self.index.num_best = k\n        if approx:\n            v = self.index.vector_by_id(doc_id_internal)\n            if scipy.sparse.issparse(v):\n                v_list = []\n                for j,v in zip(v.indices, v.data):\n                    v_list.append((j,v))\n                v = gensim.matutils.sparse2full(v_list, self.vec_size)\n            (ids,scores) = self.index_annoy.get_nns_by_vector(v, k, search_k=-1, include_distances=True)\n            scores = [1.0-score for score in scores]\n            sims = zip(ids,scores)\n        else:\n            sims =  list(self.index.similarity_by_id(doc_id_internal))\n        if translate_id:\n            res = []\n            for (sim_id,score) in sims:\n                if not sim_id == doc_id_internal:\n                    res.append((self.seq2meta[sim_id][\'id\'],score))\n            return res\n        else:\n            return self._remove_query_doc(doc_id_internal,sims)\n            \n    def get_meta(self,doc_id):\n        return self.id2meta[doc_id]\n\n\n    def score(self,k=1,approx=False):\n        """"""\n        score a model\n\n        Parameters\n        ----------\n\n        k : int\n           number of neighbours to return\n        approx : bool\n           run approx nearest neighbour search using Annoy\n\n        Returns\n        -------\n\n        accuracy metric - avg jaccard distance of returned tags to ground truth tags in meta data\n        """"""        \n        start = current_milli_time()\n        score_sum = 0\n        num_docs = len(self.seq2meta)\n        for query_id in range(0,num_docs):\n            meta_correct = self.seq2meta[query_id]\n            meta_correct_tags = set(meta_correct[\'tags\'].split(\',\'))\n            sims =  self.nn(query_id,k,approx=approx)\n            doc_score_sum = 0\n            doc_scored = 0\n            for (doc_id,sim) in sims:\n                meta_doc = self.seq2meta[doc_id]\n                meta_doc_tags = set(meta_doc[\'tags\'].split(\',\'))\n                logger.debug(""%s %s"",meta_correct_tags,meta_doc_tags)\n                score = jaccard(meta_correct_tags,meta_doc_tags)\n                logger.debug(""score %s %s"",meta_doc[\'id\'],score)\n                doc_scored += 1\n                doc_score_sum += score\n            if doc_scored > 0:\n                score_sum += doc_score_sum/float(doc_scored)\n        end = current_milli_time()\n        duration_secs = (end-start)/1000\n        duration_per_call = duration_secs/float(num_docs)\n        accuracy = score_sum/float(num_docs)\n        logger.info(""accuracy: %f time: %d secs avg_call_time: %f"",accuracy,duration_secs,duration_per_call)\n        return accuracy\n\n\n'"
python/seldon/text/ngram_recommend.py,0,"b'import sys, getopt, argparse\nimport dawg\nimport numpy as np\nimport math\nfrom seldon import Recommender\nimport operator\nfrom collections import defaultdict\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef find_ngrams(input_list, n):\n  return zip(*[input_list[i:] for i in range(n)])\n\n\nclass NgramModel(Recommender):\n\n    def __init__(self,dawg=None):\n        """"""\n        Provide n-gram recommender based on loaded arpa ngram model.\n        """"""\n        self.dawg=dawg\n\n    def load_model(self,arpa_file):\n        """"""\n        Load an arpa model. Creates internal list for keys, probabilities and backoffs.\n        """"""\n        keys = []\n        probas = []\n        backoffs = []\n        idx = 0\n        ngram_size = 0\n        in_preamble = False\n        with open(arpa_file) as f:\n            for line in f:\n                line = line.rstrip()\n                if line == ""\\\\data\\\\"":\n                    in_preamble = True\n                elif line.endswith(""-grams:""):\n                    in_preamble = False\n                    ngram_size = int(line[1:line.index(""-"")])\n                    logger.info(""Loading ngrams of length %d"",ngram_size)\n                elif line == ""\\\\end\\\\"":\n                    pass\n                else:\n                    if not in_preamble and len(line) > 0:\n                        parts = line.split(""\\t"")\n                        if  not parts[1] in [\'<unk>\',\'<s>\',\'</s>\']:\n                            proba = float(parts[0])\n                            ngram = parts[1]\n                            if len(parts) > 2:\n                                backoff = float(parts[2])\n                            else:\n                                backoff = 0\n                            data = (unicode(ngram, ""utf-8""),idx)\n                            keys.append(data)\n                            probas.append(proba)\n                            backoffs.append(backoff)\n                            idx += 1\n        self.keys = keys\n        self.probas = np.array(probas)\n        self.backoffs = np.array(backoffs)\n\n    def create_trie(self):\n        """"""\n        Create a DAWG from the keys for fast prefix retrieval\n        """"""\n        self.dawg = dawg.IntCompletionDAWG(self.keys)\n\n    def fit(self,arpa_file):\n        """"""\n        Load an arpa file and store in internal DAWG\n        """"""\n        self.load_model(arpa_file)\n        self.create_trie()\n        \n    def score(self,items,k):\n        """"""\n        Find best next item given prefix using backoff probabilities.\n        """"""\n        recs = defaultdict(float)\n        found = 0\n        max_ngram_size = len(items) if len(items)<3 else 3\n        for ngram_size in range(1,max_ngram_size+1):\n            ngrams = find_ngrams(items,ngram_size)\n            for ngram in ngrams:\n                prefix = "" "".join(map(str, ngram))\n                prefix = unicode(prefix+"" "",\'utf8\')\n                logger.info(""searching for %s"",prefix)\n                tot_proba = 0\n                ngram_found = 0\n                for key in self.dawg.iterkeys(prefix):\n                    tokens = key.split()\n                    if len(tokens) == ngram_size+1:\n                        proba = self.probas[self.dawg[key]]\n                        proba = math.pow(10,proba)\n                        logger.info(""%s --> %f"",key,proba)\n                        tot_proba += proba\n                        found += 1\n                        ngram_found += 1\n                        if not (""<s>"" in key or ""</s>"" in key):\n                            recs[key.split()[-1]] += proba\n                if ngram_found > 0:\n                    proba = self.backoffs[self.dawg[prefix[0:-1]]]\n                    proba = math.pow(10,proba)\n                    tot_proba += proba\n                    logger.info(""backoff proba for %s %f "",prefix,proba)\n                    logger.info(""total proba %f"",tot_proba)\n        if found > 0:\n            recs_sorted = sorted(recs.items(), key=operator.itemgetter(0),reverse=True)\n            return recs_sorted[0:k]\n        else:\n            return []\n\n    def recommend(self,user,ids,recent_interactions,client,limit):\n        """"""\n        Recommend items\n\n        Parameters\n        ----------\n\n        user : long\n           user id\n        ids : list(long)\n           item ids to score\n        recent_interactions : list(long)\n           recent items the user has interacted with\n        client : str\n           name of client to recommend for (business group, company, product..)\n        limit : int\n           number of recommendations to return\n\n\n        Returns\n        -------\n        list of pairs of (item_id,score)\n        """"""\n        recommendations = []\n        scores = self.score(recent_interactions[::-1],limit)\n        for (key,score) in scores:\n          ikey = int(key)\n          if not ikey in recent_interactions:\n            recommendations.append((ikey,score))\n        return recommendations\n\n                \nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'test ngram prediction\')\n    parser.add_argument(\'--model\', help=\'arpa model\', required=True)\n    parser.add_argument(\'--search\', help=\'prefix to search space separated tokens\', required=True)\n    parser.add_argument(\'--k\', help=\'number of recommendations\', type=int, default=5)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    ngram_predict = NgramModel()\n    ngram_predict.fit(args.model)\n    items = args.search.split()\n    print ngram_predict.recommend(1,[],items,""test"",args.k)\n'"
python/seldon/text/tagrecommend.py,0,"b'from collections import defaultdict\nfrom seldon.pipeline.pandas_pipelines import BasePandasEstimator \nfrom sklearn.base import BaseEstimator,ClassifierMixin\nimport logging\nimport operator\nfrom seldon.util import DeprecationHelper\n\nlogger = logging.getLogger(__name__)\n\n\nclass TagRecommender(BaseEstimator):\n\n    def __init__(self,max_s2_size=0.1,min_s2_size=25,min_score=0.0):\n        """"""\n        Simple tag recommender using jaccard or asymetric cooccurrence of tags as discussed in Borkur Sigurbjornsson and Roelof van Zwol. 2008. Flickr tag recommendation based on collective knowledge. In Proceedings of the 17th international conference on World Wide Web (WWW 08). ACM, New York, NY, USA, 327-336\n\n        max_s2_size : int, optional\n           max percentage size of candidate tag documents for jaccard distance calc\n        min_s2_size : int, optional\n           min absolute size of candidate tag documents for asymmetric coccurrence score\n        min_score : float\n           min score for any tag for it to be returned\n        """"""\n        self.tag_map = defaultdict(set)\n        self.max_s2_size=max_s2_size\n        self.min_s2_size=min_s2_size\n        self.min_score=min_score\n        \n    def fit(self,corpus,split_char=\',\'):\n        """"""\n        Process a corpus and fir data.\n\n        Parameters\n        ----------\n\n        corpus : object\n           a corpus object that follows seldon.text.DefaultJsonCorpus\n        split_char : str\n           character to split tags \n\n        Returns\n        -------\n        \n        trained TagRecommender object\n\n        """"""\n        processed = 0\n        for j in corpus.get_meta():\n            processed += 1\n            if processed % 1000 == 0:\n                logger.info(""Processed %s"",processed)\n            doc_id = j[\'id\']\n            for tag in j[\'tags\'].split(split_char):\n                self.tag_map[tag].add(long(doc_id))\n        self.tag_map_size = float(len(self.tag_map))\n        return self\n\n    def jaccard(self,s1,s2,max_s2_size=0.1):\n        """"""\n        Return jaccard distance between two sets (of documents)\n\n        Parameters\n        ----------\n\n        s1 : set\n           set (of document ids)\n        s2 : set \n           set (of document ids)\n        max_s2_size : int, optional\n           the max percentage size of s2 for a result to be returned. Can be set to ignore very popular tags returning non-zero scores \n        """"""\n        p = len(s1)/self.tag_map_size\n        if p <= max_s2_size:\n            intersection_size = len(s1.union(s2))\n            if intersection_size > 0:\n                return len(s1.intersection(s2))/float(intersection_size)\n            else:\n                return 0.0\n        else:\n            return 0.0\n\n    def asymmetric_occur(self,s1,s2,min_s2_size=25):\n        """"""\n        Return asymmetric occurrence of set s1 against s2\n\n        Parameters\n        ----------\n\n        s1 : set\n           set (of document ids)\n        s2 : set \n           set (of document ids)\n        min_s2_size : int, optional\n           the absolute min number of documents in s2. Increase to stop very unlikely tags being recommended.\n        """"""        \n        s2_size = len(s2)\n        if s2_size >= min_s2_size:\n            return len(s1.intersection(s2))/float(len(s2))\n        else:\n            return 0.0\n\n    def knn(self,tag,k=5,metric=\'jaccard\',exclusions=[]):\n        """"""\n        Get k nearest neighbours of a tag\n\n        Parameters\n        ----------\n\n        tag : str\n           query tag\n        k : int\n           number of neighbours to return\n        metric : str\n           metric to use, \'jaccard\' or \'asym\'\n        excclusions : list of str\n           tags to exclude\n\n        Returns\n        -------\n\n        list of tuples of tag,score\n        """"""\n        scores = {}\n        tag_sig = self.tag_map[tag]\n        for tag_candidate in self.tag_map:\n            if not (tag == tag_candidate or tag_candidate in exclusions):\n                score = 0.0\n                if metric == \'jaccard\' or metric== \'both\':\n                    score += self.jaccard(tag_sig,self.tag_map[tag_candidate],max_s2_size=self.max_s2_size)\n                if metric == \'asym\' or metric== \'both\':\n                    score += self.asymmetric_occur(tag_sig,self.tag_map[tag_candidate],min_s2_size=self.min_s2_size)\n                if score > 0:\n                    scores[tag_candidate] = score\n        sorted_scores = sorted(scores.items(), key=operator.itemgetter(1),reverse=True)\n        top_scores = sorted_scores[:k]\n        f_scores = []\n        for (tag,score) in top_scores:\n            if score < self.min_score:\n                break\n            else:\n                f_scores.append((tag,score))\n        return f_scores\n\n    def recommend(self,tags,k=5,knn_k=5,metric=\'both\'):\n        """"""\n        recommend tags for a given set of tags\n\n        Parameters\n        ----------\n\n        tags : str\n           query tags\n        k : int\n           number of tags to return\n        knn_k : int\n           number of nearest neighbours for each tag to collect\n        metric : str\n           metric to use, \'jaccard\' or \'asym\'\n\n        Returns\n        -------\n\n        sorted list of tuples of tag,score\n\n        """"""\n        scores = defaultdict(float)\n        for tag in tags:\n            for (tag_recommended,score) in self.knn(tag,k=knn_k,metric=metric,exclusions=tags):\n                scores[tag_recommended] += score\n        sorted_scores = sorted(scores.items(), key=operator.itemgetter(1),reverse=True)\n        return sorted_scores[:k]\n\nTag_Recommender = DeprecationHelper(TagRecommender)'"
docker/examples/US_stocks_fund/src/Explainer.py,0,"b""import numpy as np\nimport sklearn as sk\nimport requests\nimport json\nimport scipy.misc\nimport time\nimport operator\nimport matplotlib.pyplot as plt\nimport scipy.spatial.distance as ssd\nfrom sklearn import linear_model\nfrom flask import jsonify, request\n\n_globals={}\n\ndef encode_input(in_vector):\n    data = [feature for i,feature in enumerate(in_vector)]\n    return {'data':data}\n\ndef get_token():\n    try:\n        p = '{url}/token?consumer_key={key}&consumer_secret={secret}'.format(\n            url=_globals['url'],\n            key=_globals['key'],\n            secret=_globals['secret'])\n        r = requests.post(p)\n        print 'token retrieved'\n        print p\n    \n    except:\n        print 'Failed to retrieve token'\n        return ''\n    \n    return r.json().get('access_token')\n\ndef request_prediction(in_vector,token):\n    req = requests.post('{url}/predict?oauth_token={token}'.format(url=_globals['url'],token=token),\n                        json = encode_input(in_vector))\n    return req\n\nclass Predictor():\n\n    def __init__(self,\n                 host,\n                 key,\n                 secret,\n                 _globals=_globals):\n\n        self._globals = _globals\n        self._globals['url'] = host\n        self._globals['key'] = key\n        self._globals['secret'] = secret\n        self._globals['token'] = get_token()\n    \n    def predict(self,\n                in_vector):\n\n        r = request_prediction(in_vector,\n                               _globals['token'])\n        if r.json().get('error_id') == 8: # Token expired\n            _globals['token'] = get_token()\n            r = request_prediction(request.json,\n                                   _globals['token'])\n\n        predictions = r.json()\n#        print predictions\n        list_preds = [(int(x['predictedClass']), float(x['prediction']))\n                      for x in predictions['predictions']]\n        list_preds.sort()\n#        d_preds = {pred[0]:pred[1] for pred in list_preds}\n        preds = [x[1] for x in list_preds]\n#        pred = (preds.index(max(preds)),max(preds))\n#        d_preds_json = json.dumps(d_preds)\n        \n        return preds\n\nclass Explainer():\n\n    def __init__(self,\n                 predictor_url,\n                 predictor_key,\n                 predictor_secret,\n                 nb_samples=100,\n                 perturbation=0.1,\n                 sim_stddev=1):\n        \n        self.nb_samples = nb_samples\n        self.perturbation = perturbation\n        self.predictor_instance = Predictor(predictor_url,\n                                            predictor_key,\n                                            predictor_secret)\n        self.sim_stddev = sim_stddev\n\n\n    def get_features_scores(self,in_vector):\n        \n        self.features_scores = {}\n        for i,f in enumerate(in_vector):\n            if i % 10 == 0:\n                print 'Getting score for feature %i of %i' % (i,len(in_vector))\n            importance = 0\n            samples_counter = 0\n            pvector = np.copy(in_vector)\n                               \n            tpred1_0 = time.time()\n            preds = self.predictor_instance.predict(pvector)\n            tpred1_f = time.time()-tpred1_0\n            if n % 10 == 0 and i % 10 == 0:\n                print '    original:' \n                print '    time for first query to  microservice: %f' % tpred1_f\n                print '    prediction:', preds.index(max(preds))\n                logprob = -np.log(max(preds))\n                pred_idx = preds.index(max(preds))\n                    \n            pvector[i] = 0\n            tpred2_0 = time.time()\n            preds_f0 = self.predictor_instance.predict(pvector)\n            tpred2_f = time.time() - tpred2_0\n            if n % 10 == 0 and i % 10 == 0:\n                print '    time for second query to microservice: %f' % tpred2_f\n                print '    prediction:', preds.index(max(preds))\n            logprob_f0 = -np.log(preds_f0[pred_idx])\n\n            try:\n                importance += np.abs((logprob - logprob_f0)/logprob)\n            except ZeroDivisionError:\n                logprob = 0.0000000001\n                importance += np.abs((logprob - logprob_f0)/logprob)\n                \n            samples_counter += 1\n            \n            for n in range(self.nb_samples):\n                ttot_0 = time.time()\n                pvector = np.copy(in_vector)\n                random_idxs = np.random.choice(len(in_vector),\n                                                size = int(self.perturbation*len(in_vector)),\n                                                replace=False)\n\n                if i not in random_idxs:\n                    random_values = np.asarray([np.random.normal(in_vector[idx],\n                                                                 self.sim_stddev)\n                                                for idx in random_idxs]) \n                    pvector[random_idxs] = random_values\n                    \n                    tpred1_0 = time.time()\n                    preds = self.predictor_instance.predict(pvector)\n                    tpred1_f = time.time()-tpred1_0\n                    if n % 10 == 0 and i % 10 == 0:\n                        print '    samples generated: %i of %i' % (n,self.nb_samples)\n                        print '    time for first query to  microservice: %f' % tpred1_f\n                        print '    prediction:', preds.index(max(preds))\n                    logprob = -np.log(max(preds))\n                    pred_idx = preds.index(max(preds))\n                    \n                    pvector[i] = 0\n                    tpred2_0 = time.time()\n                    preds_f0 = self.predictor_instance.predict(pvector)\n                    tpred2_f = time.time() - tpred2_0\n                    if n % 10 == 0 and i % 10 == 0:\n                        print '    time for second query to microservice: %f' % tpred2_f\n                        print '    prediction:', preds.index(max(preds))\n                    logprob_f0 = -np.log(preds_f0[pred_idx])\n\n                    try:\n                        importance += np.abs((logprob - logprob_f0)/logprob)\n                    except ZeroDivisionError:\n                        logprob = 0.0000000001\n                        importance += np.abs((logprob - logprob_f0)/logprob)\n                        \n                    samples_counter += 1\n\n                else:\n                    pass\n\n                ttot_f = time.time() - ttot_0\n                if n % 10 == 0 and i % 10 == 0:\n                    print '    total time for 1 sample: %f' % ttot_f\n\n            try:\n                importance = importance/float(samples_counter)\n            except ZeroDivisionError:\n                importance = 0\n                \n            self.features_scores[i] = importance\n\n        return self.features_scores\n\n    def get_top_features(self,nb_top_features):\n\n        sorted_scores = sorted(self.features_scores.items(),\n                               key=operator.itemgetter(1),\n                               reverse=True)\n        \n        if nb_top_features == 'all':\n            return sorted_scores\n        else:\n            return sorted_scores[:nb_top_features]\n        \n"""
docker/examples/US_stocks_fund/src/__init__.py,0,b''
docker/examples/anomaly_detection/inne/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""label""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/anomaly_detection/inne/inne_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.util as sutl\nimport seldon.pipeline.auto_transforms as pauto\nfrom sklearn.pipeline import Pipeline\n#import anomaly_wrapper as aw\n#import AnomalyDetection as anod\nimport seldon.anomaly_wrapper as aw\nimport seldon.anomaly.AnomalyDetection as anod\nimport sys\nimport logging\n\ndef run_pipeline(events,models):\n\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""label""])\n    detector = anod.iNNEDetector()\n\n    wrapper = aw.AnomalyWrapper(clf=detector,excluded=[""label""])\n\n    transformers = [(""tAuto"",tAuto),(""clf"",wrapper)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit_transform(df)\n    pw.save_pipeline(p,models)\n\n\nif __name__ == \'__main__\':\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.DEBUG)\n\n    parser = argparse.ArgumentParser(prog=\'xgb_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/finefoods/scripts/create_model.py,0,"b'import sys, getopt, argparse\nimport seldon.fileutil as fu\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.tfidf_transform as ptfidf\nimport seldon.pipeline.util as sutl\nimport seldon.sklearn_estimator as ske\nfrom sklearn.pipeline import Pipeline\nimport seldon.pipeline.cross_validation as cf\nfrom sklearn.externals import joblib\nimport seldon.xgb as xg\nimport logging\nimport sys\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\nclass XGBoostModel(object):\n    \n    def __init__(self,data_folder,model_folder):\n        self.data_folder = data_folder\n        self.model_folder = model_folder\n\n    def train(self,sample):\n\n        tTfidf = ptfidf.Tfidf_transform(input_feature=""review"",output_feature=""tfidf"",target_feature=""sentiment"",min_df=10,max_df=0.7,select_features=False,topn_features=50000,stop_words=""english"",ngram_range=[1,2])\n\n\n        tFilter2 = bt.Include_features_transform(included=[""tfidf"",""sentiment""])\n\n        svmTransform = bt.Svmlight_transform(output_feature=""svmfeatures"",excluded=[""sentiment""],zero_based=False)\n\n        classifier_xg = xg.XGBoostClassifier(target=""sentiment"",svmlight_feature=""svmfeatures"",silent=1,max_depth=5,n_estimators=200,objective=\'binary:logistic\',scale_pos_weight=0.2)\n\n        cv = cf.Seldon_KFold(classifier_xg,metric=\'auc\',save_folds_folder=""./folds"")\n    \n        transformers = [(""tTfidf"",tTfidf),(""tFilter2"",tFilter2),(""svmTransform"",svmTransform),(""cv"",cv)]\n\n        p = Pipeline(transformers)\n\n        pw = sutl.Pipeline_wrapper()\n        df = pw.create_dataframe_from_files([self.data_folder],df_format=""csv"")\n        if sample < 1.0:\n            logger.info(""sampling dataset to size %s "",sample)\n            df = df.sample(frac=sample,random_state=1)\n        \n        logger.info(""Data frame shape %d , %d"",df.shape[0],df.shape[1])\n\n        df2 = p.fit_transform(df)\n        pw.save_pipeline(p,self.model_folder)\n        logger.info(""cross validation scores %s"",cv.get_scores())\n\n        return p\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'pipeline_example\')\n    parser.add_argument(\'--data\', help=\'data folder\', required=True)\n    parser.add_argument(\'--model\', help=\'model output folder\', required=True)\n    parser.add_argument(\'--sample\', help=\'run on sample of raw data\', default=1.0, type=float)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n\n    x = XGBoostModel(data_folder=args.data,model_folder=args.model)\n    x.train(args.sample)\n\n\n\n\n'"
docker/examples/iris/keras/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""name""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/iris/keras/keras_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.util as sutl\nfrom sklearn.pipeline import Pipeline\nimport seldon.pipeline.auto_transforms as pauto\nimport seldon.keras as sk\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nimport sys\n\n\ndef  create_model(input_width,num_classes):\n    model = Sequential()                         \n    print ""input width="",input_width\n    model.add(Dense(5, init=\'uniform\',input_dim=input_width))\n    model.add(Activation(\'tanh\'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(num_classes))\n    model.add(Activation(\'softmax\'))\n\n    return model\n\n\ndef run_pipeline(events,models):\n    tNameId = bt.Feature_id_transform(min_size=0,exclude_missing=True,zero_based=True,input_feature=""name"",output_feature=""nameId"")\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""nameId"",""name""])\n    keras = sk.KerasClassifier(model_create=create_model,target=""nameId"",target_readable=""name"")\n    transformers = [(""tName"",tNameId),(""tAuto"",tAuto),(""keras"",keras)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    print events\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit(df)\n    pw.save_pipeline(p,models)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'keras_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/iris/scikit/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""name""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/iris/scikit/scikit_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.util as sutl\nimport seldon.pipeline.auto_transforms as pauto\nimport seldon.sklearn_estimator as ske\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nimport seldon.pipeline.cross_validation as cf\nimport sys\nimport logging\n\ndef run_pipeline(events,models):\n\n    tNameId = bt.Feature_id_transform(min_size=0,exclude_missing=True,zero_based=True,input_feature=""name"",output_feature=""nameId"")\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""nameId"",""name""])\n    sk_classifier = RandomForestClassifier(verbose=1)\n    classifier = ske.SKLearnClassifier(clf=sk_classifier,target=""nameId"",excluded=[""name""],target_readable=""name"")\n\n    cv = cf.Seldon_KFold(classifier,5)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n    transformers = [(""tName"",tNameId),(""tAuto"",tAuto),(""cv"",cv)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit_transform(df)\n    pw.save_pipeline(p,models)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n\nif __name__ == \'__main__\':\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'xgb_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/iris/vw/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""name""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/iris/vw/vw_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.auto_transforms as pauto\nimport seldon.pipeline.util as sutl\nfrom sklearn.pipeline import Pipeline\nimport seldon.vw as vw\nimport sys\n\ndef run_pipeline(events,models):\n    tNameId = bt.Feature_id_transform(min_size=0,exclude_missing=True,zero_based=True,input_feature=""name"",output_feature=""nameId"")\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""nameId"",""name""])\n    vwc = vw.VWClassifier(target=""nameId"",target_readable=""name"")\n    transformers = [(""tName"",tNameId),(""tAuto"",tAuto),(""vw"",vwc)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit(df)\n    pw.save_pipeline(p,models)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'xgb_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/iris/xgboost/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""name""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/iris/xgboost/xgb_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.util as sutl\nimport seldon.pipeline.auto_transforms as pauto\nfrom sklearn.pipeline import Pipeline\nimport seldon.xgb as xg\nimport seldon.pipeline.cross_validation as cf\nimport sys\nimport logging\n\ndef run_pipeline(events,models):\n\n    tNameId = bt.Feature_id_transform(min_size=0,exclude_missing=True,zero_based=True,input_feature=""name"",output_feature=""nameId"")\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""nameId"",""name""])\n    xgb = xg.XGBoostClassifier(target=""nameId"",target_readable=""name"",excluded=[""name""],learning_rate=0.1,silent=1)\n    cv = cf.Seldon_KFold(xgb,5)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n    transformers = [(""tName"",tNameId),(""tAuto"",tAuto),(""cv"",cv)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit_transform(df)\n    pw.save_pipeline(p,models)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n\nif __name__ == \'__main__\':\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'xgb_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/iris/xgboost_rpc/create-json.py,0,"b'import sys\nimport json\n\nfor line in sys.stdin:\n    line = line.rstrip()\n    try:\n        (f1,f2,f3,f4,cl) = line.split(\',\')\n        d = {}\n        d[""f1""] = float(f1)\n        d[""f2""] = float(f2)\n        d[""f3""] = float(f3)\n        d[""f4""] = float(f4)\n        d[""name""] = cl\n        j = json.dumps(d,sort_keys=True)\n        print j\n    except:\n        continue\n'"
docker/examples/iris/xgboost_rpc/xgb_pipeline.py,0,"b'import sys, getopt, argparse\nimport seldon.pipeline.basic_transforms as bt\nimport seldon.pipeline.util as sutl\nimport seldon.pipeline.auto_transforms as pauto\nfrom sklearn.pipeline import Pipeline\nimport seldon.xgb as xg\nimport seldon.pipeline.cross_validation as cf\nimport sys\nimport logging\n\ndef run_pipeline(events,models):\n\n    tNameId = bt.Feature_id_transform(min_size=0,exclude_missing=True,zero_based=True,input_feature=""name"",output_feature=""nameId"")\n    tAuto = pauto.Auto_transform(max_values_numeric_categorical=2,exclude=[""nameId"",""name""])\n    xgb = xg.XGBoostClassifier(target=""nameId"",target_readable=""name"",excluded=[""name""],learning_rate=0.1,silent=1)\n    cv = cf.Seldon_KFold(xgb,5)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n    transformers = [(""tName"",tNameId),(""tAuto"",tAuto),(""cv"",cv)]\n    p = Pipeline(transformers)\n\n    pw = sutl.Pipeline_wrapper()\n    df = pw.create_dataframe_from_files(events)\n    df2 = p.fit_transform(df)\n    pw.save_pipeline(p,models)\n    logger.info(""cross validation scores %s"",cv.get_scores())\n\n\nif __name__ == \'__main__\':\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'xgb_pipeline\')\n    parser.add_argument(\'--events\', help=\'events folder\', required=True)\n    parser.add_argument(\'--models\', help=\'output models folder\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    run_pipeline([args.events],args.models)\n\n'"
docker/examples/reuters/data/create_csv.py,0,"b'import json\nimport codecs \nimport os\nimport unicodecsv\nfrom collections import OrderedDict\nimport os\n\n#data_size = ""justTen""\ndata_size = ""full""\noutfile = os.path.abspath(""reuters-21578.csv"")\n\nrow_count = 0\nordered_fieldnames = OrderedDict([(\'id\',None),(\'title\',None),(\'body\',None)])\nwith open(outfile,\'wt\') as f:\n    dw = unicodecsv.DictWriter(f, delimiter=\',\', fieldnames=ordered_fieldnames, encoding=\'utf-8\')\n    dw.writeheader()\n    \n    for filename in os.listdir(""reuters-21578-json/data/""+data_size):\n        f = open(""reuters-21578-json/data/""+data_size+""/""+filename)\n        js = json.load(f)\n        for j in js:\n            if \'topics\' in j and \'body\' in j:\n                r = {}\n                r[""id""] = j[\'id\']\n                r[""title""] = j[\'title\']\n                r[""body""] = j[\'body\']\n                dw.writerow(r)\n                row_count += 1\n\nprint ""finished writing csv data"".format(**locals())\nprint ""rows: {row_count}"".format(**locals())\nprint ""file: {outfile}"".format(**locals())\n\n'"
docker/examples/reuters/microservice/build_model.py,0,"b'import json\nimport codecs \nimport os\n\ndocs = []\nfor filename in os.listdir(""reuters-21578-json/data/full""):\n    f = open(""reuters-21578-json/data/full/""+filename)\n    js = json.load(f)\n    for j in js:\n        if \'topics\' in j and \'body\' in j:\n            d = {}\n            d[""id""] = j[\'id\']\n            d[""text""] = j[\'body\'].replace(""\\n"","""")\n            d[""title""] = j[\'title\']\n            d[""tags""] = "","".join(j[\'topics\'])\n            docs.append(d)\n\nprint ""loaded "",len(docs),"" documents""\n\nfrom  seldon.text import DocumentSimilarity,DefaultJsonCorpus\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ncorpus = DefaultJsonCorpus(docs)\nds = DocumentSimilarity(model_type=\'gensim_lsi\')\nds.fit(corpus)\nprint ""built model""\n\nimport seldon\nrw = seldon.Recommender_wrapper()\nrw.save_recommender(ds,""reuters_recommender"")\nprint ""saved recommender""\n\n\n'"
offline-jobs/spark/scripts/item-similarity/create-sql.py,0,"b'#!/usr/bin/python\nimport getopt, argparse\nimport json\nimport sys \n\nbatchSize = 5000\nnumInserts = 0\nsqlInsertPrefix = ""insert into item_similarity_new values ""\nprint ""truncate item_similarity_new;""\nsql = sqlInsertPrefix\nfor line in sys.stdin:\n    line = line.rstrip()\n    j = json.loads(line)\n    item1 = j[\'item1\']\n    item2 = j[\'item2\']\n    sim = j[\'sim\']\n    if numInserts > 0:\n        sql = sql + "",""\n    sql = sql +  "" (%s,%s,%s)"" % (item1,item2,sim)\n    numInserts += 1\n    if numInserts >= batchSize:\n        sql = sql + "";""\n        print sql\n        numInserts = 0;\n        sql = sqlInsertPrefix\n\nif numInserts > 0:\n    sql = sql + "";""\n    print sql;\n\nprint ""rename table item_similarity to item_similarity_old,item_similarity_new to item_similarity,item_similarity_old to item_similarity_new;""\n\n\n\n\n'"
offline-jobs/sv/docker/scripts/semantic-vectors.py,0,"b'#!/usr/bin/env python\nimport sys, getopt, argparse\nfrom kazoo.client import KazooClient\nimport json\nfrom subprocess import call\nfrom os import makedirs\nimport os\nimport math\nfrom filechunkio import FileChunkIO\nfrom shutil import copy\nfrom boto.s3.connection import S3Connection\n\nclass SemanticVectorsError(Exception):\n    pass\n\ndef loadZookeeperOptions(opts,zk):\n    node = ""/all_clients/""+opts[\'client\']+""/offline/semvec""\n    if zk.exists(node):\n        data, stat = zk.get(node)\n        jStr = data.decode(""utf-8"")\n        print ""Found zookeeper configuration:"",jStr\n        j = json.loads(jStr)\n        for key in j:\n            opts[key] = j[key]\n\ndef activateModel(args,folder,zk):\n    node = ""/all_clients/""+args.client+""/svtext""\n    print ""Activating model in zookeper at node "",node,"" with data "",folder\n    if zk.exists(node):\n        zk.set(node,folder)\n    else:\n        zk.create(node,folder,makepath=True)\n\n\ndef uploadModel(args,folder):\n    if folder.startswith(""/""):\n        copyToLocalFolder(folder)\n    elif folder.startswith(""s3:/""):\n        copyToS3(args,folder[4:])\n    elif folder.startswith(""s3n:/""):\n        copyToS3(args,folder[5:])\n\ndef copyToLocalFolder(folder):\n    try:\n        makedirs(folder)\n    except:\n        print folder,""exists""\n    copy(""./docvectors.txt"",folder+""/docvectors.txt"")\n    copy(""./termvectors.txt"",folder+""/termvectors.txt"")\n\n\n\ndef copyToS3(args,folder):\n    if args.awskey:\n        c = S3Connection(args.awskey, args.awssecret)\n    else:\n        c = S3Connection()\n    bucketName = folder.split(\'/\')[1]\n    path = ""/"".join(folder.split(\'/\')[2:])\n    b = c.get_bucket(bucketName)\n    # Get file info\n    source_path = \'./docvectors.txt\'\n    source_size = os.stat(source_path).st_size\n    # Create a multipart upload request\n    uploadPath = path + ""/"" + os.path.basename(source_path)\n    print ""uploading to bucket "",bucketName,"" path "",uploadPath\n    mp = b.initiate_multipart_upload(uploadPath)\n    # Use a chunk size of 50 MiB (feel free to change this)\n    chunk_size = 10485760\n    chunk_count = int(math.ceil(source_size / float(chunk_size)))\n    for i in range(chunk_count):\n        offset = chunk_size * i\n        bytes = min(chunk_size, source_size - offset)\n        with FileChunkIO(source_path, \'r\', offset=offset,\n                         bytes=bytes) as fp:\n            print ""uploading to s3 chunk "",(i+1),""/"",chunk_count\n            mp.upload_part_from_file(fp, part_num=i + 1)\n    # Finish the upload\n    print ""completing transfer to s3""\n    mp.complete_upload()\n\ndef createLuceneIndex(args):\n    params = [""java"", ""-cp"", ""/semvec/semvec-lucene-tools.jar"", ""io.seldon.semvec.CreateLuceneIndexFromDb"",""-l"",""index"",""-jdbc"",args.jdbc,""-itemType"",str(args.itemType),""-raw-ids"",""-use-item-attrs"",""-attr-names"",args.tagAttrs,""-recreate"",""-debug"",""-item-limit"",str(args.itemLimit)]\n    res = call(params)\n    if res > 0:\n        raise LuceneError(""Failed to create lucene index"")\n\ndef createSV(args):\n    params = [""java"",""-cp"",""/semvec/semvec-lucene-tools.jar"",""pitt.search.semanticvectors.BuildIndex"",""-trainingcycles"",""1"",""-maxnonalphabetchars"",""-1"",""-minfrequency"",""0"",""-maxfrequency"",""1000000"",""-luceneindexpath"",""index"",""-indexfileformat"",""text""]\n    res = call(params)\n    if res > 0:\n        raise SemanticVectorsError(""Failed to create semantic vectors index"")\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'semantic-vectors\')\n    parser.add_argument(\'-c\', \'--client\', help=\'client\', required=True)\n    parser.add_argument(\'-z\', \'--zookeeper\', help=\'zookeeper\')\n    parser.add_argument(\'--tagAttrs\', help=\'attribute names in Seldon DB for textual item meta data\')\n    parser.add_argument(\'--outputPath\', help=\'output folder to store model\' , default=""/seldon-models"")\n    parser.add_argument(\'--startDay\', help=\'day to store model in\' , default=""1"")\n    parser.add_argument(\'--activate\', help=\'activate model\', action=\'store_true\')\n    parser.add_argument(\'--awskey\', help=\'aws key\')\n    parser.add_argument(\'--awssecret\', help=\'aws secret\')\n    parser.add_argument(\'--itemLimit\', help=\'max number of items to take from db\', default=""-1"")\n    parser.add_argument(\'--itemType\', help=\'item type to restrict items to\', default=""1"")\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    if args.zookeeper:\n        zk = KazooClient(hosts=args.zookeeper)\n        zk.start()\n        loadZookeeperOptions(opts,zk)\n        print ""tagAttrs="",args.tagAttrs\n        createLuceneIndex(args)\n        createSV(args)\n        folder = str(args.outputPath + ""/"" + args.client + ""/svtext/"" + str(args.startDay))\n        uploadModel(args,folder)\n        if args.activate:\n            activateModel(args,folder,zk)\n        zk.stop()\n\n\n'"
offline-jobs/vw/docker/scripts/vw.py,0,"b'import sys, getopt, argparse\nfrom seldon.vw import *\nimport json\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(prog=\'vw\')\n    parser.add_argument(\'--client\', help=\'client\', required=True)\n    parser.add_argument(\'--zkHosts\', help=\'zookeeper\')\n    parser.add_argument(\'--inputPath\', help=\'input base folder to find features data\')\n    parser.add_argument(\'--outputPath\', help=\'output folder to store model\')\n    parser.add_argument(\'--day\', help=\'days to get features data for\' , type=int)\n    parser.add_argument(\'--activate\', help=\'activate model in zookeeper\', action=\'store_true\')\n    parser.add_argument(\'--awsKey\', help=\'aws key - needed if input or output is on AWS and no IAM\')\n    parser.add_argument(\'--awsSecret\', help=\'aws secret - needed if input or output on AWS  and no IAM\')\n    parser.add_argument(\'--vwArgs\', help=\'vw training args\')\n    parser.add_argument(\'--features\', help=\'JSON providing per feature vw mapping presently label,split - default is try to create feature values as float otherwise treat as categorical\')\n    parser.add_argument(\'--namespaces\', help=\'JSON providing per feature namespace mapping - default is no namespaces\')\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    vwArgs = {""zk_hosts"" : opts.get(\'zkHosts\'), ""awsKey"" : opts.get(\'awsKey\'), ""awsSecret"" : opts.get(\'awsSecret\') }\n    vw = VWSeldon(**vwArgs)\n    conf = {}\n    for k in opts:\n        if opts[k]:\n            if k == ""features"" or k == ""namespaces"":\n                conf[k] = json.loads(opts[k])\n            else:\n                conf[k] = opts[k]\n    print conf\n    vw.train(args.client,conf)\n'"
python/seldon/pipeline/tests/__init__.py,0,b''
python/seldon/pipeline/tests/test_auto_transforms.py,0,"b'import unittest\nfrom  .. import auto_transforms as at\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\nclass Test_AutoTransforms(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        t = at.AutoTransform(ignore_vals=[""NA"",""""])\n        transformers = [(""auto"",t)]\n        p = Pipeline(transformers)\n        df = pd.DataFrame([True,False])\n        df2 = p.fit_transform(df)\n        self.assertTrue(df2[0][0] == 1)\n        self.assertTrue(df2[0][1] == 0)\n        joblib.dump(p,""/tmp/auto_pl"")\n        p2 = joblib.load(""/tmp/auto_pl"")\n        df3 = p2.transform(df)\n        self.assertTrue(df3[0][0] == 1)\n        self.assertTrue(df3[0][1] == 0)\n\n    def test_bool_col(self):\n        df = pd.DataFrame([True,False])\n        t = at.AutoTransform(ignore_vals=[""NA"",""""])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(df2[0][0] == 1)\n        self.assertTrue(df2[0][1] == 0)\n\n    def test_boolean_col_with_missing(self):\n        df = pd.DataFrame([{""a"":""true""},{""a"":""false""},{""a"":""""}])\n        t = at.AutoTransform(ignore_vals=[""NA""])\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        self.assertTrue(df2[""a""][0] == ""true"")\n        self.assertTrue(df2[""a""][1] == ""false"")\n        self.assertTrue(df2[""a""][2] == ""UKN"")\n\n    def test_boolean_col_with_missing2(self):\n        df = pd.DataFrame([{""a"":""true""},{""a"":""false""},{""a"":""""}])\n        t = at.AutoTransform(ignore_vals=[""NA""],cat_missing_val=""?"")\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        self.assertTrue(df2[""a""][0] == ""true"")\n        self.assertTrue(df2[""a""][1] == ""false"")\n        self.assertTrue(df2[""a""][2] == ""?"")\n\n    def test_boolean_col2(self):\n        df = pd.DataFrame([{""a"":1},{""a"":0},{""a"":""false""}])\n        t = at.AutoTransform(ignore_vals=[""NA""])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(df2[""a""][0] == 1)\n        self.assertTrue(df2[""a""][1] == 0)\n\n    def test_change_type_when_ignored_removed(self):\n        df = pd.DataFrame([{""a"":""NA""},{""a"":10},{""a"":12},{""a"":8}])\n        t = at.AutoTransform(ignore_vals=[""NA""])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(df2[""a""][0] == 0.0)\n        self.assertTrue(df2[""a""][1] == 0.0)\n        self.assertAlmostEqual(df2[""a""][2],1.224745,places=4)\n\n    def test_categorical(self):\n        df = pd.DataFrame([{""a"":""""},{""a"":""v1""},{""a"":""v2""},{""a"":""v3""}])\n        t = at.AutoTransform(ignore_vals=[""NA""])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(df2[""a""][0] == ""UKN"")\n        self.assertTrue(df2[""a""][1] == ""v1"")\n        self.assertTrue(df2[""a""][2] == ""v2"")\n\n\n    """"""\n    Test that categorical values can be limited. Those appearing less than some value are removed.\n    """"""\n    def test_categorical_values_limit(self):\n        df = pd.DataFrame([{""a"":10,""b"":1},{""a"":5,""b"":2},{""a"":10,""b"":3}])\n        t = at.AutoTransform(max_values_numeric_categorical=2)\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertEqual(df2[""a""][0],""a_10"")\n\n    def test_ignored_values(self):\n        df = pd.DataFrame([{""a"":10},{""a"":99},{""a"":12},{""a"":8}])\n        t = at.AutoTransform(ignore_vals=[99])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(df2[""a""][0] == 0.0)\n        self.assertTrue(df2[""a""][1] == 0.0)\n        self.assertAlmostEqual(df2[""a""][2],1.224745,places=4)\n\n\n    def test_dates(self):\n        df = pd.DataFrame([{""a"":""30JAN14:15:11:00"",""b"":""20 Jan 2015""},{""a"":""31JAN14:10:11:00"",""b"":""20 Jan 2015""}])\n        t = at.AutoTransform(custom_date_formats=[""%d%b%y:%H:%M:%S""],date_cols=[""a""])\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertAlmostEqual(df2[""a_h1""][0],-0.707,places=2)\n\n    def test_dates2(self):\n        df = pd.DataFrame([{""a"":""28-09-15""},{""a"":""22-03-15""}])\n        t = at.AutoTransform(custom_date_formats=[""%d-%m-%y""],date_cols=[""a""],date_transforms=[False,True,True,True])\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        #self.assertAlmostEqual(df2[""a_h1""][0],-0.707,places=2)\n\n\n    def test_drop_constant_cols(self):\n        df = pd.DataFrame([{""a"":10,""b"":11},{""a"":10,""b"":12}])\n        t = at.AutoTransform()\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(len(df2.columns) == 1)\n\n    def test_drop_duplicate_cols(self):\n        df = pd.DataFrame([{""a"":12,""b"":12},{""a"":10,""b"":10}])\n        t = at.AutoTransform()\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertTrue(len(df2.columns) == 1)\n\n\n    def test_min_max_limit(self):\n        df = pd.DataFrame([{""a"":9,""b"":12},{""a"":12,""b"":10}])\n        df2 = pd.DataFrame([{""a"":1,""b"":12},{""a"":15,""b"":10}])\n        t = at.AutoTransform(min_max_limit=True)\n        t.fit(df)\n        df3 = t.transform(df2)\n        self.assertTrue(df3[""a""][0] == -1)\n        self.assertTrue(df3[""a""][1] == 1)\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/pipeline/tests/test_basic_transforms.py,0,"b'# coding=utf-8\nimport unittest\nfrom .. import basic_transforms as bt\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\n\nclass Test_BinaryTransform(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        df = pd.DataFrame.from_dict([{""a"":""something""},{}])\n        t = bt.BinaryTransform(input_feature=""a"",output_feature=""abin"")\n        transformers = [(""BinaryTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.fit_transform(df)\n        self.assertEquals(df[""abin""][0],1)\n        self.assertEquals(df[""abin""][1],0)\n\n    def test_sklearn_pipeline_numbers(self):\n        df = pd.DataFrame.from_dict([{""a"":2},{""a"":0}])\n        t = bt.BinaryTransform(input_feature=""a"",output_feature=""abin"")\n        transformers = [(""BinaryTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.fit_transform(df)\n        print df2\n        self.assertEquals(df[""abin""][0],1)\n        self.assertEquals(df[""abin""][1],0)\n\n    def test_sklearn_pipeline_str_numbers(self):\n        df = pd.DataFrame.from_dict([{""a"":""2""},{""a"":""0""}])\n        t = bt.BinaryTransform(input_feature=""a"",output_feature=""abin"")\n        transformers = [(""BinaryTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.fit_transform(df)\n        print df2\n        self.assertEquals(df[""abin""][0],1)\n        self.assertEquals(df[""abin""][1],0)\n\n\nclass Test_ExcludeFeaturesTransform(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        df = pd.DataFrame.from_dict([{""a"":""something"",""b"":1},{""a"":""something2""}])\n        t = bt.ExcludeFeaturesTransform(excluded=[""b""])\n        transformers = [(""exclude_transform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.fit_transform(df)\n        self.assertEquals(len(df2.columns),1)\n\n\nclass Test_SplitTransform(unittest.TestCase):\n\n\n    def test_multiple_cols(self):\n        t = bt.SplitTransform(input_features=[""a"",""b""],output_feature=""res"")\n        df = pd.DataFrame.from_dict([{""a"":""a b"",""b"":""c d"",""c"":3},{""a"":""word1"",""b"":""word2""}])\n        transformers = [(""SplitTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.transform(df)\n        self.assertTrue(len(df2[""res""][0]) == 4)\n\n    def test_multiple_cols_with_missing_cols(self):\n        t = bt.SplitTransform(input_features=[""a"",""b""],output_feature=""res"")\n        df = pd.DataFrame.from_dict([{""a"":""a b"",""c"":3},{""b"":""word2""}])\n        transformers = [(""SplitTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.transform(df)\n        self.assertTrue(len(df2[""res""][0]) == 2)\n        self.assertTrue(len(df2[""res""][1]) == 1)\n\n    def test_multiple_cols_numbers_ignored(self):\n        t = bt.SplitTransform(input_features=[""a"",""b""],ignore_numbers=True,output_feature=""res"")\n        df = pd.DataFrame.from_dict([{""a"":""a b"",""b"":""c 1"",""c"":3}])\n        transformers = [(""SplitTransform"",t)]\n        p = Pipeline(transformers)\n        df2 = p.transform(df)\n        self.assertTrue(len(df2[""res""][0]) == 3)\n\n\nclass Test_IncludeFeaturesTransform(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n        t = bt.IncludeFeaturesTransform(included=[""a"",""b""])\n        transformers = [(""include_features"",t)]\n        p = Pipeline(transformers)\n        df = pd.DataFrame.from_dict([{""a"":1,""b"":2,""c"":3}])\n        df2 = p.fit_transform(df)\n        self.assertTrue(sorted(df2.columns) == sorted([""a"",""b""]))\n        joblib.dump(p,""/tmp/p"")\n        p2 = joblib.load(""/tmp/p"")\n        df3 = p2.transform(df)\n        self.assertTrue(sorted(df3.columns) == sorted([""a"",""b""]))\n\n    def test_include(self):\n        t = bt.IncludeFeaturesTransform(included=[""a"",""b""])\n        df = pd.DataFrame.from_dict([{""a"":1,""b"":2,""c"":3}])\n        df2 = t.transform(df)\n        self.assertTrue(sorted(df2.columns) == sorted([""a"",""b""]))\n\n    def test_include_missing_column(self):\n        t = bt.IncludeFeaturesTransform(included=[""a"",""b""])\n        df = pd.DataFrame.from_dict([{""a"":1,""c"":3}])\n        df2 = t.transform(df)\n        self.assertTrue(sorted(df2.columns) == sorted([""a""]))\n\nclass Test_ExistFeaturesTransform(unittest.TestCase):\n\n    def test_exist(self):\n        t = bt.ExistFeaturesTransform(included=[""b""])\n        df = pd.DataFrame.from_dict([{""a"":1,""b"":2},{""a"":3}])\n        df2 = t.transform(df)\n        self.assertTrue(df.shape[0] == 1)\n\nclass Test_SvmlightTransform(unittest.TestCase):\n\n    def test_svm_with_list(self):\n        df = pd.DataFrame([{""a"":{""abc"":1,""def"":2}},{""a"":{""gh1"":1,""def"":2}}])\n        t = bt.SvmlightTransform(included=[""a""],output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        self.assertEquals(df2[""svm""][0][0],(1,1))\n\n    def test_svm_with_unicode(self):\n        df = pd.DataFrame([{""a"":{u""abc"":1,u""\xc2\xa3def"":2}},{""a"":{""gh1"":1,""def"":2}}])\n        t = bt.SvmlightTransform(included=[""a""],output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        self.assertEquals(df2[""svm""][0][0],(1,1))\n\n\n    def test_svm(self):\n        df = pd.DataFrame([{""a"":1.2,""b"":""word"",""c"":[""a"",""b""],""d"":{""a"":1}},{""a"":3,""c"":[""b"",""d""]},{""c"":{""k"":1,""k2"":""word""}}])\n        t = bt.SvmlightTransform(output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        print df2\n        self.assertEquals(df2[""svm""][0][1],(5,1.2))\n\n    def test_svm_with_include(self):\n        df = pd.DataFrame([{""a"":1.2,""b"":""word"",""c"":[""a"",""b""],""d"":{""a"":1}},{""a"":3,""c"":[""b"",""d""]},{""c"":{""k"":1,""k2"":""word""}}])\n        t = bt.SvmlightTransform(included=[""a""],output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2[""svm""][0][0],(1,1.2))\n\n    def test_svm_with_include_and_categorical(self):\n        df = pd.DataFrame([{""a"":""word1""},{""a"":""word2""},{""a"":""word1""}])\n        t = bt.SvmlightTransform(included=[""a""],output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2[""svm""][0][0],(2,1))\n\n    def test_svm_with_exclude(self):\n        df = pd.DataFrame([{""a"":1.2,""b"":""word"",""c"":[""a"",""b""],""d"":{""a"":1}},{""a"":3,""c"":[""b"",""d""]},{""c"":{""k"":1,""k2"":""word""}}])\n        t = bt.SvmlightTransform(excluded=[""b"",""c"",""d""],output_feature=""svm"")\n        t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2[""svm""][0][0],(1,1.2))\n\n\nclass Test_FeatureIdTransform(unittest.TestCase):\n\n    def test_ids_max_classes(self):\n        df = pd.DataFrame([{""a"":""type1""},{""a"":""type2""},{""a"":""type1""}])\n        t = bt.FeatureIdTransform(max_classes=1,exclude_missing=True,input_feature=""a"",output_feature=""id"")\n        r = t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2.shape[0],2)\n\n    def test_ids_max_classes_zero_based(self):\n        df = pd.DataFrame([{""a"":""type1""},{""a"":""type2""},{""a"":""type1""}])\n        t = bt.FeatureIdTransform(max_classes=1,exclude_missing=True,input_feature=""a"",output_feature=""id"",zero_based=True)\n        r = t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2.shape[0],2)\n\n\n\n    def test_ids_exclude_too_few(self):\n        df = pd.DataFrame([{""a"":""type1""},{""a"":""type2""},{""a"":""type1""}])\n        t = bt.FeatureIdTransform(min_size=2,exclude_missing=True,input_feature=""a"",output_feature=""id"")\n        r = t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2.shape[0],2)\n\n    def test_ids(self):\n        df = pd.DataFrame([{""a"":""type1""},{""a"":""type2""},{""a"":""type1""}])\n        t = bt.FeatureIdTransform(min_size=1,exclude_missing=True,input_feature=""a"",output_feature=""id"")\n        r = t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2.shape[0],3)\n        self.assertEquals(df2[""id""][0],1)\n\n    def test_missing_input_feature(self):\n        df = pd.DataFrame([{""a"":""type1""},{""a"":""type2""},{""a"":""type1""}])\n        t = bt.FeatureIdTransform(min_size=1,exclude_missing=True,input_feature=""a"",output_feature=""id"")\n        r = t.fit(df)\n        df2 = t.transform(df)\n        self.assertEquals(df2.shape[0],3)\n        self.assertFalse(\'missing\' in df2)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n'"
python/seldon/pipeline/tests/test_bayes_optimizer.py,0,"b'import unittest\nfrom .. import bayes_optimize as bopt\nimport seldon.xgb as xgb\nimport pandas as pd\nimport logging\n\n\nclass Test_BayesOptimizer(unittest.TestCase):\n\n    def test_kfold(self):\n        x = xgb.XGBoostClassifier(target=""target"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        t = bopt.BayesOptimizer(x,{\'learning_rate\': (0.01, 0.3),\'n_estimators\': (10,1000)},param_int=[\'n_estimators\'])\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        t.fit(df)\n        print t.get_params()\n        print t.get_best_score()\n\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/pipeline/tests/test_cross_validation.py,0,"b'import unittest\nfrom .. import cross_validation as cf\nimport seldon.xgb as xgb\nimport pandas as pd\nimport logging\n\n\nclass Test_SeldonKFold(unittest.TestCase):\n\n    def test_kfold(self):\n        x = xgb.XGBoostClassifier(target=""target"",learning_rate=0.1,silent=0,objective=\'binary:logistic\')\n        t = cf.SeldonKFold(x,3)\n        f1 = {""target"":0,""b"":1.0,""c"":0}\n        f2 = {""target"":1,""b"":0,""c"":2.0}\n        fs = []\n        for i in range (1,50):\n            fs.append(f1)\n            fs.append(f2)\n        print ""features=>"",fs\n        df = pd.DataFrame.from_dict(fs)\n        t.fit(df)\n\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/pipeline/tests/test_sklearn.py,0,"b'import unittest\nfrom .. import sklearn_transform as ssk\nimport pandas as pd\nimport json\nimport os.path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\n\nclass Test_SklearnTransform(unittest.TestCase):\n\n\n    def test_standard_scaler(self):\n        df = pd.DataFrame.from_dict([{""a"":1,""b"":2},{""a"":2,""b"":3}])\n        t = ssk.SklearnTransform(input_features=[""a""],output_features=[""a_scaled""],transformer=StandardScaler())\n        t.fit(df)\n        df_2 = t.transform(df)\n        print df_2\n        self.assertEquals(df_2[""a_scaled""][0],-1)\n        self.assertEquals(df_2[""a_scaled""][1],1)\n        \n    def test_label_binarizer(self):\n        df = pd.DataFrame.from_dict([{""a"":""a""},{""a"":""b""},{""a"":""c""}])\n        t = ssk.SklearnTransform(input_features=[""a""],output_features_prefix=""a"",transformer=LabelBinarizer())\n        t.fit(df)\n        df_2 = t.transform(df)\n        self.assertEquals(df_2[""a_0""][0],1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
python/seldon/pipeline/tests/test_tfidf_transform.py,3,"b'import unittest\nfrom seldon.pipeline import tfidf_transform as tf\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\n\nclass Test_TfidfTransform(unittest.TestCase):\n\n    def test_sklearn_pipeline(self):\n                df = pd.DataFrame([{""likeids"": [102313626475894, 110544635673389, 125327794232468, 1381292875499997, 1405744859651437, 1410444839224829, 143981935773732, 1461778597368720, 159634840722133, 1606136206319782, 1617969055108441, 175338329150218, 208633805894647, 244944385603396, 245732105485076, 264050273731539, 302410296457933, 316001081653, 379092115616379, 388308794577146, 430801740267217, 446108528831354, 447497665393027, 450086458346414, 470402606305707, 488128054598370, 508960455844413, 518468471599787, 56531631380, 67920382572, 7310480740, 863556617025433, 943587665656008],},{""likeids"": [100318233349756, 111541275541080, 118478001539872, 1392733037644503, 14202933641, 1450755685155363, 145930618882430, 1459747570939968, 1482351465350198, 149829238430713, 1519856064948500, 1562464497303561, 173955149455632, 198812000402, 234877673256809, 258941513699, 261039940586093, 269993129865393, 277030342456725, 307688779325497, 322739367839522, 353924261385391, 376344085771485, 396795520381965, 405199642913939, 407889119354319, 461654403900239, 484970471577531, 49852438689, 511808655562689, 525243884184515, 527731407332180, 533522990127878, 544053285683692, 554468308013797, 605579839502285, 606676766022011, 663419033716761, 6665038402, 684466768336969, 747130848662418, 771257532932834, 782938198415086, 876613622381314]}])\n                t = tf.TfidfTransform(input_feature=""likeids"",output_feature=""tfidf"")\n                transformers = [(""tfidf"",t)]\n                p = Pipeline(transformers)\n                df2 = p.fit_transform(df)\n                self.assertAlmostEqual(df2[""tfidf""][0][u""1405744859651437""],0.174077655956)\n                joblib.dump(p,""/tmp/p"")\n                p2 = joblib.load(""/tmp/p"")\n                df3 = p2.transform(df)\n        \n\n    def test_list_input_feature(self):\n                df = pd.DataFrame([{""likeids"": [102313626475894, 110544635673389, 125327794232468, 1381292875499997, 1405744859651437, 1410444839224829, 143981935773732, 1461778597368720, 159634840722133, 1606136206319782, 1617969055108441, 175338329150218, 208633805894647, 244944385603396, 245732105485076, 264050273731539, 302410296457933, 316001081653, 379092115616379, 388308794577146, 430801740267217, 446108528831354, 447497665393027, 450086458346414, 470402606305707, 488128054598370, 508960455844413, 518468471599787, 56531631380, 67920382572, 7310480740, 863556617025433, 943587665656008],},{""likeids"": [100318233349756, 111541275541080, 118478001539872, 1392733037644503, 14202933641, 1450755685155363, 145930618882430, 1459747570939968, 1482351465350198, 149829238430713, 1519856064948500, 1562464497303561, 173955149455632, 198812000402, 234877673256809, 258941513699, 261039940586093, 269993129865393, 277030342456725, 307688779325497, 322739367839522, 353924261385391, 376344085771485, 396795520381965, 405199642913939, 407889119354319, 461654403900239, 484970471577531, 49852438689, 511808655562689, 525243884184515, 527731407332180, 533522990127878, 544053285683692, 554468308013797, 605579839502285, 606676766022011, 663419033716761, 6665038402, 684466768336969, 747130848662418, 771257532932834, 782938198415086, 876613622381314]}])\n                t = tf.TfidfTransform(input_feature=""likeids"",output_feature=""tfidf"")\n                t.fit(df)\n                df2 = t.transform(df)\n                self.assertAlmostEqual(df2[""tfidf""][0][u""1405744859651437""],0.174077655956)\n\n    def test_chi_sq(self):\n                df = pd.DataFrame([{""likeids"": [102313626475894, 110544635673389, 125327794232468, 1381292875499997, 1405744859651437, 1410444839224829, 143981935773732, 1461778597368720, 159634840722133, 1606136206319782, 1617969055108441, 175338329150218, 208633805894647, 244944385603396, 245732105485076, 264050273731539, 302410296457933, 316001081653, 379092115616379, 388308794577146, 430801740267217, 446108528831354, 447497665393027, 450086458346414, 470402606305707, 488128054598370, 508960455844413, 518468471599787, 56531631380, 67920382572, 7310480740, 863556617025433, 943587665656008],""target"":1},{""likeids"": [100318233349756, 111541275541080, 118478001539872, 1392733037644503, 14202933641, 1450755685155363, 145930618882430, 1459747570939968, 1482351465350198, 149829238430713, 1519856064948500, 1562464497303561, 173955149455632, 198812000402, 234877673256809, 258941513699, 261039940586093, 269993129865393, 277030342456725, 307688779325497, 322739367839522, 353924261385391, 376344085771485, 396795520381965, 405199642913939, 407889119354319, 461654403900239, 484970471577531, 49852438689, 511808655562689, 525243884184515, 527731407332180, 533522990127878, 544053285683692, 554468308013797, 605579839502285, 606676766022011, 663419033716761, 6665038402, 684466768336969, 747130848662418, 771257532932834, 782938198415086, 876613622381314],""target"":1}])\n                t = tf.TfidfTransform(input_feature=""likeids"",output_feature=""tfidf"",select_features=True,topn_features=2,target_feature=""target"")\n                t.fit(df)\n                df2 = t.transform(df)\n                print df2\n                self.assertAlmostEqual(df2[""tfidf""][0][u""943587665656008""],0.174077655956)\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n\n'"
python/seldon/pipeline/tests/test_util.py,0,"b'import unittest\nfrom .. import util as sutils\nfrom .. import basic_transforms as bt\nimport pandas as pd\nimport json\nimport os.path\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.externals import joblib\nimport logging\n\nclass Test_PipelineWrapper(unittest.TestCase):\n\n    def _create_test_json(self,fname,as_list=False):\n        f = open(fname,""w"")\n        if as_list:\n            f.write(""["")\n        j = {}\n        j[""a""] = 1\n        j[""b""] = 2\n        f.write(json.dumps(j,sort_keys=True))\n        if as_list:\n            f.write("",\\n"")\n        else:\n            f.write(""\\n"")\n        j = {}\n        j[""a""] = 3\n        j[""b""] = 4\n        f.write(json.dumps(j,sort_keys=True))\n        if as_list:\n            f.write(""]\\n"")\n        else:\n            f.write(""\\n"")\n        f.close()\n\n    def test_load_json_folders(self):\n        w = sutils.PipelineWrapper()\n        data_folder = w.get_work_folder()+""/events""\n        if not os.path.exists(data_folder):\n            os.makedirs(data_folder)\n        fname = data_folder+""/""+""test.json""\n        self._create_test_json(fname)\n        df = w.create_dataframe([data_folder])\n        print df\n\n    def test_load_json_file(self):\n        w = sutils.PipelineWrapper()\n        fname = w.get_work_folder()+""/""+""test.json""\n        self._create_test_json(fname,as_list=True)\n        df = w.create_dataframe(fname)\n        print df\n\n    def test_save_dataframe(self):\n        w = sutils.PipelineWrapper()\n        df = pd.DataFrame.from_dict([{""a"":""a b"",""b"":""c d"",""c"":3},{""a"":""word1"",""b"":""word2""}])\n        fname = w.get_work_folder()+""/""+""saved.json""\n        w.save_dataframe(df,fname,""csv"",csv_index=False)\n        df2 = w.create_dataframe(fname,df_format=""csv"")\n        from pandas.util.testing import assert_frame_equal\n        assert_frame_equal(df.sort(axis=1), df2.sort(axis=1), check_names=True)\n\n    def test_save_pipeline(self):\n        w = sutils.PipelineWrapper()\n        t = bt.IncludeFeaturesTransform(included=[""a"",""b""])\n        transformers = [(""include_features"",t)]\n        p = Pipeline(transformers)\n        df = pd.DataFrame.from_dict([{""a"":1,""b"":2,""c"":3}])\n        df2 = p.fit_transform(df)\n        self.assertTrue(sorted(df2.columns) == sorted([""a"",""b""]))\n        dest_folder = w.get_work_folder()+""/dest_pipeline""\n        w.save_pipeline(p,dest_folder)\n        p2 = w.load_pipeline(dest_folder)\n        df3 = p2.transform(df)\n        self.assertTrue(sorted(df3.columns) == sorted([""a"",""b""]))\n\nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n'"
python/seldon/text/tests/__init__.py,0,b''
python/seldon/text/tests/test_docsim.py,0,"b'import unittest\nfrom seldon.text import DocumentSimilarity, DefaultJsonCorpus\nimport logging\nfrom gensim import interfaces, utils\nfrom gensim.corpora.dictionary import Dictionary\nimport copy\n\n\nclass Test_DocumentSimilarity(unittest.TestCase):\n\n    def get_docs(self):\n        return [{""id"":1,""text"":""an article about sports and football, Arsenel, Liverpool"",""tags"":""football""},\n                {""id"":2,""text"":""an article about football and finance, Liverpool, Arsenel"",""tags"":""football""},\n                {""id"":3,""text"":""an article about money and lending"",""tags"":""money""},\n                {""id"":4,""text"":""an article about money and banking and lending"",""tags"":""money""}]\n\n\n    def test_sklearn_nmf(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = DocumentSimilarity(model_type=""sklearn_nmf"")\n        ds.fit(corpus)\n        res = ds.nn(0,k=1)\n        self.assertEqual(res[0][0],1)\n\n    def test_gensim_lsi(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = DocumentSimilarity(model_type=""gensim_lsi"")\n        ds.fit(corpus)\n        res = ds.nn(0,k=1)\n        self.assertEqual(res[0][0],1)\n\n\n    def test_gensim_rp(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = DocumentSimilarity(model_type=""gensim_rp"")\n        ds.fit(corpus)\n        res = ds.nn(0,k=1)\n        self.assertEqual(res[0][0],1)\n\n    def test_gensim_lsi(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = DocumentSimilarity(model_type=""gensim_lsi"")\n        ds.fit(corpus)\n        score = ds.score(k=1)\n        self.assertEqual(score,1.0)\n\n\n\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n'"
python/seldon/text/tests/test_tagrecommend.py,0,"b'import unittest\nfrom seldon.text import TagRecommender, DefaultJsonCorpus\nimport logging\n\n\nclass Test_TagRecommender(unittest.TestCase):\n\n    def get_docs(self):\n        return [{""id"":1,""text"":"""",""tags"":""football,soccer""},\n                {""id"":2,""text"":"""",""tags"":""money,finance""},\n                {""id"":3,""text"":"""",""tags"":""football""},\n                {""id"":4,""text"":"""",""tags"":""money""}]\n\n\n    def test_jaccard(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = TagRecommender(max_s2_size=1.0,min_s2_size=0)\n        ds.fit(corpus)\n        res = ds.knn(""football"",k=1,metric=\'jaccard\')\n        self.assertEqual(res[0],(""soccer"",0.5))\n\n    def test_jaccard_max_constraint(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = TagRecommender(max_s2_size=0.1,min_s2_size=0)\n        ds.fit(corpus)\n        res = ds.knn(""football"",k=1,metric=\'jaccard\')\n        self.assertEqual(len(res),0)\n\n    def test_asym(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = TagRecommender(max_s2_size=1.0,min_s2_size=0)\n        ds.fit(corpus)\n        res = ds.knn(""football"",k=1,metric=\'asym\')\n        self.assertEqual(res[0],(""soccer"",1.0))\n\n    def test_asym_min_constraint(self):\n        corpus = DefaultJsonCorpus(self.get_docs())\n        ds = TagRecommender(max_s2_size=1.0,min_s2_size=10)\n        ds.fit(corpus)\n        res = ds.knn(""football"",k=1,metric=\'asym\')\n        self.assertEqual(len(res),0)\n\n\n        \nif __name__ == \'__main__\':\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n    unittest.main()\n'"
docker/examples/iris/xgboost_rpc/python/iris_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: iris.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'iris.proto\',\n  package=\'io.seldon.microservice.iris\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\niris.proto\\x12\\x1bio.seldon.microservice.iris\\""D\\n\\x12IrisPredictRequest\\x12\\n\\n\\x02\\x66\\x31\\x18\\x01 \\x01(\\x02\\x12\\n\\n\\x02\\x66\\x32\\x18\\x02 \\x01(\\x02\\x12\\n\\n\\x02\\x66\\x33\\x18\\x03 \\x01(\\x02\\x12\\n\\n\\x02\\x66\\x34\\x18\\x04 \\x01(\\x02\\x42/\\n\\x1bio.seldon.microservice.irisB\\x0eIrisClassifierP\\x01\\x62\\x06proto3\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_IRISPREDICTREQUEST = _descriptor.Descriptor(\n  name=\'IrisPredictRequest\',\n  full_name=\'io.seldon.microservice.iris.IrisPredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'f1\', full_name=\'io.seldon.microservice.iris.IrisPredictRequest.f1\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'f2\', full_name=\'io.seldon.microservice.iris.IrisPredictRequest.f2\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'f3\', full_name=\'io.seldon.microservice.iris.IrisPredictRequest.f3\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'f4\', full_name=\'io.seldon.microservice.iris.IrisPredictRequest.f4\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=43,\n  serialized_end=111,\n)\n\nDESCRIPTOR.message_types_by_name[\'IrisPredictRequest\'] = _IRISPREDICTREQUEST\n\nIrisPredictRequest = _reflection.GeneratedProtocolMessageType(\'IrisPredictRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _IRISPREDICTREQUEST,\n  __module__ = \'iris_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.microservice.iris.IrisPredictRequest)\n  ))\n_sym_db.RegisterMessage(IrisPredictRequest)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\033io.seldon.microservice.irisB\\016IrisClassifierP\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n# @@protoc_insertion_point(module_scope)\n'"
docker/examples/iris/xgboost_rpc/python/iris_rpc_client.py,0,"b'import os\nimport sys, getopt, argparse\nimport logging\nimport json\nimport grpc\nimport iris_pb2\nimport seldon_pb2\nfrom google.protobuf import any_pb2\nimport requests\n\nclass IrisRpcClient(object):\n\n    def __init__(self,host,http_transport,http_port,rpc_port):\n        self.host = host\n        self.http_transport = http_transport\n        self.http_port = http_port\n        self.rpc_port = rpc_port\n\n\n    def getToken(self,key,secret):\n        params = {}\n        params[""consumer_key""] = key\n        params[""consumer_secret""] = secret\n        url = self.http_transport+""://""+self.host+"":""+str(self.http_port)+""/token""\n        r = requests.get(url,params=params)\n        if r.status_code == requests.codes.ok:\n            j = json.loads(r.text)\n            return j[""access_token""]\n        else:\n            print ""failed call to get token""\n            return None\n\n    def callRpc(self,token,jStr):\n        j = json.loads(jStr)\n        \n        channel = grpc.insecure_channel(self.host+\':\'+str(self.rpc_port))\n        stub = seldon_pb2.SeldonStub(channel)\n\n        data = iris_pb2.IrisPredictRequest(f1=j[""f1""],f2=j[""f2""],f3=j[""f3""],f4=j[""f4""])\n        dataAny = any_pb2.Any()\n        dataAny.Pack(data)\n        meta = seldon_pb2.ClassificationRequestMeta(puid=""12345"")\n        request = seldon_pb2.ClassificationRequest(meta=meta,data=dataAny)\n        metadata = [(b\'oauth_token\', token)]\n        reply = stub.Classify(request,999,metadata=metadata)\n        print reply\n\nif __name__ == \'__main__\':\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n    parser = argparse.ArgumentParser(prog=\'create_replay\')\n    parser.add_argument(\'--host\', help=\'rpc server host\', default=""localhost"")\n    parser.add_argument(\'--http-transport\', help=\'http or https\', default=""http"")\n    parser.add_argument(\'--http-port\', help=\'http server port\', type=int, default=30015)\n    parser.add_argument(\'--rpc-port\', help=\'rpc server port\', type=int, default=30017)\n    parser.add_argument(\'--key\', help=\'oauth consumer key\')\n    parser.add_argument(\'--secret\', help=\'oauth consumer secret\')\n    parser.add_argument(\'--features-json\', help=\'JSON features to use for prediction, should contain features f1,f2,f3,f4 as floats\', required=True)\n\n    args = parser.parse_args()\n    opts = vars(args)\n    rpc = IrisRpcClient(host=args.host,http_transport=args.http_transport,http_port=args.http_port,rpc_port=args.rpc_port)\n    token = rpc.getToken(args.key,args.secret)\n    if not token is None:\n        print ""Got token:"",token\n        rpc.callRpc(token,args.features_json)\n    else:\n        print ""failed to get token""\n'"
docker/examples/iris/xgboost_rpc/python/iris_rpc_microservice.py,0,"b'from concurrent import futures\nimport time\nimport sys, getopt, argparse\nimport seldon.pipeline.util as sutl\nimport random\nimport iris_pb2\nimport grpc\nimport google.protobuf\nfrom google.protobuf import any_pb2\nimport pandas as pd \nfrom seldon.microservice.rpc import CustomDataHandler\nfrom seldon.microservice import Microservices\n\nclass BadDataError(Exception):\n    def __init__(self, value):\n        self.value = value\n    def __str__(self):\n        return repr(self.value)\n\nclass IrisCustomDataHandler(CustomDataHandler):\n\n    def getData(self, request):\n        anyMsg = request.data\n        dc = iris_pb2.IrisPredictRequest()\n        success = anyMsg.Unpack(dc)\n        if success:\n            df = pd.DataFrame([{""f1"":dc.f1,""f2"":dc.f2,""f3"":dc.f3,""f4"":dc.f4}])\n            return df\n        else:\n            context.set_code(grpc.StatusCode.INTERNAL)\n            context.set_details(\'Invalid data\')\n            raise BadDataError(\'Invalid data\')\n\n\nif __name__ == ""__main__"":\n    import logging\n    logger = logging.getLogger()\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(name)s : %(message)s\', level=logging.DEBUG)\n    logger.setLevel(logging.INFO)\n\n\n    parser = argparse.ArgumentParser(prog=\'microservice\')\n    parser.add_argument(\'--model-name\', help=\'name of model\', required=True)\n    parser.add_argument(\'--pipeline\', help=\'location of prediction pipeline\', required=True)\n    parser.add_argument(\'--aws-key\', help=\'aws key\', required=False)\n    parser.add_argument(\'--aws-secret\', help=\'aws secret\', required=False)\n\n    args = parser.parse_args()\n    opts = vars(args)\n\n    m = Microservices(aws_key=args.aws_key,aws_secret=args.aws_secret)\n    cd = IrisCustomDataHandler()\n    m.create_prediction_rpc_microservice(args.pipeline,args.model_name,cd)\n\n\n'"
docker/examples/iris/xgboost_rpc/python/seldon_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: seldon.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'seldon.proto\',\n  package=\'io.seldon.api.rpc\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x0cseldon.proto\\x12\\x11io.seldon.api.rpc\\x1a\\x19google/protobuf/any.proto\\""w\\n\\x15\\x43lassificationRequest\\x12:\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32,.io.seldon.api.rpc.ClassificationRequestMeta\\x12\\""\\n\\x04\\x64\\x61ta\\x18\\x02 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\"")\\n\\x19\\x43lassificationRequestMeta\\x12\\x0c\\n\\x04puid\\x18\\x01 \\x01(\\t\\""\\xb3\\x01\\n\\x13\\x43lassificationReply\\x12\\x38\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32*.io.seldon.api.rpc.ClassificationReplyMeta\\x12<\\n\\x0bpredictions\\x18\\x02 \\x03(\\x0b\\x32\\\'.io.seldon.api.rpc.ClassificationResult\\x12$\\n\\x06\\x63ustom\\x18\\x03 \\x01(\\x0b\\x32\\x14.google.protobuf.Any\\""M\\n\\x17\\x43lassificationReplyMeta\\x12\\x0c\\n\\x04puid\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tmodelName\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tvariation\\x18\\x03 \\x01(\\t\\""V\\n\\x14\\x43lassificationResult\\x12\\x12\\n\\nprediction\\x18\\x01 \\x01(\\x01\\x12\\x16\\n\\x0epredictedClass\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nconfidence\\x18\\x03 \\x01(\\x01\\x32h\\n\\x06Seldon\\x12^\\n\\x08\\x43lassify\\x12(.io.seldon.api.rpc.ClassificationRequest\\x1a&.io.seldon.api.rpc.ClassificationReply\\""\\x00\\x42$\\n\\x11io.seldon.api.rpcB\\rPredictionAPIP\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_CLASSIFICATIONREQUEST = _descriptor.Descriptor(\n  name=\'ClassificationRequest\',\n  full_name=\'io.seldon.api.rpc.ClassificationRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'io.seldon.api.rpc.ClassificationRequest.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'io.seldon.api.rpc.ClassificationRequest.data\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=62,\n  serialized_end=181,\n)\n\n\n_CLASSIFICATIONREQUESTMETA = _descriptor.Descriptor(\n  name=\'ClassificationRequestMeta\',\n  full_name=\'io.seldon.api.rpc.ClassificationRequestMeta\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'puid\', full_name=\'io.seldon.api.rpc.ClassificationRequestMeta.puid\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=183,\n  serialized_end=224,\n)\n\n\n_CLASSIFICATIONREPLY = _descriptor.Descriptor(\n  name=\'ClassificationReply\',\n  full_name=\'io.seldon.api.rpc.ClassificationReply\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'io.seldon.api.rpc.ClassificationReply.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predictions\', full_name=\'io.seldon.api.rpc.ClassificationReply.predictions\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom\', full_name=\'io.seldon.api.rpc.ClassificationReply.custom\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=227,\n  serialized_end=406,\n)\n\n\n_CLASSIFICATIONREPLYMETA = _descriptor.Descriptor(\n  name=\'ClassificationReplyMeta\',\n  full_name=\'io.seldon.api.rpc.ClassificationReplyMeta\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'puid\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.puid\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'modelName\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.modelName\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variation\', full_name=\'io.seldon.api.rpc.ClassificationReplyMeta.variation\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=408,\n  serialized_end=485,\n)\n\n\n_CLASSIFICATIONRESULT = _descriptor.Descriptor(\n  name=\'ClassificationResult\',\n  full_name=\'io.seldon.api.rpc.ClassificationResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'prediction\', full_name=\'io.seldon.api.rpc.ClassificationResult.prediction\', index=0,\n      number=1, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predictedClass\', full_name=\'io.seldon.api.rpc.ClassificationResult.predictedClass\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'confidence\', full_name=\'io.seldon.api.rpc.ClassificationResult.confidence\', index=2,\n      number=3, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=487,\n  serialized_end=573,\n)\n\n_CLASSIFICATIONREQUEST.fields_by_name[\'meta\'].message_type = _CLASSIFICATIONREQUESTMETA\n_CLASSIFICATIONREQUEST.fields_by_name[\'data\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_CLASSIFICATIONREPLY.fields_by_name[\'meta\'].message_type = _CLASSIFICATIONREPLYMETA\n_CLASSIFICATIONREPLY.fields_by_name[\'predictions\'].message_type = _CLASSIFICATIONRESULT\n_CLASSIFICATIONREPLY.fields_by_name[\'custom\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\nDESCRIPTOR.message_types_by_name[\'ClassificationRequest\'] = _CLASSIFICATIONREQUEST\nDESCRIPTOR.message_types_by_name[\'ClassificationRequestMeta\'] = _CLASSIFICATIONREQUESTMETA\nDESCRIPTOR.message_types_by_name[\'ClassificationReply\'] = _CLASSIFICATIONREPLY\nDESCRIPTOR.message_types_by_name[\'ClassificationReplyMeta\'] = _CLASSIFICATIONREPLYMETA\nDESCRIPTOR.message_types_by_name[\'ClassificationResult\'] = _CLASSIFICATIONRESULT\n\nClassificationRequest = _reflection.GeneratedProtocolMessageType(\'ClassificationRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUEST,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationRequest)\n  ))\n_sym_db.RegisterMessage(ClassificationRequest)\n\nClassificationRequestMeta = _reflection.GeneratedProtocolMessageType(\'ClassificationRequestMeta\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUESTMETA,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationRequestMeta)\n  ))\n_sym_db.RegisterMessage(ClassificationRequestMeta)\n\nClassificationReply = _reflection.GeneratedProtocolMessageType(\'ClassificationReply\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREPLY,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationReply)\n  ))\n_sym_db.RegisterMessage(ClassificationReply)\n\nClassificationReplyMeta = _reflection.GeneratedProtocolMessageType(\'ClassificationReplyMeta\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREPLYMETA,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationReplyMeta)\n  ))\n_sym_db.RegisterMessage(ClassificationReplyMeta)\n\nClassificationResult = _reflection.GeneratedProtocolMessageType(\'ClassificationResult\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESULT,\n  __module__ = \'seldon_pb2\'\n  # @@protoc_insertion_point(class_scope:io.seldon.api.rpc.ClassificationResult)\n  ))\n_sym_db.RegisterMessage(ClassificationResult)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\n\\021io.seldon.api.rpcB\\rPredictionAPIP\\001\'))\nimport grpc\nfrom grpc.beta import implementations as beta_implementations\nfrom grpc.beta import interfaces as beta_interfaces\nfrom grpc.framework.common import cardinality\nfrom grpc.framework.interfaces.face import utilities as face_utilities\n\n\nclass SeldonStub(object):\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Classify = channel.unary_unary(\n        \'/io.seldon.api.rpc.Seldon/Classify\',\n        request_serializer=ClassificationRequest.SerializeToString,\n        response_deserializer=ClassificationReply.FromString,\n        )\n\n\nclass SeldonServicer(object):\n\n  def Classify(self, request, context):\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_SeldonServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Classify\': grpc.unary_unary_rpc_method_handler(\n          servicer.Classify,\n          request_deserializer=ClassificationRequest.FromString,\n          response_serializer=ClassificationReply.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'io.seldon.api.rpc.Seldon\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n\n\nclass BetaSeldonServicer(object):\n  def Classify(self, request, context):\n    context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\nclass BetaSeldonStub(object):\n  def Classify(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n    raise NotImplementedError()\n  Classify.future = None\n\n\ndef beta_create_Seldon_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n  request_deserializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationRequest.FromString,\n  }\n  response_serializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationReply.SerializeToString,\n  }\n  method_implementations = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): face_utilities.unary_unary_inline(servicer.Classify),\n  }\n  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n  return beta_implementations.server(method_implementations, options=server_options)\n\n\ndef beta_create_Seldon_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n  request_serializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationRequest.SerializeToString,\n  }\n  response_deserializers = {\n    (\'io.seldon.api.rpc.Seldon\', \'Classify\'): ClassificationReply.FromString,\n  }\n  cardinalities = {\n    \'Classify\': cardinality.Cardinality.UNARY_UNARY,\n  }\n  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n  return beta_implementations.dynamic_stub(channel, \'io.seldon.api.rpc.Seldon\', cardinalities, options=stub_options)\n# @@protoc_insertion_point(module_scope)\n'"
