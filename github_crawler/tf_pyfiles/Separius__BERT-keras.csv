file_path,api_count,code
__init__.py,0,b''
data/dataset.py,0,"b'import random\nimport numpy as np\nfrom data.vocab import TextEncoder\nfrom typing import List, NamedTuple, Optional, Dict, Any\n\n\nclass TaskWeightScheduler:\n    def __init__(self, active_in_pretrain: bool, active_in_finetune: bool,\n                 pretrain_value: float = 1.0, finetune_value: float = 1.0):\n        self.active_in_pretrain = active_in_pretrain\n        self.active_in_finetune = active_in_finetune\n        self.pretrain_value = pretrain_value\n        self.finetune_value = finetune_value\n\n    def get(self, is_pretrain: bool, step: int) -> float:\n        if is_pretrain and self.active_in_pretrain:\n            return self.pretrain_value\n        if not is_pretrain and self.active_in_finetune:\n            return self.finetune_value\n        raise ValueError()\n\n\nclass TaskMetadata(NamedTuple):\n    name: str  # ""lm"" will be considered differently (will use tied decoder)\n    is_token_level: bool\n    num_classes: int\n    dropout: float\n    weight_scheduler: TaskWeightScheduler\n\n\nclass TokenTaskData(NamedTuple):\n    target: List[int]\n    target_mask: List[bool]\n\n\nclass SentenceTaskData(NamedTuple):\n    target: int\n    target_index: int\n\n\nclass TaskDataBatch(NamedTuple):\n    target: np.array  # (int32) batch_size for sentence level tasks or batch_size, seq_len for token level tasks\n    target_mask: np.array  # (int8) same as target (will ignore zeros)\n\n\nclass Sentence(NamedTuple):\n    tokens: List[int]\n    padding_mask: List[bool]\n    segments: Optional[List[int]] = None\n    token_classification: Optional[Dict[str, TokenTaskData]] = None\n    sentence_classification: Optional[Dict[str, SentenceTaskData]] = None\n\n\nclass SentenceBatch(NamedTuple):\n    tokens: np.array  # (int32) batch_size, seq_len\n    padding_mask: np.array  # (int8) batch_size, seq_len (0 or 1, zeros should be ignored (1 == use, 0 == padded))\n    segments: np.array  # (int32) batch_size, seq_len\n    token_classification: Dict[str, TaskDataBatch]  # task_name(\'lm\' is special) : task_data\n    sentence_classification: Dict[str, TaskDataBatch]  # task_name : task_data\n\n\ndef create_attention_mask(pad_mask: Optional[np.array], is_causal: bool, batch_size: Optional[int] = None,\n                          length: Optional[int] = None, bert_attention: bool = False) -> np.array:\n    if pad_mask is not None:\n        assert pad_mask.ndim == 2\n        batch_size, length = pad_mask.shape\n    if is_causal:\n        b = np.cumsum(np.eye(length, dtype=np.float32), axis=0)\n    else:\n        b = np.ones((length, length), dtype=np.float32)\n    b = np.reshape(b, [1, 1, length, length])\n    b = np.repeat(b, batch_size, axis=0)  # B, 1, L, L\n    if pad_mask is not None:\n        _pad_mask = pad_mask[..., np.newaxis]\n        _pad_mask = np.repeat(_pad_mask, length, 2)\n        _pad_mask_t = np.transpose(_pad_mask, [0, 2, 1])\n        if bert_attention:\n            tmp = _pad_mask_t\n        else:\n            tmp = _pad_mask * _pad_mask_t\n        tmp = tmp[:, np.newaxis, ...]\n        if b is None:\n            b = tmp.astype(np.float32)\n        else:\n            b = b * tmp\n    return b\n\n\ndef _trim_seq(seq: Optional[List[Any]], length: int, from_end: bool = True) -> Optional[List[Any]]:\n    if seq is None:\n        return None\n    return seq[:length] if from_end else seq[-length:]\n\n\ndef _trim_sentence_target(task_dict: Dict[str, SentenceTaskData], desired_len: int,\n                          orig_seq_len: int, from_end: bool = True) -> Dict[\n    str, SentenceTaskData]:\n    trimmed_task_dict = {}\n    for k, v in task_dict.items():\n        target_index = v.target_index\n        if orig_seq_len > desired_len:\n            if from_end and target_index > desired_len:\n                target_index = -1\n            if not from_end:\n                target_index -= orig_seq_len - desired_len\n        if target_index >= 0:\n            trimmed_task_dict[k] = SentenceTaskData(v.target, target_index)\n    return trimmed_task_dict\n\n\ndef _trim_sentence(sentence: Sentence, length: int, from_end: bool = True) -> Sentence:\n    return Sentence(_trim_seq(sentence.tokens, length, from_end),\n                    _trim_seq(sentence.padding_mask, length, from_end),\n                    _trim_seq(sentence.segments, length, from_end),\n                    {k: TokenTaskData(_trim_seq(v.target, length, from_end),\n                                      _trim_seq(v.target_mask, length, from_end)) for k, v in\n                     sentence.token_classification.items()} if sentence.token_classification is not None else {},\n                    _trim_sentence_target(sentence.sentence_classification, length, len(sentence.tokens),\n                                          from_end) if sentence.sentence_classification is not None else {})\n\n\ndef check_sent_len(sentence: Sentence, min_len: Optional[int], max_len: Optional[int], from_end: bool = True) -> \\\n        Optional[Sentence]:\n    if min_len is not None and len(sentence.tokens) < min_len:\n        return None\n    if max_len is not None and len(sentence.tokens) > max_len:\n        return _trim_sentence(sentence, max_len, from_end)\n    return sentence\n\n\ndef msk_sentence(sentence: List[int], vocab_size: int, keep_prob: float,\n                 mask_prob: float, rand_prob: float) -> Sentence:\n    prediction_target = [0] * len(sentence)\n    prediction_mask = [False] * len(sentence)\n    new_sent = sentence.copy()\n    for i in range(len(sentence)):\n        probability = random.random()\n        if probability > keep_prob:\n            prediction_target[i] = sentence[i]\n            prediction_mask[i] = True\n            if probability < (mask_prob + keep_prob):\n                new_sent[i] = vocab_size + TextEncoder.MSK_OFFSET\n            elif probability < (mask_prob + rand_prob + keep_prob):\n                new_sent[i] = random.randrange(vocab_size)\n    return Sentence(new_sent, [True] * len(new_sent), None,\n                    token_classification={\'lm\': TokenTaskData(prediction_target, prediction_mask)},\n                    sentence_classification={})\n\n\ndef _pad_seq(seq: List[Any], pad_token: Any, pad_len: int, is_post_pad: bool = True) -> List[Any]:\n    return (seq + [pad_token] * pad_len) if is_post_pad else ([pad_token] * pad_len + seq)\n\n\ndef pad(sentence: Sentence, pad_id: int, max_len: int, is_post_pad: bool = True) -> Sentence:\n    pad_len = max_len - len(sentence.tokens)\n    if pad_len == 0:\n        return sentence\n    return Sentence(_pad_seq(sentence.tokens, pad_id, pad_len, is_post_pad),\n                    _pad_seq(sentence.padding_mask, False, pad_len, is_post_pad),\n                    _pad_seq(sentence.segments, 0, pad_len, is_post_pad),\n                    {k: TokenTaskData(_pad_seq(v.target, 0, pad_len, is_post_pad),\n                                      _pad_seq(v.target_mask, False, pad_len, is_post_pad)) for k, v in\n                     sentence.token_classification.items()} if sentence.token_classification is not None else {},\n                    {k: SentenceTaskData(v.target, v.target_index + (0 if is_post_pad else pad_len)) for k, v in\n                     sentence.sentence_classification.items()} if sentence.sentence_classification is not None else {})\n\n\ndef generate_pos_ids(batch_size: int, max_len: int) -> np.array:\n    return np.repeat(np.arange(max_len, dtype=np.int32).reshape(1, -1), batch_size, 0)\n'"
data/lm_dataset.py,0,"b""import os\nimport random\nimport numpy as np\nfrom contextlib import ExitStack\nfrom data.vocab import TextEncoder\nfrom typing import List, Optional, Generator, TextIO, Tuple, Dict\nfrom data.dataset import (Sentence, pad, msk_sentence, check_sent_len,\n                          SentenceBatch, TaskDataBatch, TokenTaskData, SentenceTaskData)\n\n\ndef lm_generator(text_corpus_address: str, text_encoder: TextEncoder, keep_prob: float = 0.85,\n                 mask_prob: float = 0.15 * 0.8, rand_prob: float = 0.15 * 0.1, min_len: Optional[int] = None,\n                 max_len: Optional[int] = 512, file_jump_prob: float = 0.1, mismatch_prob: float = 0.5,\n                 num_file_pointers: int = 8, is_causal: bool = False, use_single_sentence: bool = False,\n                 batch_size: int = 256) -> Generator[SentenceBatch, None, None]:\n    if not (0.0 <= mask_prob <= 1.0 and 0.0 <= rand_prob <= 1.0 and\n            0.0 <= keep_prob <= 1.0 and 0.0 <= file_jump_prob <= 1.0):\n        raise ValueError('all probablities should be between zero and one')\n    if mask_prob + rand_prob + keep_prob > 1.0:\n        raise ValueError('sum of mask, rand and keep probablities should be less than 1.0')\n    if use_single_sentence:\n        generator = _get_lm_generator_single(text_corpus_address, text_encoder, keep_prob, mask_prob, rand_prob,\n                                             min_len, max_len, file_jump_prob, num_file_pointers)\n    else:\n        in_memory = file_jump_prob == 0.0 and num_file_pointers == 1\n        generator = _get_lm_generator_double(text_corpus_address, text_encoder, keep_prob, mask_prob, rand_prob,\n                                             min_len, max_len, mismatch_prob, in_memory, file_jump_prob,\n                                             num_file_pointers)\n    batch = []\n    for item in generator:\n        batch.append(item)\n        if len(batch) == batch_size:\n            batch = make_next_token_prediction(batch) if is_causal else batch\n            batch = _create_batch(batch, text_encoder.pad_id, max_len)\n            yield batch\n            batch = []\n\n\ndef make_next_token_prediction(batch: List[Sentence]) -> List[Sentence]:\n    for item in batch:\n        for i in range(len(item.tokens) - 1):\n            item.token_classification['lm'].target[i] = item.tokens[i + 1]\n            item.token_classification['lm'].target_mask[i] = True\n        item.token_classification['lm'].target[-1] = 0\n        item.token_classification['lm'].target_mask[-1] = False\n    return batch\n\n\ndef _grab_line(files: List[TextIO], file_size: int, jump_prob: float) -> str:\n    file = files[random.randrange(len(files))]\n    if random.random() < jump_prob:\n        file.seek(random.randrange(file_size))\n        file.readline()  # discard - bound to be partial line\n    random_line = file.readline()\n    if len(random_line) == 0:  # we have hit the end\n        file.seek(0)\n        random_line = file.readline()\n    return random_line\n\n\ndef _create_token_task_batch(batch: List[Sentence]) -> Dict[str, TaskDataBatch]:\n    batch_keys = set(batch[0].token_classification.keys())\n    for item in batch:\n        assert batch_keys == set(batch[0].token_classification.keys())\n    result = {}\n    for key in batch_keys:\n        result[key] = TaskDataBatch(\n            np.array([item.token_classification[key].target for item in batch], dtype=np.int32),\n            np.array([item.token_classification[key].target_mask for item in batch], dtype=np.int32))\n    return result\n\n\ndef _create_sent_task_batch(batch: List[Sentence]) -> Dict[str, TaskDataBatch]:\n    batch_keys = set(batch[0].sentence_classification.keys())\n    for item in batch:\n        assert batch_keys == set(batch[0].sentence_classification.keys())\n    result = {}\n    for key in batch_keys:\n        result[key] = TaskDataBatch(\n            np.array([item.sentence_classification[key].target for item in batch], dtype=np.int32),\n            np.array([item.sentence_classification[key].target_index for item in batch], dtype=np.int32))\n    return result\n\n\ndef _create_batch(batch: List[Sentence], pad_id: int, max_len: Optional[int] = None) -> SentenceBatch:\n    if max_len is None:\n        max_len = max(len(item.tokens) for item in batch)\n    padded_batch = [pad(item, pad_id, max_len) for item in batch]\n    return SentenceBatch(\n        np.array([item.tokens for item in padded_batch], dtype=np.int32),\n        np.array([item.padding_mask for item in padded_batch], dtype=np.int8),\n        np.array([item.segments for item in padded_batch], dtype=np.int32),\n        _create_token_task_batch(padded_batch), _create_sent_task_batch(padded_batch)\n    )\n\n\ndef _get_lm_generator_single(text_corpus_address: str, text_encoder: TextEncoder, keep_prob: float, mask_prob: float,\n                             rand_prob: float, min_len: Optional[int], max_len: Optional[int], jump_prob,\n                             num_files) -> Generator[Sentence, None, None]:\n    _max_len = float('inf') if max_len is None else max_len - 2\n    _min_len = 0 if min_len is None else min_len - 2\n    file_size = os.stat(text_corpus_address).st_size\n    with ExitStack() as stack:\n        files = [stack.enter_context(open(text_corpus_address)) for _ in range(num_files)]\n\n        def _encode_line(line: str) -> Optional[Sentence]:\n            return check_sent_len(\n                msk_sentence(text_encoder.encode(line.rstrip()), len(text_encoder), keep_prob, mask_prob, rand_prob),\n                _min_len, _max_len)\n\n        def _yield_sentence(sent: Sentence) -> Sentence:\n            lm = sent.token_classification['lm']\n            return Sentence(\n                [text_encoder.bos_id] + sent.tokens + [text_encoder.eos_id],\n                [True] + sent.padding_mask + [True],\n                [0] * (len(sent.tokens) + 2),\n                {'lm': TokenTaskData([0] + lm.target + [0], [False] + lm.target_mask + [False])},\n                {}\n            )\n\n        while True:\n            sent = _grab_line(files, file_size, jump_prob)\n            encoded = _encode_line(sent)\n            if not encoded:\n                continue\n            yield _yield_sentence(encoded)\n\n\ndef _get_lm_generator_double(text_corpus_address: str, text_encoder: TextEncoder, keep_prob: float, mask_prob: float,\n                             rand_prob: float, min_len: Optional[int], max_len: Optional[int],\n                             mismatch_prob: float, in_memory: bool, jump_prob: float, num_files: int) -> Generator[\n    Sentence, None, None]:\n    _max_len = float('inf') if max_len is None else max_len - 3\n    _min_len = 0 if min_len is None else min_len - 3\n    file_size = os.stat(text_corpus_address).st_size\n    current_line_number = 0\n    with ExitStack() as stack:\n        if in_memory:\n            with open(text_corpus_address) as f:\n                all_lines = [text_encoder.encode(line.rstrip()) for line in f]\n            files = None\n        else:\n            all_lines = None\n            files = [stack.enter_context(open(text_corpus_address)) for _ in range(num_files)]\n        max_line_number = len(all_lines) if all_lines else float('inf')\n\n        def _encode_line(line: str, half: bool, from_end: bool = False) -> Optional[Sentence]:\n            return check_sent_len(\n                msk_sentence(text_encoder.encode(line.rstrip()), len(text_encoder), keep_prob, mask_prob, rand_prob),\n                _min_len // (2 if half else 1), _max_len // (2 if half else 1), from_end=from_end)\n\n        def _yield_sentence(sent1: Sentence, sent2: Optional[Sentence] = None) -> Sentence:\n            lm = sent1.token_classification['lm']\n            if sent2 is None:\n                split_idx = random.randint(_min_len // 2, len(sent1.tokens) - _min_len // 2)\n                return Sentence(\n                    [text_encoder.bos_id] + sent1.tokens[:split_idx] + [text_encoder.del_id] + sent1.tokens[\n                                                                                               split_idx:] + [\n                        text_encoder.eos_id],\n                    [True] + sent1.padding_mask[:split_idx] + [True] + sent1.padding_mask[split_idx:] + [True],\n                    [0] * (split_idx + 2) + [1] * (1 + len(sent1.tokens) - split_idx),\n                    {'lm': TokenTaskData([0] + lm.target[:split_idx] + [0] + lm.target[split_idx:] + [0],\n                                         [False] + lm.target_mask[:split_idx] + [False] + lm.target_mask[split_idx:] + [\n                                             False])},\n                    {}\n                )\n            lm_ = sent2.token_classification['lm']\n            return Sentence(\n                [text_encoder.bos_id] + sent1.tokens + [text_encoder.del_id] + sent2.tokens + [text_encoder.eos_id],\n                [True] + sent1.padding_mask + [True] + sent2.padding_mask + [True],\n                [0] * (2 + len(sent1.tokens)) + [1] * (1 + len(sent2.tokens)),\n                {'lm': TokenTaskData([0] + lm.target + [0] + lm_.target + [0],\n                                     [False] + lm.target_mask + [False] + lm_.target_mask + [False])},\n                {}\n            )\n\n        def _calc_encoded(line: str, _all_lines: Optional[List[str]] = None, _files: Optional[List[TextIO]] = None) -> \\\n                Optional[Tuple[Optional[Sentence], Optional[Sentence]]]:\n            if random.random() < mismatch_prob:\n                _encoded1 = _encode_line(line, half=True)\n                if _all_lines is not None:\n                    line2 = _all_lines[random.randrange(len(_all_lines))]\n                else:\n                    line2 = _grab_line(_files, file_size, jump_prob)\n                _encoded2 = _encode_line(line2, half=True, from_end=True)\n                if _encoded2 is None:\n                    return None\n            else:\n                _encoded1 = _encode_line(line, half=False)\n                _encoded2 = None\n            return _encoded1, _encoded2\n\n        while True:\n            encoded1, encoded2 = _calc_encoded(\n                all_lines[current_line_number] if all_lines else _grab_line(files, file_size, jump_prob), all_lines,\n                files)\n            if encoded1 is None:\n                continue\n            if all_lines:\n                current_line_number += 1\n                if current_line_number == max_line_number:\n                    current_line_number = 0\n            yield _yield_sentence(encoded1, encoded2)\n\n\ndef dummy_lm_generator(vocab_size: int, max_len: int, batch_size: int, steps: int, easy: bool = True):  # identity\n    def dummy_generator():\n        for _ in range(steps):\n            seq_len = random.randint(1, max_len - 1)\n            tokens = [random.randrange(vocab_size) for i in range(seq_len)]\n            tokens[-1] = eos_id\n            yield Sentence(\n                tokens=tokens,\n                padding_mask=[True] * seq_len,\n                segments=[0] * seq_len,\n                token_classification={\n                    'lm': TokenTaskData(tokens if easy else [random.randrange(vocab_size) for i in range(seq_len)],\n                                        [True] * seq_len),\n                    'lm_untied': TokenTaskData(\n                        tokens if easy else [random.randrange(vocab_size) for i in range(seq_len)], [True] * seq_len)\n                },\n                sentence_classification={'count': SentenceTaskData(seq_len % 2, seq_len - 1)}\n            )\n\n    pad_id = vocab_size + TextEncoder.PAD_OFFSET\n    eos_id = vocab_size + TextEncoder.EOS_OFFSET\n    generator = dummy_generator()\n    batch = []\n    for item in generator:\n        batch.append(item)\n        if len(batch) == batch_size:\n            batch = _create_batch(batch, pad_id, max_len)\n            yield batch\n            batch = []\n"""
data/vocab.py,0,"b""import os\nfrom typing import List, Optional\n\ntry:\n    import sentencepiece as spm\nexcept:\n    print('if you want sentencepiece encoder, please install sentencepiece')\n\ntry:\n    from openai.text_utils import TextEncoder as _OpenAITextEncoder\nexcept:\n    print('if you want to use OpenAI\\'s encoder and pretrained model, please install spacy, and ftfy')\n\ntry:\n    from google_bert.tokenization import FullTokenizer\nexcept:\n    print('if you want to use Google\\'s encoder and pretrained models, please clone the bert submodule')\n\n\n# TOKEN_IDs = {unk=0, vocab={1..vocab_size-1}, specials(pad,bos,del,eos,msk)}\n\n\nclass TextEncoder:\n    PAD_OFFSET = 0\n    MSK_OFFSET = 1\n    BOS_OFFSET = 2\n    DEL_OFFSET = 3  # delimiter\n    EOS_OFFSET = 4\n    SPECIAL_COUNT = 5\n    NUM_SEGMENTS = 2\n    BERT_UNUSED_COUNT = 99  # bert pretrained models\n    BERT_SPECIAL_COUNT = 4  # they don't have DEL\n\n    def __init__(self, vocab_size: int):\n        # NOTE you MUST always put unk at 0, then regular vocab, then special tokens, and then pos\n        self.vocab_size = vocab_size\n        self.unk_id = 0\n        self.pad_id = vocab_size + self.PAD_OFFSET\n        self.msk_id = vocab_size + self.MSK_OFFSET\n        self.bos_id = vocab_size + self.BOS_OFFSET\n        self.del_id = vocab_size + self.DEL_OFFSET\n        self.eos_id = vocab_size + self.EOS_OFFSET\n\n    def __len__(self) -> int:\n        return self.vocab_size\n\n    def encode(self, sent: str) -> List[int]:\n        raise NotImplementedError()\n\n\nclass SentencePieceTextEncoder(TextEncoder):\n    def __init__(self, text_corpus_address: Optional[str], model_name: str = 'spm',\n                 vocab_size: int = 30000, spm_model_type: str = 'unigram') -> None:\n        super().__init__(vocab_size)\n        if not os.path.exists('{}.model'.format(model_name)):\n            if spm_model_type.lower() not in ('unigram', 'bpe', 'char', 'word'):\n                raise ValueError(\n                    '{} is not a valid model_type for sentence piece, '\n                    'valid options are: unigram, bpe, char, word'.format(spm_model_type))\n            spm.SentencePieceTrainer.Train(\n                '--input={input} --model_prefix={model_name} --vocab_size={vocab_size} '\n                '--character_coverage={coverage} --model_type={model_type} '\n                '--pad_id=-1 --unk_id=0 --bos_id=-1 --eos_id=-1 --input_sentence_size=100000000 '.format(\n                    input=text_corpus_address, model_name=model_name, vocab_size=vocab_size, coverage=1,\n                    model_type=spm_model_type.lower()))\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load('{}.model'.format(model_name))\n\n    def encode(self, sent: str) -> List[int]:\n        return self.sp.encode_as_ids(sent)\n\n\nclass OpenAITextEncoder(TextEncoder):\n    def __init__(self, encoder_path: str = './openai/model/encoder_bpe_40000.json',\n                 bpe_path: str = './openai/model/vocab_40000.bpe') -> None:\n        self.encoder = _OpenAITextEncoder(encoder_path, bpe_path)\n        super().__init__(len(self.encoder.encoder))\n\n    def encode(self, sent: str) -> List[int]:\n        return self.encoder.encode([sent], verbose=False)[0]\n\n\nclass BERTTextEncoder(TextEncoder):\n    def __init__(self, vocab_file: str, do_lower_case: bool = True) -> None:\n        self.tokenizer = FullTokenizer(vocab_file, do_lower_case)\n        super().__init__(len(self.tokenizer.vocab))\n        self.bert_unk_id = self.tokenizer.vocab['[UNK]']\n        self.bert_msk_id = self.tokenizer.vocab['[MASK]']\n\n    def standardize_ids(self, ids: List[int]) -> List[int]:\n        for i in range(len(ids)):\n            if ids[i] == self.bert_unk_id:  # UNK\n                ids[i] = 0\n            else:  # VOCAB\n                ids[i] -= self.bert_msk_id\n        return ids\n\n    def encode(self, sent: str) -> List[int]:\n        return self.standardize_ids(self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sent)))\n"""
tests/__init__.py,0,"b""'''The file contains nothing but avoid the module conflict.'''"""
tests/test_bert.py,15,"b'import unittest\nimport numpy as np\n\nfrom transformer import refresh_keras_backend\nrefresh_keras_backend(use_tpu=False) # there are too many settings to use tpu on tensorflow model, so using gpu for the test may be great\n\nimport tensorflow as tf\nfrom transformer.load import load_google_bert\nfrom data.vocab import TextEncoder, BERTTextEncoder\nfrom google_bert.modeling import BertConfig, BertModel, get_assignment_map_from_checkpoint\n\n\nclass TestBert(unittest.TestCase):\n    def __init__(self, method_name: str = \'runTest\') -> None:\n        super().__init__(methodName=method_name)\n\n    def test_same_result(self):\n        base_location = \'./google_bert/downloads/multilingual_L-12_H-768_A-12/\'\n        bert_config = BertConfig.from_json_file(base_location + \'bert_config.json\')\n        init_checkpoint = base_location + \'bert_model.ckpt\'\n\n        def model_fn_builder(bert_config, init_checkpoint):\n            """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n            def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n                """"""The `model_fn` for TPUEstimator.""""""\n\n                unique_ids = features[""unique_ids""]\n                input_ids = features[""input_ids""]\n                input_mask = features[""input_mask""]\n                input_type_ids = features[""input_type_ids""]\n\n                model = BertModel(\n                    config=bert_config,\n                    is_training=False,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    token_type_ids=input_type_ids,\n                    use_one_hot_embeddings=False)\n\n                if mode != tf.estimator.ModeKeys.PREDICT:\n                    raise ValueError(""Only PREDICT modes are supported: %s"" % (mode))\n\n                tvars = tf.trainable_variables()\n                scaffold_fn = None\n                (assignment_map, _) = get_assignment_map_from_checkpoint(\n                    tvars, init_checkpoint)\n                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n                predictions = {\n                    ""unique_id"": unique_ids,\n                    ""seq_out"": model.get_sequence_output()\n                }\n\n                output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n                    mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n                return output_spec\n\n            return model_fn\n\n        batch_size = 8\n        seq_len = 5\n        xmb = np.random.randint(106, bert_config.vocab_size - 106, (batch_size, seq_len))\n        xmb2 = np.random.randint(0, 2, (batch_size, seq_len), dtype=np.int32)\n        xmb3 = np.random.randint(0, 2, (batch_size, seq_len), dtype=np.int32)\n\n        def input_fn(params):\n            d = tf.data.Dataset.from_tensor_slices({\n                ""unique_ids"":\n                    tf.constant([0, 1, 2], shape=[batch_size], dtype=tf.int32),\n                ""input_ids"":\n                    tf.constant(\n                        xmb, shape=[batch_size, seq_len],\n                        dtype=tf.int32),\n                ""input_mask"":\n                    tf.constant(\n                        xmb2,\n                        shape=[batch_size, seq_len],\n                        dtype=tf.int32),\n                ""input_type_ids"":\n                    tf.constant(\n                        xmb3,\n                        shape=[batch_size, seq_len],\n                        dtype=tf.int32),\n            })\n\n            d = d.batch(batch_size=batch_size, drop_remainder=False)\n            return d\n\n        model_fn = model_fn_builder(bert_config=bert_config, init_checkpoint=init_checkpoint)\n        is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n        run_config = tf.contrib.tpu.RunConfig(master=None, tpu_config=tf.contrib.tpu.TPUConfig(num_shards=8,\n                                                                                               per_host_input_for_training=is_per_host))\n        estimator = tf.contrib.tpu.TPUEstimator(use_tpu=False, model_fn=model_fn, config=run_config,\n                                                predict_batch_size=batch_size)\n        tf_result = [r for r in estimator.predict(input_fn)]\n\n        import keras.backend as K\n\n        K.set_learning_phase(0)\n        my_model = load_google_bert(base_location, max_len=seq_len)\n\n        from data.dataset import create_attention_mask, generate_pos_ids\n\n        pos = generate_pos_ids(batch_size, seq_len)\n        k_mask = create_attention_mask(xmb2, False, None, None, True)\n        bert_encoder = BERTTextEncoder(base_location + \'vocab.txt\')\n        for b in range(len(xmb)):\n            xmb[b] = np.array(bert_encoder.standardize_ids(xmb[b].tolist()))\n        k_output = my_model.predict([xmb, xmb3, pos, k_mask])\n        max_max = 0\n        for i in range(batch_size):\n            if k_mask[i].mean() != 0:  # TODO (when mask == full zero, keras_res != tf_res)\n                new_max = np.abs(k_output[i] - tf_result[i][\'seq_out\']).max()\n                if new_max > max_max:\n                    max_max = new_max\n        assert max_max < 5e-5, max_max  # TODO reduce the error (I think it\'s because of the LayerNorm)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_data.py,0,"b""import os\nimport re\nimport random\nimport tempfile\nimport numpy as np\nfrom unittest import TestCase\nfrom typing import Optional, List\nfrom data.vocab import TextEncoder\nfrom data.dataset import (create_attention_mask, Sentence, pad, check_sent_len,\n                          msk_sentence, SentenceTaskData, TokenTaskData)\nfrom data.lm_dataset import _create_batch, _grab_line, make_next_token_prediction, dummy_lm_generator\n\n\nclass TestData(TestCase):\n    def __init__(self, method_name: str = 'runTest') -> None:\n        super().__init__(methodName=method_name)\n        self.vocab_size = 100\n\n    def setUp(self) -> None:\n        pass\n\n    def generate_random_seq(self, length: int, max: Optional[int] = None) -> List[int]:\n        return [random.randrange(self.vocab_size if max is None else max) for i in range(length)]\n\n    def generate_random_mask(self, length: int) -> List[bool]:\n        return [random.random() < 0.5 for _ in range(length)]\n\n    def generate_sentence(self, length: int) -> Sentence:\n        return Sentence(self.generate_random_seq(length), [True] * length, [0] * length,\n                        {'lm': TokenTaskData(self.generate_random_seq(length),\n                                             self.generate_random_mask(length))}, {})\n\n    def test_pad(self):\n        bert_sent = self.generate_sentence(5)\n        lm_orig = bert_sent.token_classification['lm']\n        pad_id = self.vocab_size + TextEncoder.PAD_OFFSET\n        padded_sent = pad(bert_sent, pad_id, 10)\n        lm = padded_sent.token_classification['lm']\n        assert len(padded_sent.padding_mask) == len(padded_sent.segments) == len(lm.target_mask) == len(\n            padded_sent.tokens) == len(lm.target) == 10\n        for i in range(5):\n            assert padded_sent.padding_mask[i]\n            assert padded_sent.segments[i] == bert_sent.segments[i]\n            assert lm.target[i] == lm_orig.target[i]\n            assert lm.target_mask[i] == lm_orig.target_mask[i]\n            assert padded_sent.tokens[i] == bert_sent.tokens[i]\n        for i in range(5, 10):\n            assert not padded_sent.padding_mask[i]\n            assert padded_sent.segments[i] == 0\n            assert lm.target[i] == 0\n            assert lm.target_mask[i] == 0\n            assert padded_sent.tokens[i] == pad_id\n\n    def test_create_batch(self):\n        max_len = 64\n        pad_id = self.vocab_size + TextEncoder.PAD_OFFSET\n        for batch_size in [32, 1]:\n            sentences = []\n            for i in range(batch_size):\n                sentences.append(self.generate_sentence(random.randint(1, max_len - 5)))\n            for i in range(2):\n                if i == 0:\n                    batch = _create_batch(sentences, pad_id, max_len)\n                else:\n                    batch = _create_batch(sentences, pad_id)\n                    max_len = max([len(sent.tokens) for sent in sentences])\n                assert batch.tokens.shape == (batch_size, max_len)\n                assert batch.tokens.dtype == np.int32\n                assert batch.segments.shape == (batch_size, max_len)\n                assert batch.segments.dtype == np.int32\n                assert batch.padding_mask.shape == (batch_size, max_len)\n                assert batch.padding_mask.dtype == np.int8\n                assert batch.token_classification['lm'].target.shape == (batch_size, max_len)\n                assert batch.token_classification['lm'].target.dtype == np.int32\n                assert batch.token_classification['lm'].target_mask.shape == (batch_size, max_len)\n                assert batch.token_classification['lm'].target_mask.dtype == np.int32\n\n    def test_msk_sentence(self):\n        seq_len = 32\n        sentence = self.generate_random_seq(seq_len)\n\n        masked_sentence = msk_sentence(sentence, vocab_size=self.vocab_size, keep_prob=1.0, mask_prob=0.0,\n                                       rand_prob=0.0)\n        assert len(sentence) == len(masked_sentence.tokens) == len(\n            masked_sentence.token_classification['lm'].target) == len(\n            masked_sentence.token_classification['lm'].target_mask)\n        for i in range(seq_len):\n            assert masked_sentence.tokens[i] == sentence[i]\n            assert masked_sentence.token_classification['lm'].target[i] == 0\n            assert masked_sentence.token_classification['lm'].target_mask[i] == 0\n\n        masked_sentence = msk_sentence(sentence, vocab_size=self.vocab_size, keep_prob=0.0, mask_prob=1.0,\n                                       rand_prob=0.0)\n        assert len(sentence) == len(masked_sentence.tokens) == len(\n            masked_sentence.token_classification['lm'].target) == len(\n            masked_sentence.token_classification['lm'].target_mask)\n        for i in range(seq_len):\n            assert masked_sentence.tokens[i] == self.vocab_size + TextEncoder.MSK_OFFSET\n            assert masked_sentence.token_classification['lm'].target[i] == sentence[i]\n            assert masked_sentence.token_classification['lm'].target_mask[i] == 1\n\n        masked_sentence = msk_sentence(sentence, vocab_size=self.vocab_size, keep_prob=0.0, mask_prob=0.0,\n                                       rand_prob=0.0)\n        assert len(sentence) == len(masked_sentence.tokens) == len(\n            masked_sentence.token_classification['lm'].target) == len(\n            masked_sentence.token_classification['lm'].target_mask)\n        for i in range(seq_len):\n            assert masked_sentence.tokens[i] == sentence[i]\n            assert masked_sentence.token_classification['lm'].target[i] == sentence[i]\n            assert masked_sentence.token_classification['lm'].target_mask[i] == 1\n\n        sentence = [index + self.vocab_size for index in sentence]\n        masked_sentence = msk_sentence(sentence, vocab_size=self.vocab_size, keep_prob=0.0, mask_prob=0.0,\n                                       rand_prob=1.0)\n        assert len(sentence) == len(masked_sentence.tokens) == len(\n            masked_sentence.token_classification['lm'].target) == len(\n            masked_sentence.token_classification['lm'].target_mask)\n        for i in range(seq_len):\n            assert masked_sentence.tokens[i] != sentence[i]\n            assert masked_sentence.token_classification['lm'].target[i] == sentence[i]\n            assert masked_sentence.token_classification['lm'].target_mask[i] == 1\n\n    def test_make_causal(self):\n        pad_id = self.vocab_size + TextEncoder.PAD_OFFSET\n        orig_sentence = self.generate_sentence(5)\n        result = _create_batch(make_next_token_prediction([orig_sentence]), pad_id)\n        lm = result.token_classification['lm']\n        assert (np.array(orig_sentence.tokens)[1:] == lm.target[0, :-1]).all()\n        assert lm.target[0, -1] == 0\n        assert (lm.target_mask[0, :-1] == 1).all()\n        assert lm.target_mask[0, -1] == 0\n\n    def test_grab_line(self):\n        fp1 = tempfile.TemporaryFile(mode='w+')\n        fp2 = tempfile.TemporaryFile(mode='w+')\n        for i in range(100):\n            fp1.write('hello world {}!\\n'.format(i))\n            fp2.write('hi universe {}!\\n'.format(i))\n        fp1.seek(0)\n        fp2.seek(0)\n        for i in range(200):\n            line = _grab_line([fp1], os.stat(fp1.fileno()).st_size, jump_prob=0.0)\n            assert line == 'hello world {}!\\n'.format(i % 100)\n        fp1.seek(0)\n        i = j = 0\n        for _ in range(200):\n            line = _grab_line([fp1, fp2], os.stat(fp1.fileno()).st_size, jump_prob=0.0)\n            if line.startswith('hello'):\n                assert line == 'hello world {}!\\n'.format(i % 100)\n                i += 1\n            else:\n                assert line == 'hi universe {}!\\n'.format(j % 100)\n                j += 1\n        fp1.seek(0)\n        fp2.seek(0)\n        pattern = re.compile('(hello world)|(hi universe) \\d+!\\\\n')\n        for _ in range(200):\n            line = _grab_line([fp1, fp2], os.stat(fp1.fileno()).st_size, jump_prob=1.0)\n            assert pattern.match(line) is not None\n        fp1.close()\n        fp2.close()\n\n    def test_create_mask(self):\n        batch_size = 3\n        length = 5\n        pad_mask = np.array([[1, 1, 1, 0, 0], [1, 1, 0, 0, 1], [0, 0, 0, 0, 0]], dtype=np.int8)\n        is_causal = False\n        mask = create_attention_mask(pad_mask, is_causal)\n        assert mask.shape == (batch_size, 1, length, length)\n        assert mask.dtype == np.float32\n        assert (mask[0, 0] == np.array([[1, 1, 1, 0, 0],\n                                        [1, 1, 1, 0, 0],\n                                        [1, 1, 1, 0, 0],\n                                        [0, 0, 0, 0, 0],\n                                        [0, 0, 0, 0, 0]], dtype=np.float32)).all()\n        assert (mask[1, 0] == np.array([[1, 1, 0, 0, 1],\n                                        [1, 1, 0, 0, 1],\n                                        [0, 0, 0, 0, 0],\n                                        [0, 0, 0, 0, 0],\n                                        [1, 1, 0, 0, 1]], dtype=np.float32)).all()\n        assert (mask[2, 0] == np.zeros((length, length), dtype=np.float32)).all()\n\n        is_causal = True\n        mask = create_attention_mask(pad_mask, is_causal)\n        assert mask.shape == (batch_size, 1, length, length)\n        assert mask.dtype == np.float32\n        assert (mask[0, 0] == np.array([[1, 0, 0, 0, 0],\n                                        [1, 1, 0, 0, 0],\n                                        [1, 1, 1, 0, 0],\n                                        [0, 0, 0, 0, 0],\n                                        [0, 0, 0, 0, 0]], dtype=np.float32)).all()\n        assert (mask[1, 0] == np.array([[1, 0, 0, 0, 0],\n                                        [1, 1, 0, 0, 0],\n                                        [0, 0, 0, 0, 0],\n                                        [0, 0, 0, 0, 0],\n                                        [1, 1, 0, 0, 1]], dtype=np.float32)).all()\n        assert (mask[2, 0] == np.zeros((length, length), dtype=np.float32)).all()\n\n        is_causal = False\n        mask = create_attention_mask(None, is_causal, batch_size, length)\n        assert mask.shape == (batch_size, 1, length, length)\n        assert mask.dtype == np.float32\n        for i in range(3):\n            assert (mask[i, 0] == np.ones((length, length), dtype=np.float32)).all()\n\n        is_causal = True\n        mask = create_attention_mask(None, is_causal, batch_size, length)\n        assert mask.shape == (batch_size, 1, length, length)\n        assert mask.dtype == np.float32\n        tri = np.array([[1, 0, 0, 0, 0],\n                        [1, 1, 0, 0, 0],\n                        [1, 1, 1, 0, 0],\n                        [1, 1, 1, 1, 0],\n                        [1, 1, 1, 1, 1]], dtype=np.float32)\n        for i in range(3):\n            assert (mask[i, 0] == tri).all()\n\n    def test_check_sent_len(self):\n        orig_length = 10\n        class_target = 2\n        original_sent = self.generate_sentence(orig_length)\n        original_sent.sentence_classification['sc'] = SentenceTaskData(class_target, 0)\n        original_sent.sentence_classification['sc_ok'] = SentenceTaskData(class_target + 1, 5)\n        assert check_sent_len(original_sent, min_len=10, max_len=None) is not None\n        assert check_sent_len(original_sent, min_len=11, max_len=None) is None\n        res = check_sent_len(original_sent, min_len=None, max_len=7, from_end=False)\n        assert len(res.tokens) == len(res.padding_mask) == len(res.token_classification['lm'].target) == len(\n            res.token_classification['lm'].target_mask) == 7\n        assert res.tokens[0] == original_sent.tokens[3]\n        assert set(res.sentence_classification.keys()) == {'sc_ok'}\n        assert res.sentence_classification['sc_ok'].target == class_target + 1\n        assert res.sentence_classification['sc_ok'].target_index == 5 - 3\n\n    def test_generation(self):\n        lm_generator = dummy_lm_generator(self.vocab_size, 32, 32, 100)\n        for i, sentence_batch in enumerate(lm_generator):\n            assert sentence_batch.tokens.shape == (32, 32)\n        assert i == 100 // 32 - 1\n"""
tests/test_transformer.py,6,"b'import os\nimport uuid\nimport json\n\nfrom transformer import refresh_keras_backend\nrefresh_keras_backend(use_tpu=False) # tpu mode doesn\'t support switch backend to theano\n\nimport keras\nimport numpy as np\nfrom importlib import reload\nfrom keras import backend as K\nfrom data.vocab import TextEncoder\nfrom unittest import TestCase, SkipTest\nfrom data.lm_dataset import dummy_lm_generator\nfrom transformer.train import train_model, load_model\nfrom transformer.model import create_transformer\nfrom transformer.load import load_openai_transformer\nfrom transformer.layers import MultiHeadAttention, LayerNormalization, Gelu\nfrom data.dataset import create_attention_mask, TaskMetadata, TaskWeightScheduler\n\n\ndef set_keras_backend(backend):\n    global K\n    if K.backend() != backend:\n        os.environ[\'KERAS_BACKEND\'] = backend\n        reload(K)\n        assert K.backend() == backend\n\n\nclass TestTransformer(TestCase):\n    def __init__(self, method_name: str = \'runTest\') -> None:\n        super().__init__(methodName=method_name)\n        self.vocab_size = 23\n        self.num_heads = 2\n        self.num_layers = 2\n        self.embedding_dim = 6\n        self.d_hid = 12\n        self.max_len = 7\n        self.supported_backends = {\'tensorflow\', \'theano\'}\n        self.original_backend = K.backend()\n\n    def tearDown(self):\n        set_keras_backend(self.original_backend)\n\n    def list_backends(self, orig_backend=None):\n        if orig_backend is None:\n            orig_backend = K.backend()\n        # always start from the default backend\n        return [orig_backend] + list(self.supported_backends - {orig_backend})\n\n    def create_small_model(self, use_attn_mask: bool):\n        return create_transformer(vocab_size=self.vocab_size,\n                                  num_heads=self.num_heads, num_layers=self.num_layers,\n                                  embedding_dim=self.embedding_dim, d_hid=self.d_hid,\n                                  max_len=self.max_len, use_attn_mask=use_attn_mask)\n\n    @staticmethod\n    def compare_two_models(model_a, model_b):\n        assert len(model_a.weights) == len(model_b.weights)\n        for x, y in zip(model_a.weights, model_b.weights):\n            assert (K.eval(x) == K.eval(y)).all()\n\n    def test_train(self):\n        model = self.create_small_model(use_attn_mask=True)\n        batch_size = 3\n        generator = dummy_lm_generator(self.vocab_size, self.max_len, batch_size, 10000, False)\n        tasks_meta_data = [TaskMetadata(\'lm\', True, self.vocab_size + TextEncoder.SPECIAL_COUNT, 0.1,\n                                        TaskWeightScheduler(True, False)),\n                           TaskMetadata(\'lm_untied\', True, self.vocab_size + TextEncoder.SPECIAL_COUNT, 0.3,\n                                        TaskWeightScheduler(False, True)),\n                           TaskMetadata(\'count\', False, 2, 0.1, TaskWeightScheduler(True, True))]\n        model = train_model(model, True, tasks_meta_data, generator, generator, pretrain_steps=100, pretrain_epochs=3,\n                            finetune_steps=50, finetune_epochs=2, verbose=0)\n        path = \'/tmp/{}.model\'.format(uuid.uuid4())\n        model.save_weights(path)\n        loaded_model = load_model(path, self.create_small_model(use_attn_mask=True), tasks_meta_data)\n        assert len(model.inputs) == len(loaded_model.inputs)\n        assert len(model.outputs) == len(loaded_model.outputs)\n        self.compare_two_models(model, loaded_model)\n\n    def test_save_load_all(self):\n        for backend in self.list_backends():\n            try:\n                set_keras_backend(backend)\n            except ModuleNotFoundError:\n                continue\n            K.set_learning_phase(0)  # test\n            for use_attn_mask in [True, False]:\n                model = self.create_small_model(use_attn_mask)\n                path = \'/tmp/{}.model\'.format(uuid.uuid4())\n                try:\n                    model.save(path)\n                    new_model = keras.models.load_model(path, custom_objects={\'MultiHeadAttention\': MultiHeadAttention,\n                                                                              \'LayerNormalization\': LayerNormalization,\n                                                                              \'Gelu\': Gelu})\n                    TestTransformer.compare_two_models(model, new_model)\n                except Exception as e:\n                    raise e\n                finally:\n                    if os.path.exists(path):\n                        os.remove(path)\n\n    def test_save_load_weights(self):\n        for backend in self.list_backends():\n            try:\n                set_keras_backend(backend)\n            except ModuleNotFoundError:\n                continue\n            K.set_learning_phase(0)  # test\n            for use_attn_mask in [True, False]:\n                model = self.create_small_model(use_attn_mask)\n                path = \'/tmp/{}.model\'.format(uuid.uuid4())\n                try:\n                    model.save_weights(path)\n                    model.load_weights(path)\n                except Exception as e:\n                    raise e\n                finally:\n                    if os.path.exists(path):\n                        os.remove(path)\n\n    def test_same_result(self):\n        orig_backend = K.backend()\n        batch_size = 3\n        xmb = np.random.randint(0, self.vocab_size, (batch_size, self.max_len, 2), dtype=np.int32)\n        xmb[:, :, 1] = np.random.randint(0, self.max_len, (batch_size, self.max_len), dtype=np.int32)\n        for use_attn_mask in [True, False]:\n            inputs = [xmb[:, :, 0], np.zeros((batch_size, self.max_len), dtype=np.int32), xmb[:, :, 1]]\n            results_x = {}\n            if use_attn_mask:\n                mask = create_attention_mask(None, True, batch_size, self.max_len)\n                inputs.append(mask)\n            for backend in self.list_backends(orig_backend):\n                try:\n                    set_keras_backend(backend)\n                except ModuleNotFoundError:\n                    continue\n                K.set_learning_phase(0)  # test\n                model = self.create_small_model(use_attn_mask)\n                model = load_openai_transformer(use_attn_mask=use_attn_mask, max_len=self.max_len,\n                                                use_one_embedding_dropout=True)\n                results_x[backend] = model.predict(inputs, batch_size=batch_size)\n                del model\n            set_keras_backend(orig_backend)\n            for k1 in results_x.keys():\n                for k2 in results_x.keys():\n                    if k1 == k2:\n                        continue\n                    assert np.allclose(results_x[k1], results_x[k2], atol=1.e-4, rtol=1.e-4)\n\n    def test_different_backends_work(self):\n        for use_attn_mask in [True, False]:\n            orig_backend = K.backend()\n            for backend in self.list_backends(orig_backend):\n                try:\n                    set_keras_backend(backend)\n                except ModuleNotFoundError:\n                    pass\n                K.set_learning_phase(0)  # test\n                model = self.create_small_model(use_attn_mask)\n                del model\n            set_keras_backend(orig_backend)\n\n    def test_different_backends_load_openai(self):\n        try:\n            import tensorflow as tf\n        except ImportError:\n            raise SkipTest(\'tensorflow is not installed, so we can not compare results with the released model\')\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n        from openai.train import dropout, embed, block, find_trainable_variables\n\n        n_vocab = 40478\n        n_ctx = 7\n        n_embd = 768\n        embd_pdrop = 0.1\n        n_layer = 12\n        n_batch_train = 2\n        n_transfer = 1 + 12 * 12\n\n        def model(X, train=False, reuse=False):\n            with tf.variable_scope(\'model\', reuse=reuse):\n                we = tf.get_variable(""we"", [n_vocab + TextEncoder.SPECIAL_COUNT + n_ctx, n_embd],\n                                     initializer=tf.random_normal_initializer(stddev=0.02))\n                we = dropout(we, embd_pdrop, train)\n                h = embed(X, we)\n                for layer in range(n_layer):\n                    h = block(h, \'h%d\' % layer, train=train, scale=True)\n                return h\n\n        X_train = tf.placeholder(tf.int32, [n_batch_train, n_ctx, 2])\n        res = model(X_train)\n\n        params = find_trainable_variables(\'model\')\n        sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        sess.run(tf.global_variables_initializer())\n\n        with open(\'openai/model/params_shapes.json\') as f:\n            shapes = json.load(f)\n        offsets = np.cumsum([np.prod(shape) for shape in shapes])\n        init_params = [np.load(\'openai/model/params_{}.npy\'.format(n)) for n in range(10)]\n        init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n        init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n        init_params[0] = init_params[0][:n_ctx]\n        init_params[0] = np.concatenate(\n            [init_params[1], (np.random.randn(TextEncoder.SPECIAL_COUNT, n_embd) * 0.02).astype(np.float32),\n             init_params[0]], 0)\n        del init_params[1]\n\n        sess.run([p.assign(ip) for p, ip in zip(params[:n_transfer], init_params[:n_transfer])])\n        xmb = np.random.randint(0, n_vocab, (n_batch_train, n_ctx, 2))\n        xmb[:, :, 1] = np.random.randint(0, n_ctx, (n_batch_train, n_ctx))\n        xmb_tf = xmb.copy()\n        xmb_tf[:, :, 1] += n_vocab + TextEncoder.SPECIAL_COUNT\n        tf_result = sess.run(res, {X_train: xmb_tf})\n\n        for backend in self.list_backends():\n            try:\n                set_keras_backend(backend)\n            except ModuleNotFoundError:\n                continue\n            K.set_learning_phase(0)\n            keras_model = load_openai_transformer(use_attn_mask=True, use_one_embedding_dropout=False, max_len=n_ctx)\n            mask = create_attention_mask(None, True, n_batch_train, n_ctx)\n            k_result = keras_model.predict(\n                [xmb[:, :, 0], np.zeros((n_batch_train, n_ctx), dtype=np.int64), xmb[:, :, 1], mask],\n                batch_size=n_batch_train)\n\n            if K.backend() != \'tensorflow\':\n                assert np.allclose(tf_result, k_result, atol=1.e-4, rtol=1.e-4)\n            else:\n                assert (tf_result == k_result).all()\n'"
transformer/__init__.py,6,"b""'''This file is for compatibility.'''\r\n\r\nimport sys\r\n\r\n\r\ndef tpu_compatible():\r\n    '''Fit the tpu problems we meet while using keras tpu model'''\r\n    if not hasattr(tpu_compatible, 'once'):\r\n        tpu_compatible.once = True\r\n    else:\r\n        return\r\n    import tensorflow as tf\r\n    import tensorflow.keras.backend as K\r\n    _version = tf.__version__.split('.')\r\n    is_correct_version = int(_version[0]) >= 1 and (int(_version[0]) >= 2 or int(_version[1]) >= 13)\r\n    from tensorflow.contrib.tpu.python.tpu.keras_support import KerasTPUModel\r\n    def initialize_uninitialized_variables():\r\n        sess = K.get_session()\r\n        uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\r\n        init_op = tf.variables_initializer(\r\n            [v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables]\r\n        )\r\n        sess.run(init_op)\r\n\r\n    _tpu_compile = KerasTPUModel.compile\r\n\r\n    def tpu_compile(self,\r\n                    optimizer,\r\n                    loss=None,\r\n                    metrics=None,\r\n                    loss_weights=None,\r\n                    sample_weight_mode=None,\r\n                    weighted_metrics=None,\r\n                    target_tensors=None,\r\n                    **kwargs):\r\n        if not is_correct_version:\r\n            raise ValueError('You need tensorflow >= 1.3 for better keras tpu support!')\r\n        _tpu_compile(self, optimizer, loss, metrics, loss_weights,\r\n                     sample_weight_mode, weighted_metrics,\r\n                     target_tensors, **kwargs)\r\n        initialize_uninitialized_variables()  # for unknown reason, we should run this after compile sometimes\r\n\r\n    KerasTPUModel.compile = tpu_compile\r\n\r\n\r\ndef replace_keras_to_tf_keras():\r\n    tpu_compatible()\r\n    import tensorflow as tf\r\n    sys.modules['keras'] = tf.keras\r\n    globals()['keras'] = tf.keras\r\n    import keras.backend as K\r\n    K.tf = tf\r\n\r\n\r\ndef clean_keras_module():\r\n    modules = [i for i in sys.modules.keys()]\r\n    for i in modules:\r\n        if i.split('.')[0] == 'keras':\r\n            del sys.modules[i]\r\n\r\n\r\ndef refresh_keras_backend(use_tpu=True):\r\n    clean_keras_module()\r\n    import keras.backend as K\r\n    if use_tpu and K.backend() != 'theano':\r\n        clean_keras_module()\r\n        replace_keras_to_tf_keras()\r\n        import keras.backend as K\r\n    return K\r\n\r\n\r\nrefresh_keras_backend()\r\n"""
transformer/embedding.py,0,"b""import keras\nimport numpy as np\nfrom data.vocab import TextEncoder\nfrom transformer.layers import LayerNormalization\n\n\ndef _get_pos_encoding_matrix(max_len: int, d_emb: int) -> np.array:\n    pos_enc = np.array(\n        [[pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] if pos != 0 else np.zeros(d_emb) for pos in\n         range(max_len)], dtype=np.float32)\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n    return pos_enc\n\n\n# NOTE that for vocab_size you should also add special_count\nclass Embedding(keras.layers.Layer):\n    def __init__(self, output_dim: int = 768, dropout: float = 0.1, vocab_size: int = 30000 + TextEncoder.SPECIAL_COUNT,\n                 max_len: int = 512, trainable_pos_embedding: bool = True, use_one_dropout: bool = False,\n                 use_embedding_layer_norm: bool = False, layer_norm_epsilon: float = 1e-5, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.use_one_dropout = use_one_dropout\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.vocab_size = vocab_size\n        self.trainable_pos_embedding = trainable_pos_embedding\n\n        self.segment_emb = keras.layers.Embedding(TextEncoder.NUM_SEGMENTS, output_dim, input_length=max_len,\n                                                  name='SegmentEmbedding')\n        if not trainable_pos_embedding:\n            self.pos_emb = keras.layers.Embedding(max_len, output_dim, trainable=False, input_length=max_len,\n                                                  name='PositionEmbedding',\n                                                  weights=[_get_pos_encoding_matrix(max_len, output_dim)])\n        else:\n            self.pos_emb = keras.layers.Embedding(max_len, output_dim, input_length=max_len, name='PositionEmbedding')\n        self.token_emb = keras.layers.Embedding(vocab_size, output_dim, input_length=max_len, name='TokenEmbedding')\n        self.embedding_dropout = keras.layers.Dropout(dropout, name='EmbeddingDropOut')\n        self.add_embeddings = keras.layers.Add(name='AddEmbeddings')\n        self.use_embedding_layer_norm = use_embedding_layer_norm\n        if self.use_embedding_layer_norm:\n            self.embedding_layer_norm = LayerNormalization(layer_norm_epsilon)\n        else:\n            self.embedding_layer_norm = None\n        self.layer_norm_epsilon = layer_norm_epsilon\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][0], input_shape[0][1], self.output_dim\n\n    def get_config(self):\n        config = {\n            'max_len': self.max_len,\n            'use_one_dropout': self.use_one_dropout,\n            'output_dim': self.output_dim,\n            'dropout': self.dropout,\n            'vocab_size': self.vocab_size,\n            'trainable_pos_embedding': self.trainable_pos_embedding,\n            'embedding_layer_norm': self.use_embedding_layer_norm,\n            'layer_norm_epsilon': self.layer_norm_epsilon\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def __call__(self, inputs, **kwargs):\n        tokens, segment_ids, pos_ids = inputs\n        segment_embedding = self.segment_emb(segment_ids)\n        pos_embedding = self.pos_emb(pos_ids)\n        token_embedding = self.token_emb(tokens)\n        if self.use_one_dropout:\n            summation = self.add_embeddings([segment_embedding, pos_embedding, token_embedding])\n            if self.embedding_layer_norm:\n                summation = self.embedding_layer_norm(summation)\n            return self.embedding_dropout(summation)\n        summation = self.add_embeddings(\n            [self.embedding_dropout(segment_embedding), self.embedding_dropout(pos_embedding),\n             self.embedding_dropout(token_embedding)])\n        if self.embedding_layer_norm:\n            summation = self.embedding_layer_norm(summation)\n        return summation\n"""
transformer/funcs.py,0,"b""import math\nimport numpy as np\nimport keras.backend as K\nfrom keras.layers import Dropout\n\n\ndef shape_list(x):\n    if K.backend() != 'theano':\n        tmp = K.int_shape(x)\n    else:\n        tmp = x.shape\n    tmp = list(tmp)\n    tmp[0] = -1\n    return tmp\n\n\ndef split_heads(x, n: int, k: bool = False):  # B, L, C\n    x_shape = shape_list(x)\n    m = x_shape[-1]\n    new_x_shape = x_shape[:-1] + [n, m // n]\n    new_x = K.reshape(x, new_x_shape)\n    return K.permute_dimensions(new_x, [0, 2, 3, 1] if k else [0, 2, 1, 3])\n\n\ndef merge_heads(x):\n    new_x = K.permute_dimensions(x, [0, 2, 1, 3])\n    x_shape = shape_list(new_x)\n    new_x_shape = x_shape[:-2] + [np.prod(x_shape[-2:])]\n    return K.reshape(new_x, new_x_shape)\n\n\n# q and v are B, H, L, C//H ; k is B, H, C//H, L ; mask is B, 1, L, L\ndef scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n    w = K.batch_dot(q, k)  # w is B, H, L, L\n    w = w / K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n    if attn_mask is not None:\n        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n    w = K.softmax(w)\n    w = Dropout(attention_dropout)(w)\n    return K.batch_dot(w, v)  # it is B, H, L, C//H [like v]\n\n\ndef scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n    w = theano_matmul(q, k)\n    w = w / K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n    if attn_mask is not None:\n        attn_mask = K.repeat_elements(attn_mask, shape_list(v)[1], 1)\n        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n    w = K.T.exp(w - w.max()) / K.T.exp(w - w.max()).sum(axis=-1, keepdims=True)\n    w = Dropout(attention_dropout)(w)\n    return theano_matmul(w, v)\n\n\ndef multihead_attention(x, attn_mask, n_head: int, n_state: int, attention_dropout: float, neg_inf: float):\n    _q, _k, _v = x[:, :, :n_state], x[:, :, n_state:2 * n_state], x[:, :, -n_state:]\n    q = split_heads(_q, n_head)  # B, H, L, C//H\n    k = split_heads(_k, n_head, k=True)  # B, H, C//H, L\n    v = split_heads(_v, n_head)  # B, H, L, C//H\n    if K.backend() == 'tensorflow':\n        a = scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout, neg_inf)\n    else:\n        a = scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout, neg_inf)\n    return merge_heads(a)\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + K.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n\n\n# https://stackoverflow.com/a/42194662/2796084\ndef theano_matmul(a, b, _left=False):\n    assert a.ndim == b.ndim\n    ndim = a.ndim\n    assert ndim >= 2\n    if _left:\n        b, a = a, b\n    if ndim == 2:\n        return K.T.dot(a, b)\n    else:\n        # If a is broadcastable but b is not.\n        if a.broadcastable[0] and not b.broadcastable[0]:\n            # Scan b, but hold a steady.\n            # Because b will be passed in as a, we need to left multiply to maintain\n            #  matrix orientation.\n            output, _ = K.theano.scan(theano_matmul, sequences=[b], non_sequences=[a[0], 1])\n        # If b is broadcastable but a is not.\n        elif b.broadcastable[0] and not a.broadcastable[0]:\n            # Scan a, but hold b steady.\n            output, _ = K.theano.scan(theano_matmul, sequences=[a], non_sequences=[b[0]])\n        # If neither dimension is broadcastable or they both are.\n        else:\n            # Scan through the sequences, assuming the shape for this dimension is equal.\n            output, _ = K.theano.scan(theano_matmul, sequences=[a, b])\n        return output\n"""
transformer/layers.py,1,"b""import math\nimport keras.backend as K\nfrom keras.layers import Layer\nfrom keras.initializers import Ones, Zeros\nfrom transformer.funcs import gelu, multihead_attention\n\n\nclass MultiHeadAttention(Layer):\n    def __init__(self, n_head: int, n_state: int, attention_dropout: float, use_attn_mask: bool, neg_inf: float,\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.n_head = n_head\n        self.n_state = n_state\n        self.attention_dropout = attention_dropout\n        self.use_attn_mask = use_attn_mask\n        self.neg_inf = neg_inf\n\n    def compute_output_shape(self, input_shape):\n        x = input_shape[0] if self.use_attn_mask else input_shape\n        return x[0], x[1], x[2] // 3\n\n    def call(self, inputs, **kwargs):\n        x = inputs[0] if self.use_attn_mask else inputs\n        attn_mask = inputs[1] if self.use_attn_mask else None\n        return multihead_attention(x, attn_mask, self.n_head, self.n_state, self.attention_dropout, self.neg_inf)\n\n    def get_config(self):\n        config = {\n            'n_head': self.n_head,\n            'n_state': self.n_state,\n            'attention_dropout': self.attention_dropout,\n            'use_attn_mask': self.use_attn_mask,\n            'neg_inf': self.neg_inf,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass LayerNormalization(Layer):\n    def __init__(self, eps: float = 1e-5, **kwargs) -> None:\n        self.eps = eps\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n        super().build(input_shape)\n\n    def call(self, x, **kwargs):\n        u = K.mean(x, axis=-1, keepdims=True)\n        s = K.mean(K.square(x - u), axis=-1, keepdims=True)\n        z = (x - u) / K.sqrt(s + self.eps)\n        return self.gamma * z + self.beta\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            'eps': self.eps,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Gelu(Layer):\n    def __init__(self, accurate: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.accurate = accurate\n\n    def call(self, inputs, **kwargs):\n        if not self.accurate:\n            return gelu(inputs)\n        if K.backend() == 'tensorflow':\n            erf = K.tf.erf\n        else:\n            erf = K.T.erf\n        return inputs * 0.5 * (1.0 + erf(inputs / math.sqrt(2.0)))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def get_config(self):\n        config = {\n            'accurate': self.accurate,\n        }\n        base_config = super().get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n"""
transformer/load.py,2,"b""import json\nimport keras\nimport numpy as np\nimport tensorflow as tf\nimport keras.backend as K\nfrom data.vocab import TextEncoder\nfrom google_bert.modeling import BertConfig\nfrom transformer.model import create_transformer\n\n\ndef load_openai_transformer(path: str = './openai/model/', use_attn_mask: bool = True,\n                            use_one_embedding_dropout: bool = False, max_len: int = 512) -> keras.Model:\n    with open(path + 'params_shapes.json') as f:\n        shapes = json.load(f)\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n    init_params = [np.load(path + 'params_{}.npy'.format(n)) for n in range(10)]\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n    init_params[0] = init_params[0][:min(512, max_len)]\n    # add special token embedding to token embedding\n    init_params[1] = np.concatenate(\n        (init_params[1], np.random.randn(TextEncoder.SPECIAL_COUNT, 768).astype(np.float32) * 0.02), axis=0)\n    init_params = [np.zeros((TextEncoder.NUM_SEGMENTS, 768)).astype(np.float32)] + init_params  # segment embedding\n    model = create_transformer(embedding_dim=768, embedding_dropout=0.1, vocab_size=40478,\n                               max_len=min(512, max_len), use_attn_mask=use_attn_mask, trainable_pos_embedding=True,\n                               num_heads=12, num_layers=12, use_one_embedding_dropout=use_one_embedding_dropout,\n                               d_hid=4 * 768, attention_dropout=0.1, residual_dropout=0.1)\n    model.set_weights(init_params)\n    return model\n\n\ndef load_google_bert(base_location: str = './google_bert/downloads/multilingual_L-12_H-768_A-12/',\n                     use_attn_mask: bool = True, max_len: int = 512, verbose: bool = False) -> keras.Model:\n    bert_config = BertConfig.from_json_file(base_location + 'bert_config.json')\n    init_checkpoint = base_location + 'bert_model.ckpt'\n    var_names = tf.train.list_variables(init_checkpoint)\n    check_point = tf.train.load_checkpoint(init_checkpoint)\n    vocab_size = bert_config.vocab_size - TextEncoder.BERT_SPECIAL_COUNT - TextEncoder.BERT_UNUSED_COUNT\n    model = create_transformer(embedding_layer_norm=True, neg_inf=-10000.0, use_attn_mask=use_attn_mask,\n                               vocab_size=vocab_size, accurate_gelu=True, layer_norm_epsilon=1e-12, max_len=max_len,\n                               use_one_embedding_dropout=True, d_hid=bert_config.intermediate_size,\n                               embedding_dim=bert_config.hidden_size, num_layers=bert_config.num_hidden_layers,\n                               num_heads=bert_config.num_attention_heads,\n                               residual_dropout=bert_config.hidden_dropout_prob,\n                               attention_dropout=bert_config.attention_probs_dropout_prob)\n    if K.backend() == 'tensorflow':\n        weights = [np.zeros(w.shape) for w in model.weights]\n    else:\n        weights = [np.zeros(w.get_value().shape) for w in model.weights]\n    for var_name, _ in var_names:\n        w_id = None\n        qkv = None\n        unsqueeze = False\n        parts = var_name.split('/')\n        first_vars_size = 5\n        if parts[1] == 'embeddings':\n            n = parts[-1]\n            if n == 'token_type_embeddings':\n                w_id = 0\n            elif n == 'position_embeddings':\n                w_id = 1\n            elif n == 'word_embeddings':\n                w_id = 2\n            elif n == 'gamma':\n                w_id = 3\n            elif n == 'beta':\n                w_id = 4\n            else:\n                raise ValueError()\n        elif parts[2].startswith('layer_'):\n            layer_number = int(parts[2][len('layer_'):])\n            if parts[3] == 'attention':\n                if parts[-1] == 'beta':\n                    w_id = first_vars_size + layer_number * 12 + 5\n                elif parts[-1] == 'gamma':\n                    w_id = first_vars_size + layer_number * 12 + 4\n                elif parts[-2] == 'dense':\n                    if parts[-1] == 'bias':\n                        w_id = first_vars_size + layer_number * 12 + 3\n                    elif parts[-1] == 'kernel':\n                        w_id = first_vars_size + layer_number * 12 + 2\n                        unsqueeze = True\n                    else:\n                        raise ValueError()\n                elif parts[-2] == 'key' or parts[-2] == 'query' or parts[-2] == 'value':\n                    w_id = first_vars_size + layer_number * 12 + (0 if parts[-1] == 'kernel' else 1)\n                    unsqueeze = parts[-1] == 'kernel'\n                    qkv = parts[-2][0]\n                else:\n                    raise ValueError()\n            elif parts[3] == 'intermediate':\n                if parts[-1] == 'bias':\n                    w_id = first_vars_size + layer_number * 12 + 7\n                elif parts[-1] == 'kernel':\n                    w_id = first_vars_size + layer_number * 12 + 6\n                    unsqueeze = True\n                else:\n                    raise ValueError()\n            elif parts[3] == 'output':\n                if parts[-1] == 'beta':\n                    w_id = first_vars_size + layer_number * 12 + 11\n                elif parts[-1] == 'gamma':\n                    w_id = first_vars_size + layer_number * 12 + 10\n                elif parts[-1] == 'bias':\n                    w_id = first_vars_size + layer_number * 12 + 9\n                elif parts[-1] == 'kernel':\n                    w_id = first_vars_size + layer_number * 12 + 8\n                    unsqueeze = True\n                else:\n                    raise ValueError()\n\n        if w_id is not None and qkv is None:\n            if verbose:\n                print(var_name, ' -> ', model.weights[w_id].name)\n            if w_id == 1:  # pos embedding\n                weights[w_id][:max_len, :] = check_point.get_tensor(var_name)[:max_len,\n                                             :] if not unsqueeze else check_point.get_tensor(var_name)[\n                                                                      None, :max_len, :]\n            elif w_id == 2:  # word embedding\n                # ours: unk, [vocab], pad, msk(mask), bos(cls), del(use sep again), eos(sep)\n                # theirs: pad, 99 unused, unk, cls, sep, mask, [vocab]\n                saved = check_point.get_tensor(var_name)  # vocab_size, emb_size\n                # weights[our_position] = saved[their_position]\n                weights[w_id][0] = saved[1 + TextEncoder.BERT_UNUSED_COUNT]  # unk\n                weights[w_id][1:vocab_size] = saved[-vocab_size + 1:]\n                weights[w_id][vocab_size + TextEncoder.PAD_OFFSET] = saved[0]\n                weights[w_id][vocab_size + TextEncoder.MSK_OFFSET] = saved[4 + TextEncoder.BERT_UNUSED_COUNT]\n                weights[w_id][vocab_size + TextEncoder.BOS_OFFSET] = saved[2 + TextEncoder.BERT_UNUSED_COUNT]\n                weights[w_id][vocab_size + TextEncoder.DEL_OFFSET] = saved[3 + TextEncoder.BERT_UNUSED_COUNT]\n                weights[w_id][vocab_size + TextEncoder.EOS_OFFSET] = saved[3 + TextEncoder.BERT_UNUSED_COUNT]\n            else:\n                weights[w_id][:] = check_point.get_tensor(var_name) if not unsqueeze else \\\n                    check_point.get_tensor(var_name)[\n                        None, ...]\n        elif w_id is not None:\n            if verbose:\n                print(var_name, ' -> ', model.weights[w_id].name, '::', qkv)\n            p = {'q': 0, 'k': 1, 'v': 2}[qkv]\n            if weights[w_id].ndim == 3:\n                dim_size = weights[w_id].shape[1]\n                weights[w_id][0, :, p * dim_size:(p + 1) * dim_size] = check_point.get_tensor(\n                    var_name) if not unsqueeze else \\\n                    check_point.get_tensor(var_name)[\n                        None, ...]\n            else:\n                dim_size = weights[w_id].shape[0] // 3\n                weights[w_id][p * dim_size:(p + 1) * dim_size] = check_point.get_tensor(var_name)\n        else:\n            if verbose:\n                print('not mapped: ', var_name)  # TODO pooler, cls/predictions, cls/seq_relationship\n    model.set_weights(weights)\n    return model\n"""
transformer/model.py,0,"b""import keras\nimport keras.backend as K\nfrom data.vocab import TextEncoder\nfrom transformer.embedding import Embedding\nfrom keras.layers import Conv1D, Dropout, Add, Input\nfrom transformer.layers import MultiHeadAttention, Gelu, LayerNormalization\n\n\nclass MultiHeadSelfAttention:\n    def __init__(self, n_state: int, n_head: int, attention_dropout: float,\n                 use_attn_mask: bool, layer_id: int, neg_inf: float) -> None:\n        assert n_state % n_head == 0\n        self.c_attn = Conv1D(3 * n_state, 1, name='layer_{}/c_attn'.format(layer_id))\n        self.attn = MultiHeadAttention(n_head, n_state, attention_dropout, use_attn_mask,\n                                       neg_inf, name='layer_{}/self_attention'.format(layer_id))\n        self.c_attn_proj = Conv1D(n_state, 1, name='layer_{}/c_attn_proj'.format(layer_id))\n\n    def __call__(self, x, mask):\n        output = self.c_attn(x)\n        output = self.attn(output) if mask is None else self.attn([output, mask])\n        return self.c_attn_proj(output)\n\n\nclass PositionWiseFF:\n    def __init__(self, n_state: int, d_hid: int, layer_id: int, accurate_gelu: bool) -> None:\n        self.c_fc = Conv1D(d_hid, 1, name='layer_{}/c_fc'.format(layer_id))\n        self.activation = Gelu(accurate=accurate_gelu, name='layer_{}/gelu'.format(layer_id))\n        self.c_ffn_proj = Conv1D(n_state, 1, name='layer_{}/c_ffn_proj'.format(layer_id))\n\n    def __call__(self, x):\n        output = self.activation(self.c_fc(x))\n        return self.c_ffn_proj(output)\n\n\nclass EncoderLayer:\n    def __init__(self, n_state: int, n_head: int, d_hid: int, residual_dropout: float, attention_dropout: float,\n                 use_attn_mask: bool, layer_id: int, neg_inf: float, ln_epsilon: float, accurate_gelu: bool) -> None:\n        self.attention = MultiHeadSelfAttention(n_state, n_head, attention_dropout, use_attn_mask, layer_id, neg_inf)\n        self.drop1 = Dropout(residual_dropout, name='layer_{}/ln_1_drop'.format(layer_id))\n        self.add1 = Add(name='layer_{}/ln_1_add'.format(layer_id))\n        self.ln1 = LayerNormalization(ln_epsilon, name='layer_{}/ln_1'.format(layer_id))\n        self.ffn = PositionWiseFF(n_state, d_hid, layer_id, accurate_gelu)\n        self.drop2 = Dropout(residual_dropout, name='layer_{}/ln_2_drop'.format(layer_id))\n        self.add2 = Add(name='layer_{}/ln_2_add'.format(layer_id))\n        self.ln2 = LayerNormalization(ln_epsilon, name='layer_{}/ln_2'.format(layer_id))\n\n    def __call__(self, x, mask):\n        a = self.attention(x, mask)\n        n = self.ln1(self.add1([x, self.drop1(a)]))\n        f = self.ffn(n)\n        return self.ln2(self.add2([n, self.drop2(f)]))\n\n\ndef create_transformer(embedding_dim: int = 768, embedding_dropout: float = 0.1, vocab_size: int = 30000,\n                       max_len: int = 512, trainable_pos_embedding: bool = True, num_heads: int = 12,\n                       num_layers: int = 12, attention_dropout: float = 0.1, use_one_embedding_dropout: bool = False,\n                       d_hid: int = 768 * 4, residual_dropout: float = 0.1, use_attn_mask: bool = True,\n                       embedding_layer_norm: bool = False, neg_inf: float = -1e9, layer_norm_epsilon: float = 1e-5,\n                       accurate_gelu: bool = False) -> keras.Model:\n    vocab_size += TextEncoder.SPECIAL_COUNT\n    tokens = Input(batch_shape=(None, max_len), name='token_input', dtype='int32')\n    segment_ids = Input(batch_shape=(None, max_len), name='segment_input', dtype='int32')\n    pos_ids = Input(batch_shape=(None, max_len), name='position_input', dtype='int32')\n    attn_mask = Input(batch_shape=(None, 1, max_len, max_len), name='attention_mask_input',\n                      dtype=K.floatx()) if use_attn_mask else None\n    inputs = [tokens, segment_ids, pos_ids]\n    embedding_layer = Embedding(embedding_dim, embedding_dropout, vocab_size, max_len, trainable_pos_embedding,\n                                use_one_embedding_dropout, embedding_layer_norm, layer_norm_epsilon)\n    x = embedding_layer(inputs)\n    for i in range(num_layers):\n        x = EncoderLayer(embedding_dim, num_heads, d_hid, residual_dropout,\n                         attention_dropout, use_attn_mask, i, neg_inf, layer_norm_epsilon, accurate_gelu)(x, attn_mask)\n    if use_attn_mask:\n        inputs.append(attn_mask)\n    return keras.Model(inputs=inputs, outputs=[x], name='Transformer')\n"""
transformer/train.py,5,"b""import keras\nimport numpy as np\nimport keras.backend as K\nfrom typing import List, Generator, Optional\nfrom keras.layers import Dropout, Input, Lambda, TimeDistributed, Dense\nfrom data.dataset import TaskMetadata, SentenceBatch, create_attention_mask, generate_pos_ids\n\n\ndef _mask_loss(y_true, y_pred, y_mask, element_wise_loss):\n    l = K.switch(y_mask, element_wise_loss(y_true, y_pred), K.zeros_like(y_mask, dtype=K.floatx()))\n    return K.sum(l) / (K.cast(K.sum(y_mask), dtype='float32') + K.epsilon())\n\n\ndef classification_loss(y_true, y_pred):\n    return K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n\n\ndef masked_classification_loss(y_true, y_pred, y_mask):\n    return _mask_loss(y_true, y_pred, y_mask, classification_loss)\n\n\ndef sparse_gather(y_pred, target_indices, task_name):\n    clf_h = Lambda(lambda x: K.reshape(x, (-1, K.int_shape(x)[-1])), name=task_name + '_flatten')(y_pred)\n    return Lambda(lambda x: K.gather(x[0], K.cast(x[1], 'int32')), name=task_name + '_gather')([clf_h, target_indices])\n\n\ndef pass_through_loss(y_true, y_pred):\n    return y_pred\n\n\ndef load_model(weights_path: str, base_model: keras.Model, tasks_meta_data: List[TaskMetadata]):\n    model = train_model(base_model, is_causal=False, tasks_meta_data=tasks_meta_data,\n                        pretrain_generator=None, finetune_generator=None)\n    model.load_weights(weights_path)\n    return model\n\n\ndef train_model(base_model: keras.Model, is_causal: bool, tasks_meta_data: List[TaskMetadata], pretrain_generator,\n                finetune_generator, pretrain_epochs: int = 1, pretrain_optimizer='adam', pretrain_steps: int = 1000000,\n                pretrain_callbacks=None, finetune_epochs: int = 1, finetune_optimizer='adam',\n                finetune_steps: int = 10000, finetune_callbacks=None, verbose: int = 0,\n                TPUStrategy: Optional['tf.contrib.tpu.TPUDistributionStrategy'] = None):\n    if TPUStrategy is not None:\n        import tensorflow as tf\n    token_input = base_model.inputs[0]\n    segment_input = base_model.inputs[1]\n    position_input = base_model.inputs[2]\n    uses_attn_mask = len(base_model.inputs) == 4\n    max_len = K.int_shape(base_model.inputs[0])[1]\n    if uses_attn_mask:\n        attention_mask_input = base_model.inputs[3]\n    all_logits = []\n    all_tasks = {task.name: task for task in tasks_meta_data}\n    task_nodes = {}\n    sent_level_mask_inputs = []\n    assert len(all_tasks) == len(tasks_meta_data)\n    for task in all_tasks.values():\n        task_loss_weight = Input(batch_shape=(None, 1), dtype='float32', name=task.name + '_loss_weight')\n        if task.is_token_level:\n            if task.name == 'lm':\n                decoder = Lambda(lambda x: K.dot(x, K.transpose(base_model.get_layer('TokenEmbedding').weights[0])),\n                                 name='lm_logits')\n            else:\n                decoder = Dense(units=task.num_classes, name=task.name + '_logits')\n            logits = TimeDistributed(decoder, name=task.name + '_logits_time_distributed')(\n                Dropout(task.dropout)(base_model.outputs[0]))\n            task_target = Input(batch_shape=(None, max_len,), dtype='int32', name=task.name + '_target_input')\n            task_mask = Input(batch_shape=(None, max_len), dtype='int8' if TPUStrategy is None else 'int32',\n                              name=task.name + '_mask_input')\n            task_loss = Lambda(lambda x: x[0] * masked_classification_loss(x[1], x[2], x[3]), name=task.name + '_loss')(\n                [task_loss_weight, task_target, logits, task_mask])\n        else:\n            task_mask = Input(batch_shape=(None, 1), dtype='int32', name=task.name + '_mask_input')\n            decoder_input = sparse_gather(base_model.outputs[0], task_mask, task.name)\n            logits = Dense(units=task.num_classes, name=task.name + '_logits')(Dropout(task.dropout)(decoder_input))\n            task_target = Input(batch_shape=(None, 1), dtype='int32', name=task.name + '_target_input')\n            task_loss = Lambda(lambda x: x[0] * classification_loss(x[1], x[2]), name=task.name + '_loss')(\n                [task_loss_weight, task_target, logits])\n            sent_level_mask_inputs.append(task_mask)\n        task_nodes[task.name] = {\n            'target': task_target,\n            'mask': task_mask,\n            'loss_weight': task_loss_weight,\n            'loss': task_loss,\n        }\n        all_logits.append(logits)\n\n    def get_generator(sentence_generator: Generator[SentenceBatch, None, None], is_pretrain: bool):\n        for i, batch in enumerate(sentence_generator):\n            batch_size, seq_len = batch.tokens.shape\n            x = [batch.tokens, batch.segments, generate_pos_ids(batch_size, max_len)]\n            y = []\n            if uses_attn_mask:\n                x.append(create_attention_mask(batch.padding_mask, is_causal))\n            for task_name in task_nodes.keys():\n                if is_pretrain:\n                    cond = all_tasks[task_name].weight_scheduler.active_in_pretrain\n                else:\n                    cond = all_tasks[task_name].weight_scheduler.active_in_finetune\n                if cond:\n                    if task_name in batch.sentence_classification:\n                        task_data_batch = batch.sentence_classification[task_name]\n                    else:\n                        task_data_batch = batch.token_classification[task_name]\n                    x.append(task_data_batch.target)\n                    if all_tasks[task_name].is_token_level:\n                        x.append(task_data_batch.target_mask)\n                    else:\n                        x.append((task_data_batch.target_mask + np.arange(batch_size) * seq_len).astype(np.int32))\n                    x.append(\n                        np.repeat(np.array(\n                            [all_tasks[task_name].weight_scheduler.get(is_pretrain, i)]), batch_size,\n                            0))\n                    y.append(np.repeat(np.array([0.0]), batch_size, 0))\n            yield x, y\n\n    def train_step(is_pretrain: bool):\n        _inputs = [token_input, segment_input, position_input]\n        _outputs = []\n        if uses_attn_mask:\n            _inputs.append(attention_mask_input)\n        for task_name in task_nodes.keys():\n            if is_pretrain:\n                cond = all_tasks[task_name].weight_scheduler.active_in_pretrain\n            else:\n                cond = all_tasks[task_name].weight_scheduler.active_in_finetune\n            if cond:\n                _inputs.append(task_nodes[task_name]['target'])\n                _inputs.append(task_nodes[task_name]['mask'])\n                _inputs.append(task_nodes[task_name]['loss_weight'])\n                _outputs.append(task_nodes[task_name]['loss'])\n        _generator = get_generator(pretrain_generator if is_pretrain else finetune_generator, is_pretrain)\n        _model = keras.Model(inputs=_inputs, outputs=_outputs)\n        if TPUStrategy is not None:\n            '''\n            Create TPUStrategy like this:\n            tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n            TPUStrategy = tf.contrib.tpu.TPUDistributionStrategy(\n                tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n            )\n            '''\n            _model = tf.contrib.tpu.keras_to_tpu_model(_model, strategy=TPUStrategy)\n        _model.compile(pretrain_optimizer if is_pretrain else finetune_optimizer, loss=pass_through_loss)\n        _model.fit_generator(_generator, steps_per_epoch=pretrain_steps if is_pretrain else finetune_steps,\n                             verbose=verbose, callbacks=pretrain_callbacks if is_pretrain else finetune_callbacks,\n                             shuffle=False, epochs=pretrain_epochs if is_pretrain else finetune_epochs)\n\n    if pretrain_generator is not None:\n        train_step(True)\n    if finetune_generator is not None:\n        train_step(False)\n\n    ret_model = keras.Model(inputs=base_model.inputs + sent_level_mask_inputs, outputs=all_logits)\n    if TPUStrategy is not None:\n        ret_model = tf.contrib.tpu.keras_to_tpu_model(ret_model, strategy=TPUStrategy)\n        # Compile for TPU model predicting for the first time. Also you can call compile for training after this\n        ret_model.compile(finetune_optimizer, loss=pass_through_loss)\n    return ret_model\n"""
