file_path,api_count,code
__init__.py,0,b''
bashlint/__init__.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom bashlint import bparser, tokenizer\n\nparse = bparser.parse\nparsesingle = bparser.parsesingle\nsplit = bparser.split\n'"
bashlint/bash.py,0,"b'""""""\nGazetteers for bash.\n""""""\nutility_stats = """"""\n1,find,103,5457,566\n2,xargs,32,981,90\n3,grep,82,871,90\n4,rm,17,519,40\n5,ls,84,351,40\n6,echo,5,325,30\n7,sort,50,317,30\n8,chmod,14,274,38\n9,wc,13,221,19\n10,cut,15,197,16\n11,head,12,163,18\n12,chown,15,156,17\n13,cat,19,154,13\n14,mv,20,143,21\n15,cp,49,140,10\n16,mkdir,10,132,17\n17,tail,20,119,18\n18,dirname,4,99,15\n19,tr,11,92,17\n20,uniq,20,91,8\n21,split,21,89,12\n22,tar,50,82,12\n23,readlink,18,80,6\n24,tee,6,76,7\n25,basename,8,74,5\n26,ln,29,74,7\n27,read,11,70,8\n28,rsync,172,68,10\n29,which,13,67,14\n30,mount,14,65,5\n31,ssh,43,58,5\n32,file,25,57,8\n33,pwd,6,54,4\n34,du,43,53,8\n35,md5sum,14,52,4\n36,ifconfig,12,50,5\n37,shopt,5,48,5\n38,od,33,46,6\n39,cd,3,46,4\n40,comm,9,45,4\n41,diff,71,44,3\n42,hostname,26,44,4\n43,df,27,43,4\n44,rename,12,42,5\n45,mktemp,12,42,7\n46,date,18,40,5\n47,nl,24,40,5\n48,column,6,39,7\n49,dig,13,39,1\n50,paste,6,38,2\n51,history,8,37,6\n52,rev,4,36,2\n53,zcat,10,36,8\n54,touch,15,35,1\n55,cal,7,34,1\n56,chgrp,15,34,4\n57,whoami,2,33,4\n58,ping,28,33,7\n59,gzip,34,32,4\n60,rmdir,7,32,5\n61,seq,8,31,10\n62,tree,37,29,2\n63,tac,8,28,6\n64,bzip2,34,28,2\n65,fold,10,28,3\n66,join,15,28,4\n67,cpio,54,27,4\n68,who,32,26,6\n69,pstree,30,25,3\n70,uname,20,24,4\n71,env,8,24,6\n72,kill,9,21,1\n""""""\n\ntop_100_utilities = {\n    \'echo\',\n    \'bash\',\n    \'sh\',\n    \'chgrp\',\n    \'cpio\',\n    \'file\',\n    \'rename\',\n    \'compress\',\n    \'pwd\',\n    \'cd\',\n    \'ls\',\n    \'mkdir\',\n    \'rmdir\',\n    \'cat\',\n    \'zcat\',\n    \'tac\',\n    \'cp\',\n    \'mv\',\n    \'rm\',\n    \'shred\',\n    \'head\',\n    \'tail\',\n    \'less\',\n    \'zless\',\n    \'more\',\n    \'grep\',\n    \'egrep\',\n    \'fgrep\',\n    \'which\',\n    \'chmod\',\n    \'chown\',\n    \'history\',\n    \'clear\',\n    \'logout\',\n    \'exit\',\n    \'sudo\',\n    \'su\',\n    \'wc\',\n    \'sort\',\n    \'ssh\',\n    \'ssh-keygen\',\n    \'scp\',\n    \'rsync\',\n    \'source\',\n    \'export\',\n    \'ln\',\n    \'readlink\',\n    \'sleep\',\n    \'ps\',\n    \'pstree\',\n    \'jobs\',\n    \'bg\',\n    \'fg\',\n    \'kill\',\n    \'top\',\n    \'nohup\',\n    \'time\',\n    \'seq\',\n    \'cut\',\n    \'paste\',\n    \'awk\',\n    \'sed\',\n    \'date\',\n    \'cal\',\n    \'gzip\',\n    \'gunzip\',\n    \'bzip2\',\n    \'bunzip2\',\n    \'tar\',\n    \'uniq\',\n    \'dirname\',\n    \'basename\',\n    \'set\',\n    \'unset\',\n    \'env\',\n    \'uname\',\n    \'df\',\n    \'du\',\n    \'bind\',\n    \'alias\',\n    \'unalias\',\n    \'column\',\n    \'find\',\n    \'touch\',\n    \'diff\',\n    \'comm\',\n    \'join\',\n    \'md5\',\n    \'md5sum\',\n    \'tr\',\n    \'od\',\n    \'split\',\n    \'nano\',\n    \'emacs\',\n    \'vim\',\n    \'tree\',\n    \'screen\',\n    \'tmux\',\n    \'yes\',\n    \'nl\',\n    \'whoami\',\n    \'groups\',\n    \'who\',\n    \'w\',\n    \'hostname\',\n    \'finger\',\n    \'read\',\n    \'tee\',\n    \'shopt\',\n    \'pushd\',\n    \'popd\',\n    \'true\',\n    \'false\',\n    \'shift\',\n    \'g++\',\n    \'xargs\',\n    \'crontab\',\n    \'info\',\n    \'apropos\',\n    \'fold\',\n    \'rev\',\n    \'mount\',\n    \'mktemp\',\n    \'watch\',\n    \'ping\',\n    \'dig\',\n    \'ifconfig\',\n    \'wget\',\n    \'elinks\',\n    \'curl\',\n    \'apt-get\',\n    \'brew\',\n    \'yum\'\n}\n\n# Compilers\nBLACK_LIST = {\n    \'cpp\',\n    \'g++\',\n    \'java\',\n    \'perl\',\n    \'python\',\n    \'ruby\',\n    \'nano\',\n    \'emacs\',\n    \'vim\',\n    \'awk\',\n    \'sed\',\n    \'less\',\n    \'more\',\n    \'screen\',\n    \'brew\',\n    \'yum\',\n    \'apt-get\',\n    \'tmux\'\n}\n\n# Flow controls\nGREY_LIST = {\n    \'alias\',\n    \'unalias\',\n    \'set\',\n    \'unset\',\n    \'source\',\n    \'export\',\n    \'shift\',\n    \'true\',\n    \'false\',\n    \'pushd\',\n    \'popd\'\n}\n\n# --- Linux Utility by Category --- #\n\ncategory_0_builtins = {\n    \'cd\',\n    \'jobs\',\n    \'bg\',\n    \'fg\',\n    \'set\',\n    \'unset\',\n    \'popd\',\n    \'pushd\',\n    \'source\',\n    \'shopt\',\n    \'set\'\n}\n\ncategory_1_user_commands = {\n    \'pwd\',\n    \'ls\',\n    \'mkdir\',\n    \'rmdir\',\n    \'echo\',\n    \'cat\',\n    \'gzip\',\n    \'gunzip\',\n    \'zcat\',\n    \'tac\',\n    \'cp\',\n    \'mv\',\n    \'rm\',\n    \'man\',\n    \'head\',\n    \'tail\',\n    \'less\',\n    \'zless\',\n    \'more\',\n    \'grep\',\n    \'egrep\',\n    \'fgrep\',\n    \'which\',\n    \'chmod\',\n    \'chown\',\n    \'chgrp\',\n    \'su\',\n    \'wc\',\n    \'sort\',\n    \'ssh\',\n    \'ssh-keygen\',\n    \'scp\',\n    \'rsync\',\n    \'ln\',\n    \'readlink\',\n    \'sleep\',\n    \'ps\',\n    \'pstree\',\n    \'kill\',\n    \'top\',\n    \'nohup\',\n    \'time\',\n    \'seq\',\n    \'cut\',\n    \'sed\',\n    \'paste\',\n    \'which\',\n    \'rename\',\n    \'screen\',\n    \'md5\',\n    \'wget\',\n    \'tmux\',\n    \'find\',\n    \'locate\',\n    \'updatedb\',\n    \'xargs\',\n    \'dig\'\n}\n\ncategory_2_system_calls = {}\n\ncategory_3_library_functions = {}\n\ncategory_7_conventions_and_miscellany = {}\n\ncategory_8_administration_and_privileged_commands = {\n    \'ping\',\n    \'sudo\',\n    \'mount\',\n    \'ifconfig\'\n}\n\nfindutils = {\n    \'find\',\n    \'locate\',\n    \'updatedb\',\n    \'xargs\'\n}\n\n# --- Other Constants Lists --- #\n\npattern_argument_types = {\n    \'Regex\',\n    \'File\',\n    \'Directory\',\n    \'Path\'\n}\n\nquantity_argument_types = {\n    \'Number\',\n    \'+Number\',\n    \'-Number\',\n    \'Quantity\',\n    \'+Quantity\',\n    \'-Quantity\',\n    \'Size\',\n    \'+Size\',\n    \'-Size\',\n    \'Timespan\',\n    \'+Timespan\',\n    \'-Timespan\',\n    \'DateTime\',\n    \'+DateTime\',\n    \'-DateTime\',\n    \'Permission\',\n    \'+Permission\',\n    \'-Permission\'\n}\n\nargument_types = pattern_argument_types | quantity_argument_types | \\\n                 {\'Type\', \'Unknown\'}\n\nfind_common_args = {\n    \'.\',\n    \'./\',\n    \'~\',\n    \'~/\',\n    \'/\',\n}\n\nreserved_tokens = {\n    \'+\',\n    \';\',\n    \'{}\'\n}\n\nright_associate_unary_logic_operators = {\n    \'!\',\n    \'-not\'\n}\n\nleft_associate_unary_logic_operators = {\n    \'-prune\'\n}\n\nbinary_logic_operators = {\n    \'-and\',\n    \'-or\',\n    \'||\',\n    \'&&\',\n    \'-o\',\n    \'-a\'\n}\n'"
bashlint/bast.py,0,"b'class node(object):\n    """"""\n    This class represents a node in the AST built while parsing command lines.\n    It\'s basically an object container for various attributes, with a slightly\n    specialised representation to make it a little easier to debug the parser.\n    """"""\n\n    def __init__(self, **kwargs):\n        assert \'kind\' in kwargs\n        self.__dict__.update(kwargs)\n\n    def dump(self, indent=\'  \'):\n        return _dump(self, indent)\n\n    def __repr__(self):\n        chunks = []\n        d = dict(self.__dict__)\n        kind = d.pop(\'kind\')\n        for k, v in sorted(d.items()):\n            chunks.append(\'%s=%r\' % (k, v))\n        return \'%sNode(%s)\' % (kind.title(), \' \'.join(chunks))\n\n    def __eq__(self, other):\n        if not isinstance(other, node):\n            return False\n        return self.__dict__ == other.__dict__\n\nclass nodevisitor(object):\n    def _visitnode(self, n, *args, **kwargs):\n        k = n.kind\n        self.visitnode(n)\n        return getattr(self, \'visit%s\' % k)(n, *args, **kwargs)\n\n    def visit(self, n):\n        k = n.kind\n        if k == \'operator\':\n            self._visitnode(n, n.op)\n        elif k == \'list\':\n            dochild = self._visitnode(n, n.parts)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k == \'reservedword\':\n            self._visitnode(n, n.word)\n        elif k == \'pipe\':\n            self._visitnode(n, n.pipe)\n        elif k == \'pipeline\':\n            dochild = self._visitnode(n, n.parts)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k == \'compound\':\n            dochild = self._visitnode(n, n.list, n.redirects)\n            if dochild is None or dochild:\n                for child in n.list:\n                    self.visit(child)\n                for child in n.redirects:\n                    self.visit(child)\n        elif k in (\'if\', \'for\', \'while\', \'until\'):\n            dochild = self._visitnode(n, n.parts)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k == \'command\':\n            dochild = self._visitnode(n, n.parts)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k == \'function\':\n            dochild = self._visitnode(n, n.name, n.body, n.parts)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k == \'redirect\':\n            dochild = self._visitnode(n, n.input, n.type, n.output, n.heredoc)\n            if dochild is None or dochild:\n                if isinstance(n.output, node):\n                    self.visit(n.output)\n                if n.heredoc:\n                    self.visit(n.heredoc)\n        elif k in (\'word\'):\n            dochild = self._visitnode(n, n.word)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k in (\'variable\'):\n            dochild = self._visitnode(n, n.word)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k in (\'string\'):\n            dochild = self._visitnode(n, n.word)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k in (\'number\'):\n            dochild = self._visitnode(n, n.word)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k in (\'assignment\'):\n            dochild = self._visitnode(n, n.word)\n            if dochild is None or dochild:\n                for child in n.parts:\n                    self.visit(child)\n        elif k in (\'parameter\', \'tilde\', \'heredoc\'):\n            self._visitnode(n, n.value)\n        elif k in (\'commandsubstitution\', \'processsubstitution\'):\n            dochild = self._visitnode(n, n.command)\n            if dochild is None or dochild:\n                self.visit(n.command)\n        else:\n            raise ValueError(\'unknown node kind %r\' % k)\n        self.visitnodeend(n)\n\n    def visitnode(self, n):\n        pass\n    def visitnodeend(self, n):\n        pass\n    def visitoperator(self, n, op):\n        pass\n    def visitlist(self, n, parts):\n        pass\n    def visitpipe(self, n, pipe):\n        pass\n    def visitpipeline(self, n, parts):\n        pass\n    def visitcompound(self, n, list, redirects):\n        pass\n    def visitif(self, node, parts):\n        pass\n    def visitfor(self, node, parts):\n        pass\n    def visitwhile(self, node, parts):\n        pass\n    def visituntil(self, node, parts):\n        pass\n    def visitcommand(self, n, parts):\n        pass\n    def visitfunction(self, n, name, body, parts):\n        pass\n    def visitword(self, n, word):\n        pass\n    def visitvariable(self, n, word):\n        pass\n    def visitassignment(self, n, parts):\n        pass\n    def visitreservedword(self, n, word):\n        pass\n    def visitparameter(self, n, value):\n        pass\n    def visittilde(self, n, value):\n        pass\n    def visitredirect(self, n, input, type, output, heredoc):\n        pass\n    def visitheredoc(self, n, value):\n        pass\n    def visitprocesssubstitution(self, n, command):\n        pass\n    def visitcommandsubstitution(self, n, command):\n        pass\n\ndef _dump(tree, indent=\'  \'):\n    def _format(n, level=0):\n        if isinstance(n, node):\n            d = dict(n.__dict__)\n            kind = d.pop(\'kind\')\n            if kind == \'list\' and level > 0:\n                level = level + 1\n            fields = []\n            v = d.pop(\'s\', None)\n            if v:\n                fields.append((\'s\', _format(v, level)))\n            for k, v in sorted(d.items()):\n                if not v or k == \'parts\':\n                    continue\n                llevel = level\n                if isinstance(v, node):\n                    llevel += 1\n                    fields.append((k, \'\\n\' + (indent * llevel) + _format(v, llevel)))\n                else:\n                    fields.append((k, _format(v, level)))\n            if kind == \'function\':\n                fields = [f for f in fields if f[0] not in (\'name\', \'body\')]\n            v = d.pop(\'parts\', None)\n            if v:\n                fields.append((\'parts\', _format(v, level)))\n            return \'\'.join([\n                \'%sNode\' % kind.title(),\n                \'(\',\n                \', \'.join((\'%s=%s\' % field for field in fields)),\n                \')\'])\n        elif isinstance(n, list):\n            lines = [\'[\']\n            lines.extend((indent * (level + 1) + _format(x, level + 1) + \',\'\n                         for x in n))\n            if len(lines) > 1:\n                lines.append(indent * (level) + \']\')\n            else:\n                lines[-1] += \']\'\n            return \'\\n\'.join(lines)\n        return repr(n)\n\n    if not isinstance(tree, node):\n        raise TypeError(\'expected node, got %r\' % tree.__class__.__name__)\n    return _format(tree)\n\ndef findfirstkind(parts, kind):\n    for i, node in enumerate(parts):\n        if node.kind == kind:\n            return i\n    return -1\n\nclass posconverter(nodevisitor):\n    def __init__(self, string):\n        self.string = string\n\n    def visitnode(self, node):\n        assert hasattr(node, \'pos\'), \'node %r is missing pos attr\' % node\n        start, end = node.__dict__.pop(\'pos\')\n        node.s = self.string[start:end]\n\nclass posshifter(nodevisitor):\n    def __init__(self, count):\n        self.count = count\n\n    def visitnode(self, node):\n        #assert node.pos[1] + base <= endlimit\n        node.pos = (node.pos[0] + self.count, node.pos[1] + self.count)\n'"
bashlint/bparser.py,0,"b'import os, copy\n\nfrom bashlint import yacc, tokenizer, state, bast, subst, flags, errors, heredoc\n\ndef _partsspan(parts):\n    return parts[0].pos[0], parts[-1].pos[1]\n\ntokens = [e.name for e in tokenizer.tokentype]\nprecedence = (\n    (\'left\', \'AMPERSAND\', \'SEMICOLON\', \'NEWLINE\', \'EOF\'),\n    (\'left\', \'AND_AND\', \'OR_OR\'),\n    (\'right\', \'BAR\', \'BAR_AND\')\n)\n\ndef p_inputunit(p):\n    \'\'\'inputunit : simple_list simple_list_terminator\n                 | NEWLINE\n                 | error NEWLINE\n                 | EOF\'\'\'\n    # XXX\n    if p.lexer._parserstate & flags.parser.CMDSUBST:\n        p.lexer._parserstate.add(flags.parser.EOFTOKEN)\n\n    if isinstance(p[1], bast.node):\n        p[0] = p[1]\n        # accept right here in case the input contains more lines that are\n        # not part of the current command\n        p.accept()\n\ndef p_word_list(p):\n    \'\'\'word_list : WORD\n                 | word_list WORD\'\'\'\n    parserobj = p.context\n    if len(p) == 2:\n        p[0] = [_expandword(parserobj, p.slice[1])]\n    else:\n        p[0] = p[1]\n        p[0].append(_expandword(parserobj, p.slice[2]))\n\ndef p_redirection_heredoc(p):\n    \'\'\'redirection : LESS_LESS WORD\n                   | NUMBER LESS_LESS WORD\n                   | REDIR_WORD LESS_LESS WORD\n                   | LESS_LESS_MINUS WORD\n                   | NUMBER LESS_LESS_MINUS WORD\n                   | REDIR_WORD LESS_LESS_MINUS WORD\'\'\'\n    parserobj = p.context\n    assert isinstance(parserobj, _parser)\n\n    output = bast.node(kind=\'word\', word=p[len(p)-1], parts=[],\n                      pos=p.lexspan(len(p)-1))\n    if len(p) == 3:\n        p[0] = bast.node(kind=\'redirect\', input=None, type=p[1], heredoc=None,\n                        output=output, pos=(p.lexpos(1), p.endlexpos(2)))\n    else:\n        p[0] = bast.node(kind=\'redirect\', input=p[1], type=p[2], heredoc=None,\n                        output=output, pos=(p.lexpos(1), p.endlexpos(3)))\n\n    if p.slice[len(p)-2].ttype == tokenizer.tokentype.LESS_LESS:\n        parserobj.redirstack.append((p[0], False))\n    else:\n        parserobj.redirstack.append((p[0], True))\n\ndef p_redirection(p):\n    \'\'\'redirection : GREATER WORD\n                   | LESS WORD\n                   | NUMBER GREATER WORD\n                   | NUMBER LESS WORD\n                   | REDIR_WORD GREATER WORD\n                   | REDIR_WORD LESS WORD\n                   | GREATER_GREATER WORD\n                   | NUMBER GREATER_GREATER WORD\n                   | REDIR_WORD GREATER_GREATER WORD\n                   | GREATER_BAR WORD\n                   | NUMBER GREATER_BAR WORD\n                   | REDIR_WORD GREATER_BAR WORD\n                   | LESS_GREATER WORD\n                   | NUMBER LESS_GREATER WORD\n                   | REDIR_WORD LESS_GREATER WORD\n                   | LESS_LESS_LESS WORD\n                   | NUMBER LESS_LESS_LESS WORD\n                   | REDIR_WORD LESS_LESS_LESS WORD\n                   | LESS_AND NUMBER\n                   | NUMBER LESS_AND NUMBER\n                   | REDIR_WORD LESS_AND NUMBER\n                   | GREATER_AND NUMBER\n                   | NUMBER GREATER_AND NUMBER\n                   | REDIR_WORD GREATER_AND NUMBER\n                   | LESS_AND WORD\n                   | NUMBER LESS_AND WORD\n                   | REDIR_WORD LESS_AND WORD\n                   | GREATER_AND WORD\n                   | NUMBER GREATER_AND WORD\n                   | REDIR_WORD GREATER_AND WORD\n                   | GREATER_AND DASH\n                   | NUMBER GREATER_AND DASH\n                   | REDIR_WORD GREATER_AND DASH\n                   | LESS_AND DASH\n                   | NUMBER LESS_AND DASH\n                   | REDIR_WORD LESS_AND DASH\n                   | AND_GREATER WORD\n                   | AND_GREATER_GREATER WORD\'\'\'\n    parserobj = p.context\n    if len(p) == 3:\n        output = p[2]\n        if p.slice[2].ttype == tokenizer.tokentype.WORD:\n            output = _expandword(parserobj, p.slice[2])\n        p[0] = bast.node(kind=\'redirect\', input=None, type=p[1], heredoc=None,\n                        output=output, pos=(p.lexpos(1), p.endlexpos(2)))\n    else:\n        output = p[3]\n        if p.slice[3].ttype == tokenizer.tokentype.WORD:\n            output = _expandword(parserobj, p.slice[3])\n        p[0] = bast.node(kind=\'redirect\', input=p[1], type=p[2], heredoc=None,\n                        output=output, pos=(p.lexpos(1), p.endlexpos(3)))\n\ndef _expandword(parser, tokenword):\n    if parser._expansionlimit == -1:\n        # we enter this branch in the following conditions:\n        # - currently parsing a substitution as a result of an expansion\n        # - the previous expansion had limit == 0\n        #\n        # this means that this node is a descendant of a substitution in an\n        # unexpanded word and will be filtered in the limit == 0 condition below\n        #\n        # (the reason we even expand when limit == 0 is to get quote removal)\n        node = bast.node(kind=\'word\', word=tokenword,\n                        pos=(tokenword.lexpos, tokenword.endlexpos), parts=[])\n        return node\n    else:\n        quoted = bool(tokenword.flags & flags.word.QUOTED)\n        doublequoted = quoted and tokenword.value[0] == \'""\'\n\n        # TODO set qheredocument\n        parts, expandedword = subst._expandwordinternal(parser,\n                                                        tokenword, 0,\n                                                        doublequoted, 0, 0)\n\n        # limit reached, don\'t include substitutions (still expanded to get\n        # quote removal though)\n        if parser._expansionlimit == 0:\n            parts = [node for node in parts if \'substitution\' not in node.kind]\n\n        node = bast.node(kind=\'word\', word=expandedword,\n                        pos=(tokenword.lexpos, tokenword.endlexpos), parts=parts)\n        return node\n\ndef p_simple_command_element(p):\n    \'\'\'simple_command_element : WORD\n                              | ASSIGNMENT_WORD\n                              | redirection\'\'\'\n    if isinstance(p[1], bast.node):\n        p[0] = [p[1]]\n        return\n\n    parserobj = p.context\n    p[0] = [_expandword(parserobj, p.slice[1])]\n\n    # change the word node to an assignment if necessary\n    if p.slice[1].ttype == tokenizer.tokentype.ASSIGNMENT_WORD:\n        p[0][0].kind = \'assignment\'\n\ndef p_redirection_list(p):\n    \'\'\'redirection_list : redirection\n                        | redirection_list redirection\'\'\'\n    if len(p) == 2:\n        p[0] = [p[1]]\n    else:\n        p[0] = p[1]\n        p[0].append(p[2])\n\ndef p_simple_command(p):\n    \'\'\'simple_command : simple_command_element\n                      | simple_command simple_command_element\'\'\'\n\n    p[0] = p[1]\n    if len(p) == 3:\n        p[0].extend(p[2])\n\ndef p_command(p):\n    \'\'\'command : simple_command\n               | shell_command\n               | shell_command redirection_list\n               | function_def\n               | coproc\'\'\'\n    if isinstance(p[1], bast.node):\n        p[0] = p[1]\n        if len(p) == 3:\n            assert p[0].kind == \'compound\'\n            p[0].redirects.extend(p[2])\n            assert p[0].pos[0] < p[0].redirects[-1].pos[1]\n            p[0].pos = (p[0].pos[0], p[0].redirects[-1].pos[1])\n    else:\n        p[0] = bast.node(kind=\'command\', parts=p[1], pos=_partsspan(p[1]))\n\ndef p_shell_command(p):\n    \'\'\'shell_command : for_command\n                     | case_command\n                     | WHILE compound_list DO compound_list DONE\n                     | UNTIL compound_list DO compound_list DONE\n                     | select_command\n                     | if_command\n                     | subshell\n                     | group_command\n                     | arith_command\n                     | cond_command\n                     | arith_for_command\'\'\'\n    if len(p) == 2:\n        p[0] = p[1]\n    else:\n        # while or until\n        assert p[2].kind == \'list\'\n\n        parts = _makeparts(p)\n        kind = parts[0].word\n        assert kind in (\'while\', \'until\')\n        p[0] = bast.node(kind=\'compound\',\n                        redirects=[],\n                        list=[bast.node(kind=kind, parts=parts, pos=_partsspan(parts))],\n                        pos=_partsspan(parts))\n\n    assert p[0].kind == \'compound\'\n\ndef _makeparts(p):\n    parts = []\n    for i in range(1, len(p)):\n        if isinstance(p[i], bast.node):\n            parts.append(p[i])\n        elif isinstance(p[i], list):\n            parts.extend(p[i])\n        elif isinstance(p.slice[i], tokenizer.token):\n            if p.slice[i].ttype == tokenizer.tokentype.WORD:\n                parserobj = p.context\n                parts.append(_expandword(parserobj, p.slice[i]))\n            else:\n                parts.append(bast.node(kind=\'reservedword\', word=p[i],\n                                      pos=p.lexspan(i)))\n        else:\n            pass\n\n    return parts\n\ndef p_for_command(p):\n    \'\'\'for_command : FOR WORD newline_list DO compound_list DONE\n                   | FOR WORD newline_list LEFT_CURLY compound_list RIGHT_CURLY\n                   | FOR WORD SEMICOLON newline_list DO compound_list DONE\n                   | FOR WORD SEMICOLON newline_list LEFT_CURLY compound_list RIGHT_CURLY\n                   | FOR WORD newline_list IN word_list list_terminator newline_list DO compound_list DONE\n                   | FOR WORD newline_list IN word_list list_terminator newline_list LEFT_CURLY compound_list RIGHT_CURLY\n                   | FOR WORD newline_list IN list_terminator newline_list DO compound_list DONE\n                   | FOR WORD newline_list IN list_terminator newline_list LEFT_CURLY compound_list RIGHT_CURLY\'\'\'\n    parts = _makeparts(p)\n    # find the operatornode that we might have there due to\n    # list_terminator/newline_list and convert it to a reservedword so its\n    # considered as part of the for loop\n    for i, part in enumerate(parts):\n        if part.kind == \'operator\' and part.op == \';\':\n            parts[i] = bast.node(kind=\'reservedword\', word=\';\', pos=part.pos)\n            break # there could be only one in there...\n\n    p[0] = bast.node(kind=\'compound\',\n                    redirects=[],\n                    list=[bast.node(kind=\'for\', parts=parts, pos=_partsspan(parts))],\n                    pos=_partsspan(parts))\n\ndef p_arith_for_command(p):\n    \'\'\'arith_for_command : FOR ARITH_FOR_EXPRS list_terminator newline_list DO compound_list DONE\n                         | FOR ARITH_FOR_EXPRS list_terminator newline_list LEFT_CURLY compound_list RIGHT_CURLY\n                         | FOR ARITH_FOR_EXPRS DO compound_list DONE\n                         | FOR ARITH_FOR_EXPRS LEFT_CURLY compound_list RIGHT_CURLY\'\'\'\n    raise NotImplementedError(\'arithmetic for\')\n\ndef p_select_command(p):\n    \'\'\'select_command : SELECT WORD newline_list DO list DONE\n                      | SELECT WORD newline_list LEFT_CURLY list RIGHT_CURLY\n                      | SELECT WORD SEMICOLON newline_list DO list DONE\n                      | SELECT WORD SEMICOLON newline_list LEFT_CURLY list RIGHT_CURLY\n                      | SELECT WORD newline_list IN word_list list_terminator newline_list DO list DONE\n                      | SELECT WORD newline_list IN word_list list_terminator newline_list LEFT_CURLY list RIGHT_CURLY\'\'\'\n    raise NotImplementedError(\'select command\')\n\ndef p_case_command(p):\n    \'\'\'case_command : CASE WORD newline_list IN newline_list ESAC\n                    | CASE WORD newline_list IN case_clause_sequence newline_list ESAC\n                    | CASE WORD newline_list IN case_clause ESAC\'\'\'\n    raise NotImplementedError (\'case command\')\n\ndef p_function_def(p):\n    \'\'\'function_def : WORD LEFT_PAREN RIGHT_PAREN newline_list function_body\n                    | FUNCTION WORD LEFT_PAREN RIGHT_PAREN newline_list function_body\n                    | FUNCTION WORD newline_list function_body\'\'\'\n    parts = _makeparts(p)\n    body = parts[-1]\n    name = parts[bast.findfirstkind(parts, \'word\')]\n\n    p[0] = bast.node(kind=\'function\', name=name, body=body, parts=parts,\n                    pos=_partsspan(parts))\n\ndef p_function_body(p):\n    \'\'\'function_body : shell_command\n                     | shell_command redirection_list\'\'\'\n    assert p[1].kind == \'compound\'\n\n    p[0] = p[1]\n    if len(p) == 3:\n        p[0].redirects.extend(p[2])\n        assert p[0].pos[0] < p[0].redirects[-1].pos[1]\n        p[0].pos = (p[0].pos[0], p[0].redirects[-1].pos[1])\n\ndef p_subshell(p):\n    \'\'\'subshell : LEFT_PAREN compound_list RIGHT_PAREN\'\'\'\n    lparen = bast.node(kind=\'reservedword\', word=p[1], pos=p.lexspan(1))\n    rparen = bast.node(kind=\'reservedword\', word=p[3], pos=p.lexspan(3))\n    parts = [lparen, p[2], rparen]\n    p[0] = bast.node(kind=\'compound\', list=parts, redirects=[],\n                    pos=_partsspan(parts))\n\ndef p_coproc(p):\n    \'\'\'coproc : COPROC shell_command\n              | COPROC shell_command redirection_list\n              | COPROC WORD shell_command\n              | COPROC WORD shell_command redirection_list\n              | COPROC simple_command\'\'\'\n    raise NotImplementedError(\'coproc\')\n\ndef p_if_command(p):\n    \'\'\'if_command : IF compound_list THEN compound_list FI\n                  | IF compound_list THEN compound_list ELSE compound_list FI\n                  | IF compound_list THEN compound_list elif_clause FI\'\'\'\n    # we currently don\'t distinguish the various lists that make up the\n    # command, because it\'s not needed later on. if there will be a need\n    # we can always add different nodes for elif/else.\n    parts = _makeparts(p)\n    p[0] = bast.node(kind=\'compound\',\n                    redirects=[],\n                    list=[bast.node(kind=\'if\', parts=parts, pos=_partsspan(parts))],\n                    pos=_partsspan(parts))\n\ndef p_group_command(p):\n    \'\'\'group_command : LEFT_CURLY compound_list RIGHT_CURLY\'\'\'\n    lcurly = bast.node(kind=\'reservedword\', word=p[1], pos=p.lexspan(1))\n    rcurly = bast.node(kind=\'reservedword\', word=p[3], pos=p.lexspan(3))\n    parts = [lcurly, p[2], rcurly]\n    p[0] = bast.node(kind=\'compound\', list=parts, redirects=[],\n                    pos=_partsspan(parts))\n\ndef p_arith_command(p):\n    \'\'\'arith_command : ARITH_CMD\'\'\'\n    raise NotImplementedError(\'arithmetic command\')\n\ndef p_cond_command(p):\n    \'\'\'cond_command : COND_START COND_CMD COND_END\'\'\'\n    raise NotImplementedError(\'cond command\')\n\ndef p_elif_clause(p):\n    \'\'\'elif_clause : ELIF compound_list THEN compound_list\n                   | ELIF compound_list THEN compound_list ELSE compound_list\n                   | ELIF compound_list THEN compound_list elif_clause\'\'\'\n    parts = []\n    for i in range(1, len(p)):\n        if isinstance(p[i], bast.node):\n            parts.append(p[i])\n        else:\n            parts.append(bast.node(kind=\'reservedword\', word=p[i], pos=p.lexspan(i)))\n    p[0] = parts\n\ndef p_case_clause(p):\n    \'\'\'case_clause : pattern_list\n                   | case_clause_sequence pattern_list\'\'\'\n    raise NotImplementedError(\'case clause\')\n\ndef p_pattern_list(p):\n    \'\'\'pattern_list : newline_list pattern RIGHT_PAREN compound_list\n                    | newline_list pattern RIGHT_PAREN newline_list\n                    | newline_list LEFT_PAREN pattern RIGHT_PAREN compound_list\n                    | newline_list LEFT_PAREN pattern RIGHT_PAREN newline_list\'\'\'\n    raise NotImplementedError(\'pattern list\')\n\ndef p_case_clause_sequence(p):\n    \'\'\'case_clause_sequence : pattern_list SEMI_SEMI\n                            | case_clause_sequence pattern_list SEMI_SEMI\n                            | pattern_list SEMI_AND\n                            | case_clause_sequence pattern_list SEMI_AND\n                            | pattern_list SEMI_SEMI_AND\n                            | case_clause_sequence pattern_list SEMI_SEMI_AND\'\'\'\n    raise NotImplementedError(\'case clause\')\n\ndef p_pattern(p):\n    \'\'\'pattern : WORD\n               | pattern BAR WORD\'\'\'\n    raise NotImplementedError(\'pattern\')\n\ndef p_list(p):\n    \'\'\'list : newline_list list0\'\'\'\n    p[0] = p[2]\n\ndef p_compound_list(p):\n    \'\'\'compound_list : list\n                     | newline_list list1\'\'\'\n    if len(p) == 2:\n        p[0] = p[1]\n    else:\n        parts = p[2]\n        if len(parts) > 1:\n            p[0] = bast.node(kind=\'list\', parts=parts, pos=_partsspan(parts))\n        else:\n            p[0] = parts[0]\n\ndef p_list0(p):\n    \'\'\'list0 : list1 NEWLINE newline_list\n             | list1 AMPERSAND newline_list\n             | list1 SEMICOLON newline_list\'\'\'\n    parts = p[1]\n    if len(parts) > 1 or p.slice[2].ttype != tokenizer.tokentype.NEWLINE:\n        parts.append(bast.node(kind=\'operator\', op=p[2], pos=p.lexspan(2)))\n        p[0] = bast.node(kind=\'list\', parts=parts, pos=_partsspan(parts))\n    else:\n        p[0] = parts[0]\n\ndef p_list1(p):\n    \'\'\'list1 : list1 AND_AND newline_list list1\n             | list1 OR_OR newline_list list1\n             | list1 AMPERSAND newline_list list1\n             | list1 SEMICOLON newline_list list1\n             | list1 NEWLINE newline_list list1\n             | pipeline_command\'\'\'\n    if len(p) == 2:\n        p[0] = [p[1]]\n    else:\n        p[0] = p[1]\n        # XXX newline\n        p[0].append(bast.node(kind=\'operator\', op=p[2], pos=p.lexspan(2)))\n        p[0].extend(p[len(p) - 1])\n\ndef p_simple_list_terminator(p):\n    \'\'\'simple_list_terminator : NEWLINE\n                              | EOF\'\'\'\n    pass\n\ndef p_list_terminator(p):\n    \'\'\'list_terminator : NEWLINE\n                       | SEMICOLON\n                       | EOF\'\'\'\n    if p[1] == \';\':\n        p[0] = bast.node(kind=\'operator\', op=\';\', pos=p.lexspan(1))\n\ndef p_newline_list(p):\n    \'\'\'newline_list : empty\n                    | newline_list NEWLINE\'\'\'\n    pass\n\ndef p_simple_list(p):\n    \'\'\'simple_list : simple_list1\n                   | simple_list1 AMPERSAND\n                   | simple_list1 SEMICOLON\'\'\'\n    tok = p.lexer\n    heredoc.gatherheredocuments(tok)\n\n    if len(p) == 3 or len(p[1]) > 1:\n        parts = p[1]\n        if len(p) == 3:\n            parts.append(bast.node(kind=\'operator\', op=p[2], pos=p.lexspan(2)))\n        p[0] = bast.node(kind=\'list\', parts=parts, pos=_partsspan(parts))\n    else:\n        assert len(p[1]) == 1\n        p[0] = p[1][0]\n\n    if (len(p) == 2 and p.lexer._parserstate & flags.parser.CMDSUBST and\n            p.lexer._current_token.nopos() == p.lexer._shell_eof_token):\n        # accept the input\n        p.accept()\n\ndef p_simple_list1(p):\n    \'\'\'simple_list1 : simple_list1 AND_AND newline_list simple_list1\n                    | simple_list1 OR_OR newline_list simple_list1\n                    | simple_list1 AMPERSAND simple_list1\n                    | simple_list1 SEMICOLON simple_list1\n                    | pipeline_command\'\'\'\n    if len(p) == 2:\n        p[0] = [p[1]]\n    else:\n        p[0] = p[1]\n        p[0].append(bast.node(kind=\'operator\', op=p[2], pos=p.lexspan(2)))\n        p[0].extend(p[len(p) - 1])\n\ndef p_pipeline_command(p):\n    \'\'\'pipeline_command : pipeline\n                        | BANG pipeline_command\n                        | timespec pipeline_command\n                        | timespec list_terminator\n                        | BANG list_terminator\'\'\'\n    if len(p) == 2:\n        if len(p[1]) == 1:\n            p[0] = p[1][0]\n        else:\n            p[0] = bast.node(kind=\'pipeline\', parts=p[1],\n                            pos=(p[1][0].pos[0], p[1][-1].pos[1]))\n    else:\n        # XXX timespec\n        node = bast.node(kind=\'reservedword\', word=\'!\', pos=p.lexspan(1))\n        if p[2].kind == \'pipeline\':\n            p[0] = p[2]\n            p[0].parts.insert(0, node)\n            p[0].pos = (p[0].parts[0].pos[0], p[0].parts[-1].pos[1])\n        else:\n            p[0] = bast.node(kind=\'pipeline\', parts=[node, p[2]],\n                            pos=(node.pos[0], p[2].pos[1]))\n\ndef p_pipeline(p):\n    \'\'\'pipeline : pipeline BAR newline_list pipeline\n                | pipeline BAR_AND newline_list pipeline\n                | command\'\'\'\n    if len(p) == 2:\n        p[0] = [p[1]]\n    else:\n        p[0] = p[1]\n        p[0].append(bast.node(kind=\'pipe\', pipe=p[2], pos=p.lexspan(2)))\n        p[0].extend(p[len(p) - 1])\n\ndef p_timespec(p):\n    \'\'\'timespec : TIME\n                | TIME TIMEOPT\n                | TIME TIMEOPT TIMEIGN\'\'\'\n    raise NotImplementedError(\'time command\')\n\ndef p_empty(p):\n    \'\'\'empty :\'\'\'\n    pass\n\ndef p_error(p):\n    assert isinstance(p, tokenizer.token)\n\n    if p.ttype == tokenizer.tokentype.EOF:\n        raise errors.ParsingError(\'unexpected EOF\',\n                                  p.lexer.source,\n                                  len(p.lexer.source))\n    else:\n        raise errors.ParsingError(\'unexpected token %r\' % p.value,\n                                  p.lexer.source, p.lexpos)\n\nyaccparser = yacc.yacc(tabmodule=\'bashlint.parsetab\',\n              outputdir=os.path.dirname(__file__),\n              debug=False)\n\n# some hack to fix yacc\'s reduction on command substitutions:\n# which state to fix is derived from static transition tables\n# as states are changeable among python versions and architectures\n# the only state that is considered fixed is the initial state: 0\ndef get_correction_states():\n    reduce = yaccparser.goto[0][\'simple_list\'] #~10\n    state2 = yaccparser.action[reduce][\'NEWLINE\'] #63\n    state1 = yaccparser.goto[reduce][\'simple_list_terminator\'] #~10\n    return state1, state2\n\ndef get_correction_rightparen_states():\n    state1 = yaccparser.goto[0][\'pipeline_command\']\n    state2 = yaccparser.goto[0][\'simple_list1\'] #11\n    state_temp = yaccparser.action[state2][\'SEMICOLON\'] #65\n    state3 = yaccparser.goto[state_temp][\'simple_list1\']\n    return state1, state2, state3\n\nfor tt in tokenizer.tokentype:\n    states = get_correction_states()\n    yaccparser.action[states[0]][tt.name] = -1\n    yaccparser.action[states[1]][tt.name] = -141\n\nstates = get_correction_rightparen_states()\nyaccparser.action[states[0]][\'RIGHT_PAREN\'] = -155\nyaccparser.action[states[1]][\'RIGHT_PAREN\'] = -148\nyaccparser.action[states[2]][\'RIGHT_PAREN\'] = -154\n\ndef parsesingle(s, strictmode=True, expansionlimit=None, convertpos=False):\n    \'\'\'like parse, but only consumes a single top level node, e.g. parsing\n    \'a\\nb\' will only return a node for \'a\', leaving b unparsed\'\'\'\n    p = _parser(s, strictmode=strictmode, expansionlimit=expansionlimit)\n    tree = p.parse()\n    if convertpos:\n        bast.posconverter(s).visit(tree)\n    return tree\n\ndef parse(s, strictmode=True, expansionlimit=None, convertpos=False):\n    \'\'\'parse the input string, returning a list of nodes\n    top level node kinds are:\n    - command - a simple command\n    - pipeline - a series of simple commands\n    - list - a series of one or more pipelines\n    - compound - contains constructs for { list; }, (list), if, for..\n    leafs are word nodes (which in turn can also contain any of the\n    aforementioned nodes due to command substitutions).\n    when strictmode is set to False, we will:\n    - skip reading a heredoc if we\'re at the end of the input\n    expansionlimit is used to limit the amount of recursive parsing done due to\n    command substitutions found during word expansion.\n    \'\'\'\n    p = _parser(s, strictmode=strictmode, expansionlimit=expansionlimit)\n    parts = [p.parse()]\n\n    class endfinder(bast.nodevisitor):\n        def __init__(self):\n            self.end = -1\n        def visitheredoc(self, node, value):\n            self.end = node.pos[1]\n\n    # find the \'real\' end incase we have a heredoc in there\n    ef = _endfinder()\n    ef.visit(parts[-1])\n    index = max(parts[-1].pos[1], ef.end) + 1\n    while index < len(s):\n        part = _parser(s[index:], strictmode=strictmode).parse()\n\n        if not isinstance(part, bast.node):\n            break\n\n        bast.posshifter(index).visit(part)\n        parts.append(part)\n        ef = _endfinder()\n        ef.visit(parts[-1])\n        index = max(parts[-1].pos[1], ef.end) + 1\n\n    if convertpos:\n        for tree in parts:\n            bast.posconverter(s).visit(tree)\n\n    return parts\n\ndef split(s):\n    \'\'\'a utility function that mimics shlex.split but handles more\n    complex shell constructs such as command substitutions inside words\n    >>> list(split(\'a b""c""\\\\\'d\\\\\'\'))\n    [\'a\', \'bcd\']\n    >>> list(split(\'a ""b $(c)"" $(d) \\\\\'$(e)\\\\\'\'))\n    [\'a\', \'b $(c)\', \'$(d)\', \'$(e)\']\n    >>> list(split(\'a b\\\\n\'))\n    [\'a\', \'b\', \'\\\\n\']\n    \'\'\'\n    p = _parser(s)\n    for t in p.tok:\n        if t.ttype == tokenizer.tokentype.WORD:\n            quoted = bool(t.flags & flags.word.QUOTED)\n            doublequoted = quoted and t.value[0] == \'""\'\n            parts, expandedword = subst._expandwordinternal(p, t, 0,\n                                                            doublequoted, 0, 0)\n            yield expandedword\n        else:\n            yield s[t.lexpos:t.endlexpos]\n\nclass _parser(object):\n    \'\'\'\n    this class is mainly used to provide context to the productions\n    when we\'re in the middle of parsing. as a hack, we shove it into the\n    YaccProduction context attribute to make it accessible.\n    \'\'\'\n    def __init__(self, s, strictmode=True, expansionlimit=None, tokenizerargs=None):\n        assert expansionlimit is None or isinstance(expansionlimit, int)\n\n        self.s = s\n        self._strictmode = strictmode\n        self._expansionlimit = expansionlimit\n\n        if tokenizerargs is None:\n            tokenizerargs = {}\n        self.parserstate = tokenizerargs.pop(\'parserstate\', state.parserstate())\n\n        self.tok = tokenizer.tokenizer(s,\n                                       parserstate=self.parserstate,\n                                       strictmode=strictmode,\n                                       **tokenizerargs)\n\n        self.redirstack = self.tok.redirstack\n\n    def parse(self):\n        # yacc.yacc returns a parser object that is not reentrant, it has\n        # some mutable state. we make a shallow copy of it so no\n        # state spills over to the next call to parse on it\n        theparser = copy.copy(yaccparser)\n        tree = theparser.parse(lexer=self.tok, context=self)\n\n        return tree\n\nclass _endfinder(bast.nodevisitor):\n    \'\'\'helper class to find the ""real"" end pos of a node that contains\n    a heredoc. this is a hack because heredoc aren\'t really part of any node\n    since they don\'t always follow the end of a node and might appear on\n    a different line\'\'\'\n    def __init__(self):\n        self.end = -1\n    def visitheredoc(self, node, value):\n        self.end = node.pos[1]\n'"
bashlint/butils.py,0,"b""import collections\n\nclass typedset(collections.MutableSet):\n    def __init__(self, type_, iterable=[]):\n        self._s = set()\n        self._type = type_\n        for v in iterable:\n            self.add(v)\n\n    def add(self, value):\n        if not isinstance(value, self._type):\n            raise ValueError('can only add items of type %s to this set' % self._type)\n        self._s.add(value)\n\n    def discard(self, value):\n        self._s.discard(value)\n\n    def __contains__(self, value):\n        return self._s.__contains__(value)\n\n    def __iter__(self):\n        return self._s.__iter__()\n\n    def __len__(self):\n        return len(self._s)\n\n    def __and__(self, value):\n        if isinstance(value, self._type):\n            value = set([value])\n        return self._s.__and__(value)\n\n    def __or__(self, value):\n        if isinstance(value, self._type):\n            value = set([value])\n        return self._s.__or__(value)\n\n    def __ior__(self, value):\n        if isinstance(value, self._type):\n            value = set([value])\n        self._s.__ior__(value)\n        return self\n\n    #def __sub__(self, value):\n    #    if isinstance(value, self._type):\n    #        value = set([value])\n    #    return self._s.__sub__(value)\n\n    def __repr__(self):\n        return self._s.__repr__()\n\nclass frozendict(collections.Mapping):\n    def __init__(self, *args, **kwargs):\n        self.__dict = dict(*args, **kwargs)\n        self.__hash = None\n\n    def __getitem__(self, key):\n        return self.__dict[key]\n\n    def copy(self, **add_or_replace):\n        return frozendict(self, **add_or_replace)\n\n    def __iter__(self):\n        return iter(self.__dict)\n\n    def __len__(self):\n        return len(self.__dict)\n\n    def __repr__(self):\n        return '<frozendict %s>' % repr(self.__dict)\n"""
bashlint/data_tools.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""Domain-specific natural Language and bash command tokenizer.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom bashlint import bash, lint, nast\n\nflag_suffix = \'<FLAG_SUFFIX>\'\n\n\ndef correct_errors_and_normalize_surface(cm):\n    return lint.correct_errors_and_normalize_surface(cm)\n\n\ndef get_utility_statistics(utility):\n    return lint.get_utility_statistics(utility)\n\n\ndef get_utilities(ast):\n    def get_utilities_fun(node):\n        utilities = set([])\n        if node.is_utility():\n            utilities.add(node.value)\n            for child in node.children:\n                utilities = utilities.union(get_utilities_fun(child))\n        elif not node.is_argument():\n            for child in node.children:\n                utilities = utilities.union(get_utilities_fun(child))\n        return utilities\n\n    if not ast:\n        return set([])\n    else:\n        return get_utilities_fun(ast)\n\n\ndef bash_tokenizer(cmd, recover_quotation=True, loose_constraints=False,\n        ignore_flag_order=False, arg_type_only=False, keep_common_args=False, with_flag_head=False,\n        with_flag_argtype=False, with_prefix=False, verbose=False):\n    """"""\n    Tokenize a bash command.\n    """"""\n    if isinstance(cmd, str):\n        tree = lint.normalize_ast(cmd, recover_quotation, verbose=verbose)\n    else:\n        tree = cmd\n    return ast2tokens(tree, loose_constraints, ignore_flag_order,\n                      arg_type_only, keep_common_args=keep_common_args, with_flag_head=with_flag_head,\n                      with_prefix=with_prefix, with_flag_argtype=with_flag_argtype)\n\n\ndef bash_parser(cmd, recover_quotation=True, verbose=False):\n    """"""\n    Parse bash command into AST.\n    """"""\n    ast = lint.normalize_ast(cmd, recover_quotation, verbose=verbose)\n    if ast is None:\n        return paren_parser(cmd)\n    else:\n        return ast\n\n\ndef ast2tokens(node, loose_constraints=False, ignore_flag_order=False,\n               arg_type_only=False, keep_common_args=False,\n               with_arg_type=False, with_flag_head=False,\n               with_flag_argtype=False, with_prefix=False,\n               indexing_args=False):\n    """"""\n    Convert a bash ast into a list of tokens.\n\n    :param loose_constraints: If set, do not check semantic coherence between\n        flags and arguments.\n    :param ignore_flag_order: If set, output flags in alphabetical order.\n    :param arg_type_only: If set, output argument semantic types instead of the\n        actual value.\n    :param: keep_common_args: If set, keep common arguments such as ""/"", "".""\n        and do not replace them with semantic types. Effective only when\n        arg_type_only is set.\n    :param with_arg_type: If set, append argument type to argument token.\n    :param with_flag_head: If set, add utility prefix to flag token.\n    :param with_flag_argtype: If set, append argument type suffix to flag token.\n    :param with_prefix: If set, add node kind prefix to token.\n    :param indexing_args: If set, append order index to argument token.\n    """"""\n    if not node:\n        return []\n\n    lc = loose_constraints\n\n    def to_tokens_fun(node):\n        tokens = []\n        if node.is_root():\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc:\n                for child in node.children:\n                    tokens += to_tokens_fun(child)\n            else:\n                tokens = to_tokens_fun(node.children[0])\n        elif node.kind == ""pipeline"":\n            assert(loose_constraints or node.get_num_of_children() > 1)\n            if lc and node.get_num_of_children() < 1:\n                tokens.append(""|"")\n            elif lc and node.get_num_of_children() == 1:\n                # treat ""singleton-pipe"" as atomic command\n                tokens += to_tokens_fun(node.children[0])\n            else:\n                for child in node.children[:-1]:\n                    tokens += to_tokens_fun(child)\n                    tokens.append(""|"")\n                tokens += to_tokens_fun(node.children[-1])\n        elif node.kind == ""commandsubstitution"":\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc and node.get_num_of_children() < 1:\n                tokens += [""$("", "")""]\n            else:\n                tokens.append(""$("")\n                tokens += to_tokens_fun(node.children[0])\n                tokens.append("")"")\n        elif node.kind == ""processsubstitution"":\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc and node.get_num_of_children() < 1:\n                tokens.append(node.value + ""("")\n                tokens.append("")"")\n            else:\n                tokens.append(node.value + ""("")\n                tokens += to_tokens_fun(node.children[0])\n                tokens.append("")"")\n        elif node.is_utility():\n            token = node.value\n            if with_prefix:\n                token = node.prefix + token\n            tokens.append(token)\n            children = sorted(node.children, key=lambda x:x.value) \\\n                if ignore_flag_order else node.children\n            for child in children:\n                tokens += to_tokens_fun(child)\n        elif node.is_option():\n            assert(loose_constraints or node.parent)\n            if \'::\' in node.value and (node.value.startswith(\'-exec\') or \n                                       node.value.startswith(\'-ok\')):\n                value, op = node.value.split(\'::\')\n                token = value\n            else:\n                token = node.value\n            if with_flag_head:\n                if node.parent:\n                    token = node.utility.value + ""@@"" + token\n                else:\n                    token = token\n            if with_prefix:\n                token = node.prefix + token\n            if with_flag_argtype:\n                suffix = \'\'\n                if node.children:\n                    for child in node.children:\n                        if child.is_argument():\n                            suffix += child.arg_type\n                        elif child.is_utility():\n                            suffix += \'UTILITY\'\n                token = token + flag_suffix + suffix\n            tokens.append(token)\n            for child in node.children:\n                tokens += to_tokens_fun(child)\n            if \'::\' in node.value and (node.value.startswith(\'-exec\') or\n                                       node.value.startswith(\'-ok\')):\n                if op == \';\':\n                    op = ""\\\\;""\n                tokens.append(op)\n        elif node.kind == \'operator\':\n            tokens.append(node.value)\n        elif node.kind == ""binarylogicop"":\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            if lc and node.get_num_of_children() > 0:\n                for child in node.children[:-1]:\n                    tokens += to_tokens_fun(child)\n                    tokens.append(node.value)\n                tokens += to_tokens_fun(node.children[-1])\n            else:\n                tokens.append(node.value)\n        elif node.kind == ""unarylogicop"":\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            if lc and node.get_num_of_children() > 0:\n                if node.associate == nast.UnaryLogicOpNode.RIGHT:\n                    tokens.append(node.value)\n                    tokens += to_tokens_fun(node.children[0])\n                else:\n                    tokens += to_tokens_fun(node.children[0])\n                    tokens.append(node.value)\n            else:\n                tokens.append(node.value)\n        elif node.kind == ""bracket"":\n            assert(loose_constraints or node.get_num_of_children() >= 1)\n            if lc and node.get_num_of_children() < 2:\n                for child in node.children:\n                    tokens += to_tokens_fun(child)\n            else:\n                tokens.append(""\\\\("")\n                for i in xrange(len(node.children)-1):\n                    tokens += to_tokens_fun(node.children[i])\n                tokens += to_tokens_fun(node.children[-1])\n                tokens.append(""\\\\)"")\n        elif node.kind == ""nt"":\n            assert(loose_constraints or node.get_num_of_children() > 0)\n            tokens.append(""("")\n            for child in node.children:\n                tokens += to_tokens_fun(child)\n            tokens.append("")"")\n        elif node.is_argument() or node.kind in [""t""]:\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            if arg_type_only and node.is_open_vocab():\n                if (keep_common_args and node.parent.is_utility() and\n                    node.parent.value == \'find\' and node.value in bash.find_common_args):\n                    # keep frequently-occurred arguments in the vocabulary\n                    # TODO: define the criteria for ""common args""\n                    token = node.value\n                else:\n                    if node.arg_type in bash.quantity_argument_types:\n                        if node.value.startswith(\'+\'):\n                            token = \'+{}\'.format(node.arg_type)\n                        elif node.value.startswith(\'-\'):\n                            token = \'-{}\'.format(node.arg_type)\n                        else:\n                            token = node.arg_type\n                    else:\n                        token = node.arg_type\n            else:\n                token = node.value\n            if with_prefix:\n                token = node.prefix + token\n            if with_arg_type:\n                token = token + ""_"" + node.arg_type\n            if indexing_args and node.to_index():\n                token = token + ""-{:02d}"".format(node.index)\n\n            tokens.append(token)\n            if lc:\n                for child in node.children:\n                    tokens += to_tokens_fun(child)\n        return tokens\n\n    return to_tokens_fun(node)\n\n\ndef ast2command(node, loose_constraints=False, ignore_flag_order=False):\n    return lint.serialize_ast(node, loose_constraints=loose_constraints,\n                              ignore_flag_order=ignore_flag_order)\n\n\ndef ast2template(node, loose_constraints=False, ignore_flag_order=False,\n                 arg_type_only=True, indexing_args=False,\n                 keep_common_args=False):\n    """"""\n    Convert a bash AST to a template that contains only reserved words and\n    argument types flags are alphabetically ordered.\n    """"""\n    tokens = ast2tokens(node, loose_constraints, ignore_flag_order,\n                        arg_type_only=arg_type_only, \n                        indexing_args=indexing_args,\n                        keep_common_args=keep_common_args)\n    return \' \'.join(tokens)\n\ndef cmd2template(cmd, recover_quotation=True, arg_type_only=True,\n                loose_constraints=False, verbose=False):\n    """"""\n    Convert a bash command to a template that contains only reserved words\n    and argument types flags are alphabetically ordered.\n    """"""\n    tree = lint.normalize_ast(cmd, recover_quotation, verbose=verbose)\n    return ast2template(tree, loose_constraints=loose_constraints, \n                        arg_type_only=arg_type_only)\n\n\ndef pretty_print(node, depth=0):\n    """"""\n    Pretty print the AST.\n    """"""\n    try:\n        str = ""    "" * depth + node.kind.upper() + \'(\' + node.value + \')\'\n        if node.is_argument():\n            str += \'<\' + node.arg_type + \'>\'\n        print(str)\n        for child in node.children:\n            pretty_print(child, depth+1)\n    except AttributeError:\n        print(""    "" * depth)\n\n\ndef ast2list(node, order=\'dfs\', _list=None, ignore_flag_order=False,\n             arg_type_only=False, keep_common_args=False,\n             with_flag_head=False, with_prefix=False):\n    """"""\n    Linearize the AST.\n    """"""\n    if order == \'dfs\':\n        if node.is_argument() and node.is_open_vocab() and arg_type_only:\n            token = node.arg_type\n        elif node.is_option() and with_flag_head:\n            token = node.utility.value + \'@@\' + node.value if node.utility \\\n                else node.value\n        else:\n            token = node.value\n        if with_prefix:\n            token = node.prefix + token\n        _list.append(token)\n        if node.get_num_of_children() > 0:\n            if node.is_utility() and ignore_flag_order:\n                children = sorted(node.children, key=lambda x:x.value)\n            else:\n                children = node.children\n            for child in children:\n                ast2list(child, order, _list, ignore_flag_order, arg_type_only,\n                         keep_common_args, with_flag_head, with_prefix)\n            _list.append(nast._H_NO_EXPAND)\n        else:\n            _list.append(nast._V_NO_EXPAND)\n    return _list\n\n\n# --- Other syntax parsers --- #\n\ndef paren_parser(line):\n    """"""A simple parser for parenthesized sequence.""""""\n    def order_child_fun(node):\n        for child in node.children:\n            order_child_fun(child)\n        if len(node.children) > 1 and node.children[0].value in [""and"", ""or""]:\n            node.children = node.children[:1] + sorted(node.children[1:],\n                    key=lambda x:(x.value if x.kind == ""t"" else (\n                        x.children[0].value if x.children else x.value)))\n\n    if not line.startswith(""(""):\n        line = ""( "" + line\n    if not line.endswith("")""):\n        line = line + "" )""\n    words = line.strip().split()\n\n    root = nast.Node(kind=""root"", value=""root"")\n    stack = []\n\n    i = 0\n    while i < len(words):\n        word = words[i]\n        if word == ""("":\n            if stack:\n                # creates non-terminal\n                node = nast.Node(kind=""nt"", value=""<n>"")\n                stack[-1].add_child(node)\n                node.parent = stack[-1]\n                stack.append(node)\n            else:\n                stack.append(root)\n        elif word == "")"":\n            if stack:\n                stack.pop()\n        else:\n            if stack:\n                node = nast.Node(kind=""t"", value=word)\n                stack[-1].add_child(node)\n                node.parent = stack[-1]\n        i += 1\n        if len(stack) == 0:\n            break\n\n    # order nodes\n    order_child_fun(root)\n\n    return root\n\n# --- Test functions --- #\n\ndef batch_parse(input_file):\n    """"""\n    Parse the input_file each line of which is a bash command.\n    """"""\n    with open(input_file) as f:\n        i = 0\n        for cmd in f:\n            print(""{}. {}"".format(i, cmd))\n            ast = bash_parser(cmd)\n            pretty_print(ast)\n            i += 1\n\ndef test_bash_parser():\n    while True:\n        try:\n            cmd = input(""> "")\n            norm_tree = bash_parser(cmd)\n            # pruned_tree = normalizer.prune_ast(norm_tree)\n            print()\n            print(""AST:"")\n            pretty_print(norm_tree, 0)\n            # print(""Pruned AST:"")\n            # pretty_print(pruned_tree, 0)\n            # search_history = ast2list(norm_tree, \'dfs\', list=[])\n            # for state in search_history:\n            #     print(state)\n            print(get_utilities(norm_tree))\n            print(""Command Template:"")\n            print(ast2template(norm_tree, ignore_flag_order=False))\n            print(""Command: "")\n            print(ast2command(norm_tree, ignore_flag_order=False))\n            # print(""Pruned Command Template:"")\n            # print(ast2template(pruned_tree, ignore_flag_order=False))\n            print()\n        except EOFError as ex:\n            break\n\n\ndef test_bash_tokenizer():\n\n    def test(cmd):\n        tokens = bash_tokenizer(cmd)\n        print(\'cmd: {}\'.format(cmd))\n        print(\'tokens: {}\'.format(tokens))\n\n    cmd1 = \'find . -name ""*.andnav"" | rename -vn ""s/\\.andnav$/.tile/""\'\n    test(cmd1)\n    cmd2 = \'find /volume1/uploads -name ""*.mkv"" -exec mv \\{\\} \\{\\}.avi \\;\'\n    test(cmd2)\n    cmd3 = \'touch -d ""$(date -r filename) - 2 hours"" filename\'\n    test(cmd3)\n\n\nif __name__ == ""__main__"":\n    # input_file = sys.argv[1]\n    # batch_parse(input_file)\n    # test_bash_parser()\n    test_bash_tokenizer()\n'"
bashlint/errors.py,0,"b""class ParsingError(Exception):\n    def __init__(self, message, s, position):\n        self.message = message\n        self.s = s\n        self.position = position\n\n        assert position <= s\n        super(ParsingError, self).__init__('%s (position %d)' % (message, position))\n\n\nclass LintParsingError(Exception):\n    def __init__(self, message, s, position):\n        self.message = message\n        self.s = s\n        self.position = position\n\n        assert position <= s\n        super(LintParsingError, self).__init__('%s (position %d)' % (message, position))\n\n\nclass SubCommandError(Exception):\n    def __init__(self, message, s, position):\n        self.message = message\n        self.s = s\n        self.position = position\n\n        assert position <= s\n        super(SubCommandError, self).__init__('%s (position %d)' % (message, position))\n\n\nclass FlagError(Exception):\n    def __init__(self, message, s, position):\n        self.message = message\n        self.s = s\n        self.position = position\n\n        assert position <= s\n        super(FlagError, self).__init__('%s (position %d)' % (message, position))\n"""
bashlint/flags.py,0,"b'import enum\n\nparser = enum.Enum(\'parserflags\', [\n    \'CASEPAT\', # in a case pattern list\n    \'ALEXPNEXT\', # expand next word for aliases\n    \'ALLOWOPNBRC\', # allow open brace for function def\n    \'NEEDCLOSBRC\', # need close brace\n    \'DBLPAREN\', # double-paren parsing\n    \'SUBSHELL\', # ( ... ) subshell\n    \'CMDSUBST\', # $( ... ) command substitution\n    \'CASESTMT\', # parsing a case statement\n    \'CONDCMD\', # parsing a [[...]] command\n    \'CONDEXPR\', # parsing the guts of [[...]]\n    \'ARITHFOR\', # parsing an arithmetic for command - unused\n    \'ALEXPAND\', # OK to expand aliases - unused\n    \'EXTPAT\', # parsing an extended shell pattern\n    \'COMPASSIGN\', # parsing x=(...) compound assignment\n    \'ASSIGNOK\', # assignment statement ok in this context\n    \'EOFTOKEN\', # yylex checks against shell_eof_token\n    \'REGEXP\', # parsing an ERE/BRE as a single word\n    \'HEREDOC\', # reading body of here-document\n    \'REPARSE\', # re-parsing in parse_string_to_word_list\n    \'REDIRLIST\', # parsing a list of redirections preceding a simple command name\n    ])\n\nword = enum.Enum(\'wordflags\', [\n    \'QUOTED\', # Some form of quote character is present\n    \'DQUOTE\', # word should be treated as if double-quoted\n    \'HASQUOTEDNULL\', # word contains a quoted null character\n\n    \'SPLITSPACE\', # Split this word on "" "" regardless of IFS\n    \'NOSPLIT\', # Do not perform word splitting on this word because ifs is empty string\n    \'NOSPLIT2\', # Don\'t split word except for $@ expansion (using spaces) because context does not allow it\n\n    \'TILDEEXP\', # Tilde expand this assignment word\n    \'ITILDE\', # Internal flag for word expansion\n\n    \'HASDOLLAR\', # Dollar sign present\n    \'DOLLARAT\', # $@ and its special handling\n    \'DOLLARSTAR\', # $* and its special handling\n\n    \'NOCOMSUB\', # Don\'t perform command substitution on this word\n    \'NOPROCSUB\', # don\'t perform process substitution\n\n    \'NOTILDE\', # Don\'t perform tilde expansion on this word\n    \'NOEXPAND\', # Don\'t expand at all -- do quote removal\n    \'NOBRACE\', # Don\'t perform brace expansion\n    \'NOGLOB\', # Do not perform globbing on this word\n\n    \'ASSIGNMENT\', # This word is a variable assignment\n    \'ASSNBLTIN\', # word is a builtin command that takes assignments\n    \'ASSIGNRHS\', # Word is rhs of an assignment statement\n    \'ASSIGNARG\', # word is assignment argument to command\n    \'ASSIGNASSOC\', # word looks like associative array assignment\n    \'ASSIGNARRAY\', # word looks like a compound indexed array assignment\n    \'ASSNGLOBAL\', # word is a global assignment to declare (declare/typeset -g)\n    \'ASSIGNINT\', # word is an integer assignment to declare\n    \'COMPASSIGN\', # Compound assignment\n\n    \'HASCTLESC\', # word contains literal CTLESC characters\n    \'ARRAYIND\', # word is an array index being expanded\n    ])\n'"
bashlint/grammar.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""\nConsume tokens in a bash command and update the set of possible next states for\nthe parser.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nUTIL_S = 0\nCOMPOUND_FLAG_S = 1\nFLAG_S = 2\nCOMMAND_S = 3\nARG_COMMAND_S = 4\nEXEC_COMMAND_S = 5\nARG_S = 6\nOPERATOR_S = 7\nEOF_S = 8\n\n\nclass BashGrammarState(object):\n    def __init__(self, type):\n        self.type = type\n\n    def get_utility(self):\n        cur = self\n        while (cur):\n            if cur.is_utility():\n                return cur\n            cur = cur.parent\n        raise ValueError(\'No utility state found\')\n\n    def is_argument(self):\n        return self.type == ARG_S\n\n    def is_command(self):\n        return self.type == COMMAND_S or self.type == ARG_COMMAND_S \\\n            or self.type == EXEC_COMMAND_S\n\n    def is_compound_flag(self):\n        return self.type == COMPOUND_FLAG_S\n\n    def is_flag(self):\n        return self.type == FLAG_S\n\n    def is_utility(self):\n        return self.type == UTIL_S\n\n    def is_eof(self):\n        return self.type == EOF_S\n\n\nclass UtilityState(BashGrammarState):\n    def __init__(self, name):\n        super(UtilityState, self).__init__(UTIL_S)\n        self.name = name\n        self.compound_flag = CompoundFlagState(self)\n        self.positional_arguments = []\n        self.eof = EOFState()\n        self.argument_only = False\n\n    def add_flag(self, flag):\n        self.compound_flag.add_flag(flag)\n\n    def add_positional_argument(self, arg):\n        self.positional_arguments.append(arg)\n        arg.parent = self\n\n    def next_states(self):\n        if self.argument_only:\n            next_states = []\n        else:\n            next_states = [self.compound_flag]\n        for arg_state in self.positional_arguments:\n            if not arg_state.filled or (arg_state.is_list\n                    and arg_state.list_separator == \' \'):\n                next_states.append(arg_state)\n        next_states.append(self.eof)\n        return next_states\n\n    def serialize(self):\n        header = self.name\n        header += \' \' + self.compound_flag.serialize()\n        for arg in self.positional_arguments:\n            header += \' \' + arg.serialize()\n        return header\n\n\nclass CompoundFlagState(BashGrammarState):\n    def __init__(self, parent):\n        super(CompoundFlagState, self).__init__(COMPOUND_FLAG_S)\n        self.parent = parent\n        self.flag_index = {}\n\n    def add_flag(self, flag):\n        self.flag_index[flag.flag_name] = flag\n        flag.parent = self\n\n    def serialize(self):\n        header = \'\'\n        for flag in sorted(self.flag_index.keys()):\n            header += \' \' + self.flag_index[flag].serialize()\n        return header\n\n\nclass FlagState(BashGrammarState):\n    def __init__(self, flag_name, optional):\n        super(FlagState, self).__init__(FLAG_S)\n        self.flag_name = flag_name\n        self.optional = optional\n        self.parent = None\n        self.argument = None\n\n    def add_argument(self, argument):\n        if self.argument is None:\n            self.argument = argument\n        else:\n            self.argument.rsb = argument\n        argument.parent = self\n\n    def serialize(self):\n        header = \'{}\'.format(self.flag_name)\n        if self.argument:\n            arg = self.argument\n            while arg:\n                header += \' \' + arg.serialize()\n                arg = arg.rsb\n        if self.optional:\n            header = \'[ {} ]\'.format(header)\n        return header\n\n\nclass ArgumentState(BashGrammarState):\n    def __init__(self, arg_name, arg_type, optional=False, is_list=False,\n                 list_separator=\' \', regex_format=None, no_space=False):\n        """"""\n        :member arg_name: Name of argument as appeared in the man page.\n        :member arg_type: Semantic type of argument as assigned in the synopsis.\n        :member optional: If set, argument is optional.\n        :member is_list: If set, argument can be a list.\n        :member list_separator: Argument list separator.\n        :member regex_format: Pattern which specifies the structure to parse\n            the argument.\n        :member no_space: No space between the argument and the flag it is\n            attached to.\n        :member filled: If set, at least\n        :member parent: Parent state.\n        :member rsb: Right sibling state.\n        """"""\n        super(ArgumentState, self).__init__(ARG_S)\n        self.arg_name = arg_name\n        self.arg_type = arg_type\n        self.optional = optional\n        self.is_list = is_list\n        self.list_separator = list_separator\n        self.regex_format = regex_format\n        self.no_space = no_space\n        self.filled = False\n        self.parent = None\n        self.rsb = None\n\n    def serialize(self):\n        header = \'{} ({})\'.format(self.arg_type, self.arg_name)\n        if self.no_space:\n            header = \'~~\' + header\n        if self.regex_format:\n            header += \'<{}>\'.format(self.regex_format)\n        if self.is_list:\n            header += \'{}...\'.format(self.list_separator)\n        if self.optional:\n            header = \'[ {} ]\'.format(header)\n\n        return header\n\n\nclass ArgCommandState(BashGrammarState):\n    def __init__(self):\n        super(ArgCommandState, self).__init__(ARG_COMMAND_S)\n        self.no_space = False\n        self.filled = False\n        self.parent = None\n        self.rsb = None\n\n    def serialize(self):\n        return \'COMMAND\'\n\n\nclass ExecCommandState(BashGrammarState):\n    def __init__(self, stop_tokens):\n        super(ExecCommandState, self).__init__(EXEC_COMMAND_S)\n        self.stop_tokens = stop_tokens\n        self.no_space = False\n        self.filled = False\n        self.parent = None\n        self.rsb = None\n\n    def serialize(self):\n        return \'{}$${}\'.format(\'COMMAND\', \',\'.join(self.stop_tokens))\n\n\nclass CommandState(BashGrammarState):\n    def __init__(self):\n        super(CommandState, self).__init__(COMMAND_S)\n        self.filled = False\n        self.parent = None\n        self.rsb = None\n\n    def serialize(self):\n        return \'COMMAND_EOS\'\n\n\nclass EOFState(BashGrammarState):\n    def __init__(self):\n        super(EOFState, self).__init__(EOF_S)\n\n\nclass BashGrammar(object):\n    def __init__(self):\n        self.name2type = {}\n        self.grammar = {}\n        self.next_states = None     # pointer on the current position in the grammar tree\n\n    def allow_eof(self):\n        for state in self.next_states:\n            if state.is_eof():\n                return True\n        return False\n\n    def get_next_state(self, state_type):\n        for state in self.next_states:\n            if state.type == state_type:\n                return state\n\n    def consume(self, token):\n        if token in self.grammar:\n            utility_state = self.grammar[token]\n            self.next_states = utility_state.next_states()\n            return True\n        else:\n            return False\n\n    def push(self, token, state_type):\n        state = self.get_next_state(state_type)\n        if state_type == COMPOUND_FLAG_S:\n            if token.startswith(\'--\'):\n                # long option\n                if \'=\' in token:\n                    flag_token, flag_arg = token.split(\'=\', 1)\n                else:\n                    flag_token, flag_arg = token, \'\'\n                if flag_token in state.flag_index:\n                    flag_state = state.flag_index[flag_token]\n                    if flag_state.argument:\n                        arg_state = flag_state.argument\n                        if not flag_arg:\n                            self.next_states = [arg_state]\n                            return [(flag_token, \'__OPEN__\')]\n                        else:\n                            return [(flag_token, (flag_arg, arg_state.arg_type))]\n                    else:\n                        if not flag_arg:\n                            return [(flag_token, None)]\n                        else:\n                            raise ValueError(\'Unexpected flag argument ""{}""\'.format(token))\n                else:\n                    if flag_token == \'--\':\n                        state.parent.argument_only = True\n                    else:\n                        raise ValueError(\'Unrecognized long flag ""{}""\'.format(flag_token))\n            elif token in state.flag_index:\n                flag_token = token\n                state = state.flag_index[flag_token]\n                if state.argument and not state.argument.no_space:\n                    self.next_states = [state.argument]\n                    return [(flag_token, \'__OPEN__\')]\n                else:\n                    return [(flag_token, None)]\n            else:\n                flag_token = token[:2]\n                if flag_token in state.flag_index:\n                    flag_state = state.flag_index[flag_token]\n                    if flag_state.argument:\n                        # Case 1: flag has an argument\n                        flag_arg = token[2:]\n                        arg_state = flag_state.argument\n                        return [(flag_token, (flag_arg, arg_state.arg_type))]\n                    else:\n                        if len(token) > 2:\n                            # Case 2: multiple flags specified at the same time\n                            flag_list = [(flag_token, None)]\n                            for j in xrange(2, len(token)):\n                                flag_token = \'-\' + token[j]\n                                if flag_token in state.flag_index:\n                                    if not state.flag_index[flag_token].argument:\n                                        flag_list.append((flag_token, None))\n                                    else:\n                                        if j < len(token) - 1:\n                                            arg_state = state.flag_index[flag_token].argument\n                                            flag_list.append((flag_token, (token[j+1:], arg_state.arg_type)))\n                                            break\n                                        else:\n                                            flag_list.append((flag_token, None))\n                                else:\n                                    raise ValueError(\'Unrecognized flag ""{}""\'.format(flag_token))\n                            return flag_list\n                        else:\n                            # Case 5: the token does not match any flag state\n                            return None\n                else:\n                    # Case 3: argument specified with a single \'-\'\n                    if flag_token.startswith(\'-\') and \'-\' in state.flag_index \\\n                            and state.flag_index[\'-\'].argument:\n                        flag_arg = token[1:]\n                        arg_state = state.flag_index[\'-\'].argument\n                        return [(\'-\', (flag_arg, arg_state.arg_type))]\n                    # Case 4: argument specified with a single \'+\'\n                    elif flag_token.startswith(\'+\') and \'+\' in state.flag_index \\\n                            and state.flag_index[\'+\'].argument:\n                        flag_arg = token[1:]\n                        arg_state = state.flag_index[\'+\'].argument\n                        return [(\'+\', (flag_arg, arg_state.arg_type))]\n                    else:\n                        # Case 5: the token does not match any flag state\n                        return None\n        elif state_type == COMMAND_S:\n            self.next_states = state.get_utility().next_states()\n        elif state_type == ARG_COMMAND_S:\n            self.next_states = state.get_utility().next_states()\n        elif state_type == EXEC_COMMAND_S:\n            self.next_states = state.get_utility().next_states()\n        elif state_type == OPERATOR_S:\n            for i, next_state in enumerate(self.next_states):\n                if next_state.is_compound_flag():\n                    del(self.next_states[i])\n        elif state.type == ARG_S:\n            state.filled = True\n            if state.rsb:\n                # continue interpreting the next argument of the same parent state\n                self.next_states = [state.rsb]\n                return \'__SAME_PARENT__\'\n            else:\n                self.next_states = state.get_utility().next_states()\n                return \'__PARENT_CHANGE__\'\n\n    def make_grammar(self, input_file):\n        """"""\n        Build utility grammar from man-page synopsis.\n        """"""\n        with open(input_file, encoding=\'utf-8\') as f:\n            content = f.readlines()\n\n        reading_type = False\n        reading_constants = False\n        reading_synopsis = False\n\n        for line in content:\n            l = line.strip()\n            if not l:\n                continue\n            if l == \'type\':\n                reading_type = True\n                reading_constants = False\n                reading_synopsis = False\n            elif l == \'constants\':\n                reading_constants = True\n                reading_type = False\n                reading_synopsis = False\n            elif l == \'PrimitiveCmd ::=\':\n                reading_synopsis = True\n                reading_type = False\n                reading_constants = False\n            elif reading_type:\n                type, names = line.strip().split(\' \', 1)\n                for name in names.strip()[1:-1].split(\',\'):\n                    name = name.strip()\n                    if not name in self.name2type:\n                        self.name2type[name] = type\n                    else:\n                        raise ValueError(\n                            \'Ambiguity in name type: ""{}"" ({} vs. {})\'.format(\n                                name, self.name2type[name], type))\n            elif reading_synopsis:\n                self.make_utility(line)\n\n        print(\'Bashlint grammar set up ({} utilities)\'.format(len(self.grammar)))\n        print()\n\n    def make_utility(self, line):\n        line = line.strip()\n        if line.startswith(\'* \'):\n            line = line[2:]\n\n        if len(line.strip().split()) == 1:\n            utility = line.strip()\n            u_state = UtilityState(utility)\n            self.grammar[utility] = u_state\n            return\n\n        utility, synopsis = line.strip().split(\' \', 1)\n        synopsis += \' \'\n        if not utility in self.grammar:\n            u_state = UtilityState(utility)\n            self.grammar[utility] = u_state\n        else:\n            u_state = self.grammar[utility]\n\n        # parse the synopsis of a utility into flag portion and argument\n        # portion\n        stack = []\n        status = \'IDLE\'\n        flag_synopsis = \'\'\n        arg_synopsis = \'\'\n        for i in xrange(len(synopsis)):\n            c = synopsis[i]\n            if status == \'IDLE\':\n                if c == \'-\' or c == \'+\':\n                    status = \'READING_FLAG\'\n                    flag_synopsis += c\n                elif c == \'[\':\n                    # reading either an optional flag or an optional argument\n                    status = \'READING_OPTIONAL\'\n                    stack.append(\'[\')\n                elif c.strip():\n                    status = \'READING_ARGUMENT\'\n                    arg_synopsis += c\n            elif status == \'READING_FLAG\':\n                if c == \' \' or c == \'\\n\':\n                    self.make_flag(u_state, flag_synopsis)\n                    flag_synopsis = \'\'\n                    status = \'IDLE\'\n                else:\n                    flag_synopsis += c\n            elif status == \'READING_ARGUMENT\':\n                if c == \' \' or c == \'\\n\':\n                    self.make_positional_argument(u_state, arg_synopsis)\n                    arg_synopsis = \'\'\n                    status = \'IDLE\'\n                else:\n                    arg_synopsis += c\n            elif status == \'READING_OPTIONAL\':\n                if c == \'-\' or c == \'+\':\n                    status = \'READING_OPTIONAL_FLAG\'\n                    flag_synopsis += c\n                elif c.strip():\n                    status = \'READING_OPTIONAL_ARGUMENT\'\n                    arg_synopsis += c\n            elif status == \'READING_OPTIONAL_FLAG\':\n                if c == \']\':\n                    stack.pop()\n                    if not stack:\n                        self.make_flag(u_state, flag_synopsis.strip(), optional=True)\n                        flag_synopsis = \'\'\n                        status = \'IDLE\'\n                    else:\n                        flag_synopsis += c\n                else:\n                    flag_synopsis += c\n                    if c == \'[\':\n                        stack.append(\'[\')\n            elif status == \'READING_OPTIONAL_ARGUMENT\':\n                if c == \']\':\n                    stack.pop()\n                    if not stack:\n                        self.make_positional_argument\\\n                            (u_state, arg_synopsis.strip(), optional=True)\n                        arg_synopsis = \'\'\n                        status = \'IDLE\'\n                    else:\n                        arg_synopsis += c\n                else:\n                    arg_synopsis += c\n                    if c == \'[\':\n                        stack.append(\'[\')\n\n    def make_positional_argument(self, u_state, synopsis, optional=False):\n        assert(u_state is not None)\n        arg = self.make_argument(synopsis, optional=optional)\n        u_state.add_positional_argument(arg)\n\n    def make_flag_argument(self, f_state, synopsis, optional=False):\n        assert(f_state is not None)\n        f_state.add_argument(self.make_argument(synopsis, optional=optional))\n\n    def make_argument(self, synopsis, optional=False):\n        if synopsis.lower() == \'command_eos\':\n            arg = CommandState()\n        elif \'$$\' in synopsis:\n            _, stop_token_synopsis = synopsis.split(\'$$\', 1)\n            stop_token_list = stop_token_synopsis.split(\',\')\n            arg = ExecCommandState(stop_token_list)\n        else:\n            if synopsis.endswith(\'...\'):\n                is_list = True\n                synopsis = synopsis[:-3]\n                if synopsis[-1] in [\',\', \'|\']:\n                    list_separator = synopsis[-1]\n                    synopsis = synopsis[:-1]\n                else:\n                    list_separator = \' \'\n            else:\n                is_list = False\n                list_separator = \' \'\n\n            no_space = synopsis.startswith(\'~~\')\n            if no_space:\n                synopsis = synopsis[2:]\n\n            if \'<\' in synopsis:\n                assert(synopsis.endswith(\'>\'))\n                arg_name, format = synopsis[:-1].split(\'<\', 1)\n                arg_name = arg_name.lower()\n                arg_type = self.name2type[arg_name]\n                arg = ArgumentState(arg_name, arg_type, optional=optional, is_list=is_list,\n                    list_separator=list_separator, regex_format=format, no_space=no_space)\n            else:\n                arg_name = synopsis.lower()\n                arg_type = self.name2type[arg_name]\n                if arg_type == \'Command\':\n                    arg = ArgCommandState()\n                else:\n                    arg = ArgumentState(arg_name, arg_type, optional=optional,\n                        is_list=is_list, list_separator=list_separator, no_space=no_space)\n        return arg\n\n    def make_flag(self, u_state, synopsis, optional=False):\n        assert(u_state is not None)\n        synopsis += \' \'\n        if synopsis.startswith(\'--\'):\n            # long flag option\n            if \'=\' in synopsis:\n                status = \'READING_FLAG\'\n                stack = []\n                flag = None\n                flag_name = \'\'\n                arg_synopsis = \'\'\n                for i in xrange(len(synopsis)):\n                    c = synopsis[i]\n                    if status == \'READING_FLAG\':\n                        if c == \'=\' or c == \'[\':\n                            flag = FlagState(flag_name, optional=optional)\n                            if c == \'[\':\n                                stack.append(c)\n                                status = \'READING_OPTIONAL_FLAG_ARGUMENT\'\n                            else:\n                                status = \'READING_FLAG_ARGUMENT\'\n                        elif c.strip():\n                            flag_name += c\n                    elif status == \'READING_FLAG_ARGUMENT\':\n                        if c == \' \':\n                            self.make_flag_argument(flag, arg_synopsis, optional=False)\n                        elif c.strip():\n                            arg_synopsis += c\n                    elif status == \'READING_OPTIONAL_FLAG_ARGUMENT\':\n                        if c == \']\':\n                            stack.pop()\n                            if not stack:\n                                self.make_flag_argument(flag, arg_synopsis, optional=False)\n                            else:\n                                arg_synopsis += c\n                        elif c.strip() and c != \'=\':\n                            arg_synopsis += c\n            else:\n                flag = FlagState(synopsis.strip(), optional=optional)\n            u_state.add_flag(flag)\n        else:\n            status = \'IDLE\'\n            stack = []\n            flag = None\n            flag_name = \'\'\n            arg_synopsis = \'\'\n            optional_synopsis = \'\'\n            for i in xrange(len(synopsis)):\n                c = synopsis[i]\n                if status == \'IDLE\':\n                    if c == \'-\' or c == \'+\':\n                        flag_name += c\n                        status = \'READING_FLAG_NAME\'\n                    elif c == \'[\':\n                        stack.append(c)\n                        status = \'READING_OPTIONAL\'\n                    elif c == \']\':\n                        stack.pop()\n                    elif c.strip() and c != \'|\':\n                        arg_synopsis += c\n                        status = \'READING_FLAG_ARGUMENT\'\n                elif status == \'READING_OPTIONAL\':\n                    if c == \'-\' or c == \'+\':\n                        optional_synopsis += c\n                        status = \'READING_OPTIONAL_SECONDARY_FLAG\'\n                    elif c.strip():\n                        if c == \'[\':\n                            stack.append(c)\n                        arg_synopsis += c\n                        status = \'READING_OPTIONAL_FLAG_ARGUMENT\'\n                elif status == \'READING_FLAG_NAME\':\n                    if c == \' \' or c == \'|\':\n                        flag = self.split_flags(u_state, flag_name, optional=optional)\n                        flag_name = \'\'\n                        status = \'IDLE\'\n                    elif c == \']\':\n                        mark = stack.pop()\n                        if not mark.endswith(\'FLAG_NAME\'):\n                            flag = self.split_flags(u_state, flag_name, optional=optional)\n                            flag_name = \'\'\n                            status = \'IDLE\'\n                        else:\n                            flag_name += c\n                    elif c.strip():\n                        if c == \'[\':\n                            stack.append(c + \'FLAG_NAME\')\n                        flag_name += c\n                elif status == \'READING_FLAG_ARGUMENT\':\n                    if c == \' \':\n                        self.make_flag_argument(flag, arg_synopsis, optional=False)\n                        arg_synopsis = \'\'\n                        status = \'IDLE\'\n                    elif c == \']\':\n                        mark = stack.pop()\n                        if not mark.endswith(\'FLAG_ARGUMENT\'):\n                            self.make_flag_argument(flag, arg_synopsis, optional=False)\n                            arg_synopsis = \'\'\n                            status = \'IDLE\'\n                        else:\n                            arg_synopsis += c\n                    elif c.strip():\n                        if c == \'[\':\n                            stack.append(c + \'FLAG_ARGUMENT\')\n                        arg_synopsis += c\n                elif status == \'READING_OPTIONAL_FLAG_ARGUMENT\':\n                    if c == \' \':\n                        self.make_flag_argument(flag, arg_synopsis, optional=True)\n                        arg_synopsis = \'\'\n                        status = \'IDLE\'\n                    elif c == \']\':\n                        mark = stack.pop()\n                        if not mark.endswith(\'OPTIONAL_FLAG_ARGUMENT\'):\n                            self.make_flag_argument(flag, arg_synopsis, optional=True)\n                            arg_synopsis = \'\'\n                            status = \'IDLE\'\n                        else:\n                            arg_synopsis += c\n                    elif c.strip():\n                        if c == \'[\':\n                            stack.append(c + \'OPTIONAL_FLAG_ARGUMENT\')\n                        arg_synopsis += c\n                elif status == \'READING_OPTIONAL_SECONDARY_FLAG\':\n                    if c == \']\':\n                        mark = stack.pop()\n                        if mark.endswith(\'OPTIONAL_SECONDARY_FLAG\'):\n                            self.make_flag(u_state, optional_synopsis, optional=True)\n                            optional_synopsis = \'\'\n                        else:\n                            optional_synopsis += c\n                    elif c.strip():\n                        if c == \'[\':\n                            stack.append(c + \'OPTIONAL_SECONDARY_FLAG\')\n                        optional_synopsis += c\n\n    def split_flags(self, u_state, flag_name, optional=False):\n        """"""\n        If multiple flags were specified in the same synopsis, split them.\n        """"""\n        flag = None\n        if flag_name.endswith(\'::\'):\n            # split flags\n            flag_prefix = flag_name[0]\n            for i in xrange(1, len(flag_name)-2):\n                new_flag_name = flag_prefix + flag_name[i]\n                flag = FlagState(new_flag_name, optional=optional)\n                u_state.add_flag(flag)\n        else:\n            flag = FlagState(flag_name, optional=optional)\n            u_state.add_flag(flag)\n        return flag\n\n\nbg = BashGrammar()\nbg.make_grammar(os.path.join(os.path.dirname(__file__), \'grammar\', \'grammar100.txt\'))\n'"
bashlint/heredoc.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom bashlint import bast, errors\n\ndef gatherheredocuments(tokenizer):\n    # if we\'re at the end of the input and we\'re not strict, allow skipping\n    # reading the heredoc\n    while tokenizer.redirstack:\n        if tokenizer._peekc() is None and not tokenizer._strictmode:\n            tokenizer._shell_input_line_index += 1\n            return\n\n        redirnode, killleading = tokenizer.redirstack.pop(0)\n        makeheredoc(tokenizer, redirnode, 0, killleading)\n\ndef makeheredoc(tokenizer, redirnode, lineno, killleading):\n    # redirword = string_quote_removal(redirectnode.word)\n    redirword = redirnode.output.word\n    document = []\n\n    startpos = tokenizer._shell_input_line_index\n\n    #fullline = self.tok.readline(bool(redirword.output.flags & flags.word.QUOTED))\n    fullline = tokenizer.readline(False)\n    while fullline:\n        if killleading:\n            while fullline[0] == \'\\t\':\n                fullline = fullline[1:]\n\n        if not fullline:\n            continue\n\n        if fullline[:-1] == redirword and fullline[len(redirword)] == \'\\n\':\n            document.append(fullline[:-1])\n            # document_done\n            break\n\n        document.append(fullline)\n        #fullline = self.readline(bool(redirnode.flags & flags.word.QUOTED))\n        fullline = tokenizer.readline(False)\n\n    if not fullline:\n        raise errors.ParsingError(""here-document at line %d delimited by end-of-file (wanted %r)"" % (lineno, redirword), tokenizer._shell_input_line, tokenizer._shell_input_line_index)\n\n    document = \'\'.join(document)\n    endpos = tokenizer._shell_input_line_index - 1\n\n    assert hasattr(redirnode, \'heredoc\')\n    redirnode.heredoc = bast.node(kind=\'heredoc\', value=document,\n                                  pos=(startpos, endpos))\n\n    # if the heredoc immediately follows this node, fix its end pos\n    if redirnode.pos[1] + 1 == startpos:\n        redirnode.pos = (redirnode.pos[0], endpos)\n\n    return document\n'"
bashlint/lint.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""\nParse the option list of a bash command and assign each argument a type.\n\nOutput a Bashlex (https://github.com/idank/bashlex) AST augmented with the\nfollowing syntactic sugars:\n    1. every token is linked to its corresponding attach point:\n        flag -> utility,\n        argument -> utility,\n        argument -> flag;\n    2. the arguments are decorated with semantic types.\n\nReport syntactic errors and wrong flag usages if there is any.\n\nRelated repository:\n    - Bashlex (https://github.com/idank/bashlex)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport os\nimport re\nimport sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\n# bash grammar\nfrom bashlint.grammar import *\n\n# bashlex stuff\nfrom bashlint import bast, errors, tokenizer, bparser\nfrom bashlint.nast import *\n\nfrom nlp_tools import constants\n\n\ndef correct_errors_and_normalize_surface(cmd):\n    # special normalization for certain commands\n    ## remove all ""sudo""\'s\n    cmd = cmd.replace(""sudo"", """")\n\n    ## normalize utilities called with full path\n    cmd = cmd.replace(""/usr/bin/find"", ""find"")\n    cmd = cmd.replace(""/bin/find"", ""find"")\n    cmd = cmd.replace(""/usr/bin/grep"", ""grep"")\n    cmd = cmd.replace(""/bin/rm"", ""rm"")\n    cmd = cmd.replace(""/bin/mv"", ""mv"")\n    \n    ## correct common typos\n    cmd = cmd.replace(""\'{}\'"", ""{}"")\n    cmd = cmd.replace(""\\""{}\\"""", ""{}"")\n    cmd = cmd.replace(""-i{}"", ""-I {}"")\n    cmd = cmd.replace(""-i%"", ""-I %"")\n    cmd = cmd.replace(""-I{}"", ""-I {}"")\n    cmd = cmd.replace("" [] "", "" {} "")\n    cmd = cmd.replace(""-L."", ""-L"")\n    cmd = cmd.replace(""-mitime"", ""-mtime"")\n    cmd = cmd.replace(""-dev"", ""-xdev"")\n    cmd = cmd.replace(""-regex-type"", ""-regextype"")\n    cmd = cmd.replace("" ( "", "" \\\\( "")\n    cmd = cmd.replace("" ) "", "" \\\\) "")\n    cmd = cmd.replace(""-\\\\("", ""\\\\("")\n    cmd = cmd.replace(""-\\\\)"", ""\\\\)"")\n    cmd = cmd.replace(""\\""\\\\)"", "" \\\\)"")\n    cmd = cmd.replace(""\\\\(-"", ""\\\\( -"")\n    cmd = cmd.replace(""e\\\\)"", ""e \\\\)"")\n    cmd = cmd.replace(""-\\\\!"", ""!"")\n    try:\n        cmd = cmd.replace(""\xe2\x80\x94 "", ""-"")\n        cmd = cmd.replace(""\xe2\x80\x93"", ""-"")\n        cmd = cmd.replace(""\xe2\x80\x94"", ""-"")\n        cmd = cmd.replace(""\xe2\x80\x9c"", \'""\')\n        cmd = cmd.replace(""\xe2\x80\x9d"", \'""\')\n        cmd = cmd.replace(""-\\xd0\\xbe"", ""-o"")\n        cmd = cmd.replace(""\\xe2\\x80\\x93 "", ""-"")\n        cmd = cmd.replace(\'\xe2\x80\x98\', \'\\\'\')\n        cmd = cmd.replace(\'\xe2\x80\x99\', \'\\\'\')\n    except UnicodeDecodeError:\n        cmd = cmd.replace(""\xe2\x80\x94 "".decode(\'utf-8\'), ""-"")\n        cmd = cmd.replace(""\xe2\x80\x93"".decode(\'utf-8\'), ""-"")\n        cmd = cmd.replace(""\xe2\x80\x94"".decode(\'utf-8\'), ""-"")\n        cmd = cmd.replace(""\xe2\x80\x9c"".decode(\'utf-8\'), \'""\')\n        cmd = cmd.replace(""\xe2\x80\x9d"".decode(\'utf-8\'), \'""\')\n        cmd = cmd.replace(""\\xd0\\xbe"".decode(\'utf-8\'), ""o"")\n        cmd = cmd.replace(""\\xe2\\x80\\x93 "".decode(\'utf-8\') , ""-"")\n        cmd = cmd.replace(\'\xe2\x80\x98\'.decode(\'utf-8\'), \'\\\'\')\n        cmd = cmd.replace(\'\xe2\x80\x99\'.decode(\'utf-8\'), \'\\\'\')\n\n    # more typo fixes\n    cmd = re.sub(""-prin($| )"", \'-print\', cmd)\n    cmd = cmd.replace(""/bin/echo"", ""echo"")\n    cmd = cmd.replace("" exec sed "", "" -exec sed "")\n    cmd = cmd.replace("" xargs -iname "", "" xargs "")\n    cmd = cmd.replace("" -chour +1 "", "" -cmin 60 "")\n    cmd = cmd.replace("" -target-directory "", "" --target-directory="")\n    cmd = cmd.replace(""- perm"", ""-perm"")\n    cmd = cmd.replace("" perm"", "" -perm"")\n    cmd = cmd.replace(""\'-rd\\\\n\' "", \'\')\n\n    ## remove shell character\n    if cmd.startswith(""$ ""):\n        cmd = re.sub(""^\\$ "", \'\', cmd)\n    if cmd.startswith(""# ""):\n        cmd = re.sub(""^\\# "", \'\', cmd)\n    if cmd.startswith(""$find ""):\n        cmd = re.sub(""^\\$find "", ""find "", cmd)\n    if cmd.startswith(""#find ""):\n        cmd = re.sub(""^\\#find "", ""find "", cmd)\n\n    ## the first argument of ""tar"" is always interpreted as an option\n    tar_fix = re.compile(\' tar \\w\')\n    if cmd.startswith(\'tar\'):\n        cmd = \' \' + cmd\n    for w in re.findall(tar_fix, cmd):\n        cmd = cmd.replace(w, w.replace(\' tar \', \' tar -\'))\n    cmd = cmd.strip()\n\n    return cmd\n\ndef attach_to_tree(node, parent):\n    node.parent = parent\n    node.lsb = parent.get_right_child()\n    parent.add_child(node)\n    if node.lsb:\n        node.lsb.rsb = node\n\ndef detach_from_tree(node, parent):\n    if not parent:\n        return\n    parent.remove_child(node)\n    node.parent = None\n    if node.lsb:\n        node.lsb.rsb = node.rsb\n    if node.rsb:\n        node.rsb.lsb = node.lsb\n    node.rsb = None\n    node.lsb = None\n\n\ndef safe_bashlex_parse(cmd, start_pos=0, verbose=False):\n    """"""\n    Call bashlex with all exceptions properly catched.\n    """"""\n    def increment_bashlex_tree_offset(tree, offset):\n        if tree.kind == \'word\':\n            tree.pos = (tree.pos[0]+offset, tree.pos[1]+offset)\n        if tree.parts:\n            for child in tree.parts:\n                increment_bashlex_tree_offset(child, offset)\n    try:\n        tree = bparser.parse(cmd)\n        if start_pos > 0:\n            increment_bashlex_tree_offset(tree[0], start_pos)\n    except tokenizer.MatchedPairError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - MatchedPairError"" % cmd)\n        return None\n    except errors.ParsingError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - ParsingError"" % cmd)\n        return None\n    except NotImplementedError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - NotImplementedError"" % cmd)\n        return None\n    except IndexError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - IndexError"" % cmd)\n        # empty command\n        return None\n    except AttributeError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - AttributeError"" % cmd)\n        # not a bash command\n        return None\n    except AssertionError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - AssertionError"" % cmd)\n        # not a bash command\n        return None\n    except NameError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - NameError"" % cmd)\n        # not a bash command\n        return None\n    except TypeError:\n        if verbose:\n            print(""Bashlex cannot parse: %s - AssertionError"" % cmd)\n        return None\n    if len(tree) > 1:\n        if verbose:\n            print(""Doesn\'t support command with multiple root nodes: %s"" % cmd)\n        return None\n    return tree\n\ndef normalize_ast(cmd, recover_quotes=True, verbose=False):\n    """"""\n    Convert the bashlex parse tree of a command into the normalized form.\n\n    :param cmd: bash command to parse\n    :param recover_quotes: if set, retain quotation marks in the command\n    :param verbose: if set, print error message.\n    :return normalized_tree\n    """"""\n    cmd = cmd.replace(\'\\n\', \' \').strip()\n    cmd = correct_errors_and_normalize_surface(cmd)\n    if not cmd:\n        return None\n\n    def is_unary_logic_op(node, parent):\n        if node.word == ""!"":\n            return parent and parent.is_command(""find"")\n        return node.word in bash.right_associate_unary_logic_operators \\\n               or node.word in bash.left_associate_unary_logic_operators\n\n    def is_binary_logic_op(node, parent):\n        if node.word == \'-o\':\n            if parent and parent.is_command(""find""):\n                node.word = ""-or""\n                return True\n            else:\n                return False\n        if node.word == \'-a\':\n            if parent and parent.is_command(""find""):\n                node.word = ""-and""\n                return True\n            else:\n                return False\n        if node.word == \',\':\n            if parent and parent.is_command(""find""):\n                node.word = ""-and""\n                return True\n            else:\n                return False\n        return node.word in bash.binary_logic_operators\n\n    def is_parenthesis(node, parent):\n        if node.word in [\'(\', \')\', \'\\\\(\', \'\\\\)\']:\n            if parent and parent.is_command(\'find\'):\n                return True\n            else:\n                return False\n\n    def recover_node_quotes(node):\n        return cmd[node.pos[0] : node.pos[1]]\n\n    def normalize_word(node, recover_quotes=True):\n        w = recover_node_quotes(node) if recover_quotes else node.word\n        return w\n\n    def normalize_argument(node, current, arg_type):\n        value = normalize_word(node, recover_quotes)\n        norm_node = ArgumentNode(value=value, arg_type=arg_type)\n        attach_to_tree(norm_node, current)\n        return norm_node\n\n    def normalize_command(node, current=None):\n        bash_grammar = BashGrammar()\n        bash_grammar.name2type = bg.name2type\n\n        if not node or not node.parts:\n            return\n        input = node.parts\n        num_tokens = len(node.parts)\n\n        bast_node = input[0]\n        if bast_node.kind == \'assignment\':\n            normalize(bast_node, current, \'assignment\')\n        elif bast_node.kind == \'redirect\':\n            normalize(bast_node, current, \'redirect\')\n        elif bast_node.kind == \'commandsubstitution\':\n            normalize(bast_node, current, \'commandsubstitution\')\n        elif bast_node.kind == \'word\' and not bast_node.parts:\n            token = normalize_word(bast_node)\n            head = UtilityNode(token, parent=current, lsb=current.get_right_child())\n            if current:\n                current.add_child(head)\n\n            # If utility grammar is not known, parse into a simple two-level tree\n            if not bg.consume(token):\n                raise errors.LintParsingError(\n                    ""Warning: grammar not found - utility {}"".format(token), num_tokens, 0)\n                for bast_node in input[1:]:\n                    if bast_node.kind == \'word\' and (not bast_node.parts\n                            or (bast_node.parts[0].kind == \'parameter\' and\n                                bast_node.word.startswith(\'-\'))):\n                        token = normalize_word(bast_node)\n                        if token.startswith(\'-\'):\n                            child = FlagNode(token, parent=head, lsb=head.get_right_child())\n                        else:\n                            child = ArgumentNode(token, arg_type=\'Unknown\', parent=head,\n                                                 lsb=head.get_right_child())\n                        head.add_child(child)\n                    else:\n                        normalize(bast_node, head)\n                return\n\n            current, i = head, 1\n            bash_grammar.grammar = {head.value: copy.deepcopy(bg.grammar[head.value])}\n            bash_grammar.consume(head.value)\n\n            while i < len(input):\n                bast_node = input[i]\n                # \'--\': signal the end of options\n                if bast_node.kind == \'word\' and bast_node.word == \'--\':\n                    op = OperatorNode(\'--\', parent=current, lsb=current.get_right_child())\n                    current.add_child(op)\n                    bash_grammar.push(\'--\', OPERATOR_S)\n                    i += 1\n                    continue\n                # examine each possible next states in order\n                matched = False\n                for next_state in bash_grammar.next_states:\n                    if next_state.is_compound_flag():\n                        # Next state is a flag\n                        if bast_node.kind != \'word\' or (bast_node.parts and not (\n                                bast_node.word.startswith(\'-\') and\n                                    bast_node.parts[0].kind == \'parameter\')):\n                            continue\n                        if is_parenthesis(bast_node, current):\n                            flag = FlagNode(bast_node.word, parent=current,\n                                            lsb=current.get_right_child())\n                            current.add_child(flag)\n                            matched = True\n                            i += 1\n                            break\n                        elif is_unary_logic_op(bast_node, current):\n                            flag = UnaryLogicOpNode(bast_node.word, parent=current,\n                                                    lsb=current.get_right_child())\n                            current.add_child(flag)\n                            matched = True\n                            i += 1\n                            break\n                        elif is_binary_logic_op(bast_node, current):\n                            flag = BinaryLogicOpNode(bast_node.word, parent=current,\n                                                     lsb=current.get_right_child())\n                            current.add_child(flag)\n                            matched = True\n                            i += 1\n                            break\n                        else:\n                            token = normalize_word(bast_node)\n                            try:\n                                result = bash_grammar.push(token, COMPOUND_FLAG_S)\n                            except ValueError as e:\n                                raise errors.FlagError(e.args[0], num_tokens, i)\n                            if result:\n                                for flag_token, flag_arg in result:\n                                    flag = FlagNode(flag_token, parent=current,\n                                                    lsb=current.get_right_child())\n                                    current.add_child(flag)\n                                    if flag_arg == \'__OPEN__\':\n                                        # Incomplete AST, expecting flag argument\n                                        current = flag\n                                    elif flag_arg is not None:\n                                        # Argument is specified with flag\n                                        argument = ArgumentNode(flag_arg[0], arg_type=flag_arg[1],\n                                            parent=flag, lsb=flag.get_right_child())\n                                        flag.add_child(argument)\n                                matched = True\n                                i += 1\n                                break\n                    elif next_state.is_command():\n                        # Next state is a nested bash command\n                        new_command_node = bast.node(\n                            kind=""command"", word="""", parts=[], pos=(-1,-1))\n                        if next_state.type == ARG_COMMAND_S:\n                            if bast_node.kind == \'word\' and not bast_node.parts:\n                                token = normalize_word(bast_node)\n                                if constants.with_quotation(token):\n                                    subcommand = token[1:-1]\n                                    start_pos = bast_node.pos[0] + 1\n                                    tree = safe_bashlex_parse(subcommand, start_pos=start_pos,\n                                                              verbose=verbose)\n                                    if tree is None:\n                                        raise errors.SubCommandError(\n                                            \'Error in subcommand string: {}\'.format(token),\n                                            num_tokens, i)\n                                    normalize(tree[0], current)\n                                    bash_grammar.push(token, next_state.type)\n                                    i += 1\n                                else:\n                                    continue \n                            else:\n                                normalize(bast_node, current, \'command\')\n                                i += 1\n                        elif next_state.type == EXEC_COMMAND_S:\n                            new_input = []\n                            j = i\n                            while j < len(input):\n                                if hasattr(input[j], \'word\') and \\\n                                        input[j].word in next_state.stop_tokens:\n                                    break\n                                else:\n                                    new_input.append(input[j])\n                                    j += 1\n                            new_command_node.parts = new_input\n                            normalize_command(new_command_node, current)\n                            if j < len(input):\n                                current.value += (\'::\' + input[j].word)\n                                bash_grammar.push(input[j], EXEC_COMMAND_S)\n                            else:\n                                if verbose:\n                                    print(""Warning: -exec missing stop token - ; added"")\n                                current.value += (\'::\' + \';\')\n                                bash_grammar.push(\';\', EXEC_COMMAND_S)\n                            i = j + 1\n                        else:\n                            # Interpret all of the rest of the tokens as content of the nested command\n                            new_command_node.parts = input[i:]\n                            normalize_command(new_command_node, current)\n                            bash_grammar.push(\'\', next_state.type)\n                            i = len(input)\n                        current = current.utility\n                        matched = True\n                        break\n                    elif next_state.is_argument():\n                        # Next state is an argument\n                        if bast_node.kind == \'word\' and not bast_node.parts:\n                            token = normalize_word(bast_node)\n                            if next_state.is_list and next_state.list_separator != \' \':\n                                list_separator = next_state.list_separator\n                                argument = ArgumentNode(token, arg_type=next_state.arg_type,\n                                    parent=current, lsb=current.get_right_child(),\n                                    list_members=token.split(list_separator),\n                                    list_separator=list_separator)\n                            else:\n                                argument = ArgumentNode(token, arg_type=next_state.arg_type,\n                                    parent=current, lsb=current.get_right_child())\n                            current.add_child(argument)\n                            status = bash_grammar.push(token, ARG_S)\n                        else:\n                            normalize(bast_node, current, next_state.arg_type)\n                            status = bash_grammar.push(\'\', ARG_S)\n                        if status != \'__SAME_PARENT__\':\n                            current = current.utility\n                        i += 1\n                        matched = True\n                        break\n\n                if not matched:\n                    if bast_node.kind == \'redirect\' or bast_node.kind == \'operator\':\n                        i += 1\n                        matched = True\n                    else:\n                        raise errors.LintParsingError(\'Unmatched token\', num_tokens, i)\n\n            if bash_grammar.allow_eof():\n                post_process_command(head)\n                return\n            else:\n                raise errors.LintParsingError(\'Incomplete command\', num_tokens, i)\n        else:\n            if bast_node.parts:\n                normalize(bast_node, current)\n            else:\n                raise errors.LintParsingError(\n                    \'Utility needs to be a BAST node of ""Word"" type"" {}\'.format(bast_node),\n                    num_tokens, 0)\n\n    def post_process_command(head):\n        # process (embedded) parenthese -- treat as implicit ""-and""\n        def organize_buffer(lparenth, rparenth):\n            node = lparenth\n            while node != rparenth:\n                node = node.rsb\n            node = lparenth.rsb\n            if node.rsb == rparenth:\n                return lparenth.rsb\n            else:\n                norm_node = BracketNode()\n                while node != rparenth:\n                    attach_to_tree(node, norm_node)\n                    node = node.rsb\n                return norm_node\n\n        stack = []\n        depth = 0\n\n        def pop_stack_content(depth, rparenth, stack_top=None):\n            # popping pushed states off the stack\n            popped = stack.pop()\n            while (popped.value != ""(""):\n                head.remove_child(popped)\n                popped = stack.pop()\n            lparenth = popped\n            if not rparenth:\n                # unbalanced brackets\n                rparenth = ArgumentNode(value="")"")\n                make_parent_child(stack_top.parent, rparenth)\n                make_sibling(stack_top, rparenth)\n            new_child = organize_buffer(lparenth, rparenth)\n            i = head.substitute_parentheses(\n                lparenth, rparenth, new_child)\n            depth -= 1\n            if depth > 0:\n                # embedded parenthese\n                stack.append(new_child)\n            return depth, i\n\n        i = 0\n        while i < head.get_num_of_children():\n            child = head.children[i]\n            if child.value == ""("":\n                stack.append(child)\n                depth += 1\n            elif child.value == "")"":\n                assert(depth >= 0)\n                # fix imbalanced parentheses: missing \'(\'\n                if depth == 0:\n                    # simply drop the single \')\'\n                    detach_from_tree(child, child.parent)\n                else:\n                    depth, i = pop_stack_content(depth, child)\n            else:\n                if depth > 0:\n                    stack.append(child)\n\n            i += 1\n\n        # fix imbalanced parentheses: missing \')\'\n        while (depth > 0):\n            depth, _ = pop_stack_content(depth, None, stack[-1])\n\n        assert(len(stack) == 0)\n        assert(depth == 0)\n\n        # recover omitted arguments\n        if head.value == ""find"":\n            arguments = []\n            for child in head.children:\n                if child.is_argument():\n                    arguments.append(child)\n            if head.get_num_of_children() > 0 and len(arguments) < 1:\n                norm_node = ArgumentNode(value=""."", arg_type=""Path"")\n                make_sibling(norm_node, head.children[0])\n                norm_node.parent = head\n                head.children.insert(0, norm_node)\n\n        # ""grep"" normalization\n        if head.value == ""egrep"":\n            head.value = ""grep""\n            flag_present = False\n            for child in head.children:\n                if child.is_option() and child.value in [""-E"", ""--extended-regexp""]:\n                    flag_present = True\n            if not flag_present:\n                norm_node = FlagNode(value=""-E"")\n                if head.has_children():\n                    make_sibling(norm_node, head.children[0])\n                norm_node.parent = head\n                head.children.insert(0, norm_node)\n\n        if head.value == ""fgrep"":\n            head.value = ""grep""\n            flag_present = False\n            for child in head.children:\n                if child.is_option() and child.value in [""-F"", ""--fixed-strings""]:\n                    flag_present = True\n            if not flag_present:\n                norm_node = FlagNode(value=""-F"")\n                if head.has_children():\n                    make_sibling(norm_node, head.children[0])\n                norm_node.parent = head\n                head.children.insert(0, norm_node)\n\n        # ""xargs"" normalization\n        def normalize_replace_str(node, r_str, n_str):\n            for child in node.children:\n                if child.is_argument():\n                    if r_str in child.value:\n                        child.value = child.value.replace(r_str, n_str)\n                        if child.value == n_str:\n                            child.arg_type = ""ReservedWord""\n                else:\n                    normalize_replace_str(child, r_str, n_str)\n\n        has_repl_str = False\n        if head.value == ""xargs"":\n            for flag in head.get_flags():\n                if flag.value == ""-I"":\n                    has_repl_str = True\n                    repl_str = flag.get_argument()\n                    assert(repl_str is not None)\n                    if repl_str.value != ""{}"":\n                        utility = head.get_subcommand()\n                        assert(utility is not None)\n                        normalize_replace_str(utility, repl_str.value, \'{}\')\n                        repl_str.value = ""{}""\n                        repl_str.arg_type = ""ReservedWord""\n\n            # add -I {} if not present\n            utility = head.get_subcommand()\n            if not has_repl_str and utility is not None:\n                for i in xrange(head.get_num_of_children()):\n                    if head.children[i].is_utility():\n                        repl_str_flag_node = FlagNode(""-I"")\n                        repl_str_node = ArgumentNode(""{}"", ""ReservedWord"")\n                        repl_str_node2 = ArgumentNode(""{}"", ""ReservedWord"")\n\n                        head.children.insert(i, repl_str_flag_node)\n                        repl_str_flag_node.parent = head\n                        repl_str_flag_node.lsb = head.children[i-1]\n                        head.children[i-1].rsb = repl_str_flag_node\n\n                        make_parent_child(repl_str_flag_node, repl_str_node)\n                        sub_command = head.children[i+1]\n                        repl_str_node2.parent = sub_command\n                        repl_str_node2.lsb = sub_command.get_right_child()\n                        sub_command.children.append(repl_str_node2)\n                        break\n\n    def normalize(node, current, arg_type=""""):\n        # recursively normalize each subtree\n        if not type(node) is bast.node:\n            raise ValueError(\'type(node) is not bast.node\')\n        if node.kind == \'word\':\n            # assign fine-grained types\n            if node.parts:\n                # Compound arguments\n                # commandsubstitution, processsubstitution, parameter\n                if node.parts[0].kind == ""processsubstitution"":\n                    if \'>\' in node.word:\n                        norm_node = ProcessSubstitutionNode(\'>\')\n                        attach_to_tree(norm_node, current)\n                        for child in node.parts:\n                            normalize(child, norm_node)\n                    elif \'<\' in node.word:\n                        norm_node = ProcessSubstitutionNode(\'<\')\n                        attach_to_tree(norm_node, current)\n                        for child in node.parts:\n                            normalize(child, norm_node)\n                elif node.parts[0].kind == ""commandsubstitution"":\n                    norm_node = CommandSubstitutionNode()\n                    attach_to_tree(norm_node, current)\n                    for child in node.parts:\n                        normalize(child, norm_node)\n                elif (node.parts[0].kind == ""parameter"" or\n                      node.parts[0].kind == ""tilde""):\n                    normalize_argument(node, current, arg_type)\n                else:\n                    for child in node.parts:\n                        normalize(child, current)\n            else:\n                normalize_argument(node, current, arg_type)\n        elif node.kind == ""pipeline"":\n            norm_node = PipelineNode()\n            attach_to_tree(norm_node, current)\n            if len(node.parts) % 2 == 0:\n                raise ValueError(""Error: pipeline node must have odd number of parts (%d)""\n                      % len(node.parts))\n            for child in node.parts:\n                if child.kind == ""command"":\n                    normalize(child, norm_node)\n                elif not child.kind == ""pipe"":\n                    raise ValueError(\n                        ""Error: unrecognized type of child of pipeline node"")\n        elif node.kind == ""list"":\n            if len(node.parts) > 2:\n                # multiple commands, not supported\n                raise ValueError(""Unsupported: list of length >= 2"")\n            else:\n                normalize(node.parts[0], current)\n        elif node.kind == ""commandsubstitution"" or \\\n             node.kind == ""processsubstitution"":\n            normalize(node.command, current)\n        elif node.kind == ""command"":\n            try:\n                normalize_command(node, current)\n            except AssertionError:\n                raise AssertionError(""normalized_command AssertionError"")\n        elif hasattr(node, \'parts\'):\n            for child in node.parts:\n                # skip current node\n                normalize(child, current)\n        elif node.kind == ""redirect"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""operator"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""parameter"":\n            # not supported\n            raise ValueError(""Unsupported: parameters"")\n        elif node.kind == ""compound"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""list"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""for"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""if"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""while"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""until"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""assignment"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""function"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""tilde"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n        elif node.kind == ""heredoc"":\n            # not supported\n            raise ValueError(""Unsupported: %s"" % node.kind)\n\n    tree = safe_bashlex_parse(cmd, verbose=verbose)\n    if tree is None:\n        return tree\n\n    normalized_tree = Node(kind=""root"")\n    try:\n        normalize(tree[0], normalized_tree)\n    except ValueError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n    except AttributeError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n    except AssertionError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n    except errors.SubCommandError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n    except errors.LintParsingError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n    except errors.FlagError as err:\n        if verbose:\n            print(""%s - %s"" % (err.args[0], cmd))\n        return None\n\n    if len(normalized_tree.children) == 0:\n        # parsing not successful if the normalized tree consists of the root\n        # node only\n        return None\n\n    return normalized_tree\n\ndef serialize_ast(node, loose_constraints=False, ignore_flag_order=False):\n    if not node:\n        return \'\'\n\n    lc = loose_constraints\n    ifo = ignore_flag_order\n\n    def to_command_fun(node):\n        str = \'\'\n        if node.is_root():\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc:\n                for child in node.children:\n                    str += to_command_fun(child)\n            else:\n                str += to_command_fun(node.get_left_child())\n        elif node.kind == \'pipeline\':\n            assert(loose_constraints or node.get_num_of_children() > 1)\n            if lc and node.get_num_of_children() < 1:\n                str += \'\'\n            elif lc and node.get_num_of_children() == 1:\n                str += to_command_fun(node.get_left_child())\n            else:\n                for child in node.children[:-1]:\n                    str += to_command_fun(child)\n                    str += \' | \'\n                str += to_command_fun(node.get_right_child())\n        elif node.kind == ""commandsubstitution"":\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc and node.get_num_of_children() < 1:\n                str += \'\'\n            else:\n                str += \'$(\'\n                str += to_command_fun(node.get_left_child())\n                str += \')\'\n        elif node.kind == \'processsubstitution\':\n            assert(loose_constraints or node.get_num_of_children() == 1)\n            if lc and node.get_num_of_children() < 1:\n                str += \'\'\n            else:\n                str += \'{}(\'.format(node.value)\n                str += to_command_fun(node.get_left_child())\n                str += \')\'\n        elif node.is_utility():\n            str += node.value + \' \'\n            children = sorted(node.children, key=lambda x:x.value) \\\n                if ifo else node.children\n            for child in children:\n                str += to_command_fun(child) + \' \'\n            str = str.strip()\n        elif node.is_option():\n            assert(loose_constraints or node.parent)\n            if \'::\' in node.value:\n                value, op = node.value.split(\'::\')\n                str += value + \' \'\n            else:\n                arg_connector = \'=\' if (node.is_long_option() and\n                                        node.children) else \' \'\n                str += node.value + arg_connector\n            for child in node.children:\n                str += to_command_fun(child) + \' \'\n            if \'::\' in node.value:\n                if op == \';\':\n                    op = ""\\\\;""\n                str += op + \' \'\n            str = str.strip()\n        elif node.kind == \'operator\':\n            str += \'--\'\n        elif node.kind == ""binarylogicop"":\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            if lc and node.get_num_of_children() > 0:\n                for child in node.children[:-1]:\n                    str += to_command_fun(child) + \' \'\n                    str += node.value + \' \'\n                str += to_command_fun(node.children[-1])\n                str = str.strip()\n            else:\n                str += node.value\n        elif node.kind == ""unarylogicop"":\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            if lc and node.get_num_of_children() > 0:\n                if node.associate == UnaryLogicOpNode.RIGHT:\n                    str += \'{} {}\'.format(\n                        node.value, to_command_fun(node.get_left_child()))\n                else:\n                    str += \'{} {}\'.format(\n                        to_command_fun(node.get_left_child()), node.value)\n            else:\n                str += node.value\n        elif node.kind == ""bracket"":\n            assert(loose_constraints or node.get_num_of_children() >= 1)\n            if lc and node.get_num_of_children() < 2:\n                for child in node.children:\n                    str += to_command_fun(child)\n            else:\n                str += ""\\\\( ""\n                for i in xrange(len(node.children)):\n                    str += to_command_fun(node.children[i]) + \' \'\n                str += ""\\\\)""\n        elif node.is_argument():\n            assert(loose_constraints or node.get_num_of_children() == 0)\n            str += node.value\n            if lc:\n                for child in node.children:\n                    str += to_command_fun(child)\n        return str\n\n    return to_command_fun(node)\n\n\ndef get_utility_statistics(utility):\n    return len(bg.grammar[utility].compound_flag.flag_index)\n'"
bashlint/nast.py,0,"b'""""""\nNode Classes for the Normalized Bash AST.\n""""""\n\nimport collections\n\nfrom bashlint import bash\n\n_H_NO_EXPAND = \'__SP__H_NO_EXPAND\'\n_V_NO_EXPAND = \'__SP__V_NO_EXPAND\'\n\nKIND_PREFIX = \'<KIND_PREFIX>\'\n\n\ndef make_parent_child(parent, child):\n    parent.add_child(child)\n    child.parent = parent\n\ndef make_sibling(lsb, rsb):\n    if lsb:\n        lsb.rsb = rsb\n    if rsb:\n        rsb.lsb = lsb\n\nclass Node(object):\n    num_child = -1          # number of children taken by node\n                            # -1 indicates ""any number of""\n    children_types = []     # list of compatible types of children\n\n    def __init__(self, parent=None, lsb=None, kind="""", value=""""):\n        """"""\n        :member parent: pointer to parent node\n        :member lsb: pointer to left sibling node\n        :member rsb: pointer to right sibling node\n        :member kind: [\'pipeline\',\n                      \'utility\',\n                      \'unarylogicop\',\n                      \'binarylogicop\'\n                      \'flag\',\n                      \'root\',\n                      \'argument\',\n                      \'commandsubstitution\',\n                      \'processsubstitution\',\n                      \'bracket\'\n                     ]\n        :member value: string value of the node\n        :member children: list of child nodes\n        """"""\n        self.parent = parent\n        self.lsb = lsb\n        self.rsb = None\n        self.kind = kind\n        self.value = value\n        self.children = []\n\n    def add_child(self, child, index=None):\n        lsb = self.get_right_child()\n        self.children.append(child)\n        if lsb:\n            lsb.rsb = child\n\n    def get_children(self):\n        return self.children\n\n    # node label used for evaluation ONLY\n    def get_label(self):\n        return self.kind.upper() + ""_"" + self.value\n\n    def get_left_child(self):\n        if len(self.children) >= 1:\n            return self.children[0]\n        return None\n\n    def get_right_child(self):\n        if len(self.children) >= 1:\n            return self.children[-1]\n        return None\n\n    def get_2nd_right_child(self):\n        if len(self.children) >= 2:\n            return self.children[-2]\n        return None\n\n    def get_num_of_children(self):\n        return len(self.children)\n\n    def has_children(self):\n        return len(self.children) > 0\n\n    def is_reserved(self):\n        if self.kind != ""argument"":\n            return True\n\n    def is_command(self, value):\n        return self.kind == ""utility"" and self.value == value\n\n    def is_utility(self):\n        return self.kind == ""utility""\n\n    def is_open_vocab(self):\n        return False\n\n    def is_option(self):\n        return self.kind == ""flag""\n\n    def is_argument(self):\n        return self.kind == ""argument""\n\n    def is_root(self):\n        return self.kind == ""root""\n\n    def remove_child(self, child):\n        if child in self.children:\n            self.children.remove(child)\n\n    def remove_child_by_index(self, index):\n        self.children.pop(index)\n\n    def replace_child(self, child, new_child):\n        new_child.parent = child.parent\n        index = self.children.index(child)\n        self.remove_child(child)\n        self.children.insert(index, new_child)\n        make_sibling(child.lsb, new_child)\n        make_sibling(new_child, child.rsb)\n\n    def substitute_parentheses(self, lp, rp, new_child):\n        # substitute parenthese expression with single node\n        assert(lp.parent == rp.parent)\n        new_child.parent = rp.parent\n        make_sibling(lp.lsb, new_child)\n        make_sibling(new_child, rp.rsb)\n        index = self.children.index(lp)\n        self.remove_child(lp)\n        self.remove_child(rp)\n        self.children.insert(index, new_child)\n        return index\n\n    @property\n    def prefix(self):\n        return self.kind.upper() + KIND_PREFIX\n\n    @property\n    def symbol(self):\n        return self.prefix + self.value\n\n    @property\n    def utility(self):\n        ancester = self\n        while ancester is not None:\n            if ancester.kind == ""utility"":\n                return ancester\n            # if no parent utility is detect, return ""root""\n            ancester = ancester.parent\n        raise ValueError(\'No head utility found!\')\n\n    @property\n    def grandparent(self):\n        return self.parent.parent\n\nclass UtilityNode(Node):\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(UtilityNode, self).__init__(parent, lsb, ""utility"", value)\n        self.arg_dict = {\'\': collections.defaultdict(int)}\n\n    def add_child(self, child, index=None):\n        super(UtilityNode, self).add_child(child)\n        if child.is_argument() and not child.is_bracket():\n            # command argument\n            self.arg_dict[\'\'][child.arg_type] += 1\n            child.set_index(self.arg_dict[\'\'][child.arg_type])\n\n    def get_flags(self):\n        flags = []\n        for child in self.children:\n            if child.is_option():\n                flags.append(child)\n        return flags\n\n    def get_subcommand(self):\n        for child in self.children:\n            if child.is_utility():\n                return child\n\nclass FlagNode(Node):\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(FlagNode, self).__init__(parent, lsb, ""flag"", value)\n\n    def add_child(self, child, index=None):\n        super(FlagNode, self).add_child(child)\n        if child.is_argument():\n            if not self.value in self.utility.arg_dict:\n                self.utility.arg_dict[self.value] \\\n                    = collections.defaultdict(int)\n            self.utility.arg_dict[self.value][child.arg_type] += 1\n            child.set_index(\n                self.utility.arg_dict[self.value][child.arg_type])\n\n    def get_argument(self):\n        for child in self.children:\n            if child.kind == ""argument"":\n                return child\n\n    def is_long_option(self):\n        return self.value.startswith(\'--\')\n\nclass ArgumentNode(Node):\n    num_child = 0\n\n    def __init__(self, value=\'\', arg_type=\'\', parent=None, lsb=None,\n                 list_members=None, list_separator=None):\n        super(ArgumentNode, self).__init__(parent, lsb, ""argument"", value)\n        self.arg_type = arg_type\n        self.index = 1\n        self.list_separator = list_separator\n        self.list_members = list_members\n\n    def is_bracket(self):\n        return self.value == ""("" or self.value == "")""\n    \n    def is_reserved(self):\n        return self.value in bash.reserved_tokens\n\n    def is_open_vocab(self):\n        if self.is_reserved():\n            return False\n        if self.arg_type == \'Type\':\n            return False\n        if self.arg_type == \'Option\':\n            return False\n        if self.arg_type == \'Format\':\n            return False\n        # if self.arg_type == ""Size"":\n        #     return False\n        # if self.arg_type == ""Time"":\n        #     return False\n        # if self.arg_type == ""Number"":\n        #     return False\n        # if self.arg_type == ""Permission"":\n        #     return False\n        return True\n\n    def to_index(self):\n        if self.parent.kind == ""utility"":\n            return self.utility.arg_dict[\'\'][self.arg_type] > 1\n        else:\n            return self.utility.arg_dict[self.parent.value][self.arg_type] > 1\n\n    def set_index(self, ind):\n        self.index = ind\n\nclass OperatorNode(Node):\n    num_child = 0\n\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(OperatorNode, self).__init__(\n            parent, lsb, kind=\'operator\', value=value)\n\nclass UnaryLogicOpNode(Node):\n    num_child = 1\n    children_types = [set([\'flag\', \'bracket\', \'unarylogicop\', \'binarylogicop\'])]\n    LEFT = 0\n    RIGHT = 1\n\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(UnaryLogicOpNode, self).__init__(parent, lsb, \'unarylogicop\', value)\n        if value in bash.right_associate_unary_logic_operators:\n            self.associate = UnaryLogicOpNode.RIGHT\n        elif value in bash.left_associate_unary_logic_operators:\n            self.associate = UnaryLogicOpNode.LEFT\n        else:\n            raise ValueError(""Unrecognized unary logic operator: {}"".format(value))\n\nclass BinaryLogicOpNode(Node):\n    num_child = -1\n    children_types = [set([\'flag\', \'bracket\', \'unarylogicop\', \'binarylogicop\'])]\n\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(BinaryLogicOpNode, self).__init__(parent, lsb, \'binarylogicop\', value)\n\nclass BracketNode(Node):\n    num_child = -1\n    children_types = [set([\'flag\', \'bracket\', \'unarylogicop\', \'binarylogicop\'])]\n\n    def __init__(self, parent=None, lsb=None):\n        super(BracketNode, self).__init__(parent, lsb, \'bracket\', \'\')\n\nclass RedirectNode(Node):\n    num_child = 2\n\n    def __init__(self, value=\'\', parent=None, lsb=None):\n        super(RedirectNode, self).__init__(parent, lsb, \'redirect\', value)\n\nclass PipelineNode(Node):\n    children_types = [set([\'utility\'])]\n\n    def __init__(self, parent=None, lsb=None):\n        super(PipelineNode, self).__init__(parent, lsb, \'pipeline\')\n\nclass CommandSubstitutionNode(Node):\n    num_child = 1\n    children_types = [set([\'pipe\', \'utility\'])]\n\n    def __init__(self, parent=None, lsb=None):\n        super(CommandSubstitutionNode, self).__init__(parent, lsb)\n        self.kind = ""commandsubstitution""\n\nclass ProcessSubstitutionNode(Node):\n    num_child = 1\n    children_types = [set([\'pipe\', \'utility\'])]\n\n    def __init__(self, value, parent=None, lsb=None):\n        super(ProcessSubstitutionNode, self).__init__(parent, lsb)\n        self.kind = ""processsubstitution""\n        if value in [""<"", "">""]:\n            self.value = value\n        else:\n            raise ValueError(""Value of a processsubstitution has to be \'<\' or \'>\'."")\n'"
bashlint/rewrites.py,0,"b'""""""\nExtract bash templates that are semantically equivalent from a parallel corpus.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os, sys\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom bashlint import data_tools\nfrom nlp_tools import tokenizer\n\n\ndef extract_rewrites(data):\n    """"""Extract all pairs of rewrites from a parallel corpus.""""""\n    nls, cms = data\n\n    # Step 1: group pairs with the same natural language description.\n    group_pairs_by_nl = collections.defaultdict(set)\n    for nl, cm in zip(nls, cms):\n        nl = nl.strip()\n        cm = cm.strip()\n        if nl.lower() == ""na"":\n            continue\n        if not nl:\n            continue\n        if not cm:\n            continue\n        nl_tokens, _ = tokenizer.ner_tokenizer(nl)\n        nl_temp = \' \'.join(nl_tokens)\n        cm_temp = data_tools.cmd2template(cm)\n        if not cm_temp in group_pairs_by_nl[nl_temp]:\n            group_pairs_by_nl[nl_temp].add(cm_temp)\n\n    # Step 2: cluster the commands with the same natural language explanations.\n    merged = set()\n    nls = group_pairs_by_nl.keys()\n    for i in xrange(len(nls)):\n        nl = nls[i]\n        cm_temp_set = group_pairs_by_nl[nl]\n        for j in xrange(i+1, len(nls)):\n            nl2 = nls[j]\n            cm_temp_set2 = group_pairs_by_nl[nl2]\n            if len(cm_temp_set & cm_temp_set2) >= 2:\n                for cm_temp in cm_temp_set:\n                    if not cm_temp in group_pairs_by_nl[nl2]:\n                        group_pairs_by_nl[nl2].add(cm_temp)\n                merged.add(i)\n\n    # Step 3: remove redundant clusters after merge.\n    rewrites = {}\n    for i in xrange(len(nls)):\n        if not i in merged:\n            rewrites[nls[i]] = group_pairs_by_nl[nls[i]]\n\n    # Step 4: print extracted rewrites and store in database.\n    for nl, cm_temps in sorted(rewrites.items(), key=lambda x: len(x[1]),\n                               reverse=True):\n        if len(cm_temps) >= 2:\n            for cm_temp1 in cm_temps:\n                for cm_temp2 in cm_temps:\n                    if cm_temp1 == cm_temp2:\n                        continue\n                    print(""* {} --> {}"".format(cm_temp1, cm_temp2))\n            print()'"
bashlint/shutils.py,0,"b'def single_quote(s):\n    if s[0] == ""\'"" and len(s) == 1:\n        return ""\\\\\'""\n\n    l = [""\'""]\n\n    for c in s:\n        l.append(c)\n        if c == ""\'"":\n            l.extend([""\\\\\'\'""])\n\n    l.append(""\'"")\n\n    return \'\'.join(l)\n\ndef double_quote(s):\n    return s\n\ndef legal_number(s):\n    try:\n        x = int(s)\n        return True\n    except ValueError:\n        return False\n\ndef legal_identifier(name):\n    pass\n\ndef removequotes(s, heredoc=False, doublequotes=False):\n    r = \'\'\n    sindex = 0\n    dquote = False\n    while sindex < len(s):\n        c = s[sindex]\n        if c == \'\\\\\':\n            sindex += 1\n            if sindex == len(s):\n                r += \'\\\\\'\n                return r\n            c = s[sindex]\n            if ((heredoc and doublequotes) or dquote) and not _shellquote(c):\n                r += \'\\\\\'\n            r += c\n        elif c == ""\'"":\n            if (heredoc and doublequotes) or dquote:\n                r += c\n                sindex += 1\n            else:\n                t = s.find(""\'"", sindex + 1)\n                if t == -1:\n                    t = len(s)\n                else:\n                    t += 1\n\n                r += s[sindex + 1:t-1]\n                sindex = t\n        elif c == \'""\':\n            dquote = not dquote\n            sindex += 1\n        else:\n            r += c\n            sindex += 1\n    return r\n'"
bashlint/state.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom bashlint import flags, butils\n\nparserstate = lambda: butils.typedset(flags.parser)\n'"
bashlint/subst.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nfrom bashlint import bast, flags, tokenizer, errors\n\ndef _recursiveparse(parserobj, base, sindex, tokenizerargs=None):\n    # TODO: fix this hack that prevents mutual import\n    from bashlint import bparser\n\n    tok = parserobj.tok\n\n    if tokenizerargs is None:\n        tokenizerargs = {\'parserstate\' : copy.copy(tok._parserstate),\n                         \'lastreadtoken\' : tok._last_read_token,\n                         \'tokenbeforethat\' : tok._token_before_that,\n                         \'twotokensago\' : tok._two_tokens_ago}\n\n    string = base[sindex:]\n    newlimit = parserobj._expansionlimit\n    if newlimit is not None:\n        newlimit -= 1\n    p = bparser._parser(string, tokenizerargs=tokenizerargs,\n                        expansionlimit=newlimit)\n    node = p.parse()\n\n    endp = node.pos[1]\n    _adjustpositions(node, sindex, len(base))\n\n    return node, endp\n\ndef _parsedolparen(parserobj, base, sindex):\n    copiedps = copy.copy(parserobj.parserstate)\n    copiedps.add(flags.parser.CMDSUBST)\n    copiedps.add(flags.parser.EOFTOKEN)\n    string = base[sindex:]\n\n    tokenizerargs = {\'eoftoken\' : tokenizer.token(tokenizer.tokentype.RIGHT_PAREN, \')\'),\n                     \'parserstate\' : copiedps,\n                     \'lastreadtoken\' : parserobj.tok._last_read_token,\n                     \'tokenbeforethat\' : parserobj.tok._token_before_that,\n                     \'twotokensago\' : parserobj.tok._two_tokens_ago}\n\n    node, endp = _recursiveparse(parserobj, base, sindex, tokenizerargs)\n\n    if string[endp] != \')\':\n        while endp > 0 and string[endp-1] == \'\\n\':\n            endp -= 1\n\n    return node, sindex + endp\n\ndef _extractcommandsubst(parserobj, string, sindex, sxcommand=False):\n    if string[sindex] == \'(\':\n        raise NotImplementedError(\'arithmetic expansion\')\n        #return _extractdelimitedstring(parserobj, string, sindex, \'$(\', \'(\', \'(\', sxcommand=True)\n    else:\n        node, si = _parsedolparen(parserobj, string, sindex)\n        si += 1\n        return bast.node(kind=\'commandsubstitution\', command=node, pos=(sindex - 2, si)), si\n\ndef _extractprocesssubst(parserobj, string, sindex):\n    #return _extractdelimitedstring(tok, string, sindex, starter, \'(\', \')\', sxcommand=True)\n    node, si = _parsedolparen(parserobj, string, sindex)\n    return node, si + 1\n\n#def _extractdelimitedstring(parserobj, string, sindex, opener, altopener, closer,\n#                            sxcommand=False):\n#    parts = []\n#    incomment = False\n#    passchar = False\n#    nestinglevel = 1\n#    i = sindex\n\n#    while nestinglevel:\n#        if i >= len(string):\n#            break\n#        c = string[i]\n#        if incomment:\n#            if c == \'\\n\':\n#                incomment = False\n#            i += 1\n#            continue\n#        elif passchar:\n#            passchar = False\n#            i += 1\n#            continue\n\n#        if sxcommand and c == \'#\' and (i == 0 or string[i-1] == \'\\n\' or\n#                                       tokenizer._shellblank(string[i-1])):\n#            incomment = True\n#            i += 1\n#            continue\n\n#        if c == \'\\\\\':\n#            passchar = True\n#            i += 1\n#            continue\n\n#        if sxcommand and string[i:i+2] == \'$(\':\n#            si = i + 2\n#            node, si = _extractcommandsubst(parserobj, string, si, sxcommand=sxcommand)\n#            parts.append(node)\n#            i = si + 1\n#            continue\n\n#        if string.startswith(opener, i):\n#            si = i + len(opener)\n#            nodes, si = _extractdelimitedstring(parserobj, string, si, opener, altopener,\n#                                                closer, sxcommand=sxcommand)\n#            parts.extend(nodes)\n#            i = si + 1\n#            continue\n\n#        if string.startswith(altopener, i):\n#            si = i + len(altopener)\n#            nodes, si = _extractdelimitedstring(parserobj, string, si, altopener, altopener,\n#                                                closer, sxcommand=sxcommand)\n#            parts.extend(nodes)\n#            i = si + 1\n#            continue\n\n#        # 1327\n#        if string.startswith(closer, i):\n#            i += len(closer) - 1\n#            nestinglevel -= 1\n#            if nestinglevel == 0:\n#                break\n\n#        if c == \'`\':\n#            si = i + 1\n#            t = _stringextract(string, si, \'`\', sxcommand=sxcommand)\n#            i = si + 1\n#            continue\n\n#        if c in ""\'\\"""":\n#            si = i +1\n#            if c == \'""\':\n#                i = _skipsinglequoted(string, si)\n#            else:\n#                i = _skipdoublequoted(string, si)\n#            continue\n\n#        i += 1\n\n#    if i == len(string) and nestinglevel:\n#        raise errors.ParsingError(\'bad substitution: no closing %r in %s\' % (closer, string))\n\n#    return parts, i\n\ndef _paramexpand(parserobj, string, sindex):\n    node = None\n    zindex = sindex + 1\n    c = string[zindex] if zindex < len(string) else None\n    if c and c in \'0123456789$#?-!*@\':\n        # XXX 7685\n        node = bast.node(kind=\'parameter\', value=c,\n                         pos=(sindex, zindex+1))\n    elif c == \'{\':\n        # XXX 7863\n        # TODO not start enough, doesn\'t consider escaping\n        zindex = string.find(\'}\', zindex + 1)\n        node = bast.node(kind=\'parameter\', value=string[sindex + 2:zindex],\n                         pos=(sindex, zindex+1))\n        # TODO\n        # return _parameterbraceexpand(string, zindex)\n    elif c == \'(\':\n        return _extractcommandsubst(parserobj, string, zindex + 1)\n    elif c == \'[\':\n        raise NotImplementedError(\'arithmetic substitution\')\n        #return _extractarithmeticsubst(string, zindex + 1)\n    else:\n        tindex = zindex\n        for zindex in range(tindex, len(string) + 1):\n            if zindex == len(string):\n                break\n            if not string[zindex].isalnum() and not string[zindex] == \'_\':\n                break\n        temp1 = string[sindex:zindex]\n        if temp1:\n            return (bast.node(kind=\'parameter\', value=temp1[1:], pos=(sindex, zindex)),\n                    zindex)\n\n    if zindex < len(string):\n        zindex += 1\n\n    return node, zindex\n\ndef _adjustpositions(node_, base, endlimit):\n    class v(bast.nodevisitor):\n        def visitnode(self, node):\n            assert node.pos[1] + base <= endlimit\n            node.pos = (node.pos[0] + base, node.pos[1] + base)\n    visitor = v()\n    visitor.visit(node_)\n\ndef _expandwordinternal(parserobj, wordtoken, qheredocument, qdoublequotes, quoted, isexp):\n    # bash/subst.c L8132\n    istring = \'\'\n    parts = []\n    tindex = [0]\n    sindex = [0]\n    string = wordtoken.value\n    def nextchar():\n        sindex[0] += 1\n        if sindex[0] < len(string):\n            return string[sindex[0]]\n    def peekchar():\n        if sindex[0]+1 < len(string):\n            return string[sindex[0]+1]\n\n    while True:\n        if sindex[0] == len(string):\n            break\n            # goto finished_with_string\n        c = string[sindex[0]]\n        if c in \'<>\':\n            if (nextchar() != \'(\' or qheredocument or qdoublequotes or\n                (wordtoken.flags & set([flags.word.DQUOTE, flags.word.NOPROCSUB]))):\n                sindex[0] -= 1\n\n                # goto add_character\n                sindex[0] += 1\n                istring += c\n            else:\n                tindex = sindex[0] + 1\n\n                node, sindex[0] = _extractprocesssubst(parserobj, string, tindex)\n\n                parts.append(bast.node(kind=\'processsubstitution\', command=node,\n                                       pos=(tindex - 2, sindex[0])))\n                istring += string[tindex - 2:sindex[0]]\n                # goto dollar_add_string\n        # TODO\n        # elif c == \'=\':\n        #     pass\n        # elif c == \':\':\n        #     pass\n        elif c == \'~\':\n            if (wordtoken.flags & set([flags.word.NOTILDE, flags.word.DQUOTE]) or\n                (sindex[0] > 0 and not (wordtoken.flags & flags.word.NOTILDE)) or\n                qdoublequotes or qheredocument):\n                wordtoken.flags.clear()\n                wordtoken.flags.add(flags.word.ITILDE)\n                sindex[0] += 1\n                istring += c\n            else:\n                stopatcolon = wordtoken.flags & set([flags.word.ASSIGNRHS,\n                                                    flags.word.ASSIGNMENT,\n                                                    flags.word.TILDEEXP])\n                expand = True\n                for i in range(sindex[0], len(string)):\n                    r = string[i]\n                    if r == \'/\':\n                        break\n                    if r in ""\\\\\'\\"""":\n                        expand = False\n                        break\n                    if stopatcolon and r == \':\':\n                        break\n                else:\n                    # go one past the end if we didn\'t exit early\n                    i += 1\n\n                if i > sindex[0] and expand:\n                    node = bast.node(kind=\'tilde\', value=string[sindex[0]:i],\n                                     pos=(sindex[0], i))\n                    parts.append(node)\n                istring += string[sindex[0]:i]\n                sindex[0] = i\n\n        elif c == \'$\' and len(string) > 1:\n            tindex = sindex[0]\n            node, sindex[0] = _paramexpand(parserobj, string, sindex[0])\n            if node:\n                parts.append(node)\n            istring += string[tindex:sindex[0]]\n        elif c == \'`\':\n            tindex = sindex[0]\n            # bare instance of ``\n            if nextchar() == \'`\':\n                sindex[0] += 1\n                istring += \'``\'\n            else:\n                x = _stringextract(string, sindex[0], ""`"")\n                if x == -1:\n                    raise errors.ParsingError(\'bad substitution: no closing ""`"" \'\n                                              \'in %s\' % string)\n                else:\n                    if wordtoken.flags & flags.word.NOCOMSUB:\n                        pass\n                    else:\n                        sindex[0] = x\n\n                        word = string[tindex+1:sindex[0]]\n                        command, ttindex = _recursiveparse(parserobj, word, 0)\n                        _adjustpositions(command, tindex+1, len(string))\n                        ttindex += 1 # ttindex is on the closing char\n\n                        # assert sindex[0] == ttindex\n                        # go one past the closing `\n                        sindex[0] += 1\n\n                        node = bast.node(kind=\'commandsubstitution\',\n                                         command=command,\n                                         pos=(tindex, sindex[0]))\n                        parts.append(node)\n                        istring += string[tindex:sindex[0]]\n\n        elif c == \'\\\\\':\n            istring += string[sindex[0]+1:sindex[0]+2]\n            sindex[0] += 2\n        elif c == \'""\':\n            sindex[0] += 1\n            continue\n\n            # 8513\n            #if qdoublequotes or qheredocument:\n            #    sindex[0] += 1\n            #else:\n            #    tindex = sindex[0] + 1\n            #    parts, sindex[0] = _stringextractdoublequoted(string, sindex[0])\n            #    if tindex == 1 and sindex[0] == len(string):\n            #        quotedstate = \'wholly\'\n            #    else:\n            #        quotedstate = \'partially\'\n\n        elif c == ""\'"":\n            # entire string surronded by single quotes, no expansion is\n            # going to happen\n            if sindex[0] == 0 and string[-1] == ""\'"":\n                return [], string[1:-1]\n\n            # check if we\'re inside double quotes\n            if not qdoublequotes:\n                # look for the closing \', we know we have one or otherwise\n                # this wouldn\'t tokenize due to unmatched \'\n                tindex = sindex[0]\n                sindex[0] = string.find(""\'"", sindex[0]) + 1\n\n                istring += string[tindex+1:sindex[0]-1]\n            else:\n                # this is a single quote inside double quotes, add it\n                istring += c\n                sindex[0] += 1\n        else:\n            istring += string[sindex[0]:sindex[0]+1]\n            sindex[0] += 1\n\n    if parts:\n        class v(bast.nodevisitor):\n            def visitnode(self, node):\n                assert node.pos[1] + wordtoken.lexpos <= wordtoken.endlexpos\n                node.pos = (node.pos[0] + wordtoken.lexpos,\n                            node.pos[1] + wordtoken.lexpos)\n        visitor = v()\n        for node in parts:\n            visitor.visit(node)\n\n    return parts, istring\n\ndef _stringextract(string, sindex, charlist, sxvarname=False):\n    found = False\n    i = sindex\n    while i < len(string):\n        c = string[i]\n        if c == \'\\\\\':\n            if i + 1 < len(string):\n                i += 1\n            else:\n                break\n        elif sxvarname and c == \'[\':\n            ni = _skipsubscript(string, i, 0)\n            if string[ni] == \']\':\n                i = ni\n        elif c in charlist:\n            found = True\n            break\n        else:\n            i += 1\n    if found:\n        return i\n    else:\n        return -1\n'"
bashlint/tokenizer.py,0,"b'import re, collections, enum\n\nfrom bashlint import flags, shutils, butils, errors, heredoc, state\n\nsh_syntaxtab = collections.defaultdict(set)\n\ndef _addsyntax(chars, symbol):\n    for c in chars:\n        sh_syntaxtab[c].add(symbol)\n\n_addsyntax(\'\\\\`$""\\n\', \'dquote\')\n_addsyntax(\'()<>;&|\', \'meta\')\n_addsyntax(\'""`\\\'\', \'quote\')\n_addsyntax(\'$<>\', \'exp\')\n_addsyntax(""()<>;&| \\t\\n"", \'break\')\n\ndef _shellblank(c):\n    return c in \' \\t\'\n\ndef _shellmeta(c):\n    return \'meta\' in sh_syntaxtab[c]\n\ndef _shellquote(c):\n    return \'quote\' in sh_syntaxtab[c]\n\ndef _shellexp(c):\n    return \'exp\' in sh_syntaxtab[c]\n\ndef _shellbreak(c):\n    return \'break\' in sh_syntaxtab[c]\n\nclass tokentype(enum.Enum):\n    IF = 1\n    THEN = 2\n    ELSE = 3\n    ELIF = 4\n    FI = 5\n    CASE = 6\n    ESAC = 7\n    FOR = 8\n    SELECT = 9\n    WHILE = 10\n    UNTIL = 11\n    DO = 12\n    DONE = 13\n    FUNCTION = 14\n    COPROC = 15\n    COND_START = 16\n    COND_END = 17\n    COND_ERROR = 18\n    IN = 19\n    BANG = \'!\'\n    TIME = 21\n    TIMEOPT = 22\n    TIMEIGN = 23\n    WORD = 24\n    ASSIGNMENT_WORD = 25\n    REDIR_WORD = 26\n    NUMBER = 27\n    ARITH_CMD = 28\n    ARITH_FOR_EXPRS = 29\n    COND_CMD = 30\n    AND_AND = \'&&\'\n    OR_OR = \'||\'\n    GREATER_GREATER = \'>>\'\n    LESS_LESS = \'<<\'\n    LESS_AND = \'<&\'\n    LESS_LESS_LESS = \'<<<\'\n    GREATER_AND = \'>&\'\n    SEMI_SEMI = \';;\'\n    SEMI_AND = \';&\'\n    SEMI_SEMI_AND = \';;&\'\n    LESS_LESS_MINUS = \'<<-\'\n    AND_GREATER = \'&>\'\n    AND_GREATER_GREATER = \'&>>\'\n    LESS_GREATER = \'<>\'\n    GREATER_BAR = \'>|\'\n    BAR_AND = \'|&\'\n    LEFT_CURLY = 47\n    RIGHT_CURLY = 48\n    EOF = \'$end\'\n    LEFT_PAREN = \'(\'\n    RIGHT_PAREN = \')\'\n    BAR = \'|\'\n    SEMICOLON = \';\'\n    DASH = \'-\'\n    NEWLINE = \'\\n\'\n    LESS = \'<\'\n    GREATER = \'>\'\n    AMPERSAND = \'&\'\n\n_reserved = set([\n    tokentype.AND_AND, tokentype.BANG, tokentype.BAR_AND, tokentype.DO,\n    tokentype.DONE, tokentype.ELIF, tokentype.ELSE, tokentype.ESAC,\n    tokentype.FI, tokentype.IF, tokentype.OR_OR, tokentype.SEMI_SEMI,\n    tokentype.SEMI_AND, tokentype.SEMI_SEMI_AND, tokentype.THEN,\n    tokentype.TIME, tokentype.TIMEOPT, tokentype.TIMEIGN, tokentype.COPROC,\n    tokentype.UNTIL, tokentype.WHILE])\n\nfor c in \'\\n;()|&{}\':\n    _reserved.add(c)\n\n# word_token_alist\nvalid_reserved_first_command = {\n    ""if"" : tokentype.IF,\n    ""then"" : tokentype.THEN,\n    ""else"" : tokentype.ELSE,\n    ""elif"" : tokentype.ELIF,\n    ""fi"" : tokentype.FI,\n    ""case"" : tokentype.CASE,\n    ""esac"" : tokentype.ESAC,\n    ""for"" : tokentype.FOR,\n    ""select"" : tokentype.SELECT,\n    ""while"" : tokentype.WHILE,\n    ""until"" : tokentype.UNTIL,\n    ""do"" : tokentype.DO,\n    ""done"" : tokentype.DONE,\n    ""in"" : tokentype.IN,\n    ""function"" : tokentype.FUNCTION,\n    ""time"" : tokentype.TIME,\n    ""{"" : tokentype.LEFT_CURLY,\n    ""}"" : tokentype.RIGHT_CURLY,\n    ""!"" : tokentype.BANG,\n    ""[["" : tokentype.COND_START,\n    ""]]"" : tokentype.COND_END,\n    ""coproc"" : tokentype.COPROC\n}\n\nclass MatchedPairError(errors.ParsingError):\n    def __init__(self, startline, message, tokenizer):\n        # TODO use startline?\n        super(MatchedPairError, self).__init__(message,\n                                               tokenizer.source,\n                                               tokenizer._shell_input_line_index - 1)\n\nwordflags = flags.word\nparserflags = flags.parser\n\nclass token(object):\n    def __init__(self, type_, value, pos=None, flags=None):\n        if type_ is not None:\n            assert isinstance(type_, tokentype)\n\n        if flags is None:\n            flags = set()\n\n        self.ttype = type_\n\n        self.value = value\n        if pos is not None:\n            self.lexpos = pos[0]\n            self.endlexpos = pos[1]\n            assert self.lexpos < self.endlexpos, (self.lexpos, self.endlexpos)\n        else:\n            self.lexpos = self.endlexpos = None\n\n        self.flags = flags\n\n    @property\n    def type(self):\n        if self.ttype:\n            # make yacc see our EOF token as its own special one $end\n            if self.ttype == tokentype.EOF:\n                return \'$end\'\n            else:\n                return self.ttype.name\n\n    def __nonzero__(self):\n        return not (self.ttype is None and self.value is None)\n\n    __bool__ = __nonzero__\n\n    def __eq__(self, other):\n        return isinstance(other, token) and (self.type == other.type and\n                                             self.value == other.value and\n                                             self.lexpos == other.lexpos and\n                                             self.endlexpos == other.endlexpos and\n                                             self.flags == other.flags)\n\n    def __repr__(self):\n        s = [\'<\', self.type]\n        if self.lexpos is not None and self.endlexpos is not None:\n            s.append(\'@%d:%d\' % (self.lexpos, self.endlexpos))\n        if self.value:\n            s.append(\' \')\n            s.append(repr(self.value))\n\n        if self.flags:\n            prettyflags = \' \'.join([e.name for e in self.flags])\n            s.append(\' (%s)\' % prettyflags)\n        s.append(\'>\')\n        return \'\'.join(s)\n\n    def nopos(self):\n        return self.__class__(self.ttype, self.value, flags=self.flags)\n\neoftoken = token(tokentype.EOF, None)\n\nclass tokenizer(object):\n    def __init__(self, s, parserstate, strictmode=True, eoftoken=None,\n                 lastreadtoken=None, tokenbeforethat=None, twotokensago=None):\n        self._shell_eof_token = eoftoken\n        self._shell_input_line = s\n        self._added_newline = False\n        if self._shell_input_line and self._shell_input_line[-1] != \'\\n\':\n            self._shell_input_line += \'\\n\' # bash/parse.y L2431\n            self._added_newline = True\n        self._shell_input_line_index = 0\n        # self._shell_input_line_terminator = None\n        self._two_tokens_ago = twotokensago or token(None, None)\n        self._token_before_that = tokenbeforethat or token(None, None)\n        self._last_read_token = lastreadtoken or token(None, None)\n        self._current_token = token(None, None)\n\n        # This implements one-character lookahead/lookbehind across physical\n        # input lines, to avoid something being lost because it\'s pushed back\n        # with shell_ungetc when we\'re at the start of a line.\n        self._eol_ungetc_lookahead = None\n\n        # token waiting to be read\n        self._token_to_read = None\n\n        self._parserstate = parserstate\n        self._line_number = 0\n        self._open_brace_count = 0\n        self._esacs_needed_count = 0\n\n        self._dstack = []\n\n        # a stack of positions to record the start and end of a token\n        self._positions = []\n\n        self._strictmode = strictmode\n\n        # hack: the tokenizer needs access to the stack of redirection\n        # nodes when it reads heredocs. this instance is shared between\n        # the tokenizer and the parser, which also needs it\n        self.redirstack = []\n\n    @property\n    def source(self):\n        if self._added_newline:\n            return self._shell_input_line[:-1]\n        return self._shell_input_line\n\n    def __iter__(self):\n        while True:\n            t = self.token()\n            # we\'re finished when we see the eoftoken OR when we added a newline\n            # to the input and we\'re there now\n            if t is eoftoken or (self._added_newline and\n                                 t.lexpos + 1 == len(self._shell_input_line)):\n                break\n            yield t\n\n    def _createtoken(self, type_, value, flags=None):\n        \'\'\'create a token with position information\'\'\'\n        pos = None\n        assert len(self._positions) >= 2, (type_, value)\n        p2 = self._positions.pop()\n        p1 = self._positions.pop()\n        pos = [p1, p2]\n        return token(type_, value, pos, flags)\n\n    def token(self):\n        self._two_tokens_ago, self._token_before_that, self._last_read_token = \\\n            self._token_before_that, self._last_read_token, self._current_token\n\n        self._current_token = self._readtoken()\n        if isinstance(self._current_token, tokentype):\n            self._recordpos()\n            self._current_token = self._createtoken(self._current_token,\n                                                    self._current_token.value)\n\n        if (self._parserstate & parserflags.EOFTOKEN and\n            self._current_token.ttype == self._shell_eof_token):\n            self._current_token = eoftoken\n            # bash/parse.y L2626\n        self._parserstate.discard(parserflags.EOFTOKEN)\n\n        return self._current_token\n\n    def _readtoken(self):\n        character = None\n        peek_char = None\n\n        if self._token_to_read is not None:\n            t = self._token_to_read\n            self._token_to_read = None\n            return t\n\n        # bashlint/parse.y L2989 COND_COMMAND\n        character = self._getc(True)\n        while character is not None and _shellblank(character):\n            character = self._getc(True)\n\n        if character is None:\n            return eoftoken\n\n        if character == \'#\':\n            self._discard_until(\'\\n\')\n            self._getc(False)\n            character = \'\\n\'\n\n        self._recordpos(1)\n\n        if character == \'\\n\':\n            # bashlint/parse.y L3034 ALIAS\n            heredoc.gatherheredocuments(self)\n\n            self._parserstate.discard(parserflags.ASSIGNOK)\n            return tokentype(character)\n\n        if self._parserstate & parserflags.REGEXP:\n            return self._readtokenword(character)\n\n        if _shellmeta(character) and not (self._parserstate & parserflags.DBLPAREN):\n            self._parserstate.discard(parserflags.ASSIGNOK)\n            peek_char = self._getc(True)\n\n            both = character\n            if peek_char:\n                both += peek_char\n            if character == peek_char:\n                if character == \'<\':\n                    peek_char = self._getc()\n                    if peek_char == \'-\':\n                        return tokentype.LESS_LESS_MINUS\n                    elif peek_char == \'<\':\n                        return tokentype.LESS_LESS_LESS\n                    else:\n                        self._ungetc(peek_char)\n                        return tokentype.LESS_LESS\n                elif character == \'>\':\n                    return tokentype.GREATER_GREATER\n                elif character == \';\':\n                    self._parserstate |= parserflags.CASEPAT\n                    # bashlint/parse.y L3085 ALIAS\n                    peek_char = self._getc()\n                    if peek_char == \'&\':\n                        return tokentype.SEMI_SEMI_AND\n                    else:\n                        self._ungetc(peek_char)\n                        return tokentype.SEMI_SEMI\n                elif character == \'&\':\n                    return tokentype.AND_AND\n                elif character == \'|\':\n                    return tokentype.OR_OR\n                # bashlint/parse.y L3105\n            elif both == \'<&\':\n                return tokentype.LESS_AND\n            elif both == \'>&\':\n                return tokentype.GREATER_AND\n            elif both == \'<>\':\n                return tokentype.LESS_GREATER\n            elif both == \'>|\':\n                return tokentype.GREATER_BAR\n            elif both == \'&>\':\n                peek_char = self._getc()\n                if peek_char == \'>\':\n                    return tokentype.AND_GREATER_GREATER\n                else:\n                    self._ungetc(peek_char)\n                    return tokentype.AND_GREATER\n            elif both == \'|&\':\n                return tokentype.BAR_AND\n            elif both == \';&\':\n                return tokentype.SEMI_AND\n\n            self._ungetc(peek_char)\n            if character == \')\' and self._last_read_token.value == \'(\' and self._token_before_that.ttype == tokentype.WORD:\n                self._parserstate.add(parserflags.ALLOWOPNBRC)\n                # bashlint/parse.y L3155\n\n            if character == \'(\' and not self._parserstate & parserflags.CASEPAT:\n                self._parserstate.add(parserflags.SUBSHELL)\n            elif self._parserstate & parserflags.CASEPAT and character == \')\':\n                self._parserstate.discard(parserflags.CASEPAT)\n            elif self._parserstate & parserflags.SUBSHELL and character == \')\':\n                self._parserstate.discard(parserflags.SUBSHELL)\n\n            if character not in \'<>\' or peek_char != \'(\':\n                return tokentype(character)\n\n        if character == \'-\' and (self._last_read_token.ttype == tokentype.LESS_AND or self._last_read_token.ttype == tokentype.GREATER_AND):\n            return tokentype(character)\n\n        return self._readtokenword(character)\n\n    def _readtokenword(self, c):\n        d = {}\n        d[\'all_digit_token\'] = c.isdigit()\n        d[\'dollar_present\'] = d[\'quoted\'] = d[\'pass_next_character\'] = d[\'compound_assignment\'] = False\n\n        tokenword = []\n\n        def handleshellquote():\n            self._push_delimiter(c)\n            try:\n                ttok = self._parse_matched_pair(c, c, c, parsingcommand=(c == \'`\'))\n            finally:\n                self._pop_delimiter()\n\n            tokenword.append(c)\n            tokenword.extend(ttok)\n            d[\'all_digit_token\'] = False\n            d[\'quoted\'] = True\n            if not d[\'dollar_present\']:\n                d[\'dollar_present\'] = c == \'""\' and \'$\' in ttok\n\n        def handleshellexp():\n            peek_char = self._getc()\n            if peek_char == \'(\' or (c == \'$\' and peek_char in \'{[\'):\n                # try:\n                if peek_char == \'{\':\n                    ttok = self._parse_matched_pair(cd, \'{\', \'}\', firstclose=True, dolbrace=True)\n                elif peek_char == \'(\':\n                    self._push_delimiter(peek_char)\n                    ttok = self._parse_comsub(cd, \'(\', \')\', parsingcommand=True)\n                    self._pop_delimiter()\n                else:\n                    ttok = self._parse_matched_pair(cd, \'[\', \']\')\n                # except MatchedPairError:\n                #   return -1\n\n                tokenword.append(c)\n                tokenword.append(peek_char)\n                tokenword.extend(ttok)\n                d[\'dollar_present\'] = True\n                d[\'all_digit_token\'] = False\n\n                # goto next_character\n            elif c == \'$\' and peek_char in \'\\\'""\':\n                self._push_delimiter(peek_char)\n                try:\n                    ttok = self._parse_matched_pair(peek_char, peek_char, peek_char,\n                                                    allowesc=(peek_char == ""\'""))\n                # except MatchedPairError:\n                #    return -1\n                finally:\n                    self._pop_delimiter()\n\n                #if peek_char == ""\'"":\n                #    # XXX ansiexpand\n                #    ttok = shutils.single_quote(ttok)\n                #else:\n                #    ttok = shutils.double_quote(ttok)\n\n                tokenword.append(c)\n                tokenword.append(peek_char)\n                tokenword.extend(ttok)\n                d[\'quoted\'] = True\n                d[\'all_digit_token\'] = False\n\n                # goto next_character\n            elif c == \'$\' and peek_char == \'$\':\n                tokenword.append(\'$\')\n                tokenword.append(\'$\')\n                d[\'dollar_present\'] = True\n                d[\'all_digit_token\'] = False\n\n                # goto next_character\n            else:\n                self._ungetc(peek_char)\n                return True\n\n            # bashlint/parse.y L4699 ARRAY_VARS\n\n        def handleescapedchar():\n            tokenword.append(c)\n            d[\'all_digit_token\'] &= c.isdigit()\n            if not d[\'dollar_present\']:\n                d[\'dollar_present\'] = c == \'$\'\n\n        while True:\n            if c is None:\n                break\n\n            if d[\'pass_next_character\']:\n                d[\'pass_next_character\'] = False\n                handleescapedchar()\n                # goto escaped_character\n            else:\n                cd = self._current_delimiter()\n                gotonext = False\n                if c == \'\\\\\':\n                    peek_char = self._getc(False)\n\n                    if peek_char == \'\\n\':\n                        c = \'\\n\'\n                        gotonext = True\n                        # goto next_character\n                    else:\n                        self._ungetc(peek_char)\n\n                        if (cd is None or cd == \'`\' or\n                            (cd == \'""\' and peek_char is not None and\n                             \'dquote\' in sh_syntaxtab[peek_char])):\n                            d[\'pass_next_character\'] = True\n                            d[\'quoted\'] = True\n\n                            handleescapedchar()\n                            gotonext = True\n                            # goto got_character\n                elif _shellquote(c):\n                    handleshellquote()\n                    gotonext = True\n                    # goto next_character\n                # bashlint/parse.y L4542\n                # bashlint/parse.y L4567\n                elif _shellexp(c):\n                    gotonext = not handleshellexp()\n                    # bashlint/parse.y L4699\n                if not gotonext:\n                    if _shellbreak(c):\n                        self._ungetc(c)\n                        break\n                    else:\n                        handleescapedchar()\n\n            # got_character\n            # got_escaped_character\n\n            # tokenword.append(c)\n            # all_digit_token &= c.isdigit()\n            # if not dollar_present:\n            #     dollar_present = c == \'$\'\n\n            # next_character\n            cd = self._current_delimiter()\n            c = self._getc(cd != ""\'"" and not d[\'pass_next_character\'])\n\n        # got_token\n        self._recordpos()\n\n        tokenword = \'\'.join(tokenword)\n\n        if d[\'all_digit_token\'] and (c in \'<>\' or self._last_read_token.ttype in (tokentype.LESS_AND, tokentype.GREATER_AND)) and shutils.legal_number(tokenword):\n            return self._createtoken(tokentype.NUMBER, int(tokenword))\n\n        # bashlint/parse.y L4811\n        specialtokentype = self._specialcasetokens(tokenword)\n        if specialtokentype:\n            return self._createtoken(specialtokentype, tokenword)\n\n        if not d[\'dollar_present\'] and not d[\'quoted\'] and self._reserved_word_acceptable(self._last_read_token):\n            if tokenword in valid_reserved_first_command:\n                ttype = valid_reserved_first_command[tokenword]\n                ps = self._parserstate\n                if ps & parserflags.CASEPAT and ttype != tokentype.ESAC:\n                    pass\n                elif ttype == tokentype.TIME and not self._time_command_acceptable():\n                    pass\n                elif ttype == tokentype.ESAC:\n                    ps.discard(parserflags.CASEPAT)\n                    ps.discard(parserflags.CASESTMT)\n                elif ttype == tokentype.CASE:\n                    ps.add(parserflags.CASESTMT)\n                elif ttype == tokentype.COND_END:\n                    ps.discard(parserflags.CONDCMD)\n                    ps.discard(parserflags.CONDEXPR)\n                elif ttype == tokentype.COND_START:\n                    ps.add(parserflags.CONDCMD)\n                elif ttype == tokentype.LEFT_CURLY:\n                    self._open_brace_count += 1\n                elif ttype == tokentype.RIGHT_CURLY and self._open_brace_count:\n                    self._open_brace_count -= 1\n                return self._createtoken(ttype, tokenword)\n\n        tokenword = self._createtoken(tokentype.WORD, tokenword, butils.typedset(wordflags))\n        if d[\'dollar_present\']:\n            tokenword.flags.add(wordflags.HASDOLLAR)\n        if d[\'quoted\']:\n            tokenword.flags.add(wordflags.QUOTED)\n        if d[\'compound_assignment\'] and tokenword[-1] == \')\':\n            tokenword.flags.add(wordflags.COMPASSIGN)\n        if self._is_assignment(tokenword.value, bool(self._parserstate & parserflags.COMPASSIGN)):\n            tokenword.flags.add(wordflags.ASSIGNMENT)\n            if self._assignment_acceptable(self._last_read_token):\n                tokenword.flags.add(wordflags.NOSPLIT)\n                if self._parserstate & parserflags.COMPASSIGN:\n                    tokenword.flags.add(wordflags.NOGLOB)\n\n        # bashlint/parse.y L4865\n        if self._command_token_position(self._last_read_token):\n            pass\n\n        if tokenword.value[0] == \'{\' and tokenword.value[-1] == \'}\' and c in \'<>\':\n            if shutils.legal_identifier(tokenword.value[1:]):\n                # XXX is this needed?\n                tokenword.value = tokenword.value[1:]\n                tokenword.ttype = tokentype.REDIR_WORD\n\n            return tokenword\n\n        if len(tokenword.flags & set([wordflags.ASSIGNMENT, wordflags.NOSPLIT])) == 2:\n            tokenword.ttype = tokentype.ASSIGNMENT_WORD\n\n        if self._last_read_token.ttype == tokentype.FUNCTION:\n            self._parserstate.add(parserflags.ALLOWOPNBRC)\n            self._function_dstart = self._line_number\n        elif self._last_read_token.ttype in (tokentype.CASE, tokentype.SELECT, tokentype.FOR):\n            pass # bashlint/parse.y L4907\n\n        return tokenword\n\n    def _parse_comsub(self, doublequotes, open, close, parsingcommand=False,\n                      dquote=False, firstclose=False):\n        peekc = self._getc(False)\n        self._ungetc(peekc)\n\n        if peekc == \'(\':\n            return self._parse_matched_pair(doublequotes, open, close)\n\n        count = 1\n        dollarok = True\n\n        checkcase = bool(parsingcommand and (doublequotes is None or doublequotes not in ""\'\\"""") and not dquote)\n        checkcomment = checkcase\n\n        startlineno = self._line_number\n        heredelim = \'\'\n        stripdoc = insideheredoc = insidecomment = insideword = insidecase = False\n        readingheredocdelim = False\n        wasdollar = passnextchar = False\n        reservedwordok = True\n        lexfirstind = -1\n        lexrwlen = 0\n\n        ret = \'\'\n\n        while count:\n            c = self._getc(doublequotes != ""\'"" and not insidecomment and not passnextchar)\n\n            if c is None:\n                raise MatchedPairError(startlineno, \'unexpected EOF while looking for matching %r\' % close, self)\n\n            # bashlint/parse.y L3571\n            if c == \'\\n\':\n                if readingheredocdelim and heredelim:\n                    readingheredocdelim = False\n                    insideheredoc = True\n                    lexfirstind = len(ret) + 1\n                elif insideheredoc:\n                    tind = lexfirstind\n                    while stripdoc and ret[tind] == \'\\t\':\n                        tind += 1\n                    if ret[tind:] == heredelim:\n                        stripdoc = insideheredoc = False\n                        heredelim = \'\'\n                        lexfirstind = -1\n                    else:\n                        lexfirstind = len(ret) + 1\n            # bashlint/parse.y L3599\n            if insideheredoc and c == close and count == 1:\n                tind = lexfirstind\n                while stripdoc and ret[tind] == \'\\t\':\n                    tind += 1\n                if ret[tind:] == heredelim:\n                    stripdoc = insideheredoc = False\n                    heredelim = \'\'\n                    lexfirstind = -1\n\n            if insidecomment or insideheredoc:\n                ret += c\n\n                if insidecomment and c == \'\\n\':\n                    insidecomment = False\n\n                continue\n\n            if passnextchar:\n                passnextchar = False\n                # XXX is this needed?\n                # if doublequotes != ""\'"" and c == \'\\n\':\n                #     if ret:\n                #         ret = ret[:-1]\n                # else:\n                #     ret += c\n                ret += c\n                continue\n\n            if _shellbreak(c):\n                insideword = False\n            else:\n                if insideword:\n                    lexwlen += 1\n                else:\n                    insideword = True\n                    lexwlen = 0\n\n            if _shellblank(c) and not readingheredocdelim and not lexrwlen:\n                ret += c\n                continue\n\n            # bashlint/parse.y L3686\n            if readingheredocdelim:\n                if lexfirstind == -1 and not _shellbreak(c):\n                    lexfirstind = len(ret)\n                elif lexfirstind >= 0 and not passnextchar and _shellbreak(c):\n                    if not heredelim:\n                        nestret = ret[lexfirstind:]\n                        heredelim = shutils.removequotes(nestret)\n                    if c == \'\\n\':\n                        insideheredoc = True\n                        readingheredocdelim = False\n                        lexfirstind = len(ret) + 1\n                    else:\n                        lexfirstind = -1\n\n            if not reservedwordok and checkcase and not insidecomment and (_shellmeta(c) or c == \'\\n\'):\n                ret += c\n                peekc = self._getc(True)\n                if c == peekc and c in \'&|;\':\n                    ret += peekc\n                    reservedwordok = True\n                    lexrwlen = 0\n                    continue\n                elif c == \'\\n\' or c in \'&|;\':\n                    self._ungetc(peekc)\n                    reservedwordok = True\n                    lexrwlen = 0\n                    continue\n                elif c is None:\n                    raise MatchedPairError(startlineno, \'unexpected EOF while looking for matching %r\' % close, self) # pragma: no coverage\n                else:\n                    ret = ret[:-1]\n                    self._ungetc(peekc)\n\n            # bashlint/parse.y L3761\n            if reservedwordok:\n                if c.islower():\n                    ret += c\n                    lexrwlen += 1\n                    continue\n                elif lexrwlen == 4 and _shellbreak(c):\n                    if ret[-4:] == \'case\':\n                        insidecase = True\n                    elif ret[-4:] == \'esac\':\n                        insidecase = False\n                    reservedwordok = False\n                elif (checkcomment and c == \'#\' and (lexrwlen == 0 or\n                        (insideword and lexwlen == 0))):\n                    pass\n                elif (not insidecase and (_shellblank(c) or c == \'\\n\') and\n                    lexrwlen == 2 and ret[-2:] == \'do\'):\n                    lexrwlen = 0\n                elif insidecase and c != \'\\n\':\n                    reservedwordok = False\n                elif not _shellbreak(c):\n                    reservedwordok = False\n\n            if not insidecomment and checkcase and c == \'<\':\n                ret += c\n                peekc = self._getc(True)\n                if peekc is None:\n                    raise MatchedPairError(startlineno, \'unexpected EOF while looking for matching %r\' % close, self)\n                if peekc == c:\n                    ret += peekc\n                    peekc = self._getc(True)\n                    if peekc is None:\n                        raise MatchedPairError(startlineno, \'unexpected EOF while looking for matching %r\' % close, self)\n                    elif peekc == \'-\':\n                        ret += peekc\n                        stripdoc = True\n                    else:\n                        self._ungetc(peekc)\n\n                    if peekc != \'<\':\n                        readingheredocdelim = True\n                        lexfirstind = -1\n\n                    continue\n                else:\n                    c = peekc\n            elif checkcomment and not insidecomment and c == \'#\' and ((reservedwordok\n                    and lexrwlen == 0) or insideword or lexwlen == 0):\n                insidecomment = True\n\n            if c == close and not insidecase:\n                count -= 1\n            elif not firstclose and not insidecase and c == open:\n                count += 1\n\n            ret += c\n\n            if count == 0:\n                break\n\n            if c == \'\\\\\':\n                passnextchar = True\n\n            # bashlint/parse.y L3897\n            if _shellquote(c):\n                self._push_delimiter(c)\n                try:\n                    if wasdollar and c == ""\'"":\n                        nestret = self._parse_matched_pair(c, c, c,\n                                                           allowesc=True,\n                                                           dquote=True)\n                    else:\n                        nestret = self._parse_matched_pair(c, c, c,\n                                                           dquote=True)\n                finally:\n                    self._pop_delimiter()\n\n                # XXX is this necessary?\n                # if wasdollar and c == ""\'"" and not rdquote:\n                #     if not rdquote:\n                #         nestret = shutils.single_quote(nestret)\n                #     ret = ret[:-2]\n                # elif wasdollar and c == \'""\' and not rdquote:\n                #     nestret = shutils.double_quote(nestret)\n                #     ret = ret[:-2]\n\n                ret += nestret\n            # check for $(), $[], or ${} inside command substitution\n            elif wasdollar and c in \'({[\':\n                if not insidecase and open == c:\n                    count -= 1\n                if c == \'(\':\n                    nestret = self._parse_comsub(None, \'(\', \')\',\n                                                 parsingcommand=True,\n                                                 dquote=False)\n                elif c == \'{\':\n                    nestret = self._parse_matched_pair(None, \'{\', \'}\',\n                                                       firstclose=True,\n                                                       dolbrace=True,\n                                                       dquote=True)\n                elif c == \'[\':\n                    nestret = self._parse_matched_pair(None, \'[\', \']\',\n                                                       dquote=True)\n\n                ret += nestret\n\n            wasdollar = c == \'$\'\n\n        return ret\n\n    def _parse_matched_pair(self, doublequotes, open, close, parsingcommand=False, allowesc=False, dquote=False, firstclose=False, dolbrace=False, arraysub=False):\n        count = 1\n        dolbracestate = \'\'\n        if dolbrace:\n            dolbracestate = \'param\'\n\n        insidecomment = False\n        lookforcomments = False\n        sawdollar = False\n\n        if parsingcommand and doublequotes not in ""`\'\\"""" and dquote:\n            lookforcomments = True\n\n        rdquote = True if doublequotes == \'""\' else dquote\n        passnextchar = False\n        startlineno = self._line_number\n\n        ret = \'\'\n\n        def handledollarword():\n            if open == c:\n                count -= 1\n\n            # bashlint/parse.y L3486\n            if c == \'(\':\n                return self._parse_comsub(None, \'(\', \')\',\n                                          parsingcommand=True,\n                                          dquote=False)\n            elif c == \'{\':\n                return self._parse_matched_pair(None, \'{\', \'}\',\n                                                firstclose=True,\n                                                dquote=rdquote,\n                                                dolbrace=True)\n            elif c == \'[\':\n                return self._parse_matched_pair(None, \'[\', \']\', dquote=rdquote)\n            else:\n                assert False # pragma: no cover\n\n        while count:\n            c = self._getc(doublequotes != ""\'"" and not passnextchar)\n            if c is None:\n                raise MatchedPairError(startlineno, \'unexpected EOF while looking for matching %r\' % close, self)\n\n            # bashlint/parse.y L3285\n            # if c == \'\\n\':\n            #    continue\n\n            if insidecomment:\n                ret += c\n                if c == \'\\n\':\n                    insidecomment = False\n                continue\n            elif lookforcomments and not insidecomment and c == \'#\' and (not ret\n                    or ret[-1] == \'\\n\' or _shellblank(ret[-1])):\n                insidecomment = True\n\n            # last char was backslash\n            if passnextchar:\n                passnextchar = False\n                #if doublequotes != ""\'"" and c == \'\\n\':\n                #    if ret:\n                #        ret = ret[:-1]\n                #    continue\n                ret += c\n                continue\n            elif c == close:\n                count -= 1\n            elif open != close and sawdollar and open == \'{\' and c == open:\n                count += 1\n            elif not firstclose and c == open:\n                count += 1\n\n            ret += c\n            if count == 0:\n                break\n\n            if open == ""\'"":\n                if allowesc and c == ""\\\\"":\n                    passnextchar = True\n                continue\n            if c == ""\\\\"":\n                passnextchar = True\n            if dolbrace:\n                if dolbracestate == \'param\':\n                    if len(ret) > 1:\n                        dd = {\'%\' : \'quote\', \'#\' : \'quote\', \'/\' : \'quote2\', \'^\' : \'quote\',\n                                \',\' : \'quote\'}\n                        if c in dd:\n                            dolbracestate = dd[c]\n                    elif c in \'#%^,~:-=?+/\':\n                        dolbracestate = \'op\'\n                if dolbracestate == \'op\' and c in \'#%^,~:-=?+/\':\n                    dolbracestate = \'word\'\n\n            if dolbracestate not in \'quote2\' and dquote and dolbrace and c == ""\'"":\n                continue\n\n            if open != close:\n                if _shellquote(c):\n                    self._push_delimiter(c)\n                    try:\n                        if sawdollar and ""\'"":\n                            nestret = self._parse_matched_pair(c, c, c, parsingcommand=parsingcommand, allowesc=True, dquote=dquote, firstclose=firstclose, dolbrace=dolbrace)\n                        else:\n                            nestret = self._parse_matched_pair(c, c, c, parsingcommand=parsingcommand, allowesc=allowesc, dquote=dquote, firstclose=firstclose, dolbrace=dolbrace)\n                    finally:\n                        self._pop_delimiter()\n\n                    # bashlint/parse.y L3419\n                    if sawdollar and c == ""\'"":\n                        pass\n                    elif sawdollar and c == \'""\':\n                        ret = ret[:-2] # back up before the $""\n\n                    ret += nestret\n                elif arraysub and sawdollar and c in \'({[\':\n                    # goto parse_dollar_word\n                    ret += handledollarword()\n            elif open == \'""\' and c == \'`\':\n                ret += self._parse_matched_pair(None, \'`\', \'`\', parsingcommand=parsingcommand, allowesc=allowesc, dquote=dquote, firstclose=firstclose, dolbrace=dolbrace)\n            elif open != \'`\' and sawdollar and c in \'({[\':\n                ret += handledollarword()\n\n            sawdollar = c == \'$\'\n\n        return ret\n\n\n    def _is_assignment(self, value, iscompassign):\n        c = value[0]\n\n        def legalvariablechar(x):\n            return x.isalpha() or x == \'_\'\n\n        if not legalvariablechar(c):\n            return\n\n        for i, c in enumerate(value):\n            if c == \'=\':\n                return i\n\n            # bash/general.c L289\n            if c == \'+\' and i + 1 < len(value) and value[i+1] == \'=\':\n                return i+1\n\n            if not legalvariablechar(c):\n                return False\n\n    def _command_token_position(self, token):\n        return (token.ttype == tokentype.ASSIGNMENT_WORD or\n                self._parserstate & parserflags.REDIRLIST or\n                (token.ttype not in (tokentype.SEMI_SEMI, tokentype.SEMI_AND, tokentype.SEMI_SEMI_AND) and self._reserved_word_acceptable(token)))\n\n    def _assignment_acceptable(self, token):\n        return self._command_token_position(token) and not self._parserstate & parserflags.CASEPAT\n\n    def _time_command_acceptable(self):\n        pass\n\n    def _reserved_word_acceptable(self, tok):\n        if not tok or (tok.ttype in _reserved or tok.value in _reserved):\n            return True\n        # bash/parse.y L4955 cOPROCESS_SUPPORT\n\n        if (self._last_read_token.ttype == tokentype.WORD and\n            self._token_before_that.ttype == tokentype.FUNCTION):\n            return True\n\n        return False\n\n    def _pop_delimiter(self):\n        self._dstack.pop()\n\n    def _push_delimiter(self, c):\n        self._dstack.append(c)\n\n    def _current_delimiter(self):\n        if self._dstack:\n            return self._dstack[-1]\n\n    def _ungetc(self, c):\n        if (self._shell_input_line and self._shell_input_line_index\n            and self._shell_input_line_index <= len(self._shell_input_line)):\n            self._shell_input_line_index -= 1\n        else:\n            self._eol_ungetc_lookahead = c\n\n    def _getc(self, remove_quoted_newline=True):\n        if self._eol_ungetc_lookahead is not None:\n            c = self._eol_ungetc_lookahead\n            self._eol_ungetc_lookahead = None\n            return c\n\n        # bash/parse.y L2220\n\n        while True:\n            if self._shell_input_line_index < len(self._shell_input_line):\n                c = self._shell_input_line[self._shell_input_line_index]\n                self._shell_input_line_index += 1\n            else:\n                c = None\n\n            if c == \'\\\\\' and remove_quoted_newline and self._shell_input_line[self._shell_input_line_index] == \'\\n\':\n                self._line_number += 1\n                continue\n            else:\n                return c\n\n            #if c is None and self._shell_input_line_terminator is None:\n            #    if self._shell_input_line_index != 0:\n            #        return \'\\n\'\n            #    else:\n            #        return None\n\n            #return c\n\n    def _discard_until(self, character):\n        c = self._getc(False)\n        while c is not None and c != character:\n            c = self._getc(False)\n        if c is not None:\n            self._ungetc(c)\n\n    def _recordpos(self, relativeoffset=0):\n        \'\'\'record the current index of the tokenizer into the positions stack\n        while adding relativeoffset from it\'\'\'\n        self._positions.append(self._shell_input_line_index - relativeoffset)\n\n    def readline(self, removequotenewline):\n        linebuffer = []\n        passnext = indx = 0\n        while True:\n            c = self._getc()\n            if c is None:\n                if indx == 0:\n                    return None\n                c = \'\\n\'\n\n            if passnext:\n                linebuffer.append(c)\n                indx += 1\n                passnext = False\n            elif c == \'\\\\\' and removequotenewline:\n                peekc = self._getc()\n                if peekc == \'\\n\':\n                    self._line_number += 1\n                    continue\n                else:\n                    self._ungetc(peekc)\n                    passnext = True\n                    linebuffer.append(c)\n                    indx += 1\n            else:\n                linebuffer.append(c)\n                indx += 1\n\n            if c == \'\\n\':\n                return \'\'.join(linebuffer)\n\n    def _peekc(self, *args):\n        peek_char = self._getc(*args)\n        # only unget if we actually read something\n        if peek_char is not None:\n            self._ungetc(peek_char)\n        return peek_char\n\n    def _specialcasetokens(self, tokstr):\n        if (self._last_read_token.ttype == tokentype.WORD and\n            self._token_before_that.ttype in (tokentype.FOR,\n                                              tokentype.CASE,\n                                              tokentype.SELECT) and\n            tokstr == \'in\'):\n                if self._token_before_that.ttype == tokentype.CASE:\n                    self._parserstate.add(parserflags.CASEPAT)\n                    self._esacs_needed_count += 1\n                return tokentype.IN\n\n        if (self._last_read_token.ttype == tokentype.WORD and\n            self._token_before_that.ttype in (tokentype.FOR, tokentype.SELECT) and\n            tokstr == \'do\'):\n            return tokentype.DO\n\n        if self._esacs_needed_count:\n            self._esacs_needed_count -= 1\n            if tokstr == \'esac\':\n                self._parserstate.discard(parserflags.CASEPAT)\n                return tokentype.ESAC\n\n        if self._parserstate & parserflags.ALLOWOPNBRC:\n            self._parserstate.discard(parserflags.ALLOWOPNBRC)\n            if tokstr == \'{\':\n                self._open_brace_count += 1\n                # bash/parse.y L2887\n                return tokentype.LEFT_CURLY\n\n        if (self._last_read_token.ttype == tokentype.ARITH_FOR_EXPRS and\n            tokstr == \'do\'):\n            return tokentype.DO\n\n        if (self._last_read_token.ttype == tokentype.ARITH_FOR_EXPRS and\n            tokstr == \'{\'):\n            self._open_brace_count += 1\n            return tokentype.LEFT_CURLY\n\n        if (self._open_brace_count and\n            self._reserved_word_acceptable(self._last_read_token) and\n            tokstr == \'}\'):\n            self._open_brace_count -= 1\n            return tokentype.RIGHT_CURLY\n\n        if self._last_read_token.ttype == tokentype.TIME and tokstr == \'-p\':\n            return tokentype.TIMEOPT\n\n        if self._last_read_token.ttype == tokentype.TIMEOPT and tokstr == \'--\':\n            return tokentype.TIMEIGN\n\n        if self._parserstate & parserflags.CONDEXPR and tokstr == \']]\':\n            return tokentype.COND_END\n'"
bashlint/yacc.py,1,"b'# -----------------------------------------------------------------------------\n# ply: yacc.py\n#\n# Copyright (C) 2001-2011,\n# David M. Beazley (Dabeaz LLC)\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n# \n# * Redistributions of source code must retain the above copyright notice,\n#   this list of conditions and the following disclaimer.  \n# * Redistributions in binary form must reproduce the above copyright notice, \n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.  \n# * Neither the name of the David Beazley or Dabeaz LLC may be used to\n#   endorse or promote products derived from this software without\n#  specific prior written permission. \n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# -----------------------------------------------------------------------------\n#\n# This implements an LR parser that is constructed from grammar rules defined\n# as Python functions. The grammer is specified by supplying the BNF inside\n# Python documentation strings.  The inspiration for this technique was borrowed\n# from John Aycock\'s Spark parsing system.  PLY might be viewed as cross between\n# Spark and the GNU bison utility.\n#\n# The current implementation is only somewhat object-oriented. The\n# LR parser itself is defined in terms of an object (which allows multiple\n# parsers to co-exist).  However, most of the variables used during table\n# construction are defined in terms of global variables.  Users shouldn\'t\n# notice unless they are trying to define multiple parsers at the same\n# time using threads (in which case they should have their head examined).\n#\n# This implementation supports both SLR and LALR(1) parsing.  LALR(1)\n# support was originally implemented by Elias Ioup (ezioup@alumni.uchicago.edu),\n# using the algorithm found in Aho, Sethi, and Ullman ""Compilers: Principles,\n# Techniques, and Tools"" (The Dragon Book).  LALR(1) has since been replaced\n# by the more efficient DeRemer and Pennello algorithm.\n#\n# :::::::: WARNING :::::::\n#\n# Construction of LR parsing tables is fairly complicated and expensive.\n# To make this module run fast, a *LOT* of work has been put into\n# optimization---often at the expensive of readability and what might\n# consider to be good Python ""coding style.""   Modify the code at your\n# own risk!\n# ----------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__version__    = ""3.4""\n__tabversion__ = ""3.2""       # Table version\n\n#-----------------------------------------------------------------------------\n#                     === User configurable parameters ===\n#\n# Change these to modify the default behavior of yacc (if you wish)\n#-----------------------------------------------------------------------------\n\nyaccdebug   = 0                # Debugging mode.  If set, yacc generates a\n                               # a \'parser.out\' file in the current directory\n\ndebug_file  = \'parser.out\'     # Default name of the debugging file\ntab_module  = \'parsetab\'       # Default name of the table module\ndefault_lr  = \'LALR\'           # Default LR table generation method\n\nerror_count = 3                # Number of symbols that must be shifted to leave recovery mode\n\nyaccdevel   = 0                # Set to True if developing yacc.  This turns off optimized\n                               # implementations of certain functions.\n\nresultlimit = 40               # Size limit of results when running in debug mode.\n\npickle_protocol = 0            # Protocol to use when writing pickle files\n\nimport re, types, sys, os.path\n\nfrom bashlint import butils\n\n# Compatibility function for python 2.6/3.0\nif sys.version_info[0] < 3:\n    def func_code(f):\n        return f.func_code\nelse:\n    def func_code(f):\n        return f.__code__\n\n# Compatibility\ntry:\n    MAXINT = sys.maxint\nexcept AttributeError:\n    MAXINT = sys.maxsize\n\n# Python 2.x/3.0 compatibility.\ndef load_ply_lex():\n    if sys.version_info[0] < 3:\n        import lex\n    else:\n        import ply.lex as lex\n    return lex\n\n# This object is a stand-in for a logging object created by the \n# logging module.   PLY will use this by default to create things\n# such as the parser.out file.  If a user wants more detailed\n# information, they can create their own logging object and pass\n# it into PLY.\n\nclass PlyLogger(object):\n    def __init__(self,f):\n        self.f = f\n    def debug(self,msg,*args,**kwargs):\n        self.f.write((msg % args) + ""\\n"")\n    info     = debug\n\n    def warning(self,msg,*args,**kwargs):\n        self.f.write(""WARNING: ""+ (msg % args) + ""\\n"")\n\n    def error(self,msg,*args,**kwargs):\n        self.f.write(""ERROR: "" + (msg % args) + ""\\n"")\n\n    critical = debug\n\n# Null logger is used when no output is generated. Does nothing.\nclass NullLogger(object):\n    def __getattribute__(self,name):\n        return self\n    def __call__(self,*args,**kwargs):\n        return self\n        \n# Exception raised for yacc-related errors\nclass YaccError(Exception):   pass\n\nclass YaccAccept(Exception):   pass\n\n# Format the result message that the parser produces when running in debug mode.\ndef format_result(r):\n    repr_str = repr(r)\n    if \'\\n\' in repr_str: repr_str = repr(repr_str)\n    if len(repr_str) > resultlimit:\n        repr_str = repr_str[:resultlimit]+"" ...""\n    result = ""<%s @ 0x%x> (%s)"" % (type(r).__name__,id(r),repr_str)\n    return result\n\n\n# Format stack entries when the parser is running in debug mode\ndef format_stack_entry(r):\n    repr_str = repr(r)\n    if \'\\n\' in repr_str: repr_str = repr(repr_str)\n    if len(repr_str) < 16:\n        return repr_str\n    else:\n        return ""<%s @ 0x%x>"" % (type(r).__name__,id(r))\n\n#-----------------------------------------------------------------------------\n#                        ===  LR Parsing Engine ===\n#\n# The following classes are used for the LR parser itself.  These are not\n# used during table construction and are independent of the actual LR\n# table generation algorithm\n#-----------------------------------------------------------------------------\n\n# This class is used to hold non-terminal grammar symbols during parsing.\n# It normally has the following attributes set:\n#        .type       = Grammar symbol type\n#        .value      = Symbol value\n#        .lineno     = Starting line number\n#        .endlineno  = Ending line number (optional, set automatically)\n#        .lexpos     = Starting lex position\n#        .endlexpos  = Ending lex position (optional, set automatically)\n\nclass YaccSymbol:\n    def __str__(self):    return self.type\n    def __repr__(self):   return str(self)\n\n# This class is a wrapper around the objects actually passed to each\n# grammar rule.   Index lookup and assignment actually assign the\n# .value attribute of the underlying YaccSymbol object.\n# The lineno() method returns the line number of a given\n# item (or 0 if not defined).   The linespan() method returns\n# a tuple of (startline,endline) representing the range of lines\n# for a symbol.  The lexspan() method returns a tuple (lexpos,endlexpos)\n# representing the range of positional information for a symbol.\n\nclass YaccProduction:\n    def __init__(self,s,stack=None):\n        self.slice = s\n        self.stack = stack\n        self.lexer = None\n        self.parser= None\n        self.context= None\n    def __getitem__(self,n):\n        if n >= 0: return self.slice[n].value\n        else: return self.stack[n].value\n\n    def __setitem__(self,n,v):\n        self.slice[n].value = v\n\n    def __getslice__(self,i,j):\n        return [s.value for s in self.slice[i:j]]\n\n    def __len__(self):\n        return len(self.slice)\n\n    def lineno(self,n):\n        return getattr(self.slice[n],""lineno"",0)\n\n    def set_lineno(self,n,lineno):\n        self.slice[n].lineno = lineno\n\n    def linespan(self,n):\n        startline = getattr(self.slice[n],""lineno"",0)\n        endline = getattr(self.slice[n],""endlineno"",startline)\n        return startline,endline\n\n    def lexpos(self,n):\n        return getattr(self.slice[n],""lexpos"",0)\n\n    def endlexpos(self,n):\n        return getattr(self.slice[n],""endlexpos"",0)\n\n    def lexspan(self,n):\n        startpos = getattr(self.slice[n],""lexpos"",0)\n        endpos = getattr(self.slice[n],""endlexpos"",startpos)\n        return startpos,endpos\n\n    def error(self):\n       raise SyntaxError\n\n    def accept(self):\n       raise YaccAccept\n\n\n# -----------------------------------------------------------------------------\n#                               == LRParser ==\n#\n# The LR Parsing engine.\n# -----------------------------------------------------------------------------\n\nclass LRParser:\n    def __init__(self,lrtab,errorf):\n        # make sure these are immutable\n        self.productions = tuple(lrtab.lr_productions)\n        self.action      = butils.frozendict(lrtab.lr_action)\n        self.goto        = butils.frozendict(lrtab.lr_goto)\n        self.errorfunc   = errorf\n\n    def errok(self):\n        self.errorok     = 1\n\n    def restart(self):\n        del self.statestack[:]\n        del self.symstack[:]\n        sym = YaccSymbol()\n        sym.type = \'$end\'\n        self.symstack.append(sym)\n        self.statestack.append(0)\n\n    def parse(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None,context=None):\n        if debug or yaccdevel:\n            if isinstance(debug,int):\n                debug = PlyLogger(sys.stderr)\n            return self.parsedebug(input,lexer,debug,tracking,tokenfunc,context)\n        elif tracking:\n            return self.parseopt(input,lexer,debug,tracking,tokenfunc,context)\n        else:\n            return self.parseopt_notrack(input,lexer,debug,tracking,tokenfunc,context)\n        \n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parsedebug().\n    #\n    # This is the debugging enabled version of parse().  All changes made to the\n    # parsing engine should be made here.   For the non-debugging version,\n    # copy this code to a method parseopt() and delete all of the sections\n    # enclosed in:\n    #\n    #      #--! DEBUG\n    #      statements\n    #      #--! DEBUG\n    #\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    def parsedebug(self,input=None,lexer=None,debug=None,tracking=0,tokenfunc=None,context=None):\n        lookahead = None                 # Current lookahead symbol\n        lookaheadstack = [ ]             # Stack of lookahead symbols\n        actions = self.action            # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)\n        pslice  = YaccProduction(None)   # Production object passed to grammar rules\n        errorcount = 0                   # Used during error recovery \n\n        # --! DEBUG\n        debug.info(""PLY: PARSE DEBUG START"")\n        # --! DEBUG\n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            lex = load_ply_lex()\n            lexer = lex.lexer\n\n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        # pslice.parser = self._parser\n        pslice.context = context\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n           # Tokenize function\n           get_token = lexer.token\n        else:\n           get_token = tokenfunc\n\n        # Set up the state and symbol stacks\n\n        statestack = [ ]                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = [ ]                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = ""$end""\n        symstack.append(sym)\n        state = 0\n        while 1:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we\'ll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n            # --! DEBUG\n            debug.debug(\'\')\n            debug.debug(\'State  : %s\', state)\n            # --! DEBUG\n\n            if not lookahead:\n                if not lookaheadstack:\n                    lookahead = get_token()     # Get the next token\n                else:\n                    lookahead = lookaheadstack.pop()\n                if not lookahead:\n                    lookahead = YaccSymbol()\n                    lookahead.type = ""$end""\n\n            # --! DEBUG\n            debug.debug(\'Stack  : %s\',\n                        (""%s . %s"" % ("" "".join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())\n            # --! DEBUG\n\n            # Check the action table\n            ltype = lookahead.type\n            t = actions[state].get(ltype)\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n                    \n                    # --! DEBUG\n                    debug.debug(""Action : Shift and goto state %s"", t)\n                    # --! DEBUG\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount: errorcount -=1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n                    accept = False\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n                    # --! DEBUG\n                    if plen:\n                        debug.info(""Action : Reduce rule [%s] with %s and goto state %d"", p.str, ""[""+"","".join([format_stack_entry(_v.value) for _v in symstack[-plen:]])+""]"",-t)\n                    else:\n                        debug.info(""Action : Reduce rule [%s] with %s and goto state %d"", p.str, [],-t)\n                        \n                    # --! DEBUG\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n                        # --! TRACKING\n                        if tracking:\n                           t1 = targ[1]\n                           sym.lineno = t1.lineno\n                           sym.lexpos = t1.lexpos\n                           t1 = targ[-1]\n                           sym.endlineno = getattr(t1,""endlineno"",t1.lineno)\n                           sym.endlexpos = getattr(t1,""endlexpos"",t1.lexpos)\n\n                        # --! TRACKING\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n                        \n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            del statestack[-plen:]\n                            try:\n                                p.callable(pslice)\n                            except YaccAccept:\n                                accept = True\n                            # --! DEBUG\n                            debug.info(""Result : %s"", format_result(pslice[0]))\n                            # --! DEBUG\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n\n                        if not accept:\n                            continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    \n                    else:\n\n                        # --! TRACKING\n                        if tracking:\n                           sym.lineno = lexer.lineno\n                           sym.lexpos = lexer.lexpos\n                        # --! TRACKING\n\n                        targ = [ sym ]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            p.callable(pslice)\n                            # --! DEBUG\n                            debug.info(""Result : %s"", format_result(pslice[0]))\n                            # --! DEBUG\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0 or accept:\n                    n = symstack[-1]\n                    result = getattr(n,""value"",None)\n                    # --! DEBUG\n                    debug.info(""Done   : Returning %s"", format_result(result))\n                    debug.info(""PLY: PARSE DEBUG END"")\n                    # --! DEBUG\n                    return result\n\n            if t == None:\n\n                # --! DEBUG\n                debug.error(\'Error  : %s\',\n                            (""%s . %s"" % ("" "".join([xx.type for xx in symstack][1:]), str(lookahead))).lstrip())\n                # --! DEBUG\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an \'error\' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = 0\n                    errtoken = lookahead\n                    if errtoken.type == ""$end"":\n                        pass\n                        #errtoken = None               # End of file!\n                    if self.errorfunc:\n                        global errok,token,restart\n                        errok = self.errok        # Set some special functions available in error recovery\n                        token = get_token\n                        restart = self.restart\n                        if errtoken and not hasattr(errtoken,\'lexer\'):\n                            errtoken.lexer = lexer\n                        tok = self.errorfunc(errtoken)\n                        del errok, token, restart   # Delete special functions\n\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken,""lineno""): lineno = lookahead.lineno\n                            else: lineno = 0\n                            if lineno:\n                                sys.stderr.write(""yacc: Syntax error at line %d, token=%s\\n"" % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write(""yacc: Syntax error, token=%s"" % errtoken.type)\n                        else:\n                            sys.stderr.write(""yacc: Parse error in input. EOF\\n"")\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we\'re in this state, the\n                # entire parse has been rolled back and we\'re completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != ""$end"":\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we\'re\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == ""$end"":\n                    # Whoa. We\'re really hosed here. Bail out\n                    return\n\n                if lookahead.type != \'error\':\n                    sym = symstack[-1]\n                    if sym.type == \'error\':\n                        # Hmmm. Error is on top of stack, we\'ll just nuke input\n                        # symbol and continue\n                        lookahead = None\n                        continue\n                    t = YaccSymbol()\n                    t.type = \'error\'\n                    if hasattr(lookahead,""lineno""):\n                        t.lineno = lookahead.lineno\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    symstack.pop()\n                    statestack.pop()\n                    state = statestack[-1]       # Potential bug fix\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError(""yacc: internal parser error!!!\\n"")\n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parseopt().\n    #\n    # Optimized version of parse() method.  DO NOT EDIT THIS CODE DIRECTLY.\n    # Edit the debug version above, then copy any modifications to the method\n    # below while removing #--! DEBUG sections.\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n    def parseopt(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None,context=None):\n        lookahead = None                 # Current lookahead symbol\n        lookaheadstack = [ ]             # Stack of lookahead symbols\n        actions = self.action            # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)\n        pslice  = YaccProduction(None)   # Production object passed to grammar rules\n        errorcount = 0                   # Used during error recovery \n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            lex = load_ply_lex()\n            lexer = lex.lexer\n        \n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        #pslice.parser = self\n        pslice.context = context\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n           # Tokenize function\n           get_token = lexer.token\n        else:\n           get_token = tokenfunc\n\n        # Set up the state and symbol stacks\n\n        statestack = [ ]                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = [ ]                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = \'$end\'\n        symstack.append(sym)\n        state = 0\n        while 1:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we\'ll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n            if not lookahead:\n                if not lookaheadstack:\n                    lookahead = get_token()     # Get the next token\n                else:\n                    lookahead = lookaheadstack.pop()\n                if not lookahead:\n                    lookahead = YaccSymbol()\n                    lookahead.type = \'$end\'\n\n            # Check the action table\n            ltype = lookahead.type\n            t = actions[state].get(ltype)\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount: errorcount -=1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n                        # --! TRACKING\n                        if tracking:\n                           t1 = targ[1]\n                           if not hasattr(t1, \'lineno\') or not hasattr(t1, \'lexpos\'):\n                               t1.lineno = t1.lexpos = 0\n                           sym.lineno = t1.lineno\n                           sym.lexpos = t1.lexpos\n                           t1 = targ[-1]\n                           sym.endlineno = getattr(t1,""endlineno"",t1.lineno)\n                           sym.endlexpos = getattr(t1,""endlexpos"",t1.lexpos)\n\n                        # --! TRACKING\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n                        \n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            del statestack[-plen:]\n                            p.callable(pslice)\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    \n                    else:\n\n                        # --! TRACKING\n                        if tracking:\n                           sym.lineno = lexer.lineno\n                           sym.lexpos = lexer.lexpos\n                        # --! TRACKING\n\n                        targ = [ sym ]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            p.callable(pslice)\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0:\n                    n = symstack[-1]\n                    return getattr(n,""value"",None)\n\n            if t == None:\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an \'error\' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = 0\n                    errtoken = lookahead\n                    if errtoken.type == \'$end\':\n                        pass\n                        #errtoken = None               # End of file!\n                    if self.errorfunc:\n                        global errok,token,restart\n                        errok = self.errok        # Set some special functions available in error recovery\n                        token = get_token\n                        restart = self.restart\n                        if errtoken and not hasattr(errtoken,\'lexer\'):\n                            errtoken.lexer = lexer\n                        tok = self.errorfunc(errtoken)\n                        del errok, token, restart   # Delete special functions\n\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken,""lineno""): lineno = lookahead.lineno\n                            else: lineno = 0\n                            if lineno:\n                                sys.stderr.write(""yacc: Syntax error at line %d, token=%s\\n"" % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write(""yacc: Syntax error, token=%s"" % errtoken.type)\n                        else:\n                            sys.stderr.write(""yacc: Parse error in input. EOF\\n"")\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we\'re in this state, the\n                # entire parse has been rolled back and we\'re completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != \'$end\':\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we\'re\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == \'$end\':\n                    # Whoa. We\'re really hosed here. Bail out\n                    return\n\n                if lookahead.type != \'error\':\n                    sym = symstack[-1]\n                    if sym.type == \'error\':\n                        # Hmmm. Error is on top of stack, we\'ll just nuke input\n                        # symbol and continue\n                        lookahead = None\n                        continue\n                    t = YaccSymbol()\n                    t.type = \'error\'\n                    if hasattr(lookahead,""lineno""):\n                        t.lineno = lookahead.lineno\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    symstack.pop()\n                    statestack.pop()\n                    state = statestack[-1]       # Potential bug fix\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError(""yacc: internal parser error!!!\\n"")\n\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    # parseopt_notrack().\n    #\n    # Optimized version of parseopt() with line number tracking removed. \n    # DO NOT EDIT THIS CODE DIRECTLY. Copy the optimized version and remove\n    # code in the #--! TRACKING sections\n    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n    def parseopt_notrack(self,input=None,lexer=None,debug=0,tracking=0,tokenfunc=None,context=None):\n        lookahead = None                 # Current lookahead symbol\n        lookaheadstack = [ ]             # Stack of lookahead symbols\n        actions = self.action            # Local reference to action table (to avoid lookup on self.)\n        goto    = self.goto              # Local reference to goto table (to avoid lookup on self.)\n        prod    = self.productions       # Local reference to production list (to avoid lookup on self.)\n        pslice  = YaccProduction(None)   # Production object passed to grammar rules\n        errorcount = 0                   # Used during error recovery \n\n        # If no lexer was given, we will try to use the lex module\n        if not lexer:\n            lex = load_ply_lex()\n            lexer = lex.lexer\n        \n        # Set up the lexer and parser objects on pslice\n        pslice.lexer = lexer\n        #pslice.parser = self\n        pslice.context = context\n\n        # If input was supplied, pass to lexer\n        if input is not None:\n            lexer.input(input)\n\n        if tokenfunc is None:\n           # Tokenize function\n           get_token = lexer.token\n        else:\n           get_token = tokenfunc\n\n        # Set up the state and symbol stacks\n\n        statestack = [ ]                # Stack of parsing states\n        self.statestack = statestack\n        symstack   = [ ]                # Stack of grammar symbols\n        self.symstack = symstack\n\n        pslice.stack = symstack         # Put in the production\n        errtoken   = None               # Err token\n\n        # The start state is assumed to be (0,$end)\n\n        statestack.append(0)\n        sym = YaccSymbol()\n        sym.type = \'$end\'\n        symstack.append(sym)\n        state = 0\n        while 1:\n            # Get the next symbol on the input.  If a lookahead symbol\n            # is already set, we just use that. Otherwise, we\'ll pull\n            # the next token off of the lookaheadstack or from the lexer\n\n            if not lookahead:\n                if not lookaheadstack:\n                    lookahead = get_token()     # Get the next token\n                else:\n                    lookahead = lookaheadstack.pop()\n                if not lookahead:\n                    lookahead = YaccSymbol()\n                    lookahead.type = \'$end\'\n\n            # Check the action table\n            ltype = lookahead.type\n            t = actions[state].get(ltype)\n\n            if t is not None:\n                if t > 0:\n                    # shift a symbol on the stack\n                    statestack.append(t)\n                    state = t\n\n                    symstack.append(lookahead)\n                    lookahead = None\n\n                    # Decrease error count on successful shift\n                    if errorcount: errorcount -=1\n                    continue\n\n                if t < 0:\n                    # reduce a symbol on the stack, emit a production\n                    p = prod[-t]\n                    pname = p.name\n                    plen  = p.len\n                    accept = False\n\n                    # Get production function\n                    sym = YaccSymbol()\n                    sym.type = pname       # Production name\n                    sym.value = None\n\n                    if plen:\n                        targ = symstack[-plen-1:]\n                        targ[0] = sym\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # below as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n                        \n                        try:\n                            # Call the grammar rule with our special slice object\n                            del symstack[-plen:]\n                            del statestack[-plen:]\n                            try:\n                                p.callable(pslice)\n                            except YaccAccept:\n                                accept = True\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n\n                        if not accept:\n                            continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n    \n                    else:\n\n                        targ = [ sym ]\n\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n                        # The code enclosed in this section is duplicated \n                        # above as a performance optimization.  Make sure\n                        # changes get made in both locations.\n\n                        pslice.slice = targ\n\n                        try:\n                            # Call the grammar rule with our special slice object\n                            p.callable(pslice)\n                            symstack.append(sym)\n                            state = goto[statestack[-1]][pname]\n                            statestack.append(state)\n                        except SyntaxError:\n                            # If an error was set. Enter error recovery state\n                            lookaheadstack.append(lookahead)\n                            symstack.pop()\n                            statestack.pop()\n                            state = statestack[-1]\n                            sym.type = \'error\'\n                            lookahead = sym\n                            errorcount = error_count\n                            self.errorok = 0\n                        continue\n                        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n                if t == 0 or accept:\n                    n = symstack[-1]\n                    return getattr(n,""value"",None)\n\n            if t == None:\n\n                # We have some kind of parsing error here.  To handle\n                # this, we are going to push the current token onto\n                # the tokenstack and replace it with an \'error\' token.\n                # If there are any synchronization rules, they may\n                # catch it.\n                #\n                # In addition to pushing the error token, we call call\n                # the user defined p_error() function if this is the\n                # first syntax error.  This function is only called if\n                # errorcount == 0.\n                if errorcount == 0 or self.errorok:\n                    errorcount = error_count\n                    self.errorok = 0\n                    errtoken = lookahead\n                    if errtoken.type == \'$end\':\n                        pass\n                        #errtoken = None               # End of file!\n                    if self.errorfunc:\n                        global errok,token,restart\n                        errok = self.errok        # Set some special functions available in error recovery\n                        token = get_token\n                        restart = self.restart\n                        if errtoken and not hasattr(errtoken,\'lexer\'):\n                            errtoken.lexer = lexer\n                        tok = self.errorfunc(errtoken)\n                        del errok, token, restart   # Delete special functions\n\n                        if self.errorok:\n                            # User must have done some kind of panic\n                            # mode recovery on their own.  The\n                            # returned token is the next lookahead\n                            lookahead = tok\n                            errtoken = None\n                            continue\n                    else:\n                        if errtoken:\n                            if hasattr(errtoken,""lineno""): lineno = lookahead.lineno\n                            else: lineno = 0\n                            if lineno:\n                                sys.stderr.write(""yacc: Syntax error at line %d, token=%s\\n"" % (lineno, errtoken.type))\n                            else:\n                                sys.stderr.write(""yacc: Syntax error, token=%s"" % errtoken.type)\n                        else:\n                            sys.stderr.write(""yacc: Parse error in input. EOF\\n"")\n                            return\n\n                else:\n                    errorcount = error_count\n\n                # case 1:  the statestack only has 1 entry on it.  If we\'re in this state, the\n                # entire parse has been rolled back and we\'re completely hosed.   The token is\n                # discarded and we just keep going.\n\n                if len(statestack) <= 1 and lookahead.type != \'$end\':\n                    lookahead = None\n                    errtoken = None\n                    state = 0\n                    # Nuke the pushback stack\n                    del lookaheadstack[:]\n                    continue\n\n                # case 2: the statestack has a couple of entries on it, but we\'re\n                # at the end of the file. nuke the top entry and generate an error token\n\n                # Start nuking entries on the stack\n                if lookahead.type == \'$end\':\n                    # Whoa. We\'re really hosed here. Bail out\n                    return\n\n                if lookahead.type != \'error\':\n                    sym = symstack[-1]\n                    if sym.type == \'error\':\n                        # Hmmm. Error is on top of stack, we\'ll just nuke input\n                        # symbol and continue\n                        lookahead = None\n                        continue\n                    t = YaccSymbol()\n                    t.type = \'error\'\n                    if hasattr(lookahead,""lineno""):\n                        t.lineno = lookahead.lineno\n                    t.value = lookahead\n                    lookaheadstack.append(lookahead)\n                    lookahead = t\n                else:\n                    symstack.pop()\n                    statestack.pop()\n                    state = statestack[-1]       # Potential bug fix\n\n                continue\n\n            # Call an error function here\n            raise RuntimeError(""yacc: internal parser error!!!\\n"")\n\n# -----------------------------------------------------------------------------\n#                          === Grammar Representation ===\n#\n# The following functions, classes, and variables are used to represent and\n# manipulate the rules that make up a grammar. \n# -----------------------------------------------------------------------------\n\nimport re\n\n# regex matching identifiers\n_is_identifier = re.compile(r\'^[a-zA-Z0-9_-]+$\')\n\n# -----------------------------------------------------------------------------\n# class Production:\n#\n# This class stores the raw information about a single production or grammar rule.\n# A grammar rule refers to a specification such as this:\n#\n#       expr : expr PLUS term \n#\n# Here are the basic attributes defined on all productions\n#\n#       name     - Name of the production.  For example \'expr\'\n#       prod     - A list of symbols on the right side [\'expr\',\'PLUS\',\'term\']\n#       prec     - Production precedence level\n#       number   - Production number.\n#       func     - Function that executes on reduce\n#       file     - File where production function is defined\n#       lineno   - Line number where production function is defined\n#\n# The following attributes are defined or optional.\n#\n#       len       - Length of the production (number of symbols on right hand side)\n#       usyms     - Set of unique symbols found in the production\n# -----------------------------------------------------------------------------\n\nclass Production(object):\n    reduced = 0\n    def __init__(self,number,name,prod,precedence=(\'right\',0),func=None,file=\'\',line=0):\n        self.name     = name\n        self.prod     = tuple(prod)\n        self.number   = number\n        self.func     = func\n        self.callable = None\n        self.file     = file\n        self.line     = line\n        self.prec     = precedence\n\n        # Internal settings used during table construction\n        \n        self.len  = len(self.prod)   # Length of the production\n\n        # Create a list of unique production symbols used in the production\n        self.usyms = [ ]             \n        for s in self.prod:\n            if s not in self.usyms:\n                self.usyms.append(s)\n\n        # List of all LR items for the production\n        self.lr_items = []\n        self.lr_next = None\n\n        # Create a string representation\n        if self.prod:\n            self.str = ""%s -> %s"" % (self.name,"" "".join(self.prod))\n        else:\n            self.str = ""%s -> <empty>"" % self.name\n\n    def __str__(self):\n        return self.str\n\n    def __repr__(self):\n        return ""Production(""+str(self)+"")""\n\n    def __len__(self):\n        return len(self.prod)\n\n    def __nonzero__(self):\n        return 1\n\n    def __getitem__(self,index):\n        return self.prod[index]\n            \n    # Return the nth lr_item from the production (or None if at the end)\n    def lr_item(self,n):\n        if n > len(self.prod): return None\n        p = LRItem(self,n)\n\n        # Precompute the list of productions immediately following.  Hack. Remove later\n        try:\n            p.lr_after = Prodnames[p.prod[n+1]]\n        except (IndexError,KeyError):\n            p.lr_after = []\n        try:\n            p.lr_before = p.prod[n-1]\n        except IndexError:\n            p.lr_before = None\n\n        return p\n    \n    # Bind the production function name to a callable\n    def bind(self,pdict):\n        if self.func:\n            self.callable = pdict[self.func]\n\n# This class serves as a minimal standin for Production objects when\n# reading table data from files.   It only contains information\n# actually used by the LR parsing engine, plus some additional\n# debugging information.\nclass MiniProduction(object):\n    def __init__(self,str,name,len,func,file,line):\n        self.name     = name\n        self.len      = len\n        self.func     = func\n        self.callable = None\n        self.file     = file\n        self.line     = line\n        self.str      = str\n    def __str__(self):\n        return self.str\n    def __repr__(self):\n        return ""MiniProduction(%s)"" % self.str\n\n    # Bind the production function name to a callable\n    def bind(self,pdict):\n        if self.func:\n            self.callable = pdict[self.func]\n\n\n# -----------------------------------------------------------------------------\n# class LRItem\n#\n# This class represents a specific stage of parsing a production rule.  For\n# example: \n#\n#       expr : expr . PLUS term \n#\n# In the above, the ""."" represents the current location of the parse.  Here\n# basic attributes:\n#\n#       name       - Name of the production.  For example \'expr\'\n#       prod       - A list of symbols on the right side [\'expr\',\'.\', \'PLUS\',\'term\']\n#       number     - Production number.\n#\n#       lr_next      Next LR item. Example, if we are \' expr -> expr . PLUS term\'\n#                    then lr_next refers to \'expr -> expr PLUS . term\'\n#       lr_index   - LR item index (location of the ""."") in the prod list.\n#       lookaheads - LALR lookahead symbols for this item\n#       len        - Length of the production (number of symbols on right hand side)\n#       lr_after    - List of all productions that immediately follow\n#       lr_before   - Grammar symbol immediately before\n# -----------------------------------------------------------------------------\n\nclass LRItem(object):\n    def __init__(self,p,n):\n        self.name       = p.name\n        self.prod       = list(p.prod)\n        self.number     = p.number\n        self.lr_index   = n\n        self.lookaheads = { }\n        self.prod.insert(n,""."")\n        self.prod       = tuple(self.prod)\n        self.len        = len(self.prod)\n        self.usyms      = p.usyms\n\n    def __str__(self):\n        if self.prod:\n            s = ""%s -> %s"" % (self.name,"" "".join(self.prod))\n        else:\n            s = ""%s -> <empty>"" % self.name\n        return s\n\n    def __repr__(self):\n        return ""LRItem(""+str(self)+"")""\n\n# -----------------------------------------------------------------------------\n# rightmost_terminal()\n#\n# Return the rightmost terminal from a list of symbols.  Used in add_production()\n# -----------------------------------------------------------------------------\ndef rightmost_terminal(symbols, terminals):\n    i = len(symbols) - 1\n    while i >= 0:\n        if symbols[i] in terminals:\n            return symbols[i]\n        i -= 1\n    return None\n\n# -----------------------------------------------------------------------------\n#                           === GRAMMAR CLASS ===\n#\n# The following class represents the contents of the specified grammar along\n# with various computed properties such as first sets, follow sets, LR items, etc.\n# This data is used for critical parts of the table generation process later.\n# -----------------------------------------------------------------------------\n\nclass GrammarError(YaccError): pass\n\nclass Grammar(object):\n    def __init__(self,terminals):\n        self.Productions  = [None]  # A list of all of the productions.  The first\n                                    # entry is always reserved for the purpose of\n                                    # building an augmented grammar\n\n        self.Prodnames    = { }     # A dictionary mapping the names of nonterminals to a list of all\n                                    # productions of that nonterminal.\n\n        self.Prodmap      = { }     # A dictionary that is only used to detect duplicate\n                                    # productions.\n\n        self.Terminals    = { }     # A dictionary mapping the names of terminal symbols to a\n                                    # list of the rules where they are used.\n\n        for term in terminals:\n            self.Terminals[term] = []\n\n        self.Terminals[\'error\'] = []\n\n        self.Nonterminals = { }     # A dictionary mapping names of nonterminals to a list\n                                    # of rule numbers where they are used.\n\n        self.First        = { }     # A dictionary of precomputed FIRST(x) symbols\n\n        self.Follow       = { }     # A dictionary of precomputed FOLLOW(x) symbols\n\n        self.Precedence   = { }     # Precedence rules for each terminal. Contains tuples of the\n                                    # form (\'right\',level) or (\'nonassoc\', level) or (\'left\',level)\n\n        self.UsedPrecedence = { }   # Precedence rules that were actually used by the grammer.\n                                    # This is only used to provide error checking and to generate\n                                    # a warning about unused precedence rules.\n\n        self.Start = None           # Starting symbol for the grammar\n\n\n    def __len__(self):\n        return len(self.Productions)\n\n    def __getitem__(self,index):\n        return self.Productions[index]\n\n    # -----------------------------------------------------------------------------\n    # set_precedence()\n    #\n    # Sets the precedence for a given terminal. assoc is the associativity such as\n    # \'left\',\'right\', or \'nonassoc\'.  level is a numeric level.\n    #\n    # -----------------------------------------------------------------------------\n\n    def set_precedence(self,term,assoc,level):\n        assert self.Productions == [None],""Must call set_precedence() before add_production()""\n        if term in self.Precedence:\n            raise GrammarError(""Precedence already specified for terminal \'%s\'"" % term)\n        if assoc not in [\'left\',\'right\',\'nonassoc\']:\n            raise GrammarError(""Associativity must be one of \'left\',\'right\', or \'nonassoc\'"")\n        self.Precedence[term] = (assoc,level)\n \n    # -----------------------------------------------------------------------------\n    # add_production()\n    #\n    # Given an action function, this function assembles a production rule and\n    # computes its precedence level.\n    #\n    # The production rule is supplied as a list of symbols.   For example,\n    # a rule such as \'expr : expr PLUS term\' has a production name of \'expr\' and\n    # symbols [\'expr\',\'PLUS\',\'term\'].\n    #\n    # Precedence is determined by the precedence of the right-most non-terminal\n    # or the precedence of a terminal specified by %prec.\n    #\n    # A variety of error checks are performed to make sure production symbols\n    # are valid and that %prec is used correctly.\n    # -----------------------------------------------------------------------------\n\n    def add_production(self,prodname,syms,func=None,file=\'\',line=0):\n\n        if prodname in self.Terminals:\n            raise GrammarError(""%s:%d: Illegal rule name \'%s\'. Already defined as a token"" % (file,line,prodname))\n        if prodname == \'error\':\n            raise GrammarError(""%s:%d: Illegal rule name \'%s\'. error is a reserved word"" % (file,line,prodname))\n        if not _is_identifier.match(prodname):\n            raise GrammarError(""%s:%d: Illegal rule name \'%s\'"" % (file,line,prodname))\n\n        # Look for literal tokens \n        for n,s in enumerate(syms):\n            if s[0] in ""\'\\"""":\n                 try:\n                     c = eval(s)\n                     if (len(c) > 1):\n                          raise GrammarError(""%s:%d: Literal token %s in rule \'%s\' may only be a single character"" % (file,line,s, prodname))\n                     if not c in self.Terminals:\n                          self.Terminals[c] = []\n                     syms[n] = c\n                     continue\n                 except SyntaxError:\n                     pass\n            if not _is_identifier.match(s) and s != \'%prec\':\n                raise GrammarError(""%s:%d: Illegal name \'%s\' in rule \'%s\'"" % (file,line,s, prodname))\n        \n        # Determine the precedence level\n        if \'%prec\' in syms:\n            if syms[-1] == \'%prec\':\n                raise GrammarError(""%s:%d: Syntax error. Nothing follows %%prec"" % (file,line))\n            if syms[-2] != \'%prec\':\n                raise GrammarError(""%s:%d: Syntax error. %%prec can only appear at the end of a grammar rule"" % (file,line))\n            precname = syms[-1]\n            prodprec = self.Precedence.get(precname,None)\n            if not prodprec:\n                raise GrammarError(""%s:%d: Nothing known about the precedence of \'%s\'"" % (file,line,precname))\n            else:\n                self.UsedPrecedence[precname] = 1\n            del syms[-2:]     # Drop %prec from the rule\n        else:\n            # If no %prec, precedence is determined by the rightmost terminal symbol\n            precname = rightmost_terminal(syms,self.Terminals)\n            prodprec = self.Precedence.get(precname,(\'right\',0)) \n            \n        # See if the rule is already in the rulemap\n        map = ""%s -> %s"" % (prodname,syms)\n        if map in self.Prodmap:\n            m = self.Prodmap[map]\n            raise GrammarError(""%s:%d: Duplicate rule %s. "" % (file,line, m) +\n                               ""Previous definition at %s:%d"" % (m.file, m.line))\n\n        # From this point on, everything is valid.  Create a new Production instance\n        pnumber  = len(self.Productions)\n        if not prodname in self.Nonterminals:\n            self.Nonterminals[prodname] = [ ]\n\n        # Add the production number to Terminals and Nonterminals\n        for t in syms:\n            if t in self.Terminals:\n                self.Terminals[t].append(pnumber)\n            else:\n                if not t in self.Nonterminals:\n                    self.Nonterminals[t] = [ ]\n                self.Nonterminals[t].append(pnumber)\n\n        # Create a production and add it to the list of productions\n        p = Production(pnumber,prodname,syms,prodprec,func,file,line)\n        self.Productions.append(p)\n        self.Prodmap[map] = p\n\n        # Add to the global productions list\n        try:\n            self.Prodnames[prodname].append(p)\n        except KeyError:\n            self.Prodnames[prodname] = [ p ]\n        return 0\n\n    # -----------------------------------------------------------------------------\n    # set_start()\n    #\n    # Sets the starting symbol and creates the augmented grammar.  Production \n    # rule 0 is S\' -> start where start is the start symbol.\n    # -----------------------------------------------------------------------------\n\n    def set_start(self,start=None):\n        if not start:\n            start = self.Productions[1].name\n        if start not in self.Nonterminals:\n            raise GrammarError(""start symbol %s undefined"" % start)\n        self.Productions[0] = Production(0,""S\'"",[start])\n        self.Nonterminals[start].append(0)\n        self.Start = start\n\n    # -----------------------------------------------------------------------------\n    # find_unreachable()\n    #\n    # Find all of the nonterminal symbols that can\'t be reached from the starting\n    # symbol.  Returns a list of nonterminals that can\'t be reached.\n    # -----------------------------------------------------------------------------\n\n    def find_unreachable(self):\n        \n        # Mark all symbols that are reachable from a symbol s\n        def mark_reachable_from(s):\n            if reachable[s]:\n                # We\'ve already reached symbol s.\n                return\n            reachable[s] = 1\n            for p in self.Prodnames.get(s,[]):\n                for r in p.prod:\n                    mark_reachable_from(r)\n\n        reachable   = { }\n        for s in list(self.Terminals) + list(self.Nonterminals):\n            reachable[s] = 0\n\n        mark_reachable_from( self.Productions[0].prod[0] )\n\n        return [s for s in list(self.Nonterminals)\n                        if not reachable[s]]\n    \n    # -----------------------------------------------------------------------------\n    # infinite_cycles()\n    #\n    # This function looks at the various parsing rules and tries to detect\n    # infinite recursion cycles (grammar rules where there is no possible way\n    # to derive a string of only terminals).\n    # -----------------------------------------------------------------------------\n\n    def infinite_cycles(self):\n        terminates = {}\n\n        # Terminals:\n        for t in self.Terminals:\n            terminates[t] = 1\n\n        terminates[\'$end\'] = 1\n\n        # Nonterminals:\n\n        # Initialize to false:\n        for n in self.Nonterminals:\n            terminates[n] = 0\n\n        # Then propagate termination until no change:\n        while 1:\n            some_change = 0\n            for (n,pl) in self.Prodnames.items():\n                # Nonterminal n terminates iff any of its productions terminates.\n                for p in pl:\n                    # Production p terminates iff all of its rhs symbols terminate.\n                    for s in p.prod:\n                        if not terminates[s]:\n                            # The symbol s does not terminate,\n                            # so production p does not terminate.\n                            p_terminates = 0\n                            break\n                    else:\n                        # didn\'t break from the loop,\n                        # so every symbol s terminates\n                        # so production p terminates.\n                        p_terminates = 1\n\n                    if p_terminates:\n                        # symbol n terminates!\n                        if not terminates[n]:\n                            terminates[n] = 1\n                            some_change = 1\n                        # Don\'t need to consider any more productions for this n.\n                        break\n\n            if not some_change:\n                break\n\n        infinite = []\n        for (s,term) in terminates.items():\n            if not term:\n                if not s in self.Prodnames and not s in self.Terminals and s != \'error\':\n                    # s is used-but-not-defined, and we\'ve already warned of that,\n                    # so it would be overkill to say that it\'s also non-terminating.\n                    pass\n                else:\n                    infinite.append(s)\n\n        return infinite\n\n\n    # -----------------------------------------------------------------------------\n    # undefined_symbols()\n    #\n    # Find all symbols that were used the grammar, but not defined as tokens or\n    # grammar rules.  Returns a list of tuples (sym, prod) where sym in the symbol\n    # and prod is the production where the symbol was used. \n    # -----------------------------------------------------------------------------\n    def undefined_symbols(self):\n        result = []\n        for p in self.Productions:\n            if not p: continue\n\n            for s in p.prod:\n                if not s in self.Prodnames and not s in self.Terminals and s != \'error\':\n                    result.append((s,p))\n        return result\n\n    # -----------------------------------------------------------------------------\n    # unused_terminals()\n    #\n    # Find all terminals that were defined, but not used by the grammar.  Returns\n    # a list of all symbols.\n    # -----------------------------------------------------------------------------\n    def unused_terminals(self):\n        unused_tok = []\n        for s,v in self.Terminals.items():\n            if s != \'error\' and not v:\n                unused_tok.append(s)\n\n        return unused_tok\n\n    # ------------------------------------------------------------------------------\n    # unused_rules()\n    #\n    # Find all grammar rules that were defined,  but not used (maybe not reachable)\n    # Returns a list of productions.\n    # ------------------------------------------------------------------------------\n\n    def unused_rules(self):\n        unused_prod = []\n        for s,v in self.Nonterminals.items():\n            if not v:\n                p = self.Prodnames[s][0]\n                unused_prod.append(p)\n        return unused_prod\n\n    # -----------------------------------------------------------------------------\n    # unused_precedence()\n    #\n    # Returns a list of tuples (term,precedence) corresponding to precedence\n    # rules that were never used by the grammar.  term is the name of the terminal\n    # on which precedence was applied and precedence is a string such as \'left\' or\n    # \'right\' corresponding to the type of precedence. \n    # -----------------------------------------------------------------------------\n\n    def unused_precedence(self):\n        unused = []\n        for termname in self.Precedence:\n            if not (termname in self.Terminals or termname in self.UsedPrecedence):\n                unused.append((termname,self.Precedence[termname][0]))\n                \n        return unused\n\n    # -------------------------------------------------------------------------\n    # _first()\n    #\n    # Compute the value of FIRST1(beta) where beta is a tuple of symbols.\n    #\n    # During execution of compute_first1, the result may be incomplete.\n    # Afterward (e.g., when called from compute_follow()), it will be complete.\n    # -------------------------------------------------------------------------\n    def _first(self,beta):\n\n        # We are computing First(x1,x2,x3,...,xn)\n        result = [ ]\n        for x in beta:\n            x_produces_empty = 0\n\n            # Add all the non-<empty> symbols of First[x] to the result.\n            for f in self.First[x]:\n                if f == \'<empty>\':\n                    x_produces_empty = 1\n                else:\n                    if f not in result: result.append(f)\n\n            if x_produces_empty:\n                # We have to consider the next x in beta,\n                # i.e. stay in the loop.\n                pass\n            else:\n                # We don\'t have to consider any further symbols in beta.\n                break\n        else:\n            # There was no \'break\' from the loop,\n            # so x_produces_empty was true for all x in beta,\n            # so beta produces empty as well.\n            result.append(\'<empty>\')\n\n        return result\n\n    # -------------------------------------------------------------------------\n    # compute_first()\n    #\n    # Compute the value of FIRST1(X) for all symbols\n    # -------------------------------------------------------------------------\n    def compute_first(self):\n        if self.First:\n            return self.First\n\n        # Terminals:\n        for t in self.Terminals:\n            self.First[t] = [t]\n\n        self.First[\'$end\'] = [\'$end\']\n\n        # Nonterminals:\n\n        # Initialize to the empty set:\n        for n in self.Nonterminals:\n            self.First[n] = []\n\n        # Then propagate symbols until no change:\n        while 1:\n            some_change = 0\n            for n in self.Nonterminals:\n                for p in self.Prodnames[n]:\n                    for f in self._first(p.prod):\n                        if f not in self.First[n]:\n                            self.First[n].append( f )\n                            some_change = 1\n            if not some_change:\n                break\n        \n        return self.First\n\n    # ---------------------------------------------------------------------\n    # compute_follow()\n    #\n    # Computes all of the follow sets for every non-terminal symbol.  The\n    # follow set is the set of all symbols that might follow a given\n    # non-terminal.  See the Dragon book, 2nd Ed. p. 189.\n    # ---------------------------------------------------------------------\n    def compute_follow(self,start=None):\n        # If already computed, return the result\n        if self.Follow:\n            return self.Follow\n\n        # If first sets not computed yet, do that first.\n        if not self.First:\n            self.compute_first()\n\n        # Add \'$end\' to the follow list of the start symbol\n        for k in self.Nonterminals:\n            self.Follow[k] = [ ]\n\n        if not start:\n            start = self.Productions[1].name\n\n        self.Follow[start] = [ \'$end\' ]\n\n        while 1:\n            didadd = 0\n            for p in self.Productions[1:]:\n                # Here is the production set\n                for i in range(len(p.prod)):\n                    B = p.prod[i]\n                    if B in self.Nonterminals:\n                        # Okay. We got a non-terminal in a production\n                        fst = self._first(p.prod[i+1:])\n                        hasempty = 0\n                        for f in fst:\n                            if f != \'<empty>\' and f not in self.Follow[B]:\n                                self.Follow[B].append(f)\n                                didadd = 1\n                            if f == \'<empty>\':\n                                hasempty = 1\n                        if hasempty or i == (len(p.prod)-1):\n                            # Add elements of follow(a) to follow(b)\n                            for f in self.Follow[p.name]:\n                                if f not in self.Follow[B]:\n                                    self.Follow[B].append(f)\n                                    didadd = 1\n            if not didadd: break\n        return self.Follow\n\n\n    # -----------------------------------------------------------------------------\n    # build_lritems()\n    #\n    # This function walks the list of productions and builds a complete set of the\n    # LR items.  The LR items are stored in two ways:  First, they are uniquely\n    # numbered and placed in the list _lritems.  Second, a linked list of LR items\n    # is built for each production.  For example:\n    #\n    #   E -> E PLUS E\n    #\n    # Creates the list\n    #\n    #  [E -> . E PLUS E, E -> E . PLUS E, E -> E PLUS . E, E -> E PLUS E . ]\n    # -----------------------------------------------------------------------------\n\n    def build_lritems(self):\n        for p in self.Productions:\n            lastlri = p\n            i = 0\n            lr_items = []\n            while 1:\n                if i > len(p):\n                    lri = None\n                else:\n                    lri = LRItem(p,i)\n                    # Precompute the list of productions immediately following\n                    try:\n                        lri.lr_after = self.Prodnames[lri.prod[i+1]]\n                    except (IndexError,KeyError):\n                        lri.lr_after = []\n                    try:\n                        lri.lr_before = lri.prod[i-1]\n                    except IndexError:\n                        lri.lr_before = None\n\n                lastlri.lr_next = lri\n                if not lri: break\n                lr_items.append(lri)\n                lastlri = lri\n                i += 1\n            p.lr_items = lr_items\n\n# -----------------------------------------------------------------------------\n#                            == Class LRTable ==\n#\n# This basic class represents a basic table of LR parsing information.  \n# Methods for generating the tables are not defined here.  They are defined\n# in the derived class LRGeneratedTable.\n# -----------------------------------------------------------------------------\n\nclass VersionError(YaccError): pass\n\nclass LRTable(object):\n    def __init__(self):\n        self.lr_action = None\n        self.lr_goto = None\n        self.lr_productions = None\n        self.lr_method = None\n\n    def read_table(self,module):\n        if isinstance(module,types.ModuleType):\n            parsetab = module\n        else:\n            if sys.version_info[0] < 3:\n                exec(""import %s as parsetab"" % module)\n            else:\n                env = { }\n                exec(""import %s as parsetab"" % module, env, env)\n                parsetab = env[\'parsetab\']\n\n        if parsetab._tabversion != __tabversion__:\n            raise VersionError(""yacc table file version is out of date"")\n\n        self.lr_action = parsetab._lr_action\n        self.lr_goto = parsetab._lr_goto\n\n        self.lr_productions = []\n        for p in parsetab._lr_productions:\n            self.lr_productions.append(MiniProduction(*p))\n\n        self.lr_method = parsetab._lr_method\n        return parsetab._lr_signature\n\n    def read_pickle(self,filename):\n        try:\n            import cPickle as pickle\n        except ImportError:\n            import pickle\n\n        in_f = open(filename,""rb"")\n\n        tabversion = pickle.load(in_f)\n        if tabversion != __tabversion__:\n            raise VersionError(""yacc table file version is out of date"")\n        self.lr_method = pickle.load(in_f)\n        signature      = pickle.load(in_f)\n        self.lr_action = pickle.load(in_f)\n        self.lr_goto   = pickle.load(in_f)\n        productions    = pickle.load(in_f)\n\n        self.lr_productions = []\n        for p in productions:\n            self.lr_productions.append(MiniProduction(*p))\n\n        in_f.close()\n        return signature\n\n    # Bind all production function names to callable objects in pdict\n    def bind_callables(self,pdict):\n        for p in self.lr_productions:\n            p.bind(pdict)\n    \n# -----------------------------------------------------------------------------\n#                           === LR Generator ===\n#\n# The following classes and functions are used to generate LR parsing tables on \n# a grammar.\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# digraph()\n# traverse()\n#\n# The following two functions are used to compute set valued functions\n# of the form:\n#\n#     F(x) = F\'(x) U U{F(y) | x R y}\n#\n# This is used to compute the values of Read() sets as well as FOLLOW sets\n# in LALR(1) generation.\n#\n# Inputs:  X    - An input set\n#          R    - A relation\n#          FP   - Set-valued function\n# ------------------------------------------------------------------------------\n\ndef digraph(X,R,FP):\n    N = { }\n    for x in X:\n       N[x] = 0\n    stack = []\n    F = { }\n    for x in X:\n        if N[x] == 0: traverse(x,N,stack,F,X,R,FP)\n    return F\n\ndef traverse(x,N,stack,F,X,R,FP):\n    stack.append(x)\n    d = len(stack)\n    N[x] = d\n    F[x] = FP(x)             # F(X) <- F\'(x)\n\n    rel = R(x)               # Get y\'s related to x\n    for y in rel:\n        if N[y] == 0:\n             traverse(y,N,stack,F,X,R,FP)\n        N[x] = min(N[x],N[y])\n        for a in F.get(y,[]):\n            if a not in F[x]: F[x].append(a)\n    if N[x] == d:\n       N[stack[-1]] = MAXINT\n       F[stack[-1]] = F[x]\n       element = stack.pop()\n       while element != x:\n           N[stack[-1]] = MAXINT\n           F[stack[-1]] = F[x]\n           element = stack.pop()\n\nclass LALRError(YaccError): pass\n\n# -----------------------------------------------------------------------------\n#                             == LRGeneratedTable ==\n#\n# This class implements the LR table generation algorithm.  There are no\n# public methods except for write()\n# -----------------------------------------------------------------------------\n\nclass LRGeneratedTable(LRTable):\n    def __init__(self,grammar,method=\'LALR\',log=None):\n        if method not in [\'SLR\',\'LALR\']:\n            raise LALRError(""Unsupported method %s"" % method)\n\n        self.grammar = grammar\n        self.lr_method = method\n\n        # Set up the logger\n        if not log:\n            log = NullLogger()\n        self.log = log\n\n        # Internal attributes\n        self.lr_action     = {}        # Action table\n        self.lr_goto       = {}        # Goto table\n        self.lr_productions  = grammar.Productions    # Copy of grammar Production array\n        self.lr_goto_cache = {}        # Cache of computed gotos\n        self.lr0_cidhash   = {}        # Cache of closures\n\n        self._add_count    = 0         # Internal counter used to detect cycles\n\n        # Diagonistic information filled in by the table generator\n        self.sr_conflict   = 0\n        self.rr_conflict   = 0\n        self.conflicts     = []        # List of conflicts\n\n        self.sr_conflicts  = []\n        self.rr_conflicts  = []\n\n        # Build the tables\n        self.grammar.build_lritems()\n        self.grammar.compute_first()\n        self.grammar.compute_follow()\n        self.lr_parse_table()\n\n    # Compute the LR(0) closure operation on I, where I is a set of LR(0) items.\n\n    def lr0_closure(self,I):\n        self._add_count += 1\n\n        # Add everything in I to J\n        J = I[:]\n        didadd = 1\n        while didadd:\n            didadd = 0\n            for j in J:\n                for x in j.lr_after:\n                    if getattr(x,""lr0_added"",0) == self._add_count: continue\n                    # Add B --> .G to J\n                    J.append(x.lr_next)\n                    x.lr0_added = self._add_count\n                    didadd = 1\n\n        return J\n\n    # Compute the LR(0) goto function goto(I,X) where I is a set\n    # of LR(0) items and X is a grammar symbol.   This function is written\n    # in a way that guarantees uniqueness of the generated goto sets\n    # (i.e. the same goto set will never be returned as two different Python\n    # objects).  With uniqueness, we can later do fast set comparisons using\n    # id(obj) instead of element-wise comparison.\n\n    def lr0_goto(self,I,x):\n        # First we look for a previously cached entry\n        g = self.lr_goto_cache.get((id(I),x),None)\n        if g: return g\n\n        # Now we generate the goto set in a way that guarantees uniqueness\n        # of the result\n\n        s = self.lr_goto_cache.get(x,None)\n        if not s:\n            s = { }\n            self.lr_goto_cache[x] = s\n\n        gs = [ ]\n        for p in I:\n            n = p.lr_next\n            if n and n.lr_before == x:\n                s1 = s.get(id(n),None)\n                if not s1:\n                    s1 = { }\n                    s[id(n)] = s1\n                gs.append(n)\n                s = s1\n        g = s.get(\'$end\',None)\n        if not g:\n            if gs:\n                g = self.lr0_closure(gs)\n                s[\'$end\'] = g\n            else:\n                s[\'$end\'] = gs\n        self.lr_goto_cache[(id(I),x)] = g\n        return g\n\n    # Compute the LR(0) sets of item function\n    def lr0_items(self):\n\n        C = [ self.lr0_closure([self.grammar.Productions[0].lr_next]) ]\n        i = 0\n        for I in C:\n            self.lr0_cidhash[id(I)] = i\n            i += 1\n\n        # Loop over the items in C and each grammar symbols\n        i = 0\n        while i < len(C):\n            I = C[i]\n            i += 1\n\n            # Collect all of the symbols that could possibly be in the goto(I,X) sets\n            asyms = { }\n            for ii in I:\n                for s in ii.usyms:\n                    asyms[s] = None\n\n            for x in asyms:\n                g = self.lr0_goto(I,x)\n                if not g:  continue\n                if id(g) in self.lr0_cidhash: continue\n                self.lr0_cidhash[id(g)] = len(C)\n                C.append(g)\n\n        return C\n\n    # -----------------------------------------------------------------------------\n    #                       ==== LALR(1) Parsing ====\n    #\n    # LALR(1) parsing is almost exactly the same as SLR except that instead of\n    # relying upon Follow() sets when performing reductions, a more selective\n    # lookahead set that incorporates the state of the LR(0) machine is utilized.\n    # Thus, we mainly just have to focus on calculating the lookahead sets.\n    #\n    # The method used here is due to DeRemer and Pennelo (1982).\n    #\n    # DeRemer, F. L., and T. J. Pennelo: ""Efficient Computation of LALR(1)\n    #     Lookahead Sets"", ACM Transactions on Programming Languages and Systems,\n    #     Vol. 4, No. 4, Oct. 1982, pp. 615-649\n    #\n    # Further details can also be found in:\n    #\n    #  J. Tremblay and P. Sorenson, ""The Theory and Practice of Compiler Writing"",\n    #      McGraw-Hill Book Company, (1985).\n    #\n    # -----------------------------------------------------------------------------\n\n    # -----------------------------------------------------------------------------\n    # compute_nullable_nonterminals()\n    #\n    # Creates a dictionary containing all of the non-terminals that might produce\n    # an empty production.\n    # -----------------------------------------------------------------------------\n\n    def compute_nullable_nonterminals(self):\n        nullable = {}\n        num_nullable = 0\n        while 1:\n           for p in self.grammar.Productions[1:]:\n               if p.len == 0:\n                    nullable[p.name] = 1\n                    continue\n               for t in p.prod:\n                    if not t in nullable: break\n               else:\n                    nullable[p.name] = 1\n           if len(nullable) == num_nullable: break\n           num_nullable = len(nullable)\n        return nullable\n\n    # -----------------------------------------------------------------------------\n    # find_nonterminal_trans(C)\n    #\n    # Given a set of LR(0) items, this functions finds all of the non-terminal\n    # transitions.    These are transitions in which a dot appears immediately before\n    # a non-terminal.   Returns a list of tuples of the form (state,N) where state\n    # is the state number and N is the nonterminal symbol.\n    #\n    # The input C is the set of LR(0) items.\n    # -----------------------------------------------------------------------------\n\n    def find_nonterminal_transitions(self,C):\n         trans = []\n         for state in range(len(C)):\n             for p in C[state]:\n                 if p.lr_index < p.len - 1:\n                      t = (state,p.prod[p.lr_index+1])\n                      if t[1] in self.grammar.Nonterminals:\n                            if t not in trans: trans.append(t)\n             state = state + 1\n         return trans\n\n    # -----------------------------------------------------------------------------\n    # dr_relation()\n    #\n    # Computes the DR(p,A) relationships for non-terminal transitions.  The input\n    # is a tuple (state,N) where state is a number and N is a nonterminal symbol.\n    #\n    # Returns a list of terminals.\n    # -----------------------------------------------------------------------------\n\n    def dr_relation(self,C,trans,nullable):\n        dr_set = { }\n        state,N = trans\n        terms = []\n\n        g = self.lr0_goto(C[state],N)\n        for p in g:\n           if p.lr_index < p.len - 1:\n               a = p.prod[p.lr_index+1]\n               if a in self.grammar.Terminals:\n                   if a not in terms: terms.append(a)\n\n        # This extra bit is to handle the start state\n        if state == 0 and N == self.grammar.Productions[0].prod[0]:\n           terms.append(\'$end\')\n\n        return terms\n\n    # -----------------------------------------------------------------------------\n    # reads_relation()\n    #\n    # Computes the READS() relation (p,A) READS (t,C).\n    # -----------------------------------------------------------------------------\n\n    def reads_relation(self,C, trans, empty):\n        # Look for empty transitions\n        rel = []\n        state, N = trans\n\n        g = self.lr0_goto(C[state],N)\n        j = self.lr0_cidhash.get(id(g),-1)\n        for p in g:\n            if p.lr_index < p.len - 1:\n                 a = p.prod[p.lr_index + 1]\n                 if a in empty:\n                      rel.append((j,a))\n\n        return rel\n\n    # -----------------------------------------------------------------------------\n    # compute_lookback_includes()\n    #\n    # Determines the lookback and includes relations\n    #\n    # LOOKBACK:\n    #\n    # This relation is determined by running the LR(0) state machine forward.\n    # For example, starting with a production ""N : . A B C"", we run it forward\n    # to obtain ""N : A B C .""   We then build a relationship between this final\n    # state and the starting state.   These relationships are stored in a dictionary\n    # lookdict.\n    #\n    # INCLUDES:\n    #\n    # Computes the INCLUDE() relation (p,A) INCLUDES (p\',B).\n    #\n    # This relation is used to determine non-terminal transitions that occur\n    # inside of other non-terminal transition states.   (p,A) INCLUDES (p\', B)\n    # if the following holds:\n    #\n    #       B -> LAT, where T -> epsilon and p\' -L-> p\n    #\n    # L is essentially a prefix (which may be empty), T is a suffix that must be\n    # able to derive an empty string.  State p\' must lead to state p with the string L.\n    #\n    # -----------------------------------------------------------------------------\n\n    def compute_lookback_includes(self,C,trans,nullable):\n\n        lookdict = {}          # Dictionary of lookback relations\n        includedict = {}       # Dictionary of include relations\n\n        # Make a dictionary of non-terminal transitions\n        dtrans = {}\n        for t in trans:\n            dtrans[t] = 1\n\n        # Loop over all transitions and compute lookbacks and includes\n        for state,N in trans:\n            lookb = []\n            includes = []\n            for p in C[state]:\n                if p.name != N: continue\n\n                # Okay, we have a name match.  We now follow the production all the way\n                # through the state machine until we get the . on the right hand side\n\n                lr_index = p.lr_index\n                j = state\n                while lr_index < p.len - 1:\n                     lr_index = lr_index + 1\n                     t = p.prod[lr_index]\n\n                     # Check to see if this symbol and state are a non-terminal transition\n                     if (j,t) in dtrans:\n                           # Yes.  Okay, there is some chance that this is an includes relation\n                           # the only way to know for certain is whether the rest of the\n                           # production derives empty\n\n                           li = lr_index + 1\n                           while li < p.len:\n                                if p.prod[li] in self.grammar.Terminals: break      # No forget it\n                                if not p.prod[li] in nullable: break\n                                li = li + 1\n                           else:\n                                # Appears to be a relation between (j,t) and (state,N)\n                                includes.append((j,t))\n\n                     g = self.lr0_goto(C[j],t)               # Go to next set\n                     j = self.lr0_cidhash.get(id(g),-1)     # Go to next state\n\n                # When we get here, j is the final state, now we have to locate the production\n                for r in C[j]:\n                     if r.name != p.name: continue\n                     if r.len != p.len:   continue\n                     i = 0\n                     # This look is comparing a production "". A B C"" with ""A B C .""\n                     while i < r.lr_index:\n                          if r.prod[i] != p.prod[i+1]: break\n                          i = i + 1\n                     else:\n                          lookb.append((j,r))\n            for i in includes:\n                 if not i in includedict: includedict[i] = []\n                 includedict[i].append((state,N))\n            lookdict[(state,N)] = lookb\n\n        return lookdict,includedict\n\n    # -----------------------------------------------------------------------------\n    # compute_read_sets()\n    #\n    # Given a set of LR(0) items, this function computes the read sets.\n    #\n    # Inputs:  C        =  Set of LR(0) items\n    #          ntrans   = Set of nonterminal transitions\n    #          nullable = Set of empty transitions\n    #\n    # Returns a set containing the read sets\n    # -----------------------------------------------------------------------------\n\n    def compute_read_sets(self,C, ntrans, nullable):\n        FP = lambda x: self.dr_relation(C,x,nullable)\n        R =  lambda x: self.reads_relation(C,x,nullable)\n        F = digraph(ntrans,R,FP)\n        return F\n\n    # -----------------------------------------------------------------------------\n    # compute_follow_sets()\n    #\n    # Given a set of LR(0) items, a set of non-terminal transitions, a readset,\n    # and an include set, this function computes the follow sets\n    #\n    # Follow(p,A) = Read(p,A) U U {Follow(p\',B) | (p,A) INCLUDES (p\',B)}\n    #\n    # Inputs:\n    #            ntrans     = Set of nonterminal transitions\n    #            readsets   = Readset (previously computed)\n    #            inclsets   = Include sets (previously computed)\n    #\n    # Returns a set containing the follow sets\n    # -----------------------------------------------------------------------------\n\n    def compute_follow_sets(self,ntrans,readsets,inclsets):\n         FP = lambda x: readsets[x]\n         R  = lambda x: inclsets.get(x,[])\n         F = digraph(ntrans,R,FP)\n         return F\n\n    # -----------------------------------------------------------------------------\n    # add_lookaheads()\n    #\n    # Attaches the lookahead symbols to grammar rules.\n    #\n    # Inputs:    lookbacks         -  Set of lookback relations\n    #            followset         -  Computed follow set\n    #\n    # This function directly attaches the lookaheads to productions contained\n    # in the lookbacks set\n    # -----------------------------------------------------------------------------\n\n    def add_lookaheads(self,lookbacks,followset):\n        for trans,lb in lookbacks.items():\n            # Loop over productions in lookback\n            for state,p in lb:\n                 if not state in p.lookaheads:\n                      p.lookaheads[state] = []\n                 f = followset.get(trans,[])\n                 for a in f:\n                      if a not in p.lookaheads[state]: p.lookaheads[state].append(a)\n\n    # -----------------------------------------------------------------------------\n    # add_lalr_lookaheads()\n    #\n    # This function does all of the work of adding lookahead information for use\n    # with LALR parsing\n    # -----------------------------------------------------------------------------\n\n    def add_lalr_lookaheads(self,C):\n        # Determine all of the nullable nonterminals\n        nullable = self.compute_nullable_nonterminals()\n\n        # Find all non-terminal transitions\n        trans = self.find_nonterminal_transitions(C)\n\n        # Compute read sets\n        readsets = self.compute_read_sets(C,trans,nullable)\n\n        # Compute lookback/includes relations\n        lookd, included = self.compute_lookback_includes(C,trans,nullable)\n\n        # Compute LALR FOLLOW sets\n        followsets = self.compute_follow_sets(trans,readsets,included)\n\n        # Add all of the lookaheads\n        self.add_lookaheads(lookd,followsets)\n\n    # -----------------------------------------------------------------------------\n    # lr_parse_table()\n    #\n    # This function constructs the parse tables for SLR or LALR\n    # -----------------------------------------------------------------------------\n    def lr_parse_table(self):\n        Productions = self.grammar.Productions\n        Precedence  = self.grammar.Precedence\n        goto   = self.lr_goto         # Goto array\n        action = self.lr_action       # Action array\n        log    = self.log             # Logger for output\n\n        actionp = { }                 # Action production array (temporary)\n        \n        log.info(""Parsing method: %s"", self.lr_method)\n\n        # Step 1: Construct C = { I0, I1, ... IN}, collection of LR(0) items\n        # This determines the number of states\n\n        C = self.lr0_items()\n\n        if self.lr_method == \'LALR\':\n            self.add_lalr_lookaheads(C)\n\n        # Build the parser table, state by state\n        st = 0\n        for I in C:\n            # Loop over each production in I\n            actlist = [ ]              # List of actions\n            st_action  = { }\n            st_actionp = { }\n            st_goto    = { }\n            log.info("""")\n            log.info(""state %d"", st)\n            log.info("""")\n            for p in I:\n                log.info(""    (%d) %s"", p.number, str(p))\n            log.info("""")\n\n            for p in I:\n                    if p.len == p.lr_index + 1:\n                        if p.name == ""S\'"":\n                            # Start symbol. Accept!\n                            st_action[""$end""] = 0\n                            st_actionp[""$end""] = p\n                        else:\n                            # We are at the end of a production.  Reduce!\n                            if self.lr_method == \'LALR\':\n                                laheads = p.lookaheads[st]\n                            else:\n                                laheads = self.grammar.Follow[p.name]\n                            for a in laheads:\n                                actlist.append((a,p,""reduce using rule %d (%s)"" % (p.number,p)))\n                                r = st_action.get(a,None)\n                                if r is not None:\n                                    # Whoa. Have a shift/reduce or reduce/reduce conflict\n                                    if r > 0:\n                                        # Need to decide on shift or reduce here\n                                        # By default we favor shifting. Need to add\n                                        # some precedence rules here.\n                                        sprec,slevel = Productions[st_actionp[a].number].prec\n                                        rprec,rlevel = Precedence.get(a,(\'right\',0))\n                                        if (slevel < rlevel) or ((slevel == rlevel) and (rprec == \'left\')):\n                                            # We really need to reduce here.\n                                            st_action[a] = -p.number\n                                            st_actionp[a] = p\n                                            if not slevel and not rlevel:\n                                                log.info(""  ! shift/reduce conflict for %s resolved as reduce"",a)\n                                                self.sr_conflicts.append((st,a,\'reduce\'))\n                                            Productions[p.number].reduced += 1\n                                        elif (slevel == rlevel) and (rprec == \'nonassoc\'):\n                                            st_action[a] = None\n                                        else:\n                                            # Hmmm. Guess we\'ll keep the shift\n                                            if not rlevel:\n                                                log.info(""  ! shift/reduce conflict for %s resolved as shift"",a)\n                                                self.sr_conflicts.append((st,a,\'shift\'))\n                                    elif r < 0:\n                                        # Reduce/reduce conflict.   In this case, we favor the rule\n                                        # that was defined first in the grammar file\n                                        oldp = Productions[-r]\n                                        pp = Productions[p.number]\n                                        if oldp.line > pp.line:\n                                            st_action[a] = -p.number\n                                            st_actionp[a] = p\n                                            chosenp,rejectp = pp,oldp\n                                            Productions[p.number].reduced += 1\n                                            Productions[oldp.number].reduced -= 1\n                                        else:\n                                            chosenp,rejectp = oldp,pp\n                                        self.rr_conflicts.append((st,chosenp,rejectp))\n                                        log.info(""  ! reduce/reduce conflict for %s resolved using rule %d (%s)"", a,st_actionp[a].number, st_actionp[a])\n                                    else:\n                                        raise LALRError(""Unknown conflict in state %d"" % st)\n                                else:\n                                    st_action[a] = -p.number\n                                    st_actionp[a] = p\n                                    Productions[p.number].reduced += 1\n                    else:\n                        i = p.lr_index\n                        a = p.prod[i+1]       # Get symbol right after the "".""\n                        if a in self.grammar.Terminals:\n                            g = self.lr0_goto(I,a)\n                            j = self.lr0_cidhash.get(id(g),-1)\n                            if j >= 0:\n                                # We are in a shift state\n                                actlist.append((a,p,""shift and go to state %d"" % j))\n                                r = st_action.get(a,None)\n                                if r is not None:\n                                    # Whoa have a shift/reduce or shift/shift conflict\n                                    if r > 0:\n                                        if r != j:\n                                            raise LALRError(""Shift/shift conflict in state %d"" % st)\n                                    elif r < 0:\n                                        # Do a precedence check.\n                                        #   -  if precedence of reduce rule is higher, we reduce.\n                                        #   -  if precedence of reduce is same and left assoc, we reduce.\n                                        #   -  otherwise we shift\n                                        rprec,rlevel = Productions[st_actionp[a].number].prec\n                                        sprec,slevel = Precedence.get(a,(\'right\',0))\n                                        if (slevel > rlevel) or ((slevel == rlevel) and (rprec == \'right\')):\n                                            # We decide to shift here... highest precedence to shift\n                                            Productions[st_actionp[a].number].reduced -= 1\n                                            st_action[a] = j\n                                            st_actionp[a] = p\n                                            if not rlevel:\n                                                log.info(""  ! shift/reduce conflict for %s resolved as shift"",a)\n                                                self.sr_conflicts.append((st,a,\'shift\'))\n                                        elif (slevel == rlevel) and (rprec == \'nonassoc\'):\n                                            st_action[a] = None\n                                        else:\n                                            # Hmmm. Guess we\'ll keep the reduce\n                                            if not slevel and not rlevel:\n                                                log.info(""  ! shift/reduce conflict for %s resolved as reduce"",a)\n                                                self.sr_conflicts.append((st,a,\'reduce\'))\n\n                                    else:\n                                        raise LALRError(""Unknown conflict in state %d"" % st)\n                                else:\n                                    st_action[a] = j\n                                    st_actionp[a] = p\n\n            # Print the actions associated with each terminal\n            _actprint = { }\n            for a,p,m in actlist:\n                if a in st_action:\n                    if p is st_actionp[a]:\n                        log.info(""    %-15s %s"",a,m)\n                        _actprint[(a,m)] = 1\n            log.info("""")\n            # Print the actions that were not used. (debugging)\n            not_used = 0\n            for a,p,m in actlist:\n                if a in st_action:\n                    if p is not st_actionp[a]:\n                        if not (a,m) in _actprint:\n                            log.debug(""  ! %-15s [ %s ]"",a,m)\n                            not_used = 1\n                            _actprint[(a,m)] = 1\n            if not_used:\n                log.debug("""")\n\n            # Construct the goto table for this state\n\n            nkeys = { }\n            for ii in I:\n                for s in ii.usyms:\n                    if s in self.grammar.Nonterminals:\n                        nkeys[s] = None\n            for n in nkeys:\n                g = self.lr0_goto(I,n)\n                j = self.lr0_cidhash.get(id(g),-1)\n                if j >= 0:\n                    st_goto[n] = j\n                    log.info(""    %-30s shift and go to state %d"",n,j)\n\n            action[st] = st_action\n            actionp[st] = st_actionp\n            goto[st] = st_goto\n            st += 1\n\n\n    # -----------------------------------------------------------------------------\n    # write()\n    #\n    # This function writes the LR parsing tables to a file\n    # -----------------------------------------------------------------------------\n\n    def write_table(self,modulename,outputdir=\'\',signature=""""):\n        basemodulename = modulename.split(""."")[-1]\n        filename = os.path.join(outputdir,basemodulename) + "".py""\n        try:\n            f = open(filename,""w"")\n\n            f.write(""""""\n# %s\n# This file is automatically generated. Do not edit.\n_tabversion = %r\n\n_lr_method = %r\n\n_lr_signature = %r\n    """""" % (filename, __tabversion__, self.lr_method, signature))\n\n            # Change smaller to 0 to go back to original tables\n            smaller = 1\n\n            # Factor out names to try and make smaller\n            if smaller:\n                items = { }\n\n                for s,nd in self.lr_action.items():\n                   for name,v in nd.items():\n                      i = items.get(name)\n                      if not i:\n                         i = ([],[])\n                         items[name] = i\n                      i[0].append(s)\n                      i[1].append(v)\n\n                f.write(""\\n_lr_action_items = {"")\n                for k,v in items.items():\n                    f.write(""%r:(["" % k)\n                    for i in v[0]:\n                        f.write(""%r,"" % i)\n                    f.write(""],["")\n                    for i in v[1]:\n                        f.write(""%r,"" % i)\n\n                    f.write(""]),"")\n                f.write(""}\\n"")\n\n                f.write(""""""\n_lr_action = { }\nfor _k, _v in _lr_action_items.items():\n   for _x,_y in zip(_v[0],_v[1]):\n      if not _x in _lr_action:  _lr_action[_x] = { }\n      _lr_action[_x][_k] = _y\ndel _lr_action_items\n"""""")\n\n            else:\n                f.write(""\\n_lr_action = { "");\n                for k,v in self.lr_action.items():\n                    f.write(""(%r,%r):%r,"" % (k[0],k[1],v))\n                f.write(""}\\n"");\n\n            if smaller:\n                # Factor out names to try and make smaller\n                items = { }\n\n                for s,nd in self.lr_goto.items():\n                   for name,v in nd.items():\n                      i = items.get(name)\n                      if not i:\n                         i = ([],[])\n                         items[name] = i\n                      i[0].append(s)\n                      i[1].append(v)\n\n                f.write(""\\n_lr_goto_items = {"")\n                for k,v in items.items():\n                    f.write(""%r:(["" % k)\n                    for i in v[0]:\n                        f.write(""%r,"" % i)\n                    f.write(""],["")\n                    for i in v[1]:\n                        f.write(""%r,"" % i)\n\n                    f.write(""]),"")\n                f.write(""}\\n"")\n\n                f.write(""""""\n_lr_goto = { }\nfor _k, _v in _lr_goto_items.items():\n   for _x,_y in zip(_v[0],_v[1]):\n       if not _x in _lr_goto: _lr_goto[_x] = { }\n       _lr_goto[_x][_k] = _y\ndel _lr_goto_items\n"""""")\n            else:\n                f.write(""\\n_lr_goto = { "");\n                for k,v in self.lr_goto.items():\n                    f.write(""(%r,%r):%r,"" % (k[0],k[1],v))\n                f.write(""}\\n"");\n\n            # Write production table\n            f.write(""_lr_productions = [\\n"")\n            for p in self.lr_productions:\n                if p.func:\n                    f.write(""  (%r,%r,%d,%r,%r,%d),\\n"" % (p.str,p.name, p.len, p.func,p.file,p.line))\n                else:\n                    f.write(""  (%r,%r,%d,None,None,None),\\n"" % (str(p),p.name, p.len))\n            f.write(""]\\n"")\n            f.close()\n\n        except IOError:\n            e = sys.exc_info()[1]\n            sys.stderr.write(""Unable to create \'%s\'\\n"" % filename)\n            sys.stderr.write(str(e)+""\\n"")\n            return\n\n\n    # -----------------------------------------------------------------------------\n    # pickle_table()\n    #\n    # This function pickles the LR parsing tables to a supplied file object\n    # -----------------------------------------------------------------------------\n\n    def pickle_table(self,filename,signature=""""):\n        try:\n            import cPickle as pickle\n        except ImportError:\n            import pickle\n        outf = open(filename,""wb"")\n        pickle.dump(__tabversion__,outf,pickle_protocol)\n        pickle.dump(self.lr_method,outf,pickle_protocol)\n        pickle.dump(signature,outf,pickle_protocol)\n        pickle.dump(self.lr_action,outf,pickle_protocol)\n        pickle.dump(self.lr_goto,outf,pickle_protocol)\n\n        outp = []\n        for p in self.lr_productions:\n            if p.func:\n                outp.append((p.str,p.name, p.len, p.func,p.file,p.line))\n            else:\n                outp.append((str(p),p.name,p.len,None,None,None))\n        pickle.dump(outp,outf,pickle_protocol)\n        outf.close()\n\n# -----------------------------------------------------------------------------\n#                            === INTROSPECTION ===\n#\n# The following functions and classes are used to implement the PLY\n# introspection features followed by the yacc() function itself.\n# -----------------------------------------------------------------------------\n\n# -----------------------------------------------------------------------------\n# get_caller_module_dict()\n#\n# This function returns a dictionary containing all of the symbols defined within\n# a caller further down the call stack.  This is used to get the environment\n# associated with the yacc() call if none was provided.\n# -----------------------------------------------------------------------------\n\ndef get_caller_module_dict(levels):\n    try:\n        raise RuntimeError\n    except RuntimeError:\n        e,b,t = sys.exc_info()\n        f = t.tb_frame\n        while levels > 0:\n            f = f.f_back                   \n            levels -= 1\n        ldict = f.f_globals.copy()\n        if f.f_globals != f.f_locals:\n            ldict.update(f.f_locals)\n\n        return ldict\n\n# -----------------------------------------------------------------------------\n# parse_grammar()\n#\n# This takes a raw grammar rule string and parses it into production data\n# -----------------------------------------------------------------------------\ndef parse_grammar(doc,file,line):\n    grammar = []\n    # Split the doc string into lines\n    pstrings = doc.splitlines()\n    lastp = None\n    dline = line\n    for ps in pstrings:\n        dline += 1\n        p = ps.split()\n        if not p: continue\n        try:\n            if p[0] == \'|\':\n                # This is a continuation of a previous rule\n                if not lastp:\n                    raise SyntaxError(""%s:%d: Misplaced \'|\'"" % (file,dline))\n                prodname = lastp\n                syms = p[1:]\n            else:\n                prodname = p[0]\n                lastp = prodname\n                syms   = p[2:]\n                assign = p[1]\n                if assign != \':\' and assign != \'::=\':\n                    raise SyntaxError(""%s:%d: Syntax error. Expected \':\'"" % (file,dline))\n\n            grammar.append((file,dline,prodname,syms))\n        except SyntaxError:\n            raise\n        except Exception:\n            raise SyntaxError(""%s:%d: Syntax error in rule \'%s\'"" % (file,dline,ps.strip()))\n\n    return grammar\n\n# -----------------------------------------------------------------------------\n# ParserReflect()\n#\n# This class represents information extracted for building a parser including\n# start symbol, error function, tokens, precedence list, action functions,\n# etc.\n# -----------------------------------------------------------------------------\nclass ParserReflect(object):\n    def __init__(self,pdict,log=None):\n        self.pdict      = pdict\n        self.start      = None\n        self.error_func = None\n        self.tokens     = None\n        self.files      = {}\n        self.grammar    = []\n        self.error      = 0\n\n        if log is None:\n            self.log = PlyLogger(sys.stderr)\n        else:\n            self.log = log\n\n    # Get all of the basic information\n    def get_all(self):\n        self.get_start()\n        self.get_error_func()\n        self.get_tokens()\n        self.get_precedence()\n        self.get_pfunctions()\n        \n    # Validate all of the information\n    def validate_all(self):\n        self.validate_start()\n        self.validate_error_func()\n        self.validate_tokens()\n        self.validate_precedence()\n        self.validate_pfunctions()\n        self.validate_files()\n        return self.error\n\n    # Compute a signature over the grammar\n    def signature(self):\n        try:\n            from hashlib import md5\n        except ImportError:\n            from md5 import md5\n        try:\n            sig = md5()\n            if self.start:\n                sig.update(self.start.encode(\'latin-1\'))\n            if self.prec:\n                sig.update("""".join(["""".join(p) for p in self.prec]).encode(\'latin-1\'))\n            if self.tokens:\n                sig.update("" "".join(self.tokens).encode(\'latin-1\'))\n            for f in self.pfuncs:\n                if f[3]:\n                    sig.update(f[3].encode(\'latin-1\'))\n        except (TypeError,ValueError):\n            pass\n        return sig.digest()\n\n    # -----------------------------------------------------------------------------\n    # validate_file()\n    #\n    # This method checks to see if there are duplicated p_rulename() functions\n    # in the parser module file.  Without this function, it is really easy for\n    # users to make mistakes by cutting and pasting code fragments (and it\'s a real\n    # bugger to try and figure out why the resulting parser doesn\'t work).  Therefore,\n    # we just do a little regular expression pattern matching of def statements\n    # to try and detect duplicates.\n    # -----------------------------------------------------------------------------\n\n    def validate_files(self):\n        # Match def p_funcname(\n        fre = re.compile(r\'\\s*def\\s+(p_[a-zA-Z_0-9]*)\\(\')\n\n        for filename in self.files.keys():\n            base,ext = os.path.splitext(filename)\n            if ext != \'.py\': return 1          # No idea. Assume it\'s okay.\n\n            try:\n                f = open(filename)\n                lines = f.readlines()\n                f.close()\n            except IOError:\n                continue\n\n            counthash = { }\n            for linen,l in enumerate(lines):\n                linen += 1\n                m = fre.match(l)\n                if m:\n                    name = m.group(1)\n                    prev = counthash.get(name)\n                    if not prev:\n                        counthash[name] = linen\n                    else:\n                        self.log.warning(""%s:%d: Function %s redefined. Previously defined on line %d"", filename,linen,name,prev)\n\n    # Get the start symbol\n    def get_start(self):\n        self.start = self.pdict.get(\'start\')\n\n    # Validate the start symbol\n    def validate_start(self):\n        if self.start is not None:\n            if not isinstance(self.start,str):\n                self.log.error(""\'start\' must be a string"")\n\n    # Look for error handler\n    def get_error_func(self):\n        self.error_func = self.pdict.get(\'p_error\')\n\n    # Validate the error function\n    def validate_error_func(self):\n        if self.error_func:\n            if isinstance(self.error_func,types.FunctionType):\n                ismethod = 0\n            elif isinstance(self.error_func, types.MethodType):\n                ismethod = 1\n            else:\n                self.log.error(""\'p_error\' defined, but is not a function or method"")\n                self.error = 1\n                return\n\n            eline = func_code(self.error_func).co_firstlineno\n            efile = func_code(self.error_func).co_filename\n            self.files[efile] = 1\n\n            if (func_code(self.error_func).co_argcount != 1+ismethod):\n                self.log.error(""%s:%d: p_error() requires 1 argument"",efile,eline)\n                self.error = 1\n\n    # Get the tokens map\n    def get_tokens(self):\n        tokens = self.pdict.get(""tokens"",None)\n        if not tokens:\n            self.log.error(""No token list is defined"")\n            self.error = 1\n            return\n\n        if not isinstance(tokens,(list, tuple)):\n            self.log.error(""tokens must be a list or tuple"")\n            self.error = 1\n            return\n        \n        if not tokens:\n            self.log.error(""tokens is empty"")\n            self.error = 1\n            return\n\n        self.tokens = tokens\n\n    # Validate the tokens\n    def validate_tokens(self):\n        # Validate the tokens.\n        if \'error\' in self.tokens:\n            self.log.error(""Illegal token name \'error\'. Is a reserved word"")\n            self.error = 1\n            return\n\n        terminals = {}\n        for n in self.tokens:\n            if n in terminals:\n                self.log.warning(""Token \'%s\' multiply defined"", n)\n            terminals[n] = 1\n\n    # Get the precedence map (if any)\n    def get_precedence(self):\n        self.prec = self.pdict.get(""precedence"",None)\n\n    # Validate and parse the precedence map\n    def validate_precedence(self):\n        preclist = []\n        if self.prec:\n            if not isinstance(self.prec,(list,tuple)):\n                self.log.error(""precedence must be a list or tuple"")\n                self.error = 1\n                return\n            for level,p in enumerate(self.prec):\n                if not isinstance(p,(list,tuple)):\n                    self.log.error(""Bad precedence table"")\n                    self.error = 1\n                    return\n\n                if len(p) < 2:\n                    self.log.error(""Malformed precedence entry %s. Must be (assoc, term, ..., term)"",p)\n                    self.error = 1\n                    return\n                assoc = p[0]\n                if not isinstance(assoc,str):\n                    self.log.error(""precedence associativity must be a string"")\n                    self.error = 1\n                    return\n                for term in p[1:]:\n                    if not isinstance(term,str):\n                        self.log.error(""precedence items must be strings"")\n                        self.error = 1\n                        return\n                    preclist.append((term,assoc,level+1))\n        self.preclist = preclist\n\n    # Get all p_functions from the grammar\n    def get_pfunctions(self):\n        p_functions = []\n        for name, item in self.pdict.items():\n            if name[:2] != \'p_\': continue\n            if name == \'p_error\': continue\n            if isinstance(item,(types.FunctionType,types.MethodType)):\n                line = func_code(item).co_firstlineno\n                file = func_code(item).co_filename\n                p_functions.append((line,file,name,item.__doc__))\n\n        # Sort all of the actions by line number\n        p_functions.sort()\n        self.pfuncs = p_functions\n\n\n    # Validate all of the p_functions\n    def validate_pfunctions(self):\n        grammar = []\n        # Check for non-empty symbols\n        if len(self.pfuncs) == 0:\n            self.log.error(""no rules of the form p_rulename are defined"")\n            self.error = 1\n            return \n        \n        for line, file, name, doc in self.pfuncs:\n            func = self.pdict[name]\n            if isinstance(func, types.MethodType):\n                reqargs = 2\n            else:\n                reqargs = 1\n            if func_code(func).co_argcount > reqargs:\n                self.log.error(""%s:%d: Rule \'%s\' has too many arguments"",file,line,func.__name__)\n                self.error = 1\n            elif func_code(func).co_argcount < reqargs:\n                self.log.error(""%s:%d: Rule \'%s\' requires an argument"",file,line,func.__name__)\n                self.error = 1\n            elif not func.__doc__:\n                self.log.warning(""%s:%d: No documentation string specified in function \'%s\' (ignored)"",file,line,func.__name__)\n            else:\n                try:\n                    parsed_g = parse_grammar(doc,file,line)\n                    for g in parsed_g:\n                        grammar.append((name, g))\n                except SyntaxError:\n                    e = sys.exc_info()[1]\n                    self.log.error(str(e))\n                    self.error = 1\n\n                # Looks like a valid grammar rule\n                # Mark the file in which defined.\n                self.files[file] = 1\n\n        # Secondary validation step that looks for p_ definitions that are not functions\n        # or functions that look like they might be grammar rules.\n\n        for n,v in self.pdict.items():\n            if n[0:2] == \'p_\' and isinstance(v, (types.FunctionType, types.MethodType)): continue\n            if n[0:2] == \'t_\': continue\n            if n[0:2] == \'p_\' and n != \'p_error\':\n                self.log.warning(""\'%s\' not defined as a function"", n)\n            if ((isinstance(v,types.FunctionType) and func_code(v).co_argcount == 1) or\n                (isinstance(v,types.MethodType) and func_code(v).co_argcount == 2)):\n                try:\n                    doc = v.__doc__.split("" "")\n                    if doc[1] == \':\':\n                        self.log.warning(""%s:%d: Possible grammar rule \'%s\' defined without p_ prefix"",\n                                         func_code(v).co_filename, func_code(v).co_firstlineno,n)\n                except Exception:\n                    pass\n\n        self.grammar = grammar\n\n# -----------------------------------------------------------------------------\n# yacc(module)\n#\n# Build a parser\n# -----------------------------------------------------------------------------\n\ndef yacc(method=\'LALR\', debug=yaccdebug, module=None, tabmodule=tab_module, start=None, \n         check_recursion=1, optimize=0, write_tables=1, debugfile=debug_file,outputdir=\'\',\n         debuglog=None, errorlog = None, picklefile=None):\n    # If pickling is enabled, table files are not created\n    if picklefile:\n        write_tables = 0\n\n    if errorlog is None:\n        errorlog = PlyLogger(sys.stderr)\n\n    # Get the module dictionary used for the parser\n    if module:\n        _items = [(k,getattr(module,k)) for k in dir(module)]\n        pdict = dict(_items)\n    else:\n        pdict = get_caller_module_dict(2)\n\n    # Collect parser information from the dictionary\n    pinfo = ParserReflect(pdict,log=errorlog)\n    pinfo.get_all()\n\n    if pinfo.error:\n        raise YaccError(""Unable to build parser"")\n\n    # Check signature against table files (if any)\n    signature = pinfo.signature()\n\n    # Read the tables\n    try:\n        lr = LRTable()\n        if picklefile:\n            read_signature = lr.read_pickle(picklefile)\n        else:\n            read_signature = lr.read_table(tabmodule)\n        if optimize or (read_signature == signature):\n            try:\n                lr.bind_callables(pinfo.pdict)\n                parser = LRParser(lr,pinfo.error_func)\n                return parser\n            except Exception:\n                e = sys.exc_info()[1]\n                errorlog.warning(""There was a problem loading the table file: %s"", repr(e))\n    except VersionError:\n        e = sys.exc_info()\n        errorlog.warning(str(e))\n    except Exception:\n        pass\n\n    if debuglog is None:\n        if debug:\n            debuglog = PlyLogger(open(debugfile,""w""))\n        else:\n            debuglog = NullLogger()\n\n    debuglog.info(""Created by PLY version %s (http://www.dabeaz.com/ply)"", __version__)\n\n\n    errors = 0\n\n    # Validate the parser information\n    if pinfo.validate_all():\n        raise YaccError(""Unable to build parser"")\n    \n    if not pinfo.error_func:\n        errorlog.warning(""no p_error() function is defined"")\n\n    # Create a grammar object\n    grammar = Grammar(pinfo.tokens)\n\n    # Set precedence level for terminals\n    for term, assoc, level in pinfo.preclist:\n        try:\n            grammar.set_precedence(term,assoc,level)\n        except GrammarError:\n            e = sys.exc_info()[1]\n            errorlog.warning(""%s"",str(e))\n\n    # Add productions to the grammar\n    for funcname, gram in pinfo.grammar:\n        file, line, prodname, syms = gram\n        try:\n            grammar.add_production(prodname,syms,funcname,file,line)\n        except GrammarError:\n            e = sys.exc_info()[1]\n            errorlog.error(""%s"",str(e))\n            errors = 1\n\n    # Set the grammar start symbols\n    try:\n        if start is None:\n            grammar.set_start(pinfo.start)\n        else:\n            grammar.set_start(start)\n    except GrammarError:\n        e = sys.exc_info()[1]\n        errorlog.error(str(e))\n        errors = 1\n\n    if errors:\n        raise YaccError(""Unable to build parser"")\n\n    # Verify the grammar structure\n    undefined_symbols = grammar.undefined_symbols()\n    for sym, prod in undefined_symbols:\n        errorlog.error(""%s:%d: Symbol \'%s\' used, but not defined as a token or a rule"",prod.file,prod.line,sym)\n        errors = 1\n\n    unused_terminals = grammar.unused_terminals()\n    if unused_terminals:\n        debuglog.info("""")\n        debuglog.info(""Unused terminals:"")\n        debuglog.info("""")\n        for term in unused_terminals:\n            errorlog.warning(""Token \'%s\' defined, but not used"", term)\n            debuglog.info(""    %s"", term)\n\n    # Print out all productions to the debug log\n    if debug:\n        debuglog.info("""")\n        debuglog.info(""Grammar"")\n        debuglog.info("""")\n        for n,p in enumerate(grammar.Productions):\n            debuglog.info(""Rule %-5d %s"", n, p)\n\n    # Find unused non-terminals\n    unused_rules = grammar.unused_rules()\n    for prod in unused_rules:\n        errorlog.warning(""%s:%d: Rule \'%s\' defined, but not used"", prod.file, prod.line, prod.name)\n\n    if len(unused_terminals) == 1:\n        errorlog.warning(""There is 1 unused token"")\n    if len(unused_terminals) > 1:\n        errorlog.warning(""There are %d unused tokens"", len(unused_terminals))\n\n    if len(unused_rules) == 1:\n        errorlog.warning(""There is 1 unused rule"")\n    if len(unused_rules) > 1:\n        errorlog.warning(""There are %d unused rules"", len(unused_rules))\n\n    if debug:\n        debuglog.info("""")\n        debuglog.info(""Terminals, with rules where they appear"")\n        debuglog.info("""")\n        terms = list(grammar.Terminals)\n        terms.sort()\n        for term in terms:\n            debuglog.info(""%-20s : %s"", term, "" "".join([str(s) for s in grammar.Terminals[term]]))\n        \n        debuglog.info("""")\n        debuglog.info(""Nonterminals, with rules where they appear"")\n        debuglog.info("""")\n        nonterms = list(grammar.Nonterminals)\n        nonterms.sort()\n        for nonterm in nonterms:\n            debuglog.info(""%-20s : %s"", nonterm, "" "".join([str(s) for s in grammar.Nonterminals[nonterm]]))\n        debuglog.info("""")\n\n    if check_recursion:\n        unreachable = grammar.find_unreachable()\n        for u in unreachable:\n            errorlog.warning(""Symbol \'%s\' is unreachable"",u)\n\n        infinite = grammar.infinite_cycles()\n        for inf in infinite:\n            errorlog.error(""Infinite recursion detected for symbol \'%s\'"", inf)\n            errors = 1\n        \n    unused_prec = grammar.unused_precedence()\n    for term, assoc in unused_prec:\n        errorlog.error(""Precedence rule \'%s\' defined for unknown symbol \'%s\'"", assoc, term)\n        errors = 1\n\n    if errors:\n        raise YaccError(""Unable to build parser"")\n    \n    # Run the LRGeneratedTable on the grammar\n    if debug:\n        errorlog.debug(""Generating %s tables"", method)\n            \n    lr = LRGeneratedTable(grammar,method,debuglog)\n\n    if debug:\n        num_sr = len(lr.sr_conflicts)\n\n        # Report shift/reduce and reduce/reduce conflicts\n        if num_sr == 1:\n            errorlog.warning(""1 shift/reduce conflict"")\n        elif num_sr > 1:\n            errorlog.warning(""%d shift/reduce conflicts"", num_sr)\n\n        num_rr = len(lr.rr_conflicts)\n        if num_rr == 1:\n            errorlog.warning(""1 reduce/reduce conflict"")\n        elif num_rr > 1:\n            errorlog.warning(""%d reduce/reduce conflicts"", num_rr)\n\n    # Write out conflicts to the output file\n    if debug and (lr.sr_conflicts or lr.rr_conflicts):\n        debuglog.warning("""")\n        debuglog.warning(""Conflicts:"")\n        debuglog.warning("""")\n\n        for state, tok, resolution in lr.sr_conflicts:\n            debuglog.warning(""shift/reduce conflict for %s in state %d resolved as %s"",  tok, state, resolution)\n        \n        already_reported = {}\n        for state, rule, rejected in lr.rr_conflicts:\n            if (state,id(rule),id(rejected)) in already_reported:\n                continue\n            debuglog.warning(""reduce/reduce conflict in state %d resolved using rule (%s)"", state, rule)\n            debuglog.warning(""rejected rule (%s) in state %d"", rejected,state)\n            errorlog.warning(""reduce/reduce conflict in state %d resolved using rule (%s)"", state, rule)\n            errorlog.warning(""rejected rule (%s) in state %d"", rejected, state)\n            already_reported[state,id(rule),id(rejected)] = 1\n        \n        warned_never = []\n        for state, rule, rejected in lr.rr_conflicts:\n            if not rejected.reduced and (rejected not in warned_never):\n                debuglog.warning(""Rule (%s) is never reduced"", rejected)\n                errorlog.warning(""Rule (%s) is never reduced"", rejected)\n                warned_never.append(rejected)\n\n    # Write the table file if requested\n    if write_tables:\n        lr.write_table(tabmodule,outputdir,signature)\n\n    # Write a pickled version of the tables\n    if picklefile:\n        lr.pickle_table(picklefile,signature)\n\n    # Build the parser\n    lr.bind_callables(pinfo.pdict)\n    parser = LRParser(lr,pinfo.error_func)\n\n    return parser\n'"
data/__init__.py,0,b''
encoder_decoder/__init__.py,0,b''
encoder_decoder/beam_search.py,61,"b'""""""\nBeam decoder with length normalization\nAdapted from\ncurl -LO \'https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f\'\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom encoder_decoder.graph_utils import nest_map\n\n\nclass BeamDecoder(object):\n    def __init__(self, num_layers, start_token=-1, stop_token=-1, batch_size=1,\n                 beam_size=7, use_attention=False, use_copy=False,\n                 copy_fun=\'copynet\', alpha=1.0, locally_normalized=True):\n        """"""\n        :param num_classes: int. Number of output classes used\n        :param num_layers: int. Number of layers used in the RNN cell.\n        :param start_token: int.\n        :param stop_token: int.\n        :param beam_size: int.\n        :param use_attention: if attention is to be used.\n        :param use_copy: if copy mechnism is to be used.\n        :param alpha: parameter used for length normalization.\n        :param locally_normalized: set to true if local normalization is to be\n            performed at each search step.\n        """"""\n        self.num_layers = num_layers\n        self.start_token = start_token\n        self.stop_token = stop_token\n        self.batch_size = batch_size\n        self.beam_size = beam_size\n        self.use_attention = use_attention\n        self.use_copy = use_copy\n        self.copy_fun = copy_fun\n        self.alpha = alpha\n        self.locally_normalized = locally_normalized\n        print(""creating beam search decoder: alpha = {}"".format(self.alpha))\n\n    @classmethod\n    def _tile_along_beam(cls, beam_size, state):\n        if nest.is_sequence(state):\n            return nest_map(\n                lambda val: cls._tile_along_beam(beam_size, val),\n                state\n            )\n\n        if not isinstance(state, tf.Tensor):\n            raise ValueError(""State should be a sequence or tensor"")\n\n        tensor = state\n\n        tensor_shape = tensor.get_shape().with_rank_at_least(1)\n\n        try:\n            new_first_dim = tensor_shape[0] * beam_size\n        except:\n            new_first_dim = None\n\n        dynamic_tensor_shape = tf.unstack(tf.shape(input=tensor))\n        res = tf.expand_dims(tensor, 1)\n        res = tf.tile(res, [1, beam_size] + [1] * (tensor_shape.ndims-1))\n        res = tf.reshape(res, [-1] + list(dynamic_tensor_shape[1:]))\n        res.set_shape([new_first_dim] + list(tensor_shape[1:]))\n        return res\n\n    def wrap_cell(self, cell, output_project):\n        """"""\n        Wraps a cell for use with the beam decoder\n        """"""\n        return BeamDecoderCellWrapper(cell, output_project, self.num_layers,\n                                      self.start_token, self.stop_token,\n                                      self.batch_size, self.beam_size,\n                                      self.use_attention, self.use_copy,\n                                      self.copy_fun, self.alpha,\n                                      self.locally_normalized)\n\n    def wrap_state(self, state, output_project):\n        dummy = BeamDecoderCellWrapper(None, output_project, self.num_layers,\n                                       self.start_token, self.stop_token,\n                                       self.batch_size, self.beam_size,\n                                       self.use_attention, self.use_copy,\n                                       self.copy_fun, self.alpha,\n                                       self.locally_normalized)\n        if nest.is_sequence(state):\n            dtype = nest.flatten(state)[0].dtype\n        else:\n            dtype = state.dtype\n        return dummy._create_state(self.batch_size, dtype, cell_state=state)\n\n    def wrap_input(self, input):\n        """"""\n        Wraps an input for use with the beam decoder.\n        Should be used for the initial input at timestep zero, as well as any\n        side-channel inputs that are per-batch (e.g. attention targets)\n        """"""\n        return self._tile_along_beam(self.beam_size, input)\n\n    def unwrap_output_dense(self, final_state, include_stop_tokens=True):\n        """"""\n        Retreive the beam search output from the final state.\n        Returns a [batch_size, max_len]-sized Tensor.\n        """"""\n        res = final_state[0]\n        if include_stop_tokens:\n            res = tf.concat(axis=1, values=[res[:,1:],\n                                tf.ones_like(res[:,0:1]) * self.stop_token])\n        return res\n\n    def unwrap_output_sparse(self, final_state, include_stop_tokens=True):\n        """"""\n        Retreive the beam search output from the final state.\n        Returns a sparse tensor with underlying dimensions of\n        [batch_size, max_len]\n        """"""\n        output_dense = final_state[0]\n        mask = tf.not_equal(output_dense, self.stop_token)\n\n        if include_stop_tokens:\n            output_dense = tf.concat(axis=1, values=[output_dense[:,1:],\n                tf.ones_like(output_dense[:,0:1]) * self.stop_token])\n            mask = tf.concat(axis=1, values=[mask[:,1:],\n                tf.cast(tf.ones_like(mask[:,0:1], dtype=tf.int8), tf.bool)])\n\n        return sparse_boolean_mask(output_dense, mask)\n\n    def unwrap_output_logprobs(self, final_state):\n        """"""\n        Retreive the log-probabilities associated with the selected beams.\n        """"""\n        return final_state[1]\n\n\nclass BeamDecoderCellWrapper(tf.compat.v1.nn.rnn_cell.RNNCell):\n    def __init__(self, cell, output_project, num_layers,\n                 start_token=-1, stop_token=-1, batch_size=1, beam_size=7,\n                 use_attention=False, use_copy=False, copy_fun=\'copynet\',\n                 alpha=1.0, locally_normalized=True):\n        self.cell = cell\n        self.output_project = output_project\n        self.num_layers = num_layers\n        self.start_token = start_token\n        self.stop_token = stop_token\n        self.batch_size = batch_size\n        self.beam_size = beam_size\n        self.use_attention = use_attention\n        self.use_copy = use_copy\n        self.copy_fun = copy_fun\n        self.alpha = alpha\n        self.locally_normalized = locally_normalized\n\n        self.full_size = self.batch_size * self.beam_size\n        self.seq_len = tf.constant(1e-12, shape=[self.full_size], dtype=tf.float32)\n\n    def __call__(self, cell_inputs, state, scope=None):\n        (\n            past_beam_symbols,      # [batch_size*self.beam_size, :], right-aligned!!!\n            past_beam_logprobs,     # [batch_size*self.beam_size]\n            past_cell_states        # LSTM: ([batch_size*self.beam_size, :, dim],\n                                    #        [batch_size*self.beam_size, :, dim])\n                                    # GRU: [batch_size*self.beam_size, :, dim]\n        ) = state\n\n        past_cell_state = self.get_last_cell_state(past_cell_states)\n        if self.use_copy and self.copy_fun == \'copynet\':\n            cell_output, cell_state, alignments, attns = \\\n                self.cell(cell_inputs, past_cell_state, scope)\n        elif self.use_attention:\n            cell_output, cell_state, alignments, attns = \\\n                self.cell(cell_inputs, past_cell_state, scope)\n        else:\n            cell_output, cell_state = \\\n                self.cell(cell_inputs, past_cell_state, scope)\n\n        # [batch_size*beam_size, num_classes]\n        if self.use_copy and self.copy_fun == \'copynet\':\n            logprobs = tf.math.log(cell_output)\n        else:\n            W, b = self.output_project\n            if self.locally_normalized:\n                logprobs = tf.nn.log_softmax(tf.matmul(cell_output, W) + b)\n            else:\n                logprobs = tf.matmul(cell_output, W) + b\n        num_classes = logprobs.get_shape()[1]\n\n        # stop_mask: indicates partial sequences ending with a stop token\n        # [batch_size * beam_size]\n        # x     0\n        # _STOP 1\n        # x     0\n        # x     0\n        input_symbols = past_beam_symbols[:, -1]\n        stop_mask = tf.expand_dims(tf.cast(\n            tf.equal(input_symbols, self.stop_token), tf.float32), 1)\n\n        # done_mask: indicates stop token in the output vocabulary\n        # [1, num_classes]\n        # [- - _STOP - - -]\n        # [0 0 1 0 0 0]\n        done_mask = tf.cast(tf.reshape(tf.equal(tf.range(num_classes),\n                                                self.stop_token),\n                                       [1, num_classes]),\n                            tf.float32)\n        # set the next token distribution of partial sequences ending with\n        # a stop token to:\n        # [- - _STOP - - -]\n        # [-inf -inf 0 -inf -inf -inf]\n        logprobs = tf.add(logprobs, tf.multiply(\n            stop_mask, -1e18 * (tf.ones_like(done_mask) - done_mask)))\n        logprobs = tf.multiply(logprobs, (1 - tf.multiply(stop_mask, done_mask)))\n\n        # length normalization\n        past_logprobs_unormalized = \\\n            tf.multiply(past_beam_logprobs, tf.pow(self.seq_len, self.alpha))\n        logprobs_unormalized = \\\n            tf.expand_dims(past_logprobs_unormalized, 1) + logprobs\n        seq_len = tf.expand_dims(self.seq_len, 1) + (1 - stop_mask)\n        logprobs_batched = tf.compat.v1.div(logprobs_unormalized, tf.pow(seq_len, self.alpha))\n\n        beam_logprobs, indices = tf.nn.top_k(\n            tf.reshape(logprobs_batched, [-1, self.beam_size * num_classes]),\n            self.beam_size\n        )\n        beam_logprobs = tf.reshape(beam_logprobs, [-1])\n\n        # For continuing to the next symbols\n        parent_refs_offsets = \\\n                (tf.range(self.full_size) // self.beam_size) * self.beam_size\n        symbols = indices % num_classes # [batch_size, self.beam_size]\n        parent_refs = tf.reshape(indices // num_classes, [-1]) # [batch_size*self.beam_size]\n        parent_refs = parent_refs + parent_refs_offsets\n\n        beam_symbols = tf.concat(axis=1, values=[tf.gather(past_beam_symbols, parent_refs),\n                                                 tf.reshape(symbols, [-1, 1])])\n        self.seq_len = tf.squeeze(tf.gather(seq_len, parent_refs), axis=[1])\n\n        if self.use_attention:\n            ranked_alignments = nest_map(\n                lambda element: tf.gather(element, parent_refs), alignments)\n            ranked_attns = nest_map(\n                lambda element: tf.gather(element, parent_refs), attns)\n\n        # update cell_states\n        def concat_and_gather_tuple_states(pc_states, c_state):\n            rc_states = (\n                tf.concat(axis=1, values=[pc_states[0], tf.expand_dims(c_state[0], 1)]),\n                tf.concat(axis=1, values=[pc_states[1], tf.expand_dims(c_state[1], 1)])\n            )\n            c_states = (\n                nest_map(lambda element: tf.gather(element, parent_refs), rc_states[0]),\n                nest_map(lambda element: tf.gather(element, parent_refs), rc_states[1])\n            )\n            return c_states\n\n        if nest.is_sequence(cell_state):\n            if self.num_layers > 1:\n                ranked_cell_states = [concat_and_gather_tuple_states(pc_states, c_state)\n                    for pc_states, c_state in zip(past_cell_states, cell_state)]\n            else:\n                ranked_cell_states = concat_and_gather_tuple_states(\n                    past_cell_states, cell_state)\n        else:\n            ranked_cell_states = tf.gather(\n                tf.concat(axis=1, values=[past_cell_states, tf.expand_dims(cell_state, 1)]),\n                parent_refs)\n\n        compound_cell_state = (\n            beam_symbols,\n            beam_logprobs,\n            ranked_cell_states\n        )\n        ranked_cell_output = tf.gather(cell_output, parent_refs)\n\n        if self.use_copy and self.copy_fun == \'copynet\':\n            return ranked_cell_output, compound_cell_state, ranked_alignments, \\\n                   ranked_attns\n        elif self.use_attention:\n            return ranked_cell_output, compound_cell_state, ranked_alignments, \\\n                   ranked_attns\n        else:\n            return ranked_cell_output, compound_cell_state\n\n    def get_last_cell_state(self, past_cell_states):\n        def get_last_tuple_state(pc_states):\n            c_states, h_states = pc_states\n            lc_state = c_states[:, -1, :]\n            lh_state = h_states[:, -1, :]\n            l_state = (lc_state, lh_state)\n            return l_state\n\n        if nest.is_sequence(past_cell_states):\n            if self.num_layers > 1:\n                last_cell_state = [get_last_tuple_state(l)\n                                   for l in past_cell_states]\n            else:\n                last_cell_state = get_last_tuple_state(past_cell_states)\n        else:\n            last_cell_state = past_cell_states[:, -1, :]\n        return last_cell_state\n\n    def _create_state(self, batch_size, dtype, cell_state=None):\n        if cell_state is None:\n            cell_state = self.cell.zero_state(batch_size*self.beam_size, dtype=dtype)\n        else:\n            cell_state = BeamDecoder._tile_along_beam(self.beam_size, cell_state)\n        full_size = batch_size * self.beam_size\n        first_in_beam_mask = tf.equal(tf.range(full_size) % self.beam_size, 0)\n\n        beam_symbols = tf.fill([full_size, 1],\n                               tf.constant(self.start_token, dtype=tf.int32))\n        beam_logprobs = tf.compat.v1.where(\n            first_in_beam_mask,\n            tf.fill([full_size], 0.0),\n            tf.fill([full_size], -1e18), # top_k does not play well with -inf\n                                         # TODO: dtype-dependent value here\n        )\n\n        return (\n            beam_symbols,\n            beam_logprobs,\n            nest_map(lambda element: tf.expand_dims(element, 1), cell_state)\n        )\n\n    def zero_state(self, batch_size_times_beam_size, dtype):\n        """"""\n        Instead of calling this manually, please use\n        BeamDecoder.wrap_state(cell.zero_state(...)) instead\n        """"""\n        batch_size = batch_size_times_beam_size / self.beam_size\n        return self.cell.zero_state(batch_size, dtype)\n\n    @property\n    def output_size(self):\n        return 1\n\n\ndef sparse_boolean_mask(tensor, mask):\n    """"""\n    Creates a sparse tensor from masked elements of `tensor`\n    Inputs:\n      tensor: a 2-D tensor, [batch_size, T]\n      mask: a 2-D mask, [batch_size, T]\n    Output: a 2-D sparse tensor\n    """"""\n    mask_lens = tf.reduce_sum(input_tensor=tf.cast(mask, tf.int32), axis=-1, keepdims=True)\n    mask_shape = tf.shape(input=mask)\n    left_shifted_mask = tf.tile(\n        tf.expand_dims(tf.range(mask_shape[1]), 0),\n        [mask_shape[0], 1]\n    ) < mask_lens\n    return tf.SparseTensor(\n        indices=tf.compat.v1.where(left_shifted_mask),\n        values=tf.boolean_mask(tensor=tensor, mask=mask),\n        shape=tf.cast(tf.stack([mask_shape[0], tf.reduce_max(input_tensor=mask_lens)]),\n                      tf.int64) # For 2D only\n    )\n'"
encoder_decoder/data_utils.py,2,"b'""""""\nLibrary for converting raw data into feature vectors.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport os\nimport pickle\nimport sys\n\nimport numpy as np\nimport scipy.sparse as ssp\nimport tensorflow as tf\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom bashlint import bash, nast, data_tools\nfrom nlp_tools import constants, tokenizer\n\n# Special token symbols\n_PAD = ""__SP__PAD""\n_EOS = ""__SP__EOS""\n_UNK = ""__SP__UNK""\n_ARG_UNK = ""__SP__ARGUMENT_UNK""\n_UTL_UNK = ""__SP__UTILITY_UNK""\n_FLAG_UNK = ""__SP__FLAG_UNK""\n_ARG_START = ""__SP__ARG_START""\n_ARG_END = ""__SP__ARG_END""\n\n_GO = ""__SP__GO""                   # seq2seq start symbol\n_ROOT = ""__SP__ROOT""               # seq2tree start symbol\n\nPAD_ID = 0\nEOS_ID = 1\nUNK_ID = 2\nARG_UNK_ID = 3\nUTL_UNK_ID = 4\nFLAG_UNK_ID = 5\nH_NO_EXPAND_ID = 6\nV_NO_EXPAND_ID = 7\nGO_ID = 8\nROOT_ID = 9\nARG_START_ID = 10                  # start argument sketch\nARG_END_ID = 11                    # end argument sketch\nNUM_ID = 12                        # 1, 2, 3, 4, ...\nNUM_ALPHA_ID = 13                  # 10k, 20k, 50k, 100k, ...\nNON_ENGLISH_ID = 14                # /local/bin, hello.txt, ...\n\nTOKEN_INIT_VOCAB = [\n    _PAD,\n    _EOS,\n    _UNK,\n    _ARG_UNK,\n    _UTL_UNK,\n    _FLAG_UNK,\n    nast._H_NO_EXPAND,\n    nast._V_NO_EXPAND,\n    _GO,\n    _ROOT,\n    _ARG_START,\n    _ARG_END,\n    constants._NUMBER,\n    constants._NUMBER_ALPHA,\n    constants._REGEX\n]\n\n# Special char symbols\n_CPAD = ""__SP__CPAD""\n_CEOS = ""__SP__CEOS""\n_CUNK = ""__SP__CUNK""\n_CATOM = ""__SP__CATOM""\n_CGO = ""__SP__CGO""\n\nCPAD_ID = 0\nCEOS_ID = 1\nCUNK_ID = 2\nCATOM_ID = 3\nCLONG_ID = 4\nCGO_ID = 5\n\nCHAR_INIT_VOCAB = [\n    _CPAD,\n    _CEOS,\n    _CUNK,\n    _CATOM,\n    constants._LONG_TOKEN_IND,\n    _CGO\n]\n\ndata_splits = [\'train\', \'dev\', \'test\']\nTOKEN_SEPARATOR = \'<TOKEN_SEPARATOR>\'\n\n\nclass DataSet(object):\n    def __init__(self):\n        self.data_points = []\n        self.max_sc_length = -1\n        self.max_tg_length = -1\n        self.buckets = None\n\n\nclass DataPoint(object):\n    def __init__(self):\n        self.sc_txt = None\n        self.tg_txt = None\n        self.sc_ids = None\n        self.tg_ids = None\n        self.csc_ids = None         # CopyNet training source ids\n        self.ctg_ids = None         # CopyNet training target ids\n        self.alignments = None\n        self.sc_fillers = None      # TODO: this field is no longer used\n\n\nclass Vocab(object):\n    def __init__(self):\n        self.sc_vocab = None\n        self.tg_vocab = None\n        self.rev_sc_vocab = None\n        self.rev_tg_vocab = None\n        self.max_sc_token_size = -1\n        self.max_tg_token_size = -1\n\n\n# --- Data IO --- #\n\ndef load_data(FLAGS, use_buckets=True, load_features=True):\n    print(""Loading data from %s"" % FLAGS.data_dir)\n\n    source, target = (\'nl\', \'cm\') if not FLAGS.explain else (\'cm\', \'nl\')\n\n    train_set = read_data(FLAGS, \'train\', source, target, load_features=load_features,\n                          use_buckets=use_buckets, add_start_token=True, add_end_token=True)\n    dev_set = read_data(FLAGS, \'dev\', source, target, load_features=load_features,\n                        use_buckets=use_buckets, buckets=train_set.buckets,\n                        add_start_token=True, add_end_token=True)\n    test_set = read_data(FLAGS, \'test\', source, target, load_features=load_features,\n                         use_buckets=use_buckets, buckets=train_set.buckets,\n                         add_start_token=True, add_end_token=True)\n\n    return train_set, dev_set, test_set\n\n\ndef read_data(FLAGS, split, source, target, use_buckets=True, buckets=None,\n              add_start_token=False, add_end_token=False, load_features=True):\n\n    def get_data_file_path(data_dir, split, lang, channel):\n        return os.path.join(data_dir, \'{}.{}.{}\'.format(split, lang, channel))\n\n    def get_source_ids(s):\n        source_ids = []\n        for token in s.split(TOKEN_SEPARATOR):\n            if token in vocab.sc_vocab:\n                source_ids.append(vocab.sc_vocab[token])\n            else:\n                source_ids.append(UNK_ID)\n        return source_ids\n\n    def get_target_input_ids(s):\n        target_ids = []\n        for token in s.split(TOKEN_SEPARATOR):\n            if token in vocab.tg_vocab:\n                target_ids.append(vocab.tg_vocab[token])\n            else:\n                target_ids.append(UNK_ID)\n        if add_start_token:\n            target_ids.insert(0, ROOT_ID)\n        if add_end_token:\n            target_ids.append(EOS_ID)\n        return target_ids\n\n    if load_features:\n        vocab = load_vocabulary(FLAGS)\n\n    data_dir = FLAGS.data_dir\n    sc_path = get_data_file_path(data_dir, split, source, \'filtered\')\n    tg_path = get_data_file_path(data_dir, split, target, \'filtered\')\n    print(""source file: {}"".format(sc_path))\n    print(""target file: {}"".format(tg_path))\n    sc_file = open(sc_path, encoding=\'utf-8\')\n    tg_file = open(tg_path, encoding=\'utf-8\')\n\n    if load_features:\n        token_ext = \'normalized.{}\'.format(FLAGS.channel) \\\n            if FLAGS.normalized else FLAGS.channel\n        sc_token_path = get_data_file_path(data_dir, split, source, token_ext)\n        tg_token_path = get_data_file_path(data_dir, split, target, token_ext)\n        print(""source tokenized sequence file: {}"".format(sc_token_path))\n        print(""target tokenized sequence file: {}"".format(tg_token_path))\n        sc_token_file = open(sc_token_path, encoding=\'utf-8\')\n        tg_token_file = open(tg_token_path, encoding=\'utf-8\')\n        with open(os.path.join(data_dir, \'{}.{}.align\'.format(split, FLAGS.channel)), \'rb\') as f:\n            alignments = pickle.load(f)\n\n    dataset = []\n    num_data = 0\n    max_sc_length = 0\n    max_tg_length = 0\n\n    for i, sc_txt in enumerate(sc_file.readlines()):\n        data_point = DataPoint()\n        data_point.sc_txt = sc_txt.strip()\n        data_point.tg_txt = tg_file.readline().strip()\n        if load_features:\n            data_point.sc_ids = \\\n                get_source_ids(sc_token_file.readline().strip())\n            if len(data_point.sc_ids) > max_sc_length:\n                max_sc_length = len(data_point.sc_ids)\n            data_point.tg_ids = \\\n                get_target_input_ids(tg_token_file.readline().strip())\n            data_point.alignments = alignments[i]\n            if len(data_point.tg_ids) > max_tg_length:\n                max_tg_length = len(data_point.tg_ids)\n        dataset.append(data_point)\n        num_data += 1\n    data_size = len(dataset)\n    sc_file.close()\n    tg_file.close()\n    if load_features:\n        sc_token_file.close()\n        tg_token_file.close()\n\n    print(\'{} data points read.\'.format(num_data))\n    if load_features:\n        print(\'max_source_length = {}\'.format(max_sc_length))\n        print(\'max_target_length = {}\'.format(max_tg_length))\n\n        if FLAGS.use_copy and FLAGS.copy_fun == \'copynet\':\n            copy_token_ext = \'copy.{}\'.format(token_ext)\n            sc_copy_token_path = get_data_file_path(data_dir, split, source,\n                                                    copy_token_ext)\n            tg_copy_token_path = get_data_file_path(data_dir, split, target,\n                                                    copy_token_ext)\n            sc_token_file = open(sc_token_path, encoding=\'utf-8\')\n            tg_token_file = open(tg_token_path, encoding=\'utf-8\')\n            sc_copy_token_file = open(sc_copy_token_path, encoding=\'utf-8\')\n            tg_copy_token_file = open(tg_copy_token_path, encoding=\'utf-8\')\n            for i, data_point in enumerate(dataset):\n                sc_tokens = sc_token_file.readline().strip().split(TOKEN_SEPARATOR)\n                tg_tokens = tg_token_file.readline().strip().split(TOKEN_SEPARATOR)\n                sc_copy_tokens = \\\n                    sc_copy_token_file.readline().strip().split(TOKEN_SEPARATOR)\n                tg_copy_tokens = \\\n                    tg_copy_token_file.readline().strip().split(TOKEN_SEPARATOR)\n                data_point.csc_ids, data_point.ctg_ids = \\\n                    compute_copy_indices(sc_tokens, tg_tokens,\n                        sc_copy_tokens, tg_copy_tokens, vocab.tg_vocab, token_ext)\n            sc_token_file.close()\n            tg_token_file.close()\n            sc_copy_token_file.close()\n            tg_copy_token_file.close()\n\n    if load_features and use_buckets:\n        print(\'Group data points into buckets...\')\n        if split == \'train\':\n            # Determine bucket sizes based on the characteristics of the dataset\n            num_buckets = FLAGS.num_buckets\n            bucket_capacity = int(len(dataset) / num_buckets)\n            assert(bucket_capacity > 0)\n            # Excluding outliers (very long sequences)\n            length_cutoff = 0.01\n            # A. Determine maximum source length\n            sorted_dataset = sorted(dataset, key=lambda x:len(x.sc_ids), reverse=True)\n            max_sc_length = len(sorted_dataset[int(len(sorted_dataset) * length_cutoff)].sc_ids)\n            # B. Determine maximum target length\n            sorted_dataset = sorted(dataset, key=lambda x:len(x.tg_ids), reverse=True)\n            max_tg_length = len(sorted_dataset[int(len(sorted_dataset) * length_cutoff)].tg_ids)\n            print(\'max_source_length after filtering = {}\'.format(max_sc_length))\n            print(\'max_target_length after filtering = {}\'.format(max_tg_length))\n            # Determine thresholds for buckets of equal capacity\n            buckets = []\n            sorted_dataset = sorted(dataset, key=lambda x:len(x.sc_ids), reverse=False)\n            max_tg_len_so_far = 0\n            for i, dp in enumerate(sorted_dataset):\n                if len(dp.sc_ids) > max_sc_length:\n                    break\n                if len(dp.tg_ids) > max_tg_len_so_far:\n                    max_tg_len_so_far = len(dp.tg_ids)\n                if i > 0 and i % bucket_capacity == 0:\n                    buckets.append((len(dp.sc_ids)+1, min(max_tg_len_so_far, max_tg_length)+1))\n            if len(buckets) == 0 or buckets[-1][0] < max(max_sc_length, max_tg_length):\n                buckets.append((max_sc_length+1,\n                                min(max_tg_len_so_far, max_tg_length)+1))\n        else:\n            num_buckets = len(buckets)\n            assert(num_buckets >= 1)\n\n        dataset2 = [[] for _ in buckets]\n        for i in range(len(dataset)):\n            data_point = dataset[i]\n            # Compute bucket id\n            bucket_ids = [b for b in xrange(len(buckets))\n                          if buckets[b][0] > len(data_point.sc_ids) and\n                          buckets[b][1] > len(data_point.tg_ids)]\n            if bucket_ids:\n                bucket_id = min(bucket_ids)\n                dataset2[bucket_id].append(data_point)\n            else:\n                if split != \'train\':\n                    bucket_id = len(buckets) - 1\n                    dataset2[bucket_id].append(data_point)\n        dataset = dataset2\n        if split != \'train\':\n            assert(len(functools.reduce(lambda x, y: x + y, dataset)) == data_size)\n      \n    D = DataSet()\n    D.data_points = dataset\n    if split == \'train\' and load_features:\n        D.max_sc_length = max_sc_length\n        D.max_tg_length = max_tg_length\n        if use_buckets:\n            D.buckets = buckets\n\n    return D\n\n\ndef load_vocabulary(FLAGS):\n    data_dir = FLAGS.data_dir\n    source, target = (\'nl\', \'cm\') if not FLAGS.explain else (\'cm\', \'nl\')\n    token_ext = \'normalized.{}\'.format(FLAGS.channel) \\\n        if FLAGS.normalized else FLAGS.channel\n    vocab_ext = \'vocab.{}\'.format(token_ext)\n\n    source_vocab_path = os.path.join(data_dir, \'{}.{}\'.format(source, vocab_ext))\n    target_vocab_path = os.path.join(data_dir, \'{}.{}\'.format(target, vocab_ext))\n\n    vocab = Vocab()\n    min_vocab_frequency = 1 if FLAGS.channel == \'char\' else FLAGS.min_vocab_frequency\n    vocab.sc_vocab, vocab.rev_sc_vocab = initialize_vocabulary(\n        source_vocab_path, min_vocab_frequency)\n    vocab.tg_vocab, vocab.rev_tg_vocab = initialize_vocabulary(\n        target_vocab_path, min_vocab_frequency)\n\n    max_sc_token_size = 0\n    for v in vocab.sc_vocab:\n        if len(v) > max_sc_token_size:\n            max_sc_token_size = len(v)\n    max_tg_token_size = 0\n    for v in vocab.tg_vocab:\n        if len(v) > max_tg_token_size:\n            max_tg_token_size = len(v)\n    vocab.max_sc_token_size = max_sc_token_size\n    vocab.max_tg_token_size = max_tg_token_size\n\n    print(\'source vocabulary size = {}\'.format(len(vocab.sc_vocab)))\n    print(\'target vocabulary size = {}\'.format(len(vocab.tg_vocab)))\n    print(\'max source token size = {}\'.format(vocab.max_sc_token_size))\n    print(\'max target token size = {}\'.format(vocab.max_tg_token_size))\n\n    return vocab\n\n\ndef initialize_vocabulary(vocab_path, min_frequency=1):\n    """"""Initialize vocabulary from file.\n\n    The vocabulary is stored one-item-per-line, followed by its frequency in\n    in the training set:\n      dog   4\n      cat   3\n    will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n    also return the reversed-vocabulary [""dog"", ""cat""].\n\n    Args:\n      vocab_path: path to the file containing the vocabulary.\n\n    Returns:\n      a pair: the vocabulary (a dictionary mapping string to integers), and\n      the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n    Raises:\n      ValueError: if the provided vocab_path does not exist.\n    """"""\n    if tf.io.gfile.exists(vocab_path):\n        V = []\n        with tf.io.gfile.GFile(vocab_path, mode=""r"") as f:\n            while(True):\n                line = f.readline()\n                if line:\n                    if line.startswith(\'\\t\'):\n                        v = line[0]\n                        freq = line.strip()   \n                    else:\n                        v, freq = line[:-1].rsplit(\'\\t\', 1)\n                    if int(freq) >= min_frequency \\\n                            or data_tools.flag_suffix in v:\n                        V.append(v)\n                else:\n                    break\n        vocab = dict([(x, y) for (y, x) in enumerate(V)])\n        rev_vocab = dict([(y, x) for (y, x) in enumerate(V)])\n        assert(len(vocab) == len(rev_vocab))\n        return vocab, rev_vocab\n    else:\n        raise ValueError(""Vocabulary file %s not found."", vocab_path)\n\n\ndef load_vocabulary_frequency(FLAGS):\n    data_dir = FLAGS.data_dir\n    source, target = (\'nl\', \'cm\') if not FLAGS.explain else (\'cm\', \'nl\')\n    token_ext = \'normalized.{}\'.format(FLAGS.channel) \\\n        if FLAGS.normalized else FLAGS.channel\n    vocab_ext = \'vocab.{}\'.format(token_ext)\n\n    source_vocab_path = os.path.join(data_dir, \'{}.{}\'.format(source, vocab_ext))\n    target_vocab_path = os.path.join(data_dir, \'{}.{}\'.format(target, vocab_ext))\n\n    sc_vocab_freq = initialize_vocabulary_frequency(source_vocab_path)\n    tg_vocab_freq = initialize_vocabulary_frequency(target_vocab_path)\n\n    return sc_vocab_freq, tg_vocab_freq\n\n\ndef initialize_vocabulary_frequency(vocab_path):\n    vocab_freq = {}\n    with open(vocab_path) as f:\n        counter = 0\n        for line in f:\n            if line.startswith(\'\\t\'):\n                v = line[0]\n                freq = line.strip()\n            else:\n                v, freq = line.rsplit(\'\\t\', 1)\n            vocab_freq[counter] = int(freq)\n            counter += 1\n    return vocab_freq\n\n\n# --- Data Preparation --- #\n\ndef prepare_data(FLAGS):\n    """"""\n    Read a specified dataset, tokenize data, create vocabularies and save\n    feature files.\n\n    Save to disk:\n        (1) nl vocabulary\n        (2) cm vocabulary\n        (3) nl token ids\n        (4) cm token ids\n    """"""\n    data_dir = FLAGS.data_dir\n    channel = FLAGS.channel if FLAGS.channel else \'\'\n    if channel and FLAGS.normalized:\n        channel = \'normalized.{}\'.format(channel)\n    prepare_dataset_split(data_dir, \'train\', channel=channel)\n    prepare_dataset_split(data_dir, \'dev\', channel=channel)\n    prepare_dataset_split(data_dir, \'test\', channel=channel)\n\n\ndef prepare_dataset_split(data_dir, split, channel=\'\'):\n    """"""\n    Process a specific dataset split.\n    """"""\n    def read_parallel_data(nl_path, cm_path):\n        with open(nl_path, encoding=\'utf-8\') as f:\n            nl_list = [nl.strip() for nl in f.readlines()]\n        with open(cm_path, encoding=\'utf-8\') as f:\n            cm_list = [cm.strip() for cm in f.readlines()]\n        return nl_list, cm_list\n\n    print(""Split - {}"".format(split))\n    nl_path = os.path.join(data_dir, split + \'.nl.filtered\')\n    cm_path = os.path.join(data_dir, split + \'.cm.filtered\')\n    nl_list, cm_list = read_parallel_data(nl_path, cm_path)\n\n    # character based processing\n    if not channel or channel == \'char\':\n        prepare_channel(data_dir, nl_list, cm_list, split, channel=\'char\',\n                        parallel_data_to_tokens=parallel_data_to_characters)\n    # partial-token based processing\n    if not channel or channel == \'partial.token\':\n        prepare_channel(data_dir, nl_list, cm_list, split, channel=\'partial.token\',\n                        parallel_data_to_tokens=parallel_data_to_partial_tokens)\n    # token based processing\n    if not channel or channel == \'token\':\n        prepare_channel(data_dir, nl_list, cm_list, split, channel=\'token\',\n                        parallel_data_to_tokens=parallel_data_to_tokens)\n    # normalized token based processing\n    if not channel or channel == \'normalized.token\':\n        prepare_channel(data_dir, nl_list, cm_list, split, channel=\'normalized.token\',\n                        parallel_data_to_tokens=parallel_data_to_normalized_tokens)\n\n\ndef prepare_channel(data_dir, nl_list, cm_list, split, channel,\n                    parallel_data_to_tokens):\n    print(""    channel - {}"".format(channel))\n    # Tokenize data\n    nl_tokens, cm_tokens = \\\n        parallel_data_to_tokens(nl_list, cm_list)\n    save_channel_features_to_file(data_dir, split, channel, nl_tokens, cm_tokens,\n                                  feature_separator=TOKEN_SEPARATOR)\n    # Create or load vocabulary\n    nl_vocab_path = os.path.join(data_dir, \'nl.vocab.{}\'.format(channel))\n    cm_vocab_path = os.path.join(data_dir, \'cm.vocab.{}\'.format(channel))\n    nl_vocab = create_vocabulary(nl_vocab_path, nl_tokens) \\\n        if split == \'train\' else initialize_vocabulary(nl_vocab_path)[0]\n    cm_vocab = create_vocabulary(cm_vocab_path, cm_tokens) \\\n        if split == \'train\' else initialize_vocabulary(cm_vocab_path)\n    nl_ids = [tokens_to_ids(data_point, nl_vocab) for data_point in nl_tokens]\n    cm_ids = [tokens_to_ids(data_point, cm_vocab) for data_point in cm_tokens]\n    save_channel_features_to_file(data_dir, split, \'ids.{}\'.format(channel),\n                                  nl_ids, cm_ids, feature_separator=\' \')\n    # For copying\n    if channel == \'char\':\n        nl_copy_tokens, cm_copy_tokens = nl_tokens, cm_tokens\n    else:\n        if channel == \'partial.token\':\n            nl_copy_tokens = [nl_to_partial_tokens(nl, tokenizer.basic_tokenizer,\n                to_lower_case=False, lemmatization=False) for nl in nl_list]\n        else:\n            nl_copy_tokens = [nl_to_tokens(nl, tokenizer.basic_tokenizer,\n                to_lower_case=False, lemmatization=False) for nl in nl_list]\n        cm_copy_tokens = cm_tokens\n    save_channel_features_to_file(data_dir, split, \'copy.{}\'.format(channel),\n        nl_copy_tokens, cm_copy_tokens, feature_separator=TOKEN_SEPARATOR)\n    alignments = compute_alignments(data_dir, nl_tokens, cm_tokens, split, channel)\n    with open(os.path.join(data_dir, \'{}.{}.align\'.format(split, channel)),\n              \'wb\') as o_f:\n        pickle.dump(alignments, o_f)\n\n\ndef save_channel_features_to_file(data_dir, split, channel, nl_features,\n                                  cm_features, feature_separator):\n    convert_to_str = channel.startswith(\'ids\')\n    nl_feature_path = os.path.join(data_dir, \'{}.nl.{}\'.format(split, channel))\n    cm_feature_path = os.path.join(data_dir, \'{}.cm.{}\'.format(split, channel))\n    with open(nl_feature_path, \'w\', encoding=\'utf-8\') as o_f:\n        for data_point in nl_features:\n            if convert_to_str:\n                o_f.write(\'{}\\n\'.format(feature_separator.join(\n                    [str(x) for x in data_point])))\n            else:\n                o_f.write(\'{}\\n\'.format(feature_separator.join(data_point)))\n    with open(cm_feature_path, \'w\', encoding=\'utf-8\') as o_f:\n        for data_point in cm_features:\n            if convert_to_str:\n                o_f.write(\'{}\\n\'.format(feature_separator.join(\n                    [str(x) for x in data_point])))\n            else:\n                o_f.write(\'{}\\n\'.format(feature_separator.join(data_point)))\n\n\ndef parallel_data_to_characters(nl_list, cm_list):\n    nl_data = [nl_to_characters(nl) for nl in nl_list]\n    cm_data = [cm_to_characters(cm) for cm in cm_list]\n    return nl_data, cm_data\n\n\ndef parallel_data_to_partial_tokens(nl_list, cm_list):\n    nl_data = [nl_to_partial_tokens(nl, tokenizer.basic_tokenizer) for nl in nl_list]\n    cm_data = [cm_to_partial_tokens(cm, data_tools.bash_tokenizer) for cm in cm_list]\n    return nl_data, cm_data\n\n\ndef parallel_data_to_tokens(nl_list, cm_list):\n    nl_data = [nl_to_tokens(nl, tokenizer.basic_tokenizer) for nl in nl_list]\n    cm_data = [cm_to_tokens(cm, data_tools.bash_tokenizer) for cm in cm_list]\n    return nl_data, cm_data\n\n\ndef parallel_data_to_normalized_tokens(nl_list, cm_list):\n    nl_data = [nl_to_tokens(nl, tokenizer.ner_tokenizer) for nl in nl_list]\n    cm_data = [cm_to_tokens(cm, data_tools.bash_tokenizer, arg_type_only=True)\n               for cm in cm_list]\n    return nl_data, cm_data\n\n\ndef string_to_characters(s):\n    assert(isinstance(s, str))\n    chars = []\n    for c in s:\n        if c == \' \':\n            chars.append(constants._SPACE)\n        else:\n            chars.append(c)\n    return chars\n\n\ndef nl_to_characters(nl, use_preprocessing=False):\n    nl_data_point = []\n    if use_preprocessing:\n        nl_tokens = nl_to_tokens(nl, tokenizer.basic_tokenizer, lemmatization=False)\n        for c in \' \'.join(nl_tokens):\n            if c == \' \':\n                nl_data_point.append(constants._SPACE)\n            else:\n                nl_data_point.append(c)\n    else:\n        nl_data_point = string_to_characters(nl)\n    return nl_data_point\n\n\ndef cm_to_characters(cm, use_preprocessing=False):\n    cm_data_point = []\n    if use_preprocessing:\n        cm_tokens = cm_to_tokens(\n            cm, data_tools.bash_tokenizer, with_prefix=True,\n            with_flag_argtype=True)\n        for i, t in enumerate(cm_tokens):\n            if not nast.KIND_PREFIX in t:\n                cm_data_point.append(t)\n            else:\n                kind, token = t.split(nast.KIND_PREFIX, 1)\n                if kind.lower() == \'utility\':\n                    cm_data_point.append(token)\n                elif kind.lower() == \'flag\':\n                    cm_data_point.append(token)\n                else:\n                    for c in token:\n                        cm_data_point.append(c)\n            if i < len(cm_tokens) - 1:\n                cm_data_point.append(constants._SPACE)\n    else:\n        cm = data_tools.correct_errors_and_normalize_surface(cm)\n        cm_data_point = string_to_characters(cm)\n    return cm_data_point\n\n\ndef nl_to_partial_tokens(s, tokenizer, to_lower_case=True, lemmatization=True):\n    return string_to_partial_tokens(\n        nl_to_tokens(s, tokenizer, to_lower_case=to_lower_case,\n                     lemmatization=lemmatization), use_arg_start_end=False)\n\n\ndef cm_to_partial_tokens(s, tokenizer):\n    s = data_tools.correct_errors_and_normalize_surface(s)\n    return string_to_partial_tokens(cm_to_tokens(s, tokenizer))\n\n\ndef string_to_partial_tokens(s, use_arg_start_end=True):\n    """"""\n    Split a sequence of tokens into a sequence of partial tokens.\n\n    A partial token sequence may consist of\n        1. continuous span of alphabetical letters\n        2. continuous span of digits\n        3. a non-alpha-numerical character\n        4. _ARG_START which indicates the beginning of an argument token\n        5. _ARG_END which indicates the end of an argument token\n    """"""\n    partial_tokens = []\n\n    for token in s:\n        if not token:\n            continue\n        if token.isalpha() or token.isnumeric() \\\n                or data_tools.flag_suffix in token \\\n                or token in bash.binary_logic_operators \\\n                or token in bash.left_associate_unary_logic_operators \\\n                or token in bash.right_associate_unary_logic_operators:\n            partial_tokens.append(token)\n        else:\n            arg_partial_tokens = []\n            pt = \'\'\n            reading_alpha = False\n            reading_numeric = False\n            for c in token:\n                if reading_alpha:\n                    if c.isalpha():\n                        pt += c\n                    else:\n                        arg_partial_tokens.append(pt)\n                        reading_alpha = False\n                        pt = c\n                        if c.isnumeric():\n                            reading_numeric = True\n                elif reading_numeric:\n                    if c.isnumeric():\n                        pt += c\n                    else:\n                        arg_partial_tokens.append(pt)\n                        reading_numeric = False\n                        pt = c\n                        if c.isalpha():\n                            reading_alpha = True\n                else:\n                    if pt:\n                        arg_partial_tokens.append(pt)\n                    pt = c\n                    if c.isalpha():\n                        reading_alpha = True\n                    elif c.isnumeric():\n                        reading_numeric = True\n            if pt:\n                arg_partial_tokens.append(pt)\n            if len(arg_partial_tokens) > 1:\n                if use_arg_start_end:\n                    partial_tokens.append(_ARG_START)\n                partial_tokens.extend(arg_partial_tokens)\n                if use_arg_start_end:\n                    partial_tokens.append(_ARG_END)\n            else:\n                partial_tokens.extend(arg_partial_tokens)\n\n    return partial_tokens\n\n\ndef nl_to_tokens(s, tokenizer, to_lower_case=True, lemmatization=True):\n    """"""\n    Split a natural language string into a sequence of tokens.\n    """"""\n    tokens, _ = tokenizer(\n        s, to_lower_case=to_lower_case, lemmatization=lemmatization)\n    return tokens\n\n\ndef cm_to_tokens(s, tokenizer, loose_constraints=True, arg_type_only=False,\n                 with_prefix=False, with_flag_argtype=True):\n    """"""\n    Split a command string into a sequence of tokens.\n    """"""\n    tokens = tokenizer(s, loose_constraints=loose_constraints,\n                       arg_type_only=arg_type_only, \n                       with_prefix=with_prefix,\n                       with_flag_argtype=with_flag_argtype)\n    return tokens\n\n\ndef tokens_to_ids(tokens, vocabulary):\n    """"""\n    Map tokens into their indices in the vocabulary.\n    """"""\n    token_ids = []\n    for t in tokens:\n        if t in vocabulary:\n            token_ids.append(vocabulary[t])\n        else:\n            token_ids.append(UNK_ID)\n    return token_ids\n\n\ndef compute_copy_indices(sc_tokens, tg_tokens, sc_copy_tokens, tg_copy_tokens,\n                         tg_vocab, channel):\n    assert(len(sc_tokens) == len(sc_copy_tokens))\n    assert(len(tg_tokens) == len(tg_copy_tokens))\n    csc_ids, ctg_ids = [], []\n    init_vocab = CHAR_INIT_VOCAB if channel == \'char\' else TOKEN_INIT_VOCAB\n    for i, sc_token in enumerate(sc_tokens):\n        if (not sc_token in init_vocab) and sc_token in tg_vocab:\n            csc_ids.append(tg_vocab[sc_token])\n        else:\n            csc_ids.append(len(tg_vocab) + sc_tokens.index(sc_token))\n    for j, tg_token in enumerate(tg_tokens):\n        tg_copy_token = tg_copy_tokens[j]\n        if tg_token in tg_vocab:\n            ctg_ids.append(tg_vocab[tg_token])\n        else:\n            if tg_copy_token in sc_copy_tokens:\n                ctg_ids.append(\n                    len(tg_vocab) + sc_copy_tokens.index(tg_copy_token))\n            else:\n                if channel == \'char\':\n                    ctg_ids.append(CUNK_ID)\n                else:\n                    ctg_ids.append(UNK_ID)\n    # Append EOS symbol\n    if channel == \'char\':\n        ctg_ids.append(CEOS_ID)\n    else:\n        ctg_ids.append(EOS_ID)\n    return csc_ids, ctg_ids\n\n\ndef compute_alignments(data_dir, nl_list, cm_list, split, channel):\n    alignments = []\n    output_path = os.path.join(data_dir, \'{}.{}.align.readable\'.format(split, channel))\n    with open(output_path, \'w\') as o_f:\n        for nl_tokens, cm_tokens in zip(nl_list, cm_list):\n            alignments.append(compute_pair_alignment(nl_tokens, cm_tokens, o_f))\n    return alignments\n\n\ndef compute_pair_alignment(nl_tokens, cm_tokens, out_file):\n    """"""\n    Compute the alignments between two parallel sequences.\n    """"""\n    init_vocab = set(TOKEN_INIT_VOCAB + CHAR_INIT_VOCAB)\n    m = len(nl_tokens)\n    n = len(cm_tokens)\n\n    A = np.zeros([m, n], dtype=np.int32)\n\n    for i, x in enumerate(nl_tokens):\n        for j, y in enumerate(cm_tokens):\n            if not x in init_vocab and x == y:\n                A[i, j] = 1\n                out_file.write(\'{}-{} \'.format(i, j))\n    out_file.write(\'\\n\')\n\n    return ssp.lil_matrix(A)\n\n\ndef create_vocabulary(vocab_path, dataset, min_word_frequency=1,\n                      is_character_model=False, parallel_dataset=None):\n    """"""\n    Compute the vocabulary of a tokenized dataset and save to file.\n    """"""\n    vocab = collections.defaultdict(int)\n    num_copy = collections.defaultdict(int)\n    if parallel_dataset:\n        for i, data_point in enumerate(dataset):\n            parallel_data_point = parallel_dataset[i]\n            for token in data_point:\n                vocab[token] += 1\n                if token in parallel_data_point:\n                    num_copy[token] += 1\n        for v in vocab:\n            if vocab[v] == num_copy[v]:\n                vocab[v] = 0\n    else:\n        for data_point in dataset:\n            for token in data_point:\n                vocab[token] += 1\n    sorted_vocab = [(x, y) for x, y in sorted(\n            vocab.items(), key=lambda x:(x[1], x[0]), reverse=True) \n            if y >= min_word_frequency]\n    \n    if is_character_model:\n        # Character model\n        init_vocab = CHAR_INIT_VOCAB\n    else:\n        init_vocab = TOKEN_INIT_VOCAB\n    vocab = [(v, 1000000) for v in init_vocab]\n    for v, f in sorted_vocab:\n        if not v in init_vocab:\n            vocab.append((v, f))\n\n    with open(vocab_path, \'w\', encoding=\'utf-8\') as vocab_file:\n        for v, f in vocab:\n            vocab_file.write(\'{}\\t{}\\n\'.format(v, f))\n\n    return dict([(x[0], y) for y, x in enumerate(vocab)])\n\n\ndef group_parallel_data(dataset, attribute=\'source\', use_temp=False,\n                        tokenizer_selector=\'nl\'):\n    """"""\n    Group parallel dataset by a certain attribute.\n\n    :param dataset: a list of training quadruples (nl_str, cm_str, nl, cm)\n    :param attribute: attribute by which the data is grouped\n    :param bucket_input: if the input is grouped in buckets\n    :param use_temp: set to true if the dataset is to be grouped by the natural\n        language template; false if the dataset is to be grouped by the natural\n        language strings\n    :param tokenizer_selector: specify which tokenizer to use for making\n        templates\n\n    :return: list of (key, data group) tuples sorted by the key value.\n    """"""\n    if dataset.data_points and isinstance(dataset.data_points, list):\n        if isinstance(dataset.data_points[0], list):\n            data_points = functools.reduce(lambda x, y: x + y, dataset.data_points)\n        else:\n            data_points = dataset.data_points\n    else:\n        raise ValueError\n\n    grouped_dataset = {}\n    for i in xrange(len(data_points)):\n        data_point = data_points[i]\n        attr = data_point.sc_txt \\\n            if attribute == \'source\' else data_point.tg_txt\n        if use_temp:\n            if tokenizer_selector == \'nl\':\n                words, _ = tokenizer.ner_tokenizer(attr)\n            else:\n                words = data_tools.bash_tokenizer(attr, arg_type_only=True)\n            temp = \' \'.join(words)\n        else:\n            if tokenizer_selector == \'nl\':\n                words, _ = tokenizer.basic_tokenizer(attr)\n                temp = \' \'.join(words)\n            else:\n                temp = attr\n        if temp in grouped_dataset:\n            grouped_dataset[temp].append(data_point)\n        else:\n            grouped_dataset[temp] = [data_point]\n    return sorted(grouped_dataset.items(), key=lambda x: x[0])\n\n\nif __name__ == \'__main__\':\n    print(nl_to_partial_tokens(\'Execute md5sum command on files found by the find command\',\n                               tokenizer=tokenizer.basic_tokenizer))\n    print(cm_to_partial_tokens(""find . -perm -600 -print"",\n                               tokenizer=data_tools.bash_tokenizer))\n'"
encoder_decoder/decode_tools.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport datetime, time\nimport re\nimport shutil\n\nfrom bashlint import bash, data_tools\nfrom encoder_decoder import data_utils, slot_filling\nfrom eval import tree_dist\nfrom nlp_tools import constants, format_args, tokenizer\n\nAPOLOGY_MSG = ""Sorry, I don\'t know how to translate this command.""\n\n\ndef demo(sess, model, FLAGS):\n    """"""\n    Simple command line decoding interface.\n    """"""\n    # Decode from standard input.\n    sys.stdout.write(\'> \')\n    sys.stdout.flush()\n    sentence = sys.stdin.readline()\n\n    vocabs = data_utils.load_vocabulary(FLAGS)\n\n    while sentence:\n        if FLAGS.fill_argument_slots:\n            slot_filling_classifier = get_slot_filling_classifer(FLAGS)\n            batch_outputs, sequence_logits = translate_fun(sentence, sess, model,\n                vocabs, FLAGS, slot_filling_classifier=slot_filling_classifier)\n        else:\n            batch_outputs, sequence_logits = translate_fun(sentence, sess, model,\n                vocabs, FLAGS)\n        if FLAGS.token_decoding_algorithm == \'greedy\':\n            tree, pred_cmd, outputs = batch_outputs[0]\n            score = sequence_logits[0]\n            print(\'{} ({})\'.format(pred_cmd, score))\n        elif FLAGS.token_decoding_algorithm == \'beam_search\':\n            if batch_outputs:\n                top_k_predictions = batch_outputs[0]\n                top_k_scores = sequence_logits[0]\n                for j in xrange(min(FLAGS.beam_size, 10, len(batch_outputs[0]))):\n                    if len(top_k_predictions) <= j:\n                        break\n                    top_k_pred_tree, top_k_pred_cmd = top_k_predictions[j]\n                    print(\'Prediction {}: {} ({}) \'.format(\n                        j+1, top_k_pred_cmd, top_k_scores[j]))\n                print()\n            else:\n                print(APOLOGY_MSG)\n        print(\'> \', end=\'\')\n        sys.stdout.flush()\n        sentence = sys.stdin.readline()\n\n\ndef translate_fun(data_point, sess, model, vocabs, FLAGS,\n                  slot_filling_classifier=None):\n    tg_ids = [data_utils.ROOT_ID]\n    decoder_features = [[tg_ids]]\n    if type(data_point) is str:\n        source_str = data_point\n        encoder_features = query_to_encoder_features(data_point, vocabs, FLAGS)\n    else:\n        source_str = data_point[0].sc_txt\n        encoder_features = [[data_point[0].sc_ids]]\n        if FLAGS.use_copy and FLAGS.copy_fun == \'copynet\':\n            encoder_features.append([data_point[0].csc_ids])\n\n    if FLAGS.use_copy and FLAGS.copy_fun == \'copynet\':\n        # append dummy copynet target features (\n        # used only for computing training objectives)\n        ctg_ids = [data_utils.ROOT_ID]\n        decoder_features.append([ctg_ids])\n        # tokenize the source string with minimal changes on the token form\n        copy_tokens = [query_to_copy_tokens(source_str, FLAGS)]\n    else:\n        copy_tokens = None\n    if FLAGS.normalized:\n        _, entities = tokenizer.ner_tokenizer(source_str)\n        sc_fillers = [entities[0]]\n    else:\n        sc_fillers = None\n\n    # Which bucket does it belong to?\n    bucket_ids = [b for b in xrange(len(model.buckets))\n                  if model.buckets[b][0] > len(encoder_features[0][0])]\n    bucket_id = min(bucket_ids) if bucket_ids else (len(model.buckets) - 1)\n    \n    # Get a 1-element batch to feed the sentence to the model.\n    formatted_example = model.format_batch(\n        encoder_features, decoder_features, bucket_id=bucket_id)\n\n    # Compute neural network decoding output\n    model_outputs = model.step(sess, formatted_example, bucket_id,\n                               forward_only=True)\n    sequence_logits = model_outputs.sequence_logits\n\n    decoded_outputs = decode(model_outputs, FLAGS, vocabs, sc_fillers=sc_fillers,\n                             slot_filling_classifier=slot_filling_classifier,\n                             copy_tokens=copy_tokens)\n\n    return decoded_outputs, sequence_logits\n\n\ndef decode(model_outputs, FLAGS, vocabs, sc_fillers=None,\n           slot_filling_classifier=None, copy_tokens=None):\n    """"""\n    Transform the neural network output into readable strings and apply output\n    filtering (if any).\n    :param encoder_inputs:\n    :param model_outputs:\n    :param FLAGS:\n    :param vocabs:\n    :param sc_fillers:\n    :param slot_filling_classifier:\n    :return batch_outputs: nested list of (target_ast, target) tuples\n        - target_ast is a python tree object for target languages that we know\n          how to parse and a dummy string for those we don\'t\n        - target is the output string\n    """"""\n    rev_tg_vocab = vocabs.rev_tg_vocab\n\n    encoder_outputs = model_outputs.encoder_hidden_states\n    decoder_outputs = model_outputs.decoder_hidden_states\n\n    if FLAGS.fill_argument_slots:\n        assert(sc_fillers is not None)\n        assert(slot_filling_classifier is not None)\n        assert(encoder_outputs is not None)\n        assert(decoder_outputs is not None)\n\n    output_symbols = model_outputs.output_symbols\n    batch_size = len(output_symbols)\n    batch_outputs = []\n    num_output_examples = 0\n\n    for batch_id in xrange(batch_size):\n        def as_str(output, r_tg_vocab):\n            if output < FLAGS.tg_vocab_size:\n                token = r_tg_vocab[output]\n            else:\n                if FLAGS.use_copy and FLAGS.copy_fun == \'copynet\':\n                    source_id = output - FLAGS.tg_vocab_size\n                    if source_id >= 0 and source_id < len(copy_tokens[batch_id]):\n                        token = copy_tokens[batch_id][source_id]\n                    else:\n                        return data_utils._UNK\n                else:\n                    return data_utils._UNK\n            return token\n\n        top_k_predictions = output_symbols[batch_id]\n        if FLAGS.token_decoding_algorithm == \'beam_search\':\n            assert(len(top_k_predictions) == FLAGS.beam_size)\n            beam_outputs = []\n        else:\n            # pack greedy decoding results into size-1 beam\n            top_k_predictions = [top_k_predictions]\n\n        for beam_id in xrange(len(top_k_predictions)):\n            # Step 1: transform the neural network output into readable strings\n            prediction = top_k_predictions[beam_id]\n            outputs = [int(pred) for pred in prediction]\n            \n            # If there is an EOS symbol in outputs, cut them at that point.\n            if data_utils.EOS_ID in outputs:\n                outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n            if data_utils.PAD_ID in outputs:\n                outputs = outputs[:outputs.index(data_utils.PAD_ID)]\n            output_tokens = []\n            tg_slots = {}\n            for token_id in xrange(len(outputs)):\n                output = outputs[token_id]\n                pred_token = as_str(output, rev_tg_vocab)\n                if data_tools.flag_suffix in pred_token:\n                    pred_token = pred_token.split(data_tools.flag_suffix)[0]\n                # process argument slots\n                if pred_token in bash.argument_types:\n                    if token_id > 0 and format_args.is_min_flag(\n                        rev_tg_vocab[outputs[token_id-1]]):\n                        pred_token_type = \'Timespan\'\n                    else:\n                        pred_token_type = pred_token\n                    tg_slots[token_id] = (pred_token, pred_token_type)\n                output_tokens.append(pred_token)\n\n            if FLAGS.channel == \'partial.token\':\n                # process partial-token outputs\n                merged_output_tokens = []\n                buffer = \'\'\n                load_buffer = False\n                for token in output_tokens:\n                    if load_buffer:\n                        if token == data_utils._ARG_END:\n                            merged_output_tokens.append(buffer)\n                            load_buffer = False\n                            buffer = \'\'\n                        else:\n                            buffer += token\n                    else:\n                        if token == data_utils._ARG_START:\n                            load_buffer = True\n                        else:\n                            merged_output_tokens.append(token)\n                if buffer:\n                    merged_output_tokens.append(buffer)\n                output_tokens = merged_output_tokens\n    \n            if FLAGS.channel == \'char\':\n                target = \'\'\n                for char in output_tokens:\n                    if char == data_utils.constants._SPACE:\n                        target += \' \'\n                    else:\n                        target += char\n            else:\n                target = \' \'.join(output_tokens)\n            \n            # Step 2: checvik if the predicted command template is grammatical\n            if FLAGS.grammatical_only and not FLAGS.explain:\n                if FLAGS.dataset.startswith(\'bash\'):\n                    target = re.sub(\'( ;\\s+)|( ;$)\', \' \\\\; \', target)\n                    target_ast = data_tools.bash_parser(target, verbose=False)\n                elif FLAGS.dataset.startswith(\'regex\'):\n                    # TODO: check if a predicted regular expression is legal\n                    target_ast = \'__DUMMY_TREE__\'\n                else:\n                    target_ast = data_tools.paren_parser(target)\n                # filter out non-grammatical output\n                if target_ast is None:\n                    continue\n            else:\n                target_ast = \'__DUMMY_TREE__\'\n\n            # Step 3: check if the predicted command templates have enough\n            # slots to hold the fillers (to rule out templates that are\n            # trivially unqualified)\n            output_example = False\n            if FLAGS.explain or not FLAGS.dataset.startswith(\'bash\') \\\n                    or not FLAGS.normalized:\n                output_example = True\n            else:\n                # Step 3: match the fillers to the argument slots\n                batch_sc_fillers = sc_fillers[batch_id]\n                if len(tg_slots) >= len(batch_sc_fillers):\n                    if FLAGS.fill_argument_slots:\n                        target_ast, target, _ = slot_filling.stable_slot_filling(\n                            output_tokens, batch_sc_fillers, tg_slots, None,\n                            encoder_outputs[batch_id],\n                            decoder_outputs[batch_id*FLAGS.beam_size+beam_id],\n                            slot_filling_classifier, verbose=False)\n                    else:\n                        output_example = True\n                    if not output_example and (target_ast is not None):\n                        output_example = True\n\n            if output_example:\n                if FLAGS.token_decoding_algorithm == \'greedy\':\n                    batch_outputs.append((target_ast, target))\n                else:\n                    beam_outputs.append((target_ast, target))\n                num_output_examples += 1\n\n            # The threshold is used to increase decoding speed\n            if num_output_examples == 20:\n                break\n\n        if FLAGS.token_decoding_algorithm == \'beam_search\':\n            if beam_outputs:\n                batch_outputs.append(beam_outputs)\n\n    return batch_outputs\n\n\ndef decode_set(sess, model, dataset, top_k, FLAGS, verbose=False):\n    """"""\n    Compute top-k predictions on the dev/test dataset and write the predictions\n    to disk.\n\n    :param sess: A TensorFlow session.\n    :param model: Prediction model object.\n    :param top_k: Number of top predictions to compute.\n    :param FLAGS: Training/testing hyperparameter settings.\n    :param verbose: If set, also print decoding results to screen.\n    """"""\n    nl2bash = FLAGS.dataset.startswith(\'bash\') and not FLAGS.explain\n\n    tokenizer_selector = \'cm\' if FLAGS.explain else \'nl\'\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n    vocabs = data_utils.load_vocabulary(FLAGS)\n    rev_sc_vocab = vocabs.rev_sc_vocab\n\n    ts = datetime.datetime.fromtimestamp(time.time()).strftime(\'%Y-%m-%d-%H%M%S\')\n    pred_file_path = os.path.join(model.model_dir, \'predictions.{}.{}\'.format(\n        model.decode_sig, ts))\n    pred_file = open(pred_file_path, \'w\')\n    eval_file_path = os.path.join(model.model_dir, \'predictions.{}.{}.csv\'.format(\n        model.decode_sig, ts))\n    eval_file = open(eval_file_path, \'w\')\n    eval_file.write(\'example_id, description, ground_truth, prediction, \' +\n                    \'correct template, correct command\\n\')\n    for example_id in xrange(len(grouped_dataset)):\n        key, data_group = grouped_dataset[example_id]\n\n        sc_txt = data_group[0].sc_txt.strip()\n        sc_tokens = [rev_sc_vocab[i] for i in data_group[0].sc_ids]\n        if FLAGS.channel == \'char\':\n            sc_temp = \'\'.join(sc_tokens)\n            sc_temp = sc_temp.replace(constants._SPACE, \' \')\n        else:\n            sc_temp = \' \'.join(sc_tokens)\n        tg_txts = [dp.tg_txt for dp in data_group]\n        tg_asts = [data_tools.bash_parser(tg_txt) for tg_txt in tg_txts]\n        if verbose:\n            print(\'\\nExample {}:\'.format(example_id))\n            print(\'Original Source: {}\'.format(sc_txt.encode(\'utf-8\')))\n            print(\'Source: {}\'.format(sc_temp.encode(\'utf-8\')))\n            for j in xrange(len(data_group)):\n                print(\'GT Target {}: {}\'.format(j+1, data_group[j].tg_txt.encode(\'utf-8\')))\n\n        if FLAGS.fill_argument_slots:\n            slot_filling_classifier = get_slot_filling_classifer(FLAGS)\n            batch_outputs, sequence_logits = translate_fun(data_group, sess, model,\n                vocabs, FLAGS, slot_filling_classifier=slot_filling_classifier)\n        else:\n            batch_outputs, sequence_logits = translate_fun(data_group, sess, model,\n                vocabs, FLAGS)\n        if FLAGS.tg_char:\n            batch_outputs, batch_char_outputs = batch_outputs\n\n        eval_row = \'{},""{}"",\'.format(example_id, sc_txt.replace(\'""\', \'""""\'))\n        if batch_outputs:\n            if FLAGS.token_decoding_algorithm == \'greedy\':\n                tree, pred_cmd = batch_outputs[0]\n                if nl2bash:\n                    pred_cmd = data_tools.ast2command(\n                        tree, loose_constraints=True)\n                score = sequence_logits[0]\n                if verbose:\n                    print(\'Prediction: {} ({})\'.format(pred_cmd, score))\n                pred_file.write(\'{}\\n\'.format(pred_cmd))\n            elif FLAGS.token_decoding_algorithm == \'beam_search\':\n                top_k_predictions = batch_outputs[0]\n                if FLAGS.tg_char:\n                    top_k_char_predictions = batch_char_outputs[0]\n                top_k_scores = sequence_logits[0]\n                num_preds = min(FLAGS.beam_size, top_k, len(top_k_predictions))\n                for j in xrange(num_preds):\n                    if j > 0:\n                        eval_row = \',,\'\n                    if j < len(tg_txts):\n                        eval_row += \'""{}"",\'.format(tg_txts[j].strip().replace(\'""\', \'""""\'))\n                    else:\n                        eval_row += \',\'\n                    top_k_pred_tree, top_k_pred_cmd = top_k_predictions[j]\n                    if nl2bash:\n                        pred_cmd = data_tools.ast2command(\n                            top_k_pred_tree, loose_constraints=True)\n                    else:\n                        pred_cmd = top_k_pred_cmd\n                    pred_file.write(\'{}|||\'.format(pred_cmd.encode(\'utf-8\')))\n                    eval_row += \'""{}"",\'.format(pred_cmd.replace(\'""\', \'""""\'))\n                    temp_match = tree_dist.one_match(\n                        tg_asts, top_k_pred_tree, ignore_arg_value=True)\n                    str_match = tree_dist.one_match(\n                        tg_asts, top_k_pred_tree, ignore_arg_value=False)\n                    if temp_match:\n                        eval_row += \'y,\'\n                    if str_match:\n                        eval_row += \'y\'\n                    eval_file.write(\'{}\\n\'.format(eval_row.encode(\'utf-8\')))\n                    if verbose:\n                        print(\'Prediction {}: {} ({})\'.format(\n                            j+1, pred_cmd.encode(\'utf-8\'), top_k_scores[j]))\n                        if FLAGS.tg_char:\n                            print(\'Character-based prediction {}: {}\'.format(\n                                j+1, top_k_char_predictions[j].encode(\'utf-8\')))\n                pred_file.write(\'\\n\')\n        else:\n            print(APOLOGY_MSG)\n            pred_file.write(\'\\n\')\n            eval_file.write(\'{}\\n\'.format(eval_row))\n            eval_file.write(\'\\n\')\n            eval_file.write(\'\\n\')\n    pred_file.close()\n    eval_file.close()\n    shutil.copyfile(pred_file_path, os.path.join(FLAGS.model_dir,\n        \'predictions.{}.latest\'.format(model.decode_sig)))\n    shutil.copyfile(eval_file_path, os.path.join(FLAGS.model_dir,\n        \'predictions.{}.latest.csv\'.format(model.decode_sig)))\n\n\ndef get_slot_filling_classifer(FLAGS):\n    # create slot filling classifier\n    mapping_param_dir = os.path.join(FLAGS.model_dir, \'train.mappings.X.Y.npz\')\n    npz = np.load(mapping_param_dir)\n    train_X = npz[\'arr_0\']\n    train_Y = npz[\'arr_1\']\n    slot_filling_classifier = slot_filling.KNearestNeighborModel(FLAGS.num_nn_slot_filling, train_X, train_Y)\n    print(\'Slot filling classifier parameters loaded.\')\n    return slot_filling_classifier\n\n\n# --- Compute query features\ndef query_to_encoder_features(sentence, vocabs, FLAGS):\n    """"""\n    Convert a natural language query into feature vectors used by the encoder.\n    """"""\n    if FLAGS.channel == \'char\':\n        tokens = data_utils.nl_to_characters(sentence)\n        init_vocab = data_utils.CHAR_INIT_VOCAB\n    elif FLAGS.channel == \'partial.token\':\n        tokens = data_utils.nl_to_partial_tokens(sentence, tokenizer.basic_tokenizer)\n        init_vocab = data_utils.TOKEN_INIT_VOCAB\n    else:\n        if FLAGS.normalized:\n            tokens = data_utils.nl_to_tokens(sentence, tokenizer.ner_tokenizer)\n        else:\n            tokens = data_utils.nl_to_tokens(sentence, tokenizer.basic_tokenizer)\n        init_vocab = data_utils.TOKEN_INIT_VOCAB\n    sc_ids = data_utils.tokens_to_ids(tokens, vocabs.sc_vocab)\n    encoder_features = [[sc_ids]]\n    if FLAGS.use_copy and FLAGS.copy_fun == \'copynet\':\n        csc_ids = []\n        for i, t in enumerate(tokens):\n            if not t in init_vocab and t in vocabs.tg_vocab:\n                csc_ids.append(vocabs.tg_vocab[t])\n            else:\n                csc_ids.append(len(vocabs.tg_vocab) + i)\n        encoder_features.append([csc_ids])\n    return encoder_features\n\n\ndef query_to_copy_tokens(sentence, FLAGS):\n    if FLAGS.channel == \'char\':\n        tokens = data_utils.nl_to_characters(sentence)\n    elif FLAGS.channel == \'partial.token\':\n        tokens = data_utils.nl_to_partial_tokens(\n            sentence, tokenizer.basic_tokenizer, to_lower_case=False,\n            lemmatization=False)\n    else:\n        tokens = data_utils.nl_to_tokens(\n            sentence, tokenizer.basic_tokenizer, to_lower_case=False,\n            lemmatization=False)\n    return tokens\n\n# --- Visualization --- #\ndef visualize_attn_alignments(M, source, target, rev_sc_vocab,\n                              rev_tg_vocab, output_path):\n    target_length, source_length = M.shape\n\n    nl = [rev_sc_vocab[x] for x in source]\n    cm = []\n    for i, x in enumerate(target):\n        cm.append(rev_tg_vocab[x])\n        if rev_tg_vocab[x] == data_utils._EOS:\n            break\n\n    plt.clf()\n    if len(target) == 0:\n        i = 0\n    plt.imshow(M[:i+1, :], interpolation=\'nearest\', cmap=plt.cm.Blues)\n\n    pad_size = source_length - len(nl)\n    plt.xticks(xrange(source_length),\n               [x.replace(\'$$\', \'\') for x in reversed(\n                   nl + [data_utils._PAD] * pad_size)],\n               rotation=\'vertical\')\n    plt.yticks(xrange(len(cm)), [x.replace(\'$$\', \'\') for x in cm],\n               rotation=\'horizontal\')\n\n    plt.colorbar()\n\n    plt.savefig(output_path, bbox_inches=\'tight\')\n'"
encoder_decoder/decoder.py,37,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom encoder_decoder import data_utils, graph_utils, beam_search\n\n\nclass Decoder(graph_utils.NNModel):\n    def __init__(self, hyperparameters, scope, dim, embedding_dim,\n                 use_attention, attention_function, input_keep, output_keep,\n                 decoding_algorithm):\n        """"""\n        :param hyperparameters: Tellina model hyperparameters.\n        :param scope: Scope of the decoder. (There might be multiple decoders\n            with the same construction in the neural architecture.)\n        :param vocab_size: Output vocabulary size.\n        :param dim: Decoder dimension.\n        :param embedding_dim: Decoder embedding dimension.\n        :param use_attention: Set to True to use attention for decoding.\n        :param attention_function: The attention function to use.\n        :param input_keep: Dropout parameter for the input of the attention layer.\n        :param output_keep: Dropout parameter for the output of the attention layer.\n        :param decoding_algorithm: The decoding algorithm to use.\n            1. ""greedy""\n            2. ""beam_search""\n        """"""\n        super(Decoder, self).__init__(hyperparameters)\n        if self.forward_only:\n            self.hyperparams[\'batch_size\'] = 1\n\n        self.scope = scope\n        self.dim = dim\n        self.embedding_dim = embedding_dim\n        self.use_attention = use_attention\n        self.attention_function = attention_function\n        self.input_keep = input_keep\n        self.output_keep = output_keep\n        self.decoding_algorithm = decoding_algorithm\n\n        self.vocab_size = self.target_vocab_size\n        \n        # variable sharing\n        self.embedding_vars = False\n        self.output_project_vars = False\n\n        self.beam_decoder = beam_search.BeamDecoder(\n                self.num_layers,\n                data_utils.ROOT_ID,\n                data_utils.EOS_ID,\n                self.batch_size,\n                self.beam_size,\n                self.use_attention,\n                self.use_copy,\n                self.copy_fun,\n                self.alpha,\n                locally_normalized=(self.training_algorithm != ""bso"")\n            ) if self.decoding_algorithm == ""beam_search"" else None\n\n        self.output_project = self.output_project()\n\n    def embeddings(self):\n        with tf.compat.v1.variable_scope(self.scope + ""_embeddings"", reuse=self.embedding_vars):\n            vocab_size = self.target_vocab_size\n            print(""target token embedding size = {}"".format(vocab_size))\n            sqrt3 = math.sqrt(3)\n            initializer = tf.compat.v1.random_uniform_initializer(-sqrt3, sqrt3)\n            embeddings = tf.compat.v1.get_variable(""embedding"",\n                [vocab_size, self.embedding_dim], initializer=initializer)\n            self.embedding_vars = True\n            return embeddings\n\n    def token_features(self):\n        return np.load(self.tg_token_features_path)\n\n    def output_project(self):\n        with tf.compat.v1.variable_scope(self.scope + ""_output_project"",\n                               reuse=self.output_project_vars):\n            w = tf.compat.v1.get_variable(""proj_w"", [self.dim, self.vocab_size])\n            b = tf.compat.v1.get_variable(""proj_b"", [self.vocab_size])\n            self.output_project_vars = True\n        return (w, b)\n\n\nclass CopyCellWrapper(tf.compat.v1.nn.rnn_cell.RNNCell):\n    def __init__(self, cell, output_project, num_layers,\n                 encoder_copy_inputs, tg_vocab_size):\n        self.cell = cell\n        self.output_project = output_project\n        self.num_layers = num_layers\n        self.tg_vocab_size = tg_vocab_size\n\n        self.encoder_size = len(encoder_copy_inputs)\n        # [batch_size x max_source_length]\n        self.encoder_copy_inputs = \\\n            tf.concat(axis=1, values=[tf.expand_dims(x, 1) for x in encoder_copy_inputs])\n\n        print(""CopyCellWrapper added!"")\n\n    def __call__(self, input_embedding, state, scope=None):\n        output, state, alignments, attns = \\\n            self.cell(input_embedding, state, scope)\n\n        # Compute generation/copying mixture\n\n        # <generation probability, copying probability>\n        W, b = self.output_project\n        gen_logit = tf.matmul(output, W) + b\n        copy_logit = alignments[1]\n\n        prob = tf.nn.softmax(tf.concat([gen_logit, copy_logit], axis=1))\n        gen_prob = tf.slice(prob, [0, 0], [-1, self.tg_vocab_size])\n        copy_prob = tf.slice(prob, [0, self.tg_vocab_size], [-1, -1])\n        copy_vocab_prob = tf.squeeze(tf.matmul(tf.expand_dims(copy_prob, 1),\n            tf.one_hot(self.encoder_copy_inputs, depth=self.tg_vocab_size+copy_prob.get_shape()[1])), 1)\n\n        # mixture probability\n        mix_prob = tf.concat([gen_prob, tf.zeros(tf.shape(input=copy_prob))], 1) + \\\n                   copy_vocab_prob\n\n        return mix_prob, state, alignments, attns\n\n\nclass AttentionCellWrapper(tf.compat.v1.nn.rnn_cell.RNNCell):\n    def __init__(self, cell, attention_states, encoder_attn_masks,\n            attention_function, attention_input_keep, attention_output_keep,\n            num_heads, dim, num_layers, use_copy, tg_vocab_size=-1):\n        """"""\n        Hidden layer above attention states.\n\n        :param attention_states: 3D Tensor [batch_size x attn_length x attn_dim].\n        :param encoder_attn_masks: encoder input masks, used for masking out\n            padded contents in the encoder inputs\n        :param encoder_inputs: encoder input indices, used for generating masks\n            for copying probabilities\n        :param attention_input_keep: attention input state dropout\n        :param attention_output_keep: attention hidden state dropout\n        :param num_heads: Number of attention heads that read from from\n            attention_states. Dummy field if attention_states is None.\n        :param rnn_cell: Type of rnn cells used.\n        :param dim: Size of the hidden and output layers of the decoder, which\n            we assume to be the same.\n        :param num_layers: Number of layers in the RNN cells.\n\n        :param use_copy: Copy source tokens to the target.\n        :param copy_fun: Parameterization of the copying function.\n        """"""\n        if attention_input_keep < 1:\n            print(""attention input keep probability = {}""\n                  .format(attention_input_keep))\n            attention_states = tf.nn.dropout(\n                attention_states, 1 - (attention_input_keep))\n        attn_length = attention_states.get_shape()[1]\n        attn_dim = attention_states.get_shape()[2]\n\n        self.cell = cell\n        self.encoder_attn_masks = encoder_attn_masks\n        self.vocab_indices = tf.linalg.tensor_diag(tf.ones(tg_vocab_size))\n        self.num_heads = num_heads\n        self.dim = dim\n        self.num_layers = num_layers\n        self.attn_length = attn_length\n        self.attn_dim = attn_dim\n        self.attention_function = attention_function\n        self.attention_input_keep = attention_input_keep\n        self.attention_output_keep = attention_output_keep\n\n        hidden_features = []\n        with tf.compat.v1.variable_scope(""attention_cell_wrapper""):\n            for a in xrange(num_heads):\n                # [batch_size, attn_length, attn_dim]\n                hidden_features.append(attention_states)\n        self.hidden_features = hidden_features\n\n        self.use_copy = use_copy\n\n        print(""AttentionCellWrapper added!"")\n\n    def attention(self, state):\n        """"""Put attention masks on hidden using hidden_features and query.""""""\n        ds = []             # Results of attention reads will be stored here.\n        alignments = []     # Alignment values for each attention head.\n        if nest.is_sequence(state):  # If the query is a tuple, flatten it.\n            query_list = nest.flatten(state)\n            for q in query_list:  # Check that ndims == 2 if specified.\n                ndims = q.get_shape().ndims\n                if ndims:\n                    assert ndims == 2\n            state = tf.concat(axis=1, values=query_list)\n        for a in xrange(self.num_heads):\n            with tf.compat.v1.variable_scope(""Attention_%d"" % a):\n                y = tf.reshape(state, [-1, 1, 1, self.attn_dim])\n                # Attention mask is a softmax of v^T * tanh(...).\n                if self.attention_function == \'non-linear\':\n                    k = tf.compat.v1.get_variable(""AttnW_%d"" % a,\n                                        [1, 1, 2*self.attn_dim, self.attn_dim])\n                    l = tf.compat.v1.get_variable(""Attnl_%d"" % a,\n                                        [1, 1, 1, self.attn_dim])\n                    z = tf.reshape(self.hidden_features[a],\n                                   [-1, self.attn_length, 1, self.attn_dim])\n                    v = tf.concat(axis=3, values=[z, tf.tile(y, [1, self.attn_length, 1, 1])])\n                    s = tf.reduce_sum(\n                        input_tensor=l * tf.tanh(tf.nn.conv2d(input=v, filters=k, strides=[1,1,1,1], padding=""SAME"")), axis=[2, 3])\n                elif self.attention_function == \'inner_product\':\n                    s = tf.reduce_sum(input_tensor=tf.multiply(self.hidden_features[a], y), axis=[2])\n                else:\n                    raise NotImplementedError\n\n                # Apply attention masks\n                # [batch_size x attn_length]\n                s = s - (1 - self.encoder_attn_masks) * 1e12\n                if a == 0:\n                    alignment = tf.nn.softmax(s)    # normalized\n                    alignments.append(alignment)\n                    # Soft attention read\n                    d = tf.reduce_sum(\n                        input_tensor=tf.reshape(alignment, [-1, self.attn_length, 1])\n                            * self.hidden_features[a], axis=[1])\n                    # [batch_size, attn_dim]\n                    context = tf.reshape(d, [-1, self.attn_dim])\n                else:\n                    # Unnormalized\n                    alignments.append(s)    # unnormalized\n                    # [batch_size, attn_length, attn_dim]\n                    context = self.hidden_features[a]\n                ds.append(context)\n\n        return ds, alignments\n\n    def __call__(self, input_embedding, state, scope=None):\n        cell_output, state = self.cell(input_embedding, state, scope)\n        attns, alignments = self.attention(cell_output)\n\n        # with tf.variable_scope(""AttnStateProjection""):\n        #     attn_state = tf.nn.dropout(\n        #                     tf.tanh(tf.nn.rnn_cell.linear(\n        #                          [cell_output, attns[0]], self.dim, True)),\n        #                          self.attention_output_keep# )\n\n        with tf.compat.v1.variable_scope(""AttnOutputProjection""):\n            output = graph_utils.linear([cell_output, attns[0]], self.dim, True)\n\n        self.attention_cell_vars = True\n        return output, state, alignments, attns\n'"
encoder_decoder/encoder.py,24,"b'""""""A set of encoder modules used in the encoder-decoder framework.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport math\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom encoder_decoder import graph_utils\n\n\nclass Encoder(graph_utils.NNModel):\n    def __init__(self, hyperparameters, input_keep, output_keep):\n        super(Encoder, self).__init__(hyperparameters)\n\n        # variable reuse\n        self.char_embedding_vars = False\n        self.token_embedding_vars = False\n        self.char_rnn_vars = False\n\n        self.input_keep = input_keep\n        self.output_keep = output_keep\n\n        self.channels = []\n        self.dim = 0\n        if self.sc_token:\n            self.channels.append(\'token\')\n            self.dim += self.sc_token_dim\n        if self.sc_char:\n            self.channels.append(\'char\')\n            self.dim += self.sc_char_dim\n\n        assert(len(self.channels) > 0)\n\n    def token_representations(self, channel_inputs):\n        """"""\n        Generate token representations based on multi-channel input.\n\n        :param channel_inputs: an array of channel input indices\n            1. batch token indices\n            2. batch char indices\n        """"""\n        channel_embeddings = []\n        if self.sc_token:\n            token_embeddings = self.token_embeddings()\n            token_channel_embeddings = \\\n                [tf.nn.embedding_lookup(params=token_embeddings, ids=encoder_input)\n                 for encoder_input in channel_inputs[0]]\n            channel_embeddings.append(token_channel_embeddings)\n        if self.sc_char:\n            char_channel_embeddings = \\\n                self.char_channel_embeddings(channel_inputs[1])\n            channel_embeddings.append(char_channel_embeddings)\n        if len(channel_embeddings) == 1:\n            input_embeddings = channel_embeddings[0]\n        else:\n            input_embeddings = \\\n                [tf.concat(axis=1, values=[x, y]) for (x, y) in\n                    map(lambda x,y:(x,y), channel_embeddings[0],\n                        channel_embeddings[1])]\n        return input_embeddings\n\n    def token_embeddings(self):\n        """"""\n        Generate token representations by plain table look-up\n\n        :return: token embedding matrix [source_vocab_size, dim]\n        """"""\n        with tf.compat.v1.variable_scope(""encoder_token_embeddings"",\n                               reuse=self.token_embedding_vars):\n            vocab_size = self.source_vocab_size\n            print(""source token embedding size = {}"".format(vocab_size))\n            sqrt3 = math.sqrt(3)\n            initializer = tf.compat.v1.random_uniform_initializer(-sqrt3, sqrt3)\n            embeddings = tf.compat.v1.get_variable(""embedding"",\n                [vocab_size, self.sc_token_dim], initializer=initializer)\n            self.token_embedding_vars = True\n            return embeddings\n\n    def char_embeddings(self):\n        with tf.compat.v1.variable_scope(""encoder_char_embeddings"",\n                               reuse=self.char_embedding_vars):\n            sqrt3 = math.sqrt(3)\n            initializer = tf.compat.v1.random_uniform_initializer(-sqrt3, sqrt3)\n            embeddings = tf.compat.v1.get_variable(\n                ""embedding"", [self.source_char_vocab_size, self.sc_char_dim],\n                initializer=initializer)\n            self.char_embedding_vars = True\n            return embeddings\n\n    def token_channel_embeddings(self):\n        input = self.token_features()\n        return tf.nn.embedding_lookup(params=self.token_embeddings(), ids=input)\n\n    def char_channel_embeddings(self, channel_inputs):\n        """"""\n        Generate token representations by character composition.\n\n        :param channel_inputs: batch input char indices\n                [[batch, token_size], [batch, token_size], ...]\n        :return: embeddings_char [source_vocab_size, char_channel_dim]\n        """"""\n        inputs = [tf.squeeze(x, 1) for x in tf.split(axis=1,\n                  num_or_size_splits=self.max_source_token_size,\n                  value=tf.concat(axis=0, values=channel_inputs))]\n        input_embeddings = [tf.nn.embedding_lookup(params=self.char_embeddings(), ids=input) \n                            for input in inputs]\n        if self.sc_char_composition == \'rnn\':\n            with tf.compat.v1.variable_scope(""encoder_char_rnn"",\n                                   reuse=self.char_rnn_vars) as scope:\n                cell = graph_utils.create_multilayer_cell(\n                    self.sc_char_rnn_cell, scope,\n                    self.sc_char_dim, self.sc_char_rnn_num_layers,\n                    variational_recurrent=self.variational_recurrent_dropout)\n                rnn_outputs, rnn_states = graph_utils.RNNModel(cell, input_embeddings,\n                                                               dtype=tf.float32)\n                self.char_rnn_vars = True\n        else:\n            raise NotImplementedError\n\n        return [tf.squeeze(x, 0) for x in\n                tf.split(axis=0, num_or_size_splits=len(channel_inputs),\n                    value=tf.reshape(rnn_states[-1],\n                        [len(channel_inputs), -1, self.sc_char_dim]))]\n\n    def token_features(self):\n        return np.load(self.sc_token_features_path)\n\n    def token_char_index_matrix(self):\n        return np.load(self.sc_char_features_path)\n\n\nclass RNNEncoder(Encoder):\n    def __init__(self, hyperparameters, input_keep, output_keep):\n        super(RNNEncoder, self).__init__(\n            hyperparameters, input_keep, output_keep)\n        self.cell = self.encoder_cell()\n        self.output_dim = self.dim\n\n    def define_graph(self, encoder_channel_inputs, input_embeddings=None):\n        # Compute the continuous input representations\n        if input_embeddings is None:\n            input_embeddings = self.token_representations(encoder_channel_inputs)\n        with tf.compat.v1.variable_scope(""encoder_rnn""):\n            return graph_utils.RNNModel(self.cell, input_embeddings, dtype=tf.float32)\n\n    def encoder_cell(self):\n        """"""RNN cell for the encoder.""""""\n        with tf.compat.v1.variable_scope(""encoder_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(self.rnn_cell, scope,\n                self.dim, self.num_layers, self.input_keep, self.output_keep,\n                variational_recurrent=self.variational_recurrent_dropout)\n        return cell\n\n\nclass BiRNNEncoder(Encoder):\n    def __init__(self, hyperparameters, input_keep, output_keep):\n        super(BiRNNEncoder, self).__init__(\n            hyperparameters, input_keep, output_keep)\n        self.fw_cell = self.forward_cell()\n        self.bw_cell = self.backward_cell()\n        self.output_dim = 2 * self.dim\n        print(""encoder input dimension = {}"".format(self.dim))\n        print(""encoder output dimension = {}"".format(self.output_dim))\n\n    def define_graph(self, channel_inputs, input_embeddings=None):\n        # Each rnn in the bi-directional encoder have dimension which is half\n        # of that of the decoder.\n        # The hidden states of the two rnns are concatenated as the hidden\n        # states of the bi-directional encoder.\n\n        # Compute the continuous input representations\n        if input_embeddings is None:\n            input_embeddings = self.token_representations(channel_inputs)\n        with tf.compat.v1.variable_scope(""encoder_rnn""):\n            return graph_utils.BiRNNModel(self.fw_cell, self.bw_cell, input_embeddings,\n                num_cell_layers=self.num_layers, dtype=tf.float32)\n\n    def forward_cell(self):\n        """"""RNN cell for the forward RNN.""""""\n        with tf.compat.v1.variable_scope(""forward_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(self.rnn_cell, scope,\n                self.dim, self.num_layers, self.input_keep, self.output_keep,\n                variational_recurrent=self.variational_recurrent_dropout)\n        return cell\n\n    def backward_cell(self):\n        """"""RNN cell for the backward RNN.""""""\n        with tf.compat.v1.variable_scope(""backward_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(self.rnn_cell, scope,\n                self.dim, self.num_layers, self.input_keep, self.output_keep,\n                variational_recurrent=self.variational_recurrent_dropout)\n        return cell\n'"
encoder_decoder/framework.py,50,"b'""""""Encoder-decoder model with attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom encoder_decoder import data_utils, graph_utils\nfrom encoder_decoder.seq2seq import rnn_decoder\n\n\nDEBUG = False\n\n\nclass EncoderDecoderModel(graph_utils.NNModel):\n\n    def __init__(self, hyperparams, buckets=None):\n        """"""Create the model.\n        Hyperparameters:\n          source_vocab_size: size of the source vocabulary.\n          target_vocab_size: size of the target vocabulary.\n          buckets: a list of pairs (I, O), where I specifies maximum input length\n            that will be processed in that bucket, and O specifies maximum output\n            length. Training instances that have inputs longer than I or outputs\n            longer than O will be pushed to the next bucket and padded accordingly.\n            We assume that the list is sorted, e.g., [(2, 4), (8, 16)].e\n          size: number of units in each layer of the model.\n          num_layers: number of layers in the model.\n          max_gradient_norm: gradients will be clipped to maximally this norm.\n          batch_size: the size of the batches used during training;\n            the model construction is independent of batch_size, so it can be\n            changed after initialization if this is convenient, e.g., for decoding.\n          learning_rate: learning rate to start with.\n          learning_rate_decay_factor: decay learning rate by this much when needed.\n          use_lstm: if true, we use LSTM cells instead of GRU cells.\n          num_samples: number of samples for sampled softmax.\n          use_attention: if set, use attention model.\n        """"""\n        super(EncoderDecoderModel, self).__init__(hyperparams, buckets)\n        self.learning_rate = tf.Variable(\n            float(hyperparams[""learning_rate""]), trainable=False)\n        self.learning_rate_decay_op = self.learning_rate.assign(\n            self.learning_rate * hyperparams[""learning_rate_decay_factor""])\n\n        self.global_epoch = tf.Variable(0, trainable=False)\n\n        # Encoder.\n        self.define_encoder(self.sc_input_keep, self.sc_output_keep)\n\n        # Decoder.\n        decoder_embedding_dim = self.encoder.output_dim\n        decoder_dim = decoder_embedding_dim\n        self.define_decoder(decoder_dim, decoder_embedding_dim,\n                            self.tg_token_use_attention,\n                            self.tg_token_attn_fun,\n                            self.tg_input_keep,\n                            self.tg_output_keep)\n\n        # Character Decoder.\n        if self.tg_char:\n            self.define_char_decoder(self.decoder.dim, False,\n                self.tg_char_rnn_input_keep, self.tg_char_rnn_output_keep)\n\n        self.define_graph()\n\n    # --- Graph Operations --- #\n\n    def define_graph(self):\n        self.debug_vars = []\n\n        # Feeds for inputs.\n        self.encoder_inputs = []        # encoder inputs.\n        self.encoder_attn_masks = []    # mask out PAD symbols in the encoder\n        self.decoder_inputs = []        # decoder inputs (always start with ""_GO"").\n        self.targets = []               # decoder targets\n        self.target_weights = []        # weights at each position of the target sequence.\n        self.encoder_copy_inputs = []\n\n        for i in xrange(self.max_source_length):\n            self.encoder_inputs.append(\n                tf.compat.v1.placeholder(\n                    tf.int32, shape=[None], name=""encoder{0}"".format(i)))\n            self.encoder_attn_masks.append(\n                tf.compat.v1.placeholder(\n                    tf.float32, shape=[None], name=""attn_alignment{0}"".format(i)))\n\n        for j in xrange(self.max_target_length + 1):\n            self.decoder_inputs.append(\n                tf.compat.v1.placeholder(\n                    tf.int32, shape=[None], name=""decoder{0}"".format(j)))\n            self.target_weights.append(\n                tf.compat.v1.placeholder(\n                    tf.float32, shape=[None], name=""weight{0}"".format(j)))\n            # Our targets are decoder inputs shifted by one.\n            if j > 0 and not self.copynet:\n                self.targets.append(self.decoder_inputs[j])\n\n        if self.copynet:\n            for i in xrange(self.max_source_length):\n                self.encoder_copy_inputs.append(\n                    tf.compat.v1.placeholder(\n                        tf.int32, shape=[None], name=""encoder_copy{0}"".format(i)))\n            for j in xrange(self.max_target_length):\n                self.targets.append(\n                    tf.compat.v1.placeholder(\n                        tf.int32, shape=[None], name=""copy_target{0}"".format(i)))\n\n        # Compute training outputs and losses in the forward direction.\n        if self.buckets:\n            self.output_symbols = []\n            self.sequence_logits = []\n            self.losses = []\n            self.attn_alignments = []\n            self.encoder_hidden_states = []\n            self.decoder_hidden_states = []\n            if self.tg_char:\n                self.char_output_symbols = []\n                self.char_sequence_logits = []\n            if self.use_copy:\n                self.pointers = []\n            for bucket_id, bucket in enumerate(self.buckets):\n                with tf.compat.v1.variable_scope(tf.compat.v1.get_variable_scope(),\n                                       reuse=True if bucket_id > 0 else None):\n                    print(""creating bucket {} ({}, {})..."".format(\n                            bucket_id, bucket[0], bucket[1]))\n                    encode_decode_outputs = \\\n                        self.encode_decode(\n                            [self.encoder_inputs[:bucket[0]]],\n                            self.encoder_attn_masks[:bucket[0]],\n                            self.decoder_inputs[:bucket[1]],\n                            self.targets[:bucket[1]],\n                            self.target_weights[:bucket[1]],\n                            encoder_copy_inputs=self.encoder_copy_inputs[:bucket[0]]\n                        )\n                    self.output_symbols.append(encode_decode_outputs[\'output_symbols\'])\n                    self.sequence_logits.append(encode_decode_outputs[\'sequence_logits\'])\n                    self.losses.append(encode_decode_outputs[\'losses\'])\n                    self.attn_alignments.append(encode_decode_outputs[\'attn_alignments\'])\n                    self.encoder_hidden_states.append(\n                        encode_decode_outputs[\'encoder_hidden_states\'])\n                    self.decoder_hidden_states.append(\n                        encode_decode_outputs[\'decoder_hidden_states\'])\n                    if self.forward_only and self.tg_char:\n                         bucket_char_output_symbols = \\\n                             encode_decode_outputs[\'char_output_symbols\']\n                         bucket_char_sequence_logits =  \\\n                             encode_decode_outputs[\'char_sequence_logits\']\n                         self.char_output_symbols.append(\n                             tf.reshape(bucket_char_output_symbols,\n                                        [self.max_target_length,\n                                         self.batch_size, self.beam_size,\n                                         self.max_target_token_size + 1]))\n                         self.char_sequence_logits.append(\n                             tf.reshape(bucket_char_sequence_logits,\n                                        [self.max_target_length,\n                                        self.batch_size, self.beam_size]))\n                    if self.use_copy:\n                        self.pointers.append(encode_decode_outputs[\'pointers\'])\n        else:\n            encode_decode_outputs = self.encode_decode(\n                [self.encoder_inputs],\n                self.encoder_attn_masks,\n                self.decoder_inputs,\n                self.targets,\n                self.target_weights,\n                encoder_copy_inputs=self.encoder_copy_inputs\n            )\n            self.output_symbols = encode_decode_outputs[\'output_symbols\']\n            self.sequence_logits = encode_decode_outputs[\'sequence_logits\']\n            self.losses = encode_decode_outputs[\'losses\']\n            self.attn_alignments = encode_decode_outputs[\'attn_alignments\']\n            self.encoder_hidden_states = encode_decode_outputs[\'encoder_hidden_states\']\n            self.decoder_hidden_states = encode_decode_outputs[\'decoder_hidden_states\']\n            if self.tg_char:\n                char_output_symbols = encode_decode_outputs[\'char_output_symbols\']\n                char_sequence_logits = encode_decode_outputs[\'char_sequence_logits\']\n                self.char_output_symbols = tf.reshape(char_output_symbols,\n                                   [self.batch_size, self.beam_size,\n                                    self.max_target_length,\n                                    self.max_target_token_size])\n                self.char_sequence_logits = tf.reshape(char_sequence_logits,\n                                   [self.batch_size, self.beam_size,\n                                    self.max_target_length])\n            if self.use_copy:\n                self.pointers = encode_decode_outputs[\'pointers\']\n\n        # Gradients and SGD updates in the backward direction.\n        if not self.forward_only:\n            params = tf.compat.v1.trainable_variables()\n            if self.optimizer == ""sgd"":\n                opt = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate)\n            elif self.optimizer == ""adam"":\n                opt = tf.compat.v1.train.AdamOptimizer(\n                    self.learning_rate, beta1=0.9, beta2=0.999,\n                    epsilon=self.adam_epsilon, )\n            else:\n                raise ValueError(""Unrecognized optimizer type."")\n\n            if self.buckets:\n                self.gradient_norms = []\n                self.updates = []\n                for bucket_id, _ in enumerate(self.buckets):\n                    gradients = tf.gradients(ys=self.losses[bucket_id], xs=params)\n                    clipped_gradients, norm = tf.clip_by_global_norm(\n                        gradients, self.max_gradient_norm)\n                    self.gradient_norms.append(norm)\n                    self.updates.append(opt.apply_gradients(\n                        zip(clipped_gradients, params)))\n            else:\n                gradients = tf.gradients(ys=self.losses, xs=params)\n                clipped_gradients, norm = tf.clip_by_global_norm(\n                    gradients, self.max_gradient_norm)\n                self.gradient_norms = norm\n                self.updates = opt.apply_gradients(zip(clipped_gradients, params))\n\n        self.saver = tf.compat.v1.train.Saver(tf.compat.v1.global_variables())\n\n\n    def encode_decode(self, encoder_channel_inputs, encoder_attn_masks,\n                      decoder_inputs, targets, target_weights,\n                      encoder_copy_inputs=None):\n        bs_decoding = self.token_decoding_algorithm == \'beam_search\' \\\n            and self.forward_only\n\n        # --- Encode Step --- #\n        if bs_decoding:\n            targets = graph_utils.wrap_inputs(\n                self.decoder.beam_decoder, targets)\n            encoder_copy_inputs = graph_utils.wrap_inputs(\n                self.decoder.beam_decoder, encoder_copy_inputs)\n        encoder_outputs, encoder_states = \\\n            self.encoder.define_graph(encoder_channel_inputs)\n\n        # --- Decode Step --- #\n        if self.tg_token_use_attention:\n            attention_states = tf.concat(\n                [tf.reshape(m, [-1, 1, self.encoder.output_dim])\n                 for m in encoder_outputs], axis=1)\n        else:\n            attention_states = None\n        num_heads = 2 if (self.tg_token_use_attention and self.copynet) else 1\n\n        output_symbols, sequence_logits, output_logits, states, attn_alignments, \\\n            pointers = self.decoder.define_graph(\n                        encoder_states[-1], decoder_inputs,\n                        encoder_attn_masks=encoder_attn_masks,\n                        attention_states=attention_states,\n                        num_heads=num_heads,\n                        encoder_copy_inputs=encoder_copy_inputs)\n\n        # --- Compute Losses --- #\n        if not self.forward_only:\n            # A. Sequence Loss\n            if self.training_algorithm == ""standard"":\n                encoder_decoder_token_loss = self.sequence_loss(\n                    output_logits, targets, target_weights,\n                    graph_utils.sparse_cross_entropy)\n            elif self.training_algorithm == \'beam_search_opt\':\n                pass\n            else:\n                raise AttributeError(""Unrecognized training algorithm."")\n\n            # B. Attention Regularization\n            attention_reg = self.attention_regularization(attn_alignments) \\\n                if self.tg_token_use_attention else 0\n\n            # C. Character Sequence Loss\n            if self.tg_char:\n                # re-arrange character inputs\n                char_decoder_inputs = [\n                    tf.squeeze(x, 1) for x in tf.split(\n                        axis=1, num_or_size_splits=self.max_target_token_size + 2,\n                        value=tf.concat(axis=0, values=self.char_decoder_inputs))]\n                char_targets = [\n                    tf.squeeze(x, 1) for x in tf.split(\n                        axis=1, num_or_size_splits=self.max_target_token_size + 1,\n                        value=tf.concat(axis=0, values=self.char_targets))]\n                char_target_weights = [\n                    tf.squeeze(x, 1) for x in tf.split(\n                        axis=1, num_or_size_splits=self.max_target_token_size + 1,\n                        value=tf.concat(axis=0, values=self.char_target_weights))]\n                if bs_decoding:\n                    char_decoder_inputs = graph_utils.wrap_inputs(\n                        self.decoder.beam_decoder, char_decoder_inputs)\n                    char_targets = graph_utils.wrap_inputs(\n                        self.decoder.beam_decoder, char_targets)\n                    char_target_weights = graph_utils.wrap_inputs(\n                        self.decoder.beam_decoder, char_target_weights)\n                # get initial state from decoder output\n                char_decoder_init_state = \\\n                    tf.concat(axis=0, values=[tf.reshape(d_o, [-1, self.decoder.dim])\n                                              for d_o in states])\n                char_output_symbols, char_sequence_logits, char_output_logits, _, _ = \\\n                    self.char_decoder.define_graph(\n                        char_decoder_init_state, char_decoder_inputs)\n                encoder_decoder_char_loss = self.sequence_loss(\n                    char_output_logits, char_targets, char_target_weights,\n                    graph_utils.softmax_loss(\n                        self.char_decoder.output_project,\n                        self.tg_char_vocab_size / 2,\n                        self.tg_char_vocab_size))\n            else:\n                encoder_decoder_char_loss = 0\n\n            losses = encoder_decoder_token_loss + \\\n                     self.gamma * encoder_decoder_char_loss + \\\n                     self.beta * attention_reg\n        else:\n            losses = tf.zeros_like(decoder_inputs[0])\n\n        # --- Store encoder/decoder output states --- #\n        encoder_hidden_states = tf.concat(\n            axis=1, values=[tf.reshape(e_o, [-1, 1, self.encoder.output_dim])\n                            for e_o in encoder_outputs])\n        \n        top_states = []\n        if self.rnn_cell == \'gru\':\n            for state in states:\n                top_states.append(state[:, -self.decoder.dim:])\n        elif self.rnn_cell == \'lstm\':\n            for state in states:\n                if self.num_layers > 1:\n                    top_states.append(state[-1][1])\n                else:\n                    top_states.append(state[1])\n        decoder_hidden_states = tf.concat(axis=1,\n            values=[tf.reshape(d_o, [-1, 1, self.decoder.dim])\n                    for d_o in top_states])\n\n        O = {}\n        O[\'output_symbols\'] = output_symbols\n        O[\'sequence_logits\'] = sequence_logits\n        O[\'losses\'] = losses\n        O[\'attn_alignments\'] = attn_alignments\n        O[\'encoder_hidden_states\'] = encoder_hidden_states\n        O[\'decoder_hidden_states\'] = decoder_hidden_states\n        if self.tg_char:\n            O[\'char_output_symbols\'] = char_output_symbols\n            O[\'char_sequence_logits\'] = char_sequence_logits\n        if self.use_copy:\n            O[\'pointers\'] = pointers\n        return O\n\n\n    # Loss functions.\n    def sequence_loss(self, logits, targets, target_weights, loss_function):\n        assert(len(logits) == len(targets))\n        with tf.compat.v1.variable_scope(""sequence_loss""):\n            log_perp_list = []\n            for logit, target, weight in zip(logits, targets, target_weights):\n                crossent = loss_function(logit, target)\n                log_perp_list.append(crossent * weight)\n            log_perps = tf.add_n(log_perp_list)\n            total_size = tf.add_n(target_weights)\n            log_perps /= total_size\n\n        avg_log_perps = tf.reduce_mean(input_tensor=log_perps)\n\n        return avg_log_perps\n\n\n    def attention_regularization(self, attn_alignments):\n        """"""\n        Entropy regularization term.\n        :param attn_alignments: [batch_size, decoder_size, encoder_size]\n        """"""\n        P = tf.reduce_sum(input_tensor=attn_alignments, axis=1)\n        P_exp = tf.exp(P)\n        Z = tf.reduce_sum(input_tensor=P_exp, axis=1, keepdims=True)\n        return tf.reduce_mean(input_tensor=tf.reduce_sum(input_tensor=P_exp / Z * (P - tf.math.log(Z)), axis=1))\n\n\n    def define_encoder(self, input_keep, output_keep):\n        """"""Placeholder function.""""""\n        self.encoder = None\n\n\n    def define_decoder(self, dim, embedding_dim, use_attention,\n                       attention_function, input_keep, output_keep):\n        """"""Placeholder function.""""""\n        self.decoder = None\n\n\n    def define_char_decoder(self, dim, use_attention, input_keep, output_keep):\n        """"""\n        Define the decoder which does character-level generation of a token.\n        """"""\n        if self.tg_char_composition == \'rnn\':\n            self.char_decoder = rnn_decoder.RNNDecoder(self.hyperparams,\n                ""char_decoder"", self.tg_char_vocab_size, dim, use_attention,\n                input_keep, output_keep, self.char_decoding_algorithm)\n        else:\n            raise ValueError(""Unrecognized target character composition: {}.""\n                             .format(self.tg_char_composition))\n\n    # --- Graph Operations --- #\n\n    def format_batch(self, encoder_input_channels, decoder_input_channels, bucket_id=-1):\n        """"""\n        Convert the feature vectors into the dimensions required by the neural\n        network.\n        :param encoder_input_channels:\n            channel 0 - seq2seq encoder inputs\n            channel 1 - copynet encoder copy inputs\n        :param decoder_input_channels:\n            channel 0 - seq2seq decoder inputs\n            channel 1 - copynet decoder targets\n        """"""\n        def load_channel(inputs, output_length, reversed_output=True):\n            """"""\n            Convert a batch of feature vectors into a batched feature vector.\n            """"""\n            padded_inputs = []\n            batch_inputs = []\n            for batch_idx in xrange(batch_size):\n                input = inputs[batch_idx]\n                paddings = [data_utils.PAD_ID] * (output_length - len(input))\n                if reversed_output:\n                    padded_inputs.append(list(reversed(input + paddings)))\n                else:\n                    padded_inputs.append(input + paddings)\n            for length_idx in xrange(output_length):\n                batched_dim = np.array([padded_inputs[batch_idx][length_idx]\n                        for batch_idx in xrange(batch_size)], dtype=np.int32)\n                batch_inputs.append(batched_dim)\n            return batch_inputs\n\n        if bucket_id != -1:\n            encoder_size, decoder_size = self.buckets[bucket_id]\n        else:\n            encoder_size, decoder_size = \\\n                self.max_source_length, self.max_target_length\n        batch_size = len(encoder_input_channels[0])\n\n        # create batch-major vectors\n        batch_encoder_inputs = load_channel(\n            encoder_input_channels[0], encoder_size, reversed_output=True)\n        batch_decoder_inputs = load_channel(\n            decoder_input_channels[0], decoder_size, reversed_output=False)\n        if self.copynet:\n            batch_encoder_copy_inputs = load_channel(\n                encoder_input_channels[1], encoder_size, reversed_output=True)\n            batch_copy_targets = load_channel(\n                decoder_input_channels[1], decoder_size, reversed_output=False)\n\n        batch_encoder_input_masks = []\n        batch_decoder_input_masks = []\n        for length_idx in xrange(encoder_size):\n            batch_encoder_input_mask = np.ones(batch_size, dtype=np.float32)\n            for batch_idx in xrange(batch_size):\n                source = batch_encoder_inputs[length_idx][batch_idx]\n                if source == data_utils.PAD_ID:\n                    batch_encoder_input_mask[batch_idx] = 0.0\n            batch_encoder_input_masks.append(batch_encoder_input_mask)\n\n        for length_idx in xrange(decoder_size):\n            # Create target_weights to be 0 for targets that are padding.\n            batch_decoder_input_mask = np.ones(batch_size, dtype=np.float32)\n            for batch_idx in xrange(batch_size):\n                # We set weight to 0 if the corresponding target is a PAD symbol.\n                # The corresponding target is decoder_input shifted by 1 forward.\n                if length_idx < decoder_size - 1:\n                    target = batch_decoder_inputs[length_idx+1][batch_idx]\n                if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n                    batch_decoder_input_mask[batch_idx] = 0.0\n            batch_decoder_input_masks.append(batch_decoder_input_mask)\n\n        E = Example()\n        E.encoder_inputs = batch_encoder_inputs\n        E.encoder_attn_masks = batch_encoder_input_masks\n        E.decoder_inputs = batch_decoder_inputs\n        E.target_weights = batch_decoder_input_masks\n        if self.use_copy:\n            E.encoder_copy_inputs = batch_encoder_copy_inputs\n            E.copy_targets = batch_copy_targets\n\n        return E\n\n\n    def get_batch(self, data, bucket_id=-1, use_all=False):\n        """"""\n        Randomly sample a batch of examples from the specified bucket and\n        convert the feature vectors into the dimensions required by the neural\n        network.\n        """"""\n        encoder_inputs, decoder_inputs = [], []\n        if self.copynet:\n            encoder_copy_inputs, copy_targets = [], []\n\n        if bucket_id == -1:\n            sample_pool = data\n        else:\n            sample_pool = data[bucket_id]\n\n        # Randomly sample a batch of encoder and decoder inputs from data\n        data_ids = list(xrange(len(sample_pool)))\n        if not use_all:\n            data_ids = np.random.choice(data_ids, self.batch_size)\n        for i in data_ids:\n            data_point = sample_pool[i]\n            encoder_inputs.append(data_point.sc_ids)\n            decoder_inputs.append(data_point.tg_ids)\n            if self.copynet:\n                encoder_copy_inputs.append(data_point.csc_ids)\n                copy_targets.append(data_point.ctg_ids)\n\n        encoder_input_channels = [encoder_inputs]\n        decoder_input_channels = [decoder_inputs]\n        if self.copynet:\n            encoder_input_channels.append(encoder_copy_inputs)\n            decoder_input_channels.append(copy_targets)\n\n        return self.format_batch(\n            encoder_input_channels, decoder_input_channels, bucket_id=bucket_id)\n\n\n    def feed_input(self, E):\n        """"""\n        Assign the data vectors to the corresponding neural network variables.\n        """"""\n        encoder_size, decoder_size = len(E.encoder_inputs), len(E.decoder_inputs)\n        input_feed = {}\n        for l in xrange(encoder_size):\n            input_feed[self.encoder_inputs[l].name] = E.encoder_inputs[l]\n            input_feed[self.encoder_attn_masks[l].name] = E.encoder_attn_masks[l]\n        for l in xrange(decoder_size):\n            input_feed[self.decoder_inputs[l].name] = E.decoder_inputs[l]\n            input_feed[self.target_weights[l].name] = E.target_weights[l]\n        if self.copynet:\n            for l in xrange(encoder_size):\n                input_feed[self.encoder_copy_inputs[l].name] = \\\n                    E.encoder_copy_inputs[l]\n            for l in xrange(decoder_size-1):\n                input_feed[self.targets[l].name] = E.copy_targets[l]\n\n        # Apply dummy values to encoder and decoder inputs\n        for l in xrange(encoder_size, self.max_source_length):\n            input_feed[self.encoder_inputs[l].name] = np.zeros(\n                E.encoder_inputs[-1].shape, dtype=np.int32)\n            input_feed[self.encoder_attn_masks[l].name] = np.zeros(\n                E.encoder_attn_masks[-1].shape, dtype=np.int32)\n            if self.copynet:\n                input_feed[self.encoder_copy_inputs[l].name] = \\\n                    np.zeros(E.encoder_copy_inputs[-1].shape, dtype=np.int32)\n        for l in xrange(decoder_size, self.max_target_length + 1):\n            input_feed[self.decoder_inputs[l].name] = np.zeros(\n                E.decoder_inputs[-1].shape, dtype=np.int32)\n            input_feed[self.target_weights[l].name] = np.zeros(\n                E.target_weights[-1].shape, dtype=np.int32)\n            if self.copynet:\n                input_feed[self.targets[l-1].name] = np.zeros(\n                    E.copy_targets[-1].shape, dtype=np.int32)\n        \n        return input_feed\n\n\n    def step(self, session, formatted_example, bucket_id=-1, forward_only=False):\n        """"""Run a step of the model feeding the given inputs.\n        :param session: tensorflow session to use.\n        :param encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n        :param attn_alignments: list of numpy int vectors to feed as the mask\n            over inputs about which tokens to attend to.\n        :param decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n        :param target_weights: list of numpy float vectors to feed as target weights.\n        :param bucket_id: which bucket of the model to use.\n        :param forward_only: whether to do the backward step or only forward.\n        :param return_rnn_hidden_states: if set to True, return the hidden states\n            of the two RNNs.\n        :return (gradient_norm, average_perplexity, outputs)\n        """"""\n\n        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n        input_feed = self.feed_input(formatted_example)\n\n        # Output feed: depends on whether we do a backward step or not.\n        if not forward_only:\n            if bucket_id == -1:\n                output_feed = {\n                    \'updates\': self.updates,                    # Update Op that does SGD.\n                    \'gradient_norms\': self.gradient_norms,      # Gradient norm.\n                    \'losses\': self.losses}                      # Loss for this batch.\n            else:\n                output_feed = {\n                    \'updates\': self.updates[bucket_id],         # Update Op that does SGD.\n                    \'gradient_norms\': self.gradient_norms[bucket_id],  # Gradient norm.\n                    \'losses\': self.losses[bucket_id]}           # Loss for this batch.\n        else:\n            if bucket_id == -1:\n                output_feed = {\n                    \'output_symbols\': self.output_symbols,      # Loss for this batch.\n                    \'sequence_logits\': self.sequence_logits,        # Batch output sequence\n                    \'losses\': self.losses}                      # Batch output scores\n            else:\n                output_feed = {\n                    \'output_symbols\': self.output_symbols[bucket_id], # Loss for this batch.\n                    \'sequence_logits\': self.sequence_logits[bucket_id],   # Batch output sequence\n                    \'losses\': self.losses[bucket_id]}           # Batch output logits\n\n        if self.tg_token_use_attention:\n            if bucket_id == -1:\n                output_feed[\'attn_alignments\'] = self.attn_alignments\n            else:\n                output_feed[\'attn_alignments\'] = self.attn_alignments[bucket_id]\n\n        if bucket_id != -1:\n            assert(isinstance(self.encoder_hidden_states, list))\n            assert(isinstance(self.decoder_hidden_states, list))\n            output_feed[\'encoder_hidden_states\'] = \\\n                self.encoder_hidden_states[bucket_id]\n            output_feed[\'decoder_hidden_states\'] = \\\n                self.decoder_hidden_states[bucket_id]\n        else:\n            output_feed[\'encoder_hidden_states\'] = self.encoder_hidden_states\n            output_feed[\'decoder_hidden_states\'] = self.decoder_hidden_states\n\n        if self.use_copy:\n            output_feed[\'pointers\'] = self.pointers\n\n        extra_update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n        if extra_update_ops and not forward_only:\n            outputs, extra_updates = session.run(\n                [output_feed, extra_update_ops], input_feed)\n        else:\n            outputs = session.run(output_feed, input_feed)\n\n        O = Output()\n        if not forward_only:\n            # Gradient norm, loss, no outputs\n            O.gradient_norms = outputs[\'gradient_norms\']\n            O.losses = outputs[\'losses\']\n        else:\n            # No gradient loss, output_symbols, sequence_logits\n            O.output_symbols = outputs[\'output_symbols\']\n            O.sequence_logits = outputs[\'sequence_logits\']\n            O.losses = outputs[\'losses\']\n        # [attention_masks]\n        if self.tg_token_use_attention:\n            O.attn_alignments = outputs[\'attn_alignments\']\n\n        O.encoder_hidden_states = outputs[\'encoder_hidden_states\']\n        O.decoder_hidden_states = outputs[\'decoder_hidden_states\']\n\n        if self.use_copy:\n            O.pointers = outputs[\'pointers\']\n\n        return O\n\n\nclass Example(object):\n    """"""\n    Input data to the neural network (batched when mini-batch training is used).\n    """"""\n    def __init__(self):\n        self.encoder_inputs = None\n        self.encoder_attn_masks = None\n        self.decoder_inputs = None\n        self.target_weights = None\n        self.encoder_copy_inputs = None     # Copynet\n        self.copy_targets = None            # Copynet\n\n\nclass Output(object):\n    """"""\n    Data output from the neural network (batched when mini-batch training is used).\n    """"""\n    def __init__(self):\n        self.updates = None\n        self.gradient_norms = None\n        self.losses = None\n        self.output_symbols = None\n        self.sequence_logits = None\n        self.attn_alignments = None\n        self.encoder_hidden_states = None\n        self.decoder_hidden_states = None\n        self.pointers = None\n'"
encoder_decoder/graph_utils.py,50,"b'""""""Utility functions related to graph construction.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\n\ndef define_model(FLAGS, session, model_constructor, buckets, forward_only):\n    params = collections.defaultdict()\n\n    params[""source_vocab_size""] = FLAGS.sc_vocab_size\n    params[""target_vocab_size""] = FLAGS.tg_vocab_size\n    params[""max_source_length""] = FLAGS.max_sc_length\n    params[""max_target_length""] = FLAGS.max_tg_length\n    params[""max_source_token_size""] = FLAGS.max_sc_token_size\n    params[""max_target_token_size""] = FLAGS.max_tg_token_size\n    params[""rnn_cell""] = FLAGS.rnn_cell\n    params[""batch_size""] = FLAGS.batch_size\n    params[""num_layers""] = FLAGS.num_layers\n    params[""num_samples""] = FLAGS.num_samples\n    params[""max_gradient_norm""] = FLAGS.max_gradient_norm\n    params[""variational_recurrent_dropout""] = \\\n        FLAGS.variational_recurrent_dropout\n\n    params[""recurrent_batch_normalization""] = \\\n        FLAGS.recurrent_batch_normalization\n    params[""gramma_c""] = FLAGS.gamma_c\n    params[""beta_c""] = FLAGS.beta_c\n    params[""gramma_h""] = FLAGS.gamma_h\n    params[""beta_h""] = FLAGS.beta_h\n    params[""gramma_x""] = FLAGS.gamma_x\n    params[""beta_x""] = FLAGS.beta_x\n\n    params[""tg_token_use_attention""] = FLAGS.tg_token_use_attention\n\n    params[""sc_token""] = FLAGS.sc_token\n    params[""sc_token_dim""] = FLAGS.sc_token_dim\n    params[""sc_char""] = FLAGS.sc_char\n    # params[""sc_char_vocab_size""] = FLAGS.sc_char_vocab_size\n    # params[""sc_char_dim""] = FLAGS.sc_char_dim\n    # params[""sc_char_composition""] = FLAGS.sc_char_composition\n    # params[""sc_char_rnn_cell""] = FLAGS.sc_char_rnn_cell\n    # params[""sc_char_rnn_num_layers""] = FLAGS.sc_char_rnn_num_layers\n    # params[""sc_token_features_path""] = os.path.join(\n    #     FLAGS.data_dir, ""{}.vocab.token.feature.npy"".format(source))\n    # params[""sc_char_features_path""] = os.path.join(\n    #     FLAGS.data_dir, ""{}.vocab.char.feature.npy"".format(source))\n\n    params[""tg_token""] = FLAGS.tg_token\n    params[""tg_char""] = FLAGS.tg_char\n    # params[""tg_char_vocab_size""] = FLAGS.tg_char_vocab_size\n    # params[""tg_char_composition""] = FLAGS.tg_char_composition\n    # params[""tg_char_use_attention""] = FLAGS.tg_char_use_attention\n    # params[""tg_char_rnn_cell""] = FLAGS.tg_char_rnn_cell\n    # params[""tg_char_rnn_num_layers""] = FLAGS.tg_char_rnn_num_layers\n    # params[""tg_char_rnn_input_keep""] = FLAGS.tg_char_rnn_input_keep\n    # params[""tg_char_rnn_output_keep""] = FLAGS.tg_char_rnn_output_keep\n    # params[""tg_token_features_path""] = os.path.join(\n    #     FLAGS.data_dir, ""{}.vocab.token.feature.npy"".format(target))\n    # params[""tg_char_features_path""] = os.path.join(\n    #     FLAGS.data_dir, ""{}.vocab.char.feature.npy"".format(target))\n\n    params[""gamma""] = FLAGS.gamma\n\n    params[""optimizer""] = FLAGS.optimizer\n    params[""learning_rate""] = FLAGS.learning_rate\n    params[""learning_rate_decay_factor""] = FLAGS.learning_rate_decay_factor\n    params[""adam_epsilon""] = FLAGS.adam_epsilon\n\n    params[""steps_per_epoch""] = FLAGS.steps_per_epoch\n    params[""num_epochs""] = FLAGS.num_epochs\n\n    params[""training_algorithm""] = FLAGS.training_algorithm\n    if FLAGS.training_algorithm == ""bso"":\n        assert(FLAGS.token_decoding_algorithm == ""beam_search"")\n    params[""margin""] = FLAGS.margin\n\n    params[""use_copy""] = FLAGS.use_copy\n    params[""copy_fun""] = FLAGS.copy_fun\n    params[""chi""] = FLAGS.chi\n\n    params[""tg_token_attn_fun""] = FLAGS.tg_token_attn_fun\n    params[""beta""] = FLAGS.beta\n\n    params[""encoder_topology""] = FLAGS.encoder_topology\n    params[""decoder_topology""] = FLAGS.decoder_topology\n\n    params[""sc_input_keep""] = FLAGS.sc_input_keep\n    params[""sc_output_keep""] = FLAGS.sc_output_keep\n    params[""tg_input_keep""] = FLAGS.tg_input_keep\n    params[""tg_output_keep""] = FLAGS.tg_output_keep\n    params[""attention_input_keep""] = FLAGS.attention_input_keep\n    params[""attention_output_keep""] = FLAGS.attention_output_keep\n\n    params[""token_decoding_algorithm""] = FLAGS.token_decoding_algorithm\n    params[""char_decoding_algorithm""] = FLAGS.char_decoding_algorithm\n    params[""beam_size""] = FLAGS.beam_size\n    params[""alpha""] = FLAGS.alpha\n    params[""top_k""] = FLAGS.top_k\n\n    params[""forward_only""] = forward_only\n    params[""force_reading_input""] = FLAGS.force_reading_input\n\n    # construct model directory\n    model_subdir, decode_sig = get_decode_signature(FLAGS)\n    FLAGS.model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n    params[""model_dir""] = FLAGS.model_dir\n    params[""decode_sig""] = decode_sig\n    print(""model_dir={}"".format(FLAGS.model_dir))\n    print(""decode_sig={}"".format(decode_sig))\n\n    if forward_only:\n        # Set batch_size to 1 for decoding.\n        params[""batch_size""] = 1\n        # Reset dropout probabilities for decoding.\n        params[""attention_input_keep""] = 1.0\n        params[""attention_output_keep""] = 1.0\n        params[""sc_input_keep""] = 1.0\n        params[""sc_output_keep""] = 1.0\n        params[""tg_input_keep""] = 1.0\n        params[""tg_output_keep""] = 1.0\n\n    if FLAGS.gen_slot_filling_training_data:\n        FLAGS.batch_size = 1\n        params[""batch_size""] = 1\n        FLAGS.beam_size = 1\n        params[""beam_size""] = 1\n        FLAGS.learning_rate = 0\n        params[""learning_rate""] = 0\n        params[""force_reading_input""] = True\n        params[""create_fresh_params""] = False\n\n    if FLAGS.explain:\n        FLAGS.grammatical_only = False\n\n    model = model_constructor(params, buckets)\n    if forward_only or FLAGS.gen_slot_filling_training_data or \\\n            not FLAGS.create_fresh_params:\n        ckpt = tf.train.get_checkpoint_state(\n            os.path.join(FLAGS.model_root_dir, FLAGS.model_dir))\n        print(""Reading model parameters from %s"" % ckpt.model_checkpoint_path)\n        model.saver.restore(session, ckpt.model_checkpoint_path)\n    else:\n        if not os.path.exists(FLAGS.model_dir):\n            print(""Making model_dir..."")\n            os.mkdir(FLAGS.model_dir)\n        # else:\n        #     clean_dir(FLAGS.model_dir)\n        if FLAGS.pretrained_model_subdir:\n            # load pre-trained parameteres for advanced training algorithms\n            pretrain_dir = os.path.join(\n                FLAGS.model_root_dir, FLAGS.pretrained_model_subdir)\n            print(""Initialize the graph with pre-trained parameters from {}""\n                  .format(pretrain_dir))\n            pretrain_ckpt = tf.train.get_checkpoint_state(pretrain_dir)\n            model.saver.restore(session, pretrain_ckpt.model_checkpoint_path)\n            session.run(model.learning_rate.assign(\n                tf.constant(FLAGS.learning_rate)))\n        else:\n            print(""Initialize the graph with random parameters."")\n            session.run(tf.compat.v1.global_variables_initializer())\n\n    return model\n\n\ndef get_decode_signature(FLAGS):\n    """"""\n    Model signature is used to locate the trained parameters and\n    prediction results of a particular model.\n    """"""\n\n    # The model directory is stamped with training hyperparameter information.\n    model_subdir = FLAGS.dataset\n    if FLAGS.explain:\n        model_subdir += \'-expl\'\n    if FLAGS.channel == \'char\':\n        model_subdir += \'--char\'\n    elif FLAGS.channel == \'partial.token\':\n        model_subdir += \'--partial\'\n    else:\n        if FLAGS.sc_token:\n            model_subdir += \'-T\'\n        if FLAGS.sc_char:\n            model_subdir += \'-C\'\n        if FLAGS.tg_char:\n            model_subdir += \'-TC\'\n            model_subdir += \'-{:.1f}\'.format(FLAGS.gamma)\n    model_subdir += \'-{}\'.format(FLAGS.min_vocab_frequency)\n    model_subdir += \'-{}\'.format(FLAGS.encoder_topology)\n    model_subdir += \'-{}\'.format(FLAGS.rnn_cell)\n    model_subdir += \'-{}\'.format(FLAGS.training_algorithm)\n    if FLAGS.tg_token_use_attention:\n        model_subdir += \'-attention\'\n        model_subdir += \'-{}\'.format(FLAGS.attention_input_keep)\n        model_subdir += \'-{}\'.format(FLAGS.attention_output_keep)\n        model_subdir += \'-{:.1f}\'.format(FLAGS.beta)\n    if FLAGS.use_copy:\n        model_subdir += \'-copy\'\n        model_subdir += \'-{:.1f}\'.format(FLAGS.chi)\n    model_subdir += \'-{}\'.format(FLAGS.batch_size)\n    if FLAGS.sc_token:\n        model_subdir += \'-{}\'.format(FLAGS.sc_token_dim)\n    if FLAGS.sc_char:\n        model_subdir += \'-{}\'.format(FLAGS.sc_char_dim)\n    model_subdir += \'-{}\'.format(FLAGS.num_layers)\n    if FLAGS.recurrent_batch_normalization:\n        model_subdir += \'-rbc\'\n    model_subdir += \'-{}\'.format(FLAGS.learning_rate)\n    model_subdir += \'-{}\'.format(FLAGS.adam_epsilon)\n    model_subdir += \'-{}\'.format(FLAGS.sc_input_keep)\n    model_subdir += \'-{}\'.format(FLAGS.sc_output_keep)\n    model_subdir += \'-{}\'.format(FLAGS.tg_input_keep)\n    model_subdir += \'-{}\'.format(FLAGS.tg_output_keep)\n    if FLAGS.canonical:\n        model_subdir += \'.canonical\'\n    elif FLAGS.normalized:\n        model_subdir += \'.normalized\'\n\n    # The prediction file of a particular model is stamped with decoding \n    # hyperparameter information.\n    decode_sig = FLAGS.token_decoding_algorithm\n    if FLAGS.token_decoding_algorithm == \'beam_search\': \n        decode_sig += "".{}"".format(FLAGS.beam_size)\n    if FLAGS.fill_argument_slots:\n        decode_sig += \'.slot.filler\'\n    decode_sig += ("".test"" if FLAGS.test else "".dev"")\n    return model_subdir, decode_sig\n\n\ndef clean_dir(dir):\n    for f_name in os.listdir(dir):\n        if f_name.startswith(\'prediction\'):\n            continue\n        f_path = os.path.join(dir, f_name)\n        try:\n            if os.path.isfile(f_path):\n                os.unlink(f_path)\n        except Exception as e:\n            print(e)\n\n\ndef softmax_loss(output_project, num_samples, target_vocab_size):\n    w, b = output_project\n    if num_samples > 0 and num_samples < target_vocab_size:\n        print(""loss function = sampled_softmax_loss ({})"".format(num_samples))\n        w_t = tf.transpose(a=w)\n        def sampled_loss(outputs, labels):\n            labels = tf.reshape(labels, [-1, 1])\n            return tf.nn.sampled_softmax_loss(\n                w_t, b, labels, outputs, num_samples, target_vocab_size)\n        loss_function = sampled_loss\n    else:\n        print(""loss function = softmax_loss"")\n        def loss(outputs, labels):\n            logits = tf.matmul(outputs, w) + b\n            return tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels)\n        loss_function = loss\n    return loss_function\n\n\ndef wrap_inputs(beam_decoder, inputs):\n    return [beam_decoder.wrap_input(input) for input in inputs]\n\n\ndef sparse_cross_entropy(logits, targets):\n    return -tf.reduce_sum(input_tensor=logits * tf.one_hot(targets, logits.get_shape()[1]), axis=1)\n\n\ndef nest_map(func, nested):\n    """"""\n    Apply function to each element in a nested list.\n\n    :param func: The function to apply.\n    :param nested: The nested list to which the function is going to be applied.\n\n    :return: A list with the same structue as nested where the each element\n        is the output of applying func to the corresponding element in nest.\n    """"""\n    if not nest.is_sequence(nested):\n        return func(nested)\n    flat = nest.flatten(nested)\n    return nest.pack_sequence_as(nested, list(map(func, flat)))\n\n\ndef nest_map_dual(func, nested1, nested2):\n    if not nest.is_sequence(nested1):\n        return func(nested1, nested2)\n    flat1 = nest.flatten(nested1)\n    flat2 = nest.flatten(nested2)\n    output = [func(x, y) for x, y in zip(flat1, flat2)]\n    return nest.pack_sequence_as(nested1, list(output))\n\n\ndef create_multilayer_cell(rnn_cell, scope, dim, num_layers, input_keep_prob=1,\n                           output_keep_prob=1, variational_recurrent=True, input_dim=-1):\n    """"""\n    Create the multi-layer RNN cell.\n    :param type: Type of RNN cell.\n    :param scope: Variable scope.\n    :param dim: Dimension of hidden layers.\n    :param num_layers: Number of layers of cells.\n    :param input_keep_prob: Proportion of input to keep in dropout.\n    :param output_keep_prob: Proportion of output to keep in dropout.\n    :param variational_recurrent: If set, use variational recurrent dropout.\n        (cf. https://arxiv.org/abs/1512.05287)\n    :param input_dim: RNN input dimension, must be specified if it is\n        different from the cell state dimension.\n    :param batch_normalization: If set, use recurrent batch normalization.\n        (cf. https://arxiv.org/abs/1603.09025)\n    :param forward_only: If batch_normalization is set, inform the cell about\n        the batch normalization process.\n    :return: RNN cell as specified.\n    """"""\n    with tf.compat.v1.variable_scope(scope):\n        if rnn_cell == ""lstm"":\n            cell = tf.compat.v1.nn.rnn_cell.LSTMCell(dim, state_is_tuple=True)\n        elif rnn_cell == ""gru"":\n            cell = tf.compat.v1.nn.rnn_cell.GRUCell(dim)\n        else:\n            raise ValueError(""Unrecognized RNN cell type: {}."".format(type))\n\n        assert(input_keep_prob >= 0 and output_keep_prob >= 0)\n        if input_keep_prob < 1 or output_keep_prob < 1:\n            if input_dim == -1:\n                input_dim = dim\n            print(""-- rnn dropout input keep probability: {}"".format(input_keep_prob))\n            print(""-- rnn dropout output keep probability: {}"".format(output_keep_prob))\n            if variational_recurrent:\n                print(""-- using variational dropout"")\n            cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell,\n                input_keep_prob=input_keep_prob,\n                output_keep_prob=output_keep_prob,\n                variational_recurrent=variational_recurrent,\n                input_size=input_dim, dtype=tf.float32)\n\n        if num_layers > 1:\n            cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n                [cell] * num_layers, state_is_tuple=(rnn_cell==""lstm""))\n    return cell\n\n\ndef BiRNNModel(cell_fw, cell_bw, inputs, initial_state_fw=None,\n               initial_state_bw=None, dtype=None, sequence_length=None,\n               num_cell_layers=None, scope=None):\n  """"""Creates a bidirectional recurrent neural network.\n\n  Similar to the unidirectional case above (rnn) but takes input and builds\n  independent forward and backward RNNs with the final forward and backward\n  outputs depth-concatenated, such that the output will have the format\n  [time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of\n  forward and backward cell must match. The initial state for both directions\n  is zero by default (but can be set optionally) and no intermediate states are\n  ever returned -- the network is fully unrolled for the given (passed in)\n  length(s) of the sequence(s) or completely unrolled if length(s) is not given.\n\n  Args:\n    cell_fw: An instance of RNNCell, to be used for forward direction.\n    cell_bw: An instance of RNNCell, to be used for backward direction.\n    inputs: A length T list of inputs, each a tensor of shape\n      [batch_size, input_size].\n    initial_state_fw: (optional) An initial state for the forward RNN.\n      This must be a tensor of appropriate type and shape\n      `[batch_size x cell_fw.state_size]`.\n      If `cell_fw.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell_fw.state_size`.\n    initial_state_bw: (optional) Same as for `initial_state_fw`, but using\n      the corresponding properties of `cell_bw`.\n    dtype: (optional) The data type for the initial state.  Required if\n      either of the initial states are not provided.\n    sequence_length: (optional) An int32/int64 vector, size `[batch_size]`,\n      containing the actual lengths for each of the sequences.\n    num_cell_layers: Num of layers of the RNN cell. (Mainly used for generating\n      output state representations for multi-layer RNN cells.)\n    scope: VariableScope for the created subgraph; defaults to ""BiRNN""\n\n  Returns:\n    A tuple (outputs, output_states) where:\n      outputs is a length `T` list of outputs (one for each input), which\n        are depth-concatenated forward and backward outputs.\n      output_states is a length `T` list of hidden states (one for each step),\n        which are depth-concatenated forward and backward states.\n\n  Raises:\n    TypeError: If `cell_fw` or `cell_bw` is not an instance of `RNNCell`.\n    ValueError: If inputs is None or an empty list.\n  """"""\n\n  if not isinstance(cell_fw, tf.compat.v1.nn.rnn_cell.RNNCell):\n    raise TypeError(""cell_fw must be an instance of RNNCell"")\n  if not isinstance(cell_bw, tf.compat.v1.nn.rnn_cell.RNNCell):\n    raise TypeError(""cell_bw must be an instance of RNNCell"")\n  if not isinstance(inputs, list):\n    raise TypeError(""inputs must be a list"")\n  if not inputs:\n    raise ValueError(""inputs must not be empty"")\n\n  name = scope or ""BiRNN""\n  # Forward direction\n  with tf.compat.v1.variable_scope(name + ""_FW"") as fw_scope:\n    output_fw, states_fw = RNNModel(cell_fw, inputs, initial_state_fw, dtype,\n                                    sequence_length, scope=fw_scope)\n\n  # Backward direction\n  with tf.compat.v1.variable_scope(name + ""_BW"") as bw_scope:\n    tmp, tmp_states = RNNModel(cell_bw, _reverse_seq(inputs, sequence_length),\n      initial_state_bw, dtype, sequence_length, scope=bw_scope)\n  output_bw = _reverse_seq(tmp, sequence_length)\n  states_bw = _reverse_seq(tmp_states, sequence_length)\n\n  # Concat each of the forward/backward outputs\n  outputs = [tf.concat(axis=1, values=[fw, bw]) for fw, bw in zip(output_fw, output_bw)]\n\n  # Notice that the computation of the encoder final state uses the final state\n  # of the backward RNN without reverse!!!\n  if nest.is_sequence(cell_fw.state_size):\n    output_states = [nest_map_dual(lambda x, y: tf.concat(axis=1, values=[x, y]), fw, bw)\n                     for fw, bw in zip(states_fw, tmp_states)]\n  else:\n    if num_cell_layers > 1:\n      output_states = []\n      for fw, bw in zip(states_fw, tmp_states):\n        output_states.append(tf.concat(axis=1, values=[tf.concat(axis=1, values=[l_fw, l_bw])\n          for l_fw, l_bw in zip(tf.split(axis=1, num_or_size_splits=num_cell_layers, value=fw),\n            tf.split(axis=1, num_or_size_splits=num_cell_layers, value=bw))]))\n    else:\n      output_states = [tf.concat(axis=1, values=[fw, bw])\n                       for fw, bw in zip(states_fw, tmp_states)]\n\n  return (outputs, output_states)\n\n\ndef _reverse_seq(input_seq, lengths):\n  """"""Reverse a list of Tensors up to specified lengths.\n\n  Args:\n    input_seq: Sequence of seq_len tensors of dimension (batch_size, depth)\n    lengths:   A tensor of dimension batch_size, containing lengths for each\n               sequence in the batch. If ""None"" is specified, simply reverses\n               the list.\n\n  Returns:\n    time-reversed sequence\n  """"""\n  if lengths is None:\n    return list(reversed(input_seq))\n\n  input_shape = tf.tensor_shape.matrix(None, None)\n  for input_ in input_seq:\n    input_shape.merge_with(input_.get_shape())\n    input_.set_shape(input_shape)\n\n  # Join into (time, batch_size, depth)\n  s_joined = tf.stack(input_seq)\n\n  # TODO(schuster, ebrevdo): Remove cast when reverse_sequence takes int32\n  if lengths is not None:\n    lengths = tf.cast(lengths, dtype=tf.int64)\n\n  # Reverse along dimension 0\n  s_reversed = tf.reverse_sequence(input=s_joined, seq_lengths=lengths, seq_axis=0, batch_axis=1)\n  # Split again into list\n  result = tf.unstack(s_reversed)\n  for r in result:\n    r.set_shape(input_shape)\n  return result\n\n\ndef RNNModel(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None):\n  """"""Creates a recurrent neural network specified by RNNCell `cell`.\n\n  The simplest form of RNN network generated is:\n    state = cell.zero_state(...)\n    outputs = []\n    for input_ in inputs:\n      output, state = cell(input_, state)\n      outputs.append(output)\n    return (outputs, state)\n\n  However, a few other options are available:\n\n  An initial state can be provided.\n  If the sequence_length vector is provided, dynamic calculation is performed.\n  This method of calculation does not compute the RNN steps past the maximum\n  sequence length of the minibatch (thus saving computational time),\n  and properly propagates the state at an example\'s sequence length\n  to the final state output.\n\n  The dynamic calculation performed is, at time t for batch row b,\n    (output, state)(b, t) =\n      (t >= sequence_length(b))\n        ? (zeros(cell.output_size), states(b, sequence_length(b) - 1))\n        : cell(input(b, t), state(b, t - 1))\n\n  Args:\n    cell: An instance of RNNCell.\n    inputs: A length T list of inputs, each a tensor of shape\n      [batch_size, input_size].\n    initial_state: (optional) An initial state for the RNN.\n      If `cell.state_size` is an integer, this must be\n      a tensor of appropriate type and shape `[batch_size x cell.state_size]`.\n      If `cell.state_size` is a tuple, this should be a tuple of\n      tensors having shapes `[batch_size, s] for s in cell.state_size`.\n    dtype: (optional) The data type for the initial state.  Required if\n      initial_state is not provided.\n    sequence_length: Specifies the length of each sequence in inputs.\n      An int32 or int64 vector (tensor) size `[batch_size]`, values in `[0, T)`.\n    num_cell_layers: Num of layers of the RNN cell. (Mainly used for generating\n      output state representations for multi-layer RNN cells.)\n    scope: VariableScope for the created subgraph; defaults to ""RNN"".\n\n  Returns:\n    A pair (outputs, state) where:\n      - outputs is a length T list of outputs (one for each step)\n      - states is a length T list of hidden states (one for each step)\n\n  Raises:\n    TypeError: If `cell` is not an instance of RNNCell.\n    ValueError: If `inputs` is `None` or an empty list, or if the input depth\n      (column size) cannot be inferred from inputs via shape inference.\n  """"""\n\n  if not isinstance(cell, tf.compat.v1.nn.rnn_cell.RNNCell):\n    raise TypeError(""cell must be an instance of RNNCell"")\n  if not isinstance(inputs, list):\n    raise TypeError(""inputs must be a list"")\n  if not inputs:\n    raise ValueError(""inputs must not be empty"")\n\n  outputs = []\n  states = []\n\n  # Create a new scope in which the caching device is either\n  # determined by the parent scope, or is set to place the cached\n  # Variable using the same placement as for the rest of the RNN.\n  with tf.compat.v1.variable_scope(scope or ""RNN"") as varscope:\n    if varscope.caching_device is None:\n      varscope.set_caching_device(lambda op: op.device)\n\n    # Temporarily avoid EmbeddingWrapper and seq2seq badness\n    # TODO(lukaszkaiser): remove EmbeddingWrapper\n    if inputs[0].get_shape().ndims != 1:\n      (fixed_batch_size, input_size) = inputs[0].get_shape().with_rank(2)\n      if input_size is None:\n        raise ValueError(\n            ""Input size (second dimension of inputs[0]) must be accessible via ""\n            ""shape inference, but saw value None."")\n    else:\n      fixed_batch_size = inputs[0].get_shape().with_rank_at_least(1)[0]\n\n    if fixed_batch_size:\n      batch_size = fixed_batch_size\n    else:\n      batch_size = tf.shape(input=inputs[0])[0]\n    if initial_state is not None:\n      state = initial_state\n    else:\n      if not dtype:\n        raise ValueError(""If no initial_state is provided, ""\n                           ""dtype must be specified"")\n      state = cell.zero_state(batch_size, dtype)\n\n    if sequence_length is not None:  # Prepare variables\n      sequence_length = tf.cast(sequence_length, dtype=tf.int32)\n      zero_output = tf.zeros(\n          tf.stack([batch_size, cell.output_size]), inputs[0].dtype)\n      zero_output.set_shape(\n          tf.tensor_shape.TensorShape([fixed_batch_size.value,\n                                       cell.output_size]))\n      min_sequence_length = tf.reduce_min(input_tensor=sequence_length)\n      max_sequence_length = tf.reduce_max(input_tensor=sequence_length)\n\n    for time, input_ in enumerate(inputs):\n      if time > 0: varscope.reuse_variables()\n      # pylint: disable=cell-var-from-loop\n      call_cell = lambda: cell(input_, state)\n      # pylint: enable=cell-var-from-loop\n      if sequence_length is not None:\n        (output, state) = tf.nn.rnn._rnn_step(\n            time=time, sequence_length=sequence_length,\n            min_sequence_length=min_sequence_length,\n            max_sequence_length=max_sequence_length,\n            zero_output=zero_output, state=state,\n            call_cell=call_cell, state_size=cell.state_size)\n      else:\n        (output, state) = call_cell()\n\n      outputs.append(output)\n      states.append(state)\n    return (outputs, states)\n\n\ndef linear(args,\n            output_size,\n            bias,\n            bias_initializer=None,\n            kernel_initializer=None):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_initializer: starting value to initialize the bias\n      (default is all zeros).\n    kernel_initializer: starting value to initialize the weight.\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (nest.is_sequence(args) and not args):\n    raise ValueError(""`args` must be specified"")\n  if not nest.is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape() for a in args]\n  for shape in shapes:\n    if shape.ndims != 2:\n      raise ValueError(""linear is expecting 2D arguments: %s"" % shapes)\n    if shape[1] is None:\n      raise ValueError(""linear expects shape[1] to be provided for shape %s, ""\n                       ""but saw %s"" % (shape, shape[1]))\n    else:\n      total_arg_size += shape[1]\n\n  dtype = [a.dtype for a in args][0]\n\n  # Now the computation.\n  scope = tf.compat.v1.get_variable_scope()\n  with tf.compat.v1.variable_scope(scope) as outer_scope:\n    weights = tf.compat.v1.get_variable(\n        ""bias"", [total_arg_size, output_size],\n        dtype=dtype,\n        initializer=kernel_initializer)\n    if len(args) == 1:\n      res = tf.matmul(args[0], weights)\n    else:\n      res = tf.matmul(tf.concat(args, 1), weights)\n    if not bias:\n      return res\n    with tf.compat.v1.variable_scope(outer_scope) as inner_scope:\n      inner_scope.set_partitioner(None)\n      if bias_initializer is None:\n        bias_initializer = tf.compat.v1.constant_initializer(0.0, dtype=dtype)\n      biases = tf.compat.v1.get_variable(\n          ""kernel"", [output_size],\n          dtype=dtype,\n          initializer=bias_initializer)\n    return tf.nn.bias_add(res, biases)\n\n\nclass NNModel(object):\n    def __init__(self, hyperparams, buckets=None):\n        self.hyperparams = hyperparams\n        self.buckets = buckets\n\n    # --- model architecture hyperparameters --- #\n\n    @property\n    def encoder_topology(self):\n        return self.hyperparams[""encoder_topology""]\n\n    @property\n    def decoder_topology(self):\n        return self.hyperparams[""decoder_topology""]\n\n    @property\n    def num_layers(self):\n        return self.hyperparams[""num_layers""]\n\n    # --- training algorithm hyperparameters --- #\n\n    @property\n    def training_algorithm(self):\n        return self.hyperparams[""training_algorithm""]\n\n    @property\n    def use_sampled_softmax(self):\n        return self.num_samples > 0 and \\\n               self.num_samples < self.target_vocab_size\n\n    @property\n    def num_samples(self):\n        return self.hyperparams[""num_samples""]\n\n    @property\n    def batch_size(self):\n        return self.hyperparams[""batch_size""]\n\n    @property\n    def num_epochs(self):\n        return self.hyperparams[""num_epochs""]\n\n    @property\n    def steps_per_epoch(self):\n        return self.hyperparams[""steps_per_epoch""]\n\n    @property\n    def max_gradient_norm(self):\n        return self.hyperparams[""max_gradient_norm""]\n\n    @property\n    def optimizer(self):\n        return self.hyperparams[""optimizer""]\n\n    @property\n    def margin(self):\n        return self.hyperparams[""margin""]\n\n    @property\n    def adam_epsilon(self):\n        return self.hyperparams[""adam_epsilon""]\n\n    @property\n    def tg_token_use_attention(self):\n        return self.hyperparams[""tg_token_use_attention""]\n\n    @property\n    def tg_token_attn_fun(self):\n        return self.hyperparams[""tg_token_attn_fun""]\n\n    @property\n    def variational_recurrent_dropout(self):\n        return self.hyperparams[""variational_recurrent_dropout""]\n\n    @property\n    def attention_input_keep(self):\n        return self.hyperparams[""attention_input_keep""]\n\n    @property\n    def attention_output_keep(self):\n        return self.hyperparams[""attention_output_keep""]\n\n    @property\n    def rnn_cell(self):\n        return self.hyperparams[""rnn_cell""]\n\n    @property\n    def gamma_c(self):\n        return self.hyperparams[""gamma_c""]\n\n    @property\n    def beta_c(self):\n        return self.hyperparams[""beta_c""]\n\n    @property\n    def gamma_h(self):\n        return self.hyperparams[""gamma_h""]\n\n    @property\n    def beta_h(self):\n        return self.hyperparams[""beta_h""]\n\n    @property\n    def gamma_x(self):\n        return self.hyperparams[""gamma_x""]\n\n    @property\n    def beta_x(self):\n        return self.hyperparams[""beta_x""]\n\n\n    @property\n    def source_vocab_size(self):\n        return self.hyperparams[""source_vocab_size""]\n\n    @property\n    def target_vocab_size(self):\n        return self.hyperparams[""target_vocab_size""]\n\n    @property\n    def max_source_length(self):\n        return self.hyperparams[""max_source_length""]\n\n    @property\n    def max_target_length(self):\n        return self.hyperparams[""max_target_length""]\n\n    @property\n    def max_source_token_size(self):\n        return self.hyperparams[""max_source_token_size""]\n\n    @property\n    def max_target_token_size(self):\n        return self.hyperparams[""max_target_token_size""]\n\n    @property\n    def decode_sig(self):\n        return self.hyperparams[""decode_sig""]\n\n    @property\n    def model_dir(self):\n        return self.hyperparams[""model_dir""]\n\n    @property\n    def sc_token(self):\n        return self.hyperparams[""sc_token""]\n\n    @property\n    def sc_token_dim(self):\n        """"""\n        Source token channel embedding dimension.\n        """"""\n        return self.hyperparams[""sc_token_dim""]\n\n    @property\n    def sc_input_keep(self):\n        return self.hyperparams[""sc_input_keep""]\n\n    @property\n    def sc_output_keep(self):\n        return self.hyperparams[""sc_output_keep""]\n\n    @property\n    def sc_token_features_path(self):\n        return self.hyperparams[""sc_token_features_path""]\n\n    @property\n    def sc_char(self):\n        return self.hyperparams[""sc_char""]\n\n    @property\n    def sc_char_vocab_size(self):\n        return self.hyperparams[""sc_char_vocab_size""]\n\n    @property\n    def sc_char_dim(self):\n        """"""\n        Source character channel embedding dimension.\n        """"""\n        return self.hyperparams[""sc_char_dim""]\n\n    @property\n    def sc_char_composition(self):\n        return self.hyperparams[""sc_char_composition""]\n\n    @property\n    def sc_char_rnn_cell(self):\n        return self.hyperparams[""sc_char_rnn_cell""]\n\n    @property\n    def sc_char_rnn_num_layers(self):\n        return self.hyperparams[""sc_char_rnn_num_layers""]\n\n    @property\n    def sc_char_features_path(self):\n        return self.hyperparams[""sc_char_features_path""]\n\n    @property\n    def tg_input_keep(self):\n        return self.hyperparams[""tg_input_keep""]\n\n    @property\n    def tg_output_keep(self):\n        return self.hyperparams[""tg_output_keep""]\n\n    @property\n    def tg_token_features_path(self):\n        return self.hyperparams[""tg_token_features_path""]\n\n    @property\n    def tg_char(self):\n        return self.hyperparams[""tg_char""]\n\n    @property\n    def tg_char_vocab_size(self):\n        return self.hyperparams[""tg_char_vocab_size""]\n\n    @property\n    def tg_char_composition(self):\n        return self.hyperparams[""tg_char_composition""]\n\n    @property\n    def tg_char_rnn_cell(self):\n        return self.hyperparams[""tg_char_rnn_cell""]\n\n    @property\n    def tg_char_use_attention(self):\n        return self.hyperparams[""tg_char_use_attention""]\n\n    @property\n    def tg_char_rnn_num_layers(self):\n        return self.hyperparams[""tg_char_rnn_num_layers""]\n\n    @property\n    def tg_char_features_path(self):\n        return self.hyperparams[""tg_char_features_path""]\n\n    @property\n    def tg_char_rnn_input_keep(self):\n        return self.hyperparams[""tg_char_rnn_input_keep""]\n\n    @property\n    def tg_char_rnn_output_keep(self):\n        return self.hyperparams[""tg_char_rnn_output_keep""]\n\n    @property\n    def gamma(self):\n        return self.hyperparams[""gamma""]\n\n    # -- copy mechanism -- #\n\n    @property\n    def use_copy(self):\n        return self.hyperparams[""use_copy""]\n\n    @property\n    def copy_fun(self):\n        return self.hyperparams[""copy_fun""]\n\n    @property\n    def copynet(self):\n        return self.use_copy and self.copy_fun == \'copynet\'\n\n    @property\n    def copy_vocab_size(self):\n        return self.hyperparams[""copy_vocab_size""]\n\n    @property\n    def chi(self):\n        return self.hyperparams[""chi""]\n\n    # --- decoding algorithm hyperparameters --- #\n\n    @property\n    def forward_only(self):\n        # If set, we do not construct the backward pass in the model.\n        return self.hyperparams[""forward_only""]\n\n    @property\n    def token_decoding_algorithm(self):\n        return self.hyperparams[""token_decoding_algorithm""]\n\n    @property\n    def char_decoding_algorithm(self):\n        return self.hyperparams[""char_decoding_algorithm""]\n\n    @property\n    def beam_size(self):\n        return self.hyperparams[""beam_size""]\n\n    @property\n    def beam_order(self):\n        return self.hyperparams[""beam_order""]\n\n    @property\n    def alpha(self):\n        return self.hyperparams[""alpha""]\n\n    @property\n    def beta(self):\n        return self.hyperparams[""beta""]\n\n    @property\n    def top_k(self):\n        return self.hyperparams[""top_k""]\n\n    @property\n    def force_reading_input(self):\n        return self.hyperparams[""force_reading_input""]\n\n\nclass InfPerplexityError(Exception):\n    pass'"
encoder_decoder/meta_experiments.py,3,"b""'''\nMeta-experiments which involves training and testing the model with multiple\nhyperparamter settings.\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport numpy as np\nimport random\nimport os, sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\nfrom encoder_decoder import data_utils, graph_utils\n\n\nhyperparam_range = {\n    'attention_input_keep': [0.4, 0.6, 0.8, 1.0],\n    'attention_output_keep': [0.4, 0.6, 0.8, 1.0],\n    'universal_keep': [0.6, 0.7, 0.75, 0.8, 0.5],\n    'sc_input_keep': [0.6, 0.7, 0.8],\n    'sc_output_keep': [0.6, 0.7, 0.8],\n    'tg_input_keep': [0.6, 0.7, 0.8],\n    'tg_output_keep': [0.5, 0.6, 0.7, 0.8],\n    'adam_epsilon': [1e-8, 1e-7, 1e-5, 1e-3, 1e-1],\n    'learning_rate': [0.0001, 0.0003],\n    'sc_token_dim': [200, 150, 250],\n    'num_layers': [1, 2],\n    'num_samples': [1024, 512],\n    'beta': [0.8,0.9,1.0,1.1,1.2],\n    'min_vocab_frequency': [4, 6, 8, 10],\n    'num_buckets': [1, 2, 3, 4]\n}\n\n\ndef grid_search(train_fun, decode_fun, eval_fun, train_set, dev_set, FLAGS):\n    '''\n    Perform hyperparameter tuning of a model using grid-search.\n\n    Usage: ./run-script.sh --grid_search --tuning hp1,...\n\n    :param train_fun: Function to train the model.\n    :param decode_fun: Function to decode from the trained model.\n    :param eval_fun: Function to evaluate the decoding results.\n    :param train_set: Training dataset.\n    :param dev_set: Development dataset.\n    :param FLAGS: General model hyperparameters.\n    '''\n    FLAGS.create_fresh_params = True\n\n    hyperparameters = FLAGS.tuning.split(',')\n    num_hps = len(hyperparameters)\n    hp_range = hyperparam_range\n\n    print('======== Grid Search ========')\n    print('%d hyperparameter(s): ' % num_hps)\n    for i in xrange(num_hps):\n        print('{}: {}'.format(\n            hyperparameters[i], hp_range[hyperparameters[i]]))\n    print()\n\n    if FLAGS.dataset.startswith('bash'):\n        metrics = ['top1_temp_ms', 'top1_cms', 'top3_temp_ms', 'top3_cms', \n                   'top1_str_ms', 'top3_str_ms']\n        metrics_weights = [0.1875, 0.1875, 0.0625, 0.0625, 0.25, 0.25]\n    else:\n        metrics = ['top1_temp_ms']\n        metrics_weights = [1]\n    metrics_signature = '+'.join(\n        ['{}x{}'.format(m, mw) for m, mw in zip(metrics, metrics_weights)])\n\n    # Grid search experiment log\n    grid_search_log_file_name = 'grid_search_log.{}'.format(FLAGS.channel)\n    if FLAGS.use_copy:\n        grid_search_log_file_name += '.{}'.format(FLAGS.copy_fun)\n    if FLAGS.normalized:\n        grid_search_log_file_name += '.normalized'\n    grid_search_log_file = open(os.path.join(\n        FLAGS.model_root_dir, grid_search_log_file_name), 'w')\n\n    # Generate grid\n    param_grid = [v for v in hp_range[hyperparameters[0]]]\n    for i in xrange(1, num_hps):\n        param_grid = itertools.product(param_grid, hp_range[hyperparameters[i]])\n\n    # Initialize metrics value\n    best_hp_set = [-1] * num_hps\n    best_seed = -1\n    best_metrics_value = 0\n\n    for row in param_grid:\n        row = nest.flatten(row)\n\n        # Set current hyperaramter set\n        for i in xrange(num_hps):\n            setattr(FLAGS, hyperparameters[i], row[i])\n            if hyperparameters[i] == 'universal_keep':\n                setattr(FLAGS, 'sc_input_keep', row[i])\n                setattr(FLAGS, 'sc_output_keep', row[i])\n                setattr(FLAGS, 'tg_input_keep', row[i])\n                setattr(FLAGS, 'tg_output_keep', row[i])\n                setattr(FLAGS, 'attention_input_keep', row[i])\n                setattr(FLAGS, 'attention_output_keep', row[i])\n\n        print('Trying parameter set: ')\n        for i in xrange(num_hps):\n            print('* {}: {}'.format(hyperparameters[i], row[i]))\n\n        # Try different random seed if tuning initialization\n        num_trials = 5 if FLAGS.initialization else 1\n\n        if 'min_vocab_frequency' in hyperparameters or \\\n                'num_buckets' in hyperparameters:\n            # Read train and dev sets from disk\n            train_set, dev_set, test_set = \\\n                data_utils.load_data(FLAGS, use_buckets=True, load_mappings=False)\n            vocab = data_utils.load_vocabulary(FLAGS)\n            FLAGS.sc_vocab_size = len(vocab.sc_vocab)\n            FLAGS.tg_vocab_size = len(vocab.tg_vocab)\n            FLAGS.max_sc_token_size = vocab.max_sc_token_size\n            FLAGS.max_tg_token_size = vocab.max_tg_token_size\n\n        for t in xrange(num_trials):\n            seed = random.getrandbits(32)\n            tf.compat.v1.set_random_seed(seed)\n            metrics_value = single_round_model_eval(train_fun, decode_fun,\n                eval_fun, train_set, dev_set, metrics, metrics_weights)\n            print('Parameter set: ')\n            for i in xrange(num_hps):\n                print('* {}: {}'.format(hyperparameters[i], row[i]))\n            print('random seed: {}'.format(seed))\n            print('{} = {}'.format(metrics_signature, metrics_value))\n            grid_search_log_file.write('Parameter set: \\n')\n            for i in xrange(num_hps):\n                grid_search_log_file.write('* {}: {}\\n'.format(\n                    hyperparameters[i], row[i]))\n            grid_search_log_file.write('random seed: {}\\n'.format(seed))\n            grid_search_log_file.write('{} = {}\\n\\n'.format(\n                metrics_signature, metrics_value))\n            print('Best parameter set so far: ')\n            for i in xrange(num_hps):\n                print('* {}: {}'.format(hyperparameters[i], best_hp_set[i]))\n            print('Best random seed so far: {}'.format(best_seed))\n            print('Best evaluation metrics so far = {}'.format(best_metrics_value))\n            if metrics_value > best_metrics_value:\n                best_hp_set = row\n                best_seed = seed\n                best_metrics_value = metrics_value\n                print('\xe2\x98\xba New best parameter setting found\\n')\n\n    print()\n    print('*****************************')\n    print('Best parameter set: ')\n    for i in xrange(num_hps):\n        print('* {}: {}'.format(hyperparameters[i], best_hp_set[i]))\n    print('Best seed = {}'.format(best_seed))\n    print('Best {} = {}'.format(metrics, best_metrics_value))\n    print('*****************************')\n    grid_search_log_file.write('*****************************\\n')\n    grid_search_log_file.write('Best parameter set: \\n')\n    for i in xrange(num_hps):\n        grid_search_log_file.write(\n            '* {}: {}\\n'.format(hyperparameters[i], best_hp_set[i]))\n    grid_search_log_file.write('Best seed = {}\\n'.format(best_seed))\n    grid_search_log_file.write(\n        'Best {} = {}\\n'.format(metrics, best_metrics_value))\n    grid_search_log_file.write('*****************************')\n    grid_search_log_file.close()\n\n\ndef schedule_experiments(train_fun, decode_fun, eval_fun, train_set, dev_set,\n                         hyperparam_sets, FLAGS):\n    '''\n    Run multiple experiments with different sets of hyperparameters.\n    '''\n\n    print('===== Scheduled Experiments =====')\n    for hyperparam_set in hyperparam_sets:\n        for hp in hyperparam_set:\n            setattr(FLAGS, hp, hyperparam_set[hp])\n            if hp == 'universal_keep':\n                setattr(FLAGS, 'sc_input_keep', hyperparam_set[hp])\n                setattr(FLAGS, 'sc_output_keep', hyperparam_set[hp])\n                setattr(FLAGS, 'tg_input_keep', hyperparam_set[hp])\n                setattr(FLAGS, 'tg_output_keep', hyperparam_set[hp])\n                setattr(FLAGS, 'attention_input_keep', hyperparam_set[hp])\n                setattr(FLAGS, 'attention_output_keep', hyperparam_set[hp])\n\n        print('Trying parameter set: ')\n        for hp in hyperparam_set:\n            print('* {}: {}'.format(hp, hyperparam_set[hp]))\n            metrics = 'top1_temp_ms'\n\n        metrics_value = single_round_model_eval(\n            train_fun, decode_fun, eval_fun, train_set, dev_set, metrics)\n        print('Parameter set: ')\n        for hp in hyperparam_set:\n            print('* {}: {}'.format(hp, hyperparam_set[hp]))\n        print('{} = {}'.format(metrics, metrics_value))\n\n\ndef single_round_model_eval(train_fun, decode_fun, eval_fun, train_set,\n                            dev_set, metrics, metrics_weights):\n    '''\n    Train the model with a certain set of hyperparameters and evaluate on the\n    development set.\n\n    :param train_fun: Function to train the model.\n    :param decode_fun: Function to decode from the trained model.\n    :param eval_fun: Function to evaluate the decoding results.\n    :param train_set: Training dataset.\n    :param dev_set: Development dataset.\n    :param metrics: List of evaluation metrics used for tuning.\n    :param metrics_weights: List of evaluation metrics weights used for tuning.\n\n    :return: The weighted evaluation metrics.\n    '''\n    assert(len(metrics) > 0)\n    assert(len(metrics) == len(metrics_weights))\n    tf.compat.v1.reset_default_graph()\n    try:\n        train_fun(train_set, dev_set)\n\n        tf.compat.v1.reset_default_graph()\n        model = decode_fun(dev_set, buckets=train_set.buckets,\n                           verbose=False)\n\n        M = eval_fun(dev_set, model.model_dir, model.decode_sig, verbose=False)\n\n        metrics_value = 0\n        for m, m_w in zip(metrics, metrics_weights):\n            metrics_value += m_w * M[m]\n    except graph_utils.InfPerplexityError:\n        metrics_value = -np.inf\n\n    return metrics_value\n"""
encoder_decoder/parse_args.py,111,"b""'''Parsing input arguments'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\ndef define_input_flags():\n    # translation experiments\n    tf.compat.v1.flags.DEFINE_integer('max_train_data_size', 0,\n                                'Limit on the size of training data (0: no limit).')\n    tf.compat.v1.flags.DEFINE_integer('steps_per_epoch', 200,\n                                'How many training steps to do per checkpoint.')\n    tf.compat.v1.flags.DEFINE_integer('num_epochs', 20,\n                                'Number of training epochs')\n    tf.compat.v1.flags.DEFINE_integer('epochs_per_checkpoint', 1,\n                                'How many training steps to do per checkpoint.')\n\n    tf.compat.v1.flags.DEFINE_boolean('explain', False,\n                                'Set to True to translate code to natural language.')\n\n    tf.compat.v1.flags.DEFINE_boolean('cross_valid', False,\n                                'Set to True for cross validation.')\n    tf.compat.v1.flags.DEFINE_integer('num_folds', 5,\n                                'Number of folds in cross validation.')\n\n    tf.compat.v1.flags.DEFINE_boolean('grid_search', False,\n                                'Set to True for grid search.')\n    tf.compat.v1.flags.DEFINE_boolean('schedule_experiments', False,\n                                'Set to True for running multiple experiments with different pre-specified '\n                                'hyperparameters.')\n    tf.compat.v1.flags.DEFINE_string('tuning', 'initialization,output_keep_prob,num_samples',\n                               'List of hyperparamters to tune.')\n    tf.compat.v1.flags.DEFINE_boolean('initialization', False,\n                                'Set to try multiple random intialization and select the best one.')\n\n    tf.compat.v1.flags.DEFINE_boolean('process_data', False,\n                                'Set to True for data preprocessing.')\n    tf.compat.v1.flags.DEFINE_boolean('decode', False,\n                                'Set to True for decoding.')\n    tf.compat.v1.flags.DEFINE_boolean('test', False,\n                                'Set to True to decode and evaluate on the test set.')\n    tf.compat.v1.flags.DEFINE_boolean('eval', False,\n                                'Set to True to perform automatic evaluation.')\n    tf.compat.v1.flags.DEFINE_boolean('manual_eval', False,\n                                'Set to True to perform manual evaluation in the commandline interface.')\n    tf.compat.v1.flags.DEFINE_boolean('demo', False,\n                                'Set to True for interactive demo.')\n\n    tf.compat.v1.flags.DEFINE_boolean('gen_error_analysis_sheet', False,\n                                'Set to True to generate error analysis spreadsheet.')\n    tf.compat.v1.flags.DEFINE_boolean('gen_manual_evaluation_sheet', False,\n                                'Set to True to tabulate the output of specified baseline models in .csv files '\n                                'for manual annotation.')\n    tf.compat.v1.flags.DEFINE_boolean('gen_manual_evaluation_sheet_single_model', False,\n                                'Set to True to tabulate the output of a specific model in .csv file '\n                                'for manual annotation.')\n    tf.compat.v1.flags.DEFINE_boolean('gen_manual_evaluation_table', False,\n                                'Set to True to tabulate manual evaluation results of specified baseline systems '\n                                'in standard output.'\n                                'Opens the commandline interface when the model output contains unseen command.')\n    tf.compat.v1.flags.DEFINE_boolean('gen_auto_evaluation_table', False,\n                                'Set to True to tabulate the automatic evaluation results '\n                                'for specified baseline systems in standard output.')\n\n    # device\n    tf.compat.v1.flags.DEFINE_string('gpu', '0', 'GPU device where the computation is going to be placed.')\n    tf.compat.v1.flags.DEFINE_boolean('log_device_placement', False, 'Set to True for logging device placement.')\n\n    # data hyperparameters\n    tf.compat.v1.flags.DEFINE_string('dataset', 'bash', 'select dataset to use.')\n    tf.compat.v1.flags.DEFINE_string('channel', '', 'translation feature channel.')\n    tf.compat.v1.flags.DEFINE_string('data_dir', os.path.join(os.path.dirname(__file__), 'data'), 'Data directory')\n    tf.compat.v1.flags.DEFINE_string('model_dir', os.path.join(os.path.dirname(__file__), 'model'), 'Model directory')\n    tf.compat.v1.flags.DEFINE_string('prediction_file', None, 'path to where the decoding output is stored')\n    tf.compat.v1.flags.DEFINE_integer('sample_size', 200, 'Training data sample size')\n    tf.compat.v1.flags.DEFINE_boolean('normalized', False, 'Set to True for learning with normalized command.')\n    tf.compat.v1.flags.DEFINE_boolean('canonical', False,\n                                'Set to True for learning with normalized command with canonicalized option order.')\n    tf.compat.v1.flags.DEFINE_integer('max_sc_length', 100, 'Maximum source sequence length.')\n    tf.compat.v1.flags.DEFINE_integer('max_tg_length', 100, 'Maximum target sequence length.')\n    tf.compat.v1.flags.DEFINE_integer('sc_vocab_size', 1000, 'Source vocabulary size.')\n    tf.compat.v1.flags.DEFINE_integer('tg_vocab_size', 1000, 'Target vocabulary size.')\n    tf.compat.v1.flags.DEFINE_integer('max_sc_token_size', 100, 'Maximum source token size.')\n    tf.compat.v1.flags.DEFINE_integer('max_tg_token_size', 100, 'Maximum target token size.')\n    tf.compat.v1.flags.DEFINE_integer('min_vocab_frequency', 1,\n                                'Minimum frequency of token in the dataset that are not considered UNK.')\n    tf.compat.v1.flags.DEFINE_integer('num_buckets', 3, 'Number of buckets to use.')\n\n    # training hyperparameters\n    tf.compat.v1.flags.DEFINE_string('model_root_dir', 'model', 'Directory to save trained models.')\n    tf.compat.v1.flags.DEFINE_boolean('create_fresh_params', False,\n                                'Set to force remove previously trained models.')\n    tf.compat.v1.flags.DEFINE_string('rnn_cell', 'gru', 'Type of RNN cell to use.')\n    tf.compat.v1.flags.DEFINE_string('optimizer', 'adam',\n                               'Type of numeric optimization algorithm to use.')\n    tf.compat.v1.flags.DEFINE_float('learning_rate', 0.001, 'Learning rate.')\n    tf.compat.v1.flags.DEFINE_float('learning_rate_decay_factor', 0.99,\n                              'Learning rate decays by this much.')\n    tf.compat.v1.flags.DEFINE_float('adam_epsilon', 1e-08, 'Epsilon parameter in the Adam Optimizer.')\n    tf.compat.v1.flags.DEFINE_float('max_gradient_norm', 5.0,\n                              'Clip gradients to this norm.')\n    tf.compat.v1.flags.DEFINE_integer('batch_size', 128,\n                                'Batch size to use during training.')\n    tf.compat.v1.flags.DEFINE_integer('num_layers', 1,\n                                'Number of layers in the encoder-decoder.')\n    tf.compat.v1.flags.DEFINE_integer('num_samples', -1,\n                                'Number of samples for sampled softmax.')\n    tf.compat.v1.flags.DEFINE_integer('seed', -1, 'Random seed for graph initialization.')\n\n    tf.compat.v1.flags.DEFINE_boolean('variational_recurrent_dropout', False, 'Set to use variational ' +\n                                'recurrent dropout on the RNN cells.')\n    tf.compat.v1.flags.DEFINE_float('universal_keep', -1, 'Use the rate for all ' +\n                              'dropout layers if set to a number between 0 and 1.')\n    tf.compat.v1.flags.DEFINE_float('sc_input_keep', .5, 'Proportion of source input to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('sc_output_keep', .5, 'Proportion of source output to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('tg_input_keep', .5, 'Proportion of target input to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('tg_output_keep', .5, 'Proportion of target output to keep if dropout is used.')\n\n    tf.compat.v1.flags.DEFINE_boolean('recurrent_batch_normalization', False,\n                                'Set to use recurrent batch normalization.')\n    tf.compat.v1.flags.DEFINE_float('gamma_c', .1, 'Scale of cell state normalization.')\n    tf.compat.v1.flags.DEFINE_float('beta_c', .1, 'Offset of cell state normalization.')\n    tf.compat.v1.flags.DEFINE_float('gamma_h', .1, 'Scale of hidden state normalization.')\n    tf.compat.v1.flags.DEFINE_float('beta_h', .1, 'Offset of hidden state normalization.')\n    tf.compat.v1.flags.DEFINE_float('gamma_x', .1, 'Scale of input state normalization.')\n    tf.compat.v1.flags.DEFINE_float('beta_x', .1, 'Offset of input state normalization.')\n\n    tf.compat.v1.flags.DEFINE_string('training_algorithm', 'standard', 'training algorithm to use.')\n    tf.compat.v1.flags.DEFINE_string('pretrained_model_subdir', '', 'signature of pretrained model.')\n\n    tf.compat.v1.flags.DEFINE_string('encoder_topology', 'rnn', 'structure of the encoder.')\n    tf.compat.v1.flags.DEFINE_string('decoder_topology', 'rnn', 'structure of the decoder.')\n\n    tf.compat.v1.flags.DEFINE_boolean('tg_token_use_attention', False, 'If set, use attention for token decoder.')\n    tf.compat.v1.flags.DEFINE_boolean('tg_char_use_attention', False, 'If set, use attention for char decoder.')\n    tf.compat.v1.flags.DEFINE_string('tg_token_attn_fun', 'inner_product',\n                                'Function used for token-level attention module.')\n    tf.compat.v1.flags.DEFINE_string('tg_char_attn_fun', 'inner_product',\n                               'Function used for char-level attention module.')\n    tf.compat.v1.flags.DEFINE_float('beta', 0, 'Attention regularization weight.')\n    tf.compat.v1.flags.DEFINE_float('attention_input_keep', .5,\n                              'Proportion of attention input state to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('attention_output_keep', .5,\n                              'Proportion of attention hidden state to keep if dropout is used.')\n\n    tf.compat.v1.flags.DEFINE_float('margin', 1.0, 'margin for margin-based loss function')\n\n    # decoding hyperparameters\n    tf.compat.v1.flags.DEFINE_string('token_decoding_algorithm', 'beam_search',\n                               'decoding algorithm used for token generation.')\n    tf.compat.v1.flags.DEFINE_string('char_decoding_algorithm', 'greedy',\n                               'decoding algorithm used for character generation.')\n    tf.compat.v1.flags.DEFINE_integer('beam_size', -1, 'Size of beam for beam search.')\n    tf.compat.v1.flags.DEFINE_integer('beam_order', -1, 'Order for beam search.')\n    tf.compat.v1.flags.DEFINE_float('alpha', 0.5, 'Beam search length normalization parameter.')\n    tf.compat.v1.flags.DEFINE_integer('top_k', 5, 'Top-k highest-scoring structures to output.')\n    tf.compat.v1.flags.DEFINE_boolean('grammatical_only', True, 'If set, output only grammatical predictions.')\n\n    tf.compat.v1.flags.DEFINE_boolean('fill_argument_slots', False, 'If set, fill the argument slots in '\n                                'the output command with filler constants extracted from the natural language input.')\n\n    # slot-filling experiments\n    tf.compat.v1.flags.DEFINE_integer('num_nn_slot_filling', 1, 'Number of nearest neighbors to use in '\n                                'the nearest neighbor slot-filling classifier.')\n    tf.compat.v1.flags.DEFINE_boolean('gen_slot_filling_training_data', False,\n                                'Set to True to generate feature vectors for slot-filling training.')\n    tf.compat.v1.flags.DEFINE_boolean('eval_slot_filling', False,\n                                'Set to True for evaluation of the slot-filling classifier.')\n    tf.compat.v1.flags.DEFINE_boolean('eval_local_slot_filling', False,\n                                'Set to True for raw evaluation of the slot-filling classifier.')\n\n\n    # channel network hyperparameters\n    tf.compat.v1.flags.DEFINE_boolean('sc_token', True,\n                                'Set to True to turn on the token channel in the encoder. On by default.')\n    tf.compat.v1.flags.DEFINE_integer('sc_token_embedding_size', 1000, 'source word embedding size.')\n    tf.compat.v1.flags.DEFINE_integer('sc_token_dim', 300, 'Basic token embedding dimensions.')\n    tf.compat.v1.flags.DEFINE_boolean('sc_char', False,\n                                'Set to True to turn on the character channel in the encoder. Off by default.')\n    tf.compat.v1.flags.DEFINE_integer('sc_char_dim', 300, 'Dimension of each character embeddings.')\n    tf.compat.v1.flags.DEFINE_string('sc_char_composition', 'rnn', 'Specify the character to token composition function.')\n    tf.compat.v1.flags.DEFINE_string('sc_char_rnn_cell', 'gru', 'Type of RNN cell to use for the character model.')\n    tf.compat.v1.flags.DEFINE_integer('sc_char_rnn_num_layers', 1,\n                                'Number of layers in the RNN cell used for the character model.')\n\n    tf.compat.v1.flags.DEFINE_boolean('tg_token', True,\n                                'Set to True to turn on the token channel in the decoder. On by default.')\n    tf.compat.v1.flags.DEFINE_integer('tg_token_embedding_size', 1000, 'target word embedding size.')\n    tf.compat.v1.flags.DEFINE_boolean('tg_char', False,\n                                'Set to True to turn on character RNN extention module in the decoder.')\n    tf.compat.v1.flags.DEFINE_string('tg_char_composition', 'rnn',\n                               'Specify the model configuration used for character generation in the target.')\n    tf.compat.v1.flags.DEFINE_string('tg_char_rnn_cell', 'gru', 'Type of RNN cell to use for the character model.')\n    tf.compat.v1.flags.DEFINE_integer('tg_char_rnn_num_layers', 1,\n                                'Number of layers in the RNN cell used for the character model.')\n    tf.compat.v1.flags.DEFINE_float('tg_char_rnn_input_keep', .5,\n                                'Proportion of character target input to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('tg_char_rnn_output_keep', .5,\n                                'Proportion of character target output to keep if dropout is used.')\n    tf.compat.v1.flags.DEFINE_float('gamma', 5, 'Define the weight of the character channel loss.')\n\n    # hyperparameters for copying UNKs\n    tf.compat.v1.flags.DEFINE_boolean('use_copy', False, 'If set, use copying mechanism.')\n    tf.compat.v1.flags.DEFINE_string('copy_fun', 'implicit',\n                                'Specifying the type of copying functions to use.')\n    tf.compat.v1.flags.DEFINE_float('chi', 1, 'Copy loss weight.')\n\n    # debugging options\n    tf.compat.v1.flags.DEFINE_boolean('force_reading_input', False, 'If set, read ground truth decoder inputs for decoding.')\n\n    # Generate tables and plots charts for paper.\n    tf.compat.v1.flags.DEFINE_boolean('tabulate_example_predictions', False,\n                                'Set to True to print example predictions with Latex formattings')"""
encoder_decoder/slot_filling.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""\nFunctions for matching entity mentions in the natural language with the\ncorresponding program slots.\n""""""\n\nimport os, sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport collections, copy, re\nimport numpy as np\nfrom numpy.linalg import norm\n\nfrom bashlint import bash, data_tools\nfrom nlp_tools import constants, format_args, tokenizer\n\n\n# --- Classifiers for estimating likelihood of local matches --- #\n\nclass KNearestNeighborModel():\n    def __init__(self, k, train_X, train_Y):\n        """"""\n        :member k: number of neighboring examples to use\n        :member train_X: [size, dim] training feature matrix\n        :member train_Y: [size, label_dim] training label matrix\n        """"""\n        self.k = k\n        self.train_X = train_X\n        self.train_Y = train_Y\n\n    def predict(self, X):\n        """"""\n        :param X: [size, dim]\n        """"""\n        # [size, size]\n        sim_scores = np.matmul(X, self.train_X.T)\n        # [size, self.k]\n        nn = np.argpartition(sim_scores, -self.k, axis=1)[:, -self.k:]\n        # [size, self.k]\n        nn_weights = np.partition(sim_scores, -self.k, axis=1)[:, -self.k:]\n\n        nn_prediction = np.sum(\n            np.expand_dims(nn_weights, 2) * self.train_Y[nn], axis=1)[:, 0]\n        return nn_prediction\n\n    def eval(self, X, Y, verbose=True):\n        nn_prediction = self.predict(X)\n        # compute accuracy\n        threshold = 0.5\n        num_total = 0.0\n        num_correct = 0.0\n        for i in xrange(len(nn_prediction)):\n            if Y[i][0] == 1 and nn_prediction[i][0] >= threshold:\n                num_correct += 1\n            if Y[i][0] == 0 and nn_prediction[i][0] < threshold:\n                num_correct += 1\n            if verbose:\n                print(nn_prediction[i][0], Y[i][0])\n            num_total += 1\n        print(""Accuracy: "", num_correct / num_total)\n\n\ndef gen_slot_filling_training_data(sess, FLAGS, model, dataset, output_file):\n    print(""saving slot filling mappings to {}"".format(output_file))\n\n    X, Y = [], []\n    for bucket_id in xrange(len(model.buckets)):\n        for i in xrange(len(dataset.data_points[bucket_id])):\n            dp = dataset.data_points[bucket_id][i]\n            if dp.alignments is not None:\n                source_inds, target_inds = dp.alignments.nonzero()\n                mappings = list(zip(list(source_inds), list(target_inds)))\n                encoder_channel_inputs = [[dp.sc_ids]]\n                decoder_channel_inputs = [[dp.tg_ids]]\n                # print(bucket_id)\n                # print(encoder_channel_inputs)\n                # print(decoder_channel_inputs)\n                if FLAGS.use_copy:\n                    encoder_channel_inputs.append([dp.csc_ids])\n                    decoder_channel_inputs.append([dp.ctg_ids])\n                formatted_example = model.format_batch(\n                    encoder_channel_inputs, decoder_channel_inputs,\n                    bucket_id=bucket_id)\n                model_outputs = model.step(\n                    sess, formatted_example, bucket_id, forward_only=True)\n                encoder_outputs = model_outputs.encoder_hidden_states\n                decoder_outputs = model_outputs.decoder_hidden_states\n                # add positive examples\n                for f, s in mappings:\n                    # use reversed index for the encoder embedding matrix\n                    ff = model.buckets[bucket_id][0] - f - 1\n                    if f >= encoder_outputs.shape[1] or s >= decoder_outputs.shape[1]:\n                        continue\n                    X.append(np.concatenate([encoder_outputs[:, ff, :],\n                                             decoder_outputs[:, s, :]], axis=1))\n                    Y.append(np.array([[1, 0]]))\n                    # add negative examples\n                    # sample unmatched filler-slot pairs as negative examples\n                    if len(mappings) > 1:\n                        for n_s in [ss for _, ss in mappings if ss != s]:\n                            if n_s >= decoder_outputs.shape[1]:\n                                continue\n                            X.append(np.concatenate(\n                                [encoder_outputs[:, ff, :],\n                                 decoder_outputs[:, n_s, :]], axis=1))\n                            Y.append(np.array([[0, 1]]))\n                    # Debugging\n                    # if i == 0:\n                    #     print(ff)\n                    #     print(encoder_outputs[0])\n                    #     print(decoder_outputs[0])\n                    if len(X) > 0 and len(X) % 1000 == 0:\n                        print(\'{} examples gathered for generating slot filling \'\n                              \'features...\'.format(len(X)))\n\n    assert(len(X) == len(Y))\n    X = np.concatenate(X, axis=0)\n    X = X / np.linalg.norm(X, axis=1)[:, None]\n    Y = np.concatenate(Y, axis=0)\n\n    np.savez(output_file, X, Y)\n\n# --- Global slot filling functions --- #\n\ndef stable_slot_filling(template_tokens, sc_fillers, tg_slots, pointer_targets,\n        encoder_outputs, decoder_outputs, slot_filling_classifier, verbose=False):\n    """"""\n    Fills the argument slots using learnt local alignment scores and a greedy \n    global alignment algorithm (stable marriage).\n\n    :param template_tokens: list of tokens in the command template\n    :param sc_fillers: the slot fillers extracted from the source sequence,\n        indexed by token id\n    :param tg_slots: the argument slots in the command template, indexed by\n        token id\n    :param pointer_targets: [encoder_length, decoder_length], local alignment\n        scores between source and target tokens\n    :param encoder_outputs: [encoder_length, dim] sequence of encoder hidden states\n    :param decoder_outputs: [decoder_length, dim] sequence of decoder hidden states\n    :param slot_filling_classifier: the classifier that produces the local\n        alignment scores\n    :param verbose: print all local alignment scores if set to true\n    """"""\n\n    # Step a): prepare (binary) type alignment matrix based on type info\n    M = np.zeros([len(encoder_outputs), len(decoder_outputs)], dtype=np.int32)\n    for f in sc_fillers:\n        if f >= len(encoder_outputs):\n            print(template_tokens, f, len(encoder_outputs))\n            continue\n        surface, filler_type = sc_fillers[f]\n        matched = False\n        for s in tg_slots:\n            if s >= len(decoder_outputs):\n                print(tg_slots, s, len(decoder_outputs))\n                continue\n            slot_value, slot_type = tg_slots[s]\n            if slot_filler_type_match(slot_type, filler_type):\n                M[f, s] = 1\n                matched = True\n        if not matched:\n            # If no target slot can hold a source filler, skip the alignment\n            # step and return None\n            return None, None, None\n\n    # Step b): compute local alignment scores if they are not provided already\n    if pointer_targets is None:\n        assert(encoder_outputs is not None)\n        assert(decoder_outputs is not None)\n        assert(slot_filling_classifier is not None)\n        pointer_targets = np.zeros([len(encoder_outputs), len(decoder_outputs)])\n        for f in xrange(M.shape[0]):\n            if np.sum(M[f]) > 1:\n                X = []\n                # use reversed index for the encoder embeddings matrix\n                ff = len(encoder_outputs) - f - 1\n                cm_slots_keys = list(tg_slots.keys())\n                for s in cm_slots_keys:\n                    X.append(np.concatenate([encoder_outputs[ff:ff+1],\n                                             decoder_outputs[s:s+1]], axis=1))\n                X = np.concatenate(X, axis=0)\n                X = X / norm(X, axis=1)[:, None]\n                raw_scores = slot_filling_classifier.predict(X)\n                for ii in xrange(len(raw_scores)):\n                    s = cm_slots_keys[ii]\n                    pointer_targets[f, s] = raw_scores[ii]\n                    if verbose:\n                        print(\'\xe2\x80\xa2 alignment ({}, {}): {}\\t{}\\t{}\'.format(\n                            f, s, sc_fillers[f], tg_slots[s], raw_scores[ii]))\n\n    M = M + M * pointer_targets\n    # convert M into a dictinary representation of a sparse matrix\n    M_dict = collections.defaultdict(dict)\n    for i in xrange(M.shape[0]):\n        if np.sum(M[i]) > 0:\n            for j in xrange(M.shape[1]):\n                if M[i, j] > 0:\n                    M_dict[i][j] = M[i, j]\n    \n    mappings, remained_fillers = stable_marriage_alignment(M_dict)\n\n    if not remained_fillers:\n        for f, s in mappings:\n            template_tokens[s] = format_args.get_fill_in_value(\n                tg_slots[s], sc_fillers[f])\n        cmd = \' \'.join(template_tokens)\n        tree = data_tools.bash_parser(cmd)\n        if not tree is None:\n            fill_default_value(tree)\n        temp = data_tools.ast2command(\n            tree, loose_constraints=True, ignore_flag_order=False)\n    else:\n        tree, temp = None, None\n\n    return tree, temp, mappings\n\ndef heuristic_slot_filling(node, ner_by_category):\n    """"""\n    Fills the argument slots with heuristic rules.\n    This rule-based slot-filling algorithm has high error-rate in practice.\n\n    :param node: the ast of a command template whose slots are to be filled\n    :param entities: the slot fillers extracted from the natural language\n        sentence, indexed by token id, character position and category,\n        respectively.\n    """"""\n    if ner_by_category is None:\n        # no constants detected in the natural language query\n        return True\n\n    def slot_filling_fun(node, arguments):\n        def fill_argument(filler_type, slot_type=None):\n            surface = arguments[filler_type][0][0]\n            node.value = format_args.get_fill_in_value(\n                (node.value, slot_type), (surface, filler_type))\n            arguments[filler_type].pop(0)\n\n        if node.is_argument():\n            if node.arg_type != \'Regex\' and arguments[node.arg_type]:\n                fill_argument(node.arg_type)\n            elif node.arg_type == \'Number\':\n                if arguments[\'Timespan\']:\n                    fill_argument(filler_type=\'Timespan\', slot_type=\'Number\')\n                    return\n            elif node.arg_type == \'Path\':\n                if arguments[\'Directory\']:\n                    fill_argument(filler_type=\'Directory\', slot_type=\'Path\')\n                    return\n                if arguments[\'File\']:\n                    fill_argument(filler_type=\'File\', slot_type=\'Path\')\n                    return\n                node.value = \'.\'\n            elif node.arg_type == \'Directory\':\n                if arguments[\'File\']:\n                    fill_argument(filler_type=\'File\', slot_type=\'Directory\')\n                    return\n                if arguments[\'Regex\']:\n                    fill_argument(filler_type=\'Regex\', slot_type=\'Directory\')\n            elif node.arg_type in [\'Username\', \'Groupname\']:\n                if arguments[\'Regex\']:\n                    fill_argument(filler_type=\'Regex\', slot_type=\'Username\')\n            elif node.arg_type == \'Regex\':\n                if arguments[\'File\']:\n                    fill_argument(filler_type=\'File\', slot_type=\'Regex\')\n                    return\n                if arguments[\'Number\']:\n                    fill_argument(filler_type=\'Number\', slot_type=\'Regex\')\n                    return\n        else:\n            for child in node.children:\n                slot_filling_fun(child, arguments)\n\n    arguments = collections.defaultdict(list)\n    for filler_type in constants.type_conversion:\n        slot_type = constants.type_conversion[filler_type]\n        arguments[slot_type] = copy.deepcopy(ner_by_category[filler_type]) \\\n            if filler_type in ner_by_category else []\n\n    slot_filling_fun(node, arguments)\n\n    # The template should fit in all arguments\n    for key in arguments:\n        if arguments[key]:\n            return False\n\n    return True\n\ndef stable_marriage_alignment(M):\n    """"""\n    Return the stable marriage alignment between two sets of entities (fillers\n    and slots).\n\n    :param M: stores the raw match score between the two sets of entities\n        represented by the rows and columns of M.\n\n        M(i, j) = -inf implies that i and j are incompatible.\n    :param include_partial_mappings: if set to True, consider partial word\n        mapping between the source and target.\n\n    """"""\n    preferred_list_by_row = {}\n    for i in M:\n        preferred_list_by_row[i] = sorted(\n            [(j, M[i][j]) for j in M[i] if M[i][j] > -np.inf],\n            key=lambda x:x[1], reverse=True)\n\n    remained_rows = list(M.keys())\n    matched_cols = {}\n\n    while (remained_rows):\n        # In our application, it is possible to have both unmatched rows and\n        # unmatched columns in the end, therefore need to detect this situation.\n        preferred_list_changed = False\n        for i in remained_rows:\n            if len(preferred_list_by_row[i]) > 0:\n                j, match_score = preferred_list_by_row[i].pop(0)\n                preferred_list_changed = True\n                if not j in matched_cols:\n                    matched_cols[j] = (i, match_score)\n                    remained_rows.remove(i)\n                else:\n                    if match_score > matched_cols[j][1]:\n                        k, _ = matched_cols[j]\n                        matched_cols[j] = (i, match_score)\n                        remained_rows.remove(i)\n                        remained_rows.append(k)\n        if not preferred_list_changed:\n            break\n\n    return [(y, x) for (x, (y, score)) in sorted(matched_cols.items(),\n            key=lambda x:x[1][1], reverse=True)], remained_rows\n\n\ndef fill_default_value(node):\n    """"""\n    Fill empty slot in the bash ast with default value.\n    """"""\n    if node.is_argument():\n        if node.value in bash.argument_types:\n            if node.arg_type == \'Path\' and node.parent.is_utility() \\\n                    and node.parent.value == \'find\':\n                node.value = \'.\'\n            elif node.arg_type == \'Regex\':\n                if node.parent.is_utility() and node.parent.value == \'grep\':\n                    node.value = \'\\\'.*\\\'\'\n                elif node.parent.is_option() and node.parent.value == \'-name\' \\\n                        and node.value == \'Regex\':\n                    node.value = \'""*""\'\n                else:\n                    node.value = \'[\' + node.arg_type.lower() + \']\'\n            elif node.arg_type == \'Number\' and node.utility.value in [\'head\', \'tail\']:\n                node.value = \'10\'\n            else:\n                if node.is_open_vocab():\n                    node.value = \'[\' + node.arg_type.lower() + \']\'\n    else:\n        for child in node.children:\n            fill_default_value(child)\n\n\n# --- Slot-filling alignment induction from parallel data\n\ndef slot_filler_alignment_induction(nl, cm, verbose=False):\n    """"""Give an oracle translation pair of (nl, cm), align the slot fillers\n       extracted from the natural language with the slots in the command.\n    """"""\n\n    # Step 1: extract the token ids of the constants in the English sentence\n    # and the slots in the command\n    tokens, entities = tokenizer.ner_tokenizer(nl)\n    nl_fillers, _, _ = entities\n    cm_tokens = data_tools.bash_tokenizer(cm)\n    cm_tokens_with_types = data_tools.bash_tokenizer(cm, arg_type_only=True)\n    assert(len(cm_tokens) == len(cm_tokens_with_types))\n    cm_slots = {}\n    for i in xrange(len(cm_tokens_with_types)):\n        if cm_tokens_with_types[i] in bash.argument_types:\n            if i > 0 and format_args.is_min_flag(cm_tokens_with_types[i-1]):\n                cm_token_type = \'Timespan\'\n            else:\n                cm_token_type = cm_tokens_with_types[i]\n            cm_slots[i] = (cm_tokens[i], cm_token_type)\n    \n    # Step 2: construct one-to-one mappings for the token ids from both sides\n    M = collections.defaultdict(dict)               # alignment score matrix\n    for i in nl_fillers:\n        surface, filler_type = nl_fillers[i]\n        filler_value = format_args.extract_value(filler_type, filler_type, surface)\n        for j in cm_slots:\n            slot_value, slot_type = cm_slots[j]\n            if (filler_value and format_args.is_parameter(filler_value)) or \\\n                    slot_filler_type_match(slot_type, filler_type):\n                M[i][j] = slot_filler_value_match(\n                    slot_value, filler_value, slot_type)\n            else:\n                M[i][j] = -np.inf\n\n    mappings, remained_fillers = stable_marriage_alignment(M)\n\n    if verbose:\n        print(\'nl: {}\'.format(nl))\n        print(\'cm: {}\'.format(cm))\n        for (i, j) in mappings:\n            print(\'[{}] {} <-> [{}] {}\'.format(\n                i, nl_fillers[i][0], j, cm_slots[j][0]))\n        for i in remained_fillers:\n            print(\'filler {} is not matched to any slot\\n\'\n                    .format(nl_fillers[i][0].encode(\'utf-8\')))\n    \n    return mappings\n\n\ndef slot_filler_value_match(slot_value, filler_value, slot_type):\n    """"""(Fuzzily) compute the matching score between a slot filler extracted\n        from the natural language and a the slot in the command. Used for\n        generating alignments from the training data.\n\n       :param slot_value: slot value as shown in the bash command\n       :param filler_value: slot filler value extracted from the natural language\n       :param slot_type: category of the slot in the command\n    """"""\n    if slot_type in bash.pattern_argument_types or \\\n            (filler_value and format_args.is_parameter(filler_value)):\n        if slot_value == filler_value:\n            return 1\n\n        slot_value = constants.remove_quotation(slot_value).lower()\n        filler_value = constants.remove_quotation(filler_value).lower()\n\n        if filler_value and format_args.is_parameter(filler_value):\n            if format_args.strip(slot_value).lower() == \\\n                    format_args.strip(filler_value).lower():\n                return 1\n        else:\n            sv = format_args.strip(slot_value).lower()\n            fv = format_args.strip(filler_value).lower()\n            if sv == fv:\n                return 1\n            elif fv in sv:\n                # include partial match\n                if len(sv) - len(fv) > 10:\n                    return -np.inf\n                else:\n                    return (len(fv) + 0.0) / len(sv)\n        return -np.inf\n    else:\n        if filler_value is None:\n            if slot_type == \'Permission\':\n                return 1\n            else:\n                return 0\n        if slot_type.endswith(\'Number\'):\n            if format_args.strip_sign(slot_value) == \\\n                    format_args.extract_number(filler_value):\n                return 1\n        if format_args.strip_sign(slot_value) == \\\n                format_args.strip_sign(filler_value):\n            return 1\n        else:\n            if slot_type.endswith(\'Timespan\') or slot_type.endswith(\'Size\'):\n                if format_args.extract_number(slot_value) == \\\n                        format_args.extract_number(filler_value):\n                    return 1\n        return 0\n\ndef slot_filler_type_match(slot_type, filler_type):\n    """"""\'\n    Check if the category of a slot in the command matches that of the slot\n    filler extracted from the natural language. Used for generating alignments\n    from the training data.\n\n    :param slot_type: slot category in the bash command\n    :param filler_type: slot filler category extracted from the natural language.\n    """"""\n    c_matches = {\n        \'_NUMBER, Number\',\n        \'_NUMBER, +Number\',\n        \'_NUMBER, -Number\',\n        \'_NUMBER, Regex\',\n        \'_NUMBER, Quantity\',\n        \'_NUMBER, +Quantity\',\n        \'_NUMBER, -Quantity\',\n        \'_SIZE, Size\',\n        \'_SIZE, +Size\',\n        \'_SIZE, -Size\',\n        \'_TIMESPAN, Timespan\',\n        \'_TIMESPAN, +Timespan\',\n        \'_TIMESPAN, -Timespan\',\n        \'_DATETIME, DateTime\',\n        \'_DATETIME, +DateTime\',\n        \'_DATETIME, -DateTime\',\n        \'_NUMBER, Permission\',\n        \'_NUMBER, +Permission\',\n        \'_NUMBER, -Permission\',\n        \'_PERMISSION, Permission\',\n        \'_PERMISSION, +Permission\',\n        \'_PERMISSION, -Permission\',\n        \'_PATH, Path\',\n        \'_DIRECTORY, Directory\',\n        \'_DIRECTORY, Path\',\n        \'_FILE, Path\',\n        \'_FILE, File\',\n        \'_FILE, Directory\',\n        \'_FILE, Regex\',\n        \'_REGEX, Username\',\n        \'_REGEX, Groupname\',\n        \'_REGEX, Directory\',\n        \'_REGEX, File\',\n        \'_REGEX, Path\',\n        \'_REGEX, Regex\'\n    }\n    return \'{}, {}\'.format(filler_type, slot_type) in c_matches\n'"
encoder_decoder/translate.py,8,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\nTranslation model that generates bash commands given natural language\ndescriptions.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\' \n\nimport math\nimport numpy as np\nimport pickle\nimport time\nfrom tqdm import tqdm\n\nimport tensorflow as tf\n\nfrom encoder_decoder import data_utils\nfrom encoder_decoder import decode_tools\nfrom encoder_decoder import graph_utils\nfrom encoder_decoder import meta_experiments\nfrom encoder_decoder import parse_args\nfrom encoder_decoder import slot_filling\nfrom .seq2seq.seq2seq_model import Seq2SeqModel\nfrom .seq2tree.seq2tree_model import Seq2TreeModel\nfrom eval import eval_tools, error_analysis\n\n# Refer to parse_args.py for model parameter explanations\nFLAGS = tf.compat.v1.flags.FLAGS\nparse_args.define_input_flags()\n\n# --- Define models --- #\n\ndef define_model(session, forward_only, buckets=None):\n    """"""\n    Define tensor graphs.\n    """"""\n    if FLAGS.decoder_topology in [\'basic_tree\']:\n        return graph_utils.define_model(\n            FLAGS, session, Seq2TreeModel, buckets, forward_only)\n    elif FLAGS.decoder_topology in [\'rnn\']:\n        return graph_utils.define_model(\n            FLAGS, session, Seq2SeqModel, buckets, forward_only)\n    else:\n        raise ValueError(""Unrecognized decoder topology: {}."".format(\n            FLAGS.decoder_topology))\n\n# --- Run experiments --- #\n\ndef train(train_set, test_set, verbose=False):\n    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement)) as sess:\n        # Initialize model parameters\n        model = define_model(sess, forward_only=False, buckets=train_set.buckets)\n\n        train_bucket_sizes = [len(train_set.data_points[b])\n                              for b in xrange(len(train_set.buckets))]\n        for i, bucket in enumerate(train_set.buckets):\n            print(\'bucket {}: {} ({})\'.format(i, bucket, train_bucket_sizes[i]))\n        train_total_size = float(sum(train_bucket_sizes))\n\n        # A bucket scale is a list of increasing numbers from 0 to 1 that we\'ll\n        # use to select a bucket. Length of [scale[i], scale[i+1]] is\n        # proportional to the size if i-th training bucket, as used later.\n        train_buckets_scale = [sum(train_bucket_sizes[:i+1]) / train_total_size\n                               for i in xrange(len(train_bucket_sizes))]\n\n        loss, dev_loss, epoch_time = 0.0, 0.0, 0.0\n        current_step = 0\n        previous_losses = []\n        previous_dev_losses = []\n\n        for t in xrange(FLAGS.num_epochs):\n            print(""Epoch %d"" % (t+1))\n\n            # progress bar\n            start_time = time.time()\n            for _ in tqdm(xrange(FLAGS.steps_per_epoch)):\n                time.sleep(0.01)\n                random_number_01 = np.random.random_sample()\n                bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                                 if train_buckets_scale[i] > random_number_01])\n                formatted_example = model.get_batch(train_set.data_points, bucket_id)\n                model_outputs = model.step(\n                    sess, formatted_example, bucket_id, forward_only=False)\n                loss += model_outputs.losses\n                current_step += 1\n            epoch_time = time.time() - start_time\n\n            # Once in a while, we save checkpoint, print statistics, and run evals.\n            if t % FLAGS.epochs_per_checkpoint == 0:\n                # Print statistics for the previous epoch.\n                loss /= FLAGS.steps_per_epoch\n                if loss < 300:\n                    ppx = math.exp(loss)\n                else:\n                    print(""Training loss = {} is too large."".format(loss))\n                    if t > 1:\n                        break\n                    else:\n                        raise graph_utils.InfPerplexityError\n                print(""learning rate %.4f epoch-time %.4f perplexity %.2f"" % (\n                    model.learning_rate.eval(), epoch_time, ppx))\n\n                # Decrease learning rate if no improvement of loss was seen\n                # over last 3 times.\n                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                    sess.run(model.learning_rate_decay_op)\n                previous_losses.append(loss)\n\n                checkpoint_path = os.path.join(FLAGS.model_dir, ""translate.ckpt"")\n                # Save checkpoint and reset timer and loss.\n                model.saver.save(\n                    sess, checkpoint_path, global_step=t, write_meta_graph=False)\n\n                epoch_time, loss, dev_loss = 0.0, 0.0, 0.0\n                # Run evals on development set and print the metrics.\n                sample_size = 10\n                repeated_samples = list(range(len(train_set.buckets))) * sample_size\n                for bucket_id in repeated_samples:\n                    if len(test_set.data_points[bucket_id]) == 0:\n                        if verbose:\n                            print(""  eval: empty bucket %d"" % (bucket_id))\n                        continue\n                    formatted_example = model.get_batch(test_set.data_points, bucket_id)\n                    model_outputs = model.step(\n                        sess, formatted_example, bucket_id, forward_only=True)\n                    eval_loss = model_outputs.losses\n                    dev_loss += eval_loss\n                    eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float(\'inf\')\n                    if verbose:\n                        print(""  eval: bucket %d perplexity %.2f"" % (bucket_id, eval_ppx))\n                dev_loss = dev_loss / len(repeated_samples)\n\n                dev_perplexity = math.exp(dev_loss) if dev_loss < 1000 else float(\'inf\')\n                print(""step %d learning rate %.4f dev_perplexity %.2f""\n                        % (t+1, model.learning_rate.eval(), dev_perplexity))\n\n                # Early stop if no improvement of dev loss was seen over last 3 checkpoints.\n                if len(previous_dev_losses) > 2 and dev_loss > max(previous_dev_losses[-3:]):\n                    break\n           \n                previous_dev_losses.append(dev_loss)\n\n                sys.stdout.flush()\n\n        return model\n\n\ndef decode(dataset, buckets=None, verbose=True):\n    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement)) as sess:\n        # Initialize model parameters.\n        model = define_model(sess, forward_only=True, buckets=buckets)\n        decode_tools.decode_set(sess, model, dataset, 3, FLAGS, verbose)\n        return model\n\n\ndef eval(dataset, prediction_path=None, verbose=True):\n    if prediction_path is None:\n        model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n        model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n        prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n    print(""(Auto) evaluating "" + prediction_path)\n\n    return eval_tools.automatic_eval(prediction_path, dataset, top_k=3, FLAGS=FLAGS, verbose=verbose)\n\n\ndef manual_eval(dataset, prediction_path=None):\n    if prediction_path is None:\n        model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n        model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n        prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n    print(""(Manual) evaluating "" + prediction_path)\n\n    return eval_tools.manual_eval(prediction_path, dataset, FLAGS, top_k=3, num_examples=100, verbose=True)\n\n\ndef demo(buckets=None):\n    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement)) as sess:\n        # Initialize model parameters.\n        model = define_model(sess, forward_only=True, buckets=buckets)\n        decode_tools.demo(sess, model, FLAGS)\n\n\ndef gen_slot_filling_training_data(FLAGS, datasets):\n    # Set hyperparameters\n    token_decoding_algorithm = FLAGS.token_decoding_algorithm\n    FLAGS.token_decoding_algorithm = \'greedy\'\n    FLAGS.force_reading_input = True\n\n    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement)) as sess:\n        # Create model and load parameters.\n        train_set, dev_set, test_set = datasets\n        model = define_model(sess, forward_only=True, buckets=train_set.buckets)\n        # Save slot filling embeddings.\n        slot_filling.gen_slot_filling_training_data(sess, FLAGS, model, train_set,\n            os.path.join(FLAGS.model_dir, \'train.mappings.X.Y.npz\'))\n        slot_filling.gen_slot_filling_training_data(sess, FLAGS, model, dev_set,\n            os.path.join(FLAGS.model_dir, \'dev.mappings.X.Y.npz\'))\n        slot_filling.gen_slot_filling_training_data(sess, FLAGS, model, test_set,\n            os.path.join(FLAGS.model_dir, \'test.mappings.X.Y.npz\'))\n\n    # Restore hyperparameters\n    FLAGS.token_decoding_algorithm = token_decoding_algorithm\n    FLAGS.force_reading_input = False\n\n\ndef gen_error_analysis_sheets(dataset, model_dir=None, decode_sig=None,\n                              group_by_utility=False):\n    if model_dir is None:\n        model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n        model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n    if group_by_utility:\n        error_analysis.gen_error_analysis_csv_by_utility(\n            model_dir, decode_sig, dataset, FLAGS)\n    else:\n        error_analysis.gen_error_analysis_csv(\n            model_dir, decode_sig, dataset, FLAGS)\n\n\n# --- Schedule experiments --- #\n\ndef schedule_experiments(train_fun, decode_fun, eval_fun, train_set, dev_set):\n    # hp_set1 = {\'universal_keep\': 0.5}\n    # hp_set2 = {\'universal_keep\': 0.6}\n    # hp_set3 = {\'universal_keep\': 0.7}\n    # hyperparam_sets = [hp_set1, hp_set2, hp_set3]\n    \n    hp_set1 = {\'universal_keep\': 0.6, \'rnn_cell\': \'gru\', \'num_layers\': 2}\n    hp_set2 = {\'universal_keep\': 0.75, \'rnn_cell\': \'gru\', \'num_layers\': 2}\n    hyperparam_sets = [hp_set1, hp_set2]\n    meta_experiments.schedule_experiments(train_fun, decode_fun, eval_fun,\n        train_set, dev_set, hyperparam_sets, FLAGS)\n\n# --- Pre-processing --- #\n\ndef process_data():\n    print(""Preparing data in %s"" % FLAGS.data_dir)\n    data_utils.prepare_data(FLAGS)\n\n\ndef main(_):\n    # set GPU device\n    os.environ[""CUDA_VISIBLE_DEVICES""] = FLAGS.gpu\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n \n    # set up data and model directories\n    FLAGS.data_dir = os.path.join(\n        os.path.dirname(__file__), "".."", ""data"", FLAGS.dataset)\n    print(""Reading data from {}"".format(FLAGS.data_dir))\n\n    # set up encoder/decider dropout rate\n    if FLAGS.universal_keep >= 0 and FLAGS.universal_keep < 1:\n        FLAGS.sc_input_keep = FLAGS.universal_keep\n        FLAGS.sc_output_keep = FLAGS.universal_keep\n        FLAGS.tg_input_keep = FLAGS.universal_keep\n        FLAGS.tg_output_keep = FLAGS.universal_keep\n        FLAGS.attention_input_keep = FLAGS.universal_keep\n        FLAGS.attention_output_keep = FLAGS.universal_keep\n\n    # adjust hyperparameters for batch normalization\n    if FLAGS.recurrent_batch_normalization:\n        # larger batch size\n        FLAGS.batch_size *= 4\n        # larger initial learning rate\n        FLAGS.learning_rate *= 10\n\n    if FLAGS.decoder_topology in [\'basic_tree\']:\n        FLAGS.model_root_dir = os.path.join(\n            os.path.dirname(__file__), "".."", FLAGS.model_root_dir, ""seq2tree"")\n    elif FLAGS.decoder_topology in [\'rnn\']:\n        FLAGS.model_root_dir = os.path.join(\n            os.path.dirname(__file__), "".."", FLAGS.model_root_dir, ""seq2seq"")\n    else:\n        raise ValueError(""Unrecognized decoder topology: {}.""\n                         .format(FLAGS.decoder_topology))\n    print(""Saving models to {}"".format(FLAGS.model_root_dir))\n\n    if FLAGS.process_data:\n        process_data()\n\n    else:\n        train_set, dev_set, test_set = data_utils.load_data(\n            FLAGS, use_buckets=False, load_features=False)\n        dataset = test_set if FLAGS.test else dev_set\n\n        if FLAGS.eval:\n            eval(dataset, FLAGS.prediction_file)\n        elif FLAGS.manual_eval:\n            manual_eval(dataset, FLAGS.prediction_file)\n        elif FLAGS.gen_error_analysis_sheet:\n            gen_error_analysis_sheets(dataset, group_by_utility=True)\n        elif FLAGS.gen_manual_evaluation_sheet:\n            error_analysis.gen_manual_evaluation_csv(dataset, FLAGS)\n        elif FLAGS.gen_manual_evaluation_sheet_single_model:\n            error_analysis.gen_manual_evaluation_csv_single_model(dataset, FLAGS)\n        elif FLAGS.gen_manual_evaluation_table:\n            if FLAGS.test:\n                eval_tools.gen_manual_evaluation_table(dataset, FLAGS)\n            else:\n                eval_tools.gen_manual_evaluation_table(dataset, FLAGS, num_examples=100)\n        elif FLAGS.gen_auto_evaluation_table:\n            eval_tools.gen_automatic_evaluation_table(dataset, FLAGS)\n        elif FLAGS.tabulate_example_predictions:\n            error_analysis.tabulate_example_predictions(dataset, FLAGS, num_examples=100)\n        else:\n            train_set, dev_set, test_set = data_utils.load_data(FLAGS, use_buckets=True)\n            dataset = test_set if FLAGS.test else dev_set\n            vocab = data_utils.load_vocabulary(FLAGS)\n\n            print(""Set dataset parameters"")\n            FLAGS.max_sc_length = train_set.max_sc_length if not train_set.buckets else \\\n                train_set.buckets[-1][0]\n            FLAGS.max_tg_length = train_set.max_tg_length if not train_set.buckets else \\\n                train_set.buckets[-1][1]\n            FLAGS.sc_vocab_size = len(vocab.sc_vocab)\n            FLAGS.tg_vocab_size = len(vocab.tg_vocab)\n            FLAGS.max_sc_token_size = vocab.max_sc_token_size\n            FLAGS.max_tg_token_size = vocab.max_tg_token_size\n\n            if FLAGS.gen_slot_filling_training_data:\n                gen_slot_filling_training_data(FLAGS, [train_set, dev_set, test_set])\n\n            elif FLAGS.decode:\n                model = decode(dataset, buckets=train_set.buckets)\n                if not FLAGS.explain:\n                    eval(dataset, verbose=False)\n\n            elif FLAGS.demo:\n                demo(buckets=train_set.buckets)\n\n            elif FLAGS.grid_search:\n                meta_experiments.grid_search(\n                    train, decode, eval, train_set, dataset, FLAGS)\n            elif FLAGS.schedule_experiments:\n                schedule_experiments(\n                    train, decode, eval, train_set, dataset)\n            else:\n                # Train the model.\n                train(train_set, dataset)\n\n                if FLAGS.normalized:\n                    tf.compat.v1.reset_default_graph()\n                    gen_slot_filling_training_data(FLAGS, [train_set, dev_set, test_set])\n                    FLAGS.fill_argument_slots = True\n\n                # save model hyperparameters\n                model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n                with open(os.path.join(FLAGS.model_root_dir, model_subdir, \'hyperparameters.pkl\'), \'wb\') as o_f:\n                    flag_dict = dict()\n                    for flag in dir(FLAGS):\n                        flag_dict[flag] = getattr(FLAGS, flag)\n                    pickle.dump(flag_dict, o_f)\n\n                # Decode the new model on the development set.\n                tf.compat.v1.reset_default_graph()\n                model = decode(dataset, buckets=train_set.buckets)\n\n                # Run automatic evaluation on the development set.\n                if not FLAGS.explain:\n                    eval(dataset, verbose=True)\n\n    \nif __name__ == ""__main__"":\n    tf.compat.v1.app.run()\n'"
eval/__init__.py,0,b''
eval/error_analysis.py,0,"b'""""""\nError Analysis.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport functools\nimport os, sys\nimport random\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom bashlint import bash, data_tools\nfrom encoder_decoder import data_utils, graph_utils\nfrom eval import tree_dist\nfrom eval.eval_tools import load_predictions\nfrom eval.eval_tools import load_all_model_predictions\nfrom eval.eval_tools import load_cached_evaluations\nfrom eval.eval_tools import load_cached_correct_translations\nfrom eval.eval_tools import get_example_nl_key\n\n\nerror_types = {\n    0 : ""unmarked error"",\n    2 : ""extra utility"",\n    3 : ""missing utility"",\n    4 : ""confused utility"",\n    5 : ""extra flag"",\n    6 : ""missing flag"",\n    7 : ""confused flag"",\n    8 : ""logic error"",\n    9 : ""count error""\n}\n\n\ndef gen_manual_evaluation_csv_single_model(dataset, FLAGS):\n    """"""\n    Generate .csv spreadsheet for manual evaluation on dev/test set\n    examples for a specific model.\n    """"""\n    # Group dataset\n    tokenizer_selector = ""cm"" if FLAGS.explain else ""nl""\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n\n    # Load model predictions\n    model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n    model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n    prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n    prediction_list = load_predictions(prediction_path, top_k=3)\n    if len(grouped_dataset) != len(prediction_list):\n        raise ValueError(""ground truth list and prediction list length must ""\n                         ""be equal: {} vs. {}"".format(len(grouped_dataset),\n                                                      len(prediction_list)))\n\n    # Load additional ground truths\n    template_translations, command_translations = load_cached_correct_translations(FLAGS.data_dir)\n\n    # Load cached evaluation results\n    structure_eval_cache, command_eval_cache = load_cached_evaluations(\n        os.path.join(FLAGS.data_dir, \'manual_judgements\'))\n\n    eval_bash = FLAGS.dataset.startswith(""bash"")\n    cmd_parser = data_tools.bash_parser if eval_bash else data_tools.paren_parser\n\n    output_path = os.path.join(model_dir, \'manual.evaluations.single.model\')\n    with open(output_path, \'w\') as o_f:\n        # write spreadsheet header\n        o_f.write(\'id,description,command,correct template,correct command\\n\')\n        for example_id in range(len(grouped_dataset)):\n            data_group = grouped_dataset[example_id][1]\n            sc_txt = data_group[0].sc_txt.strip()\n            sc_key = get_example_nl_key(sc_txt)\n            command_gts = [dp.tg_txt for dp in data_group]\n            command_gts = set(command_gts + command_translations[sc_key])\n            command_gt_asts = [data_tools.bash_parser(cmd) for cmd in command_gts]\n            template_gts = [data_tools.cmd2template(cmd, loose_constraints=True) for cmd in command_gts]\n            template_gts = set(template_gts + template_translations[sc_key])\n            template_gt_asts = [data_tools.bash_parser(temp) for temp in template_gts]\n            predictions = prediction_list[example_id]\n            for i in xrange(3):\n                if i >= len(predictions):\n                    o_f.write(\',,,n,n\\n\')\n                    continue\n                pred_cmd = predictions[i]\n                pred_tree = cmd_parser(pred_cmd)\n                pred_temp = data_tools.ast2template(pred_tree, loose_constraints=True)\n                temp_match = tree_dist.one_match(\n                    template_gt_asts, pred_tree, ignore_arg_value=True)\n                str_match = tree_dist.one_match(\n                    command_gt_asts, pred_tree, ignore_arg_value=False)\n                # Match ground truths & exisitng judgements\n                command_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n                structure_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n                command_eval, structure_eval = \'\', \'\'\n                if str_match:\n                    command_eval = \'y\'\n                    structure_eval = \'y\'\n                elif temp_match:\n                    structure_eval = \'y\'\n                if command_eval_cache and \\\n                        command_example_sig in command_eval_cache:\n                    command_eval = command_eval_cache[command_example_sig]\n                if structure_eval_cache and \\\n                        structure_example_sig in structure_eval_cache:\n                    structure_eval = structure_eval_cache[structure_example_sig]\n                if i == 0:\n                    o_f.write(\'{},""{}"",""{}"",{},{}\\n\'.format(\n                        example_id, sc_txt.replace(\'""\', \'""""\'), pred_cmd.replace(\'""\', \'""""\'),\n                        structure_eval, command_eval))\n                else:\n                    o_f.write(\',,""{}"",{},{}\\n\'.format(\n                        pred_cmd.replace(\'""\', \'""""\'), structure_eval, command_eval))\n    print(\'manual evaluation spreadsheet saved to {}\'.format(output_path))\n\n\ndef gen_manual_evaluation_csv(dataset, FLAGS, num_examples=100):\n    """"""\n    Generate .csv spreadsheet for manual evaluation on a fixed set of test/dev\n    examples, predictions of different models are listed side-by-side.\n    """"""\n    # Group dataset\n    tokenizer_selector = ""cm"" if FLAGS.explain else ""nl""\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n\n    model_names, model_predictions = load_all_model_predictions(\n        grouped_dataset, FLAGS, top_k=3)\n\n    # Get FIXED dev set samples\n    random.seed(100)\n    example_ids = list(range(len(grouped_dataset)))\n    random.shuffle(example_ids)\n    sample_ids = example_ids[num_examples:num_examples+100]\n\n    # Load cached evaluation results\n    structure_eval_cache, command_eval_cache = \\\n        load_cached_evaluations(\n            os.path.join(FLAGS.data_dir, \'manual_judgements\'))\n\n    eval_bash = FLAGS.dataset.startswith(""bash"")\n    cmd_parser = data_tools.bash_parser if eval_bash \\\n        else data_tools.paren_parser\n\n    output_path = os.path.join(FLAGS.data_dir, \'manual.evaluations.csv\')\n    with open(output_path, \'w\') as o_f:\n        o_f.write(\'example_id, description, ground_truth, model, prediction, \'\n                  \'correct template, correct command\\n\')\n        for example_id in sample_ids:\n            data_group = grouped_dataset[example_id][1]\n            sc_txt = data_group[0].sc_txt.strip()\n            sc_key = get_example_nl_key(sc_txt)\n            command_gts = [dp.tg_txt for dp in data_group]\n            command_gt_asts = [data_tools.bash_parser(gt) for gt in command_gts]\n            for model_id, model_name in enumerate(model_names):\n                predictions = model_predictions[model_id][example_id]\n                for i in xrange(min(3, len(predictions))):\n                    if model_id == 0 and i == 0:\n                        output_str = \'{},""{}"",\'.format(example_id, sc_txt.replace(\'""\', \'""""\'))\n                    else:\n                        output_str = \',,\'\n                    pred_cmd = predictions[i]\n                    pred_tree = cmd_parser(pred_cmd)\n                    pred_temp = data_tools.ast2template(pred_tree, loose_constraints=True)\n                    temp_match = tree_dist.one_match(\n                        command_gt_asts, pred_tree, ignore_arg_value=True)\n                    str_match = tree_dist.one_match(\n                        command_gt_asts, pred_tree, ignore_arg_value=False)\n                    if (model_id * min(3, len(predictions)) + i) < len(command_gts):\n                        output_str += \'""{}"",\'.format(\n                            command_gts[model_id * min(\n                                3, len(predictions)) + i].strip().replace(\'""\', \'""""\'))\n                    else:\n                        output_str += \',\'\n                    output_str += \'{},""{}"",\'.format(model_name, pred_cmd.replace(\'""\', \'""""\'))\n\n                    command_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n                    structure_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n                    command_eval, structure_eval = \'\', \'\'\n                    if str_match:\n                        command_eval = \'y\'\n                        structure_eval = \'y\'\n                    elif temp_match:\n                        structure_eval = \'y\'\n                    if command_eval_cache and \\\n                            command_example_sig in command_eval_cache:\n                        command_eval = command_eval_cache[command_example_sig]\n                    if structure_eval_cache and \\\n                            structure_example_sig in structure_eval_cache:\n                        structure_eval = structure_eval_cache[structure_example_sig]\n                    output_str += \'{},{}\'.format(structure_eval, command_eval)\n                    o_f.write(\'{}\\n\'.format(output_str))\n\n    print(\'Manual evaluation results saved to {}\'.format(output_path))\n\n\ndef tabulate_example_predictions(dataset, FLAGS, num_examples=100):\n    # Group dataset\n    tokenizer_selector = ""cm"" if FLAGS.explain else ""nl""\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n\n    model_names, model_predictions = load_all_model_predictions(\n        grouped_dataset, FLAGS, top_k=1)\n\n    # Get FIXED dev set samples\n    random.seed(100)\n    example_ids = list(range(len(grouped_dataset)))\n    random.shuffle(example_ids)\n    sample_ids = example_ids[:num_examples]\n\n    # Load cached evaluation results\n    structure_eval_cache, command_eval_cache = \\\n        load_cached_evaluations(\n            os.path.join(FLAGS.data_dir, \'manual_judgements\'))\n\n    eval_bash = FLAGS.dataset.startswith(""bash"")\n    cmd_parser = data_tools.bash_parser if eval_bash \\\n        else data_tools.paren_parser\n\n    model_name_pt = {\n        \'token-seq2seq\': \'T-Seq2Seq\',\n        \'tellina\': \'Tellina\',\n        \'token-copynet\': \'T-CopyNet\',\n        \'partial.token-seq2seq\': \'ST-Seq2Seq\',\n        \'partial.token-copynet\': \'ST-CopyNet\',\n        \'char-seq2seq\': \'C-Seq2Seq\',\n        \'char-copynet\': \'C-CopyNet\'\n    }\n\n    for example_id in sample_ids:\n        print(\'Example {}\'.format(example_id))\n        data_group = grouped_dataset[example_id][1]\n        sc_txt = data_group[0].sc_txt.strip()\n        sc_key = get_example_nl_key(sc_txt)\n        command_gts = [dp.tg_txt for dp in data_group]\n        command_gt_asts = [data_tools.bash_parser(gt) for gt in command_gts]\n        output_strs = {}\n        for model_id, model_name in enumerate(model_names):\n            predictions = model_predictions[model_id][example_id]\n            for i in xrange(min(3, len(predictions))):\n                pred_cmd = predictions[i]\n                pred_tree = cmd_parser(pred_cmd)\n                pred_temp = data_tools.ast2template(pred_tree, loose_constraints=True)\n                temp_match = tree_dist.one_match(\n                    command_gt_asts, pred_tree, ignore_arg_value=True)\n                str_match = tree_dist.one_match(\n                    command_gt_asts, pred_tree, ignore_arg_value=False)\n                \n                output_str = \'& \\\\<{}> & {}\'.format(pred_cmd.replace(\'__SP__\', \'\')\n                                                           .replace(\'_\', \'\\\\_\')\n                                                           .replace(\'$\', \'\\\\$\')\n                                                           .replace(\'%\', \'\\\\%\')\n                                                           .replace(\'{{}}\', \'\\\\ttcbs\'), \n                                                   model_name_pt[model_name])\n\n                command_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n                structure_example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n                command_eval, structure_eval = \'\', \'\'\n                if str_match:\n                    command_eval = \'y\'\n                    structure_eval = \'y\'\n                elif temp_match:\n                    structure_eval = \'y\'\n                if command_eval_cache and \\\n                        command_example_sig in command_eval_cache:\n                    command_eval = command_eval_cache[command_example_sig]\n                if structure_eval_cache and \\\n                        structure_example_sig in structure_eval_cache:\n                    structure_eval = structure_eval_cache[structure_example_sig]\n                output_str += \', {},{} \\\\\\\\\'.format(structure_eval, command_eval)\n            output_strs[model_name] = output_str\n        for model_name in [\'char-seq2seq\', \n                           \'char-copynet\', \n                           \'token-seq2seq\', \n                           \'token-copynet\',\n                           \'partial.token-seq2seq\',\n                           \'partial.token-copynet\',\n                           \'tellina\']:\n            if model_name == \'char-seq2seq\':\n                print(\'\\\\multirow{{7}}{{*}}{{\\\\specialcell{{{}}}}} \'.format(sc_txt) + output_strs[model_name])\n            else:\n                print(output_strs[model_name])\n        output_str = \'& \\<{}> & Human \\\\\\\\\'.format(command_gts[0].replace(\'__SP__\', \'\')\n                                                           .replace(\'_\', \'\\\\_\')\n                                                           .replace(\'$\', \'\\\\$\')\n                                                           .replace(\'%\', \'\\\\%\')\n                                                           .replace(\'{{}}\', \'\\\\ttcbs\'))\n        print(output_str)\n        print()\n\n\ndef print_error_analysis_csv(grouped_dataset, prediction_list, FLAGS,\n        cached_evaluation_results=None, group_by_utility=False,\n        error_predictions_only=True):\n    """"""\n    Convert dev/test set examples to csv format so as to make it easier for\n    human annotators to enter their judgements.\n\n    :param grouped_dataset: dev/test set grouped by natural language.\n    :param prediction_list: model predictions.\n    :param FLAGS: experiment hyperparameters.\n    :param cached_evaluation_results: cached evaluation results from previous\n        rounds.\n    :param group_by_utility: if set, group the error examples by the utilities\n        used in the ground truth.\n    """"""\n    def mark_example(error_list, example, gt_utility=None):\n        if gt_utility:\n            error_list[gt_utility].append(example)\n        else:\n            error_list.append(example)\n\n    eval_bash = FLAGS.dataset.startswith(""bash"")\n    cmd_parser = data_tools.bash_parser if eval_bash \\\n        else data_tools.paren_parser\n    if group_by_utility:\n        utility_index = {}\n        for line in bash.utility_stats.split(\'\\n\'):\n            ind, utility, _, _ = line.split(\',\')\n            utility_index[utility] = ind\n\n    grammar_errors = collections.defaultdict(list) if group_by_utility else []\n    argument_errors = collections.defaultdict(list) if group_by_utility else []\n    example_id = 0\n    for nl_temp, data_group in grouped_dataset:\n        sc_txt = data_group[0].sc_txt.strip()\n        sc_temp = get_example_nl_key(sc_txt)\n        tg_strs = [dp.tg_txt for dp in data_group]\n        gt_trees = [cmd_parser(cm_str) for cm_str in tg_strs]\n        if group_by_utility:\n            gt_utilities = functools.reduce(lambda x,y:x|y,\n                [data_tools.get_utilities(gt) for gt in gt_trees])\n            gt_utility = sorted(\n                list(gt_utilities), key=lambda x:int(utility_index[x]))[-1]\n        else:\n            gt_utility = None\n        predictions = prediction_list[example_id]\n        example_id += 1\n        example = []\n        grammar_error, argument_error = False, False\n        for i in xrange(min(3, len(predictions))):\n            if i == 0:\n                output_str = \'{},""{}"",\'.format(\n                    example_id, sc_txt.replace(\'""\', \'""""\'))\n            else:\n                output_str = \',,\'\n            pred_cmd = predictions[i]\n            tree = cmd_parser(pred_cmd)\n\n            # evaluation ignoring flag orders\n            temp_match = tree_dist.one_match(\n                gt_trees, tree, ignore_arg_value=True)\n            str_match = tree_dist.one_match(\n                gt_trees, tree, ignore_arg_value=False)\n            if i < len(tg_strs):\n                output_str += \'""{}"",\'.format(\n                    tg_strs[i].strip().replace(\'""\', \'""""\'))\n            else:\n                output_str += \',\'\n            output_str += \'""{}"",\'.format(pred_cmd.replace(\'""\', \'""""\'))\n            if not str_match:\n                if temp_match:\n                    if i == 0:\n                        argument_error = True\n                        grammar_error = True\n                else:\n                    if i == 0:\n                        grammar_error = True\n\n            example_sig = \'{}<NL_PREDICTION>{}\'.format(sc_temp, pred_cmd)\n            if cached_evaluation_results and \\\n                    example_sig in cached_evaluation_results:\n                output_str += cached_evaluation_results[example_sig]\n            else:\n                if str_match:\n                    output_str += \'y,y\'\n                elif temp_match:\n                    output_str += \'y,\'\n            example.append(output_str)\n        if error_predictions_only:\n            if grammar_error:\n                mark_example(grammar_errors, example, gt_utility)\n            elif argument_error:\n                mark_example(argument_errors, example, gt_utility)\n        else:\n            mark_example(grammar_errors, example, gt_utility)\n\n    return grammar_errors, argument_errors\n\n\ndef gen_error_analysis_csv(model_dir, decode_sig, dataset, FLAGS, top_k=3):\n    """"""\n    Generate error analysis evaluation spreadsheet.\n        - grammar error analysis\n        - argument error analysis\n    """"""\n    # Group dataset\n    tokenizer_selector = ""cm"" if FLAGS.explain else ""nl""\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n\n    # Load model predictions\n    prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n    prediction_list = load_predictions(prediction_path, top_k)\n    if len(grouped_dataset) != len(prediction_list):\n        raise ValueError(""ground truth and predictions length must be equal: ""\n            ""{} vs. {}"".format(len(grouped_dataset), len(prediction_list)))\n\n    # Convert the predictions to csv format\n    grammar_errors, argument_errors = print_error_analysis_csv(\n        grouped_dataset, prediction_list, FLAGS)\n\n    grammar_error_path = os.path.join(model_dir, \'grammar.error.analysis.csv\')\n    random.shuffle(grammar_errors)\n    with open(grammar_error_path, \'w\') as grammar_error_file:\n        print(""Saving grammar errors to {}"".format(grammar_error_path))\n        # print csv file header\n        grammar_error_file.write(\n            \'example_id, description, ground_truth, prediction, \' +\n            \'correct template, correct command\\n\')\n        for example in grammar_errors[:100]:\n            for line in example:\n                grammar_error_file.write(\'{}\\n\'.format(line))\n\n    arg_error_path = os.path.join(model_dir, \'argument.error.analysis.csv\')\n    random.shuffle(argument_errors)\n    with open(arg_error_path, \'w\') as arg_error_file:\n        print(""Saving argument errors to {}"".format(arg_error_path))\n        # print csv file header\n        arg_error_file.write(\n            \'example_id, description, ground_truth, prediction, \' +\n            \'correct template, correct command\\n\')\n        for example in argument_errors[:100]:\n            for line in example:\n                arg_error_file.write(\'{}\\n\'.format(line))\n\n\ndef gen_error_analysis_csv_by_utility(model_dir, decode_sig, dataset, FLAGS, top_k=10):\n    """"""\n    Generate error analysis evaluation sheet grouped by utility.\n    """"""\n    # Group dataset\n    tokenizer_selector = ""cm"" if FLAGS.explain else ""nl""\n    grouped_dataset = data_utils.group_parallel_data(\n        dataset, tokenizer_selector=tokenizer_selector)\n\n    # Load model predictions\n    prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n    prediction_list = load_predictions(prediction_path, top_k)\n    if len(grouped_dataset) != len(prediction_list):\n        raise ValueError(\n            ""ground truth and predictions length must be equal: {} vs. {}""\n            .format(len(grouped_dataset), len(prediction_list)))\n\n    # Load cached evaluation results\n    cached_evaluation_results = load_cached_evaluations(model_dir)\n\n    # Convert the predictions into csv format\n    grammar_errors, argument_errors = print_error_analysis_csv(\n        grouped_dataset, prediction_list, FLAGS, cached_evaluation_results,\n        group_by_utility=True, error_predictions_only=False)\n\n    error_by_utility_path = \\\n        os.path.join(model_dir, \'error.analysis.by.utility.csv\')\n    print(""Saving grammar errors to {}"".format(error_by_utility_path))\n    with open(error_by_utility_path, \'w\') as error_by_utility_file:\n        # print csv file header\n        error_by_utility_file.write(\n            \'utility, example_id, description, groundtruth, prediction, \'\n            \'correct template, correct command\\n\')\n        for line in bash.utility_stats.split(\'\\n\'):\n            utility = line.split(\',\')[1]\n            error_examples = grammar_errors[utility]\n            if len(error_examples) <= 5:\n                for example in error_examples:\n                    for l in example:\n                        error_by_utility_file.write(\'{},{}\\n\'.format(utility, l))\n            else:\n                random.shuffle(error_examples)\n                for example in error_examples[:5]:\n                    for l in example:\n                        error_by_utility_file.write(\'{},{}\\n\'.format(utility, l))\n\n\ndef gen_accuracy_by_utility_csv(eval_by_utility_path):\n    """"""\n    Generate accuracy by utility spreadsheet table based on the evaluation by\n    utility spreadsheet.\n    """"""\n    num_template_correct = collections.defaultdict(int)\n    num_command_correct = collections.defaultdict(int)\n    num_annotation_errors = collections.defaultdict(int)\n    num_complex_tasks = collections.defaultdict(int)\n    num_examples = collections.defaultdict(int)\n    with open(eval_by_utility_path) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            utility = row[\'utility\']\n            num_examples[utility] += 1\n            if row[\'correct template\'] == \'y\':\n                num_template_correct[utility] += 1\n            if row[\'correct command\'] == \'y\':\n                num_command_correct[utility] += 1\n            if row[\'correct template\'] == \'poor description\':\n                num_annotation_errors[utility] += 1\n            if row[\'correct template\'] == \'complex task\':\n                num_complex_tasks[utility] += 1\n    output_path = os.path.join(os.path.dirname(eval_by_utility_path),\n                               \'accuracy.by.utility.csv\')\n    print(\'Save accuracy by utility metrics to {}\'.format(output_path))\n    with open(output_path, \'w\') as o_f:\n        # print csv file header\n        o_f.write(\'ID,utility,# flags,# train,# test,template accuracy,\'\n                  \'command accuracy,% annotation errors,% complex tasks,\'\n                  \'% annotation problems\\n\')\n        for line in bash.utility_stats.split(\'\\n\'):\n            utility = line.split(\',\')[1]\n            if utility in num_examples:\n                num_exps = num_examples[utility]\n                template_acc = round(\n                    float(num_template_correct[utility]) / num_exps, 2)\n                command_acc = round(\n                    float(num_command_correct[utility]) / num_exps, 2)\n                annotation_error_rate = round(\n                    float(num_annotation_errors[utility]) / num_exps, 2)\n                complex_task_rate = round(\n                    float(num_complex_tasks[utility]) / num_exps, 2)\n                o_f.write(\'{},{},{},{},{},{}\\n\'.format(line, template_acc,\n                    command_acc, annotation_error_rate, complex_task_rate,\n                    (annotation_error_rate+complex_task_rate)))\n'"
eval/eval_tools.py,0,"b'""""""\nEvaluate system performance.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport nltk\nimport numpy as np\nimport os, sys\nimport random\n\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom bashlint import data_tools\nfrom encoder_decoder import data_utils, graph_utils\nfrom eval import token_based, tree_dist\nfrom nlp_tools import constants, tokenizer\n\n\ndef manual_eval(prediction_path, dataset, FLAGS, top_k, num_examples=-1, interactive=True, verbose=True):\n    """"""\n    Conduct dev/test set evaluation.\n\n    Evaluation metrics:\n        1) full command accuracy;\n        2) command template accuracy. \n\n    :param interactive:\n        - If set, prompt the user to enter judgement if a prediction does not\n            match any of the groundtruths and the correctness of the prediction\n            has not been pre-determined;\n          Otherwise, all predictions that does not match any of the groundtruths are counted as wrong.\n    """"""\n    # Group dataset\n    grouped_dataset = data_utils.group_parallel_data(dataset)\n\n    # Load model prediction\n    prediction_list = load_predictions(prediction_path, top_k)\n\n    metrics = get_manual_evaluation_metrics(\n        grouped_dataset, prediction_list, FLAGS, num_examples=num_examples, interactive=interactive, verbose=verbose)\n\n    return metrics\n\n\ndef gen_manual_evaluation_table(dataset, FLAGS, num_examples=-1, interactive=True):\n    """"""\n    Conduct dev/test set evaluation. The results of multiple pre-specified models are tabulated in the same table.\n\n    Evaluation metrics:\n        1) full command accuracy;\n        2) command template accuracy.\n        \n    :param interactive:\n        - If set, prompt the user to enter judgement if a prediction does not\n            match any of the groundtruths and the correctness of the prediction\n            has not been pre-determined;\n          Otherwise, all predictions that does not match any of the groundtruths are counted as wrong.\n    """"""\n    # Group dataset\n    grouped_dataset = data_utils.group_parallel_data(dataset)\n\n    # Load all model predictions\n    model_names, model_predictions = load_all_model_predictions(grouped_dataset, FLAGS, top_k=3)\n\n    manual_eval_metrics = {}\n    for model_id, model_name in enumerate(model_names):\n        prediction_list = model_predictions[model_names]\n        M = get_manual_evaluation_metrics(\n            grouped_dataset, prediction_list, FLAGS, num_examples=num_examples, interactive=interactive, verbose=False)\n        manual_eval_metrics[model_name] = [M[\'acc_f\'][0], M[\'acc_f\'[1]], M[\'acc_t\'][0], M[\'acc_t\'][1]]\n\n    metrics_names = [\'Acc_F_1\', \'Acc_F_3\', \'Acc_T_1\', \'Acc_T_3\']\n    print_eval_table(model_names, metrics_names, manual_eval_metrics)\n\n\ndef get_manual_evaluation_metrics(grouped_dataset, prediction_list, FLAGS, num_examples=-1, interactive=True,\n                                  verbose=True):\n\n    if len(grouped_dataset) != len(prediction_list):\n        raise ValueError(""ground truth and predictions length must be equal: ""\n                         ""{} vs. {}"".format(len(grouped_dataset), len(prediction_list)))\n\n    # Get dev set samples (fixed)\n    random.seed(100)\n    example_ids = list(range(len(grouped_dataset)))\n    random.shuffle(example_ids)\n    if num_examples > 0:\n        sample_ids = example_ids[:num_examples]\n    else:\n        sample_ids = example_ids\n\n    # Load cached evaluation results\n    structure_eval_cache, command_eval_cache = \\\n        load_cached_evaluations(\n            os.path.join(FLAGS.data_dir, \'manual_judgements\'), verbose=True)\n\n    eval_bash = FLAGS.dataset.startswith(""bash"")\n    cmd_parser = data_tools.bash_parser if eval_bash \\\n        else data_tools.paren_parser\n\n    # Interactive manual evaluation\n    num_t_top_1_correct = 0.0\n    num_f_top_1_correct = 0.0\n    num_t_top_3_correct = 0.0\n    num_f_top_3_correct = 0.0\n\n    for exam_id, example_id in enumerate(sample_ids):\n        data_group = grouped_dataset[example_id][1]\n        sc_txt = data_group[0].sc_txt.strip()\n        sc_key = get_example_nl_key(sc_txt)\n        command_gts = [dp.tg_txt for dp in data_group]\n        command_gt_asts = [data_tools.bash_parser(gt) for gt in command_gts]\n        predictions = prediction_list[example_id]\n        top_3_s_correct_marked = False\n        top_3_f_correct_marked = False\n        for i in xrange(min(3, len(predictions))):\n            pred_cmd = predictions[i]\n            pred_ast = cmd_parser(pred_cmd)\n            pred_temp = data_tools.ast2template(pred_ast, loose_constraints=True)\n            temp_match = tree_dist.one_match(\n                command_gt_asts, pred_ast, ignore_arg_value=True)\n            str_match = tree_dist.one_match(\n                command_gt_asts, pred_ast, ignore_arg_value=False)\n            # Match ground truths & exisitng judgements\n            command_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n            structure_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n            command_eval, structure_eval = \'\', \'\'\n            if str_match:\n                command_eval = \'y\'\n                structure_eval = \'y\'\n            elif temp_match:\n                structure_eval = \'y\'\n            if command_eval_cache and command_example_key in command_eval_cache:\n                command_eval = command_eval_cache[command_example_key]\n            if structure_eval_cache and structure_example_key in structure_eval_cache:\n                structure_eval = structure_eval_cache[structure_example_key]\n            # Prompt for new judgements\n            if command_eval != \'y\':\n                if structure_eval == \'y\':\n                    if not command_eval and interactive:\n                        print(\'#{}. {}\'.format(exam_id, sc_txt))\n                        for j, gt in enumerate(command_gts):\n                            print(\'- GT{}: {}\'.format(j, gt))\n                        print(\'> {}\'.format(pred_cmd))\n                        command_eval = input(\n                            \'CORRECT COMMAND? [y/reason] \')\n                        add_judgement(FLAGS.data_dir, sc_txt, pred_cmd,\n                                      structure_eval, command_eval)\n                        print()\n                else:\n                    if not structure_eval and interactive:\n                        print(\'#{}. {}\'.format(exam_id, sc_txt))\n                        for j, gt in enumerate(command_gts):\n                            print(\'- GT{}: {}\'.format(j, gt))\n                        print(\'> {}\'.format(pred_cmd))\n                        structure_eval = input(\n                            \'CORRECT STRUCTURE? [y/reason] \')\n                        if structure_eval == \'y\':\n                            command_eval = input(\n                                \'CORRECT COMMAND? [y/reason] \')\n                        add_judgement(FLAGS.data_dir, sc_txt, pred_cmd,\n                                      structure_eval, command_eval)\n                        print()\n                structure_eval_cache[structure_example_key] = structure_eval\n                command_eval_cache[command_example_key] = command_eval\n            if structure_eval == \'y\':\n                if i == 0:\n                    num_t_top_1_correct += 1\n                if not top_3_s_correct_marked:\n                    num_t_top_3_correct += 1\n                    top_3_s_correct_marked = True\n            if command_eval == \'y\':\n                if i == 0:\n                    num_f_top_1_correct += 1\n                if not top_3_f_correct_marked:\n                    num_f_top_3_correct += 1\n                    top_3_f_correct_marked = True\n\n    metrics = {}\n    acc_f_1 = num_f_top_1_correct / len(sample_ids)\n    acc_f_3 = num_f_top_3_correct / len(sample_ids)\n    acc_t_1 = num_t_top_1_correct / len(sample_ids)\n    acc_t_3 = num_t_top_3_correct / len(sample_ids)\n    metrics[\'acc_f\'] = [acc_f_1, acc_f_3]\n    metrics[\'acc_t\'] = [acc_t_1, acc_t_3]\n\n    if verbose:\n        print(\'{} examples evaluated\'.format(len(sample_ids)))\n        print(\'Top 1 Command Acc = {:.3f}\'.format(acc_f_1))\n        print(\'Top 3 Command Acc = {:.3f}\'.format(acc_f_3))\n        print(\'Top 1 Template Acc = {:.3f}\'.format(acc_t_1))\n        print(\'Top 3 Template Acc = {:.3f}\'.format(acc_t_3))\n    return metrics\n\n\ndef add_judgement(data_dir, nl, command, correct_template=\'\', correct_command=\'\'):\n    """"""\n    Append a new judgement\n    """"""\n    data_dir = os.path.join(data_dir, \'manual_judgements\')\n    manual_judgement_path = os.path.join(\n        data_dir, \'manual.evaluations.author\')\n    if not os.path.exists(manual_judgement_path):\n        with open(manual_judgement_path, \'w\') as o_f:\n            o_f.write(\n                \'description,prediction,template,correct template,correct command\\n\')\n    with open(manual_judgement_path, \'a\') as o_f:\n        temp = data_tools.cmd2template(command, loose_constraints=True)\n        if not correct_template:\n            correct_template = \'n\'\n        if not correct_command:\n            correct_command = \'n\'\n        o_f.write(\'""{}"",""{}"",""{}"",""{}"",""{}""\\n\'.format(\n            nl.replace(\'""\', \'""""\'), command.replace(\'""\', \'""""\'),\n            temp.replace(\'""\', \'""""\'), correct_template.replace(\'""\', \'""""\'),\n            correct_command.replace(\'""\', \'""""\')))\n    print(\'new judgement added to {}\'.format(manual_judgement_path))\n\n\ndef automatic_eval(prediction_path, dataset, FLAGS, top_k, num_samples=-1, verbose=False):\n    """"""\n    Generate automatic evaluation metrics on dev/test set.\n    The following metrics are computed:\n        Top 1,3,5,10\n            1. Structure accuracy\n            2. Full command accuracy\n            3. Command keyword overlap\n            4. BLEU\n    """"""\n    grouped_dataset = data_utils.group_parallel_data(dataset)\n    try:\n        vocabs = data_utils.load_vocabulary(FLAGS)\n    except ValueError:\n        vocabs = None\n\n    # Load predictions\n    prediction_list = load_predictions(prediction_path, top_k)\n    if len(grouped_dataset) != len(prediction_list):\n        raise ValueError(""ground truth and predictions length must be equal: ""\n                         ""{} vs. {}"".format(len(grouped_dataset), len(prediction_list)))\n\n    metrics = get_automatic_evaluation_metrics(grouped_dataset, prediction_list, vocabs, FLAGS,\n                                               top_k, num_samples, verbose)\n    return metrics\n\n\ndef gen_automatic_evaluation_table(dataset, FLAGS):\n    # Group dataset\n    grouped_dataset = data_utils.group_parallel_data(dataset)\n    vocabs = data_utils.load_vocabulary(FLAGS)\n\n    model_names, model_predictions = load_all_model_predictions(grouped_dataset, FLAGS, top_k=3)\n    auto_eval_metrics = {}\n    for model_id, model_name in enumerate(model_names):\n        prediction_list = model_predictions[model_id]\n        if prediction_list is not None:\n            M = get_automatic_evaluation_metrics(\n                grouped_dataset, prediction_list, vocabs, FLAGS, top_k=3)\n            auto_eval_metrics[model_name] = [M[\'bleu\'][0], M[\'bleu\'][1], M[\'cms\'][0], M[\'cms\'][1]]\n        else:\n            print(\'Model {} skipped in evaluation\'.format(model_name))\n    metrics_names = [\'BLEU1\', \'BLEU3\', \'TM1\', \'TM3\']\n    print_eval_table(model_names, metrics_names, auto_eval_metrics)\n\n\ndef get_automatic_evaluation_metrics(grouped_dataset, prediction_list, vocabs, FLAGS, top_k,\n                                     num_samples=-1, verbose=False):\n    cmd_parser = data_tools.bash_parser\n    rev_sc_vocab = vocabs.rev_sc_vocab if vocabs is not None else None\n\n\n    # Load cached evaluation results\n    structure_eval_cache, command_eval_cache = \\\n        load_cached_evaluations(\n            os.path.join(FLAGS.data_dir, \'manual_judgements\'))\n\n    # Compute manual evaluation scores on a subset of examples\n    if num_samples > 0:\n        # Get FIXED dev set samples\n        random.seed(100)\n        example_ids = list(range(len(grouped_dataset)))\n        random.shuffle(example_ids)\n        sample_ids = example_ids[:100]\n        grouped_dataset = [grouped_dataset[i] for i in sample_ids]\n        prediction_list = [prediction_list[i] for i in sample_ids]\n\n    num_eval = 0\n    top_k_temp_correct = np.zeros([len(grouped_dataset), top_k])\n    top_k_str_correct = np.zeros([len(grouped_dataset), top_k])\n    top_k_cms = np.zeros([len(grouped_dataset), top_k])\n    top_k_bleu = np.zeros([len(grouped_dataset), top_k])\n\n    command_gt_asts_list, pred_ast_list = [], []\n    \n    for data_id in xrange(len(grouped_dataset)):\n        _, data_group = grouped_dataset[data_id]\n        sc_str = data_group[0].sc_txt.strip()\n        sc_key = get_example_nl_key(sc_str)\n        if vocabs is not None:\n            sc_tokens = [rev_sc_vocab[i] for i in data_group[0].sc_ids]\n            if FLAGS.channel == \'char\':\n                sc_features = \'\'.join(sc_tokens)\n                sc_features = sc_features.replace(constants._SPACE, \' \')\n            else:\n                sc_features = \' \'.join(sc_tokens)\n        command_gts = [dp.tg_txt.strip() for dp in data_group]\n        command_gt_asts = [cmd_parser(cmd) for cmd in command_gts]\n        command_gt_asts_list.append(command_gt_asts)\n        template_gts = [data_tools.cmd2template(cmd, loose_constraints=True) for cmd in command_gts]\n        template_gt_asts = [cmd_parser(temp) for temp in template_gts]\n        if verbose:\n            print(""Example {}"".format(data_id))\n            print(""Original Source: {}"".format(sc_str.encode(\'utf-8\')))\n            if vocabs is not None:\n                print(""Source: {}"".format([x.encode(\'utf-8\') for x in sc_features]))\n            for j, command_gt in enumerate(command_gts):\n                print(""GT Target {}: {}"".format(j + 1, command_gt.strip().encode(\'utf-8\')))\n        num_eval += 1\n        predictions = prediction_list[data_id]\n        for i in xrange(len(predictions)):\n            pred_cmd = predictions[i]\n            pred_ast = cmd_parser(pred_cmd)\n            if i == 0:\n                pred_ast_list.append(pred_ast)\n            pred_temp = data_tools.cmd2template(pred_cmd, loose_constraints=True)\n            # A) Exact match with ground truths & exisitng judgements\n            command_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n            structure_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n            # B) Match ignoring flag orders\n            temp_match = tree_dist.one_match(\n                template_gt_asts, pred_ast, ignore_arg_value=True)\n            str_match = tree_dist.one_match(\n                command_gt_asts, pred_ast, ignore_arg_value=False)\n            if command_eval_cache and command_example_key in command_eval_cache:\n                str_match = normalize_judgement(command_eval_cache[command_example_key]) == \'y\'\n            if structure_eval_cache and structure_example_key in structure_eval_cache:\n                temp_match = normalize_judgement(structure_eval_cache[structure_example_key]) == \'y\'\n            if temp_match:\n                top_k_temp_correct[data_id, i] = 1\n            if str_match:\n                top_k_str_correct[data_id, i] = 1\n            cms = token_based.command_match_score(command_gt_asts, pred_ast)\n            # if pred_cmd.strip():\n            #     bleu = token_based.sentence_bleu_score(command_gt_asts, pred_ast)   \n            # else:\n            #     bleu = 0\n            bleu = nltk.translate.bleu_score.sentence_bleu(command_gts, pred_cmd)\n            top_k_cms[data_id, i] = cms\n            top_k_bleu[data_id, i] = bleu\n            if verbose:\n                print(""Prediction {}: {} ({}, {})"".format(i + 1, pred_cmd, cms, bleu))\n        if verbose:\n            print()\n    \n    bleu = token_based.corpus_bleu_score(command_gt_asts_list, pred_ast_list)\n\n    top_temp_acc = [-1 for _ in [1, 3, 5, 10]]\n    top_cmd_acc = [-1 for _ in [1, 3, 5, 10]]\n    top_cms = [-1 for _ in [1, 3, 5, 10]]\n    top_bleu = [-1 for _ in [1, 3, 5, 10]]\n    top_temp_acc[0] = top_k_temp_correct[:, 0].mean()\n    top_cmd_acc[0] = top_k_str_correct[:, 0].mean()\n    top_cms[0] = top_k_cms[:, 0].mean()\n    top_bleu[0] = top_k_bleu[:, 0].mean()\n    print(""{} examples evaluated"".format(num_eval))\n    print(""Top 1 Template Acc = %.3f"" % top_temp_acc[0])\n    print(""Top 1 Command Acc = %.3f"" % top_cmd_acc[0])\n    print(""Average top 1 Template Match Score = %.3f"" % top_cms[0])\n    print(""Average top 1 BLEU Score = %.3f"" % top_bleu[0])\n    if len(predictions) > 1:\n        top_temp_acc[1] = np.max(top_k_temp_correct[:, :3], 1).mean()\n        top_cmd_acc[1] = np.max(top_k_str_correct[:, :3], 1).mean()\n        top_cms[1] = np.max(top_k_cms[:, :3], 1).mean()\n        top_bleu[1] = np.max(top_k_bleu[:, :3], 1).mean()\n        print(""Top 3 Template Acc = %.3f"" % top_temp_acc[1])\n        print(""Top 3 Command Acc = %.3f"" % top_cmd_acc[1])\n        print(""Average top 3 Template Match Score = %.3f"" % top_cms[1])\n        print(""Average top 3 BLEU Score = %.3f"" % top_bleu[1])\n    if len(predictions) > 3:\n        top_temp_acc[2] = np.max(top_k_temp_correct[:, :5], 1).mean()\n        top_cmd_acc[2] = np.max(top_k_str_correct[:, :5], 1).mean()\n        top_cms[2] = np.max(top_k_cms[:, :5], 1).mean()\n        top_bleu[2] = np.max(top_k_bleu[:, :5], 1).mean()\n        print(""Top 5 Template Acc = %.3f"" % top_temp_acc[2])\n        print(""Top 5 Command Acc = %.3f"" % top_cmd_acc[2])\n        print(""Average top 5 Template Match Score = %.3f"" % top_cms[2])\n        print(""Average top 5 BLEU Score = %.3f"" % top_bleu[2])\n    if len(predictions) > 5:\n        top_temp_acc[3] = np.max(top_k_temp_correct[:, :10], 1).mean()\n        top_cmd_acc[3] = np.max(top_k_str_correct[:, :10], 1).mean()\n        top_cms[3] = np.max(top_k_cms[:, :10], 1).mean()\n        top_bleu[3] = np.max(top_k_bleu[:, :10], 1).mean()\n        print(""Top 10 Template Acc = %.3f"" % top_temp_acc[3])\n        print(""Top 10 Command Acc = %.3f"" % top_cmd_acc[3])\n        print(""Average top 10 Template Match Score = %.3f"" % top_cms[3])\n        print(""Average top 10 BLEU Score = %.3f"" % top_bleu[3])\n    print(\'Corpus BLEU = %.3f\' % bleu)\n    print()\n\n    metrics = {}\n    metrics[\'acc_f\'] = top_cmd_acc\n    metrics[\'acc_t\'] = top_temp_acc\n    metrics[\'cms\'] = top_cms\n    metrics[\'bleu\'] = top_bleu\n\n    return metrics\n\n\ndef print_eval_table(model_names, metrics_names, model_metrics):\n\n    def pad_spaces(s, max_len):\n        return s + \' \' * (max_len - len(s))\n\n    # print evaluation table\n    # pad model names with spaces to create alignment\n    max_len = len(max(model_names, key=len))\n    max_len_with_offset = (int(max_len / 4) + 1) * 4\n    first_row = pad_spaces(\'Model\', max_len_with_offset)\n    for metrics_name in metrics_names:\n        first_row += \'{}    \'.format(metrics_name)\n    print(first_row.strip())\n    print(\'-\' * len(first_row))\n    for i, model_name in enumerate(model_names):\n        row = pad_spaces(model_name, max_len_with_offset)\n        for metrics in model_metrics[model_name]:\n            row += \'{:.2f}    \'.format(metrics)\n        print(row.strip())\n    print(\'-\' * len(first_row))\n\n\ndef load_all_model_predictions(grouped_dataset, FLAGS, top_k=1, model_names=(\'token_seq2seq\',\n                                                                             \'token_copynet\',\n                                                                             \'char_seq2seq\',\n                                                                             \'char_copynet\',\n                                                                             \'partial_token_seq2seq\',\n                                                                             \'partial_token_copynet\',\n                                                                             \'tellina\')):\n    """"""\n    Load predictions of multiple models (specified with ""model_names"").\n\n    :return model_predictions: List of model predictions.\n    """"""\n\n    def load_model_predictions():\n        model_subdir, decode_sig = graph_utils.get_decode_signature(FLAGS)\n        model_dir = os.path.join(FLAGS.model_root_dir, model_subdir)\n        prediction_path = os.path.join(model_dir, \'predictions.{}.latest\'.format(decode_sig))\n        prediction_list = load_predictions(prediction_path, top_k)\n        if prediction_list is not None and len(grouped_dataset) != len(prediction_list):\n            raise ValueError(""ground truth list and prediction list length must ""\n                             ""be equal: {} vs. {}"".format(len(grouped_dataset),\n                                                          len(prediction_list)))\n        return prediction_list\n\n    # Load model predictions\n    model_predictions = []\n\n    # -- Token\n    FLAGS.channel = \'token\'\n    FLAGS.normalized = False\n    FLAGS.fill_argument_slots = False\n    FLAGS.use_copy = False\n    # --- Seq2Seq\n    if \'token_seq2seq\' in model_names:\n        model_predictions.append(load_model_predictions())\n    # --- Tellina\n    if \'tellina\' in model_names:\n        FLAGS.normalized = True\n        FLAGS.fill_argument_slots = True\n        model_predictions.append(load_model_predictions())\n        FLAGS.normalized = False\n        FLAGS.fill_argument_slots = False\n    # --- CopyNet\n    if \'token_copynet\' in model_names:\n        FLAGS.use_copy = True\n        FLAGS.copy_fun = \'copynet\'\n        model_predictions.append(load_model_predictions())\n        FLAGS.use_copy = False\n    # -- Parital token\n    FLAGS.channel = \'partial.token\'\n    # --- Seq2Seq\n    if \'partial_token_seq2seq\' in model_names:\n        model_predictions.append(load_model_predictions())\n    # --- CopyNet\n    if \'partial_token_copynet\' in model_names:\n        FLAGS.use_copy = True\n        FLAGS.copy_fun = \'copynet\'\n        model_predictions.append(load_model_predictions())\n        FLAGS.use_copy = False\n    # -- Character\n    FLAGS.channel = \'char\'\n    FLAGS.batch_size = 32\n    FLAGS.min_vocab_frequency = 1\n    # --- Seq2Seq\n    if \'char_seq2seq\' in model_names:\n        model_predictions.append(load_model_predictions())\n    # --= CopyNet\n    if \'char_copynet\' in model_names:\n        FLAGS.use_copy = True\n        FLAGS.copy_fun = \'copynet\'\n        model_predictions.append(load_model_predictions())\n        FLAGS.use_copy = False\n\n    return model_names, model_predictions\n\n\ndef load_predictions(prediction_path, top_k, verbose=True):\n    """"""\n    Load model predictions (top_k per example) from disk.\n\n    :param prediction_path: path to the decoding output\n\n        We assume the file is of the format:\n\n            1. The i-th line of the file contains predictions for example i in the dataset\'\n            2. Each line contains top-k predictions separated by ""|||"".\n\n    :param top_k: Maximum number of predictions to read per example.\n    :return: List of top k predictions.\n    """"""\n\n    if os.path.exists(prediction_path):\n        with open(prediction_path) as f:\n            prediction_list = []\n            for line in f:\n                predictions = line.split(\'|||\')\n                prediction_list.append(predictions[:top_k])\n    else:\n        if verbose:\n            print(\'Warning: file not found: {}\'.format(prediction_path))\n        return None\n    if verbose:\n        print(\'{} predictions loaded from {}\'.format(\n            len(prediction_list), prediction_path))\n    return prediction_list\n\n\ndef load_cached_correct_translations(data_dir, treat_empty_as_correct=False, verbose=False):\n    """"""\n    Load cached correct translations from disk.\n\n    :return: nl -> template translation map, nl -> command translation map\n    """"""\n    command_translations = collections.defaultdict(set)\n    template_translations = collections.defaultdict(set)\n    eval_files = []\n    for file_name in os.listdir(data_dir):\n        if \'evaluations\' in file_name and not file_name.endswith(\'base\'):\n            eval_files.append(file_name)\n    for file_name in sorted(eval_files):\n        manual_judgement_path = os.path.join(data_dir, file_name)\n        with open(manual_judgement_path) as f:\n            if verbose:\n                print(\'reading cached evaluations from {}\'.format(\n                    manual_judgement_path))\n            reader = csv.DictReader(f)\n            current_nl_key = \'\'\n            for row in reader:\n                if row[\'description\']:\n                    current_nl_key = get_example_nl_key(row[\'description\'])\n                pred_cmd = row[\'prediction\']\n                if \'template\' in row:\n                    pred_temp = row[\'template\']\n                else:\n                    pred_temp = data_tools.cmd2template(pred_cmd, loose_constraints=True)\n                structure_eval = row[\'correct template\']\n                if treat_empty_as_correct:\n                    structure_eval = normalize_judgement(structure_eval)\n                command_eval = row[\'correct command\']\n                if treat_empty_as_correct:\n                    command_eval = normalize_judgement(command_eval)\n                if structure_eval == \'y\':\n                    template_translations[current_nl_key].add(pred_temp)\n                if command_eval == \'y\':\n                    command_translations[current_nl_key].add(pred_cmd)\n    print(\'{} template translations loaded\'.format(len(template_translations)))\n    print(\'{} command translations loaded\'.format(len(command_translations)))\n\n    return template_translations, command_translations\n\n\ndef load_cached_evaluations(model_dir, verbose=True):\n    """"""\n    Load cached evaluation results from disk.\n\n    :param model_dir: Directory where the evaluation result file is stored.\n    :param decode_sig: The decoding signature of the model being evaluated.\n    :return: dictionaries storing the evaluation results.\n    """"""\n    structure_eval_results = {}\n    command_eval_results = {}\n    eval_files = []\n    for file_name in os.listdir(model_dir):\n        if \'evaluations\' in file_name and not file_name.endswith(\'base\'):\n            eval_files.append(file_name)\n    for file_name in sorted(eval_files):\n        manual_judgement_path = os.path.join(model_dir, file_name)\n        ser, cer = load_cached_evaluations_from_file(manual_judgement_path, verbose=verbose)\n        for key in ser:\n            structure_eval_results[key] = ser[key]\n        for key in cer:\n            command_eval_results[key] = cer[key]\n    if verbose:\n        print(\'{} structure evaluation results loaded\'.format(len(structure_eval_results)))\n        print(\'{} command evaluation results loaded\'.format(len(command_eval_results)))\n    return structure_eval_results, command_eval_results\n\n\ndef load_cached_evaluations_from_file(input_file, treat_empty_as_correct=False, verbose=True):\n    structure_eval_results = {}\n    command_eval_results = {}\n    with open(input_file, encoding=\'utf-8\') as f:\n        if verbose:\n            print(\'reading cached evaluations from {}\'.format(input_file))\n        reader = csv.DictReader(f)\n        current_nl_key = \'\'\n        for row in reader:\n            if row[\'description\']:\n                current_nl_key = get_example_nl_key(row[\'description\'])\n            pred_cmd = row[\'prediction\']\n            if \'template\' in row:\n                pred_temp = row[\'template\']\n            else:\n                pred_temp = data_tools.cmd2template(pred_cmd, loose_constraints=True)\n            command_eval = row[\'correct command\']\n            if treat_empty_as_correct:\n                command_eval = normalize_judgement(command_eval)\n            command_example_key = \'{}<NL_PREDICTION>{}\'.format(current_nl_key, pred_cmd)\n            if command_eval:\n                command_eval_results[command_example_key] = command_eval\n            structure_eval = row[\'correct template\']\n            if treat_empty_as_correct:\n                structure_eval = normalize_judgement(structure_eval)\n            structure_example_key = \'{}<NL_PREDICTION>{}\'.format(current_nl_key, pred_temp)\n            if structure_eval:\n                structure_eval_results[structure_example_key] = structure_eval\n    return structure_eval_results, command_eval_results\n\n\ndef get_example_nl_key(nl):\n    """"""\n    Get the natural language description in an example with nuances removed.\n    """"""\n    tokens, _ = tokenizer.basic_tokenizer(nl)\n    return \' \'.join(tokens)\n\n\ndef get_example_cm_key(cm):\n    """"""\n    TODO: implement command normalization\n        1. flag order normalization\n        2. flag format normalization (long flag vs. short flag)\n        3. remove flags whose effect does not matter\n    """"""\n    return cm\n\n\ndef normalize_judgement(x):\n    if not x or x.lower() == \'y\':\n        return \'y\'\n    else:\n        return \'n\'\n'"
eval/token_based.py,0,"b'""""""\nCompute keyword overlap between two commands.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport nltk\nimport numpy as np\n\nfrom bashlint import data_tools, nast\n\n\nsmoothing = nltk.translate.bleu_score.SmoothingFunction()\n\n\ndef get_content_tokens(ast):\n    content_tokens = collections.defaultdict(int)\n    for compound_token in data_tools.ast2tokens(ast, loose_constraints=True,\n            arg_type_only=True, with_prefix=True, with_flag_argtype=True):\n        kind_token = compound_token.split(nast.KIND_PREFIX)\n        if len(kind_token) == 2:\n            kind, token = kind_token\n        else:\n            kind = \'\'\n            token = kind_token[0]\n        if kind.lower() != \'argument\':\n            content_tokens[token] += 1\n    return content_tokens\n\n\ndef CMS(ast1, ast2):\n    token_dict1 = get_content_tokens(ast1)\n    token_dict2 = get_content_tokens(ast2)\n    num_overlap = 0.0\n    for t in token_dict2:\n        if t in token_dict1:\n            num_overlap += token_dict1[t] * token_dict2[t]\n    norm1 = 0.0\n    for t in token_dict1:\n        norm1 += token_dict1[t] * token_dict1[t]\n    norm2 = 0.0\n    for t in token_dict2:\n        norm2 += token_dict2[t] * token_dict2[t]\n    if norm1 == 0 or norm2 == 0:\n        return 0\n    else:\n        return num_overlap / np.sqrt(norm1) / np.sqrt(norm2)\n\n\ndef command_match_score(gts, ast):\n    max_cms = 0.0\n    for gt in gts:\n        if CMS(ast, gt) > max_cms:\n            max_cms = CMS(ast, gt)\n    return max_cms\n\n\ndef sentence_bleu_score(gt_asts, pred_ast):\n    gt_tokens = [data_tools.bash_tokenizer(ast, ignore_flag_order=True) for ast in gt_asts]\n    pred_tokens = data_tools.bash_tokenizer(pred_ast, loose_constraints=True, ignore_flag_order=True)\n    bleu = nltk.translate.bleu_score.sentence_bleu(gt_tokens, pred_tokens,\n                                                   smoothing_function=smoothing.method1, auto_reweigh=True) \n    return bleu \n\n\ndef corpus_bleu_score(gt_asts_list, pred_ast_list):\n    gt_tokens_list = [[data_tools.bash_tokenizer(ast, ignore_flag_order=True) for ast in gt_asts] for gt_asts in gt_asts_list]\n    pred_tokens_list = [data_tools.bash_tokenizer(pred_ast, loose_constraints=True, ignore_flag_order=True) for pred_ast in pred_ast_list]\n    # print(gt_tokens, pred_tokens)\n    bleu = nltk.translate.bleu_score.corpus_bleu(gt_tokens_list, pred_tokens_list, \n                                                 smoothing_function=smoothing.method1, auto_reweigh=True)\n    return bleu\n'"
eval/tree_dist.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom bashlint import data_tools, nast\nfrom eval import zss\n\n\ndef local_dist(s1, s2, skip_argument=False):\n    score_list = {\n        ""FLAG_-ls:::"":0,\n        "":::FLAG_-ls"":0,\n        ""FLAG_-print:::"":0,\n        "":::FLAG_-print"":0,\n        ""FLAG_-print0:::"":0,\n        "":::FLAG_-print0"":0,\n        ""FLAG_-name:::FLAG_-regex"":0,\n        ""FLAG_-regex:::FLAG_-name"":0\n    }\n\n    if s1 == s2:\n        return 0\n    if s1.startswith(""ARGUMENT_"") and s2.startswith(""ARGUMENT_"") \\\n            and skip_argument:\n        return 0\n    try:\n        pair_key = "":::"".join((s1, s2))\n    except UnicodeDecodeError:\n        pair_key = "":::"".join((s1.decode(\'utf-8\'), s2.decode(\'utf-8\')))\n\n    if pair_key in score_list:\n        return score_list[pair_key]\n    else:\n        return 1\n\ndef str_local_dist(s1, s2):\n    return local_dist(s1, s2)\n\ndef temp_local_dist(s1, s2):\n    return local_dist(s1, s2, skip_argument=True)\n\ndef str_dist(ast1, ast2):\n    return zss.simple_distance(ast1, ast2, nast.Node.get_children,\n                               nast.Node.get_label, str_local_dist)\n\ndef temp_dist(ast1, ast2):\n    return zss.simple_distance(ast1, ast2, nast.Node.get_children,\n                               nast.Node.get_label, temp_local_dist)\n\n\ndef min_dist(asts, ast2, rewrite=False, ignore_arg_value=False):\n    """"""\n    Compute the minimum tree edit distance of the prediction to the set of\n        ground truth ASTs.\n    :param asts: set of gold ASTs.\n    :param ast2: predicted AST.\n    :param rewrite: set to true if rewrite ground truths with templates.\n    :param ignore_arg_value: set to true if ignore literal values in the ASTs.\n    """"""\n    # tolerate ungrammatical predictions\n    if not ast2:\n        ast2 = data_tools.bash_parser(""find"")\n\n    if rewrite:\n        raise NotImplementedError\n    else:\n        ast_rewrites = asts\n\n    min_dist = 1e8\n    for ast1 in ast_rewrites:\n        if ignore_arg_value:\n            dist = temp_dist(ast1, ast2)\n        else:\n            dist = str_dist(ast1, ast2)\n        if dist < min_dist:\n            min_dist = dist\n\n    return min_dist\n\ndef one_match(asts, ast2, rewrite=False, ignore_arg_value=False):\n    if rewrite:\n        raise NotImplementedError\n    else:\n        ast_rewrites = asts\n    cmd2 = data_tools.ast2template(ast2, loose_constraints=True,\n                                   arg_type_only=ignore_arg_value)\n    for ast1 in ast_rewrites:\n        cmd1 = data_tools.ast2template(ast1, loose_constraints=True,\n                                       arg_type_only=ignore_arg_value)\n        if cmd1 == cmd2:\n            return True\n    return False\n\ndef template_match(ast1, ast2):\n    temp1 = data_tools.ast2template(ast1, loose_constraints=True)\n    temp2 = data_tools.ast2template(ast2, loose_constraints=True)\n    return temp1 == temp2\n\ndef string_match(ast1, ast2):\n    str1 = data_tools.ast2template(ast1, loose_constraints=True, arg_type_only=False)\n    str2 = data_tools.ast2template(ast2, loose_constraints=True, arg_type_only=False)\n    return str1 == str2\n\n\nif __name__ == \'__main__\':\n    asts = [data_tools.bash_parser(\'find . -type f -print0 | xargs -0 -I {} grep -i -l __SP__UNK {}\')]\n    ast = data_tools.bash_parser(\'find . -type f -print0 | xargs -0 -I {} grep -i -l \\\'.*\\\' {}\')\n    print(one_match(asts, ast, ignore_arg_value=True))\n    print(one_match(asts, ast, ignore_arg_value=False))\n'"
nlp_tools/__init__.py,0,b''
nlp_tools/constants.py,0,"b'""""""\nSpecial word and character lists used in the NLP tools for processing natural\nlanguage commands.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport string\n\n_SPACE = ""__SP__SPACE""\n_LONG_TOKEN_IND = ""__SP__LONG_TOKEN""\n\n_NUMBER = ""_NUMBER""\n_NUMBER_ALPHA = ""_NUMBER_ALPHA""\n_SIZE = ""_SIZE""\n_TIMESPAN = ""_TIMESPAN""\n_DATETIME = ""_DATETIME""\n_PATH = ""_PATH""\n_FILE = ""_FILE""\n_DIRECTORY = ""_DIRECTORY""\n_PERMISSION = ""_PERMISSION""\n_REGEX = ""_REGEX""\n\ntype_conversion = {\n    _FILE: \'File\',\n    _DIRECTORY: \'Directory\',\n    _PATH: \'Path\',\n    _PERMISSION: \'Permission\',\n    _DATETIME: \'DateTime\',\n    _TIMESPAN: \'Timespan\',\n    _REGEX: \'Regex\',\n    _SIZE: \'Size\',\n    _NUMBER: \'Number\',\n}\n\n_QUOTED_RE = r\'(\\\'[^\\\']*\\\')|(""[^""]*"")\'\n_SPECIAL_SYMBOL_RE = r\'[^ ]*[_\\.\\*|\\\\|\\/|\\~|\\@|\\%|\\#|\\?|\\+|\\$|\\{|\\}|\\<|\\>]+[^ ]*\'\n_FILE_EXTENSION_RE1 = r\'(aiff|cda|mid|mp3|mp4|mpa|ogg|wav|wma|wpl|7z|arj|deb|pkg|\' \\\n        r\'rar|rpm|gz|bin|dmg|iso|vcd|vcr|dvd|csv|dat|db|log|mdb|sav|sql|\' \\\n        r\'xml|apk|bat|bin|cgi|pl|com|html|css|js|exe|gadget|jar|py|wsf|fnt|\' \\\n        r\'fon|otf|ttf|ai|bmp|gif|ico|jpg|jpeg|png|psd|svg|tif|java|php|txt|c|\' \\\n        r\'cpp|cc|o|htm|asp|cer|jsp|json|rss|xhtml|odp|ppt|pptx|class|h|sh|swift|\' \\\n        r\'vb|ods|xlr|xlsx|xls|ini|msi|sys|tmp|drv|dmp|dll|bak|3gp|flv|h264|avi|\' \\\n        r\'mkv|mov|m4v|rm|mpg|mpeg|swf|wmv|doc|docx|tex|pdf|rtf|wps|wpd|xz|cvs)\'\n_FILE_EXTENSION_RE = r\'((\' + _FILE_EXTENSION_RE1 + \'|\' + _FILE_EXTENSION_RE1.upper() \\\n        + \')s?)\'\n\n_PATH_RE = r\'([^ ]*\\/)+[^ ]*\'\n\n_DIGIT_RE = r\'\\d*\\.?\\d+\'\n\n_NUMERICAL_PERMISSION_RE = r\'[0-7]{3,4}\'\n_PATTERN_PERMISSION_RE = r\'(([u|g|o]+[\\+|-|=][r|w|x|s|u|t]+\\,)*[u|g|o]+[\\+|-|=][r|w|x|s|u|t]+)\'\n\n# Credit: time expressions adapted from\n# https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py\n_MONTH_RE = r\'(january|jan|jan\\.|february|feb|feb\\.|march|mar|mar\\.|april|apr|apr\\.|\' \\\n            r\'may|june|jun|jun\\.|july|jul|jul\\.|august|aug|aug\\.|september|sep|sep\\.|\' \\\n            r\'october|oct|oct\\.|november|nov|nov\\.|december|dec|dec\\.)\'\n_REL_DAY_RE = r\'(today|yesterday|tomorrow|the day before yesterday|\' \\\n              r\'the day after tomorrow)\'\n\ndigitize_month = {\n    ""jan"": ""01"",\n    ""feb"": ""02"",\n    ""mar"": ""03"",\n    ""apr"": ""04"",\n    ""may"": ""05"",\n    ""jun"": ""06"",\n    ""jul"": ""07"",\n    ""aug"": ""08"",\n    ""sep"": ""09"",\n    ""oct"": ""10"",\n    ""nov"": ""11"",\n    ""dec"": ""12""\n}\n\n_SIZE_UNIT = r\'(kilobyte|kilobyt|kilo byte|megabyte|megabyt|mega byte|gigabyte|\' \\\n             r\'gigabyt|terabyte|terabyt|block|byte|byt|kb|mb|gb|tb|c|b|k|m|g|t)\'\n\n_DURATION_UNIT = r\'(second|sec|minute|minut|min|hour|hr|day|week|month|mon|year\' \\\n                 r\'|yr|s|m|h|d|w|y)\'\n\n# Regular expressions used to tokenize an English sentence.\n_WORD_SPLIT = ""^\\s+|\\s*,\\s*|\\s+$|^[\\(|\\[|\\{|\\<]|[\\)|\\]|\\}|\\>]$""\n_WORD_SPLIT_RESPECT_SINGLE_QUOTES = ""((?:\\s+|^)\'[^\']*\'(?:\\s+|$))""\n_WORD_SPLIT_RESPECT_QUOTES = \\\n    \'({}+|{}+|(?:[^\\s,""]|""(?:\\\\.|[^""])*"")+)\'.format(\n        _WORD_SPLIT_RESPECT_SINGLE_QUOTES,\n        _PATTERN_PERMISSION_RE)\n\nENGLISH_STOPWORDS = {\n    # ""a"",\n    # ""an"",\n    ""the"",\n    # ""be"",\n    # ""is"",\n    # ""been"",\n    # ""being"",\n    # ""was"",\n    # ""were"",\n    # ""are"",\n    ""has"",\n    ""have"",\n    ""had"",\n    ""here"",\n    ""there"",\n    ""do"",\n    ""how"",\n    ""i"",\n    ""i\'d"",\n    ""i\'ll"",\n    ""i\'m"",\n    ""i\'ve"",\n    ""me"",\n    ""my"",\n    ""myself"",\n    ""can"",\n    ""could"",\n    ""did"",\n    ""do"",\n    ""does"",\n    ""doing"",\n    ""must"",\n    ""should"",\n    ""would"",\n    ""you"",\n    ""you\'d"",\n    ""you\'ll"",\n    ""you\'re"",\n    ""you\'ve"",\n    ""your"",\n    ""yours"",\n    ""yourself"",\n    ""yourselves"",\n     ""he"",\n    ""he\'d"",\n    ""he\'ll"",\n    ""he\'s"",\n    ""her"",\n    ""here"",\n    ""here\'s"",\n    ""hers"",\n    ""herself"",\n    ""him"",\n    ""himself"",\n    ""his"",\n    ""she"",\n    ""she\'d"",\n    ""she\'ll"",\n    ""she\'s"",\n    # ""it"",\n    ""it\'s"",\n    # ""its"",\n    ""itself"",\n    ""we"",\n    ""we\'d"",\n    ""we\'ll"",\n    ""we\'re"",\n    ""we\'ve"",\n    ""their"",\n    ""theirs"",\n    # ""them"",\n    ""themselves"",\n    ""then"",\n    ""there"",\n    ""there\'s"",\n    ""they"",\n    ""they\'d"",\n    ""they\'ll"",\n    ""they\'re"",\n    ""they\'ve"",\n    ""let"",\n    ""let\'s"",\n    ""this"",\n    # ""that"",\n    # ""these"",\n    # ""those"",\n    # ""what"",\n    ""what\'s"",\n    # ""which"",\n    # ""whose"",\n    ""how"",\n    ""how\'s""\n}\n\nword2num = {\'zero\':  0, \'eleven\':     11,\n            \'two\':   2, \'twelve\':     12,\n            \'three\': 3, \'thirteen\':   13,\n            \'four\':  4, \'fourteen\':   14,\n            \'five\':  5, \'fifteen\':    15,\n            \'six\':   6, \'sixteen\':    16,\n            \'seven\': 7, \'seventeen\':  17,\n            \'eight\': 8, \'eighteen\':   18,\n            \'nine\':  9, \'nineteen\':   19,\n            \'ten\':     10,\n            \'twenty\':  20,\n            \'thirty\':  30,\n            \'forty\':   40,\n            \'fifty\':   50,\n            \'sixty\':   60,\n            \'seventy\': 70,\n            \'eighty\':  80,\n            \'ninety\':  90,\n            \'twenty-four\': 24\n}\n\nsize_units = {\n    \'b\',\n    \'k\',\n    \'m\',\n    \'g\',\n    \'t\',\n    \'kb\',\n    \'mb\',\n    \'gb\',\n    \'tb\',\n    \'byt\',\n    \'byte\',\n    \'block\',\n    \'kilobyt\',\n    \'kilobyte\',\n    \'megabyt\',\n    \'megabyte\',\n    \'gigabyt\',\n    \'gigabyte\',\n    \'terabyt\',\n    \'terabyte\'\n}\n\nduration_units = {\n    \'s\',\n    \'m\',\n    \'h\',\n    \'d\',\n    \'w\',\n    \'m\',\n    \'y\',\n    \'sec\',\n    \'second\',\n    \'min\',\n    \'minut\',\n    \'minute\',\n    \'hour\',\n    \'day\',\n    \'week\',\n    \'month\',\n    \'year\'\n}\n\npunctuation = string.punctuation\n\ndef include_space(r):\n    """"""\n    A regular expression has to have a whitespace or other separator\n    at both ends.\n    """"""\n    return r\'(^|\\s)({})(\\s|$|,|\\.)\'.format(r)\n\ndef include_quotations(r):\n    return \'(\\\'({})\\\'|""({})"")\'.format(r, r)\n\ndef quotation_safe(r, rq=None):\n    """"""\n    Match a regular expression with or without quotation marks.\n    """"""\n    rq = r if rq is None else rq\n    return \'(({})|\\\'({})\\\'|""({})"")\'.format(r, rq, rq)\n\ndef polarity_safe(r):\n    """"""\n    Match a regular expression with or without ""+/-"" signs.\n    """"""\n    return r\'[+|-]?({})\'.format(r)\n\ndef add_quotations(s):\n    return \'""\' + s + \'""\'\n\ndef is_quotation(s):\n    return s in [\'""\', \'\\\'\']\n\ndef with_quotation(s):\n    if not s:\n        return False\n    return s[0] in [\'""\', \'\\\'\'] and s[-1] in [\'""\', \'\\\'\']\n\ndef starts_with_quotation(s):\n    if not s:\n        return False\n    return s[0] in [\'""\', \'\\\'\']\n\ndef ends_with_quotation(s):\n    if not s:\n        return False\n    return s[-1] in [\'""\', \'\\\'\']\n\ndef remove_quotation(s):\n    if s and s[0] in [\'""\', \'\\\'\']:\n        s = s[1:]\n    if s and s[-1] in [\'""\', \'\\\'\']:\n        s = s[:-1]\n    return s\n\ndef with_angle_brackets(s):\n    return s.startswith(\'<\') and s.endswith(\'>\')\n\ndef is_english_word(word):\n    """"""Check if a token is a normal English word.""""""\n    if word in [\'i.e\', \'i.e.\', \'e.g\', \'e.g.\',\n                \'s.a\', \'s.a.\', \'s.t\', \'s.t.\']:\n        return True\n    if word in [\'\\\'s\', \'\\\'t\']:\n        return True\n    return bool(re.match(\'^[a-zA-Z]{1}[a-z]*(-[a-z]+)*$\', word, re.IGNORECASE))\n\ndef is_stopword(w):\n    return w in ENGLISH_STOPWORDS\n'"
nlp_tools/format_args.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""\nFunctions for reformatting entities mentioned in the natural language following\nbash syntax.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime, re\n\nfrom bashlint import bash\nfrom nlp_tools import constants\n\n\n# --- Slot filling value extractors --- #\n\ndef get_fill_in_value(cm_slot, nl_filler):\n    """"""\n    Compute a command argument given the argument slot specification and the\n    entity being aligned to it (mostly dealing with file name formatting,\n    adding signs to quantities, etc.).\n    :param cm_slot: (slot_value, slot_type)\n    :param nl_filler: (filler_value, filler_type)\n    """"""\n    slot_value, slot_type = cm_slot\n    surface, filler_type = nl_filler\n    filler_value = extract_value(filler_type, slot_type, surface)\n\n    # In most cases the filler can be directly copied into the slot\n    slot_filler_value = filler_value\n\n    if slot_type in bash.quantity_argument_types:\n        if slot_value.startswith(\'+\'):\n            slot_filler_value = filler_value if filler_value.startswith(\'+\') \\\n                else \'+{}\'.format(filler_value)\n        elif slot_value.startswith(\'-\'):\n            slot_filler_value = filler_value if filler_value.startswith(\'-\') \\\n                else \'-{}\'.format(filler_value)\n\n    return slot_filler_value\n\ndef extract_value(filler_type, slot_type, surface):\n    """"""\n    Extract slot filling values from the natural language.\n    """"""\n    if filler_type in constants.type_conversion:\n        filler_type = constants.type_conversion[filler_type]\n\n    # remove quotations if there is any\n    if constants.with_quotation(surface):\n        value = constants.remove_quotation(surface)\n    else:\n        value = surface\n\n    if filler_type in [\'Directory\']:\n        value = value\n    elif filler_type == \'Number\':\n        value = extract_number(value)\n    elif filler_type == \'File\':\n        value = extract_filename(value, slot_type)\n    elif filler_type == \'Permission\':\n        value = extract_permission(value)\n    elif filler_type == \'DateTime\':\n        value = extract_datetime(value)\n    elif filler_type == \'Timespan\':\n        value = extract_timespan(value)\n    elif filler_type == \'Size\':\n        value = extract_size(value)\n    elif filler_type == \'Regex\':\n        value = value\n    elif filler_type in [\'Username\', \'Groupname\']:\n        value = value\n\n    # add quotations for pattern slots\n    if filler_type in bash.pattern_argument_types and \\\n            not constants.with_quotation(value):\n        value = constants.add_quotations(value)\n\n    return value\n\ndef extract_number(value):\n    digit_re = re.compile(constants._DIGIT_RE)\n    match = re.search(digit_re, value)\n    if match:\n        return match.group(0)\n    else:\n        return \'unrecognized_numerical_expression\'\n        # raise AttributeError(\'Cannot find number representation in\n        # pattern {}\'.format(value))\n\ndef extract_filename(value, slot_type=\'File\'):\n    """"""Extract file names""""""\n    quoted_span_re = re.compile(constants._QUOTED_RE)\n    special_symbol_re = re.compile(constants._SPECIAL_SYMBOL_RE)\n    file_extension_re = re.compile(\'{}|{}\'.format(constants._FILE_EXTENSION_RE,\n        constants._FILE_EXTENSION_RE.upper()))\n    path_re = re.compile(constants._PATH_RE)\n\n    # path\n    match = re.search(path_re, value)\n    if match:\n        return match.group(0)\n    # file extension\n    # if re.search(re.compile(r\'[^ ]*\\.[^ ]+\'), value):\n    #     # the pattern being matched represents a regular file\n    #     match = re.match(file_extension_re, strip(value))\n    #     if match:\n    #         return \'""*.\' + match.group(0) + \'""\'\n    match = re.search(file_extension_re, value)\n    if match:\n        if slot_type in [\'Directory\', \'Path\']:\n            return value\n        else:\n            if (len(match.group(0)) + 0.0) / len(strip(value)) > 0.5:\n                # avoid cases in which a file name happen to contain a\n                # substring which is the same as a file extension\n                return \'""*.\' + match.group(0) + \'""\'\n            else:\n                return value\n    # quotes\n    if re.match(quoted_span_re, value):\n        return value\n    # special symbol\n    if re.match(special_symbol_re, value):\n        return value\n    return \'unrecognized_file_name\'\n\ndef extract_permission(value):\n    """"""Extract permission patterns""""""\n    numerical_permission_re = re.compile(constants._NUMERICAL_PERMISSION_RE)\n    pattern_permission_re = re.compile(constants._PATTERN_PERMISSION_RE)\n    if re.match(numerical_permission_re, value) or \\\n            re.match(pattern_permission_re, value):\n        return value\n    else:\n        # TODO: write rules to synthesize permission pattern\n        return value\n\ndef extract_datetime(value):\n    """"""Extract date/time patterns""""""\n    standard_time = re.compile(constants.quotation_safe(\n        r\'\\d+:\\d+:\\d+\\.?\\d*\'))\n    standard_datetime_dash_re = re.compile(constants.quotation_safe(\n        r\'\\d{1,4}[-]\\d{1,4}[-]\\d{1,4}\'))\n    standard_datetime_slash_re = re.compile(constants.quotation_safe(\n        r\'\\d{1,4}[\\/]\\d{1,4}[\\/]\\d{1,4}\'))\n    textual_datetime_re = re.compile(constants.quotation_safe(\n        constants._MONTH_RE + r\'(\\s\\d{0,2})?([,|\\s]\\d{2,4})?\'))\n    rel_day_re = re.compile(constants.quotation_safe(constants._REL_DAY_RE))\n    month_re = re.compile(constants._MONTH_RE)\n    digit_re = re.compile(constants._DIGIT_RE)\n    if re.match(standard_time, value) or \\\n            re.match(standard_datetime_dash_re, value):\n        return value\n    elif re.match(standard_datetime_slash_re, value):\n        return re.sub(re.compile(r\'\\/\'), \'-\', value)\n    elif re.match(textual_datetime_re, value):\n        # TODO: refine rules for date formatting\n        month = re.search(month_re, value).group(0)\n        month = constants.digitize_month[month[:3]]\n        date_year = re.findall(digit_re, value)\n        if date_year:\n            if len(date_year) == 2:\n                date = date_year[0]\n                year = date_year[1]\n                formatted_datetime = \'{}-{}-{:02}\'.format(year, month, int(date))\n            else:\n                if \',\' in value:\n                    year = date_year[0]\n                    formatted_datetime = \'{}-{}\'.format(year, month)\n                else:\n                    date = date_year[0]\n                    formatted_datetime = \'{}-{}-{:02}\'.format(\n                        datetime.datetime.now().year, month, int(date))\n        else:\n            current_year = datetime.date.today().year\n            formatted_datetime = \'{}-{}\'.format(current_year, month)\n        return formatted_datetime\n    elif re.match(rel_day_re, value):\n        if value == \'today\':\n            date = datetime.date.today()\n        elif value == \'yesterday\':\n            date = datetime.date.today() - datetime.timedelta(1)\n        elif value == \'the day before yesterday\':\n            date = datetime.date.today() - datetime.timedelta(2)\n        elif value == \'tomorrow\':\n            date = datetime.date.today() + datetime.timedelta(1)\n        elif value == \'the day after tomorrow\':\n            date = datetime.date.today() + datetime.timedelta(2)\n        else:\n            raise AttributeError(""Cannot parse relative date expression: {}""\n                                 .format(value))\n        return date.strftime(\'%y-%m-%d\')\n    else:\n        raise AttributeError(""Cannot parse date/time: {}"".format(value))\n\ndef extract_timespan(value):\n    """"""Extract timespans""""""\n    digit_re = re.compile(constants._DIGIT_RE)\n    duration_unit_re = re.compile(constants._DURATION_UNIT)\n    m = re.search(digit_re, value)\n    number = m.group(0) if m else \'1\'\n    duration_unit = sorted(re.findall(duration_unit_re, value),\n                           key=lambda x:len(x), reverse=True)[0]\n    # TODO: refine rules for time span formatting and calculation\n    if value.startswith(\'+\'):\n        sign = \'+\'\n    elif value.startswith(\'-\'):\n        sign = \'-\'\n    else:\n        sign = \'\'\n    if duration_unit.startswith(\'y\'):\n        return sign + \'{}\'.format(int(float(number)*365))\n    if duration_unit.startswith(\'mon\'):\n        return sign + \'{}\'.format(int(float(number)*30))\n    if duration_unit.startswith(\'w\'):\n        return sign + \'{}\'.format(int(float(number)*7))\n    if duration_unit.startswith(\'d\'):\n        if \'.\' in number:\n            number = int(float(number) * 24)\n            unit = \'h\'\n        else:\n            unit = \'\'\n        return sign + \'{}{}\'.format(number, unit)\n    if duration_unit.startswith(\'h\'):\n        if \'.\' in number:\n            number = int(float(number) * 60)\n            unit = \'m\'\n        else:\n            unit = \'h\'\n        return sign + \'{}{}\'.format(number, unit)\n    if duration_unit.startswith(\'m\'):\n        if \'.\' in number:\n            number = int(float(number) * 60)\n            unit = \'s\'\n        else:\n            unit = \'m\'\n        return sign + \'{}{}\'.format(number, unit)\n    if duration_unit.startswith(\'s\'):\n        return sign + \'{}s\'.format(float(number))\n\n    raise AttributeError(""Cannot parse timespan: {}"".format(value))\n\ndef extract_size(value):\n    """"""Extract sizes""""""\n    digit_re = re.compile(constants._DIGIT_RE)\n    size_unit_re = re.compile(constants._SIZE_UNIT)\n    m = re.search(digit_re, value)\n    number = m.group(0) if m else \'1\'\n    size_unit = sorted(re.findall(size_unit_re, value),\n                       key=lambda x:len(x), reverse=True)[0]\n    if value.startswith(\'+\'):\n        sign = \'+\'\n    elif value.startswith(\'-\'):\n        sign = \'-\'\n    else:\n        sign = \'\'\n    if size_unit.startswith(\'b\'):\n        number = int(float(number))\n        unit = \'c\'\n        return sign + \'{}{}\'.format(number, unit)\n    elif size_unit.startswith(\'k\'):\n        if \'.\' in number:\n            number = int(float(number) * 1000)\n            unit = \'c\'\n        else:\n            unit = \'k\'\n        return sign + \'{}{}\'.format(number, unit)\n    elif size_unit.startswith(\'m\'):\n        if \'.\' in number:\n            number = int(float(number) * 1000)\n            unit = \'k\'\n        else:\n            unit = \'M\'\n        return sign + \'{}{}\'.format(number, unit)\n    elif size_unit.startswith(\'g\'):\n        if \'.\' in number:\n            number = int(float(number) * 1000)\n            unit = \'M\'\n        else:\n            unit = \'G\'\n        return sign + \'{}{}\'.format(number, unit)\n    elif size_unit.startswith(\'t\'):\n        number = int(float(number) * 1000)\n        unit = \'G\'\n        return sign + \'{}{}\'.format(number, unit)\n    else:\n        raise AttributeError(\'Unrecognized size unit: {}\'.format(size_unit))\n\n# --- Utils --- #\n\ndef strip(pattern):\n    while len(pattern) > 1 and \\\n            pattern[0] in [\'""\', \'\\\'\', \'*\', \'\\\\\', \'/\', \'.\', \'-\', \'+\', \'{\', \'}\']:\n        pattern = pattern[1:]\n    while len(pattern) > 1 and \\\n            pattern[-1] in [\'""\', \'\\\'\', \'\\\\\', \'/\', \'$\', \'*\', \'.\', \'-\', \'+\', \'{\', \'}\']:\n        pattern = pattern[:-1]\n    special_end_re = re.compile(r\'(\\\\n|\\{\\})$\')\n    while len(pattern) > 2 and re.search(special_end_re, pattern):\n        pattern = pattern[:-2]\n    while len(pattern) > 1 and \\\n            pattern[0] in [\'""\', \'\\\'\', \'*\', \'\\\\\', \'/\', \'.\', \'-\', \'+\', \'~\']:\n        pattern = pattern[1:]\n    while len(pattern) > 1 and \\\n            pattern[-1] in [\'""\', \'\\\'\', \'\\\\\', \'/\', \'$\', \'*\', \'.\', \'-\', \'+\', \'~\']:\n        pattern = pattern[:-1]\n    return pattern\n\ndef strip_sign(pattern):\n    if pattern[0] in [\'-\', \'+\']:\n        pattern = pattern[1:]\n    return pattern\n\ndef is_parameter(value):\n    return constants.remove_quotation(value).startswith(\'$\')\n\ndef is_min_flag(token):\n    if len(token) == 5 and token.endswith(\'min\') and token.startswith(\'-\'):\n        return True\n    return False'"
nlp_tools/ner.py,0,"b'""""""\nA named entity recognizer in the file system operation domain.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\n\nfrom . import constants\n\ndef decorate_boundaries(r):\n    """"""\n    Match named entity boundary characters s.a. quotations and whitespaces.\n    """"""\n    return constants.include_space(constants.quotation_safe(r))\n\ndef annotate(tokens):\n    """"""\n    Identify named entities in a (tokenized) sentence and replace them with the\n    corresponding categories.\n\n    The NER so far recognizes the following named entity categories:\n    - Pattern-based:\n        - File\n        - Directory\n        - Path\n        - Permission\n        - Username\n        - Groupname\n        - DateTime\n        - Other Patterns\n    - Quantity-based:\n        - Number\n        - Size\n        - Timespan\n\n    :return: 1. a list of tokens where the named entities are replaced with\n        category names\n             2. a dictionary that stores a list of named entities matched for\n        each category\n    """"""\n\n    sentence = \' \'.join(tokens)\n    ner_by_token_id = collections.defaultdict()\n    ner_by_char_pos = collections.defaultdict()\n    ner_by_category = collections.defaultdict(list)\n    entities = (ner_by_char_pos, ner_by_category)\n\n    # -- Size\n    _SIZE_RE = re.compile(decorate_boundaries(\n        constants.polarity_safe(r\'({}|a\\s)\\s*\'.format(constants._DIGIT_RE)) +\n        constants._SIZE_UNIT))\n    sentence = annotate_ner(_SIZE_RE, constants._SIZE, sentence, entities)\n\n    # -- Timespan\n    time_num_re = r\'((24\\*|60\\*)?{}|{}(\\*24|\\*60))\'.format(\n        constants._DIGIT_RE, constants._DIGIT_RE)\n    _DURATION_RE = re.compile(decorate_boundaries(constants.polarity_safe(\n        r\'({}|a\\s|this\\s|next(\\s{})?\\s|last(\\s{})?\\s|previous(\\s{})?\\s)\\s*\'.format(\n        time_num_re, time_num_re, time_num_re, time_num_re) + constants._DURATION_UNIT)))\n    sentence = annotate_ner(\n        _DURATION_RE, constants._TIMESPAN, sentence, entities)\n\n    # -- DateTime\n    # Credit: time expressions adapted from\n    # https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/timex.py\n    standard_time = r\'\\d+:\\d+:\\d+\\.?\\d*\'\n    standard_datetime = r\'\\d{1,4}[\\/-]\\d{1,4}[\\/-]\\d{1,4}([,|\\s]\' + standard_time + r\')?\'\n    textual_datetime = constants._MONTH_RE \\\n                       + r\'(\\s\\d{0,2}(st|nd|th)?)?([,|\\s]\\d{2,4})?([,|\\s]\' \\\n                       + standard_time + r\')?\'\n    _DATETIME_RE = re.compile(decorate_boundaries(constants.polarity_safe(\n                    \'(\' + constants._REL_DAY_RE + \'|\' + standard_time + \'|\' +\n                    standard_datetime + \'|\' + textual_datetime + \')\')))\n    sentence = annotate_ner(_DATETIME_RE, constants._DATETIME, sentence, entities)\n    \n    # -- Permission\n    permission_bit = r\'(suid|sgid|sticky|sticki)(\\sbit)?\'\n    permission_bit_set = r\'(set)?(uid|gid|sticky|sticki)(=\\d+)*\'\n    _PERMISSION_RE = re.compile(decorate_boundaries(constants.polarity_safe(\n                    \'(\' + constants._PATTERN_PERMISSION_RE + \'|\' +\n                    permission_bit + \'|\' + permission_bit_set + \')\')))\n    sentence = annotate_ner(\n        _PERMISSION_RE, constants._PERMISSION, sentence, entities)\n\n    # -- Number\n    _NUMBER_RE = re.compile(decorate_boundaries(\n        constants.polarity_safe(constants._DIGIT_RE)))\n    sentence = annotate_ner(_NUMBER_RE, constants._NUMBER, sentence, entities)\n\n    # -- Match all quoted patterns first to prevent partial matching within quotations\n    # -- Directory\n    _DIRECTORY_RE = re.compile(constants.include_quotations(r\'[^""\\\']*\\/\'))\n    sentence = annotate_ner(\n        _DIRECTORY_RE, constants._DIRECTORY, sentence, entities)\n\n    # -- File\n    _FILE_RE = re.compile(constants.include_quotations(r\'([^""\\\']*\\.[^ ""\\\']+)|\' +\n        r\'(([^""\\\']*\\/)+[^""\\\']*)|\' + constants._FILE_EXTENSION_RE))\n    sentence = annotate_ner(_FILE_RE, constants._FILE, sentence, entities)\n    \n    # -- Other patterns\n    _REGEX_QUOTED_RE = re.compile(constants.include_space(constants._QUOTED_RE))\n    sentence = annotate_ner(_REGEX_QUOTED_RE, constants._REGEX, sentence, entities)\n    \n    # -- Match all unquoted patterns\n    # -- Directory\n    _DIRECTORY_RE = re.compile(decorate_boundaries(r\'[^ ""\\\']*\\/\'))\n    sentence = annotate_ner(\n        _DIRECTORY_RE, constants._DIRECTORY, sentence, entities)\n    \n    # -- File\n    _FILE_RE = re.compile(r\'([^ ]*\\.[^ ]+|\' + r\'([^ ]*\\/)+[^ ]*)|(\' +\n        decorate_boundaries(constants._FILE_EXTENSION_RE) + \')\')\n    sentence = annotate_ner(_FILE_RE, constants._FILE, sentence, entities)\n    \n    # -- Other patterns\n    _REGEX_SPECIAL_RE = re.compile(decorate_boundaries(constants._SPECIAL_SYMBOL_RE))\n    sentence = annotate_ner(_REGEX_SPECIAL_RE, constants._REGEX, sentence, entities)\n\n    # prepare list of tokens\n    normalized_words = []\n    i = 0\n    for m in re.finditer(\n        re.compile(constants._WORD_SPLIT_RESPECT_QUOTES), sentence):\n        w = m.group(0)\n        # exclude isolated quotations\n        if w in [\'""\', \'\\\'\']:\n            continue\n        if set(w) == {\'-\'}:\n            if (m.start(0), m.end(0)) in ner_by_char_pos:\n                surface, category = ner_by_char_pos[(m.start(0), m.end(0))]\n                normalized_words.append(category)\n                ner_by_token_id[i] = (surface, category)\n        else:\n            if not constants.is_english_word(w):\n                # catch missed patterns in the final pass\n                normalized_words.append(constants._REGEX)\n                ner_by_token_id[i] = (w, constants._REGEX)\n                ner_by_char_pos[(m.start(0), m.end(0))] = (w, constants._REGEX)\n                ner_by_category[constants._REGEX].append(\n                    (w, m.start(0), m.end(0)))\n            else:\n                normalized_words.append(w)\n        i += 1\n\n    return normalized_words, (ner_by_token_id, ner_by_char_pos, ner_by_category)\n\ndef annotate_ner(pattern, category, sentence, entities):\n    ner_by_char_pos, ner_by_category = entities\n    for m in re.finditer(pattern, sentence):\n        surface = sentence[m.start(0):m.end(0)].strip()\n        if category == constants._DATETIME:\n            # TODO: rule-based system is not good at differentiating between\n            # ""May"" the month and ""may"" the modal verb\n            if surface == \'may\':\n                continue\n        if category in [constants._FILE, constants._REGEX,\n                        constants._DIRECTORY, constants._PATH]:\n            if surface in [\'i.e\', \'i.e.\', \'e.g\', \'e.g.\',\n                           \'s.a\', \'s.a.\', \'s.t\', \'s.t.\']:\n                continue\n        # replace recognized entities with placeholders to ensure that entity\n        # position calculation is always correct\n        rep_start = m.start(0) + 1 if re.match(r\'\\s\', sentence[m.start(0)]) \\\n            else m.start(0)\n        rep_end = m.end(0) - 1 if re.match(r\'\\s\', sentence[m.end(0)-1]) \\\n            else m.end(0)\n        sentence = sentence[:rep_start] + \'-\' * (rep_end - rep_start) + \\\n                   sentence[rep_end:]\n        ner_by_char_pos[(rep_start, rep_end)] = (surface, category)\n        ner_by_category[category].append((surface, rep_start, rep_end))\n    return sentence\n\ndef normalize_number_in_token(token):\n    return re.sub(re.compile(constants._DIGIT_RE), constants._NUMBER, token)\n'"
nlp_tools/ops.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\ndef longest_common_substring(s1, s2):\n    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n    longest, x_longest, y_longest = 0, 0, 0\n    for x in range(1, 1 + len(s1)):\n        for y in range(1, 1 + len(s2)):\n            if s1[x - 1] == s2[y - 1]:\n                m[x][y] = m[x - 1][y - 1] + 1\n                if m[x][y] > longest:\n                    longest = m[x][y]\n                    x_longest = x\n                    y_longest = y\n            else:\n                m[x][y] = 0\n    return (x_longest - longest, x_longest), (y_longest - longest, y_longest)\n'"
nlp_tools/tokenizer.py,0,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n""""""\nNatural language input tokenizer.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re, sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nfrom . import constants, ner\nfrom .spellcheck import spell_check as spc\n\n# from nltk.stem.wordnet import WordNetLemmatizer\n# lmtzr = WordNetLemmatizer()\nfrom nltk.stem import SnowballStemmer\nstemmer = SnowballStemmer(""english"")\n\n\ndef clean_sentence(sentence):\n    """"""\n    Fix punctuation errors and extract main content of a sentence.\n    """"""\n\n    # remove content in parentheses\n    _PAREN_REMOVE = re.compile(\'\\([^)]*\\)\')\n    sentence = re.sub(_PAREN_REMOVE, \'\', sentence)\n\n    try:\n        sentence = sentence.replace(""\xe2\x80\x9c"", \'""\')\\\n            .replace(""\xe2\x80\x9d"", \'""\')\\\n            .replace(\'\xe2\x80\x98\', \'\\\'\')\\\n            .replace(\'\xe2\x80\x99\', \'\\\'\')\n    except UnicodeDecodeError:\n        sentence = sentence.replace(""\xe2\x80\x9c"".decode(\'utf-8\'), \'""\')\\\n            .replace(""\xe2\x80\x9d"".decode(\'utf-8\'), \'""\')\\\n            .replace(\'\xe2\x80\x98\'.decode(\'utf-8\'), \'\\\'\')\\\n            .replace(\'\xe2\x80\x99\'.decode(\'utf-8\'), \'\\\'\')\n    sentence = sentence.replace(\'`\\\'\', \'""\')\\\n        .replace(\'``\', \'""\')\\\n        .replace(""\'\'"", \'""\')\\\n        .replace(\'(\', \' ( \')\\\n        .replace(\')\', \' ) \')\\\n        .replace(\' `\', \' \\\'\') \\\n        .replace(\'` \', \'\\\' \') \\\n        .replace(\'server`s\', \'server\\\'s\')\n\n    sentence = re.sub(\'(,\\s+)|(,$)\', \' \', sentence)\n    sentence = re.sub(\'(;\\s+)|(;$)\', \' \', sentence)\n    sentence = re.sub(\'(:\\s+)|(:$)\', \' \', sentence)\n    sentence = re.sub(\'(\\.\\s+)|(\\.$)\', \' \', sentence)\n\n    # convert abbreviation writings and negations\n    sentence = re.sub(\'\\\'s\', \' \\\'s\', sentence)\n    sentence = re.sub(\'\\\'re\', \' \\\'re\', sentence)\n    sentence = re.sub(\'\\\'ve\', \' \\\'ve\', sentence)\n    sentence = re.sub(\'\\\'d\', \' \\\'d\', sentence)\n    sentence = re.sub(\'\\\'t\', \' \\\'t\', sentence)\n\n    sentence = re.sub(""^[T|t]o "", \'\', sentence)\n    sentence = re.sub(\'\\$\\{HOME\\}\', \'\\$HOME\', sentence)\n    sentence = re.sub(\'""?normal\\/regular""?\', \'regular\', sentence)\n    sentence = re.sub(\'""?regular\\/normal""?\', \'regular\', sentence)\n    sentence = re.sub(\'""?normal/regualar""?\', \'regular\', sentence)\n    sentence = re.sub(\n        \'""?file\\/directory""?\', \'file or directory\', sentence)\n    sentence = re.sub(\n        \'""?files\\/directories""?\', \'files and directories\', sentence)\n    sentence = re.sub(\'""?name\\/path""?\', \'name or path\', sentence)\n    sentence = re.sub(\'""?names\\/paths""?\', \'name or path\', sentence)\n    sentence = re.sub(\' pattern\\\' \', \' pattern \', sentence)\n\n    return sentence\n\n\ndef space_tokenizer(sentence):\n    """"""\n    Split an already-tokenized sentence into tokens.\n    """"""\n    return sentence.split(), None\n\n\ndef basic_tokenizer(sentence, to_lower_case=True, lemmatization=True,\n                    remove_stop_words=True, correct_spell=True,\n                    separate_quotations=False, verbose=False,):\n    """"""\n    Regex-based English tokenizer.\n    :param sentence: input sentence.\n    :param to_lower_case: if set, remove capitalization at the beginning of the\n        input sentence.\n    :param lemmatization: if set, lemmatize the tokens.\n    :param remove_stop_words: if set, remove stop words.\n    :param correct_spell: if set, perform spelling error correction.\n    :param separate_quotations: if set, separate quotation marks from a quoted\n        token\n\n    :return: list of tokens obtained subjected to the tokenization criteria.\n    """"""\n    sentence = clean_sentence(sentence)\n    words = [x[0] for x in re.findall(\n        constants._WORD_SPLIT_RESPECT_QUOTES, sentence)]\n    \n    normalized_words = []\n    for i in xrange(len(words)):\n        word = words[i].strip()\n\n        if word in [\'""\', \'\\\'\']:\n            continue\n        \n        # normalize to lower cases\n        if to_lower_case:\n            if len(word) > 1 and constants.is_english_word(word) \\\n                    and not constants.with_quotation(word):\n                word = word.lower()\n        \n        # spelling correction\n        if correct_spell:\n            if word.isalpha() and word.islower() and len(word) > 2:\n                old_w = word\n                word = spc.correction(word)\n                if word != old_w:\n                    if verbose:\n                        print(""spell correction: {} -> {}"".format(old_w, word))\n       \n        # remove English stopwords\n        if remove_stop_words:\n            if word.lower() in constants.ENGLISH_STOPWORDS:\n                continue\n      \n        # covert number words into numbers\n        if word in constants.word2num:\n            word = str(constants.word2num[word])\n     \n        # lemmatization\n        if lemmatization and not constants.starts_with_quotation(word) \\\n                and not constants.ends_with_quotation(word) \\\n                and not re.match(constants._SPECIAL_SYMBOL_RE, word):\n            word = stemmer.stem(word)\n    \n        # remove empty words\n        if not word.strip():\n            continue\n\n        if separate_quotations and constants.with_quotation(word):\n            normalized_words.append(word[0])\n            normalized_words.append(word[1:-1])\n            normalized_words.append(word[-1])\n        else:\n            normalized_words.append(word)\n\n    return normalized_words, None\n\n\ndef ner_tokenizer(sentence, to_lower_case=True, lemmatization=True,\n                  remove_stop_words=True, correct_spell=True):\n    words, _ = basic_tokenizer(\n        sentence, to_lower_case=to_lower_case, lemmatization=lemmatization,\n        remove_stop_words=remove_stop_words, correct_spell=correct_spell)\n    return ner.annotate(words)\n\n# --- Utility functions --- #\n\ndef test_nl_tokenizer():\n    while True:\n        nl = input(""> "")\n        tokens, ners = basic_tokenizer(nl)\n        print(tokens, ners)\n        tokens, ners = ner_tokenizer(nl)\n        print(tokens, ners)        \n\n\nif __name__ == \'__main__\':\n    test_nl_tokenizer()\n'"
bashlint/grammar/extract_man.py,0,"b'import os\nimport subprocess\n\ntop_cmd_list_file = ""top100.txt""\nout_dir = ""gnu_man_synopsis""\n\n\ndef main():\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    top_cmds = map(lambda x: x.strip(), open(top_cmd_list_file,""r"").readlines())\n    for cmd in top_cmds:\n        try:\n            output = subprocess.check_output([cmd,\'--help\'])\n            with open(os.path.join(output_dir, cmd), \'w\') as o_f:\n                o_f.write(output)\n        except:\n            print cmd\n            continue\n\nif __name__ == \'__main__\':\n    main()\n'"
data/scripts/data_stats.py,0,"b'""""""\nCompute data statistics.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport numpy as np\nimport os, sys\nsys.path.append(\'../../\')  # for bashlint\nimport re\n\nfrom bashlint import bash, data_tools\nfrom nlp_tools.tokenizer import basic_tokenizer\n\n\ndef u_hist_to_radar_chart():\n    input_file = sys.argv[1]\n\n    u_hist = collections.defaultdict(int)\n    with open(input_file) as f:\n        for cmd in f:\n            ast = data_tools.bash_parser(cmd, verbose=False)\n            for u in data_tools.get_utilities(ast):\n                if u in bash.BLACK_LIST or u in bash.GREY_LIST:\n                    continue\n                u_hist[u] += 1\n\n    selected_utilities = []\n    for i, (u, freq) in enumerate(\n            sorted(u_hist.items(), key=lambda x:x[1], reverse=True)):\n        if i >= 50:\n            print(\'{{axis:""{}"",value:{:.2f}}},\'.format(u, freq))\n            selected_utilities.append(u)\n    print()\n\n\ndef compute_nl_stats():\n    input_file = sys.argv[1]\n    unique_sentences = set()\n    unique_words = set()\n    words_per_sent = []\n    sents_per_word = collections.defaultdict(int)\n    with open(input_file) as f:\n        for line in f:\n            nl = line.strip()\n            unique_sentences.add(nl)\n            words, _ = basic_tokenizer(nl, to_lower_case=False, lemmatization=False)\n            unique_words |= set(words)\n            words_per_sent.append(len(words))\n            for word in words:\n                sents_per_word[word] += 1\n    print(\'# unique sentences: {}\'.format(len(unique_sentences)))\n    print(\'# unique words: {}\'.format(len(unique_words)))\n    print(\'# words per sentence: average {}, median {}\'.format(\n        np.mean(words_per_sent), np.median(words_per_sent)))\n    print(\'# sentences per word: average {} median {}\'.format(\n        np.mean(sents_per_word.values()), np.median(sents_per_word.values())))\n    for w, f in sorted(sents_per_word.items(), key=lambda x:x[1], reverse=True)[:5]:\n        print(w, f)\n\ndef compute_cm_stats():\n    input_file = sys.argv[1]\n    unique_commands = set()\n    unique_templates = set()\n    unique_tokens = set()\n    tokens_per_cmd = []\n    cmds_per_token = collections.defaultdict(int)\n    with open(input_file) as f:\n        for line in f:\n            cm = line.strip()\n            unique_commands.add(cm)\n            temp = data_tools.cmd2template(cm, loose_constraints=True)\n            unique_templates.add(temp)\n            tokens = data_tools.bash_tokenizer(cm, loose_constraints=True)\n            unique_tokens |= set(tokens)\n            tokens_per_cmd.append(len(tokens))\n            for token in tokens:\n                cmds_per_token[token] += 1\n    print(\'# unique commands: {}\'.format(len(unique_commands)))\n    print(\'# unique templates: {}\'.format(len(unique_templates)))\n    print(\'# unique tokens: {}\'.format(len(unique_tokens)))\n    print(\'# tokens per command: average {}, median {}\'.format(\n        np.mean(tokens_per_cmd), np.median(tokens_per_cmd)))\n    print(\'# commands per token: average {}, median {}\'.format(\n        np.mean(cmds_per_token.values()), np.median(cmds_per_token.values())))\n\n\ndef compute_regex_stats():\n    input_file = sys.argv[1]\n    tokens = set()\n    with open(input_file) as f:\n        for line in f:\n            for t in line.strip().split():\n                tokens.add(t)\n    print(\'# unique tokens: {}\'.format(len(tokens)))                \n\n\ndef compute_jobs_stats():\n    input_file = sys.argv[1]\n    words = set()\n    with open(input_file) as f:\n        for line in f:\n            match = re.search(\'\\[(.*?)\\]\', line)\n            pattern = match.group(0)\n            for word in pattern[1:-1].split(\',\'):\n                words.add(word)\n    print(\'# unique words: {}\'.format(len(words)))\n\n\ndef compute_nlmaps_stats():\n    input_file = sys.argv[1]\n    words = set()\n    with open(input_file) as f:\n        for line in f:\n            line = line.strip()\n            if line[-1] in [\'?\', \'.\', \'!\']:\n                sent = line[:-1]\n                words.add(line[-1])\n            else:\n                sent = line\n                print(sent)\n            for word in sent.split():\n                words.add(word)\n    print(\'# unique words: {}\'.format(len(words)))\n    code_len_hist = {\n        7: 28,\n        8: 17,\n        9: 7,\n        10: 3,\n        11: 227,\n        12: 265,\n        13: 182,\n        14: 382,\n        15: 314,\n        16: 244,\n        17: 124,\n        18: 89,\n        19: 58,\n        20: 44,\n        21: 34,\n        22: 49,\n        23: 57,\n        24: 41,\n        25: 54,\n        26: 47,\n        27: 25,\n        28: 18,\n        29: 32,\n        30: 14,\n        31: 2,\n        32: 12,\n        33: 8,\n        36: 5\n    }\n    total_len = 0\n    num = 0\n    for key in code_len_hist:\n        total_len += key * code_len_hist[key]\n        num += code_len_hist[key]\n    print(\'average sentence length: {}\'.format(float(total_len) / num))\n\ndef compute_bash_stats():\n    input_file = sys.argv[1]\n    unique_utilities = set()\n    unique_flags = set()\n    unique_keywords = set()\n    cmds_per_utility = collections.defaultdict(int)\n    cmds_per_flag = collections.defaultdict(int)\n    with open(input_file) as f:\n        for line in f:\n            cm = line.strip()\n            tokens = data_tools.bash_tokenizer(cm, loose_constraints=True, with_prefix=True)\n            for token in tokens:\n                if token.startswith(\'UTILITY<KIND_PREFIX>\'):\n                    unique_utilities.add(token)\n                    cmds_per_utility[token] += 1\n                elif token.startswith(\'FLAG<KIND_PREFIX>\'):\n                    unique_flags.add(token)\n                    cmds_per_flag[token] += 1\n                elif not token.startswith(\'ARGUMENT<KIND_PREFIX>\'):\n                    unique_keywords.add(token)\n    print(\'# unique utilities: {}\'.format(len(unique_utilities)))\n    print(\'# unique flags: {}\'.format(len(unique_flags)))\n    print(\'# unique keywords: {}\'.format(len(unique_keywords)))\n    print(\'# commands per utility: average {}, median {}\'.format(\n        np.mean(list(cmds_per_utility.values())), np.median(list(cmds_per_utility.values()))))\n    print(\'# commands per flag: average {}, median {}\'.format(\n        np.mean(list(cmds_per_flag.values())), np.median(list(cmds_per_flag.values()))))\n\ndef compute_flag_stats():\n    input_file = sys.argv[1]\n    train_file = sys.argv[2]\n\n    u_hist = collections.defaultdict(int)\n    with open(input_file) as f:\n        for cmd in f:\n            ast = data_tools.bash_parser(cmd, verbose=False)\n            for u in data_tools.get_utilities(ast):\n                if u in bash.BLACK_LIST or u in bash.GREY_LIST:\n                    continue\n                u_hist[u] += 1\n    \n    sorted_u_by_freq = sorted(u_hist.items(), key=lambda x:x[1], reverse=True)\n    most_frequent_10 = [u for u, _ in sorted_u_by_freq[:10]]\n    least_frequent_10 = [u for u, _ in sorted_u_by_freq[-10:]]\n    \n    most_frequent_10_flags = collections.defaultdict(set)\n    least_frequent_10_flags = collections.defaultdict(set)\n    with open(train_file) as f:\n        for cmd in f:\n            tokens = data_tools.bash_tokenizer(cmd, loose_constraints=True,\n                                               with_flag_head=True)\n            for token in tokens:\n                if \'@@\' in token:\n                    u, f = token.split(\'@@\')\n                    if u in most_frequent_10:\n                        most_frequent_10_flags[u].add(f)\n                    if u in least_frequent_10:\n                        least_frequent_10_flags[u].add(f)\n\n    for u in most_frequent_10:\n        if u in most_frequent_10_flags:\n            print(u, data_tools.get_utility_statistics(u), len(most_frequent_10_flags[u]))\n        else:\n            print(u, data_tools.get_utility_statistics(u), 0)\n    print()\n    for u in least_frequent_10:\n        if u in least_frequent_10_flags:\n            print(u, data_tools.get_utility_statistics(u), len(least_frequent_10_flags[u]))\n        else:\n            print(u, data_tools.get_utility_statistics(u), 0)    \n    \ndef compute_mapping_stats():\n    nl_file = sys.argv[1]\n    cm_file = sys.argv[2]\n    nl_to_cm_sizes = collections.defaultdict(int)\n    cm_to_nl_sizes = collections.defaultdict(int)\n    n_f = open(nl_file)\n    c_f = open(cm_file)\n    for nl, cm in zip(n_f, c_f):\n        nl_to_cm_sizes[nl] += 1\n        cm_to_nl_sizes[cm] += 1\n    n_f.close()\n    c_f.close()\n    print(\'# cms per nl: average {}, median {}, max {}\'.format(\n        np.mean(nl_to_cm_sizes.values()), np.median(nl_to_cm_sizes.values()), np.max(nl_to_cm_sizes.values())))\n    print(\'# nls per cm: average {}, median {}, max {}\'.format(\n        np.mean(cm_to_nl_sizes.values()), np.median(cm_to_nl_sizes.values()), np.max(cm_to_nl_sizes.values())))\n\ndef count_unique_nls():\n    nl_file = sys.argv[1]\n    unique_nls = set()\n    with open(nl_file) as f:\n        for line in f:\n            nl = line.strip()\n            nl_temp = \' \'.join(basic_tokenizer(nl)[0])\n            unique_nls.add(nl_temp)\n    print(\'number of unique natural language forms: {}\'.format(len(unique_nls)))\n\n        \ndef main():\n    # compute_nl_stats()\n    # compute_cm_stats()\n    # compute_bash_stats()\n    # compute_mapping_stats()\n    count_unique_nls()\n    # u_hist_to_radar_chart()\n    # compute_flag_stats()\n    # compute_regex_stats()\n    # compute_jobs_stats()\n    # compute_nlmaps_stats()\n\nif __name__ == \'__main__\':\n    main()\n'"
data/scripts/feature_compare.py,0,"b'""""""\nPrint features from different channels side-by-side.\n""""""\n\nimport os, sys\nsys.path.append(\n    os.path.dirname(\n        os.path.dirname(\n            os.path.dirname(\n                os.path.abspath(__file__)))))\n\nfrom bashlint import data_tools\nfrom encoder_decoder import data_utils\n\n\ndef gen_compare_file(split, lang):\n     with open(\'{}.{}.compare\'.format(split, lang), \'w\') as o_f:\n        with open(\'{}.{}.{}\'.format(split, lang, channel1)) as f:\n            train_nls_channel1 = [line.strip() for line in f.readlines()]\n        with open(\'{}.{}.{}\'.format(split, lang, channel2)) as f:\n            train_nls_channel2 = [line.strip() for line in f.readlines()]\n        assert(len(train_nls_channel1) == len(train_nls_channel2))\n        for nl1, nl2 in zip(train_nls_channel1, train_nls_channel2):\n            o_f.write(\'\\t\'.join(\n                [t.split(data_tools.flag_suffix)[0] \n                    for t in nl1.split(data_utils.TOKEN_SEPARATOR)]) + \'\\n\')\n            o_f.write(\'\\t\'.join(\n                [t.split(data_tools.flag_suffix)[0] \n                    for t in nl2.split(data_utils.TOKEN_SEPARATOR)]) + \'\\n\')\n            o_f.write(\'\\n\')\n\nif __name__ == \'__main__\':\n    channel1 = sys.argv[1]\n    channel2 = sys.argv[2]\n    gen_compare_file(\'train\', \'nl\')\n    gen_compare_file(\'train\', \'cm\')\n    gen_compare_file(\'dev\', \'nl\')\n    gen_compare_file(\'dev\', \'cm\')\n'"
data/scripts/filter_data.py,0,"b'""""""\nFilter data pairs and retain only grammatical and frequently used commands.\n\nUsage: python3 filter_data.py data_dir\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os, sys\nsys.path.append(\'../../\')  # for bashlint\n\nfrom bashlint import bash, data_tools\n\ndata_splits = [\'train\', \'dev\', \'test\']\n\nNUM_UTILITIES = 100\n\ndef compute_top_utilities(path, k):\n    print(\'computing top most frequent utilities...\') \n    utilities = collections.defaultdict(int)\n    with open(path, encoding=\'utf-8\') as f:\n        while (True):\n            command = f.readline().strip()\n            if not command:\n                break\n            ast = data_tools.bash_parser(command, verbose=False)\n            for u in data_tools.get_utilities(ast):\n                utilities[u] += 1\n    top_utilities = []\n\n    freq_threshold = -1   \n    for u, freq in sorted(utilities.items(), key=lambda x:x[1], reverse=True):\n        if freq_threshold > 0 and freq < freq_threshold:\n            break \n        if u in bash.BLACK_LIST or u in bash.GREY_LIST:\n            continue\n        top_utilities.append(u)\n        print(\'{}: {} ({})\'.format(len(top_utilities), u, freq))\n        if len(top_utilities) == k:\n            freq_threshold = freq\n    top_utilities = set(top_utilities)\n    return top_utilities\n\n\ndef filter_by_most_frequent_utilities(data_dir, num_utilities):\n    def select(ast, utility_set):\n        for ut in data_tools.get_utilities(ast):\n            if not ut in utility_set:\n                print(\'Utility currently not handled: {} - {}\'.format(\n                    ut, data_tools.ast2command(ast, loose_constraints=True).encode(\'utf-8\')))\n                return False\n        return True\n\n    cm_path = os.path.join(data_dir, \'all.cm\')\n    top_utilities = compute_top_utilities(cm_path, num_utilities)\n    for split in [\'all\']:\n        nl_file_path = os.path.join(data_dir, split + \'.nl\')\n        cm_file_path = os.path.join(data_dir, split + \'.cm\')\n        with open(nl_file_path, encoding=\'utf-8\') as f:\n            nls = [nl.strip() for nl in f.readlines()]\n        with open(cm_file_path, encoding=\'utf-8\') as f:\n            cms = [cm.strip() for cm in f.readlines()]\n        nl_outfile_path = os.path.join(data_dir, split + \'.nl.filtered\')\n        cm_outfile_path = os.path.join(data_dir, split + \'.cm.filtered\')\n        with open(nl_outfile_path, \'w\', encoding=\'utf-8\') as nl_outfile:\n            with open(cm_outfile_path, \'w\', encoding=\'utf-8\') as cm_outfile:\n                for nl, cm in zip(nls, cms):\n                    if len(nl.split()) > 50:\n                        print(\'lenthy description skipped: {}\'.format(nl))\n                        continue\n                    ast = data_tools.bash_parser(cm)\n                    if ast and select(ast, top_utilities):\n                        nl_outfile.write(\'{}\\n\'.format(nl))\n                        cm_outfile.write(\'{}\\n\'.format(cm))\n                            \n\ndef gen_non_specific_description_check_csv(data_dir):\n    with open(os.path.join(data_dir, \'all.nl\')) as f:\n        nl_list = [nl.strip() for nl in f.readlines()]\n    with open(os.path.join(data_dir, \'all.cm\')) as f:\n        cm_list = [cm.strip() for cm in f.readlines()]\n    assert(len(nl_list) == len(cm_list))\n\n    with open(\'annotation_check_sheet.non.specific.csv\', \'w\') as o_f:\n        o_f.write(\'Utility,Command,Description\\n\')\n        for nl, cm in zip(nl_list, cm_list):\n            if \' specific \' in nl or \' a file \' in nl or \' a folder \' in nl \\\n                    or \' a directory \' in nl or \' some \' in nl \\\n                    or \' a pattern \' in nl or \' a string \' in nl:\n                ast = data_tools.bash_parser(cm)\n                if ast:\n                    o_f.write(\',""{}"",""{}""\\n\'.format(cm.replace(\'""\', \'""""\'), \n                        nl.replace(\'""\', \'""""\')))\n                    o_f.write(\',,<Type a new description here>\\n\')\n\n\nif __name__ == \'__main__\':\n    dataset = sys.argv[1]\n    data_dir = os.path.join(os.path.dirname(os.path.dirname(\n        os.path.realpath(__file__))), dataset)\n    filter_by_most_frequent_utilities(data_dir, NUM_UTILITIES)\n'"
data/scripts/repair_data.py,0,"b'""""""\nRepair bad annotations in the existing dataset with fixed annotations.\nKeep the original data split.\n\nUsage: python3 repair_data.py [data_directory]\n""""""\n\nimport csv\nimport os, sys\n\n\ndef import_repairs(import_dir):\n    repairs, errors, new_annotations = {}, {}, {}\n    new_annotation = False\n    for file_name in os.listdir(import_dir):\n        print(\'reading {}...\'.format(file_name))\n        if file_name.endswith(\'.csv\'):\n            with open(os.path.join(import_dir, file_name)) as f:\n                reader = csv.DictReader(f) \n                for i, row in enumerate(reader):\n                    if i % 2 == 0:\n                        command = row[\'Command\'].strip()\n                        old_description = row[\'Description\'].strip()\n                        print(command)\n                        print(old_description)\n                        print()\n                        new_annotation = (old_description == \'--\')\n                        example_sig = \'{}<NL_Command>{}\'.format(\n                            old_description, command)\n                    else:\n                        description = row[\'Description\'].strip()\n                        if description == \'<Type a new description here>\':\n                            continue\n                        elif description == \'ERROR\':\n                            errors[example_sig] = None\n                        else:\n                            if new_annotation:\n                                new_annotations[\n                                    \'{}<NL_Command>{}\'.format(description, command)] = None\n                            else:\n                                repairs[example_sig] = description\n    print(\'{} repairs, {} errors and {} new annotations loaded\'.format(\n        len(repairs), len(errors), len(new_annotations)))\n    return repairs, errors, new_annotations\n\n\ndef repair_data(nl_path, cm_path, repairs, errors, new_annotations):\n    with open(nl_path) as f:\n        nls = [line.strip() for line in f.readlines()]\n    with open(cm_path) as f:\n        cms = [line.strip() for line in f.readlines()]\n\n    repaired_data = []\n\n    # Add data repairs\n    for nl, cm in zip(nls, cms):\n        example_sig = \'{}<NL_Command>{}\'.format(nl, cm)\n        if example_sig in repairs:\n            new_nl = repairs[example_sig]\n            repaired_data.append((new_nl, cm))\n        elif example_sig in errors:\n            continue\n        else:\n            repaired_data.append((nl, cm))\n\n    # Add new annotations\n    for example_sig in new_annotations:\n        repaired_data.append(tuple(example_sig.split(\'<NL_Command>\')))\n\n    repaired_data = sorted(list(set(repaired_data)))\n    print(\'{} repaired data points in total\'.format(len(repaired_data)))\n\n    # Save repaired data to disk\n    nl_out_path = nl_path + \'.repaired\'\n    cm_out_path = cm_path + \'.repaired\'\n    nl_out = open(nl_out_path, \'w\')\n    cm_out = open(cm_out_path, \'w\')\n    for nl, cm in repaired_data:\n        nl_out.write(\'{}\\n\'.format(nl))\n        cm_out.write(\'{}\\n\'.format(cm))\n    nl_out.close()\n    cm_out.close()\n\n\nif __name__ == \'__main__\':\n    data_dir = sys.argv[1]\n    repairs, errors, new_annotations = \\\n        import_repairs(os.path.join(data_dir, \'repairs\'))\n    repair_data(os.path.join(data_dir, \'all.nl\'),\n                os.path.join(data_dir, \'all.cm\'),\n                repairs, errors, new_annotations)\n'"
data/scripts/split_data.py,0,"b'""""""\nSplit the dataset into train, dev and test randomly according to the given \nratio.\n\nUsage: python3 split_data.py [data_directory]\n""""""\n\nimport collections\nimport random\nimport re\nimport os, sys\nsys.path.append("".."")\nsys.path.append(""../../"")\nsys.path.append(""../../bashlint"")\n\nfrom nlp_tools.tokenizer import basic_tokenizer, ner_tokenizer\n\nhtml_rel2abs = re.compile(\'""/[^\\s<>]*/*http\')\nhypothes_header = re.compile(\n    \'\\<\\!\\-\\- WB Insert \\-\\-\\>.*\\<\\!\\-\\- End WB Insert \\-\\-\\>\', re.DOTALL)\n\nRANDOM_SEED = 100\n\nnl_suffix, cm_suffix = \'nl.filtered\', \'cm.filtered\'\n\ndef get_nl_temp(nl):\n    return \' \'.join(basic_tokenizer(nl)[0])\n\n\ndef split_data(data_dir):\n    def write_data(data_path, data):\n        with open(data_path, \'w\', encoding=\'utf-8\') as o_f:\n            for line in data:\n                o_f.write(line + \'\\n\')\n            print(\'{} saved\'.format(data_path))\n\n    nl_file_path = os.path.join(data_dir, \'all.{}\'.format(nl_suffix))\n    cm_file_path = os.path.join(data_dir, \'all.{}\'.format(cm_suffix))\n\n    with open(nl_file_path, encoding=\'utf-8\') as f:\n        nls = [line.strip() for line in f.readlines()]\n    with open(cm_file_path, encoding=\'utf-8\') as f:\n        cms = [line.strip() for line in f.readlines()]\n\n    assert(len(nls) == len(cms))\n\n    pairs = collections.defaultdict(list)\n\n    for nl, cm in zip(nls, cms):\n        nl_temp = get_nl_temp(nl)\n        pairs[nl_temp].append((nl, cm))\n\n    train_nl_list = []\n    train_cm_list = []\n    dev_nl_list = []\n    dev_cm_list = []\n    test_nl_list = []\n    test_cm_list = []\n\n    num_folds = 12\n    num_train = 0\n    num_dev = 0\n    num_test = 0\n\n    # randomly split data according to ratio\n    random.seed(RANDOM_SEED)\n    train_commands = set()\n    count = 0\n    random_tokens = [random.randint(0, num_folds-1) for i in range(len(pairs.keys()))] \n    with open(os.path.join(data_dir, \'random_tokens.txt\'), \'w\') as o_f:\n        for r_token in random_tokens:\n            o_f.write(\'{}\\n\'.format(r_token))\n\n    for i, nl_temp in enumerate(sorted(pairs.keys())):\n        ind = random_tokens[i]\n        if ind < num_folds - 2:\n            num_train += 1\n            for nl, cm in pairs[nl_temp]:\n                train_nl_list.append(nl)\n                train_cm_list.append(cm)\n                train_commands.add(cm)\n        elif ind == num_folds - 2:\n            num_dev += 1\n            for nl, cm in pairs[nl_temp]:\n                dev_nl_list.append(nl)\n                dev_cm_list.append(cm)\n        elif ind == num_folds - 1:\n            num_test += 1\n            for nl, cm in pairs[nl_temp]:\n                test_nl_list.append(nl)\n                test_cm_list.append(cm)\n        count += 1\n    print(len(train_nl_list), len(dev_nl_list), len(test_nl_list))\n\n    # move dev/test examples whose command has appeared in the train set to \n    # train\n    dev_nl_list_cleaned = []\n    dev_cm_list_cleaned = []\n    num_moved = 0\n    for nl, cm in zip(dev_nl_list, dev_cm_list):\n        if cm in train_commands:\n            train_nl_list.append(nl)\n            train_cm_list.append(cm)\n            num_moved += 1\n        else:\n            dev_nl_list_cleaned.append(nl)\n            dev_cm_list_cleaned.append(cm)\n    print(\'{} pairs moved from dev to train\'.format(num_moved))\n\n    test_nl_list_cleaned = []\n    test_cm_list_cleaned = []\n    num_moved = 0\n    for nl, cm in zip(test_nl_list, test_cm_list):\n        if cm in train_commands:\n            train_nl_list.append(nl)\n            train_cm_list.append(cm)\n            num_moved += 1\n        else:\n            test_nl_list_cleaned.append(nl)\n            test_cm_list_cleaned.append(cm)\n    print(\'{} pairs moved from test to train\'.format(num_moved))\n\n    # select 100 examples as dev\n    # dev_nl_list_reorg = []\n    # dev_cm_list_reorg = []\n    # indices = list(range(len(dev_nl_list_cleaned)))\n    # random.seed(RANDOM_SEED)\n    # random.shuffle(indices)\n    # dev_nl_temps = set()\n    # for count, dev_ind in enumerate(indices):\n    #     dev_nl_temps.add(get_nl_temp(dev_nl_list_cleaned[dev_ind]))\n    #     dev_nl_list_reorg.append(dev_nl_list_cleaned[dev_ind])\n    #     dev_cm_list_reorg.append(dev_cm_list_cleaned[dev_ind])\n    #     if len(dev_nl_temps) == 100:\n    #         break\n    # \n    # test_nl_list_reorg = test_nl_list_cleaned\n    # test_cm_list_reorg = test_cm_list_cleaned\n    # for i in indices[count+1:]:\n    #     test_nl_list_reorg.append(dev_nl_list_cleaned[i])\n    #     test_cm_list_reorg.append(dev_cm_list_cleaned[i])\n    \n    train_path = os.path.join(data_dir, ""train"")\n    dev_path = os.path.join(data_dir, ""dev"")\n    test_path = os.path.join(data_dir, ""test"")\n    write_data(train_path + \'.\' + nl_suffix, train_nl_list)\n    write_data(train_path + \'.\' + cm_suffix, train_cm_list)\n    write_data(dev_path + \'.\' + nl_suffix, dev_nl_list_cleaned)\n    write_data(dev_path + \'.\' + cm_suffix, dev_cm_list_cleaned)\n    write_data(test_path + \'.\' + nl_suffix, test_nl_list_cleaned)\n    write_data(test_path + \'.\' + cm_suffix, test_cm_list_cleaned)\n\n\nif __name__ == \'__main__\':\n    dataset = sys.argv[1]\n    data_dir = os.path.join(os.path.dirname(\n        os.path.realpath(os.path.dirname(__file__))), dataset)\n    split_data(data_dir)\n'"
data/scripts/utility_hist.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os, sys\nsys.path.append(\'/home/xilin/Projects/tellina/learning_module/\')\n\nimport numpy as np\n\nfrom bashlint import bash, data_tools\n\n\ndef get_u_hist_from_file(input_file):\n    u_hist = collections.defaultdict(int)\n    with open(input_file) as f:\n        for cmd in f:\n            ast = data_tools.bash_parser(cmd, verbose=False)\n            for u in data_tools.get_utilities(ast):\n                if u in bash.BLACK_LIST or u in bash.GREY_LIST:\n                    continue\n                u_hist[u] += 1\n    return u_hist\n\n\ndef u_hist_to_radar_chart(hist):\n    long_tail = 0\n    top_utilities = []\n    for i, (u, freq) in enumerate(\n            sorted(hist.items(), key=lambda x:x[1], reverse=True)):\n        if i < 50:\n            print(\'{{axis:""{}"",value:{:.2f}}},\'.format(u, freq))\n            top_utilities.append(u)\n        else:\n            long_tail += freq\n    print(\'{{axis:""{}"",value:{:.2f}}}\'.format(\'OTHER\', long_tail))\n    print()\n    return top_utilities\n\n\ndef u_hist_flag_num(hist):\n    long_tail = 0\n    output = []\n    for i, (u, freq) in enumerate(\n            sorted(hist.items(), key=lambda x:x[1], reverse=True)):\n        if i < 50:\n            output.append(\'{{axis:""{}"",value:{}}},\'.format(\n                u, data_tools.get_utility_statistics(u)))\n        else:\n            long_tail += data_tools.get_utility_statistics(u)\n    output.append(\'{{axis:""{}"",value:{}}}\'.format(\'OTHER\', long_tail))\n    for i in range(len(output)-1, -1, -1):\n        print(output[i])\n   \n\ndef get_flag_statistics(top_utilities, input_file):\n    flag_counts = {}\n    for u in top_utilities:\n        flag_counts[u] = set()\n    with open(input_file) as f:\n        for cmd in f:\n            ast = data_tools.bash_parser(cmd, verbose=False)\n            if ast:\n                # DFS \n                stack = []\n                stack.extend(ast.children)\n                while stack:\n                    node = stack.pop()\n                    if node.is_option():\n                        u = node.utility.value\n                        if u in flag_counts:\n                            flag_counts[u].add(node.value)\n                    stack.extend(node.children)\n            else:\n                print(cmd)\n    total_flag_count = 0\n    for i in range(len(top_utilities)-1, -1, -1):\n        u = top_utilities[i]\n        print(\'{{axis:""{}"",num_flags:{},num_flags_in_data:{}}},\'\n              .format(u, data_tools.get_utility_statistics(u), \n                      len(flag_counts[u])))\n        total_flag_count += len(flag_counts[u])\n    print(\'Total # distinct flags = {}\'.format(total_flag_count))\n\n\nif __name__ == \'__main__\':\n    all_path = sys.argv[1]\n    train_path = sys.argv[2]\n    dev_path = sys.argv[3]\n    test_path = sys.argv[4]\n\n    all_hist = get_u_hist_from_file(all_path)\n    train_hist = get_u_hist_from_file(train_path)\n    dev_hist = get_u_hist_from_file(dev_path)\n    test_hist = get_u_hist_from_file(test_path)\n    # for i, (u, freq) in enumerate(\n    #         sorted(all_hist.items(), key=lambda x:x[1], reverse=True)):\n    #     print(\'{},{},{},{},{}\'.format(i, u, train_hist[u], dev_hist[u], test_hist[u]))\n\n    top_utilities = u_hist_to_radar_chart(all_hist)\n    get_flag_statistics(top_utilities, all_path)\n\n'"
encoder_decoder/seq2seq/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Makes helper libraries available in the translate package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function'"
encoder_decoder/seq2seq/rnn_decoder.py,53,"b'""""""\nRNN-based decoders.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom encoder_decoder import decoder, data_utils, graph_utils\n\n\nclass RNNDecoder(decoder.Decoder):\n    def __init__(self, hyperparameters, scope, dim, embedding_dim, use_attention,\n                 attention_function, input_keep, output_keep, decoding_algorithm):\n        """"""\n        :member hyperparameters:\n        :member scope:\n        :member dim:\n        :member embedding_dim:\n        :member use_attention:\n        :member attention_function:\n        :member input_keep:\n        :member output_keep:\n        :member decoding_algorithm:\n        """"""\n        super(RNNDecoder, self).__init__(hyperparameters, scope, dim,\n            embedding_dim, use_attention, attention_function, input_keep,\n            output_keep, decoding_algorithm)\n        print(""{} dimension = {}"".format(scope, dim))\n        print(""{} decoding_algorithm = {}"".format(scope, decoding_algorithm))\n\n\n    def define_graph(self, encoder_state, decoder_inputs,\n                     input_embeddings=None, encoder_attn_masks=None,\n                     attention_states=None, num_heads=1,\n                     encoder_copy_inputs=None):\n        """"""\n        :param encoder_state: Encoder state => initial decoder state.\n        :param decoder_inputs: Decoder training inputs (""<START>, ... <EOS>"").\n        :param input_embeddings: Decoder vocabulary embedding.\n        :param encoder_attn_masks: Binary masks whose entries corresponding to non-padding tokens are 1.\n        :param attention_states: Encoder hidden states.\n        :param num_heads: Number of attention heads.\n        :param encoder_copy_inputs: Array of encoder copy inputs where the copied words are represented using target\n            vocab indices and place holding indices are used elsewhere.\n        :return output_symbols: (batched) discrete output sequences\n        :return output_logits: (batched) output sequence scores\n        :return outputs: (batched) output states for all steps\n        :return states: (batched) hidden states for all steps\n        :return attn_alignments: (batched) attention masks (if attention is used)\n        """"""\n        if self.use_attention:\n            assert(attention_states.get_shape()[1:2].is_fully_defined())\n        if encoder_copy_inputs:\n            assert(attention_states.get_shape()[1] == len(encoder_copy_inputs))\n        bs_decoding = self.forward_only and \\\n                      self.decoding_algorithm == ""beam_search""\n\n        if input_embeddings is None:\n            input_embeddings = self.embeddings()\n\n        if self.force_reading_input:\n            print(""Warning: reading ground truth decoder inputs at decoding time."")\n\n        with tf.compat.v1.variable_scope(self.scope + ""_decoder_rnn"") as scope:\n            decoder_cell = self.decoder_cell()\n            states = []\n            alignments_list = []\n            pointers = None\n\n            # Cell Wrappers -- \'Attention\', \'CopyNet\', \'BeamSearch\'\n            if bs_decoding:\n                beam_decoder = self.beam_decoder\n                state = beam_decoder.wrap_state(encoder_state, self.output_project)\n            else:\n                state = encoder_state\n                past_output_symbols = []\n                past_output_logits = []\n\n            if self.use_attention:\n                if bs_decoding:\n                    encoder_attn_masks = graph_utils.wrap_inputs(\n                        beam_decoder, encoder_attn_masks)\n                    attention_states = beam_decoder.wrap_input(attention_states)\n                encoder_attn_masks = [tf.expand_dims(encoder_attn_mask, 1)\n                    for encoder_attn_mask in encoder_attn_masks]\n                encoder_attn_masks = tf.concat(axis=1, values=encoder_attn_masks)\n                decoder_cell = decoder.AttentionCellWrapper(\n                    decoder_cell,\n                    attention_states,\n                    encoder_attn_masks,\n                    self.attention_function,\n                    self.attention_input_keep,\n                    self.attention_output_keep,\n                    num_heads,\n                    self.dim,\n                    self.num_layers,\n                    self.use_copy,\n                    self.vocab_size\n                )\n\n            if self.use_copy and self.copy_fun == \'copynet\':\n                decoder_cell = decoder.CopyCellWrapper(\n                    decoder_cell,\n                    self.output_project,\n                    self.num_layers,\n                    encoder_copy_inputs,\n                    self.vocab_size)\n\n            if bs_decoding:\n                decoder_cell = beam_decoder.wrap_cell(\n                    decoder_cell, self.output_project)\n\n            def step_output_symbol_and_logit(output):\n                epsilon = tf.constant(1e-12)\n                if self.copynet:\n                    output_logits = tf.math.log(output + epsilon)\n                else:\n                    W, b = self.output_project\n                    output_logits = tf.math.log(\n                        tf.nn.softmax(tf.matmul(output, W) + b) + epsilon)\n                output_symbol = tf.argmax(input=output_logits, axis=1)\n                past_output_symbols.append(output_symbol)\n                past_output_logits.append(output_logits)\n                return output_symbol, output_logits\n\n            for i, input in enumerate(decoder_inputs):\n                if bs_decoding:\n                    input = beam_decoder.wrap_input(input)\n\n                if i > 0:\n                    scope.reuse_variables()\n                    if self.forward_only:\n                        if self.decoding_algorithm == ""beam_search"":\n                            (\n                                past_beam_symbols,  # [batch_size*self.beam_size, max_len], right-aligned!!!\n                                past_beam_logprobs, # [batch_size*self.beam_size]\n                                past_cell_states,   # [batch_size*self.beam_size, max_len, dim]\n                            ) = state\n                            input = past_beam_symbols[:, -1]\n                        elif self.decoding_algorithm == ""greedy"":\n                            output_symbol, _ = step_output_symbol_and_logit(output)\n                            if not self.force_reading_input:\n                                input = tf.cast(output_symbol, dtype=tf.int32)\n                    else:\n                        step_output_symbol_and_logit(output)\n                    if self.copynet:\n                        decoder_input = input\n                        input = tf.compat.v1.where(input >= self.target_vocab_size,\n                                         tf.ones_like(input)*data_utils.UNK_ID, input)\n\n                input_embedding = tf.nn.embedding_lookup(params=input_embeddings, ids=input)\n\n                # Appending selective read information for CopyNet\n                if self.copynet:\n                    attn_length = attention_states.get_shape()[1]\n                    attn_dim = attention_states.get_shape()[2]\n                    if i == 0:\n                        # Append dummy zero vector to the <START> token\n                        selective_reads = tf.zeros([self.batch_size, attn_dim])\n                        if bs_decoding:\n                            selective_reads = beam_decoder.wrap_input(selective_reads)\n                    else:\n                        encoder_copy_inputs_2d = tf.concat(\n                            [tf.expand_dims(x, 1) for x in encoder_copy_inputs], axis=1)\n                        if self.forward_only:\n                            copy_input = tf.compat.v1.where(decoder_input >= self.target_vocab_size,\n                                                  tf.reduce_sum(\n                                                    input_tensor=tf.one_hot(input-self.target_vocab_size, \n                                                               depth=attn_length, dtype=tf.int32)\n                                                    * encoder_copy_inputs_2d,\n                                                    axis=1),\n                                                  decoder_input)\n                        else:\n                            copy_input = decoder_input\n                        tiled_copy_input = tf.tile(input=tf.reshape(copy_input, [-1, 1]),\n                                                   multiples=np.array([1, attn_length]))\n                        # [batch_size(*self.beam_size), max_source_length]\n                        selective_mask = tf.cast(tf.equal(tiled_copy_input, encoder_copy_inputs_2d),\n                                                 dtype=tf.float32)\n                        # [batch_size(*self.beam_size), max_source_length]\n                        weighted_selective_mask = tf.nn.softmax(selective_mask * alignments[1])\n                        # [batch_size(*self.beam_size), max_source_length, attn_dim]\n                        weighted_selective_mask_3d = tf.tile(input=tf.expand_dims(weighted_selective_mask, 2), \n                                                             multiples=np.array([1, 1, attn_dim]))\n                        # [batch_size(*self.beam_size), attn_dim]\n                        selective_reads = tf.reduce_sum(input_tensor=weighted_selective_mask_3d * attention_states, axis=1)\n                    input_embedding = tf.concat([input_embedding, selective_reads], axis=1)\n\n                if self.copynet:\n                    output, state, alignments, attns = \\\n                        decoder_cell(input_embedding, state)\n                    alignments_list.append(alignments)\n                elif self.use_attention:\n                    output, state, alignments, attns = \\\n                        decoder_cell(input_embedding, state)\n                    alignments_list.append(alignments)\n                else:\n                    output, state = decoder_cell(input_embedding, state)\n               \n                # save output states\n                if not bs_decoding:\n                    # when doing beam search decoding, the output state of each\n                    # step cannot simply be gathered step-wise outside the decoder\n                    # (speical case: beam_size = 1)\n                    states.append(state)\n\n            if self.use_attention:\n                # Tensor list --> tenosr\n                attn_alignments = tf.concat(axis=1,\n                    values=[tf.expand_dims(x[0], 1) for x in alignments_list])\n            if self.copynet:\n                pointers = tf.concat(axis=1,\n                    values=[tf.expand_dims(x[1], 1) for x in alignments_list])\n\n            if bs_decoding:\n                # Beam-search output\n                (\n                    past_beam_symbols,  # [batch_size*self.beam_size, max_len], right-aligned!!!\n                    past_beam_logprobs, # [batch_size*self.beam_size]\n                    past_cell_states,\n                ) = state\n                # [self.batch_size, self.beam_size, max_len]\n                top_k_osbs = tf.reshape(past_beam_symbols[:, 1:],\n                                        [self.batch_size, self.beam_size, -1])\n                top_k_osbs = tf.split(axis=0, num_or_size_splits=self.batch_size,\n                                      value=top_k_osbs)\n                top_k_osbs = [tf.split(axis=0, num_or_size_splits=self.beam_size,\n                                       value=tf.squeeze(top_k_output, axis=[0]))\n                              for top_k_output in top_k_osbs]\n                top_k_osbs = [[tf.squeeze(output, axis=[0]) for output in top_k_output]\n                              for top_k_output in top_k_osbs]\n                # [self.batch_size, self.beam_size]\n                top_k_seq_logits = tf.reshape(past_beam_logprobs,\n                                              [self.batch_size, self.beam_size])\n                top_k_seq_logits = tf.split(axis=0, num_or_size_splits=self.batch_size,\n                                            value=top_k_seq_logits)\n                top_k_seq_logits = [tf.squeeze(top_k_logit, axis=[0])\n                                    for top_k_logit in top_k_seq_logits]\n                if self.use_attention:\n                    attn_alignments = tf.reshape(attn_alignments,\n                            [self.batch_size, self.beam_size, len(decoder_inputs),\n                             attention_states.get_shape()[1]])\n                # LSTM: ([batch_size*self.beam_size, :, dim],\n                #        [batch_size*self.beam_size, :, dim])\n                # GRU: [batch_size*self.beam_size, :, dim]\n                if self.rnn_cell == \'lstm\':\n                    if self.num_layers == 1:\n                        c_states, h_states = past_cell_states\n                        states = list(zip(\n                            [tf.squeeze(x, axis=[1])\n                                for x in tf.split(c_states, c_states.get_shape()[1],\n                                                  axis=1)],\n                            [tf.squeeze(x, axis=[1])\n                                for x in tf.split(h_states, h_states.get_shape()[1],\n                                                  axis=1)]))\n                    else:\n                        layered_states = [list(zip(\n                                [tf.squeeze(x, axis=[1]) \n                                    for x in tf.split(c_states, c_states.get_shape()[1],\n                                                      axis=1)[1:]],\n                                [tf.squeeze(x, axis=[1])\n                                    for x in tf.split(h_states, h_states.get_shape()[1],\n                                                      axis=1)[1:]]))\n                            for c_states, h_states in past_cell_states]\n                        states = list(zip(layered_states))\n                elif self.rnn_cell in [\'gru\', \'ran\']:\n                    states = [tf.squeeze(x, axis=[1]) for x in \\\n                        tf.split(num_or_size_splits=past_cell_states.get_shape()[1],\n                                 axis=1, value=past_cell_states)][1:]\n                else:\n                    raise AttributeError(\n                        ""Unrecognized rnn cell type: {}"".format(self.rnn_cell))\n                return top_k_osbs, top_k_seq_logits, states, \\\n                       states, attn_alignments, pointers\n            else:\n                # Greedy output\n                step_output_symbol_and_logit(output)\n                output_symbols = tf.concat(\n                    [tf.expand_dims(x, 1) for x in past_output_symbols], axis=1)\n                sequence_logits = tf.add_n([tf.reduce_max(input_tensor=x, axis=1) \n                                            for x in past_output_logits])\n                return output_symbols, sequence_logits, past_output_logits, \\\n                       states, attn_alignments, pointers\n\n\n    def decoder_cell(self):\n        if self.copynet:\n            input_size = self.dim * 2\n        else:\n            input_size = self.dim\n        with tf.compat.v1.variable_scope(self.scope + ""_decoder_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(\n                self.rnn_cell, scope, self.dim, self.num_layers,\n                self.input_keep, self.output_keep,\n                variational_recurrent=self.variational_recurrent_dropout,\n                input_dim=input_size)\n        return cell\n'"
encoder_decoder/seq2seq/seq2seq_model.py,0,"b'""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom encoder_decoder import encoder\nfrom encoder_decoder.framework import EncoderDecoderModel\nfrom . import rnn_decoder\n\n\nclass Seq2SeqModel(EncoderDecoderModel):\n    """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n    This class implements a multi-layer recurrent neural network as encoder,\n    and an attention-based decoder. This is the same as the model described in\n    this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n    or into the seq2seq library for complete model implementation.\n    This class also allows to use GRU cells in addition to LSTM cells, and\n    sampled softmax to handle large output vocabulary size. A single-layer\n    version of this model, but with bi-directional encoder, was presented in\n      http://arxiv.org/abs/1409.0473\n    and sampled softmax is described in Section 3 of the following paper.\n      http://arxiv.org/abs/1412.2007\n    """"""\n\n    def __init__(self, hyperparams, buckets=None):\n        super(Seq2SeqModel, self).__init__(hyperparams, buckets)\n\n\n    def define_encoder(self, input_keep, output_keep):\n        """"""\n        Construct sequence encoder.\n        """"""\n        if self.encoder_topology == ""rnn"":\n            self.encoder = encoder.RNNEncoder(\n                self.hyperparams, input_keep, output_keep)\n        elif self.encoder_topology == ""birnn"":\n            self.encoder = encoder.BiRNNEncoder(\n                self.hyperparams, input_keep, output_keep)\n        else:\n            raise ValueError(""Unrecognized encoder type."")\n\n\n    def define_decoder(self, dim, embedding_dim, use_attention,\n            attention_function, input_keep, output_keep):\n        """"""\n        Construct sequence decoder.\n        """"""\n        if self.decoder_topology == ""rnn"":\n            self.decoder = rnn_decoder.RNNDecoder(\n                hyperparameters=self.hyperparams,\n                scope=\'token_decoder\', dim=dim,\n                embedding_dim=embedding_dim,\n                use_attention=use_attention,\n                attention_function=attention_function,\n                input_keep=input_keep,\n                output_keep=output_keep,\n                decoding_algorithm=self.token_decoding_algorithm\n            )\n        else:\n            raise ValueError(""Unrecognized decoder topology: {}."".format(\n                self.decoder_topology))\n'"
encoder_decoder/seq2tree/__init__.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Makes helper libraries available in the translate package.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function'"
encoder_decoder/seq2tree/seq2tree_model.py,0,"b'""""""Sequence-to-tree model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom encoder_decoder import encoder\nfrom encoder_decoder.framework import EncoderDecoderModel\nfrom . import tree_decoder\n\nclass Seq2TreeModel(EncoderDecoderModel):\n    """"""Sequence-to-tree models.\n    """"""\n    def __init__(self, hyperparams, buckets=None, forward_only=False):\n        """"""\n        Create the model.\n        :param hyperparams: learning hyperparameters\n        :param buckets: if not None, train bucket model.\n        :param forward_only: if set, we do not construct the backward pass.\n        """"""\n        super(Seq2TreeModel, self).__init__(hyperparams, buckets, forward_only)\n\n\n    def define_encoder(self):\n        """"""Construct sequence encoders.""""""\n        if self.encoder_topology == ""rnn"":\n            self.encoder = encoder.RNNEncoder(self.hyperparams)\n        elif self.encoder_topology == ""birnn"":\n            self.encoder = encoder.BiRNNEncoder(self.hyperparams)\n        else:\n            raise ValueError(""Unrecognized encoder type."")\n\n\n    def define_decoder(self, dim):\n        """"""Construct tree decoders.""""""\n        if self.decoder_topology == ""basic_tree"":\n            self.decoder = tree_decoder.BasicTreeDecoder(\n                self.hyperparams, dim, self.output_project())\n        else:\n            raise ValueError(""Unrecognized decoder topology: {}.""\n                             .format(self.decoder_topology))\n'"
encoder_decoder/seq2tree/tree_decoder.py,75,"b'""""""A set of tree decoder modules used in the encoder-decoder framework.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport os, sys\nif sys.version_info > (3, 0):\n    from six.moves import xrange\n\nimport tensorflow as tf\n\nfrom encoder_decoder import decoder, data_utils, graph_utils\n\nDEBUG = False\n\n\nclass BasicTreeDecoder(decoder.Decoder):\n\n    def __init__(self, hyperparams, dim, output_project=None):\n        super(BasicTreeDecoder, self).__init__(hyperparams, dim, output_project)\n\n        self.H_NO_EXPAND = tf.constant(data_utils.H_NO_EXPAND_ID, shape=[self.batch_size])\n        self.V_NO_EXPAND = tf.constant(data_utils.V_NO_EXPAND_ID, shape=[self.batch_size])\n\n    \n    def define_graph(self, encoder_state, decoder_inputs, embeddings,\n                     attention_states=None, num_heads=1,\n                     initial_state_attention=False, feed_previous=False,\n                     reuse_variables=False):\n        """"""\n        :param encoder_state: hidden state of the encoder\n        :param inputs: placeholders for the discrete inputs of the decoder\n        :param embeddings: target embeddings\n        :param attention_states: 3D Tensor [batch_size x attn_length x attn_dim].\n        :param num_heads: Number of attention heads that read from from attention_states.\n            Dummy field if attention_states is None.\n        :param initial_state_attention: If False (default), initial attentions are zero.\n            If True, initialize the attentions from the initial state and attention states\n            -- useful to resume decoding from a previously stored decoder state and attention\n            state.\n        :param feed_previous: Boolean; if True, only the first of decoder_inputs will be\n            used (the ""ROOT"" symbol), and all other decoder inputs will be generated by:\n            next = embedding_lookup(embedding, argmax(previous_output)),\n            In effect, this implements a greedy decoder. It can also be used\n            during training to emulate http://arxiv.org/abs/1506.03099.\n            If False, decoder_inputs are used as given (the standard decoder case).\n        :param reuse_variables: reuse variables in scope.\n        :return: Output states and the final hidden state of the decoder. Need\n            output_project to obtain distribution over output vocabulary.\n        """"""\n        self.E = tf.constant(np.identity(len(decoder_inputs)), dtype=tf.int32)\n\n        if self.use_attention and \\\n                not attention_states.get_shape()[1:2].is_fully_defined():\n            raise ValueError(""Shape[1] and [2] of attention_states must be known %s""\n                             % attention_states.get_shape())\n\n        with tf.compat.v1.variable_scope(""basic_tree_decoder"") as scope:\n            vertical_cell, vertical_scope = self.vertical_cell()\n            horizontal_cell, horizontal_scope = self.horizontal_cell()\n            outputs = []\n            attn_alignments = []\n\n            # search control\n            self.back_pointers = tf.constant(0, shape=[self.batch_size, 1, 1],\n                                             dtype=tf.int32)\n\n            # continuous stack used for storing LSTM states, synced with\n            # self.back_pointers\n            if self.rnn_cell == ""gru"":\n                init_state = encoder_state\n            else:\n                init_state = tf.concat(axis=1, values=[encoder_state[0], encoder_state[1]])\n\n            if self.use_attention:\n                hidden, hidden_features, v = \\\n                    self.attention_hidden_layer(attention_states, num_heads)\n                batch_size = tf.shape(input=attention_states)[0]\n                attn_dim = tf.shape(input=attention_states)[2]\n                batch_attn_size = tf.stack([batch_size, attn_dim])\n                # initial attention state\n                attns = tf.concat(axis=1, values=[tf.zeros(batch_attn_size, dtype=tf.float32)\n                         for _ in xrange(num_heads)])\n                if initial_state_attention:\n                    attns, attn_alignment = \\\n                        self.attention(v, encoder_state, hidden_features,\n                                       num_heads, hidden)\n                    attn_alignments.append(attn_alignment)\n                init_state = tf.concat(axis=1, values=[init_state] + [attns])\n            self.state = tf.expand_dims(init_state, 1)\n            self.input = tf.expand_dims(decoder_inputs[0], 1)\n            self.input = tf.expand_dims(self.input, 1)\n            self.input.set_shape([self.batch_size, 1, 1])\n            search_left_to_right_next = self.is_no_expand(self.input[:, -1, 0])\n            \n            for i in xrange(len(decoder_inputs)):\n                if DEBUG:\n                    print(""decoder step: %d"" % i)\n                if i > 0: tf.compat.v1.get_variable_scope().reuse_variables()\n\n                self.step = i + 1\n                search_left_to_right = search_left_to_right_next\n                \n                if self.use_attention:\n                    input, state, attns = self.peek()\n                else:\n                    input, state = self.peek()\n\n                input_embeddings = tf.squeeze(tf.nn.embedding_lookup(params=embeddings, ids=input),\n                                              axis=[1])\n\n                # compute batch horizontal and vertical steps.\n                if self.use_attention:\n                    v_output, v_state, v_attns, v_attn_alignment = self.attention_cell(\n                        vertical_cell, vertical_scope, input_embeddings, state, attns,\n                        hidden_features, v, num_heads, hidden)\n                    h_output, h_state, h_attns, h_attn_alignment = self.attention_cell(\n                        horizontal_cell, horizontal_scope, input_embeddings, state, attns,\n                        hidden_features, v, num_heads, hidden)\n                else:\n                    v_output, v_state = vertical_cell(input_embeddings, state, vertical_scope)\n                    h_output, h_state = horizontal_cell(input_embeddings, state, horizontal_scope)\n\n                # select horizontal or vertical computation results for each example\n                # based on its own control state.\n                switch_masks = []\n                for j in xrange(self.batch_size):\n                    mask = tf.cond(pred=search_left_to_right[j], true_fn=lambda: tf.constant([[1, 0]]),\n                                                            false_fn=lambda: tf.constant([[0, 1]]))\n                    switch_masks.append(mask)\n                switch_mask = tf.concat(axis=0, values=switch_masks)\n\n                batch_output = switch_mask(switch_mask, [h_output, v_output])\n                if self.rnn_cell == ""gru"":\n                    batch_state = switch_mask(switch_mask, [h_state, v_state])\n                elif self.rnn_cell == ""lstm"":\n                    batch_cell = switch_mask(switch_mask, [h_state[0], v_state[0]])\n                    batch_hs = switch_mask(switch_mask, [h_state[1], v_state[1]])\n                    batch_state = tf.concat(axis=1, values=[batch_cell, batch_hs])\n\n                if self.use_attention:\n                    batch_attns = switch_mask(switch_mask, [h_attns, v_attns])\n                    batch_state = tf.concat(axis=1, values=[batch_state, batch_attns])\n\n                    batch_attn_alignment = h_attn_alignment\n                    attn_alignments.append(batch_attn_alignment)\n                \n                # record output state to compute the loss.\n                outputs.append(batch_output)\n\n                if i < len(decoder_inputs) - 1:\n                    # storing states\n                    if feed_previous:\n                        # Project decoder output for next state input.\n                        W, b = self.output_project\n                        batch_projected_output = tf.matmul(batch_output, W) + b\n                        batch_output_symbol = tf.argmax(input=batch_projected_output, axis=1)\n                        batch_output_symbol = tf.cast(batch_output_symbol, dtype=tf.int32)\n                    else:\n                        batch_output_symbol = decoder_inputs[i+1]\n                    search_left_to_right_next = self.is_no_expand(batch_output_symbol)\n\n                    back_pointer = map_fn(\n                        self.back_pointer, [search_left_to_right_next,\n                                            search_left_to_right,\n                                            self.grandparent(),\n                                            self.parent(),\n                                            tf.constant(i, shape=[self.batch_size], dtype=tf.int32)],\n                        self.batch_size)\n                    back_pointer.set_shape([self.batch_size])\n                    if DEBUG:\n                        print(""back_pointer.get_shape(): {}"".format(back_pointer.get_shape()))\n\n                    next_input = map_fn(\n                        self.next_input, [search_left_to_right_next,\n                                          search_left_to_right,\n                                          self.parent_input(),\n                                          self.get_input(),\n                                          batch_output_symbol],\n                        self.batch_size)\n                    next_input.set_shape([self.batch_size])\n                    if DEBUG:\n                        print(""next_input.get_shape(): {}"".format(next_input.get_shape()))\n\n                    next_state = map_fn(\n                        self.next_state, [search_left_to_right_next,\n                                          search_left_to_right,\n                                          self.parent_state(),\n                                          self.get_state(),\n                                          batch_state],\n                        self.batch_size)\n                    if DEBUG:\n                        print(""next_state.get_shape(): {}"".format(next_state.get_shape()))\n\n                    self.push([next_input, back_pointer, next_state])\n\n        if self.rnn_cell == ""gru"":\n            final_batch_state = batch_state\n        elif self.rnn_cell == ""lstm"":\n            final_batch_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(batch_cell, batch_hs)\n\n        if self.use_attention:\n            temp = [tf.expand_dims(batch_attn_alignment, 1) for batch_attn_alignment in attn_alignments]\n            return outputs, final_batch_state, tf.concat(axis=1, values=temp)\n        else:\n            return outputs, final_batch_state\n\n\n    def back_pointer(self, x):\n        h_search_next, h_search, grandparent, parent, current = x\n        return tf.cond(pred=h_search_next,\n                       true_fn=lambda : tf.cond(pred=h_search[0], true_fn=lambda : grandparent, false_fn=lambda : parent),\n                       false_fn=lambda : tf.cond(pred=h_search[0], true_fn=lambda : parent, false_fn=lambda : current))\n\n    def next_input(self, x):\n        h_search_next, h_search, parent, current, next = x\n        return tf.cond(pred=h_search_next,\n                       true_fn=lambda : tf.cond(pred=h_search[0], true_fn=lambda : parent, false_fn=lambda : current),\n                       false_fn=lambda : next)\n\n    def next_state(self, x):\n        h_search_next, h_search, parent, current, next = x\n        return tf.cond(pred=h_search_next,\n                       true_fn=lambda : tf.cond(pred=h_search[0], true_fn=lambda : parent, false_fn=lambda : current),\n                       false_fn=lambda : next)\n\n    def parent_input(self):\n        inds = tf.nn.embedding_lookup(params=self.E, ids=tf.split(axis=0, num_or_size_splits=self.batch_size, value=self.parent()))\n        inds = tf.squeeze(inds, axis=[1])\n        inds = inds[:, :self.step]\n        return tf.reduce_sum(input_tensor=tf.multiply(self.input[:, :, 0], inds), axis=1)\n\n\n    def parent_state(self):\n        inds = tf.nn.embedding_lookup(params=self.E, ids=tf.split(axis=0, num_or_size_splits=self.batch_size, value=self.parent()))\n        inds = tf.squeeze(inds, axis=[1])\n        inds = inds[:, :self.step]\n        inds = tf.expand_dims(inds, 2)\n        inds = tf.tile(inds, tf.stack([tf.constant(1), tf.constant(1), tf.shape(input=self.state)[2]]))\n        return tf.reduce_sum(input_tensor=tf.multiply(self.state, tf.cast(inds, tf.float32)), axis=1)\n\n    def grandparent(self):\n        inds = tf.nn.embedding_lookup(params=self.E, ids=tf.split(axis=0, num_or_size_splits=self.batch_size, value=self.parent()))\n        inds = tf.squeeze(inds, axis=[1])\n        inds = inds[:, :self.step]\n        return tf.reduce_sum(input_tensor=tf.multiply(self.back_pointers[:, :, 0], inds), axis=1)\n\n    def parent(self):\n        p = self.back_pointers[:, -1, 0]\n        return p\n\n    def get_input(self):\n        return self.input[:, -1, 0]\n\n    def get_state(self):\n        return self.state[:, -1, :]\n\n    def push(self, batch_states):\n        """"""\n        :param batch_states: list of list of state tensors\n        """"""\n        batch_next_input = batch_states[0]\n        batch_next_input = tf.expand_dims(batch_next_input, 1)\n        batch_next_input = tf.expand_dims(batch_next_input, 1)\n        self.input = tf.concat(axis=1, values=[self.input, batch_next_input])\n\n        batch_back_pointers = batch_states[1]\n        batch_back_pointers = tf.expand_dims(batch_back_pointers, 1)\n        batch_back_pointers = tf.expand_dims(batch_back_pointers, 1)\n        self.back_pointers = tf.concat(axis=1, values=[self.back_pointers, batch_back_pointers])\n\n        batch_states = tf.expand_dims(batch_states[2], 1)\n        self.state = tf.concat(axis=1, values=[self.state, batch_states])\n\n\n    def peek(self):\n        """"""\n        :param batch_indices: list of stack pointers for each search thread\n        :return: batch stack state tuples\n                 (batch_parent_states, [batch_attention_states])\n        """"""\n        batch_input_symbols = self.input[:, -1, :]\n        batch_stack_states = self.state[:, -1, :]\n\n        if self.rnn_cell == ""gru"":\n            batch_states = batch_stack_states[:, :self.dim]\n            attn_start_pos = self.dim\n            batch_states.set_shape([self.batch_size, self.dim])\n        elif self.rnn_cell == ""lstm"":\n            batch_stack_cells = batch_stack_states[:, :self.dim]\n            batch_stack_hiddens = batch_stack_states[:, self.dim:2*self.dim]\n            attn_start_pos = 2 * self.dim\n            batch_states = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(batch_stack_cells, batch_stack_hiddens)\n        else:\n            raise ValueError(""Unrecognized RNN cell type."")\n\n        if self.use_attention:\n            batch_attention_states = batch_stack_states[:, attn_start_pos:]\n            return batch_input_symbols, batch_states, batch_attention_states\n        else:\n            return batch_input_symbols, batch_states\n\n\n    def is_no_expand(self, ind):\n        return tf.logical_or(self.no_vertical_expand(ind), self.no_horizontal_expand(ind))\n\n\n    def no_vertical_expand(self, ind):\n        return tf.equal(tf.cast(ind, tf.int32), self.V_NO_EXPAND)\n\n\n    def no_horizontal_expand(self, ind):\n        return tf.equal(tf.cast(ind, tf.int32), self.H_NO_EXPAND)\n\n\n    def vertical_cell(self):\n        """"""Cell that controls transition from parent to child.""""""\n        with tf.compat.v1.variable_scope(""vertical_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(self.rnn_cell, scope,\n                                                      self.dim, self.num_layers,\n                                                      self.tg_input_keep,\n                                                      self.tg_output_keep)\n        return cell, scope\n\n\n    def horizontal_cell(self):\n        """"""Cell that controls transition from left sibling to right sibling.""""""\n        with tf.compat.v1.variable_scope(""horizontal_cell"") as scope:\n            cell = graph_utils.create_multilayer_cell(self.rnn_cell, scope,\n                                                      self.dim, self.num_layers,\n                                                      self.tg_input_keep,\n                                                      self.tg_output_keep)\n        return cell, scope\n\n\ndef switch_mask(mask, candidates):\n    """"""\n    :param mask: A 2D binary matrix of size [batch_size, num_options].\n                 Each row of mask has exactly one non-zero entry.\n    :param candidates: A list of 2D matrices with length num_options.\n    :return: selections concatenated as a new batch.\n    """"""\n    assert(len(candidates) > 1)\n    threed_mask = tf.tile(tf.expand_dims(mask, 2),\n                          [1, 1, candidates[0].get_shape()[1].value])\n    threed_mask = tf.cast(threed_mask, candidates[0].dtype)\n    expanded_candidates = [tf.expand_dims(c, 1) for c in candidates]\n    candidate = tf.concat(axis=1, values=expanded_candidates)\n    return tf.reduce_sum(input_tensor=tf.multiply(threed_mask, candidate), axis=1)\n\n\ndef map_fn(fn, elems, batch_size):\n    """"""Pesudo multi-ariti scan.""""""\n    results = []\n    elem_lists = [tf.split(axis=0, num_or_size_splits=batch_size, value=elem) for elem in elems]\n    for i in xrange(batch_size):\n        args = [tf.squeeze(elem_lists[0][i], axis=[0])] + \\\n               [elem_list[i] for elem_list in elem_lists[1:]]\n        results.append(fn(args))\n    _results = tf.concat(axis=0, values=results)\n    return _results\n\n\nif __name__ == ""__main__"":\n    decoder = BasicTreeDecoder(dim=100, batch_size=1, rnn_cell=""gru"", num_layers=1,\n                               input_keep_prob=1, output_keep_prob=1,\n                               use_attention=False, use_copy=False, output_project=None)\n    decoder_inputs = [tf.compat.v1.placeholder(dtype=tf.int32, shape=[None],\n                                     name=""decoder{0}"".format(i)) for i in xrange(14)]\n    encoder_state = tf.random.normal(shape=[1, 100])\n    attention_states = tf.random.normal(shape=[8, 100])\n    target_embeddings = tf.random.normal(shape=[200, 100])\n    outputs, state, left_to_right = decoder.define_graph(encoder_state,\n                                                         decoder_inputs,\n                                                         target_embeddings,\n                                                         attention_states,\n                                                         feed_previous=False)\n\n    with tf.compat.v1.Session() as sess:\n        sess.run(tf.compat.v1.global_variables_initializer())\n        input_feed = {}\n        inputs = [[9], [10], [21], [7], [53], [105], [7], [6], [32], [51], [7], [6], [6], [6]]\n        for l in xrange(14):\n            input_feed[decoder_inputs[l].name] = inputs[l]\n        input_feed[encoder_state.name] = np.random.rand(1, 100)\n        output_feed = [state, left_to_right]\n        state = sess.run(output_feed, input_feed)\n        print(state[0])\n        print(state[1])\n'"
encoder_decoder/seq2tree/tree_rnn_cell.py,0,"b'""""""\nA class of neural tree search algorithms in Tensorflow.\n""""""\n\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import variable_scope as vs\n\nfrom encoder_decoder import rnn\n\nclass TreeRNNCell(object):\n    """"""Abstract object representing a tree-RNN cell.\n\n    self.input_size = D\n    self.state_size = H\n    self.output_size = N\n    A tree RNN cell has a state and performs some operation that takes a matrix\n    of inputs (batch_size x D). This operation results in an output matrix (batch_size x N).\n\n    This module is a common interface to implementations of a number of commonly\n    used recurrent architecture, such as LSTM (Long Short Term Memory) or GRU (\n    Gated Recurrent Unit), and a number of operators that allow dropouts, projections, or\n    embeddings for inputs.\n    """"""\n\n    def __call__(self, inputs, parent_state, cyc_state, scope=None):\n        """"""\n        :param inputs: `2-D` tensor with shape `[batch_size x D]`.\n        :param parent_state: hidden_state of parent cell,\n            `2-D` tensor with shape `[batch_size x H]`\n        :param cyc_state: hidden state of current youngest child cell,\n            `2-D` tensor with shape `[batch_size x H]`\n        :param scope: VariableScope for the created subgraph; defaults to class name.\n\n        :return:\n            A pair containing:\n            - output: A `2-D` tensor with shape `[batch_size x N]`\n            - new state: A `2-D` tensor with shape `[batch_size x H]`.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def state_size(self):\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self):\n        raise NotImplementedError(""Abstract method"")\n\n    def zero_state(self, batch_size, dtype):\n        """"""Return zero-filled state tensor(s).\n\n        Args:\n          batch_size: int, float, or unit Tensor representing the batch size.\n          dtype: the data type to use for the state.\n\n        Returns:\n          If `state_size` is an int, then the return value is a `2-D` tensor of\n          shape `[batch_size x state_size]` filled with zeros.\n\n          If `state_size` is a nested list or tuple, then the return value is\n          a nested list or tuple (of the same structure) of `2-D` tensors with\n        the shapes `[batch_size x s]` for each s in `state_size`.\n        """"""\n        state_size = self.state_size\n        state_size_flat = rnn_cell._unpacked_state(state_size)\n        zeros_flat = [\n            array_ops.zeros(array_ops.pack([batch_size, s]), dtype=dtype)\n            for s in state_size_flat]\n        for s, z in zip(state_size_flat, zeros_flat):\n            z.set_shape([None, s])\n        zeros = rnn_cell._packed_state(structure=state_size, state=zeros_flat)\n\n        return zeros\n\nclass BasicTreeLSTMCell(TreeRNNCell):\n    """"""Basic tree LSTM recurrent neural network cell.\n\n       The implementation is based on BasicLSTMCell in rnn_cell.py.\n    """"""\n    def __init__(self, num_units, forget_bias=1.0, activation=False):\n        """"""\n        Initialize the basic tree LSTM cell.\n\n        :param num_units: The number of units in the tree LSTM cell.\n                          = input_size\n                          = output_size\n                          = state_size\n        :param forget_bias: float, The bias added to forget gates.\n        :param activation: Activation function of the inner states.\n        """"""\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._activation = activation\n\n    @property\n    def state_size(self):\n        return rnn_cell.LSTMStateTuple(self._num_units, self._num_units)\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def __call__(self, inputs, parent_state, cyc_state, scope=None):\n        """"""Modified Long short-term memory for tree structure""""""\n        with vs.variable_scope(scope or type(self).__name__):   # ""BasicTreeLSTMCell""\n            # parameters of gates are concatenated into one multiply for efficiency\n            parent_c, parent_h = parent_state\n            cyc_c, cyc_h = cyc_state\n            c = rnn.linear([parent_c, cyc_c], self._num_units, True)\n            concat = rnn.linear([inputs, parent_h, cyc_h], 4 * self._num_units, True)\n\n            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n            i, j, f, o = array_ops.split(1, 4, concat)\n\n            new_c = [c * rnn_cell.sigmoid(f + self._forget_bias)\n                     + rnn_cell.sigmoid(i) * self._activation(j)]\n            new_h = self._activation(new_c) * rnn_cell.sigmoid(o)\n\n            new_state = rnn_cell.LSTMStateTuple(new_c, new_h)\n\n            return new_h, new_state\n\n'"
eval/zss/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .compare import (\n    distance,\n    simple_distance,\n)\nfrom .simple_tree import Node\n\n__all__ = ['distance', 'simple_distance', 'Node']\n__version__ = '1.1.4'\n\n"""
eval/zss/compare.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Authors: Tim Henderson and Steve Johnson\n#Email: tim.tadh@gmail.com, steve@steveasleep.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\nfrom six.moves import range\n\nimport collections\n\ntry:\n    import numpy as np\n    zeros = np.zeros\nexcept ImportError:\n    def py_zeros(dim, pytype):\n        assert len(dim) == 2\n        return [[pytype() for y in range(dim[1])]\n                for x in range(dim[0])]\n    zeros = py_zeros\n\ntry:\n    from editdist import distance as strdist\nexcept ImportError:\n    def strdist(a, b):\n        if a == b:\n            return 0\n        else:\n            return 1\n\nfrom eval.zss.simple_tree import Node\n\n\nclass AnnotatedTree(object):\n\n    def __init__(self, root, get_children):\n        self.get_children = get_children\n\n        self.root = root\n        self.nodes = list()  # a post-order enumeration of the nodes in the tree\n        self.ids = list()    # a matching list of ids\n        self.lmds = list()   # left most descendents\n        self.keyroots = None\n            # k and k\' are nodes specified in the post-order enumeration.\n            # keyroots = {k | there exists no k\'>k such that lmd(k) == lmd(k\')}\n            # see paper for more on keyroots\n\n        stack = list()\n        pstack = list()\n        stack.append((root, collections.deque()))\n        j = 0\n        while len(stack) > 0:\n            n, anc = stack.pop()\n            nid = j\n            for c in self.get_children(n):\n                a = collections.deque(anc)\n                a.appendleft(nid)\n                stack.append((c, a))\n            pstack.append(((n, nid), anc))\n            j += 1\n        lmds = dict()\n        keyroots = dict()\n        i = 0\n        while len(pstack) > 0:\n            (n, nid), anc = pstack.pop()\n            #print list(anc)\n            self.nodes.append(n)\n            self.ids.append(nid)\n            #print n.label, [a.label for a in anc]\n            if not self.get_children(n):\n                lmd = i\n                for a in anc:\n                    if a not in lmds: lmds[a] = i\n                    else: break\n            else:\n                try: lmd = lmds[nid]\n                except:\n                    import pdb\n                    pdb.set_trace()\n            self.lmds.append(lmd)\n            keyroots[lmd] = i\n            i += 1\n        self.keyroots = sorted(keyroots.values())\n\n\ndef simple_distance(A, B, get_children=Node.get_children,\n        get_label=Node.get_label, label_dist=strdist):\n    """"""Computes the exact tree edit distance between trees A and B.\n\n    Use this function if both of these things are true:\n\n    * The cost to insert a node is equivalent to ``label_dist(\'\', new_label)``\n    * The cost to remove a node is equivalent to ``label_dist(new_label, \'\')``\n\n    Otherwise, use :py:func:`zss.distance` instead.\n\n    :param A: The root of a tree.\n    :param B: The root of a tree.\n\n    :param get_children:\n        A function ``get_children(node) == [node children]``.  Defaults to\n        :py:func:`zss.Node.get_children`.\n\n    :param get_label:\n        A function ``get_label(node) == \'node label\'``.All labels are assumed\n        to be strings at this time. Defaults to :py:func:`zss.Node.get_label`.\n\n    :param label_distance:\n        A function\n        ``label_distance((get_label(node1), get_label(node2)) >= 0``.\n        This function should take the output of ``get_label(node)`` and return\n        an integer greater or equal to 0 representing how many edits to\n        transform the label of ``node1`` into the label of ``node2``. By\n        default, this is string edit distance (if available). 0 indicates that\n        the labels are the same. A number N represent it takes N changes to\n        transform one label into the other.\n\n    :return: An integer distance [0, inf+)\n    """"""\n    return distance(\n        A, B, get_children,\n        insert_cost=lambda node: label_dist(\'\', get_label(node)),\n        remove_cost=lambda node: label_dist(get_label(node), \'\'),\n        update_cost=lambda a, b: label_dist(get_label(a), get_label(b)),\n    )\n\n\ndef distance(A, B, get_children, insert_cost, remove_cost, update_cost):\n    \'\'\'Computes the exact tree edit distance between trees A and B with a\n    richer API than :py:func:`zss.simple_distance`.\n\n    Use this function if either of these things are true:\n\n    * The cost to insert a node is **not** equivalent to the cost of changing\n      an empty node to have the new node\'s label\n    * The cost to remove a node is **not** equivalent to the cost of changing\n      it to a node with an empty label\n\n    Otherwise, use :py:func:`zss.simple_distance`.\n\n    :param A: The root of a tree.\n    :param B: The root of a tree.\n\n    :param get_children:\n        A function ``get_children(node) == [node children]``.  Defaults to\n        :py:func:`zss.Node.get_children`.\n\n    :param insert_cost:\n        A function ``insert_cost(node) == cost to insert node >= 0``.\n\n    :param remove_cost:\n        A function ``remove_cost(node) == cost to remove node >= 0``.\n\n    :param update_cost:\n        A function ``update_cost(a, b) == cost to change a into b >= 0``.\n\n    :return: An integer distance [0, inf+)\n    \'\'\'\n    A, B = AnnotatedTree(A, get_children), AnnotatedTree(B, get_children)\n    treedists = zeros((len(A.nodes), len(B.nodes)), int)\n\n    def treedist(i, j):\n        Al = A.lmds\n        Bl = B.lmds\n        An = A.nodes\n        Bn = B.nodes\n\n        m = i - Al[i] + 2\n        n = j - Bl[j] + 2\n        fd = zeros((m,n), int)\n\n        ioff = Al[i] - 1\n        joff = Bl[j] - 1\n\n        for x in range(1, m): # \xce\xb4(l(i1)..i, \xce\xb8) = \xce\xb4(l(1i)..1-1, \xce\xb8) + \xce\xb3(v \xe2\x86\x92 \xce\xbb)\n            fd[x][0] = fd[x-1][0] + remove_cost(An[x+ioff])\n        for y in range(1, n): # \xce\xb4(\xce\xb8, l(j1)..j) = \xce\xb4(\xce\xb8, l(j1)..j-1) + \xce\xb3(\xce\xbb \xe2\x86\x92 w)\n            fd[0][y] = fd[0][y-1] + insert_cost(Bn[y+joff])\n\n        for x in range(1, m): ## the plus one is for the xrange impl\n            for y in range(1, n):\n                # only need to check if x is an ancestor of i\n                # and y is an ancestor of j\n                if Al[i] == Al[x+ioff] and Bl[j] == Bl[y+joff]:\n                    #                   +-\n                    #                   | \xce\xb4(l(i1)..i-1, l(j1)..j) + \xce\xb3(v \xe2\x86\x92 \xce\xbb)\n                    # \xce\xb4(F1 , F2 ) = min-+ \xce\xb4(l(i1)..i , l(j1)..j-1) + \xce\xb3(\xce\xbb \xe2\x86\x92 w)\n                    #                   | \xce\xb4(l(i1)..i-1, l(j1)..j-1) + \xce\xb3(v \xe2\x86\x92 w)\n                    #                   +-\n                    fd[x][y] = min(\n                        fd[x-1][y] + remove_cost(An[x+ioff]),\n                        fd[x][y-1] + insert_cost(Bn[y+joff]),\n                        fd[x-1][y-1] + update_cost(An[x+ioff], Bn[y+joff]),\n                    )\n                    treedists[x+ioff][y+joff] = fd[x][y]\n                else:\n                    #                   +-\n                    #                   | \xce\xb4(l(i1)..i-1, l(j1)..j) + \xce\xb3(v \xe2\x86\x92 \xce\xbb)\n                    # \xce\xb4(F1 , F2 ) = min-+ \xce\xb4(l(i1)..i , l(j1)..j-1) + \xce\xb3(\xce\xbb \xe2\x86\x92 w)\n                    #                   | \xce\xb4(l(i1)..l(i)-1, l(j1)..l(j)-1)\n                    #                   |                     + treedist(i1,j1)\n                    #                   +-\n                    p = Al[x+ioff]-1-ioff\n                    q = Bl[y+joff]-1-joff\n                    #print (p, q), (len(fd), len(fd[0]))\n                    fd[x][y] = min(\n                        fd[x-1][y] + remove_cost(An[x+ioff]),\n                        fd[x][y-1] + insert_cost(Bn[y+joff]),\n                        fd[p][q] + treedists[x+ioff][y+joff]\n                    )\n\n    for i in A.keyroots:\n        for j in B.keyroots:\n            treedist(i,j)\n\n    return treedists[-1][-1]\n'"
eval/zss/simple_tree.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Author: Tim Henderson\n#Email: tim.tadh@gmail.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\n\nimport collections\n\n\nclass Node(object):\n    """"""\n    A simple node object that can be used to construct trees to be used with\n    :py:func:`zss.distance`.\n\n    Example: ::\n\n        Node(""f"")\n            .addkid(Node(""a"")\n                .addkid(Node(""h""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""l""))))\n            .addkid(Node(""e""))\n    """"""\n\n    def __init__(self, label, children=None):\n        self.label = label\n        self.children = children or list()\n\n    @staticmethod\n    def get_children(node):\n        """"""\n        Default value of ``get_children`` argument of :py:func:`zss.distance`.\n\n        :returns: ``self.children``.\n        """"""\n        return node.children\n\n    @staticmethod\n    def get_label(node):\n        """"""\n        Default value of ``get_label`` argument of :py:func:`zss.distance`.\n\n        :returns: ``self.label``.\n        """"""\n        return node.label\n\n    def addkid(self, node, before=False):\n        """"""\n        Add the given node as a child of this node.\n        """"""\n        if before:  self.children.insert(0, node)\n        else:   self.children.append(node)\n        return self\n\n    def get(self, label):\n        """""":returns: Child with the given label.""""""\n        if self.label == label: return self\n        for c in self.children:\n            if label in c: return c.get(label)\n\n    def iter(self):\n        """"""Iterate over this node and its children in a preorder traversal.""""""\n        queue = collections.deque()\n        queue.append(self)\n        while len(queue) > 0:\n            n = queue.popleft()\n            for c in n.children: queue.append(c)\n            yield n\n\n    def __contains__(self, b):\n        if isinstance(b, str) and self.label == b: return 1\n        elif not isinstance(b, str) and self.label == b.label: return 1\n        elif (isinstance(b, str) and self.label != b) or self.label != b.label:\n            return sum(b in c for c in self.children)\n        raise TypeError(""Object %s is not of type str or Node"" % repr(b))\n\n    def __eq__(self, b):\n        if b is None: return False\n        if not isinstance(b, Node):\n            raise TypeError(""Must compare against type Node"")\n        return self.label == b.label\n\n    def __ne__(self, b):\n        return not self.__eq__(b)\n\n    def __repr__(self):\n        return super(Node, self).__repr__()[:-1] + "" %s>"" % self.label\n\n    def __str__(self):\n        s = ""%d:%s"" % (len(self.children), self.label)\n        s = \'\\n\'.join([s]+[str(c) for c in self.children])\n        return s\n'"
model/scripts/commit_model.py,0,"b'#!/usr/bin/python3\n\nimport os, sys\nimport subprocess\n\ndef strip_quotes(s):\n    assert(len(s) >= 2 and s[0] == \'""\' and s[-1] == \'""\')\n    return s[1:-1]\n\ndef commit_model_files(model_subdir):\n    cpt_path = os.path.join(model_subdir, \'checkpoint\')\n    temp_cpt_path = os.path.join(model_subdir, \'checkpoint.temp\')\n    assert(os.path.exists(cpt_path))\n    model_cpt_file = None\n    with open(temp_cpt_path, \'w\') as o_f:\n        with open(cpt_path) as f:\n            for line in f:\n                path_name, path = line.strip().split(\': \', 1)\n                if path_name == \'model_checkpoint_path\':\n                    model_cpt_file = os.path.basename(\n                        strip_quotes(path))\n                file_name = os.path.basename(strip_quotes(path))\n                o_f.write(\'{}: ""{}""\\n\'.format(path_name, file_name))\n    # subprocess.Popen(\'git add {}\'.format(cpt_path))\n    assert(model_cpt_file)\n\n    bak_cpt_path = os.path.join(model_subdir, \'checkpoint.bak\')\n    os.rename(cpt_path, bak_cpt_path)\n    print()\n    print(\'old checkpoint file saved to {}\'.format(bak_cpt_path))\n    os.rename(temp_cpt_path, cpt_path)\n    print(\'new checkpoint file saved to {}\'.format(cpt_path))\n    print()\n    subprocess.call([\'git\', \'add\', cpt_path])\n    subprocess.call([\'git\', \'add\',\n        os.path.join(model_subdir, \n            \'{}.data-00000-of-00001\'.format(\n                model_cpt_file))])\n    subprocess.call([\'git\', \'add\',\n        os.path.join(model_subdir, \'{}.index\'.format(model_cpt_file))])\n    if model_subdir.endswith(\'normalized\'):\n        subprocess.call([\'git\', \'add\',\n            os.path.join(model_subdir, \'train.mappings.X.Y.npz\')])\n    if os.path.exists(os.path.join(model_subdir, \n            \'predictions.beam_search.100.dev.latest\')):\n        subprocess.call([\'git\', \'add\',\n            os.path.join(model_subdir, \n                \'predictions.beam_search.100.dev.latest\')])\n    subprocess.call([\'git\', \'commit\', \'-m\', \'""check in model {}""\'.format(\n        model_subdir)])\n    subprocess.call([\'git\', \'push\'])\n\nif __name__ == \'__main__\':\n    model_subdir = os.path.join(\n        os.path.dirname(\n            os.path.dirname(\n                os.path.abspath(__file__))), sys.argv[1])\n    commit_model_files(model_subdir)\n\n   \n'"
nlp_tools/spellcheck/__init__.py,0,b''
nlp_tools/spellcheck/spell_check.py,0,"b'""""""Spelling Corrector in Python 3; see http://norvig.com/spell-correct.html\n\nCopyright (c) 2007-2016 Peter Norvig\nMIT license: www.opensource.org/licenses/mit-license.php\n""""""\n\n################ Spelling Corrector\n\nimport os, re, collections\nfrom collections import Counter\n\n\ncurrent_folder = os.path.dirname(__file__)\n\ndef words(text): return re.findall(r\'\\w+\', text.lower())\n\nWORDS = collections.defaultdict(int)\nif os.path.exists(os.path.join(current_folder, ""most_common.txt"")):\n    with open(os.path.join(current_folder, \'most_common.txt\')) as f:\n        for line in f:\n            word, freq = line.strip().split(\'\\t\')\n            WORDS[word] = int(freq)\n\ndef extract_top_frequent_words(input, top_k):\n    _words = Counter(words(open(input).read()))\n    WORDS = _words.most_common(top_k * 2)\n    with open(os.path.join(current_folder, ""most_common.txt""), \'w\') as o_f:\n        count = 0\n        for word, freq in WORDS:\n            if len(word) <= 2:\n                continue\n            if freq < 20:\n                break\n            o_f.write(""{}\\t{}\\n"".format(word, freq))\n            count += 1\n            if count >= top_k:\n                break\n\ndef P(word, N=sum(WORDS.values())):\n    ""Probability of `word`.""\n    return WORDS[word] / (N+0.0)\n\ndef correction(word):\n    ""Most probable spelling correction for word.""\n    return max(candidates(word), key=P)\n\ndef candidates(word):\n    ""Generate possible spelling corrections for word.""\n    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n\ndef known(words):\n    ""The subset of `words` that appear in the dictionary of WORDS.""\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    ""All edits that are one edit away from `word`.""\n    letters    = \'abcdefghijklmnopqrstuvwxyz\'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word):\n    ""All edits that are two edits away from `word`.""\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\n################ Test Code\n\ndef unit_tests():\n    assert correction(\'speling\') == \'spelling\'              # insert\n    assert correction(\'korrectud\') == \'corrected\'           # replace 2\n    assert correction(\'bycycle\') == \'bicycle\'               # replace\n    assert correction(\'inconvient\') == \'inconvenient\'       # insert 2\n    assert correction(\'arrainged\') == \'arranged\'            # delete\n    assert correction(\'peotry\') ==\'poetry\'                  # transpose\n    assert correction(\'peotryy\') ==\'poetry\'                 # transpose + delete\n    assert correction(\'word\') == \'word\'                     # known\n    assert correction(\'quintessential\') == \'quintessential\' # unknown\n    assert words(\'This is a TEST.\') == [\'this\', \'is\', \'a\', \'test\']\n    assert Counter(words(\'This is a test. 123; A TEST this is.\')) == (\n           Counter({\'123\': 1, \'a\': 2, \'is\': 2, \'test\': 2, \'this\': 2}))\n    assert len(WORDS) == 32192\n    assert sum(WORDS.values()) == 1115504\n    assert WORDS.most_common(10) == [\n     (\'the\', 79808),\n     (\'of\', 40024),\n     (\'and\', 38311),\n     (\'to\', 28765),\n     (\'in\', 22020),\n     (\'a\', 21124),\n     (\'that\', 12512),\n     (\'he\', 12401),\n     (\'was\', 11410),\n     (\'it\', 10681)]\n    assert WORDS[\'the\'] == 79808\n    assert P(\'quintessential\') == 0\n    assert 0.07 < P(\'the\') < 0.08\n    return \'unit_tests pass\'\n\ndef spelltest(tests, verbose=False):\n    ""Run correction(wrong) on all (right, wrong) pairs; report results.""\n    import time\n    start = time.clock()\n    good, unknown = 0, 0\n    n = len(tests)\n    for right, wrong in tests:\n        w = correction(wrong)\n        good += (w == right)\n        if w != right:\n            unknown += (right not in WORDS)\n            if verbose:\n                print(\'correction({}) => {} ({}); expected {} ({})\'\n                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n    dt = time.clock() - start\n    print(\'{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second \'\n          .format(good / n, n, unknown / n, n / dt))\n\ndef Testset(lines):\n    ""Parse \'right: wrong1 wrong2\' lines into [(\'right\', \'wrong1\'), (\'right\', \'wrong2\')] pairs.""\n    return [(right, wrong)\n            for (right, wrongs) in (line.split(\':\') for line in lines)\n            for wrong in wrongs.split()]\n\n\nif __name__ == ""__main__"":\n    if not os.path.exists(os.path.join(current_folder, ""most_common.txt"")):\n        # extract most common words from text file\n        extract_top_frequent_words(os.path.join(current_folder, ""html.txt""), 30000)\n\n    while True:\n        try:\n            word = raw_input(""> "")\n            print(correction(word))\n        except EOFError as ex:\n            break\n\n'"
data/bash/manual_judgements/inter_annotator_agreement.py,0,"b'""""""\nCompute the inter-annotator agreement.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport os\nimport sys\n\nfrom bashlint import data_tools\nfrom eval.eval_tools import load_cached_evaluations_from_file\nfrom eval.eval_tools import get_example_nl_key, get_example_cm_key\nfrom eval.eval_tools import normalize_judgement\n\ndef iaa(a1, a2):\n    assert(len(a1) == len(a2))\n    num_agree = 0\n    for i in range(len(a1)):\n        if a1[i].lower() == a2[i].lower():\n            num_agree += 1\n    return float(num_agree) / len(a1)\n\ndef read_annotations(input_file):\n    command_judgements, template_judgements = [], []\n    with open(input_file) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            command_eval = normalize_judgement(row[\'correct command\'].strip())\n            template_eval = normalize_judgement(row[\'correct template\'].strip())\n            command_judgements.append(command_eval)\n            template_judgements.append(template_eval)\n    return command_judgements, template_judgements\n\ndef inter_annotator_agreement(input_files1, input_files2):\n    command_judgements1, template_judgements1 = [], []\n    command_judgements2, template_judgements2 = [], []\n    for input_file in input_files1:\n        cj, tj = read_annotations(input_file)\n        command_judgements1.extend(cj)\n        template_judgements1.extend(tj)\n    for input_file in input_files2:\n        cj, tj = read_annotations(input_file)\n        command_judgements2.extend(cj)\n        template_judgements2.extend(tj)\n    print(\'IAA-F: {}\'.format(iaa(command_judgements1, command_judgements2)))\n    print(\'IAA-T: {}\'.format(iaa(template_judgements1, template_judgements2)))\n\ndef combine_annotations_multi_files():\n    """"""\n    Combine multiple annotations files and discard the annotations that has a conflict.\n    """"""\n\n    input_dir = sys.argv[1]\n\n    template_evals = {}\n    command_evals = {}\n    discarded_keys = set({})\n\n    for in_csv in os.listdir(input_dir):\n        in_csv_path = os.path.join(input_dir, in_csv)\n        with open(in_csv_path) as f:\n            reader = csv.DictReader(f)\n            current_description = \'\'\n            for row in reader:\n                template_eval = normalize_judgement(row[\'correct template\'])\n                command_eval = normalize_judgement(row[\'correct command\'])\n                description = get_example_nl_key(row[\'description\'])\n                if description.strip():\n                    current_description = description\n                else:\n                    description = current_description\n                prediction = row[\'prediction\']\n                example_key = \'{}<NL_PREDICTION>{}\'.format(description, prediction)\n                if example_key in template_evals and template_evals[example_key] != template_eval:\n                    discarded_keys.add(example_key)\n                    continue\n                if example_key in command_evals and command_evals[example_key] != command_eval:\n                    discarded_keys.add(example_key)\n                    continue\n                template_evals[example_key] = template_eval\n                command_evals[example_key] = command_eval\n            print(\'{} read ({} manually annotated examples, {} discarded)\'.format(in_csv_path, len(template_evals), len(discarded_keys)))\n\n    # Write to new file\n    assert(len(template_evals) == len(command_evals))\n    with open(\'manual_annotations.additional\', \'w\') as o_f:\n        o_f.write(\'description,prediction,template,correct template,correct comand\\n\')\n        for key in sorted(template_evals.keys()):\n            if key in discarded_keys:\n                continue\n            description, prediction = key.split(\'<NL_PREDICTION>\')\n            template_eval = template_evals[example_key]\n            command_eval = command_evals[example_key]\n            pred_tree = data_tools.bash_parser(prediction)\n            pred_temp = data_tools.ast2template(pred_tree, loose_constraints=True)\n            o_f.write(\'""{}"",""{}"",""{}"",{},{}\\n\'.format(\n                description.replace(\'""\', \'""""\'),\n                prediction.replace(\'""\', \'""""\'),\n                pred_temp.replace(\'""\', \'""""\'),\n                template_eval,\n                command_eval\n            ))\n\n    \ndef combine_annotations_multi_annotators():\n    """"""\n    Combine the annotations input by three annotators.\n\n    :param input_file1: main annotation file 1.\n    :param input_file2: main annotation file 2 (should contain the same number of\n        lines as input_file1).\n    :param input_file3: supplementary annotation file which contains annotations\n        of lines in input_file1 and input_file2 that contain a disagreement.\n    :param output_file: file that contains the combined annotations.\n    """"""\n    input_file1 = sys.argv[1]\n    input_file2 = sys.argv[2]\n    input_file3 = sys.argv[3]\n    output_file = sys.argv[4]\n    o_f = open(output_file, \'w\')\n    o_f.write(\'description,prediction,template,correct template,correct command,\'\n              \'correct template A,correct command A,\'\n              \'correct template B,correct command B,\'\n              \'correct template C,correct command C\\n\')\n    sup_structure_eval, sup_command_eval = load_cached_evaluations_from_file(\n        input_file3, treat_empty_as_correct=True)\n\n    with open(input_file1) as f1:\n        with open(input_file2) as f2:\n            reader1 = csv.DictReader(f1)\n            reader2 = csv.DictReader(f2)\n            current_desp = \'\'\n            for row1, row2 in zip(reader1, reader2):\n                row1_template_eval = normalize_judgement(row1[\'correct template\'].strip())\n                row1_command_eval = normalize_judgement(row1[\'correct command\'].strip())\n                row2_template_eval = normalize_judgement(row2[\'correct template\'].strip())\n                row2_command_eval = normalize_judgement(row2[\'correct command\'].strip())\n                if row1[\'description\']:\n                    current_desp = row1[\'description\'].strip()\n                sc_key = get_example_nl_key(current_desp)\n                pred_cmd = row1[\'prediction\'].strip()\n                if not pred_cmd:\n                    row1_template_eval, row1_command_eval = \'n\', \'n\'\n                    row2_template_eval, row2_command_eval = \'n\', \'n\'\n                pred_temp = data_tools.cmd2template(pred_cmd, loose_constraints=True)\n                structure_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n                command_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n                row3_template_eval, row3_command_eval = None, None\n                if structure_example_key in sup_structure_eval:\n                    row3_template_eval = sup_structure_eval[structure_example_key]\n                if command_example_key in sup_command_eval:\n                    row3_command_eval = sup_command_eval[command_example_key]\n                if row1_template_eval != row2_template_eval or row1_command_eval != row2_command_eval:\n                    if row1_template_eval != row2_template_eval:\n                        if row3_template_eval is None:\n                            print(structure_example_key)\n                        assert(row3_template_eval is not None)\n                        template_eval = row3_template_eval\n                    else:\n                        template_eval = row1_template_eval\n                    if row1_command_eval != row2_command_eval:\n                        # if row3_command_eval is None:\n                        #     print(command_example_key)\n                        assert(row3_command_eval is not None)\n                        command_eval = row3_command_eval\n                    else:\n                        command_eval = row1_command_eval\n                else:\n                    template_eval = row1_template_eval\n                    command_eval = row1_command_eval\n                if row3_template_eval is None:\n                    row3_template_eval = \'\'\n                if row3_command_eval is None:\n                    row3_command_eval = \'\'\n                o_f.write(\'""{}"",""{}"",""{}"",{},{},{},{},{},{},{},{}\\n\'.format(\n                    current_desp.replace(\'""\', \'""""\'), pred_cmd.replace(\'""\', \'""""\'), pred_temp.replace(\'""\', \'""""\'),\n                    template_eval, command_eval,\n                    row1_template_eval, row1_command_eval,\n                    row2_template_eval, row2_command_eval,\n                    row3_template_eval, row3_command_eval))\n    o_f.close()\n\ndef print_error_analysis_sheet():\n    input_file1 = sys.argv[1]\n    input_file2 = sys.argv[2]\n    input_file3 = sys.argv[3]\n    output_file = sys.argv[4]\n    o_f = open(output_file, \'w\')\n    o_f.write(\'description,model,prediction,correct template,correct command,\'\n              \'correct template A,correct command A,\'\n              \'correct template B,correct command B,\'\n              \'correct template C,correct command C\\n\')\n    sup_structure_eval, sup_command_eval = load_cached_evaluations_from_file(\n        input_file3, treat_empty_as_correct=True)\n    # for key in sup_structure_eval:\n    #     print(key)\n    # print(\'------------------\')\n    with open(input_file1) as f1:\n        with open(input_file2) as f2:\n            reader1 = csv.DictReader(f1)\n            reader2 = csv.DictReader(f2)\n            current_desp = \'\'\n            for row_id, (row1, row2) in enumerate(zip(reader1, reader2)):\n                if row1[\'description\']:\n                    current_desp = row1[\'description\'].strip()\n                model_name = row2[\'model\']\n                if not model_name in [\'partial.token-copynet\', \'tellina\']:\n                    continue\n                if row_id % 3 != 0:\n                    continue\n                row1_template_eval = normalize_judgement(row1[\'correct template\'].strip())\n                row1_command_eval = normalize_judgement(row1[\'correct command\'].strip())\n                row2_template_eval = normalize_judgement(row2[\'correct template\'].strip())\n                row2_command_eval = normalize_judgement(row2[\'correct command\'].strip())\n                sc_key = get_example_nl_key(current_desp)\n                pred_cmd = row1[\'prediction\'].strip()\n                if not pred_cmd:\n                    row1_template_eval, row1_command_eval = \'n\', \'n\'\n                    row2_template_eval, row2_command_eval = \'n\', \'n\'\n                pred_temp = data_tools.cmd2template(pred_cmd, loose_constraints=True)\n                structure_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_temp)\n                command_example_key = \'{}<NL_PREDICTION>{}\'.format(sc_key, pred_cmd)\n                row3_template_eval, row3_command_eval = None, None\n                if structure_example_key in sup_structure_eval:\n                    row3_template_eval = sup_structure_eval[structure_example_key]\n                if command_example_key in sup_command_eval:\n                    row3_command_eval = sup_command_eval[command_example_key]\n                if row1_template_eval != row2_template_eval or row1_command_eval != row2_command_eval:\n                    if row1_template_eval != row2_template_eval:\n                        if row3_template_eval is None:\n                            print(pred_cmd, structure_example_key)\n                        assert (row3_template_eval is not None)\n                        template_eval = row3_template_eval\n                    else:\n                        template_eval = row1_template_eval\n                    if row1_command_eval != row2_command_eval:\n                        # if row3_command_eval is None:\n                        #     print(command_example_key)\n                        assert (row3_command_eval is not None)\n                        command_eval = row3_command_eval\n                    else:\n                        command_eval = row1_command_eval\n                else:\n                    template_eval = row1_template_eval\n                    command_eval = row1_command_eval\n                if row3_template_eval is None:\n                    row3_template_eval = \'\'\n                if row3_command_eval is None:\n                    row3_command_eval = \'\'\n                o_f.write(\'""{}"",""{}"",""{}"",{},{},{},{},{},{},{},{}\\n\'.format(\n                    current_desp.replace(\'""\', \'""""\'), model_name, pred_cmd.replace(\'""\', \'""""\'),\n                    template_eval, command_eval,\n                    row1_template_eval, row1_command_eval,\n                    row2_template_eval, row2_command_eval,\n                    row3_template_eval, row3_command_eval))\n    o_f.close()\n\ndef compute_error_overlap():\n    input_file = sys.argv[1]\n    template_judgements = []\n    command_judgements = []\n    with open(input_file) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            template_eval = row[\'correct template\']\n            command_eval = row[\'correct command\']\n            if row[\'model\'] == \'tellina\':\n                template_judgements.append([template_eval])\n                command_judgements.append([command_eval])\n            else:\n                template_judgements[-1].append(template_eval)\n                command_judgements[-1].append(command_eval)\n    temp_error_hist = [0, 0, 0, 0]\n    for t1, t2 in template_judgements:\n        if t1 == \'y\' and t2 == \'y\':\n            temp_error_hist[0] += 1\n        elif t1 == \'y\' and t2 == \'n\':\n            temp_error_hist[1] += 1\n        elif t1 == \'n\' and t2 == \'y\':\n            temp_error_hist[2] += 1\n        elif t1 == \'n\' and t2 == \'n\':\n            temp_error_hist[3] += 1\n    print(\'Template Judgements:\')\n    print(\'\\ty y: {}\'.format(temp_error_hist[0]))\n    print(\'\\ty n: {}\'.format(temp_error_hist[1]))\n    print(\'\\tn y: {}\'.format(temp_error_hist[2]))\n    print(\'\\tn n: {}\'.format(temp_error_hist[3]))\n    command_error_hist = [0, 0, 0, 0]\n    for c1, c2 in command_judgements:\n        if c1 == \'y\' and c2 == \'y\':\n            command_error_hist[0] += 1\n        elif c1 == \'y\' and c2 == \'n\':\n            command_error_hist[1] += 1\n        elif c1 == \'n\' and c2 == \'y\':\n            command_error_hist[2] += 1\n        elif c1 == \'n\' and c2 == \'n\':\n            command_error_hist[3] += 1\n    print(\'Command Judgements""\')\n    print(\'\\ty y: {}\'.format(command_error_hist[0]))\n    print(\'\\ty n: {}\'.format(command_error_hist[1]))\n    print(\'\\tn y: {}\'.format(command_error_hist[2]))\n    print(\'\\tn n: {}\'.format(command_error_hist[3]))\n\ndef compute_error_category():\n    input_file = sys.argv[1]\n    tellina_error_hist = collections.defaultdict(int)\n    pc_error_hist = collections.defaultdict(int)\n    with open(input_file) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            error_cat = row[\'error category\']\n            if error_cat:\n                if row[\'model\'] == \'tellina\':\n                    tellina_error_hist[error_cat] += 1\n                elif row[\'model\'] == \'partial.token-copynet\':\n                    pc_error_hist[error_cat] += 1\n                else:\n                    raise ValueError\n    print(\'Tellina errors:\')\n    for ec, freq in sorted(tellina_error_hist.items(), key=lambda x:x[1], reverse=True):\n        print(ec, freq)\n    print()\n    print(\'Sub-token CopyNet errors:\')\n    for ec, freq in sorted(pc_error_hist.items(), key=lambda x:x[1], reverse=True):\n        print(ec, freq)\n\ndef export_annotation_differences(input_file1, input_file2, output_file, command_header):\n    o_f = open(output_file, \'w\')\n    o_f.write(\'description,{},correct template A,correct command A,correct template B,correct command B\\n\'.format(\n        command_header))\n    with open(input_file1) as f1:\n        with open(input_file2) as f2:\n            reader1 = csv.DictReader(f1)\n            reader2 = csv.DictReader(f2)\n            current_desp = \'\'\n            desp_written = False\n            for row1, row2 in zip(reader1, reader2):\n                if row1[\'description\']:\n                    current_desp = row1[\'description\']\n                    desp_written = False\n                if not row1[command_header]:\n                    continue\n                row1_template_eval = normalize_judgement(row1[\'correct template\'].strip())\n                row1_command_eval = normalize_judgement(row1[\'correct command\'].strip())\n                row2_template_eval = normalize_judgement(row2[\'correct template\'].strip())\n                row2_command_eval = normalize_judgement(row2[\'correct command\'].strip())\n                if (row1_template_eval != row2_template_eval) or \\\n                        (row1_command_eval != row2_command_eval):\n                    if not desp_written:\n                        o_f.write(\'""{}"",""{}"",{},{},{},{}\\n\'.format(\n                            current_desp.replace(\'""\', \'""""\'), row1[command_header].replace(\'""\', \'""""\'),\n                            row1_template_eval, row1_command_eval, row2_template_eval, row2_command_eval))\n                        desp_written = True\n                    else:\n                        o_f.write(\',""{}"",{},{},{},{}\\n\'.format(row1[command_header].replace(\'""\', \'""""\'),\n                            row1_template_eval, row1_command_eval, row2_template_eval, row2_command_eval))\n    o_f.close()\n\ndef main():\n    # print_error_analysis_sheet()\n    # combine_annotations_multi_annotators()\n    # input_files1 = [\'unreleased_files/manual.evaluations.test.stc.annotator.1.csv\', \'unreleased_files/manual.evaluations.test.tellina.annotator.1.csv\']\n    # input_files2 = [\'unreleased_files/manual.evaluations.test.stc.annotator.2.csv\', \'unreleased_files/manual.evaluations.test.tellina.annotator.2.csv\']\n    # input_files1 = [\'unreleased_files/NL-Cmd Judgement (Hamid) - pc.csv\', \'unreleased_files/NL-Cmd Judgement (Hamid) - tellina.csv\']\n    # input_files2 = [\'unreleased_files/NL-Cmd Judgement (Shridhar) - pc.csv\', \'unreleased_files/NL-Cmd Judgement (Shridhar) - tellina.csv\']\n    # input_files1 = [\'unreleased_files/manual.evaluations.dev.samples.annotator.1.csv\']\n    # input_files2 = [\'unreleased_files/manual.evaluations.dev.samples.annotator.2.csv\']\n    # inter_annotator_agreement(input_files1, input_files2)\n    # compute_error_overlap()\n    # compute_error_category()\n    combine_annotations_multi_files()    \n\nif __name__ == \'__main__\':\n    main()\n'"
eval/zss/tests/__init__.py,0,b''
eval/zss/tests/test_api.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Author: Tim Henderson\n#Email: tim.tadh@gmail.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\n\nfrom zss import (\n    distance,\n    simple_distance,\n    Node,\n)\n\n\ntry:\n    from editdist import distance as strdist\nexcept ImportError:\n    def strdist(a, b):\n        if a == b:\n            return 0\n        else:\n            return 1\n\ndef weird_dist(A, B):\n    return 10 * strdist(A, B)\n\nclass WeirdNode(object):\n\n    def __init__(self, label):\n        self.my_label = label\n        self.my_children = list()\n\n    @staticmethod\n    def get_children(node):\n        return node.my_children\n\n    @staticmethod\n    def get_label(node):\n        return node.my_label\n\n    def addkid(self, node, before=False):\n        if before:  self.my_children.insert(0, node)\n        else:   self.my_children.append(node)\n        return self\n\n\ndef test_paper_tree():\n    Node = WeirdNode\n    A = (\n      Node(""f"")\n        .addkid(Node(""d"")\n          .addkid(Node(""a""))\n          .addkid(Node(""c"")\n            .addkid(Node(""b""))\n          )\n        )\n        .addkid(Node(""e""))\n    )\n    B = (\n      Node(""f"")\n        .addkid(Node(""c"")\n          .addkid(Node(""d"")\n            .addkid(Node(""a""))\n            .addkid(Node(""b""))\n          )\n        )\n        .addkid(Node(""e""))\n    )\n    #print A\n    #print\n    #print B\n    dist = simple_distance(A, B, WeirdNode.get_children, WeirdNode.get_label,\n        weird_dist)\n    assert dist == 20\n\n\ndef test_rich_api():\n    insert_cost = lambda node: 1\n    remove_cost = lambda node: 1\n    small_update_cost = lambda a, b: 1\n    large_update_cost = lambda a, b: 3\n    no_insert_cost = lambda node: 0\n\n    A = Node(\'a\')\n    B = Node(\'b\')\n    # prefer update\n    assert distance(\n        A, B, Node.get_children, insert_cost, remove_cost,\n        small_update_cost) == 1\n    # prefer insert/remove\n    assert distance(\n        A, B, Node.get_children, insert_cost, remove_cost,\n        large_update_cost) == 2\n\n    C = Node(\'a\', [Node(\'x\')])\n    assert (\n        distance(\n            A, C, Node.get_children, insert_cost, remove_cost,\n            small_update_cost) >\n        distance(\n            A, C, Node.get_children, no_insert_cost, remove_cost,\n            small_update_cost)\n    )\n'"
eval/zss/tests/test_compare.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Authors: Tim Henderson and Steve Johnson\n#Email: tim.tadh@gmail.com, steve@steveasleep.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\n\nfrom zss import (\n    compare,\n    Node,\n)\n\n\ndef simple_trees():\n    A = (\n        Node(""f"")\n            .addkid(Node(""d"")\n                .addkid(Node(""a""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n    B = (\n        Node(""f"")\n            .addkid(Node(""c"")\n                .addkid(Node(""d"")\n                    .addkid(Node(""a""))\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n    return A, B\n\ndef test_nodes():\n    A, B = [compare.AnnotatedTree(t, t.get_children) for t in simple_trees()]\n    for i, nid in enumerate(reversed(A.ids)):\n        assert nid == i\n    for i, nid in enumerate(reversed(B.ids)):\n        assert nid == i\n\ndef test_left_most_descendent():\n    A, B = [compare.AnnotatedTree(t, t.get_children) for t in simple_trees()]\n    assert A.lmds[0] == 0\n    assert A.lmds[1] == 1\n    assert A.lmds[2] == 1\n    assert A.lmds[3] == 0\n    assert A.lmds[4] == 4\n    assert A.lmds[5] == 0\n\n    assert B.lmds[0] == 0\n    assert B.lmds[1] == 1\n    assert B.lmds[2] == 0\n    assert B.lmds[3] == 0\n    assert B.lmds[4] == 4\n    assert B.lmds[5] == 0\n\ndef test_keyroots():\n    A, B = [compare.AnnotatedTree(t, t.get_children) for t in simple_trees()]\n    assert A.keyroots == [2, 4, 5]\n    assert B.keyroots == [1, 4, 5]\n'"
eval/zss/tests/test_metricspace.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Author: Tim Henderson\n#Email: tim.tadh@gmail.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\nfrom six.moves import map\nfrom six.moves import range\n\nimport copy\nimport itertools\nimport os\nimport sys\nimport random\nimport unittest\nfrom random import randint, seed, shuffle\n\nfrom zss import (\n    simple_distance,\n    Node,\n)\nfrom zss.compare import strdist\n\nseed(os.urandom(15))\n\nN = 3\n\n\ndef product(*args, **kwds):\n    # product(\'ABCD\', \'xy\') --> Ax Ay Bx By Cx Cy Dx Dy\n    # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111\n    pools = list(map(tuple, args)) * kwds.get(\'repeat\', 1)\n    result = [[]]\n    for pool in pools:\n        result = [x+[y] for x in result for y in pool]\n    for prod in result:\n        yield tuple(prod)\n\n\nif not hasattr(itertools, \'product\'):\n    setattr(itertools, \'product\', product)\n\n\ntree1_nodes = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\']\ndef tree1():\n    return (\n        Node(""f"")\n            .addkid(Node(""d"")\n                .addkid(Node(""a""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n\n\ntree2_nodes = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\']\ndef tree2():\n    return (\n        Node(""a"")\n            .addkid(Node(""c"")\n                .addkid(Node(""d"")\n                    .addkid(Node(""b""))\n                    .addkid(Node(""e""))))\n            .addkid(Node(""f""))\n        )\n\n\ntree3_nodes = [\'a\',\'b\',\'c\',\'d\',\'e\',\'f\']\ndef tree3():\n    return (\n        Node(""a"")\n            .addkid(Node(""d"")\n                .addkid(Node(""f""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n\ntree4_nodes = [\'q\',\'b\',\'c\',\'d\',\'e\',\'f\']\ndef tree4():\n    return (\n        Node(""f"")\n            .addkid(Node(""d"")\n                .addkid(Node(""q""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n\n\ndef randtree(depth=2, alpha=\'abcdefghijklmnopqrstuvwxyz\', repeat=2, width=2):\n    labels = [\'\'.join(x) for x in itertools.product(alpha, repeat=repeat)]\n    shuffle(labels)\n    labels = (x for x in labels)\n    root = Node(""root"")\n    p = [root]\n    c = list()\n    for x in range(depth-1):\n        for y in p:\n            for z in range(randint(1,1+width)):\n                n = Node(next(labels))\n                y.addkid(n)\n                c.append(n)\n        p = c\n        c = list()\n    return root\n\n\nclass TestTestNode(unittest.TestCase):\n\n    def test_contains(self):\n        root = tree1()\n        self.assertTrue(""a"" in root)\n        self.assertTrue(""b"" in root)\n        self.assertTrue(""c"" in root)\n        self.assertTrue(""d"" in root)\n        self.assertTrue(""e"" in root)\n        self.assertTrue(""f"" in root)\n        self.assertFalse(""q"" in root)\n\n    def test_get(self):\n        root = tree1()\n        self.assertEqual(root.get(""a"").label, ""a"")\n        self.assertEqual(root.get(""b"").label, ""b"")\n        self.assertEqual(root.get(""c"").label, ""c"")\n        self.assertEqual(root.get(""d"").label, ""d"")\n        self.assertEqual(root.get(""e"").label, ""e"")\n        self.assertEqual(root.get(""f"").label, ""f"")\n\n        self.assertNotEqual(root.get(""a"").label, ""x"")\n        self.assertNotEqual(root.get(""b"").label, ""x"")\n        self.assertNotEqual(root.get(""c"").label, ""x"")\n        self.assertNotEqual(root.get(""d"").label, ""x"")\n        self.assertNotEqual(root.get(""e"").label, ""x"")\n        self.assertNotEqual(root.get(""f"").label, ""x"")\n\n        self.assertEqual(root.get(""x""), None)\n\n    def test_iter(self):\n        root = tree1()\n        self.assertEqual(list(x.label for x in root.iter()), [\'f\',\'d\',\'e\',\'a\',\'c\',\'b\'])\n\n\nclass TestCompare(unittest.TestCase):\n    def test_distance(self):\n        trees = itertools.product([tree1(), tree2(), tree3(), tree4()], repeat=2)\n        for a,b in trees:\n            ab = simple_distance(a,b)\n            ba = simple_distance(b,a)\n            #print \'-----------------------------\'\n            #print a\n            #print \'------\'\n            #print b\n            #print \'------\'\n            #print ab, ba\n            self.assertEqual(ab,ba)\n            self.assertTrue((ab == 0 and a is b) or a is not b)\n            #break\n        trees = itertools.product([tree1(), tree2(), tree3(), tree4()], repeat=3)\n        for a,b,c in trees:\n            ab = simple_distance(a,b)\n            bc = simple_distance(b,c)\n            ac = simple_distance(a,c)\n            self.assertTrue(ac <= ab + bc)\n            #break\n\n    #def test_randtree(self):\n        #print randtree(5, repeat=3, width=2)\n\n    def test_symmetry(self):\n        trees = itertools.product((randtree(5, repeat=3, width=2) for x in range(N)), repeat=2)\n        for a,b in trees:\n            ab = simple_distance(a,b)\n            ba = simple_distance(b,a)\n            #print \'-----------------------------\'\n            #print ab, ba\n            self.assertEqual(ab, ba)\n\n    def test_nondegenercy(self):\n        trees = itertools.product((randtree(5, repeat=3, width=2) for x in range(N)), repeat=2)\n        for a,b in trees:\n            d = simple_distance(a,b)\n            #print \'-----------------------------\'\n            #print d, a is b\n            self.assertTrue((d == 0 and a is b) or a is not b)\n\n    def test_triangle_inequality(self):\n        trees = itertools.product((randtree(5, repeat=3, width=2) for x in range(N)), (randtree(5, repeat=3, width=2) for x in range(N)), (randtree(5, repeat=3, width=2) for x in range(N)))\n        for a,b,c in trees:\n            #print \'--------------------------------\'\n            ab = simple_distance(a,b)\n            bc = simple_distance(b,c)\n            ac = simple_distance(a,c)\n            #print ab, bc, ac\n            self.assertTrue(ac <= ab + bc)\n\n\n    def test_labelchange(self):\n\n        for A in (randtree(5, repeat=3, width=2) for x in range(N*4)):\n            B = copy.deepcopy(A)\n            node = random.choice([n for n in B.iter()])\n            old_label = str(node.label)\n            node.label = \'xty\'\n            assert simple_distance(A, B) == strdist(old_label, node.label)\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) > 1:\n        import cProfile\n        cProfile.run(\'unittest.main()\', \'profile\')\n    else:\n        unittest.main()\n\n'"
eval/zss/tests/test_regress.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#Author: Tim Henderson\n#Email: tim.tadh@gmail.com\n#For licensing see the LICENSE file in the top level directory.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport os\nfrom random import seed\n\nfrom zss.compare import (\n    simple_distance,\n    distance,\n    strdist,\n    Node,\n)\n\n\nseed(os.urandom(15))\n\n\ndef test_empty_tree_distance():\n    assert simple_distance(Node(\'\'), Node(\'\')) == 0\n    assert simple_distance(Node(\'a\'), Node(\'\')) == 1\n    assert simple_distance(Node(\'\'), Node(\'b\')) == 1\n\n\ndef test_paper_tree():\n    A = (\n      Node(""f"")\n        .addkid(Node(""d"")\n          .addkid(Node(""a""))\n          .addkid(Node(""c"")\n            .addkid(Node(""b""))\n          )\n        )\n        .addkid(Node(""e""))\n    )\n    B = (\n      Node(""f"")\n        .addkid(Node(""c"")\n          .addkid(Node(""d"")\n            .addkid(Node(""a""))\n            .addkid(Node(""b""))\n          )\n        )\n        .addkid(Node(""e""))\n    )\n    #print A\n    #print\n    #print B\n    dist = simple_distance(A,B)\n    assert dist == 2\n\n\ndef test_simplelabelchange():\n    A = (\n        Node(""f"")\n            .addkid(Node(""a"")\n                .addkid(Node(""h""))\n                .addkid(Node(""c"")\n                    .addkid(Node(""l""))))\n            .addkid(Node(""e""))\n        )\n    B = (\n        Node(""f"")\n            .addkid(Node(""a"")\n                .addkid(Node(""d""))\n                .addkid(Node(""r"")\n                    .addkid(Node(""b""))))\n            .addkid(Node(""e""))\n        )\n    dist = simple_distance(A,B)\n    print(dist)\n    assert dist == 3\n    #print \'distance\', d\n\n\ndef test_incorrect_behavior_regression():\n    A = (\n     Node(""a"")\n       .addkid(Node(""b"")\n         .addkid(Node(""x""))\n         .addkid(Node(""y""))\n       )\n     )\n    B = (\n     Node(""a"")\n       .addkid(Node(""x""))\n       .addkid(Node(""b"")\n         .addkid(Node(""y""))\n       )\n     )\n    dist = simple_distance(A, B)\n    print(dist)\n    assert dist == 2\n\ndef test_dict():\n    A = {\n        \'name\': \'tree\', \n        \'children\': [\n            {\n                \'name\': \'child 1\'\n            },\n            {\n                \'name\': \'child 2\'  \n            }\n            ]}\n    B = {\n        \'name\': \'tree\', \n        \'children\': [\n            {\n                \'name\': \'child 1\'\n            },\n            {\n                \'name\': \'child Z\'  \n            }\n            ]}\n    dist = simple_distance(\n        A,\n        B,\n        get_children=lambda x: x[\'children\'] if \'children\' in x else [],\n        get_label=lambda x: x[\'name\'],\n        label_dist=lambda x,y: 1 if x!=y else 0\n    )\n    assert dist == 1\n\ndef test_wrong_index_regression():\n    A = Node(\'r\')\n    B = (\n        Node(""a"")\n          .addkid(Node(""b"")\n            .addkid(Node(""c""))\n            .addkid(Node(""d""))\n          )\n          .addkid(Node(""e"")\n            .addkid(Node(""f""))\n            .addkid(Node(""g""))\n          )\n     )\n    def insert(n):\n        return ord(n.label[0])\n    def remove(n):\n        return ord(n.label[0])\n    def update(a,b):\n        return 10000*strdist(a.label, b.label)\n    dist = distance(A, B,\n        get_children=(lambda n: n.children),\n        insert_cost=insert,\n        remove_cost=remove,\n        update_cost=update,\n    )\n    assert dist == 814\n\n'"
