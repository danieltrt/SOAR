file_path,api_count,code
setup.py,0,"b'#!/usr/bin/python\n# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# To publish to PyPi, use:\n# python setup.py bdist_wheel upload -r pypi\n\nimport setuptools\n\nwith open(\'tensorfx/_version.py\') as vf:\n  exec(vf.read())\n\nwith open(\'requirements.txt\') as rf:\n  dependencies = rf.readlines()\n  dependencies = map(lambda d: d.strip(), dependencies)\n  dependencies = filter(lambda d: d and not d.startswith(\'#\'), dependencies)\n\nsetuptools.setup(\n  name=\'tensorfx\',\n  version=__version__,\n  packages=[\n    \'tensorfx\',\n    \'tensorfx.data\',\n    \'tensorfx.training\',\n    \'tensorfx.prediction\',\n    \'tensorfx.tools\',\n    \'tensorfx.models\',\n    \'tensorfx.models.nn\'\n  ],\n  entry_points={\n    \'console_scripts\': [\n      \'tfx = tensorfx.tools.tfx:main\'\n    ],\n  },\n  data_files=[(\'.\', [\'requirements.txt\'])],\n  install_requires=dependencies,\n  author=\'Nikhil Kothari\',\n  author_email=\'nikhilk@twitter\',\n  url=\'https://github.com/TensorLab/tensorfx\',\n  license=""Apache Software License"",\n  description=\'TensorFX Framework for training and serving machine learning models with TensorFlow\',\n  keywords=[\n    \'TensorLab\',\n    \'TensorFlow\',\n    \'Machine Learning\',\n    \'Deep Learning\',\n    \'Google\'\n  ],\n  classifiers=[\n    # https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    \'Development Status :: 3 - Alpha\',\n    \'Environment :: Other Environment\',\n    \'Intended Audience :: Developers\',\n    \'License :: OSI Approved :: Apache Software License\'\n    \'Programming Language :: Python\',\n    \'Programming Language :: Python :: 2.7\',\n    \'Operating System :: OS Independent\',\n    \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    \'Topic :: Software Development :: Libraries\',\n    \'Topic :: Software Development :: Libraries :: Python Modules\'\n  ]\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# TensorFX documentation build configuration file, created by\n# sphinx-quickstart on Mon Feb 20 23:55:56 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n\nimport os\nimport sys\nimport sphinx_rtd_theme\n\nsys.path.append(os.path.abspath(\'../\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n  \'sphinx.ext.autodoc\',\n  \'sphinx.ext.githubpages\',\n  \'sphinxcontrib.napoleon\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'TensorFX\'\ncopyright = u\'2017, TensorLab Project\'\nauthor = u\'TensorLab Project\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = \'en\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_show_sourcelink = False\nhtml_show_sphinx = False\nhtml_use_opensearch = \'\'\nhtml_title = \'\'\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'apidoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TensorFX.tex\', u\'TensorFX Documentation\',\n     u\'TensorLab\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'tensorfx\', u\'TensorFX Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'TensorFX\', u\'TensorFX Documentation\',\n     author, \'TensorFX\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n'"
src/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx module declaration.\n\nimport tensorfx.data as data\nimport tensorfx.training as training\nimport tensorfx.prediction as prediction\n\nfrom _version import __version__\n'"
src/_version.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _version.py\n# Declares package version.\n\n__version__ = \'0.1.4\'\n'"
tests/main.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# main.py\n# Entrypoint for tests\n\nimport os\nimport sys\nimport unittest\n\n# Add the library being tested to be on the path and then import it\nsys.path.append(os.path.abspath(os.path.join(__file__, \'../..\')))\n\n# Load the test modules\nimport data.dataset_tests\nimport data.schema_tests\nimport data.features_tests\nimport training.config_tests\n\n_TEST_MODULES = [\n  data.dataset_tests,\n  data.schema_tests,\n  data.features_tests,\n  training.config_tests\n]\n\n\ndef main():\n  suite = unittest.TestSuite()\n  for m in _TEST_MODULES:\n    suite.addTests(unittest.defaultTestLoader.loadTestsFromModule(m))\n\n  runner = unittest.TextTestRunner()\n  result = runner.run(suite)\n\n  sys.exit(len(result.errors) + len(result.failures))\n\n\nif __name__ == \'__main__\':\n  main()\n\n'"
samples/iris/data.py,1,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# data.py\n# A utility to generate data in TF.Example protobufs saved into a TF.Record file.\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.core.example.example_pb2 as examples\n\ndef load_data():\n  # Load data into DataFrame objects.\n  columns = [\'species\', \'petal_length\', \'petal_width\', \'sepal_length\', \'sepal_width\']\n  df_train = pd.read_csv(\'data/train.csv\', names=columns)\n  df_eval = pd.read_csv(\'data/eval.csv\', names=columns)\n\n  return df_train, df_eval\n\ndef convert_data(df, path):\n  writer = tf.python_io.TFRecordWriter(path)\n  for index, row in df.iterrows():\n    example = examples.Example()\n    features = example.features\n    features.feature[\'species\'].bytes_list.value.append(row[\'species\'])\n    features.feature[\'petal_length\'].float_list.value.append(row[\'petal_length\'])\n    features.feature[\'petal_width\'].float_list.value.append(row[\'petal_width\'])\n    features.feature[\'sepal_length\'].float_list.value.append(row[\'sepal_length\'])\n    features.feature[\'sepal_width\'].float_list.value.append(row[\'sepal_width\'])\n\n    record = example.SerializeToString()\n    writer.write(record)\n  writer.close()\n\n\ndef main():\n  df_train, df_eval = load_data()\n  convert_data(df_train, \'data/train.tfrecord\')\n  convert_data(df_eval, \'data/eval.tfrecord\')\n\n\nif __name__ == \'__main__\':\n  main()\n'"
src/data/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.data module declaration.\n\nfrom _schema import SchemaFieldType, SchemaField, Schema\nfrom _metadata import Metadata\nfrom _features import FeatureType, Feature, FeatureSet\nfrom _transforms import Transformer\n\nfrom _dataset import DataSet, DataSource\nfrom _ds_csv import CsvDataSet, CsvDataSource\nfrom _ds_df import DataFrameDataSet, DataFrameDataSource\nfrom _ds_examples import ExamplesDataSet, ExamplesDataSource\n'"
src/data/_dataset.py,2,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _dataset.py\n# Implementation of DataSet and DataSource classes.\n\nimport tensorflow as tf\nfrom tensorflow.python.lib.io import file_io as tfio\nfrom ._schema import Schema\nfrom ._metadata import Metadata\nfrom ._features import FeatureSet\n\n\nclass DataSet(object):\n  """"""A class representing data to be used within a job.\n\n  A DataSet contains one or more DataSource instances, each associated with a name.\n  """"""\n  def __init__(self, datasources, schema, metadata, features):\n    """"""Initializes a DataSet with the specified DataSource instances.\n\n    Arguments:\n      datasources: the set of contained DataSource instances key\'ed by name.\n      schema: the description of the source data.\n      metadata: additional per-field information associated with the data.\n      features: the optional description of the transformed data.\n    """"""\n    self._datasources = datasources\n\n    if type(schema) is str:\n      # Interpret this as a file path if the value is a string\n      schema = tfio.read_file_to_string(schema)\n      schema = Schema.parse(schema)\n    self._schema = schema\n\n    if metadata:\n      if type(metadata) is str:\n        # Interpret this as a file path if the value is a string\n        metadata = tfio.read_file_to_string(metadata)\n        metadata = Metadata.parse(metadata)\n    self._metadata = metadata\n\n    if features:\n      if type(features) is str:\n        # Interpret this as a file path if the value is a string\n        features = tfio.read_file_to_string(features)\n        features = FeatureSet.parse(features)\n    self._features = features\n\n  @property\n  def schema(self):\n    """"""Retrives the schema associated with the DataSet.\n    """"""\n    return self._schema\n\n  @property\n  def metadata(self):\n    """"""Retrives the metadata associated with the DataSet.\n    """"""\n    return self._metadata\n\n  @property\n  def features(self):\n    """"""Retrives the features defined with the DataSet.\n    """"""\n    return self._features\n\n  @property\n  def sources(self):\n    """"""Retrieves the names of the contained DataSource instances.\n    """"""\n    return self._datasources.keys()\n\n  def __getitem__(self, index):\n    """"""Retrieves a named DataSource within the DataSet.\n\n    Arguments:\n      index: the name of the DataSource to retrieve.\n    Returns:\n      The DataSource if there is one with the specified name; None otherwise.\n    """"""\n    return self._datasources.get(index, None)\n\n  def __len__(self):\n    """"""Retrieves the number of contained DataSource instances.\n    """"""\n    return len(self._datasources)\n\n  def parse_instances(self, instances, prediction=False):\n    """"""Parses input instances according to the associated schema, metadata and features.\n\n    Arguments:\n      instances: The tensor containing input strings.\n      prediction: Whether the instances are being parsed for producing predictions or not.\n    Returns:\n      A dictionary of tensors key\'ed by feature names.\n    """"""\n    raise NotImplementedError()\n\n\nclass DataSource(object):\n  """"""A base class representing data that can be read for use in a job.\n  """"""\n  def __init__(self):\n    """"""Initializes an instance of a DataSource.\n    """"""\n    pass\n\n  def read(self, batch=128, shuffle=False, shuffle_buffer=1000, epochs=0, threads=1):\n    """"""Reads the data represented by this DataSource using a TensorFlow reader.\n\n    Arguments:\n      batch: The number of records to read at a time.\n      shuffle: Whether to shuffle the list of files.\n      shuffle_buffer: When shuffling, the number of extra items to keep in the queue for randomness.\n      epochs: The number of epochs or passes over the data to perform.\n      threads: the number of threads to use to read from the queue.\n    Returns:\n      A tensor containing a list of instances read.\n    """"""\n    instances = self.read_instances(batch, shuffle, epochs)\n\n    queue_capacity = (threads + 3) * batch\n    if shuffle:\n      queue_capacity = queue_capacity + shuffle_buffer\n      return tf.train.shuffle_batch([instances],\n                                    batch_size=batch, allow_smaller_final_batch=True,\n                                    enqueue_many=True,\n                                    capacity=queue_capacity,\n                                    min_after_dequeue=shuffle_buffer,\n                                    num_threads=threads,\n                                    name=\'shuffle_batch\')\n    else:\n      return tf.train.batch([instances], batch_size=batch, allow_smaller_final_batch=True,\n                            enqueue_many=True, capacity=queue_capacity,\n                            num_threads=threads,\n                            name=\'batch\')\n\n  def read_instances(self, count, shuffle, epochs):\n    """"""Reads the data represented by this DataSource using a TensorFlow reader.\n\n    Arguments:\n      count: The number of instances to read in at most.\n      shuffle: Whether to shuffle the input queue of files.\n      epochs: The number of epochs or passes over the data to perform.\n    Returns:\n      A tensor containing instances that are read.\n    """"""\n    raise NotImplementedError(\'read_instances must be implemented in a derived class.\')\n'"
src/data/_ds_csv.py,11,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _ds_csv.py\n# Implementation of CsvDataSource.\n\nimport tensorflow as tf\nfrom ._dataset import DataSet, DataSource\nfrom ._schema import SchemaFieldType\n\n\nclass CsvDataSet(DataSet):\n  """"""A DataSet representing data in csv format.\n  """"""\n  def __init__(self, schema, metadata=None, features=None, **kwargs):\n    """"""Initializes a CsvDataSet with the specified DataSource instances.\n\n    Arguments:\n      schema: the description of the source data.\n      metadata: additional per-field information associated with the data.\n      features: the optional description of the transformed data.\n      kwargs: the set of CsvDataSource instances or csv paths to populate this DataSet with.\n    """"""\n    datasources = {}\n    for name, value in kwargs.iteritems():\n      if isinstance(value, str):\n        value = CsvDataSource(value)\n\n      if isinstance(value, CsvDataSource):\n        datasources[name] = value\n      else:\n        raise ValueError(\'The specified DataSource is not a CsvDataSource\')\n\n    if not len(datasources):\n      raise ValueError(\'At least one DataSource must be specified.\')\n\n    super(CsvDataSet, self).__init__(datasources, schema, metadata, features)\n\n  def parse_instances(self, instances, prediction=False):\n    """"""Parses input instances according to the associated schema.\n\n    Arguments:\n      instances: The tensor containing input strings.\n      prediction: Whether the instances are being parsed for producing predictions or not.\n    Returns:\n      A dictionary of tensors key\'ed by field names.\n    """"""\n    return parse_csv(self.schema, instances, prediction)\n\n\nclass CsvDataSource(DataSource):\n  """"""A DataSource representing one or more csv files.\n  """"""\n  def __init__(self, path, delimiter=\',\'):\n    """"""Initializes an instance of a CsvDataSource with the specified csv file(s).\n\n    Arguments:\n      path: the csv file containing the data. This can be a pattern to represent a set of files.\n      delimiter: the delimiter character used.\n    """"""\n    super(CsvDataSource, self).__init__()\n    self._path = path\n    self._delimiter = delimiter\n\n  @property\n  def path(self):\n    """"""Retrives the path represented by the DataSource.\n    """"""\n    return self._path\n\n  def read_instances(self, count, shuffle, epochs):\n    """"""Reads the data represented by this DataSource using a TensorFlow reader.\n\n    Arguments:\n      epochs: The number of epochs or passes over the data to perform.\n    Returns:\n      A tensor containing instances that are read.\n    """"""\n    # None implies unlimited; switch the value to None when epochs is 0.\n    epochs = epochs or None\n\n    files = tf.train.match_filenames_once(self._path, name=\'files\')\n    queue = tf.train.string_input_producer(files, num_epochs=epochs, shuffle=shuffle,\n                                           name=\'queue\')\n    reader = tf.TextLineReader(name=\'reader\')\n    _, instances = reader.read_up_to(queue, count, name=\'read\')\n\n    return instances\n\n\ndef parse_csv(schema, instances, prediction):\n  """"""A wrapper around decode_csv that parses csv instances based on provided Schema information.\n  """"""\n  if prediction:\n    # For training and evaluation data, the expectation is the target column is always present.\n    # For prediction however, the target may or may not be present.\n    # - In true prediction use-cases, the target is unknown and never present.\n    # - In prediction for model evaluation use-cases, the target is present.\n    # To use a single prediction graph, the missing target needs to be detected by comparing\n    # number of columns in instances with number of columns defined in the schema. If there are\n    # fewer columns, then prepend a \',\' (with assumption that target is always the first column).\n    #\n    # To get the number of columns in instances, split on the \',\' on the first instance, and use\n    # the first dimension of the shape of the resulting substring values.\n    columns = tf.shape(tf.string_split([instances[0]], delimiter=\',\').values)[0]\n    instances = tf.cond(tf.less(columns, len(schema)),\n                        lambda: tf.string_join([tf.constant(\',\'), instances]),\n                        lambda: instances)\n\n  # Convert the schema into a set of tensor defaults, to be used for parsing csv data.\n  defaults = []\n  for field in schema:\n    if field.length != 1:\n      # TODO: Support variable length, and list columns in csv.\n      raise ValueError(\'Unsupported schema field ""%s"". Length must be 1.\' % field.name)\n\n    if field.type == SchemaFieldType.integer:\n      field_default = tf.constant(0, dtype=tf.int64)\n    elif field.type == SchemaFieldType.real:\n      field_default = tf.constant(0.0, dtype=tf.float32)\n    else:\n      # discrete, text, binary\n      field_default = tf.constant(\'\', dtype=tf.string)\n    defaults.append([field_default])\n\n  values = tf.decode_csv(instances, defaults, name=\'csv\')\n\n  parsed_instances = {}\n  for field, value in zip(schema, values):\n    # The parsed values are scalars, so each tensor is of shape (None,); turn them into tensors\n    # of shape (None, 1).\n    parsed_instances[field.name] = tf.expand_dims(value, axis=1, name=field.name)\n\n  return parsed_instances\n'"
src/data/_ds_df.py,3,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _ds_df.py\n# Implementation of DataFrameDataSet and DataFrameDataSource.\n\nimport numpy as np\nimport tensorflow as tf\nfrom ._dataset import DataSet, DataSource\nfrom ._schema import Schema, SchemaField, SchemaFieldType\nfrom ._ds_csv import parse_csv\n\n\nclass DataFrameDataSet(DataSet):\n  """"""A DataSet representing data loaded as Pandas DataFrame instances.\n  """"""\n  def __init__(self, features=None, **kwargs):\n    """"""Initializes a DataFrameDataSet with the specified DataSource instances.\n\n    Arguments:\n      features: the optional description of the transformed data.\n      kwargs: the set of CsvDataSource instances or csv paths to populate this DataSet with.\n    """"""\n    # Import pandas here, rather than always, to restrict loading the library at startup, as well as\n    # having only a soft-dependency on the library.\n    # Since the user is passing in DataFrame instances, the assumption is the library has been\n    # loaded, and can be assumed to be installed.\n    import pandas as pd\n\n    def create_schema(df):\n      fields = []\n      for name, dtype in zip(df.columns, df.dtypes):\n        if type(dtype) == pd.types.dtypes.CategoricalDtype:\n          fields.append(SchemaField.discrete(name))\n        elif dtype in (np.int32, np.int64):\n          fields.append(SchemaField.integer(name))\n        elif dtype in (np.float32, np.float64):\n          fields.append(SchemaField.real(name))\n        else:\n          raise ValueError(\'Unsupported data type ""%s"" in column ""%s""\' % (str(dtype), name))\n      return Schema(fields)\n\n    def create_metadata(df):\n      metadata = {}\n      for name, dtype in zip(df.columns, df.dtypes):\n        md = {}\n        if type(dtype) == pd.types.dtypes.CategoricalDtype:\n          entries = list(df[name].unique())\n          if np.nan in entries:\n            entries.remove(np.nan)\n          md[\'vocab\'] = {\'entries\': sorted(entries)}\n        elif dtype in (np.int32, np.int64, np.float32, np.float64):\n          for stat, stat_value in df[name].describe().iteritems():\n            if stat == \'min\':\n              md[\'min\'] = stat_value\n            if stat == \'max\':\n              md[\'max\'] = stat_value\n        metadata[name] = md\n      return metadata\n\n    schema = None\n    metadata = None\n    datasources = {}\n    for name, value in kwargs.iteritems():\n      if isinstance(value, pd.DataFrame):\n        value = DataFrameDataSource(value)\n\n      if isinstance(value, DataFrameDataSource):\n        datasources[name] = value\n      else:\n        raise ValueError(\'The specified DataSource is not a DataFrameDataSource\')\n\n      if not schema:\n        schema = create_schema(value.dataframe)\n      if not metadata:\n        metadata = create_metadata(value.dataframe)\n\n    if not len(datasources):\n      raise ValueError(\'At least one DataSource must be specified.\')\n\n    super(DataFrameDataSet, self).__init__(datasources, schema, metadata, features)\n\n  def parse_instances(self, instances, prediction=False):\n    """"""Parses input instances according to the associated schema.\n\n    Arguments:\n      instances: The tensor containing input strings.\n      prediction: Whether the instances are being parsed for producing predictions or not.\n    Returns:\n      A dictionary of tensors key\'ed by feature names.\n    """"""\n    return parse_csv(self.schema, instances, prediction)\n\n\nclass DataFrameDataSource(DataSource):\n  """"""A DataSource representing a Pandas DataFrame.\n\n  This class is useful for working with local/in-memory data.\n  """"""\n  def __init__(self, df):\n    """"""Initializes an instance of a DataFrameDataSource with the specified Pandas DataFrame.\n\n    Arguments:\n      df: the DataFrame instance to use.\n    """"""\n    super(DataFrameDataSource, self).__init__()\n    self._df = df\n  \n  @property\n  def dataframe(self):\n    """"""Retrieves the DataFrame represented by this DataSource.\n    """"""\n    return self._df\n\n  def read_instances(self, count, shuffle, epochs):\n    """"""Reads the data represented by this DataSource using a TensorFlow reader.\n\n    Arguments:\n      epochs: The number of epochs or passes over the data to perform.\n    Returns:\n      A tensor containing instances that are read.\n    """"""\n    # None implies unlimited; switch the value to None when epochs is 0.\n    epochs = epochs or None\n\n    with tf.device(\'\'):\n      # Ensure the device is local and the queue, dequeuing and lookup all happen on the default\n      # device, which is required for the py_func operation.\n\n      # A UDF that given a batch of indices, returns a batch of string (csv formatted) instances\n      # from the DataFrame.\n      df = self._df\n      def reader(indices):\n        rows = df.iloc[indices]\n        return [map(lambda r: \',\'.join(r), rows.values.astype(\'string\'))]\n\n      queue = tf.train.range_input_producer(self._df.shape[0], num_epochs=epochs, shuffle=shuffle,\n                                            name=\'queue\')\n      indices = queue.dequeue_up_to(count)\n      instances = tf.py_func(reader, [indices], tf.string, name=\'read\')\n      instances.set_shape((None,))\n\n    return instances\n'"
src/data/_ds_examples.py,12,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _ds_examples.py\n# Implementation of ExamplesDataSource.\n\nimport tensorflow as tf\nfrom ._dataset import DataSet, DataSource\nfrom ._schema import SchemaFieldType\n\n\nclass ExamplesDataSet(DataSet):\n  """"""A DataSet representing data in tf.Example protobuf within a TFRecord format.\n  """"""\n  def __init__(self, schema, metadata=None, features=None, **kwargs):\n    """"""Initializes a ExamplesDataSet with the specified DataSource instances.\n\n    Arguments:\n      schema: the description of the source data.\n      metadata: additional per-field information associated with the data.\n      features: the optional description of the transformed data.\n      kwargs: the set of ExamplesDataSource instances or TFRecord paths to populate this DataSet.\n    """"""\n    datasources = {}\n    for name, value in kwargs.iteritems():\n      if isinstance(value, str):\n        value = ExamplesDataSource(value)\n\n      if isinstance(value, ExamplesDataSource):\n        datasources[name] = value\n      else:\n        raise ValueError(\'The specified DataSource is not a ExamplesDataSource\')\n\n    if not len(datasources):\n      raise ValueError(\'At least one DataSource must be specified.\')\n\n    super(ExamplesDataSet, self).__init__(datasources, schema, metadata, features)\n\n  def parse_instances(self, instances, prediction=False):\n    """"""Parses input instances according to the associated schema.\n\n    Arguments:\n      instances: The tensor containing input strings.\n      prediction: Whether the instances are being parsed for producing predictions or not.\n    Returns:\n      A dictionary of tensors key\'ed by field names.\n    """"""\n    # Convert the schema into an equivalent Example schema (expressed as features in Example\n    # terminology).\n    features = {}\n    for field in self.schema:\n      if field.type == SchemaFieldType.integer:\n        dtype = tf.int64\n        default_value = [0]\n      elif field.type == SchemaFieldType.real:\n        dtype = tf.float32\n        default_value = [0.0]\n      else:\n        # discrete\n        dtype = tf.string\n        default_value = [\'\']\n\n      if field.length == 0:\n        feature = tf.VarLenFeature(dtype=dtype)\n      else:\n        if field.length != 1:\n          default_value = default_value * field.length\n        feature = tf.FixedLenFeature(shape=[field.length], dtype=dtype, default_value=default_value)\n\n      features[field.name] = feature\n\n    return tf.parse_example(instances, features, name=\'examples\')\n\n\nclass ExamplesDataSource(DataSource):\n  """"""A DataSource representing one or more TFRecord files containing tf.Example data.\n  """"""\n  def __init__(self, path, compressed=False):\n    """"""Initializes an instance of a ExamplesDataSource with the specified TFRecord file(s).\n\n    Arguments:\n      path: TFRecord file containing the data. This can be a pattern to represent a set of files.\n      compressed: Whether the TFRecord files are compressed.\n    """"""\n    super(ExamplesDataSource, self).__init__()\n    self._path = path\n    self._compressed = compressed\n\n  @property\n  def path(self):\n    """"""Retrives the path represented by the DataSource.\n    """"""\n    return self._path\n\n  def read_instances(self, count, shuffle, epochs):\n    """"""Reads the data represented by this DataSource using a TensorFlow reader.\n\n    Arguments:\n      epochs: The number of epochs or passes over the data to perform.\n    Returns:\n      A tensor containing instances that are read.\n    """"""\n    # None implies unlimited; switch the value to None when epochs is 0.\n    epochs = epochs or None\n\n    options = None\n    if self._compressed:\n      options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n\n    files = tf.train.match_filenames_once(self._path, name=\'files\')\n    queue = tf.train.string_input_producer(files, num_epochs=epochs, shuffle=shuffle,\n                                           name=\'queue\')\n    reader = tf.TFRecordReader(options=options, name=\'reader\')\n    _, instances = reader.read_up_to(queue, count, name=\'read\')\n\n    return instances\n'"
src/data/_features.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _features.py\n# Implementation of FeatureSet and related class.\n\nimport enum\nimport tensorflow as tf\nimport yaml\n\n\nclass FeatureType(enum.Enum):\n  """"""Defines the type of Feature instances.\n  """"""\n  identity = \'identity\'\n  target = \'target\'\n  concat = \'concat\'\n  log = \'log\'\n  scale = \'scale\'\n  bucketize = \'bucketize\'\n  one_hot = \'one-hot\'\n\n\ndef _lookup_feature_type(s):\n  for t in FeatureType:\n    if t.value == s:\n      return t\n  raise ValueError(\'Invalid FeatureType ""%s"".\' % s)\n\n\nclass Feature(object):\n  """"""Defines a named feature within a FeatureSet.\n  """"""\n  def __init__(self, name, type, fields=None, features=None, transform=None):\n    """"""Initializes a Feature with its name and source fields.\n\n    Arguments:\n      name: the name of the feature.\n      type: the type of the feature.\n      fields: the names of the fields making up this feature.\n      features: the names of the features making up this feature in case of composite features.\n      transform: transform configuration to produce the feature.\n    """"""\n    self._name = name\n    self._type = type\n    self._fields = fields\n    self._features = features\n    self._transform = transform\n\n  @classmethod\n  def identity(cls, name, field=None):\n    """"""Creates a feature representing an un-transformed schema field.\n\n    Arguments:\n      name: the name of the feature.\n      field: the name of the field. If absenst, this uses the name as the field name as well.\n    Returns:\n      An instance of a Feature.\n    """"""\n    if not field:\n      # Optimize for an identity feature named the same as the field it represents.\n      field = name\n    return cls(name, FeatureType.identity, fields=[field])\n\n  @classmethod\n  def target(cls, name, field):\n    """"""Creates a feature representing the target value.\n    \n    Arguments:\n      name: the name of the feature.\n      field: the name of the field.\n    Returns:\n      An instance of a Feature.\n    """"""\n    return cls(name, FeatureType.target, fields=[field])\n\n  @classmethod\n  def concatenate(cls, name, *args):\n    """"""Creates a composite feature that is a concatenation of multiple features.\n\n    Arguments:\n      name: the name of the feature.\n      args: the sequence of features to concatenate.\n    Returns:\n      An instance of a Feature.\n    """"""\n    if not len(args):\n      raise ValueError(\'One or more features must be specified.\')\n\n    if type(args[0]) == list:\n      features = args[0]\n    else:\n      features = list(args)\n\n    return cls(name, FeatureType.concat, features=features)\n\n  @classmethod\n  def log(cls, name, field):\n    """"""Creates a feature representing a log value of a numeric field.\n\n    Arguments:\n      name: The name of the feature.\n      field: The name of the field to create the feature from.\n    Returns:\n      An instance of a Feature.\n    """"""\n    return cls(name, FeatureType.log, fields=[field])\n\n  @classmethod\n  def scale(cls, name, field, range=(0, 1)):\n    """"""Creates a feature representing a scaled version of a numeric field.\n\n    In order to perform scaling, the metadata will be looked up for the field, to retrieve min, max\n    and mean values.\n\n    Arguments:\n      name: The name of the feature.\n      field: The name of the field to create the feature from.\n      range: The target range of the feature.\n    Returns:\n      An instance of a Feature.\n    """"""\n    # TODO: What about the other scaling approaches, besides this (min-max scaling)?\n    transform = {\'min\': range[0], \'max\': range[1]}\n    return cls(name, FeatureType.scale, fields=[field], transform=transform)\n\n  @classmethod\n  def bucketize(cls, name, field, boundaries):\n    """"""Creates a feature representing a bucketized version of a numeric field.\n\n    The value is returned is the index of the bucket that the value falls into in one-hot\n    representation.\n\n    Arguments:\n      name: The name of the feature.\n      field: The name of the field to create the feature from.\n      boundaries: The list of bucket boundaries.\n    Returns:\n      An instance of a Feature.\n    """"""\n    transform = {\'boundaries\': \',\'.join(map(str, boundaries))}\n    return cls(name, FeatureType.bucketize, fields=[field], transform=transform)\n\n  @classmethod\n  def one_hot(cls, name, field):\n    """"""Creates a feature representing a one-hot representation of a discrete field.\n\n    Arguments:\n      name: The name of the feature.\n      field: The name of the field to create the feature from.\n    Returns:\n      An instance of a Feature.\n    """"""\n    return cls(name, FeatureType.one_hot, fields=[field])\n\n  @property\n  def name(self):\n    """"""Retrieves the name of the feature.\n    """"""\n    return self._name\n\n  @property\n  def features(self):\n    """"""Retrieves the features making up a composite feature.\n    """"""\n    return self._features\n  \n  @property\n  def field(self):\n    """"""Retrieves the field making up the feature if the feature is based on a single field.\n    """"""\n    if len(self._fields) == 1:\n      return self._fields[0]\n    return None\n\n  @property\n  def fields(self):\n    """"""Retrieves the fields making up the feature.\n    """"""\n    return self._fields\n  \n  @property\n  def type(self):\n    """"""Retrieves the type of the feature.\n    """"""\n    return self._type\n  \n  @property\n  def transform(self):\n    """"""Retrieves the transform configuration to produce the feature.\n    """"""\n    return self._transform\n\n  def format(self):\n    """"""Retrieves the raw serializable representation of the features.\n    """"""\n    data = {\'name\': self._name, \'type\': self._type.value}\n    if self._fields:\n      data[\'fields\'] = \',\'.join(self._fields)\n    if self._transform:\n      data[\'transform\'] = self._transform\n    if self._features:\n      data[\'features\'] = map(lambda f: f.format(), self._features)\n    return data\n\n  @staticmethod\n  def parse(data):\n    """"""Parses a feature from its serialized data representation.\n\n    Arguments:\n      data: A dictionary holding the serialized representation.\n    Returns:\n      The parsed Feature instance.\n    """"""\n    name = data[\'name\']\n    feature_type = _lookup_feature_type(data.get(\'type\', \'identity\'))\n    transform = data.get(\'transform\', None)\n\n    fields = None\n    features = None\n    if feature_type == FeatureType.concat:\n      features = []\n      for f in data[\'features\']:\n        feature = Feature.parse(f)\n        features.append(feature)\n    else:\n      fields = data.get(\'fields\', name)\n      if type(fields) is str:\n        fields = map(lambda n: n.strip(), fields.split(\',\'))\n\n    return Feature(name, feature_type, fields=fields, features=features, transform=transform)\n\n\nclass FeatureSet(object):\n  """"""Represents the set of features consumed by a model during training and prediction.\n\n  A FeatureSet contains a set of named features. Features are derived from input fields specified\n  in a schema and constructed using a transformation.\n  """"""\n  def __init__(self, features):\n    """"""Initializes a FeatureSet from its specified set of features.\n\n    Arguments:\n      features: the list of features within a FeatureSet.\n    """"""\n    self._features = features\n    self._features_map = dict(map(lambda f: (f.name, f), features))\n\n  @staticmethod\n  def create(*args):\n    """"""Creates a FeatureSet from a set of features.\n\n    Arguments:\n      args: a list or sequence of features defining the FeatureSet.\n    Returns:\n      A FeatureSet instance.\n    """"""\n    if not len(args):\n      raise ValueError(\'One or more features must be specified.\')\n\n    if type(args[0]) == list:\n      return FeatureSet(args[0])\n    else:\n      return FeatureSet(list(args))\n\n  @staticmethod\n  def parse(spec):\n    """"""Parses a FeatureSet from a YAML specification.\n\n    Arguments:\n      spec: The feature specification to parse.\n    Returns:\n      A FeatureSet instance.\n    """"""\n    if isinstance(spec, FeatureSet):\n      return spec\n\n    spec = yaml.safe_load(spec)\n\n    features = []\n    for f in spec[\'features\']:\n      feature = Feature.parse(f)\n      features.append(feature)\n\n    return FeatureSet(features)\n\n  def __getitem__(self, index):\n    """"""Retrives the specified Feature by name.\n\n    Arguments:\n      index: the name of the feature.\n    Returns:\n      The SchemaField if it exists; None otherwise.\n    """"""\n    return self._features_map.get(index, None)\n\n  def __len__(self):\n    """"""Retrieves the number of Features defined.\n    """"""\n    return len(self._features)\n\n  def __iter__(self):\n    """"""Creates an iterator over the features in the FeatureSet.\n    """"""\n    for feature in self._features:\n      yield feature\n'"
src/data/_metadata.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _metadata.py\n# Implementation of Metadata.\n\nimport ujson\n\n\nclass Metadata(object):\n  """"""This class encapsulates metadata for individual fields within a dataset.\n\n  Metadata is key\'ed by individual field names, and is represented as key/value pairs, specific\n  to the type of the field, and the analysis performed to generate the metadata.\n  """"""\n  def __init__(self, md):\n    """"""Initializes an instance of a Metadata object.\n\n    Arguments:\n      md: the metadata map key\'ed by field names.\n    """"""\n    self._md = md\n\n  @staticmethod\n  def parse(metadata):\n    """"""Parses a Metadata instance from a JSON specification.\n\n    Arguments:\n      metadata: The metadata to parse.\n    Returns:\n      A Metadata instance.\n    """"""\n    md = ujson.loads(metadata)\n    return Metadata(md)\n\n  def __getitem__(self, index):\n    """"""Retrieves the metadata of the specified field by name.\n\n    Arguments:\n      index: the name of the field whose metadata is to be retrieved.\n    Returns:\n      The metadata dictionary for the specified field, or an empty dictionary.\n    """"""\n    return self._md.get(index, {})\n\n  def __len__(self):\n    """"""Retrieves the number of Features defined.\n    """"""\n    return len(self._md)\n'"
src/data/_schema.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _schema.py\n# Implementation of Schema and related classes.\n\nimport enum\nimport yaml\n\n\nclass SchemaFieldType(enum.Enum):\n  """"""Defines the types of SchemaField instances.\n  """"""\n  integer = \'integer\'\n  real = \'real\'\n  discrete = \'discrete\'\n\n\nclass SchemaField(object):\n  """"""Defines a named and typed field within a Schema.\n  """"""\n  def __init__(self, name, type, length):\n    """"""Initializes a SchemaField with its name and type.\n\n    Arguments:\n      name: the name of the field.\n      type: the type of the field.\n      length: the valence of the field (0 implies variable length)\n    """"""\n    self._name = name\n    self._type = type\n    self._length = length\n\n    # TODO: Add support for default values\n\n  @classmethod\n  def discrete(cls, name, length=1):\n    """"""Creates a field representing a discrete value.\n\n    Arguments:\n      name: the name of the field.\n      length: the valence of the field (0 implies variable length)\n    """"""\n    return cls(name, SchemaFieldType.discrete, length)\n\n  @classmethod\n  def integer(cls, name, length=1):\n    """"""Creates a field representing an integer.\n\n    Arguments:\n      name: the name of the field.\n      length: the valence of the field (0 implies variable length)\n    """"""\n    return cls(name, SchemaFieldType.integer, length)\n\n  @classmethod\n  def real(cls, name, length=1):\n    """"""Creates a field representing a real number.\n\n    Arguments:\n      name: the name of the field.\n      length: the valence of the field (0 implies variable length)\n    """"""\n    return cls(name, SchemaFieldType.real, length)\n\n  @property\n  def name(self):\n    """"""Retrieves the name of the field.\n    """"""\n    return self._name\n\n  @property\n  def type(self):\n    """"""Retrieves the type of the field.\n    """"""\n    return self._type\n\n  @property\n  def length(self):\n    """"""Retrieves the length of the field.\n    """"""\n    return self._length\n\n  @property\n  def numeric(self):\n    """"""Returns whether the field is a numeric type, i.e. integer or real.\n    """"""\n    return self._type in [SchemaFieldType.integer, SchemaFieldType.real]\n\n\nclass Schema(object):\n  """"""Defines the schema of a DataSet.\n\n  The schema represents the structure of the source data before it is transformed into features.\n  """"""\n  def __init__(self, fields):\n    """"""Initializes a Schema with the specified set of fields.\n\n    Arguments:\n      fields: a list of fields representing an ordered set of columns.\n    """"""\n    if not len(fields):\n      raise ValueError(\'One or more fields must be specified\')\n\n    self._fields = fields\n    self._field_map = dict(map(lambda f: (f.name, f), fields))\n\n  @staticmethod\n  def create(*args):\n    """"""Creates a Schema from a set of fields.\n\n    Arguments:\n      args: a list or sequence of ordered fields defining the schema.\n    Returns:\n      A Schema instance.\n    """"""\n    if not len(args):\n      raise ValueError(\'One or more fields must be specified.\')\n\n    if type(args[0]) == list:\n      return Schema(args[0])\n    else:\n      return Schema(list(args))\n\n  def format(self):\n    """"""Formats a Schema instance into its YAML specification.\n    \n    Returns:\n      A string containing the YAML specification.\n    """"""\n    fields = map(lambda f: {\'name\': f.name, \'type\': f.type.name, \'length\': f.length},\n                 self._fields)\n    spec = {\'fields\': fields}\n\n    return yaml.safe_dump(spec, default_flow_style=False)\n\n  @staticmethod\n  def parse(spec):\n    """"""Parses a Schema from a YAML specification.\n\n    Arguments:\n      spec: The schema specification to parse.\n    Returns:\n      A Schema instance.\n    """"""\n    if isinstance(spec, Schema):\n      return spec\n\n    spec = yaml.safe_load(spec)\n    fields = map(lambda f: SchemaField(f[\'name\'], SchemaFieldType[f[\'type\']], f.get(\'length\', 1)),\n                 spec[\'fields\'])\n    return Schema(fields)\n\n  @property\n  def fields(self):\n    """"""Retrieve the names of the fields in the schema.\n    """"""\n    return map(lambda f: f.name, self._fields)\n\n  def __getitem__(self, index):\n    """"""Retrives the specified SchemaField by name or position.\n\n    Arguments:\n      index: the name or index of the field.\n    Returns:\n      The SchemaField if it exists; None otherwise.\n    """"""\n    if type(index) is int:\n      return self._fields[index] if len(self._fields) > index else None\n    else:\n      return self._field_map.get(index, None)\n\n  def __iter__(self):\n    """"""Creates an iterator to iterate over the fields.\n    """"""\n    for field in self._fields:\n      yield field\n\n  def __len__(self):\n    """"""Retrieves the number of SchemaFields defined.\n    """"""\n    return len(self._fields)\n'"
src/data/_transforms.py,13,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _transforms.py\n# Implementation of various transforms to build features.\n\nimport tensorflow as tf\nfrom ._features import FeatureType\nfrom ._schema import SchemaFieldType\n\n\nclass Transformer(object):\n  """"""Implements transformation logic.\n  """"""\n  def __init__(self, dataset):\n    """"""Initializes a Transformer.\n\n    Arguments:\n      dataset: The dataset containing the data to be transformed into features.\n    """"""\n    self._dataset = dataset\n\n  def transform(self, instances):\n    """"""Transforms the supplied instances into features.\n\n    Arguments:\n      instances: a dictionary of tensors key\'ed by field names corresponding to the schema.\n    Returns:\n      A dictionary of tensors key\'ed by feature names corresponding to the feature set.\n    """"""\n    features = self._dataset.features\n\n    # The top-level set of features is to be represented as a map of tensors, so transform the\n    # features, and use the map result.\n    _, tensor_map = _transform_features(instances, features,\n                                        self._dataset.schema,\n                                        self._dataset.metadata)\n    return tensor_map\n\n\ndef _identity(instances, feature, schema, metadata):\n  """"""Applies the identity transform, which causes the unmodified field value to be used.\n  """"""\n  return tf.identity(instances[feature.field], name=\'identity\')\n\n\ndef _target(instances, feature, schema, metadata):\n  """"""Applies the target transform, which causes the unmodified field value to be used.\n  """"""\n  # The result of parsing csv is a tensor of shape (None, 1), and we want to return a list of\n  # scalars, or specifically, tensor of shape (None, ).\n  return tf.squeeze(instances[feature.field], name=\'target\')\n\n\ndef _concat(instances, feature, schema, metadata):\n  """"""Applies the composite transform, to compose a single tensor from a set of features.\n  """"""\n  tensors, _ = _transform_features(instances, feature.features, schema, metadata)\n  return tf.concat(tensors, axis=1, name=\'concat\')\n\n\ndef _log(instances, feature, schema, metadata):\n  """"""Applies the log transform to a numeric field.\n  """"""\n  field = schema[feature.field]\n  if not field.numeric:\n    raise ValueError(\'A log transform cannot be applied to non-numerical field ""%s"".\' %\n                     feature.field)\n\n  # Add 1 to avoid log of 0 (still assuming the field does not have negative values)\n  return tf.log(instances[feature.field] + 1, name=\'log\')\n\n\ndef _scale(instances, feature, schema, metadata):\n  """"""Applies the scale transform to a numeric field.\n  """"""\n  field = schema[feature.field]\n  if not field.numeric:\n    raise ValueError(\'A scale transform cannot be applied to non-numerical field ""%s"".\' %\n                     feature.field)\n\n  transform = feature.transform\n  md = metadata[feature.field]\n\n  value = instances[feature.field]\n\n  range_min = float(md[\'min\'])\n  range_max = float(md[\'max\'])\n  value = (value - range_min) / (range_max - range_min)\n\n  if transform:\n    target_min = float(transform[\'min\'])\n    target_max = float(transform[\'max\'])\n    if (target_min != 0.0) or (target_max != 1.0):\n      value = value * (target_max - target_min) + target_min\n\n  return tf.identity(value, name=\'scale\')\n\n\ndef _bucketize(instances, feature, schema, metadata):\n  """"""Applies the bucketize transform to a numeric field.\n  """"""\n  field = schema[feature.field]\n  if not field.numeric:\n    raise ValueError(\'A scale transform cannot be applied to non-numerical field ""%s"".\' %\n                     feature.field)\n\n  transform = feature.transform\n  boundaries = map(float, transform[\'boundaries\'].split(\',\'))\n\n  # TODO: Figure out how to use tf.case instead of this contrib op\n  from tensorflow.contrib.layers.python.ops.bucketization_op import bucketize\n\n  # Create a one-hot encoded tensor. The dimension of this tensor is the set of buckets defined\n  # by N boundaries == N + 1.\n  # A squeeze is needed to remove the extra dimension added to the shape.\n  value = instances[feature.field]\n\n  value = tf.squeeze(tf.one_hot(bucketize(value, boundaries, name=\'bucket\'),\n                                depth=len(boundaries) + 1, on_value=1.0, off_value=0.0,\n                                name=\'one_hot\'),\n                     axis=1, name=\'bucketize\')\n  value.set_shape((None, len(boundaries) + 1))\n  return value\n\n\ndef _one_hot(instances, feature, schema, metadata):\n  """"""Applies the one-hot transform to a discrete field.\n  """"""\n  field = schema[feature.field]\n  if field.type != SchemaFieldType.discrete:\n    raise ValueError(\'A one-hot transform cannot be applied to non-discrete field ""%s"".\' %\n                     feature.field)\n\n  md = metadata[feature.field]\n  if not md:\n    raise ValueError(\'A one-hot transform requires metadata listing the unique values.\')\n\n  entries = md[\'entries\']\n  table = tf.contrib.lookup.HashTable(\n    tf.contrib.lookup.KeyValueTensorInitializer(entries,\n                                                tf.range(0, len(entries), dtype=tf.int64),\n                                                tf.string, tf.int64),\n    default_value=len(entries), name=\'entries\')\n\n  # Create a one-hot encoded tensor with one added to the number of values to account for the\n  # default value returned by the table for unknown/failed lookups.\n  # A squeeze is needed to remove the extra dimension added to the shape.\n  value = instances[feature.field]\n\n  value = tf.squeeze(tf.one_hot(table.lookup(value), len(entries) + 1, on_value=1.0, off_value=0.0),\n                     axis=1,\n                     name=\'one_hot\')\n  value.set_shape((None, len(entries) + 1))\n  return value\n\n\n_transformers = {\n    FeatureType.identity.name: _identity,\n    FeatureType.target.name: _target,\n    FeatureType.concat.name: _concat,\n    FeatureType.log.name: _log,\n    FeatureType.scale.name: _scale,\n    FeatureType.bucketize.name: _bucketize,\n    FeatureType.one_hot.name: _one_hot\n  }\n\ndef _transform_features(instances, features, schema, metadata):\n  """"""Transforms a list of features, to produce a list and map of tensor values.\n  """"""\n  tensors = []\n  tensor_map = {}\n\n  for f in features:\n    transformer = _transformers[f.type.name]\n    with tf.name_scope(f.name):\n      value = transformer(instances, f, schema, metadata)\n\n    tensors.append(value)\n    tensor_map[f.name] = value\n\n  return tensors, tensor_map\n'"
src/models/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.models module declaration.\n\nfrom ._classification import ClassificationModelArguments, ClassificationModelBuilder\nfrom ._classification import StringLabelClassification\n'"
src/models/_classification.py,9,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _classification.py\n# Implements ClassificationModelBuilder and ClassificationModelArguments.\n\nimport tensorflow as tf\nimport tensorfx as tfx\n\nclass ClassificationModelArguments(tfx.training.ModelArguments):\n  """"""Arguments for classification models.\n  """"""\n  @classmethod\n  def init_parser(cls, parser):\n    """"""Initializes the argument parser.\n\n    Args:\n      parser: An argument parser instance to be initialized with arguments.\n    """"""\n    super(ClassificationModelArguments, cls).init_parser(parser)\n\n  def process(self):\n    """"""Processes the parsed arguments to produce any additional objects.\n    """"""\n    pass\n\n\nclass ClassificationModelBuilder(tfx.training.ModelBuilder):\n  """"""A ModelBuilder for building classification models.\n\n  A classification model treats the target value as a label. The label might be a discrete\n  value (which is converted to integer indices), or may be pre-indexed.\n  """"""\n  def __init__(self, args):\n    super(ClassificationModelBuilder, self).__init__(args)\n    self._classification = None\n\n  @property\n  def classification(self):\n    """"""Returns the classification helper object.\n    """"""\n    return self._classification\n\n  def build_graph_interfaces(self, dataset, config):\n    """"""Builds graph interfaces for training and evaluating a model, and for predicting using it.\n\n    A graph interface is an object containing a TensorFlow graph member, as well as members\n    corresponding to various tensors and ops within the graph.\n\n    ClassificationModelBuilder also builds a classification helper object for use during graph\n    building.\n\n    Arguments:\n      dataset: The dataset to use during training.\n      config: The training Configuration object.\n    Returns:\n      A tuple consisting of the training, evaluation and prediction interfaces.\n    """"""\n    target_feature = filter(lambda f: f.type == tfx.data.FeatureType.target, dataset.features)[0]\n    target_field = dataset.schema[target_feature.field]\n    target_metadata = dataset.metadata[target_feature.field]\n\n    if target_field.type == tfx.data.SchemaFieldType.discrete:\n      self._classification = StringLabelClassification(target_metadata[\'vocab\'][\'entries\'])\n    else:\n      self._classification = None\n\n    return super(ClassificationModelBuilder, self).build_graph_interfaces(dataset, config)\n\n\nclass StringLabelClassification(object):\n  """"""A classification scenario involving string label names.\n\n  Labels will be converted to indices when using the input, and indices back to labels to produce\n  output.\n  """"""\n  def __init__(self, labels):\n    """"""Initializes an instance of StringLabelClassification with specified label names.\n    """"""\n    self._labels = labels\n    self._num_labels = len(labels)\n\n  @property\n  def num_labels(self):\n    """"""Returns the number of labels in the model.\n    """"""\n    return self._num_labels\n\n  def keys(self, inputs):\n    """"""Retrieves the keys, if present from the inputs.\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n    Returns:\n      A tensor containing the keys if a keys feature exists, None otherwise.\n    """"""\n    return inputs.get(\'key\', None)\n\n  def features(self, inputs):\n    """"""Retrieves the features to use to build a model.\n\n    For classification models, the default behavior is to use a feature named \'X\' to represent the\n    input features for the model.\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n    Returns:\n      A tensor containing model input features.\n    """"""\n    return inputs[\'X\']\n\n  def target_labels(self, inputs):\n    """"""Retrieves the target labels to use to build a model.\n\n    For classification models, the default behavior is to use a feature named \'Y\' to represent the\n    target features for the model.\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n    Returns:\n      A tensor containing the target labels.\n    """"""\n    return inputs[\'Y\']\n\n  def target_label_indices(self, inputs, one_hot=True):\n    """"""Retrieves the target labels to use to build a model, as a set of indices.\n\n    For classification models, the default behavior is to use a feature named \'Y\' to represent the\n    target features for the model. The labels are used to perform a lookup to produce indices.\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n      one_hot: whether to convert the indices into their one-hot representation.\n    Returns:\n      A tensor containing the target labels as indices..\n    """"""\n    labels = inputs[\'Y\']\n\n    with tf.name_scope(\'label_table\'):\n      string_int_mapping =  tf.contrib.lookup.KeyValueTensorInitializer(\n        self._labels, tf.range(0, self._num_labels, dtype=tf.int64), tf.string, tf.int64)\n      table = tf.contrib.lookup.HashTable(string_int_mapping, default_value=-1)\n\n    if one_hot:\n      indices = tf.squeeze(tf.one_hot(table.lookup(labels), self._num_labels), name=\'indices\')\n    else:\n      indices = table.lookup(labels, name=\'indices\')\n\n    return indices\n\n  def output_labels(self, indices):\n    """"""Produces the output labels to represent a model\'s output.\n\n    The indices are used to lookup corresponding label names.\n\n    Arguments:\n      indices: The predicted label indices.\n    Returns:\n      A tensor containing output predicted label names.\n    """"""\n    with tf.name_scope(\'label_table\'):\n      int_string_mapping = tf.contrib.lookup.KeyValueTensorInitializer(\n        tf.range(0, self._num_labels, dtype=tf.int64), self._labels, tf.int64, tf.string)\n      table = tf.contrib.lookup.HashTable(int_string_mapping, default_value=\'\')\n\n    return table.lookup(indices, name=\'label\')\n'"
src/prediction/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.prediction module declaration.\n\nfrom _model import Model\n'"
src/prediction/_model.py,9,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _model.py\n# Implements the Model class.\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Model(object):\n  """"""A model provides performs inferences using TensorFlow to produce predictions.\n\n  A model is loaded from a checkpoint that was produced during training.\n  """"""\n  def __init__(self, session, inputs, outputs):\n    """"""Initializes a Model using a TensorFlow session containing an initialized prediction graph.\n\n    Arguments:\n      session: The TensorFlow session to use for evaluating inferences.\n      inputs: A map of input names to corresponding graph tensors.\n      outputs: A map of output names to corresponding graph tensors.\n    """"""\n    self._session = session\n    self._inputs = inputs\n    self._outputs = outputs\n\n    # Optimize for the one input key for the currently supported single input graphs\n    self._input_key = inputs[inputs.keys()[0]]\n\n  @classmethod\n  def load(cls, path):\n    """"""Imports a previously exported saved model.\n\n    Arguments:\n      - path: The location on disk where the saved model exists.\n    Returns:\n      An initialized Model object that can be used for performing prediction.\n    """"""\n    with tf.Graph().as_default() as graph:\n      session = tf.Session()\n\n      metagraph = tf.saved_model.loader.load(session, [\'serve\'], path)\n      signature = _parse_signature(metagraph)\n\n      inputs = {}\n      for alias in signature.inputs:\n        inputs[alias] = signature.inputs[alias].name\n      outputs = {}\n      for alias in signature.outputs:\n        outputs[alias] = signature.outputs[alias].name\n\n    return cls(session, inputs, outputs)\n\n\n  @staticmethod\n  def save(session, path, inputs, outputs):\n    """"""Exports the current session, the loaded graph, and variables into a saved model.\n\n    Arguments:\n      - session: the TensorFlow session with variables to save.\n      - path: the location where the output model directory should be created.\n      - inputs: the list of tensors constituting the input to the prediction graph.\n      - outputs: the list of tensors constituting the outputs of the prediction graph.\n    """"""\n    signature_map = {\'serving_default\': _build_signature(inputs, outputs)}\n    model_builder = tf.saved_model.builder.SavedModelBuilder(path)\n    model_builder.add_meta_graph_and_variables(session,\n                                               tags=[\'serve\'],\n                                               signature_def_map=signature_map,\n                                               clear_devices=True)\n    model_builder.save()\n\n  def predict(self, instances):\n    """"""Performs inference to return predictions for the specified instances of data.\n\n    Arguments:\n      - instances: either an object, or list of objects each containing feature values.\n    """"""\n    if not instances:\n      return []\n\n    # TODO: Support for DataFrames and a flag of whether to append prediction outputs to input\n    #       DataFrame.\n\n    # Run the instances through the session to retrieve the prediction outputs\n    results = self._session.run(self._outputs, feed_dict={self._input_key: instances})\n\n    # Convert outputs, which are in dictionary of lists representation (alias -> batch of values) to\n    # list of predictions representation (list of dictionaries, where each dict is alias -> value).\n    predictions = [{} for _ in range(len(instances))]\n\n    for alias in self._outputs.iterkeys():\n      values = results[alias]\n      for index, value in enumerate(values):\n        if isinstance(value, np.ndarray):\n          value = value.tolist()\n        predictions[index][alias] = value\n\n    return predictions\n\n\ndef _build_signature(inputs, outputs):\n  def tensor_alias(tensor):\n    local_name = tensor.name.split(\'/\')[-1]\n    return local_name.split(\':\')[0]\n\n  input_map = {}\n  output_map = {}\n  for tensor in inputs:\n    input_map[tensor_alias(tensor)] = tf.saved_model.utils.build_tensor_info(tensor)\n  for tensor in outputs:\n    output_map[tensor_alias(tensor)] = tf.saved_model.utils.build_tensor_info(tensor)\n\n  return tf.saved_model.signature_def_utils.build_signature_def(\n    inputs=input_map,\n    outputs=output_map,\n    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n\n\ndef _parse_signature(metagraph):\n  if not metagraph.signature_def:\n    raise ValueError(\'Invalid model. The saved model does not define a signature.\')\n  if len(metagraph.signature_def) > 1:\n    raise ValueError(\'Invalid model. Only models with a single signature are supported.\')\n\n  signature = metagraph.signature_def.get(\'serving_default\', None)\n  if not signature:\n    raise ValueError(\'Invalid model. Unexpected signature type.\')\n\n  if len(signature.inputs) != 1:\n    raise ValueError(\'Invalid model. Only models with a single input are supported.\')\n  for alias in signature.inputs:\n    if signature.inputs[alias].dtype != tf.string.as_datatype_enum:\n      raise ValueError(\'Invalid model. Only models with a string input are supported.\')\n  if len(signature.outputs) == 0:\n    raise ValueError(\'Invalid model. Only models with at least one output are supported.\')\n\n  return signature\n'"
src/tools/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.tools module declaration.\n'"
src/tools/_predict.py,2,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _predict.py\n# Implements PredictCommand.\n\nimport json\nimport os\nimport sys\nimport tensorflow as tf\nimport tensorfx as tfx\n\n\nclass PredictCommand(object):\n  """"""Implements the tfx predict command to use a model to produce predictions.\n  """"""\n  name = \'predict\'\n  help = \'Produces predictions using a model.\'\n  extra = False\n\n  @staticmethod\n  def build_parser(parser):\n    parser.add_argument(\'--model\', metavar=\'path\', type=str, required=True,\n                        help=\'The path to a previously trained model.\')\n    parser.add_argument(\'--input\', metavar=\'path\', type=str,\n                        help=\'The path to a file with input instances. Uses stdin by default.\')\n    parser.add_argument(\'--output\', metavar=\'path\', type=str,\n                        help=\'The path to a file to write outputs to. Uses stdout by default.\')\n    parser.add_argument(\'--batch-size\', metavar=\'instances\', type=int, default=10,\n                        help=\'The number of instances to predict per batch.\')\n\n  @staticmethod\n  def run(args):\n    # TODO: Figure out where to do JSON and TF initialization in more common way.\n    json.encoder.FLOAT_REPR = lambda f: (\'%.5f\' % f)\n\n    tf.logging.set_verbosity(tf.logging.ERROR)\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = str(tf.logging.ERROR)\n\n    model = tfx.prediction.Model.load(args.model)\n\n    with TextSource(args.input, args.batch_size) as source, TextSink(args.output) as sink:\n      for instances in source:\n        predictions = model.predict(instances)\n        lines = map(lambda p: json.dumps(p, sort_keys=True), predictions)\n        sink.write(lines)\n\n\nclass TextSource(object):\n\n  def __init__(self, file=None, batch_size=1):\n    self._file = file\n    self._batch_size = batch_size\n\n  def __enter__(self):\n    self._stream = open(self._file, \'r\') if self._file else sys.stdin\n    return self\n\n  def __exit__(self, type, value, traceback):\n    if self._stream and self._file:\n      self._stream.close()\n\n  def __iter__(self):\n    instances = []\n\n    while True:\n      instance = self._stream.readline().strip()\n      if not instance:\n        # EOF\n        break\n\n      instances.append(instance)\n      if len(instances) == self._batch_size:\n        # A desired batch of instances is available\n        yield instances\n        instances = []\n\n    if instances:\n      yield instances\n\n\nclass TextSink(object):\n\n  def __init__(self, file=None):\n    self._file = file\n\n  def __enter__(self):\n    self._stream = open(self._file, \'w\') if self._file else sys.stdout\n    return self\n\n  def __exit__(self, type, value, traceback):\n    if self._stream and self._file:\n      self._stream.close()\n\n  def write(self, lines):\n    for l in lines:\n      self._stream.write(l + \'\\n\')\n'"
src/tools/_scaffold.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _scaffold.py\n# Implements ScaffoldCommand\n\nimport os\nimport tensorfx as tfx\n\nclass ScaffoldCommand(object):\n  """"""Implements the tfx scaffold command to create a new TensorFX project from a template.\n  """"""\n  name = \'scaffold\'\n  help = \'Createa a new project from a template.\'\n  extra = False\n\n  @staticmethod\n  def build_parser(parser):\n    parser.add_argument(\'--name\', metavar=\'name\', type=str, required=True,\n                        help=\'The name of the model to use when instantiating the template\')\n    parser.add_argument(\'--dir\', metavar=\'path\', type=str, required=False, default=os.getcwd(),\n                        help=\'The directory in which to instantiate the template\')\n    parser.add_argument(\'--model\', metavar=\'type\', type=str, required=False, default=\'custom\',\n                        help=\'The type of model to create; eg. ""nn.FeedForwardClassification""\')\n\n  @staticmethod\n  def run(args):\n    variables = {\n      \'name\': args.name,\n      \'tensorfx_version\': tfx.__version__\n    }\n\n    contents = {\n      \'setup.py\': _scaffold_setup_py.format(**variables),\n      \'trainer/__init__.py\': _scaffold_trainer_init_py.format(**variables),\n    }\n\n    if args.model == \'custom\':\n      variables[\'model_class\'] = args.name[0].upper() + args.name[1:]\n      contents[\'trainer/main.py\'] = _scaffold_trainer_main_py_custom.format(**variables)\n      contents[\'trainer/model.py\'] = _scaffold_trainer_model_py.format(**variables)\n    else:\n      variables[\'model\'] = args.model\n      variables[\'model_set\'] = args.model.split(\'.\')[0]\n      contents[\'trainer/main.py\'] = _scaffold_trainer_main_py.format(**variables)\n\n    scaffold_path = os.path.join(args.dir, args.name)\n    for path, content in contents.iteritems():\n      content_path = os.path.join(scaffold_path, path)\n\n      content_dir = os.path.dirname(content_path)\n      if not os.path.isdir(content_dir):\n        os.makedirs(content_dir)\n\n      with open(content_path, \'w\') as content_file:\n        content_file.write(content)\n\n\n# TODO: Externalize these into a template directory\n\n_scaffold_setup_py = """"""# setup.py\n\nimport setuptools\n\n# The name and version of the package.\nname = \'{name}\'\nversion = \'1.0\'\n\n# The main modules in the package.\ntrainer_main = \'{name}.trainer.main\'\n\n\ndef main():\n  \\""""""Invokes setup to build or install a distribution of the package.\n  \\""""""\n  setuptools.setup(name=name, version=version,\n                   packages=setuptools.find_packages(),\n                   install_requires=[\n                     \'tensorfx={tensorfx_version}\'\n                   ])\n\n\nif __name__ == \'__main__\':\n  main()\n""""""\n\n_scaffold_trainer_init_py = """"""# __init__.py\n# Declaration of {name}.trainer module.\n""""""\n\n_scaffold_trainer_main_py = """"""# main.py\n# Implementation of training module.\n\nimport tenosrflow as tf\nimport tensorfx as tfx\nimport tensorfx.models.{model_set} as {model_set}\n\nargs = {model}Arguments.parse(parse_job=True)\ndataset = tfx.data.CsvDataSet(args.data_schema,\n                              train=args.data_train,\n                              eval=args.data_eval,\n                              metadata=args.data_metadata,\n                              features=args.data_features)\n\nbuilder = {model}(args)\n\ntrainer = tfx.training.ModelTrainer()\nmodel = trainer.train(builder, dataset, args.output)\n""""""\n\n_scaffold_trainer_main_py_custom = """"""# main.py\n# Implementation of training module.\n\nimport tensorflow as tf\nimport tensorfx as tfx\nimport _model as model\n\nargs = model.{model_class}Arguments.parse(parse_job=True)\ndataset = tfx.data.CsvDataSet(args.data_schema,\n                              train=args.data_train,\n                              eval=args.data_eval,\n                              metadata=args.data_metadata,\n                              features=args.data_features)\n\nbuilder = model.{model_class}(args)\n\ntrainer = tfx.training.ModelTrainer()\nmodel = trainer.train(builder, dataset, args.outupt)\n""""""\n\n_scaffold_trainer_model_py = """"""# model.py\n# Implementation of model module.\n\nimport tensorflow as tf\nimport tensorfx as tfx\n\nclass {model_class}Arguments(tfx.training.ModelArguments):\n  \\""""""Declares arguments supported by the model.\n  \\""""""\n  @classmethod\n  def init_parser(cls, parser):\n    super({model_class}Arguments, cls).init_parser(parser)\n\n    # TODO: Add additional model-specific arguments.\n\n\nclass {model_class}(tfx.training.ModelBuilder):\n  \\""""""Builds the graphs for training, evaluating and predicting with the model.\n  \\""""""\n  def __init__(self, args, dataset):\n    super({model_class}, self).__init__(args, dataset)\n\n  # TODO: Implement one or more of the graph building methods. These include one or more of\n  # build_input(), build_inference(), build_training(), build_output(), and build_evaluation() or\n  # build_training_graph(), build_evaluation_graph(), and build_prediction_graph().\n  # See the documentation for more details.\n""""""\n'"
src/tools/_train.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _train.py\n# Implements TrainCommand.\n\nimport json\nimport os\nimport subprocess\nimport sys\n\n_PORT = 14000\n\nclass TrainCommand(object):\n  """"""Implements the tfx train command to launch single node and distributed training.\n  """"""\n  name = \'train\'\n  help = \'Launches local training jobs for development.\'\n  extra = True\n\n  @staticmethod\n  def build_parser(parser):\n    parser.add_argument(\'--module\', metavar=\'name\', type=str, required=True,\n                        help=\'The name of the training module to launch\')\n    parser.add_argument(\'--output\', metavar=\'path\', type=str, default=\'output\',\n                        help=\'The path to write outputs\')\n    parser.add_argument(\'--distributed\', action=\'store_true\',\n                        help=\'Runs a multi-node (master, worker, parameter server) cluster\')\n\n  @staticmethod\n  def run(args):\n    args.extra.extend([\n      \'--job-dir\', os.path.abspath(args.output)\n    ])\n\n    cmd = [\'python\', \'-m\', args.module] + args.extra\n\n    if args.distributed:\n      print \'Launching training tasks (master, worker, parameter server)...\'\n      print \' \'.join(cmd)\n      print \'----\\n\'\n\n      ps_task = _start_task(cmd, _create_distributed_config(\'ps\'))\n      master_task = _start_task(cmd, _create_distributed_config(\'master\'))\n      worker_task = _start_task(cmd, _create_distributed_config(\'worker\'))\n    else:\n      print \'Launching training task...\'\n      print \' \'.join(cmd)\n      print \'----\\n\'\n\n      master_task = _start_task(cmd, _create_simple_config())\n      ps_task = None\n      worker_task = None\n\n    try:\n      master_task.wait()\n    finally:\n      if worker_task:\n        _kill_task(worker_task)\n      if ps_task:\n        _kill_task(ps_task)\n      _kill_task(master_task)\n\n\ndef _create_simple_config():\n  return {\n    \'task\': {\'type\': \'master\', \'index\': 0},\n    \'job\': {\'local\': True}\n  }\n\ndef _create_distributed_config(task):\n  return {\n    \'cluster\': {\n      \'ps\': [\'localhost:%d\' % _PORT],\n      \'master\': [\'localhost:%d\' % (_PORT + 1)],\n      \'worker\': [\'localhost:%d\' % (_PORT + 2)]\n    },\n    \'task\': {\'type\': task, \'index\': 0},\n    \'job\': {\'local\': True}\n  }\n\ndef _start_task(cmd, config):\n  env = os.environ.copy()\n  env[\'TF_CONFIG\'] = json.dumps(config)\n  return subprocess.Popen(cmd, env=env)\n\ndef _kill_task(process):\n  try:\n    process.terminate()\n  except:\n    pass\n'"
src/tools/tfx.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# train.py\n# tensorfx.tools.tfx module to implement the tfx command-line tool.\n\nimport argparse\nimport sys\nfrom _scaffold import ScaffoldCommand\nfrom _train import TrainCommand\nfrom _predict import PredictCommand\n\n\ndef _build_cli():\n  """"""Builds the command-line interface.\n  """"""\n  commands = [\n    ScaffoldCommand,\n    TrainCommand,\n    PredictCommand\n  ]\n\n  cli = argparse.ArgumentParser(prog=\'tfx\')\n  subparsers = cli.add_subparsers(title=\'Available commands\')\n\n  for command in commands:\n    command_parser = subparsers.add_parser(command.name, help=command.help,\n                                           usage=\'%(prog)s [--help] [options]\')\n    command_parser.set_defaults(command=command)\n    command.build_parser(command_parser)\n\n  return cli\n\n\ndef main(args=None):\n  if not args:\n    args = sys.argv[1:]\n\n  cli = _build_cli()\n  args, extra_args = cli.parse_known_args(args)\n\n  command = args.command\n  del args.command\n\n  if extra_args:\n    if command.extra:\n      args.extra = extra_args\n    else:\n      cli.error(\'unrecognized arguments %s\' % \' \'.join(extra_args))\n\n  command.run(args)\n\n\nif __name__ == \'__main__\':\n  main(sys.argv[1:])\n'"
src/training/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.training module declaration.\n\nfrom _config import Configuration\nfrom _args import ModelArguments\nfrom _model import ModelBuilder\nfrom _trainer import ModelTrainer\n'"
src/training/_args.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _args.py\n# Defines ModelArguments and related classes.\n\nimport argparse\nimport logging\nimport sys\nimport tensorfx as tfx\n\n\nclass ModelArguments(argparse.Namespace):\n\n  def process(self):\n    """"""Processes the parsed arguments to produce any additional objects.\n    """"""\n    # Convert strings to logging values\n    self.log_level = getattr(logging, self.log_level)\n    self.log_level_tensorflow = getattr(logging, self.log_level_tensorflow)\n\n  @classmethod\n  def default(cls):\n    """"""Creates an instance of the arguments with default values.\n\n    Returns:\n      The model arguments with default values.\n    """"""\n    return cls.parse(args=[])\n\n  @classmethod\n  def parse(cls, args=None, parse_job=False):\n    """"""Parses training arguments.\n\n    Arguments:\n      args: the arguments to parse. If unspecified, the process arguments are used.\n      parse_job: whether to parse the job related standard (input and output) arguments.\n    Returns:\n      The parsed arguments.\n    """"""\n    if args is None:\n      args = sys.argv[1:]\n\n    argparser = ModelArgumentsParser(add_job_arguments=parse_job)\n    cls.init_parser(argparser)\n\n    args_object = argparser.parse_args(args, namespace=cls())\n    args_object._args = args\n    args_object.process()\n\n    return args_object\n\n  @classmethod\n  def init_parser(cls, parser):\n    """"""Initializes the argument parser.\n\n    Args:\n      parser: An argument parser instance to be initialized with arguments.\n    """"""\n    session = parser.add_argument_group(title=\'Session\',\n                                        description=\'Arguments controlling the session loop.\')\n    session.add_argument(\'--max-steps\', type=int, default=1000,\n                         help=\'The number of steps to execute during the training job.\')\n    session.add_argument(\'--batch-size\', type=int, default=128,\n                         help=\'The number of instances to read and process in each training step.\')\n    session.add_argument(\'--epochs\', type=int, default=0,\n                         help=\'The number of passes over the training data to make.\')\n    session.add_argument(\'--checkpoint-interval-secs\', type=int, default=60 * 5,\n                         help=\'The frequency of checkpoints to create during the training job.\')\n\n    log_levels = [\'FATAL\', \'ERROR\', \'WARN\', \'INFO\', \'DEBUG\']\n\n    log = parser.add_argument_group(title=\'Logging and Diagnostics\',\n                                    description=\'Arguments controlling logging during training.\')\n    log.add_argument(\'--log-level-tensorflow\', metavar=\'level\', type=str, default=\'ERROR\',\n                     choices=log_levels,\n                     help=\'The logging level for TensorFlow generated log messages.\')\n    log.add_argument(\'--log-device-placement\', default=False, action=\'store_true\',\n                     help=\'Whether to log placement of ops and tensors on devices.\')\n    log.add_argument(\'--log-level\', metavar=\'level\', type=str, default=\'INFO\', choices=log_levels,\n                     help=\'The logging level for training.\')\n    log.add_argument(\'--log-interval-steps\', metavar=\'steps\', type=int, default=100,\n                     help=\'The frequency of training logs and summary events to generate.\')\n\n\nclass ModelArgumentsParser(argparse.ArgumentParser):\n\n  def __init__(self, add_job_arguments):\n    # TODO: Add description, epilogue, etc.\n    super(ModelArgumentsParser, self).__init__(prog=\'trainer\', usage=\'%(prog)s [--help] [options]\')\n    self.var_args_action = AddVarArgAction\n\n    job = self.add_argument_group(title=\'Job\',\n                                  description=\'Arguments defining job inputs and outputs.\')\n    job.add_argument(\'--data-schema\', metavar=\'path\', type=str, required=False,\n                     help=\'The schema (columns, types) of the data being referenced (YAML).\')\n    job.add_argument(\'--data-metadata\', metavar=\'path\', type=str, required=False,\n                     help=\'The statistics and vocabularies of the data being referenced (JSON).\')\n    job.add_argument(\'--data-features\', metavar=\'path\', type=str, required=False,\n                     help=\'The set of features to transform the raw data into (YAML).\')\n    job.add_argument(\'--data-train\', metavar=\'path\', type=str, required=False,\n                     help=\'The data to use for training. This can include wildcards.\')\n    job.add_argument(\'--data-eval\', metavar=\'path\', type=str, required=False,\n                     help=\'The data to use for evaluation. This can include wildcards.\')\n\n    # The framework uses output, but Cloud ML Engine uses job-dir. Only one should be provided.\n    job.add_argument(\'--output\', type=str, dest=\'output\', required=False,\n                     help=\'The output path to use for training outputs,\')\n    job.add_argument(\'--job-dir\', type=str, dest=\'output\', required=False,\n                     help=\'For Cloud ML Engine compatibility only. Use --output instead.\')\n\n  def _parse_optional(self, arg_string):\n    suffix_index = arg_string.find(\':\')\n    if suffix_index < 0:\n      return super(ModelArgumentsParser, self)._parse_optional(arg_string)\n\n    original_arg_string = arg_string\n    suffix = arg_string[suffix_index + 1:]\n    arg_string = arg_string[0:suffix_index]\n\n    option_tuple = super(ModelArgumentsParser, self)._parse_optional(arg_string)\n    if not option_tuple:\n      return option_tuple\n\n    action, option_string, explicit_arg = option_tuple\n    if isinstance(action, AddVarArgAction):\n      return action, suffix, explicit_arg\n    else:\n      self.exit(-1, message=\'Unknown argument %s\' % original_arg_string)\n\n\nclass AddVarArgAction(argparse.Action):\n  def __init__(self,\n               option_strings,\n               dest,\n               nargs=None,\n               const=None,\n               default=None,\n               type=None,\n               choices=None,\n               required=False,\n               help=None,\n               metavar=None):\n    super(AddVarArgAction, self).__init__(\n      option_strings=option_strings,\n      dest=dest,\n      nargs=nargs,\n      const=const,\n      default=default,\n      type=type,\n      choices=choices,\n      required=required,\n      help=help,\n      metavar=metavar)\n\n  def __call__(self, parser, namespace, values, option_string=None):\n    index = 0\n    try:\n      index = int(option_string) - 1\n    except ValueError:\n      pass\n\n    list = getattr(namespace, self.dest)\n    if list is None:\n      list = []\n      setattr(namespace, self.dest, list)\n\n    if index >= len(list):\n      list.extend([self.default] * (index + 1 - len(list)))\n    list[index] = values\n'"
src/training/_config.py,3,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _config.py\n# Implements TrainingConfig.\n\nimport json\nimport os\nimport tensorflow as tf\n\n_TASK_PARAM_SERVER = \'ps\'\n_TASK_WORKER = \'worker\'\n_TASK_MASTER = \'master\'\n\n\nclass Configuration(object):\n  """"""Contains configuration information for the training process.\n  """"""\n  def __init__(self, task, cluster, job, env):\n    """"""Initializes a TrainingConfig instance from the individual configuration objects.\n\n    Task configuration represents the current training task (for both single node and distributed\n    training), while cluster configuration represents the cluster and should be None in single\n    node training.\n    Job configuration represents any environment-specific representation of the training job,\n\n    Arguments:\n      task: current TensorFlow task configuration.\n      cluster: containing TensorFlow cluster configuration for distributed training.\n      job: environment-specific job configuration.\n      env: the environment-provided configuration information.\n    """"""\n    self._task = type(\'TaskSpec\', (object,), task)\n    self._cluster = tf.train.ClusterSpec(cluster) if cluster else None\n    self._job = type(\'JobSpec\', (object,), job)\n    self._env = env\n\n  @classmethod\n  def environment(cls):\n    """"""Creates a Configuration object for single node and distributed training.\n\n    This relies on looking up configuration from an environment variable, \'TF_CONFIG\' which allows\n    a hosting environment to configure the training process.\n    The specific environment variable is expected to be a JSON formatted dictionary containing\n    configuration about the current task, cluster and job.\n\n    Returns:\n      A Configuration instance matching the current environment.\n    """"""\n    env = json.loads(os.environ.get(\'TF_CONFIG\', \'{}\'))\n\n    # Note that the lookup for \'task\' must handle the case where it is missing, as well as when it\n    # is specified, but is empty, to support both single node and distributed training.\n\n    return cls(env.get(\'task\', None) or {\'type\': \'master\', \'index\': 0},\n               env.get(\'cluster\', None),\n               env.get(\'job\', {\'local\': True}),\n               env)\n  \n  @classmethod\n  def local(cls):\n    """"""Creates a Configuration object representing single node training in a process.\n\n    Returns:\n      A default Configuration instance with simple configuration.\n    """"""\n    return cls(task={\'type\': \'master\', \'index\': 0}, cluster=None, job={\'local\': True}, env={})\n\n  @property\n  def distributed(self):\n    """"""Determines if training being performed is distributed or is single node training.\n\n    Returns:\n      True if the configuration represents distributed training; False otherwise.\n    """"""\n    return self._cluster is not None\n\n  @property\n  def cluster(self):\n    """"""Retrieves the cluster definition containing the current node.\n\n    This is None if the current node is part of a single node training job.\n    """"""\n    return self._cluster\n\n  @property\n  def job(self):\n    """"""Retrieves the job definition of the current training job.\n    """"""\n    return self._job\n\n  @property\n  def task(self):\n    """"""Retrieves the task definition associated with the current node.\n\n    If no job information is provided, this is None.\n    """"""\n    return self._task\n\n  @property\n  def device(self):\n    """"""Retrieve the device associated with the current node.\n    """"""\n    return \'/job:%s/task:%d\' % (self._task.type, self._task.index)\n\n  @property\n  def master(self):\n    """"""Retrieves whether the current task is a master task.\n    """"""\n    return self._task.type == _TASK_MASTER\n\n  @property\n  def param_server(self):\n    """"""Retrieves whether the current task is a parameter server task.\n    """"""\n    return self._task.type == _TASK_PARAM_SERVER\n\n  @property\n  def worker(self):\n    """"""Retrieves whether the current task is a worker task.\n    """"""\n    return self._task.type == _TASK_WORKER\n\n  def create_device_setter(self, args):\n    """"""Creates the device setter, which assigns variables and ops to devices in distributed mode.\n\n    Arguments:\n      args: the arguments associated with the current job.\n    """"""\n    # TODO: Provide a way to provide a custom stragery or setter\n    return tf.train.replica_device_setter(cluster=self._cluster,\n                                          ps_device=\'/job:ps\',\n                                          worker_device=self.device)\n\n  def create_server(self):\n    """"""Creates the TensorFlow server, which is required for distributed training.\n    """"""\n    if not self.distributed:\n      return None\n    return tf.train.Server(self._cluster, self._task.type, self._task.index, protocol=\'grpc\')\n'"
src/training/_hooks.py,17,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _hooks.py\n# Implements various session hooks needed for training.\n\nimport logging\nimport os\nimport tensorflow as tf\nimport tensorfx as tfx\nimport time\nfrom tensorflow.core.framework import summary_pb2 as tfsummaries\n\n\nclass StopTrainingHook(tf.train.SessionRunHook):\n  """"""Stops training after a specified number of steps.\n  """"""\n  def __init__(self, job):\n    """"""Initializes an instance of StopTrainingHook.\n\n    Arguments:\n      job: The current training job.\n    """"""\n    self._global_steps = job.training.global_steps\n    self._max_steps = job.args.max_steps\n\n  def before_run(self, context):\n    return tf.train.SessionRunArgs(self._global_steps)\n\n  def after_run(self, context, values):\n    global_steps_completed = values.results\n    if global_steps_completed >= self._max_steps:\n      context.request_stop()\n\n\nclass LogSessionHook(tf.train.SessionRunHook):\n  """"""Logs the session loop by outputting steps, and throughput into logs.\n  """"""\n  _MESSAGE_FORMAT = \'Run: %.2f sec; Steps: %d; Duration: %d sec; Throughput: %.1f instances/sec\'\n  def __init__(self, job):\n    """"""Initializes an instance of LogSessionHook.\n\n    Arguments:\n      job: The current training job.\n    """"""\n    self._log_interval_steps = job.args.log_interval_steps\n    self._batch_size = job.args.batch_size\n\n    self._start_time = time.time()\n    self._steps_completed = 0\n    self._step_start_time = 0\n\n  def before_run(self, context):\n    self._step_start_time = time.time()\n\n  def after_run(self, context, values):\n    self._steps_completed += 1\n\n    if self._steps_completed == 1 or \\\n       self._steps_completed % self._log_interval_steps == 0:\n      end_time = time.time()\n      run_time = end_time - self._step_start_time\n      duration = end_time - self._start_time\n      throughput = self._steps_completed * float(self._batch_size) / float(duration)\n\n      logging.info(LogSessionHook._MESSAGE_FORMAT,\n                   run_time, self._steps_completed, duration, throughput)\n\n\nclass LogTrainingHook(tf.train.SessionRunHook):\n  """"""Logs the training job by logging progress as well as producing summary events.\n  """"""\n  _MESSAGE_FORMAT = \'Global steps: %d; Duration: %d sec; Throughput: %.1f instances/sec; Loss: %.3f\'\n  def __init__(self, job):\n    """"""Initializes an instance of LogTrainingHook.\n\n    Arguments:\n      job: The current training job.\n    """"""\n    self._global_steps = job.training.global_steps\n    self._loss = job.training.loss\n    self._summary_op = job.training.summary_op\n\n    self._log_interval_steps = job.args.log_interval_steps\n    self._max_steps = job.args.max_steps\n    self._batch_size = job.args.batch_size\n\n    self._summary_writer = tf.summary.FileWriter(job.summaries_path(\'train\'))\n    self._summary_writer.add_graph(job.training.graph)\n\n    self._start_time = time.time()\n    self._global_steps_completed = 0\n\n  def before_run(self, context):\n    current_step = self._global_steps_completed + 1\n    if (current_step % self._log_interval_steps == 0) or \\\n       (current_step + 1 >= self._max_steps):\n      return tf.train.SessionRunArgs([self._global_steps, self._loss, self._summary_op])\n    else:\n      return tf.train.SessionRunArgs([self._global_steps])\n\n  def after_run(self, context, values):\n    if len(values.results) == 1:\n      self._global_steps_completed, = values.results\n    else:\n      self._global_steps_completed, loss_value, summary = values.results\n\n      end_time = time.time()\n      duration = end_time - self._start_time\n      throughput = self._global_steps_completed * float(self._batch_size) / float(duration)\n\n      logging.info(LogTrainingHook._MESSAGE_FORMAT,\n                   self._global_steps_completed, duration, throughput, loss_value)\n\n      self._summary_writer.add_summary(summary, self._global_steps_completed)\n      _log_summary_value(self._summary_writer, \'metrics/throughput\', throughput,\n                         self._global_steps_completed)\n      self._summary_writer.flush()\n\n\nclass SaveCheckpointHook(tf.train.SessionRunHook):\n  """"""Saves checkpoints during training, evaluates them, and exports the final checkpoint as a model.\n\n  This should only be used in master tasks.\n  """"""\n  _MESSAGE_FORMAT = \'Global steps: %d; Evaluation metric: %.3f\'\n  def __init__(self, job):\n    """"""Initializes an instance of SaveCheckpointHook.\n\n    Arguments:\n      job: The current training job.\n    """"""\n    self._job = job\n\n    self._global_steps = job.training.global_steps\n    self._saver = job.training.saver\n\n    self._checkpoint_interval_secs = job.args.checkpoint_interval_secs\n\n    self._checkpoint_name = os.path.join(job.checkpoints_path, \'model.ckpt\')\n\n    self._last_save_time = time.time()\n    self._last_save_steps = 0\n\n    self._summary_writer = tf.summary.FileWriter(job.summaries_path(\'eval\'))\n    self._summary_writer.add_graph(job.evaluation.graph)\n\n  def before_run(self, context):\n    # Save a checkpoint after the first step (this produces early evaluation results), as well as,\n    # every checkpoint interval.\n    if self._last_save_steps == 0 or \\\n       time.time() - self._last_save_time >= self._checkpoint_interval_secs:\n      return tf.train.SessionRunArgs([self._global_steps])\n\n  def after_run(self, context, values):\n    if values.results:\n      global_steps_completed, = values.results\n      checkpoint = self._saver.save(context.session, self._checkpoint_name, global_steps_completed)\n      self._evaluate(checkpoint, global_steps_completed)\n\n      self._last_save_steps = global_steps_completed\n      self._last_save_time = time.time()\n\n  def end(self, session):\n    global_steps_completed = session.run(self._global_steps)\n    if global_steps_completed != self._last_save_steps:\n      checkpoint = self._saver.save(session, self._checkpoint_name, global_steps_completed)\n      self._evaluate(checkpoint, global_steps_completed)\n      self._export(checkpoint)\n\n  def _evaluate(self, checkpoint, global_steps_completed):\n    with self._job.evaluation.graph.as_default():\n      with tf.Session() as session:\n        self._job.evaluation.init_op.run()\n        self._job.evaluation.saver.restore(session, checkpoint)\n        self._job.evaluation.local_init_op.run()\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n\n        try:\n          while not coord.should_stop():\n            session.run(self._job.evaluation.eval_op)\n        except tf.errors.OutOfRangeError:\n          # Ignore the error raised at the end of an epoch of eval data.\n          pass\n        finally:\n          coord.request_stop()\n        coord.join(threads)\n\n        metric_value = session.run(self._job.evaluation.metric)\n\n        summary = session.run(self._job.evaluation.summary_op)\n        self._summary_writer.add_summary(summary, global_steps_completed)\n        self._summary_writer.flush()\n\n        logging.info(SaveCheckpointHook._MESSAGE_FORMAT, global_steps_completed, metric_value)\n\n  def _export(self, checkpoint):\n    summary_writer = tf.summary.FileWriter(self._job.summaries_path(\'prediction\'))\n    summary_writer.add_graph(self._job.prediction.graph)\n    summary_writer.close()\n\n    with self._job.prediction.graph.as_default():\n      with tf.Session() as session:\n        self._job.prediction.init_op.run()\n        self._job.prediction.saver.restore(session, checkpoint)\n        self._job.prediction.local_init_op.run()\n\n        tfx.prediction.Model.save(session, self._job.model_path,\n                                  self._job.prediction.inputs, self._job.prediction.outputs)\n\n\nclass CheckNaNLossHook(tf.train.SessionRunHook):\n  """"""Checks for NaN loss values to stop or abort training.\n  """"""\n  # TODO: Implement this\n  pass\n\n\ndef _log_summary_value(summary_writer, tag, value, global_steps):\n  summary_value = tfsummaries.Summary.Value(tag=tag, simple_value=value)\n  summary = tfsummaries.Summary(value=[summary_value])\n\n  summary_writer.add_summary(summary, global_steps)\n'"
src/training/_job.py,1,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _job.py\n# Implements Job.\n\nimport os\nimport logging\nimport yaml\nimport sys\nimport tensorflow as tf\nfrom tensorflow.python.lib.io import file_io as tfio\n\nclass Job(object):\n  """"""Represents a training job.\n  """"""\n  def __init__(self, model_builder, inputs, output, config):\n    """"""Initializes a Job instance.\n\n    Arguments:\n      model_builder: the ModelBuilder associated with the job.\n      inputs: the input dataset for the job.\n      output: the output path of the job.\n      config: the Training configuration.\n    """"""\n    self._model_builder = model_builder\n    self._inputs = inputs\n    self._output = output\n    self._config = config\n\n  @property\n  def model_builder(self):\n    """"""Retrieves the ModelBuilder being used to build model graphs.\n    """"""\n    return self._model_builder\n\n  @property\n  def args(self):\n    """"""Retrieves the arguments associated with the job.\n    """"""\n    return self._model_builder.args\n\n  @property\n  def inputs(self):\n    """"""Retrieves the input dataset of the job.\n    """"""\n    return self._inputs\n\n  @property\n  def output_path(self):\n    """"""Retrieves the output path of the job.\n    """"""\n    return self._output\n\n  @property\n  def checkpoints_path(self):\n    """"""Retrieves the checkpoints path within the output path.\n    """"""\n    return os.path.join(self._output, \'checkpoints\')\n\n  @property\n  def model_path(self):\n    """"""Retrieves the model path within the output path.\n    """"""\n    return os.path.join(self._output, \'model\')\n\n  def summaries_path(self, summary):\n    """"""Retrieves the summaries path within the output path.\n\n    Arguments:\n      summary: the type of summary.\n    """"""\n    return os.path.join(self._output, \'summaries\', summary)\n\n  @property\n  def training(self):\n    """"""Retrieves the training graph interface for the job.\n    """"""\n    return self._training\n\n  @property\n  def evaluation(self):\n    """"""Retrieves the evaluation graph interface for the job.\n    """"""\n    return self._evaluation\n\n  @property\n  def prediction(self):\n    """"""Retrieves the prediction graph interface for the job.\n    """"""\n    return self._prediction\n\n  def configure_logging(self):\n    """"""Initializes the loggers for the job.\n    """"""\n    args = self._model_builder.args\n\n    tf.logging.set_verbosity(args.log_level_tensorflow)\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = str(args.log_level_tensorflow)\n\n    logger = logging.getLogger()\n    if hasattr(self._config.job, \'local\') and not logger.handlers:\n      # Additional setup to output logs to console for local runs. On cloud, this is handled by the\n      # environment. The additional check for existing logging handler ensures that existing logging\n      # setup is used; for example, when training is invoked in context of another application.\n      if self._config.distributed:\n        format = \'%%(levelname)s %s:%d: %%(message)s\'\n        format = format % (self._config.task.type, self._config.task.index)\n      else:\n        format = \'%(levelname)s: %(message)s\'\n      \n      handler = logging.StreamHandler(stream=sys.stderr)\n      handler.setFormatter(logging.Formatter(fmt=format))\n\n      logger.addHandler(handler)\n      logger.setLevel(args.log_level)\n\n  def start(self):\n    """"""Performs startup logic, including building graphs.\n    """"""\n    if self._config.master:\n      # Save out job information for later reference alongside all other outputs.\n      job_args = \' \'.join(self._model_builder.args._args).replace(\' --\', \'\\n--\').split(\'\\n\')\n      job_info = {\n        \'config\': self._config._env,\n        \'args\': job_args\n      }\n      job_spec = yaml.safe_dump(job_info, default_flow_style=False)\n      job_file = os.path.join(self._output, \'job.yaml\')\n\n      tfio.recursive_create_dir(self._output)\n      tfio.write_string_to_file(job_file, job_spec)\n\n      # Create a checkpoints directory. This is needed to ensure checkpoint restoration logic\n      # can lookup an existing directory.\n      tfio.recursive_create_dir(self.checkpoints_path)\n\n    # Build the graphs that will be used during the course of the job.\n    self._training, self._evaluation, self._prediction = \\\n      self._model_builder.build_graph_interfaces(self._inputs, self._config)\n'"
src/training/_model.py,43,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _model.py\n# Implements the ModelBuilder base class.\n\nimport tensorflow as tf\nimport tensorfx as tfx\nfrom ._args import ModelArguments\n\n\ndef _create_interface(phase, graph, references):\n  """"""Creates an interface instance using a dynamic type with graph and references as attributes.\n  """"""\n  interface = {\'graph\': graph}\n  interface.update(references)\n\n  return type(phase + \'Interface\', (object,), interface)\n\n\nclass ModelBuilder(object):\n  """"""Builds model graphs for different phases: training, evaluation and prediction.\n\n  A model graph is an interface that encapsulates a TensorFlow graph, and references to tensors and\n  ops within that graph.\n\n  A ModelBuilder serves as a base class for various models. Each specific model adds its specific\n  logic to build the required TensorFlow graph.\n  """"""\n  def __init__(self, args):\n    """"""Initializes an instance of a ModelBuilder.\n\n    Arguments:\n      args: the arguments specified for training.\n    """"""\n    if args is None or not isinstance(args, ModelArguments):\n      raise ValueError(\'args must be an instance of ModelArguments\')\n\n    self._args = args\n\n  @property\n  def args(self):\n    """"""Retrieves the set of arguments specified for training.\n    """"""\n    return self._args\n\n  def build_graph_interfaces(self, dataset, config):\n    """"""Builds graph interfaces for training and evaluating a model, and for predicting using it.\n\n    A graph interface is an object containing a TensorFlow graph member, as well as members\n    corresponding to various tensors and ops within the graph.\n\n    Arguments:\n      dataset: The dataset to use during training.\n      config: The training Configuration object.\n    Returns:\n      A tuple consisting of the training, evaluation and prediction interfaces.\n    """"""\n    with tf.Graph().as_default() as graph:\n      with tf.device(config.create_device_setter(self._args)):\n        references = self.build_training_graph(dataset)\n        training = _create_interface(\'Training\', graph, references)\n\n    with tf.Graph().as_default() as graph:\n      references = self.build_evaluation_graph(dataset)\n      evaluation = _create_interface(\'Evaluation\', graph, references)\n\n    with tf.Graph().as_default() as graph:\n      references = self.build_prediction_graph(dataset)\n      prediction = _create_interface(\'Prediction\', graph, references)\n\n    return training, evaluation, prediction\n\n  def build_training_graph(self, dataset):\n    """"""Builds the graph to use for training a model.\n\n    This operates on the current default graph.\n\n    Args:\n      dataset: The dataset to use during training.\n    Returns:\n      The set of tensors and ops references required for training.\n    """"""\n    with tf.name_scope(\'input\'):\n      # For training, ensure the data is shuffled, and don\'t limit to any fixed number of epochs.\n      # The datasource to use is the one named as \'train\' within the dataset.\n      inputs = self.build_input(dataset, \'train\',\n                                batch=self.args.batch_size,\n                                epochs=self.args.epochs,\n                                shuffle=True)\n    \n    with tf.name_scope(\'inference\'):\n      inferences = self.build_inference(inputs, training=True)\n\n    with tf.name_scope(\'train\'):\n      # Global steps is marked as trainable (explicitly), so as to have it be saved into checkpoints\n      # for the purposes of resumed training.\n      global_steps = tf.Variable(0, name=\'global_steps\', dtype=tf.int64, trainable=True,\n                                 collections=[tf.GraphKeys.GLOBAL_VARIABLES,\n                                              tf.GraphKeys.GLOBAL_STEP,\n                                              tf.GraphKeys.TRAINABLE_VARIABLES])\n      loss, train_op = self.build_training(global_steps, inputs, inferences)\n\n    with tf.name_scope(\'initialization\'):\n      # Create the saver that will be used to save and restore (in cases of resumed training)\n      # trained variables.\n      saver = tf.train.Saver(tf.trainable_variables(), sharded=True)\n\n      init_op, local_init_op = self.build_init()\n      ready_op = tf.report_uninitialized_variables(tf.trainable_variables())\n\n    # Create the summary op that will merge all summaries across all sub-graphs\n    summary_op = tf.summary.merge_all()\n\n    scaffold = tf.train.Scaffold(init_op=init_op,\n                                 local_init_op=local_init_op,\n                                 ready_op=ready_op,\n                                 ready_for_local_init_op=ready_op,\n                                 summary_op=summary_op,\n                                 saver=saver)\n    scaffold.finalize()\n\n    return {\n      \'global_steps\': global_steps,\n      \'loss\': loss,\n      \'init_op\': init_op,\n      \'local_init_op\': local_init_op,\n      \'ready_op\': ready_op,\n      \'train_op\': train_op,\n      \'summary_op\': summary_op,\n      \'saver\': saver,\n      \'scaffold\': scaffold\n    }\n\n  def build_evaluation_graph(self, dataset):\n    """"""Builds the graph to use for evaluating a model during training.\n\n    Args:\n      dataset: The dataset to use during training.\n    Returns:\n      The set of tensors and ops references required for evaluation.\n    """"""\n    with tf.name_scope(\'input\'):\n      # For evaluation, compute the eval metric over a single pass over the evaluation data,\n      # and avoid any overhead from shuffling.\n      # The datasource to use is the one named as \'eval\' within the dataset.\n      inputs = self.build_input(dataset, \'eval\', batch=1, epochs=1, shuffle=False)\n\n    with tf.name_scope(\'inference\'):\n      inferences = self.build_inference(inputs, training=False)\n\n    with tf.name_scope(\'output\'):\n      outputs = self.build_output(inputs, inferences)\n\n    with tf.name_scope(\'evaluation\'):\n      metric, eval_op = self.build_evaluation(inputs, outputs)\n\n    with tf.name_scope(\'initialization\'):\n      # Create the saver that will be used to restore trained variables,\n      saver = tf.train.Saver(tf.trainable_variables(), sharded=True)\n\n      init_op, local_init_op = self.build_init()\n\n    # Create the summary op that will merge all summaries across all sub-graphs\n    summary_op = tf.summary.merge_all()\n\n    return {\n      \'metric\': metric,\n      \'init_op\': init_op,\n      \'local_init_op\': local_init_op,\n      \'eval_op\': eval_op,\n      \'summary_op\': summary_op,\n      \'saver\': saver\n    }\n\n  def build_prediction_graph(self, dataset):\n    """"""Builds the graph to use for predictions with the trained model.\n\n    Args:\n      dataset: The dataset to use during training.\n    Returns:\n      The set of tensors and ops references required for prediction.\n    """"""\n    with tf.name_scope(\'input\'):\n      inputs = self.build_input(dataset, source=None, batch=0, epochs=0, shuffle=False)\n\n    with tf.name_scope(\'inference\'):\n      inferences = self.build_inference(inputs, training=False)\n\n    with tf.name_scope(\'output\'):\n      outputs = self.build_output(inputs, inferences)\n\n    with tf.name_scope(\'initialization\'):\n      # Create the saver that will be used to restore trained variables.\n      saver = tf.train.Saver(tf.trainable_variables(), sharded=True)\n\n      init_op, local_init_op = self.build_init()\n\n    graph_inputs = tf.get_collection(\'inputs\')\n    if len(graph_inputs) != 1 or graph_inputs[0].dtype != tf.string:\n      raise Exception(\'Invalid prediction graph. Must have a single string input.\')\n\n    graph_outputs = tf.get_collection(\'outputs\')\n    if len(graph_outputs) == 0:\n      raise Exception(\'Invalid prediction graph. Must have at least one output.\')\n\n    return {\n      \'init_op\': init_op,\n      \'local_init_op\': local_init_op,\n      \'saver\': saver,\n      \'inputs\': graph_inputs,\n      \'outputs\': graph_outputs\n    }\n\n  def build_init(self):\n    """"""Builds the initialization sub-graph.\n\n    The default implementation creates an initialization op that initializes all variables,\n    locals for initialization, and another for all non-traininable variables and tables for local\n    initialization.\n\n    Initialization is run when the graph is first created, before training. Local initialization is\n    performed after a previously trained model is loaded.\n\n    Returns:\n      A tuple containing the init op and local init op to use to initialize the graph.\n    """"""\n    init_op = tf.variables_initializer(tf.global_variables(), name=\'init\')\n\n    # For some reason not all local variables are in the local variables collection, but some are in\n    # the global variables collection (such as those setup by reader ops).\n    # So in addition to initializing local variables in the local_init_op, we also initialize the\n    # set of variables in the global variables, that are not trainable.\n    # Just to add to the mix, tables are neither, and so must be explicitly included as well.\n    # All of these will be initialized after restoring from a checkpoint.\n    variables = tf.global_variables()\n    for trainable in tf.trainable_variables():\n      variables.remove(trainable)\n\n    local_init_op = tf.group(tf.variables_initializer(variables),\n                             tf.variables_initializer(tf.local_variables()),\n                             tf.tables_initializer(),\n                             name=\'local_init_op\')\n\n    # Add the local initialization op to the main op collection, which is looked up at model loading\n    # time, and is automatically invoked after it has been loaded.\n    tf.add_to_collection(\'saved_model_main_op\', local_init_op)\n\n    return init_op, local_init_op\n\n  def build_input(self, dataset, source, batch, epochs, shuffle):\n    """"""Builds the input sub-graph.\n\n    Arguments:\n      dataset: the dataset representing the inputs to the training.\n      source: the name of data source to use for input (for training and evaluation).\n      batch: the number of instances to read per batch.\n      epochs: the number of passes over the data.\n      shuffle: whether to shuffle the data.\n    Returns:\n      A dictionary of tensors key\'ed by feature names.\n    """"""\n    prediction = False\n    if source:\n      with tf.name_scope(\'read\'):\n        instances = dataset[source].read(batch=batch, shuffle=shuffle, epochs=epochs)\n    else:\n      prediction = True\n      instances = tf.placeholder(dtype=tf.string, shape=(None,), name=\'instances\')\n      tf.add_to_collection(\'inputs\', instances)\n\n    with tf.name_scope(\'parse\'):\n      parsed_instances = dataset.parse_instances(instances, prediction)\n\n    if dataset.features:\n      with tf.name_scope(\'transform\'):\n        transformer = tfx.data.Transformer(dataset)\n        return transformer.transform(parsed_instances)\n    else:\n      return parsed_instances\n\n  def build_inference(self, inputs, training):\n    """"""Builds the inference sub-graph.\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n      training: whether the inference sub-graph is being built for the training graph.\n    Returns:\n      The inference values.\n    """"""\n    raise NotImplementedError(\'build_inference must be implemented in a derived class.\')\n\n  def build_training(self, global_steps, inputs, inferences):\n    """"""Builds the training sub-graph.\n\n    Arguments:\n      global_steps: the global steps variable to use.\n      inputs: the dictionary of tensors corresponding to the input.\n      inferences: the inference values.\n    Returns:\n      The loss tensor, and the training op.\n    """"""\n    raise NotImplementedError(\'build_training must be implemented in a derived class.\')\n\n  def build_output(self, inputs, inferences):\n    """"""Builds the output sub-graph\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n      inferences: the inference values.\n    Returns:\n      A dictionary consisting of the output prediction tensors.\n    """"""\n    raise NotImplementedError(\'build_output must be implemented in a derived class.\')\n\n  def build_evaluation(self, inputs, outputs):\n    """"""Builds the evaluation graph.abs\n\n    Arguments:\n      inputs: the dictionary of tensors corresponding to the input.\n      outputs: the dictionary containing output tensors.\n    Returns:\n      The eval metric tensor and the eval op.\n    """"""\n    raise NotImplementedError(\'build_evaluation must be implemented in a derived class.\')\n'"
src/training/_trainer.py,4,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _trainer.py\n# Implements Trainer.\n\nimport tensorflow as tf\nimport tensorfx as tfx\nfrom _config import Configuration\nfrom _hooks import *\nfrom _job import Job\n\n\nclass ModelTrainer(object):\n  """"""Provides the functionality to train a model during a training job.\n  """"""\n  def __init__(self, config=None):\n    """"""Initializes a ModelTrainer instance.\n\n    Arguments:\n      config: an optional configuration providing information about the training job and cluster.\n    """"""\n    if not config:\n      # By default, use the configuration specified in the TF_CONFIG environment variable.\n      config = Configuration.environment()\n\n    self._config = config\n\n  @property\n  def config(self):\n    """"""Retrieves the training configuration.\n    """"""\n    return self._config\n\n  def train(self, model_builder, inputs, output):\n    """"""Runs the training process to train a model.\n\n    Arguments:\n      model_builder: the ModelBuilder to use to build graphs during training.\n      inputs: the input dataset for the job.\n      output: the output path for the job.\n    Returns:\n      The trained Model. The resulting value is only relevant for master nodes.\n    """"""\n    job = Job(model_builder, inputs, output, self._config)\n    job.configure_logging()\n\n    server = self._config.create_server()\n    if server and self._config.param_server:\n      return self._run_ps(server)\n\n    return self._run_training(server, job)\n\n  def _run_ps(self, server):\n    """"""Runs the parameter server task.\n\n    A ps task runs forever (until killed) using implementation within TensorFlow runtime.\n    """"""\n    try:\n      server.join()\n    except AbortError:\n      pass\n\n  def _run_training(self, server, job):\n    """"""Runs the worker and master tasks.\n\n    Worker and master tasks create a TensorFlow session, and run the session loop. The session\n    loop is customized via session hooks. A worker simply runs the training logic, while a master\n    is also responsible for producing and evaluating checkpoints, as well producing summary event\n    logs, and finally exporting the trained model.\n    """"""\n    job.start()\n\n    with job.training.graph.as_default() as graph:\n      master = server.target if server else \'\'\n      config = self._create_session_config(job)\n      hooks = self._create_session_hooks(job)\n\n      if self._config.master:\n        session_creator = tf.train.ChiefSessionCreator(job.training.scaffold,\n                                                       master, config, job.checkpoints_path)\n      else:\n        session_creator = tf.train.WorkerSessionCreator(job.training.scaffold, master, config)\n\n      with tf.train.MonitoredSession(session_creator, hooks) as session:\n        while not session.should_stop():\n          # TODO: Add session run timeouts\n          session.run(job.training.train_op)\n\n      if self._config.master:\n        return tfx.prediction.Model.load(job.model_path)\n      else:\n        return None\n\n  def _create_session_config(self, job):\n    """"""Creates the TensorFlow session config object.\n    """"""\n    if self._config.local:\n      # Don\'t have each process (esp. in case of distributed simulation) on the local machine to\n      # attempt using all CPUs\n      parallelism = 1\n    else:\n      # Use default\n      parallelism = 0\n\n    # Limit communication to specific devices. Specifically the goal is to disable communications\n    # across workers, so as to increase performance and reliability.\n    device_filters = [\'/job:ps\', self._config.device]\n\n    return tf.ConfigProto(log_device_placement=job.args.log_device_placement,\n                          device_filters=device_filters,\n                          intra_op_parallelism_threads=parallelism,\n                          inter_op_parallelism_threads=parallelism)\n\n  def _create_session_hooks(self, job):\n    """"""Creates the TensorFlow session hooks that customize the session loop.\n    """"""\n    hooks = []\n\n    hooks.append(LogSessionHook(job))\n    if self._config.master:\n      hooks.append(LogTrainingHook(job))\n      hooks.append(SaveCheckpointHook(job))\n    hooks.append(StopTrainingHook(job))\n\n    return hooks\n'"
tests/data/__init__.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# Tests functionality in the tensorfx data module\n\n'"
tests/data/dataset_tests.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# dataset_tests.py\n# Tests Dataset related functionality in tensorfx.data.\n\nimport unittest\nimport tensorfx as tfx\n\n\nclass TestCases(unittest.TestCase):\n\n  def test_empty_dataset(self):\n    schema = tfx.data.Schema.create(tfx.data.SchemaField.integer(\'x\'))\n    ds = tfx.data.DataSet({}, schema, None, None)\n\n    self.assertEqual(len(ds), 0)\n\n  def test_create_dataset(self):\n    schema = tfx.data.Schema.create(tfx.data.SchemaField.integer(\'x\'))\n    source = tfx.data.DataSource()\n    ds = tfx.data.DataSet({\'foo\': source}, schema, None, None)\n\n    self.assertEqual(ds[\'foo\'], source)\n\n  def test_create_multi_source_dataset(self):\n    schema = tfx.data.Schema.create(tfx.data.SchemaField.integer(\'x\'),\n                                    tfx.data.SchemaField.integer(\'y\'))\n    train = tfx.data.CsvDataSource(\'...\')\n    eval = tfx.data.CsvDataSource(\'...\')\n\n    ds = tfx.data.CsvDataSet(schema, train=train, eval=eval)\n\n    self.assertEqual(ds[\'train\'], train)\n    self.assertEqual(ds[\'eval\'], eval)\n    self.assertEqual(len(ds), 2)\n    self.assertListEqual(ds.sources, [\'train\', \'eval\'])\n'"
tests/data/features_tests.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# schema_tests.py\n# Tests FeatureSet related functionality in tensorfx.data\n\nimport unittest\nimport tensorfx as tfx\n\n\nclass TestCases(unittest.TestCase):\n\n  def test_create_featureset(self):\n    t = tfx.data.Feature.target(\'t\', \'t\')\n    x = tfx.data.Feature.identity(\'x\', \'x\')\n    features = tfx.data.FeatureSet.create(t, x)\n\n    self.assertEqual(len(features), 2)\n    self.assertEqual(features[\'t\'], t)\n\n  def test_parse_featureset(self):\n    spec = """"""\n    features:\n    - name: target\n      type: target\n      fields: c1\n    - name: f1\n      type: identity\n      fields: c3\n    """"""\n    features = tfx.data.FeatureSet.parse(spec)\n\n    self.assertEqual(len(features), 2)\n    self.assertEqual(features[\'target\'].fields[0], \'c1\')\n    self.assertEqual(features[\'target\'].type, tfx.data.FeatureType.target)\n    self.assertEqual(features[\'f1\'].type, tfx.data.FeatureType.identity)\n    self.assertEqual(features[\'f1\'].fields, [\'c3\'])\n'"
tests/data/schema_tests.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# schema_tests.py\n# Tests Schema related functionality in tensorfx.data\n\nimport unittest\nimport tensorfx as tfx\n\n\nclass TestCases(unittest.TestCase):\n\n  def test_create_single_field_schema(self):\n    f = tfx.data.SchemaField.integer(\'n\')\n    schema = tfx.data.Schema.create(f)\n\n    self.assertEqual(len(schema), 1)\n    self.assertEqual(schema[\'n\'], f)\n    self.assertEqual(schema[0], f)\n\n  def test_create_multi_field_schema(self):\n    f1 = tfx.data.SchemaField.integer(\'n\')\n    f2 = tfx.data.SchemaField.discrete(\'t\')\n    schema = tfx.data.Schema.create(f1, f2)\n\n    self.assertEqual(len(schema), 2)\n    self.assertEqual(schema[\'n\'], f1)\n    self.assertEqual(schema[1], f2)\n\n  def test_parse_schema(self):\n    spec = """"""\n    fields:\n    - name: f1\n      type: integer\n    - name: f2\n      type: real\n    - name: f3\n      type: discrete\n    """"""\n    schema = tfx.data.Schema.parse(spec)\n\n    self.assertEqual(len(schema), 3)\n    self.assertEqual(schema[0].name, \'f1\')\n    self.assertEqual(schema[\'f1\'].type, tfx.data.SchemaFieldType.integer)\n    self.assertEqual(schema[\'f2\'].type, tfx.data.SchemaFieldType.real)\n    self.assertEqual(schema[\'f3\'].type, tfx.data.SchemaFieldType.discrete)\n    self.assertEqual(schema.fields, [\'f1\', \'f2\', \'f3\'])\n  '"
tests/training/__init__.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# Tests functionality in the tensorfx training module\n\n'"
tests/training/config_tests.py,0,"b'# Copyright 2016 TensorLabs. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# config_tests.py\n# Tests config related functionality in tensorfx.\n\nimport json\nimport os\nimport unittest\n\nimport tensorfx as tfx\n\nclass TestCases(unittest.TestCase):\n\n  def test_local_config(self):\n    config = tfx.training.Configuration.local()\n\n    self.assertFalse(config.distributed)\n    self.assertIsNone(config.cluster)\n    self.assertIsNotNone(config.task)\n    self.assertEqual(config.task.type, \'master\')\n    self.assertTrue(config.master)\n\n  def test_empty_env_config(self):\n    config = tfx.training.Configuration.environment()\n\n    self.assertFalse(config.distributed)\n    self.assertIsNone(config.cluster)\n    self.assertIsNotNone(config.task)\n    self.assertEqual(config.task.type, \'master\')\n    self.assertTrue(config.master)\n\n  def test_env_config(self):\n    config = {\n      \'task\': {\n        \'type\': \'master\',\n        \'index\': 0\n      },\n      \'cluster\': {\n        \'hosts\': []\n      }\n    }\n    os.environ[\'TF_CONFIG\'] = json.dumps(config)\n\n    config = tfx.training.Configuration.environment()\n\n    self.assertTrue(config.distributed)\n    self.assertIsNotNone(config.cluster)\n    self.assertIsNotNone(config.task)\n    self.assertEqual(config.task.type, \'master\')\n    self.assertTrue(config.master)\n\n'"
samples/iris/trainer/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# Defines the iris trainer module.\n'"
samples/iris/trainer/csv.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# csv.py\n# Implements the iris classification training job using csv data.\n\nimport tensorfx as tfx\nimport tensorfx.models.nn as nn\n\nargs = nn.FeedForwardClassificationArguments.parse(parse_job=True)\ndataset = tfx.data.CsvDataSet(args.data_schema,\n                              train=args.data_train,\n                              eval=args.data_eval,\n                              metadata=args.data_metadata,\n                              features=args.data_features)\n\nclassification = nn.FeedForwardClassification(args)\n\ntrainer = tfx.training.ModelTrainer()\ntrainer.train(classification, dataset, args.output)\n'"
samples/iris/trainer/df.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# run.py\n# Demonstrates a standalone client that uses in-memory data (using a pandas DataFrame), in-code\n# definition of the dataset schema, features and the model, training, and finally predictions.\n\nimport json\nimport pandas as pd\nimport tensorfx as tfx\nimport tensorfx.models.nn as nn\n\n\ndef create_dataset():\n  """"""Programmatically build the DataSet\n  """"""\n  # Load data into DataFrame objects.\n  columns = [\'species\', \'petal_length\', \'petal_width\', \'sepal_length\', \'sepal_width\']\n  df_train = pd.read_csv(\'data/train.csv\', names=columns)\n  df_eval = pd.read_csv(\'data/eval.csv\', names=columns)\n\n  df_train[\'species\'] = df_train[\'species\'].astype(\'category\')\n  df_eval[\'species\'] = df_eval[\'species\'].astype(\'category\')\n\n  # NOTE: Ordinarily, this would be specified in YAML configuration, but defined in code to\n  # demonstrate the programmatic interface to FeatureSet and Feature objects. This is equivalent\n  # to features.yaml.\n  features = [\n    tfx.data.Feature.concatenate(\'X\',\n      tfx.data.Feature.scale(\'pl\', \'petal_length\'),\n      tfx.data.Feature.scale(\'pw\', \'petal_width\'),\n      tfx.data.Feature.scale(\'sl\', \'sepal_length\'),\n      tfx.data.Feature.scale(\'sl\', \'sepal_width\')),\n    tfx.data.Feature.target(\'Y\', \'species\')\n  ]\n\n  return tfx.data.DataFrameDataSet(features=tfx.data.FeatureSet.create(features),\n                                   train=df_train, eval=df_eval)\n\n\ndef create_args():\n  """"""Programmatically create the arguments.\n  """"""\n  # Build the arguments (programmatically starting with defaults, instead of parsing the\n  # program\'s command-line flags using parse().\n  args = nn.FeedForwardClassificationArguments.default()\n  args.batch_size = 5\n  args.max_steps = 2000\n  args.checkpoint_interval_secs = 1\n  args.hidden_layers = [(\'l1\', 20, \'relu\'), (\'l2\', 10, \'relu\')]\n\n  return args\n\n\ndef main():\n  args = create_args()\n  dataset = create_dataset()\n\n  # Define the model and the trainer to train the model\n  classification = nn.FeedForwardClassification(args)\n  trainer = tfx.training.ModelTrainer()\n\n  # Train; since this is training in-process (i.e. by default single node training), the training\n  # process is run as the \'master\' node, which happens to load and return the exported model that\n  # can conveniently be used to produce predictions.\n  print \'Training...\'\n  model = trainer.train(classification, dataset, output=\'/tmp/tensorfx/iris/df\')\n\n  # Predict; predictions are returned as a set of dictionaries, in the same order as the input\n  # instances.\n  print \'Predicting...\'\n  instances = [\n    \'6.3,3.3,6,2.5\',   # virginica\n    \'4.4,3,1.3,0.2\',   # setosa\n    \'6.1,2.8,4.7,1.2\'  # versicolor\n  ]\n  predictions = model.predict(instances)\n\n  # Print out instances and corresponding predictions\n  print \'\'\n  for instance, prediction in zip(instances, predictions):\n    print \'%s -> %s\\n\' % (instance, json.dumps(prediction, indent=2))\n\n\nif __name__ == \'__main__\':\n  main()\n'"
samples/iris/trainer/examples.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# examples.py\n# Implements the iris classification training job using examples data.\n\nimport tensorfx as tfx\nimport tensorfx.models.nn as nn\n\nargs = nn.FeedForwardClassificationArguments.parse(parse_job=True)\ndataset = tfx.data.ExamplesDataSet(args.data_schema,\n                                   train=args.data_train,\n                                   eval=args.data_eval,\n                                   metadata=args.data_metadata,\n                                   features=args.data_features)\n\nclassification = nn.FeedForwardClassification(args)\n\ntrainer = tfx.training.ModelTrainer()\ntrainer.train(classification, dataset, args.output)\n'"
src/models/nn/__init__.py,0,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# __init__.py\n# tensorfx.models.nn module declaration.\n\nfrom ._ff import FeedForwardClassificationArguments, FeedForwardClassification\n'"
src/models/nn/_ff.py,41,"b'# Copyright 2016 TensorLab. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except\n# in compliance with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software distributed under the License\n# is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n# or implied. See the License for the specific language governing permissions and limitations under\n# the License.\n\n# _ff.py\n# Implements FeedForwardClassification.\n\nimport math\nimport tensorflow as tf\nimport tensorfx as tfx\nimport tensorfx.models as models\n\n\ndef _init_parser(parser):\n  """"""Initializes the parser for feed-forward models.\n  """"""\n  optimization = parser.add_argument_group(title=\'Optimization\',\n    description=\'Arguments determining the optimizer behavior.\')\n  optimization.add_argument(\'--learning-rate\', metavar=\'rate\', type=float, default=0.01,\n                            help=\'The magnitude of learning to perform at each step.\')\n\n  nn = parser.add_argument_group(title=\'Neural Network\',\n    description=\'Arguments controlling the structure of the neural network.\')\n  nn.add_argument(\'--hidden-layers\', metavar=\'units\', type=int, required=False,\n                  action=parser.var_args_action,\n                  help=\'The size of each hidden layer to add.\')\n\ndef _process_args(args):\n  """"""Processes arguments for feed-forward models.\n  """"""\n  if args.hidden_layers:\n    args.hidden_layers = map(lambda (i, s): (\'layer_%d\' % i, s, \'relu\'),\n                            enumerate(args.hidden_layers))\n  else:\n    args.hidden_layers = []\n\n  args.optimizer = tf.train.GradientDescentOptimizer(args.learning_rate)\n\n\nclass FeedForwardClassificationArguments(models.ClassificationModelArguments):\n  """"""Arguments for feed-forward classification neural networks.\n  """"""\n  @classmethod\n  def init_parser(cls, parser):\n    """"""Initializes the argument parser.\n\n    Args:\n      parser: An argument parser instance to be initialized with arguments.\n    """"""\n    super(FeedForwardClassificationArguments, cls).init_parser(parser)\n    _init_parser(parser)\n\n  def process(self):\n    """"""Processes the parsed arguments to produce any additional objects.\n    """"""\n    super(FeedForwardClassificationArguments, self).process()\n    _process_args(self)\n\n\nclass FeedForwardClassification(models.ClassificationModelBuilder):\n  """"""A ModelBuilder for building feed-forward fully connected neural network models.\n\n  These models are also known as multi-layer perceptrons.\n  """"""\n  def __init__(self, args):\n    super(FeedForwardClassification, self).__init__(args)\n\n  def build_inference(self, inputs, training):\n    histograms = {}\n    scalars = {}\n\n    # Build a set of hidden layers. The input to the first hidden layer is\n    # the features tensor, whose shape is (batch, size).\n    x = self.classification.features(inputs)\n    x_size = x.get_shape()[1].value\n\n    for name, size, activation in self.args.hidden_layers:\n      with tf.name_scope(name):\n        weights = tf.Variable(tf.truncated_normal([x_size, size],\n                                                  stddev=1.0 / math.sqrt(float(x_size))),\n                              name=\'weights\')\n        biases = tf.Variable(tf.zeros([size]), name=\'biases\')\n        outputs = tf.nn.xw_plus_b(x, weights, biases, name=\'outputs\')\n\n        histograms[outputs.op.name + \'.activations\'] = outputs\n        scalars[outputs.op.name + \'.sparsity\'] = tf.nn.zero_fraction(outputs)\n\n        if activation:\n          activation_fn = getattr(tf.nn, activation)\n          outputs = activation_fn(outputs, name=activation)\n      x = outputs\n      x_size = size\n\n    with tf.name_scope(\'logits\'):\n      weights = tf.Variable(tf.truncated_normal([x_size, self._classification.num_labels],\n                                                stddev=1.0 / math.sqrt(float(x_size))),\n                            name=\'weights\')\n      biases = tf.Variable(tf.zeros([self._classification.num_labels]), name=\'biases\')\n      logits = tf.nn.xw_plus_b(x, weights, biases, name=\'outputs\')\n\n      histograms[logits.op.name + \'.activations\'] = logits\n      scalars[logits.op.name + \'.sparsity\'] = tf.nn.zero_fraction(logits)\n\n    if training:\n      with tf.name_scope(\'\'):\n        for name, t in scalars.iteritems():\n          tf.summary.scalar(name, t)\n\n        for name, t in histograms.iteritems():\n          tf.summary.histogram(name, t)\n\n        for t in tf.trainable_variables():\n          tf.summary.histogram(t.op.name, t)\n\n    return logits\n\n  def build_training(self, global_steps, inputs, inferences):\n    with tf.name_scope(\'target\'):\n      label_indices = self.classification.target_label_indices(inputs)\n\n    with tf.name_scope(\'error\'):\n      cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=inferences,\n                                                              labels=label_indices,\n                                                              name=\'softmax_cross_entropy\')\n      loss = tf.reduce_mean(cross_entropy, name=\'loss\')\n\n      averager = tf.train.ExponentialMovingAverage(0.99, name=\'loss_averager\')\n      averaging = averager.apply([loss])\n\n    with tf.name_scope(\'\'):\n      tf.summary.scalar(\'metrics/loss\', loss)\n      tf.summary.scalar(\'metrics/loss.average\', averager.average(loss))\n\n    with tf.control_dependencies([averaging]):\n      with tf.name_scope(self.args.optimizer.get_name()):\n        gradients = self.args.optimizer.compute_gradients(loss, var_list=tf.trainable_variables())\n        train = self.args.optimizer.apply_gradients(gradients, global_steps, name=\'optimize\')\n\n      with tf.name_scope(\'\'):\n        for gradient, t in gradients:\n          if gradient is not None:\n            tf.summary.histogram(t.op.name + \'.gradients\', gradient)\n\n    return loss, train\n\n  def build_output(self, inputs, inferences):\n    scores = tf.nn.softmax(inferences, name=\'scores\')\n    tf.add_to_collection(\'outputs\', scores)\n\n    with tf.name_scope(\'labels\'):\n      label_indices = tf.arg_max(inferences, 1, name=\'arg_max\')\n      labels = self.classification.output_labels(label_indices)\n      tf.add_to_collection(\'outputs\', labels)\n\n    keys = self.classification.keys(inputs)\n    if keys:\n      # Key feature, if it exists, is a passthrough to the output.\n      # The use of identity is to name the tensor and correspondingly the output field.\n      keys = tf.identity(keys, name=\'key\')\n      tf.add_to_collection(\'outputs\', keys)\n\n    return {\n      \'label\': labels,\n      \'score\': scores\n    }\n\n  def build_evaluation(self, inputs, outputs):\n    target_labels = self.classification.target_labels(inputs)\n\n    with tf.name_scope(\'accuracy\'):\n      accuracy, eval = tf.contrib.metrics.streaming_accuracy(outputs[\'label\'], target_labels)\n\n    with tf.name_scope(\'\'):\n      tf.summary.scalar(\'metrics/accuracy\', accuracy)\n\n    return accuracy, eval\n'"
