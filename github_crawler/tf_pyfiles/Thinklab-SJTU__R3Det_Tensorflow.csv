file_path,api_count,code
data/__init__.py,0,b''
help_utils/__init__.py,0,b''
help_utils/smooth_label.py,0,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np\nimport math\n\n\ndef gaussian_label(label, num_class, u=0, sig=4.0):\n    x = np.array(range(math.floor(-num_class/2), math.ceil(num_class/2), 1))\n    y_sig = np.exp(-(x - u) ** 2 / (2 * sig ** 2))\n    return np.concatenate([y_sig[math.ceil(num_class/2)-label:],\n                           y_sig[:math.ceil(num_class/2)-label]], axis=0)\n\n\ndef rectangular_label(label, num_class, raduius=4):\n    x = np.zeros([num_class])\n    x[:raduius+1] = 1\n    x[-raduius:] = 1\n    y_sig = np.concatenate([x[-label:], x[:-label]], axis=0)\n    return y_sig\n\n\ndef pulse_label(label, num_class):\n    x = np.zeros([num_class])\n    x[label] = 1\n    return x\n\n\ndef triangle_label(label, num_class, raduius=4):\n    y_sig = np.zeros([num_class])\n    x = np.array(range(raduius+1))\n    y = -1/(raduius+1) * x + 1\n    y_sig[:raduius+1] = y\n    y_sig[-raduius:] = y[-1:0:-1]\n\n    return np.concatenate([y_sig[-label:], y_sig[:-label]], axis=0)\n\n\ndef get_all_smooth_label(num_label, label_type=0, raduius=4):\n    all_smooth_label = []\n\n    if label_type == 0:\n        for i in range(num_label):\n            all_smooth_label.append(gaussian_label(i, num_label, sig=raduius))\n    elif label_type == 1:\n        for i in range(num_label):\n            all_smooth_label.append(rectangular_label(i, num_label, raduius=raduius))\n    elif label_type == 2:\n        for i in range(num_label):\n            all_smooth_label.append(pulse_label(i, num_label))\n    elif label_type == 3:\n        for i in range(num_label):\n            all_smooth_label.append(triangle_label(i, num_label, raduius=raduius))\n    else:\n        raise Exception(\'Only support gaussian, rectangular, triangle and pulse label\')\n    return np.array(all_smooth_label)\n\n\ndef angle_smooth_label(angle_label, angle_range=90, label_type=0, raduius=4):\n    """"""\n    :param angle_label: [-angle_range,0)\n    :param angle_range: 90 or 180\n    :return:\n    """"""\n    angle_label = np.array(-np.round(angle_label), np.int32)\n    all_smooth_label = get_all_smooth_label(angle_range, label_type, raduius)\n    inx = angle_label == angle_range\n    angle_label[inx] = angle_range - 1\n    smooth_label = all_smooth_label[angle_label]\n    return np.array(smooth_label, np.float32)\n\n\nif __name__ == \'__main__\':\n    import matplotlib.pyplot as plt\n\n    # angle_label = np.array([-89.9, -45.2, -0.3, -1.9])\n    # smooth_label = angle_smooth_label(angle_label)\n\n    # y_sig = triangle_label(30, 180, raduius=8)\n    y_sig = gaussian_label(30, 180, sig=0.1)\n    # y_sig = pulse_label(30, 180)\n    # y_sig = triangle_label(0, 90)\n    x = np.array(range(0, 180, 1))\n    plt.plot(x, y_sig, ""r-"", linewidth=2)\n    plt.grid(True)\n    plt.show()\n    print(y_sig)\n'"
help_utils/tools.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport math\nimport sys\nimport os\n\nfrom libs.configs import cfgs\n\n\ndef view_bar(message, num, total):\n    rate = num / total\n    rate_num = int(rate * 40)\n    rate_nums = math.ceil(rate * 100)\n    r = \'\\r%s:[%s%s]%d%%\\t%d/%d\' % (message, "">"" * rate_num, "" "" * (40 - rate_num), rate_nums, num, total,)\n    sys.stdout.write(r)\n    sys.stdout.flush()\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef get_feature_map_size(src_len):\n    feature_map_size = []\n    src_len /= 2 ** (int(cfgs.LEVEL[0][-1])-1)\n    for _ in range(len(cfgs.LEVEL)):\n        src_len = math.ceil(src_len / 2)\n        feature_map_size.append((src_len, src_len))\n\n    return feature_map_size\n\n\ndef get_dota_short_names(label):\n    DOTA_SHORT_NAMES = {\n        \'roundabout\': \'RA\',\n        \'tennis-court\': \'TC\',\n        \'swimming-pool\': \'SP\',\n        \'storage-tank\': \'ST\',\n        \'soccer-ball-field\': \'SBF\',\n        \'small-vehicle\': \'SV\',\n        \'ship\': \'SH\',\n        \'plane\': \'PL\',\n        \'large-vehicle\': \'LV\',\n        \'helicopter\': \'HC\',\n        \'harbor\': \'HA\',\n        \'ground-track-field\': \'GTF\',\n        \'bridge\': \'BR\',\n        \'basketball-court\': \'BC\',\n        \'baseball-diamond\': \'BD\'\n    }\n\n    return DOTA_SHORT_NAMES[label]'"
libs/__init__.py,0,b''
libs/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    # Extension(\n    #     ""utils.cython_bbox"",\n    #     [""utils/bbox.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    # Extension(\n    #     ""nms.cpu_nms"",\n    #     [""nms/cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_cython_nms"",\n    #     [""rotation/rotate_cython_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\n    #     ""rotation.rotate_circle_nms"",\n    #     [""rotation/rotate_circle_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # ),\n    #\n    # Extension(\'nms.gpu_nms\',\n    #     [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    # Extension(\'rotation.rotate_gpu_nms\',\n    #     [\'rotation/rotate_nms_kernel.cu\', \'rotation/rotate_gpu_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc anrbd not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    Extension(\'rotation.rbbox_overlaps\',\n        [\'rotation/rbbox_overlaps_kernel.cu\', \'rotation/rbbox_overlaps.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n    # Extension(\'rotation.rotate_polygon_nms\',\n    #     [\'rotation/rotate_polygon_nms_kernel.cu\', \'rotation/rotate_polygon_nms.pyx\'],\n    #     library_dirs=[CUDA[\'lib64\']],\n    #     libraries=[\'cudart\'],\n    #     language=\'c++\',\n    #     runtime_library_dirs=[CUDA[\'lib64\']],\n    #     # this syntax is specific to this build system\n    #     # we\'re only going to use certain compiler args with nvcc and not with\n    #     # gcc the implementation of this trick is in customize_compiler() below\n    #     extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n    #                         \'nvcc\': [\'-arch=sm_35\',\n    #                                  \'--ptxas-options=-v\',\n    #                                  \'-c\',\n    #                                  \'--compiler-options\',\n    #                                  ""\'-fPIC\'""]},\n    #     include_dirs = [numpy_include, CUDA[\'include\']]\n    # ),\n    #\n    # Extension(\n    #     \'pycocotools._mask\',\n    #     sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n    #     include_dirs = [numpy_include, \'pycocotools\'],\n    #     extra_compile_args={\n    #         \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    # ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/__init__.py,0,b''
tools/inference.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os, sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det\nfrom libs.box_utils import draw_box_in_img\n\n\ndef detect(det_net, inference_save_path, real_test_imgname_list, draw_imgs):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not GBR\n    img_batch = tf.cast(img_plac, tf.float32)\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n    img_batch = tf.expand_dims(img_batch, axis=0)  # [1, None, None, 3]\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        pbar = tqdm(real_test_imgname_list)\n        for a_img_name in pbar:\n\n            raw_img = cv2.imread(a_img_name)\n            resized_img, detected_boxes, detected_scores, detected_categories = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}  # cv is BGR. But need RGB\n                )\n\n            nake_name = a_img_name.split(\'/\')[-1]\n\n            if draw_imgs:\n                show_indices = detected_scores >= cfgs.VIS_SCORE\n                show_scores = detected_scores[show_indices]\n                show_boxes = detected_boxes[show_indices]\n                show_categories = detected_categories[show_indices]\n\n                draw_img = np.squeeze(resized_img, 0)\n\n                if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                    draw_img = (draw_img * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n                else:\n                    draw_img = draw_img + np.array(cfgs.PIXEL_MEAN)\n                final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                    boxes=show_boxes,\n                                                                                    labels=show_categories,\n                                                                                    scores=show_scores,\n                                                                                    method=1,\n                                                                                    in_graph=False)\n                cv2.imwrite(inference_save_path + \'/\' + nake_name,\n                            final_detections[:, :, ::-1])\n\n            pbar.set_description(""{} image"".format(nake_name))\n\n\ndef inference(test_dir, inference_save_path, draw_imgs):\n\n    test_imgname_list = [os.path.join(test_dir, img_name) for img_name in os.listdir(test_dir)\n                                                          if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    faster_rcnn = build_whole_network_r3det.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                             is_training=False)\n    detect(det_net=faster_rcnn, inference_save_path=inference_save_path,\n           real_test_imgname_list=test_imgname_list, draw_imgs=draw_imgs)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'TestImgs...U need provide the test dir\')\n    parser.add_argument(\'--data_dir\', dest=\'data_dir\',\n                        help=\'data path\',\n                        default=\'demos\', type=str)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                        help=\'demo imgs to save\',\n                        default=\'inference_results\', type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\n                        help=\'gpu id \',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--draw_imgs\', \'-s\', default=False,\n                        action=\'store_true\')\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n    inference(args.data_dir,\n              inference_save_path=args.save_dir,\n              draw_imgs=args.draw_imgs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train.py,63,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                         is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_r3det.py,68,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_r3det.DetectionNetwork(\n            base_network_name=cfgs.NET_NAME,\n            is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss\': tf.constant(0., tf.float32),\n            \'refine_reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss_stage3\': tf.constant(0., tf.float32),\n            \'refine_reg_loss_stage3\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        if cfgs.USE_SUPERVISED_MASK:\n            total_loss_dict[\'mask_loss\'] = tf.constant(0., tf.float32)\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_r3det_csl.py,81,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det_csl\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\nfrom help_utils.smooth_label import angle_smooth_label\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_and_cosine_lr(init_lr, global_step, warmup_step, decay_steps, num_gpu, alpha=1e-6):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def cosine_lr(init_lr, global_step, decay_steps, num_gpu, alpha=0.0):\n        return tf.train.cosine_decay(learning_rate=init_lr,\n                                     global_step=global_step - warmup_step,\n                                     decay_steps=decay_steps // num_gpu - warmup_step,\n                                     alpha=alpha)\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: cosine_lr(init_lr, global_step, decay_steps, num_gpu, alpha))\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        # lr = warmup_and_cosine_lr(cfgs.LR, global_step, cfgs.WARM_SETP, cfgs.MAX_ITERATION, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_r3det_csl.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                                   is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss\': tf.constant(0., tf.float32),\n            \'refine_reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss_stage3\': tf.constant(0., tf.float32),\n            \'refine_reg_loss_stage3\': tf.constant(0., tf.float32),\n            \'angle_cls_loss_stage3\': tf.constant(0., tf.float32),\n            \'angle_cls_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                if cfgs.ANGLE_RANGE == 180:\n                                    gtboxes_and_label_r_ = tf.py_func(coordinate_present_convert,\n                                                                      inp=[gtboxes_and_label_r, -1],\n                                                                      Tout=tf.float32)\n                                    gtboxes_and_label_r_ = tf.reshape(gtboxes_and_label_r_, [-1, 6])\n\n                                    gt_smooth_label = tf.py_func(angle_smooth_label,\n                                                                 inp=[gtboxes_and_label_r_[:, -2], cfgs.ANGLE_RANGE,\n                                                                      cfgs.LABEL_TYPE, cfgs.RADUIUS],\n                                                                 Tout=tf.float32)\n\n                                else:\n                                    gt_smooth_label = tf.py_func(angle_smooth_label,\n                                                                 inp=[gtboxes_and_label_r[:, -2], cfgs.ANGLE_RANGE,\n                                                                      cfgs.LABEL_TYPE],\n                                                                 Tout=tf.float32)\n\n                                gt_smooth_label = tf.reshape(gt_smooth_label, [-1, cfgs.ANGLE_RANGE])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gt_smooth_label=gt_smooth_label,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0,\n                                                                              is_csl=True)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1,\n                                                                              is_csl=True)\n\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1,\n                                        is_csl=True)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                    detections_angle_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[3],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1,\n                                        is_csl=True)\n                                    tf.summary.image(\'Compare/final_detection_angle_gpu:%d\' % i, detections_angle_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_r3det_efficientnet.py,68,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det_efficientnet\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_r3det_efficientnet.DetectionNetwork(\n            base_network_name=cfgs.NET_NAME,\n            is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss\': tf.constant(0., tf.float32),\n            \'refine_reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss_stage3\': tf.constant(0., tf.float32),\n            \'refine_reg_loss_stage3\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        if cfgs.USE_SUPERVISED_MASK:\n            total_loss_dict[\'mask_loss\'] = tf.constant(0., tf.float32)\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_r3det_plusplus.py,68,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det_plusplus\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_r3det_plusplus.DetectionNetwork(\n            base_network_name=cfgs.NET_NAME,\n            is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss\': tf.constant(0., tf.float32),\n            \'refine_reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss_stage3\': tf.constant(0., tf.float32),\n            \'refine_reg_loss_stage3\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        if cfgs.USE_SUPERVISED_MASK:\n            total_loss_dict[\'mask_loss\'] = tf.constant(0., tf.float32)\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_refine_retinanet.py,65,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_refine_retinanet\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_refine_retinanet.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                                          is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'refine_cls_loss\': tf.constant(0., tf.float32),\n            \'refine_reg_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        final_gvs = []\n        with tf.variable_scope(\'Gradient_Mult\'):\n            for grad, var in grads:\n                scale = 1.\n                if \'/biases:\' in var.name:\n                    scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                if \'conv_new\' in var.name:\n                    scale *= 3.\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n\n                final_gvs.append((grad, var))\n\n        # apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n        apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            if not restorer is None:\n                restorer.restore(sess, restore_ckpt)\n                print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                                sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/multi_gpu_train_win.py,63,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport os\nimport sys\nimport numpy as np\nimport time\nsys.path.append(""../"")\n\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_win\nfrom data.io.read_tfrecord_multi_gpu import next_batch\nfrom libs.box_utils.show_box_in_tensor import draw_boxes_with_categories, draw_boxes_with_categories_and_scores\nfrom help_utils import tools\nfrom libs.box_utils.coordinate_convert import backward_convert, get_horizen_minAreaRectangle\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = cfgs.GPU_GROUP\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef sum_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    sum_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(axis=0, values=grads)\n        grad = tf.reduce_sum(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        sum_grads.append(grad_and_var)\n    return sum_grads\n\n\ndef get_gtboxes_and_label(gtboxes_and_label_h, gtboxes_and_label_r, num_objects):\n    return gtboxes_and_label_h[:int(num_objects), :].astype(np.float32), \\\n           gtboxes_and_label_r[:int(num_objects), :].astype(np.float32)\n\n\ndef warmup_lr(init_lr, global_step, warmup_step, num_gpu):\n    def warmup(end_lr, global_step, warmup_step):\n        start_lr = end_lr * 0.1\n        global_step = tf.cast(global_step, tf.float32)\n        return start_lr + (end_lr - start_lr) * global_step / warmup_step\n\n    def decay(start_lr, global_step, num_gpu):\n        lr = tf.train.piecewise_constant(global_step,\n                                         boundaries=[np.int64(cfgs.DECAY_STEP[0] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[1] // num_gpu),\n                                                     np.int64(cfgs.DECAY_STEP[2] // num_gpu)],\n                                         values=[start_lr, start_lr / 10., start_lr / 100., start_lr / 1000.])\n        return lr\n\n    return tf.cond(tf.less_equal(global_step, warmup_step),\n                   true_fn=lambda: warmup(init_lr, global_step, warmup_step),\n                   false_fn=lambda: decay(init_lr, global_step, num_gpu))\n\n\ndef train():\n\n    with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n\n        num_gpu = cfgs.NUM_GPU\n        global_step = slim.get_or_create_global_step()\n        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)\n        tf.summary.scalar(\'lr\', lr)\n\n        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)\n        retinanet = build_whole_network_win.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                             is_training=True)\n\n        with tf.name_scope(\'get_batch\'):\n            if cfgs.IMAGE_PYRAMID:\n                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)\n                shortside_len = tf.random_shuffle(shortside_len_list)[0]\n\n            else:\n                shortside_len = cfgs.IMG_SHORT_SIDE_LEN\n\n            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n                next_batch(dataset_name=cfgs.DATASET_NAME,\n                           batch_size=cfgs.BATCH_SIZE * num_gpu,\n                           shortside_len=shortside_len,\n                           is_training=True)\n\n        # data processing\n        inputs_list = []\n        for i in range(num_gpu):\n            img = tf.expand_dims(img_batch[i], axis=0)\n            if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n                img = img / tf.constant([cfgs.PIXEL_STD])\n\n            gtboxes_and_label_r = tf.py_func(backward_convert,\n                                             inp=[gtboxes_and_label_batch[i]],\n                                             Tout=tf.float32)\n            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])\n            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n\n            num_objects = num_objects_batch[i]\n            num_objects = tf.cast(tf.reshape(num_objects, [-1, ]), tf.float32)\n\n            img_h = img_h_batch[i]\n            img_w = img_w_batch[i]\n\n            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])\n\n        tower_grads = []\n        biases_regularizer = tf.no_regularizer\n        weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)\n\n        total_loss_dict = {\n            \'cls_loss\': tf.constant(0., tf.float32),\n            \'reg_loss\': tf.constant(0., tf.float32),\n            \'total_losses\': tf.constant(0., tf.float32),\n        }\n\n        with tf.variable_scope(tf.get_variable_scope()):\n            for i in range(num_gpu):\n                with tf.device(\'/gpu:%d\' % i):\n                    with tf.name_scope(\'tower_%d\' % i):\n                        with slim.arg_scope(\n                                [slim.model_variable, slim.variable],\n                                device=\'/device:CPU:0\'):\n                            with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,\n                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],\n                                                weights_regularizer=weights_regularizer,\n                                                biases_regularizer=biases_regularizer,\n                                                biases_initializer=tf.constant_initializer(0.0)):\n\n                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,\n                                                                                      inp=[inputs_list[i][1],\n                                                                                           inputs_list[i][2],\n                                                                                           inputs_list[i][3]],\n                                                                                      Tout=[tf.float32, tf.float32])\n                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-1, 5])\n                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-1, 6])\n\n                                img = inputs_list[i][0]\n                                img_shape = inputs_list[i][-2:]\n                                img = tf.image.crop_to_bounding_box(image=img,\n                                                                    offset_height=0,\n                                                                    offset_width=0,\n                                                                    target_height=tf.cast(img_shape[0], tf.int32),\n                                                                    target_width=tf.cast(img_shape[1], tf.int32))\n\n                                outputs = retinanet.build_whole_detection_network(input_img_batch=img,\n                                                                                  gtboxes_batch_h=gtboxes_and_label_h,\n                                                                                  gtboxes_batch_r=gtboxes_and_label_r,\n                                                                                  gpu_id=i)\n                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_h[:, :-1],\n                                                                              labels=gtboxes_and_label_h[:, -1],\n                                                                              method=0)\n                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,\n                                                                              boxes=gtboxes_and_label_r[:, :-1],\n                                                                              labels=gtboxes_and_label_r[:, -1],\n                                                                              method=1)\n                                tf.summary.image(\'Compare/gtboxes_h_gpu:%d\' % i, gtboxes_in_img_h)\n                                tf.summary.image(\'Compare/gtboxes_r_gpu:%d\' % i, gtboxes_in_img_r)\n\n                                if cfgs.ADD_BOX_IN_TENSORBOARD:\n                                    detections_in_img = draw_boxes_with_categories_and_scores(\n                                        img_batch=img,\n                                        boxes=outputs[0],\n                                        scores=outputs[1],\n                                        labels=outputs[2],\n                                        method=1)\n                                    tf.summary.image(\'Compare/final_detection_gpu:%d\' % i, detections_in_img)\n\n                                loss_dict = outputs[-1]\n\n                                total_losses = 0.0\n                                for k in loss_dict.keys():\n                                    total_losses += loss_dict[k]\n                                    total_loss_dict[k] += loss_dict[k] / num_gpu\n\n                                total_losses = total_losses / num_gpu\n                                total_loss_dict[\'total_losses\'] += total_losses\n\n                                if i == num_gpu - 1:\n                                    regularization_losses = tf.get_collection(\n                                        tf.GraphKeys.REGULARIZATION_LOSSES)\n                                    # weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())\n                                    total_losses = total_losses + tf.add_n(regularization_losses)\n\n                        tf.get_variable_scope().reuse_variables()\n                        grads = optimizer.compute_gradients(total_losses)\n                        if cfgs.GRADIENT_CLIPPING_BY_NORM is not None:\n                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)\n                        tower_grads.append(grads)\n\n        for k in total_loss_dict.keys():\n            tf.summary.scalar(\'{}/{}\'.format(k.split(\'_\')[0], k), total_loss_dict[k])\n\n        if len(tower_grads) > 1:\n            grads = sum_gradients(tower_grads)\n        else:\n            grads = tower_grads[0]\n\n        if cfgs.MUTILPY_BIAS_GRADIENT is not None:\n            final_gvs = []\n            with tf.variable_scope(\'Gradient_Mult\'):\n                for grad, var in grads:\n                    scale = 1.\n                    if \'/biases:\' in var.name:\n                        scale *= cfgs.MUTILPY_BIAS_GRADIENT\n                    if \'conv_new\' in var.name:\n                        scale *= 3.\n                    if not np.allclose(scale, 1.0):\n                        grad = tf.multiply(grad, scale)\n\n                    final_gvs.append((grad, var))\n            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        else:\n            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n        train_op = tf.group(apply_gradient_op, variables_averages_op)\n        # train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)\n        summary_op = tf.summary.merge_all()\n\n        # restorer, restore_ckpt = retinanet.get_restorer()\n        saver = tf.train.Saver(max_to_keep=5)\n\n        init_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\n        tfconfig = tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=False)\n        tfconfig.gpu_options.allow_growth = True\n        with tf.Session(config=tfconfig) as sess:\n            sess.run(init_op)\n\n            # sess.run(tf.initialize_all_variables())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)\n            tools.mkdir(summary_path)\n            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)\n\n            # if not restorer is None:\n            #     restorer.restore(sess, restore_ckpt)\n            #     print(\'restore model\')\n\n            for step in range(cfgs.MAX_ITERATION // num_gpu):\n                training_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n\n                if step % cfgs.SHOW_TRAIN_INFO_INTE != 0 and step % cfgs.SMRY_ITER != 0:\n                    _, global_stepnp = sess.run([train_op, global_step])\n\n                else:\n                    if step % cfgs.SHOW_TRAIN_INFO_INTE == 0 and step % cfgs.SMRY_ITER != 0:\n                        start = time.time()\n\n                        _, global_stepnp, total_loss_dict_ = \\\n                            sess.run([train_op, global_step, total_loss_dict])\n\n                        end = time.time()\n\n                        print(\'***\'*20)\n                        print(""""""%s: global_step:%d  current_step:%d""""""\n                              % (training_time, (global_stepnp-1)*num_gpu, step*num_gpu))\n                        print(""""""per_cost_time:%.3fs""""""\n                              % ((end - start) / num_gpu))\n                        loss_str = \'\'\n                        for k in total_loss_dict_.keys():\n                            loss_str += \'%s:%.3f\\n\' % (k, total_loss_dict_[k])\n                        print(loss_str)\n\n                    else:\n                        if step % cfgs.SMRY_ITER == 0:\n                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])\n                            summary_writer.add_summary(summary_str, (global_stepnp-1)*num_gpu)\n                            summary_writer.flush()\n\n                if (step > 0 and step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == 0) or (step >= cfgs.MAX_ITERATION // num_gpu - 1):\n\n                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)\n                    if not os.path.exists(save_dir):\n                        os.mkdir(save_dir)\n\n                    save_ckpt = os.path.join(save_dir, \'{}_\'.format(cfgs.DATASET_NAME) +\n                                             str((global_stepnp-1)*num_gpu) + \'model.ckpt\')\n                    saver.save(sess, save_ckpt)\n                    print(\' weights had been saved\')\n\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n\n    train()\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_dota.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                     is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n'"
tools/test_dota_r3det.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n\n            # if \'P0016\' not in img_path:\n            #     continue\n\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_dota_r3det_csl.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det_csl\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category, detection_boxes_angle = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None,\n        gt_smooth_label=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n\n            # if \'P0016\' not in img_path:\n            #     continue\n\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes_angle, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det_csl.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                               is_training=False)\n\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_dota_r3det_efficientnet.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det_efficientnet\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n\n            # if \'P0016\' not in img_path:\n            #     continue\n\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det_efficientnet.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                                        is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_dota_r3det_ms.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH,\n                                                     is_resize=False)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n            # if \'P2043.png\' not in img_path:\n            #     continue\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    for short_size in cfgs.IMG_SHORT_SIDE_LEN:\n                        max_len = cfgs.IMG_MAX_LENGTH\n                        if args.h_len < args.w_len:\n                            new_h, new_w = short_size, min(int(short_size * float(args.w_len) / args.h_len), max_len)\n                        else:\n                            new_h, new_w = min(int(short_size * float(args.h_len) / args.w_len), max_len), short_size\n                        img_resize = cv2.resize(src_img, (new_w, new_h))\n\n                        resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                            sess.run(\n                                [img_batch, detection_boxes, detection_scores, detection_category],\n                                feed_dict={img_plac: img_resize[:, :, ::-1]}\n                            )\n\n                        resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                        src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                        if len(det_boxes_r_) > 0:\n                            det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                            det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                            det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                            det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                            for ii in range(len(det_boxes_r_)):\n                                box_rotate = det_boxes_r_[ii]\n                                box_rotate[0] = box_rotate[0] + ww_\n                                box_rotate[1] = box_rotate[1] + hh_\n                                box_res_rotate.append(box_rotate)\n                                label_res_rotate.append(det_category_r_[ii])\n                                score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.1,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.05, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.05, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=1000)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det.DetectionNetwork(\n        base_network_name=cfgs.NET_NAME,\n        is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 strand\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n'"
tools/test_dota_r3det_plusplus.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det_plusplus\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n            # if \'P0016.png\' not in img_path:\n            #     continue\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=1500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det_plusplus.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                                    is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
tools/test_dota_r3det_plusplus_ms.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_r3det_plusplus\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH,\n                                                     is_resize=False)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n            # if \'P0006.png\' not in img_path:\n            #     continue\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    for short_size in cfgs.IMG_SHORT_SIDE_LEN:\n                        max_len = cfgs.IMG_MAX_LENGTH\n                        if args.h_len < args.w_len:\n                            new_h, new_w = short_size, min(int(short_size * float(args.w_len) / args.h_len), max_len)\n                        else:\n                            new_h, new_w = min(int(short_size * float(args.h_len) / args.w_len), max_len), short_size\n                        img_resize = cv2.resize(src_img, (new_w, new_h))\n\n                        resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                            sess.run(\n                                [img_batch, detection_boxes, detection_scores, detection_category],\n                                feed_dict={img_plac: img_resize[:, :, ::-1]}\n                            )\n\n                        resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                        src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                        if len(det_boxes_r_) > 0:\n                            det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                            det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                            det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                            det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                            for ii in range(len(det_boxes_r_)):\n                                box_rotate = det_boxes_r_[ii]\n                                box_rotate[0] = box_rotate[0] + ww_\n                                box_rotate[1] = box_rotate[1] + hh_\n                                box_res_rotate.append(box_rotate)\n                                label_res_rotate.append(det_category_r_[ii])\n                                score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.1,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.05, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.05, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=1000)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_r3det_plusplus.DetectionNetwork(\n        base_network_name=cfgs.NET_NAME,\n        is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result with Pascal2007 strand\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/dataset/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n'"
tools/test_dota_refine_retinanet.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport argparse\nfrom multiprocessing import Queue, Process\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.networks import build_whole_network_refine_retinanet\nfrom help_utils import tools\nfrom libs.label_name_dict.label_dict import *\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom libs.box_utils import nms_rotate\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef worker(gpu_id, images, det_net, args, result_queue):\n    os.environ[""CUDA_VISIBLE_DEVICES""] = str(gpu_id)\n\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model %d ...\' % gpu_id)\n\n        for img_path in images:\n            img = cv2.imread(img_path)\n\n            box_res_rotate = []\n            label_res_rotate = []\n            score_res_rotate = []\n\n            imgH = img.shape[0]\n            imgW = img.shape[1]\n\n            if imgH < args.h_len:\n                temp = np.zeros([args.h_len, imgW, 3], np.float32)\n                temp[0:imgH, :, :] = img\n                img = temp\n                imgH = args.h_len\n\n            if imgW < args.w_len:\n                temp = np.zeros([imgH, args.w_len, 3], np.float32)\n                temp[:, 0:imgW, :] = img\n                img = temp\n                imgW = args.w_len\n\n            for hh in range(0, imgH, args.h_len - args.h_overlap):\n                if imgH - hh - 1 < args.h_len:\n                    hh_ = imgH - args.h_len\n                else:\n                    hh_ = hh\n                for ww in range(0, imgW, args.w_len - args.w_overlap):\n                    if imgW - ww - 1 < args.w_len:\n                        ww_ = imgW - args.w_len\n                    else:\n                        ww_ = ww\n                    src_img = img[hh_:(hh_ + args.h_len), ww_:(ww_ + args.w_len), :]\n\n                    resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                        sess.run(\n                            [img_batch, detection_boxes, detection_scores, detection_category],\n                            feed_dict={img_plac: src_img[:, :, ::-1]}\n                        )\n\n                    resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                    src_h, src_w = src_img.shape[0], src_img.shape[1]\n\n                    if len(det_boxes_r_) > 0:\n                        det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                        det_boxes_r_[:, 0::2] *= (src_w / resized_w)\n                        det_boxes_r_[:, 1::2] *= (src_h / resized_h)\n                        det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n                        for ii in range(len(det_boxes_r_)):\n                            box_rotate = det_boxes_r_[ii]\n                            box_rotate[0] = box_rotate[0] + ww_\n                            box_rotate[1] = box_rotate[1] + hh_\n                            box_res_rotate.append(box_rotate)\n                            label_res_rotate.append(det_category_r_[ii])\n                            score_res_rotate.append(det_scores_r_[ii])\n\n            box_res_rotate = np.array(box_res_rotate)\n            label_res_rotate = np.array(label_res_rotate)\n            score_res_rotate = np.array(score_res_rotate)\n\n            box_res_rotate_ = []\n            label_res_rotate_ = []\n            score_res_rotate_ = []\n            threshold = {\'roundabout\': 0.1, \'tennis-court\': 0.3, \'swimming-pool\': 0.1, \'storage-tank\': 0.2,\n                         \'soccer-ball-field\': 0.3, \'small-vehicle\': 0.2, \'ship\': 0.05, \'plane\': 0.3,\n                         \'large-vehicle\': 0.1, \'helicopter\': 0.2, \'harbor\': 0.0001, \'ground-track-field\': 0.3,\n                         \'bridge\': 0.0001, \'basketball-court\': 0.3, \'baseball-diamond\': 0.3}\n\n            for sub_class in range(1, cfgs.CLASS_NUM + 1):\n                index = np.where(label_res_rotate == sub_class)[0]\n                if len(index) == 0:\n                    continue\n                tmp_boxes_r = box_res_rotate[index]\n                tmp_label_r = label_res_rotate[index]\n                tmp_score_r = score_res_rotate[index]\n\n                tmp_boxes_r = np.array(tmp_boxes_r)\n                tmp = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                tmp[:, 0:-1] = tmp_boxes_r\n                tmp[:, -1] = np.array(tmp_score_r)\n\n                try:\n                    inx = nms_rotate.nms_rotate_cpu(boxes=np.array(tmp_boxes_r),\n                                                    scores=np.array(tmp_score_r),\n                                                    iou_threshold=threshold[LABEL_NAME_MAP[sub_class]],\n                                                    max_output_size=500)\n                except:\n                    # Note: the IoU of two same rectangles is 0, which is calculated by rotate_gpu_nms\n                    jitter = np.zeros([tmp_boxes_r.shape[0], tmp_boxes_r.shape[1] + 1])\n                    jitter[:, 0] += np.random.rand(tmp_boxes_r.shape[0], ) / 1000\n                    inx = rotate_gpu_nms(np.array(tmp, np.float32) + np.array(jitter, np.float32),\n                                         float(threshold[LABEL_NAME_MAP[sub_class]]), 0)\n\n                box_res_rotate_.extend(np.array(tmp_boxes_r)[inx])\n                score_res_rotate_.extend(np.array(tmp_score_r)[inx])\n                label_res_rotate_.extend(np.array(tmp_label_r)[inx])\n\n            result_dict = {\'boxes\': np.array(box_res_rotate_), \'scores\': np.array(score_res_rotate_),\n                           \'labels\': np.array(label_res_rotate_), \'image_id\': img_path}\n            result_queue.put_nowait(result_dict)\n\n\ndef test_dota(det_net, real_test_img_list, args, txt_name):\n\n    save_path = os.path.join(\'./test_dota\', cfgs.VERSION)\n\n    nr_records = len(real_test_img_list)\n    pbar = tqdm(total=nr_records)\n    gpu_num = len(args.gpus.strip().split(\',\'))\n\n    nr_image = math.ceil(nr_records / gpu_num)\n    result_queue = Queue(500)\n    procs = []\n\n    for i, gpu_id in enumerate(args.gpus.strip().split(\',\')):\n        start = i * nr_image\n        end = min(start + nr_image, nr_records)\n        split_records = real_test_img_list[start:end]\n        proc = Process(target=worker, args=(int(gpu_id), split_records, det_net, args, result_queue))\n        print(\'process:%d, start:%d, end:%d\' % (i, start, end))\n        proc.start()\n        procs.append(proc)\n\n    for i in range(nr_records):\n        res = result_queue.get()\n\n        if args.show_box:\n\n            nake_name = res[\'image_id\'].split(\'/\')[-1]\n            tools.mkdir(os.path.join(save_path, \'dota_img_vis\'))\n            draw_path = os.path.join(save_path, \'dota_img_vis\', nake_name)\n\n            draw_img = np.array(cv2.imread(res[\'image_id\']), np.float32)\n\n            detected_indices = res[\'scores\'] >= cfgs.VIS_SCORE\n            detected_scores = res[\'scores\'][detected_indices]\n            detected_boxes = res[\'boxes\'][detected_indices]\n            detected_categories = res[\'labels\'][detected_indices]\n\n            final_detections = draw_box_in_img.draw_boxes_with_label_and_scores(draw_img,\n                                                                                boxes=detected_boxes,\n                                                                                labels=detected_categories,\n                                                                                scores=detected_scores,\n                                                                                method=1,\n                                                                                in_graph=False)\n            cv2.imwrite(draw_path, final_detections)\n\n        else:\n            CLASS_DOTA = NAME_LABEL_MAP.keys()\n            write_handle = {}\n\n            tools.mkdir(os.path.join(save_path, \'dota_res\'))\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class] = open(os.path.join(save_path, \'dota_res\', \'Task1_%s.txt\' % sub_class), \'a+\')\n\n            rboxes = forward_convert(res[\'boxes\'], with_label=False)\n\n            for i, rbox in enumerate(rboxes):\n                command = \'%s %.3f %.1f %.1f %.1f %.1f %.1f %.1f %.1f %.1f\\n\' % (res[\'image_id\'].split(\'/\')[-1].split(\'.\')[0],\n                                                                                 res[\'scores\'][i],\n                                                                                 rbox[0], rbox[1], rbox[2], rbox[3],\n                                                                                 rbox[4], rbox[5], rbox[6], rbox[7],)\n                write_handle[LABEL_NAME_MAP[res[\'labels\'][i]]].write(command)\n\n            for sub_class in CLASS_DOTA:\n                if sub_class == \'back_ground\':\n                    continue\n                write_handle[sub_class].close()\n\n            fw = open(txt_name, \'a+\')\n            fw.write(\'{}\\n\'.format(res[\'image_id\'].split(\'/\')[-1]))\n            fw.close()\n\n        pbar.set_description(""Test image %s"" % res[\'image_id\'].split(\'/\')[-1])\n\n        pbar.update(1)\n\n    for p in procs:\n        p.join()\n\n\ndef eval(num_imgs, args):\n\n    txt_name = \'{}.txt\'.format(cfgs.VERSION)\n    if not args.show_box:\n        if not os.path.exists(txt_name):\n            fw = open(txt_name, \'w\')\n            fw.close()\n\n        fr = open(txt_name, \'r\')\n        img_filter = fr.readlines()\n        print(\'****************************\'*3)\n        print(\'Already tested imgs:\', img_filter)\n        print(\'****************************\'*3)\n        fr.close()\n\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\')) and\n                             (img_name + \'\\n\' not in img_filter)]\n    else:\n        test_imgname_list = [os.path.join(args.test_dir, img_name) for img_name in os.listdir(args.test_dir)\n                             if img_name.endswith((\'.jpg\', \'.png\', \'.jpeg\', \'.tif\', \'.tiff\'))]\n\n    assert len(test_imgname_list) != 0, \'test_dir has no imgs there.\' \\\n                                        \' Note that, we only support img format of (.jpg, .png, and .tiff) \'\n\n    if num_imgs == np.inf:\n        real_test_img_list = test_imgname_list\n    else:\n        real_test_img_list = test_imgname_list[: num_imgs]\n\n    retinanet = build_whole_network_refine_retinanet.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                                      is_training=False)\n    test_dota(det_net=retinanet, real_test_img_list=real_test_img_list, args=args, txt_name=txt_name)\n\n    if not args.show_box:\n        os.remove(txt_name)\n\n\ndef parse_args():\n\n    parser = argparse.ArgumentParser(\'evaluate the result.\')\n\n    parser.add_argument(\'--test_dir\', dest=\'test_dir\',\n                        help=\'evaluate imgs dir \',\n                        default=\'/data/DOTA/test/images/\', type=str)\n    parser.add_argument(\'--gpus\', dest=\'gpus\',\n                        help=\'gpu id\',\n                        default=\'0,1,2,3,4,5,6,7\', type=str)\n    parser.add_argument(\'--eval_num\', dest=\'eval_num\',\n                        help=\'the num of eval imgs\',\n                        default=np.inf, type=int)\n    parser.add_argument(\'--show_box\', \'-s\', default=False,\n                        action=\'store_true\')\n    parser.add_argument(\'--h_len\', dest=\'h_len\',\n                        help=\'image height\',\n                        default=600, type=int)\n    parser.add_argument(\'--w_len\', dest=\'w_len\',\n                        help=\'image width\',\n                        default=600, type=int)\n    parser.add_argument(\'--h_overlap\', dest=\'h_overlap\',\n                        help=\'height overlap\',\n                        default=150, type=int)\n    parser.add_argument(\'--w_overlap\', dest=\'w_overlap\',\n                        help=\'width overlap\',\n                        default=150, type=int)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n\n    args = parse_args()\n    print(20*""--"")\n    print(args)\n    print(20*""--"")\n    eval(args.eval_num,\n         args=args)\n\n\n'"
tools/test_hrsc2016_r3det.py,10,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport os\nimport sys\nimport tensorflow as tf\nimport time\nimport cv2\nimport pickle\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\nsys.path.append(""../"")\n\nfrom data.io.image_preprocess import short_side_resize_for_inference_data\nfrom libs.configs import cfgs\nfrom libs.networks import build_whole_network_r3det\nfrom libs.val_libs import voc_eval, voc_eval_r\nfrom libs.box_utils import draw_box_in_img\nfrom libs.box_utils.coordinate_convert import forward_convert, backward_convert\nfrom help_utils import tools\n\n\ndef eval_with_plac(img_dir, det_net, num_imgs, image_ext, draw_imgs=False):\n\n    # 1. preprocess img\n    img_plac = tf.placeholder(dtype=tf.uint8, shape=[None, None, 3])  # is RGB. not BGR\n    img_batch = tf.cast(img_plac, tf.float32)\n\n    img_batch = short_side_resize_for_inference_data(img_tensor=img_batch,\n                                                     target_shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                                                     length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img_batch = (img_batch / 255 - tf.constant(cfgs.PIXEL_MEAN_)) / tf.constant(cfgs.PIXEL_STD)\n    else:\n        img_batch = img_batch - tf.constant(cfgs.PIXEL_MEAN)\n\n    img_batch = tf.expand_dims(img_batch, axis=0)\n\n    detection_boxes, detection_scores, detection_category = det_net.build_whole_detection_network(\n        input_img_batch=img_batch,\n        gtboxes_batch_h=None,\n        gtboxes_batch_r=None)\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    restorer, restore_ckpt = det_net.get_restorer()\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n        if not restorer is None:\n            restorer.restore(sess, restore_ckpt)\n            print(\'restore model\')\n\n        all_boxes_r = []\n        imgs = os.listdir(img_dir)\n        pbar = tqdm(imgs)\n        for a_img_name in pbar:\n            a_img_name = a_img_name.split(image_ext)[0]\n\n            raw_img = cv2.imread(os.path.join(img_dir,\n                                              a_img_name + image_ext))\n            raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]\n\n            resized_img, det_boxes_r_, det_scores_r_, det_category_r_ = \\\n                sess.run(\n                    [img_batch, detection_boxes, detection_scores, detection_category],\n                    feed_dict={img_plac: raw_img[:, :, ::-1]}\n                )\n\n            if draw_imgs:\n                detected_indices = det_scores_r_ >= cfgs.VIS_SCORE\n                detected_scores = det_scores_r_[detected_indices]\n                detected_boxes = det_boxes_r_[detected_indices]\n                detected_categories = det_category_r_[detected_indices]\n\n                det_detections_r = draw_box_in_img.draw_boxes_with_label_and_scores(np.squeeze(resized_img, 0),\n                                                                                    boxes=detected_boxes,\n                                                                                    labels=detected_categories,\n                                                                                    scores=detected_scores,\n                                                                                    method=1,\n                                                                                    in_graph=True)\n\n                save_dir = os.path.join(\'test_hrsc\', cfgs.VERSION, \'hrsc2016_img_vis\')\n                tools.mkdir(save_dir)\n\n                cv2.imwrite(save_dir + \'/{}.jpg\'.format(a_img_name),\n                            det_detections_r[:, :, ::-1])\n\n            if det_boxes_r_.shape[0] != 0:\n                resized_h, resized_w = resized_img.shape[1], resized_img.shape[2]\n                det_boxes_r_ = forward_convert(det_boxes_r_, False)\n                det_boxes_r_[:, 0::2] *= (raw_w / resized_w)\n                det_boxes_r_[:, 1::2] *= (raw_h / resized_h)\n                det_boxes_r_ = backward_convert(det_boxes_r_, False)\n\n            x_c, y_c, w, h, theta = det_boxes_r_[:, 0], det_boxes_r_[:, 1], det_boxes_r_[:, 2], \\\n                                    det_boxes_r_[:, 3], det_boxes_r_[:, 4]\n\n            boxes_r = np.transpose(np.stack([x_c, y_c, w, h, theta]))\n            dets_r = np.hstack((det_category_r_.reshape(-1, 1),\n                                det_scores_r_.reshape(-1, 1),\n                                boxes_r))\n            all_boxes_r.append(dets_r)\n\n            pbar.set_description(""Eval image %s"" % a_img_name)\n\n        # fw1 = open(cfgs.VERSION + \'_detections_r.pkl\', \'wb\')\n        # pickle.dump(all_boxes_r, fw1)\n        return all_boxes_r\n\n\ndef eval(num_imgs, img_dir, image_ext, test_annotation_path, draw_imgs):\n\n    retinanet = build_whole_network_r3det.DetectionNetwork(base_network_name=cfgs.NET_NAME,\n                                                           is_training=False)\n\n    all_boxes_r = eval_with_plac(img_dir=img_dir, det_net=retinanet,\n                                 num_imgs=num_imgs, image_ext=image_ext, draw_imgs=draw_imgs)\n\n    # with open(cfgs.VERSION + \'_detections_r.pkl\', \'rb\') as f2:\n    #     all_boxes_r = pickle.load(f2)\n    #\n    #     print(len(all_boxes_r))\n\n    imgs = os.listdir(img_dir)\n    real_test_imgname_list = [i.split(image_ext)[0] for i in imgs]\n\n    print(10 * ""**"")\n    print(\'rotation eval:\')\n    voc_eval_r.voc_evaluate_detections(all_boxes=all_boxes_r,\n                                       test_imgid_list=real_test_imgname_list,\n                                       test_annotation_path=test_annotation_path)\n\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a R2CNN network\')\n    parser.add_argument(\'--img_dir\', dest=\'img_dir\',\n                        help=\'images path\',\n                        default=\'/data/yangxue/dataset/HRSC2016/HRSC2016/Test/AllImages\', type=str)\n    parser.add_argument(\'--image_ext\', dest=\'image_ext\',\n                        help=\'image format\',\n                        default=\'.bmp\', type=str)\n    parser.add_argument(\'--test_annotation_path\', dest=\'test_annotation_path\',\n                        help=\'test annotate path\',\n                        default=\'/data/yangxue/dataset/HRSC2016/HRSC2016/Test/xmls\', type=str)\n    parser.add_argument(\'--gpu\', dest=\'gpu\',\n                        help=\'gpu index\',\n                        default=\'0\', type=str)\n    parser.add_argument(\'--draw_imgs\', \'-s\', default=False,\n                        action=\'store_true\')\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu\n\n    eval(np.inf, args.img_dir, args.image_ext, args.test_annotation_path, args.draw_imgs)\n\n'"
data/io/__init__.py,0,b''
data/io/convert_data_to_tfrecord.py,15,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport tensorflow as tf\nimport glob\nimport cv2\nfrom libs.label_name_dict.label_dict import *\nfrom help_utils.tools import *\n\ntf.app.flags.DEFINE_string(\'VOC_dir\', \'/data/ICDAR2015/\', \'Voc dir\')\ntf.app.flags.DEFINE_string(\'xml_dir\', \'icdar2015_xml\', \'xml dir\')\ntf.app.flags.DEFINE_string(\'image_dir\', \'train\', \'image dir\')\ntf.app.flags.DEFINE_string(\'save_name\', \'train\', \'save name\')\ntf.app.flags.DEFINE_string(\'save_dir\', \'../tfrecord/\', \'save name\')\ntf.app.flags.DEFINE_string(\'img_format\', \'.jpg\', \'format of image\')\ntf.app.flags.DEFINE_string(\'dataset\', \'ICDAR2015\', \'dataset\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 9],\n           and has [x1, y1, x2, y2, x3, y3, x4, y4, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        if child_of_root.tag == \'size\':\n            for child_item in child_of_root:\n                if child_item.tag == \'width\':\n                    img_width = int(child_item.text)\n                if child_item.tag == \'height\':\n                    img_height = int(child_item.text)\n\n        if child_of_root.tag == \'object\':\n            label = None\n            for child_item in child_of_root:\n                if child_item.tag == \'name\':\n                    label = NAME_LABEL_MAP[child_item.text]\n                if child_item.tag == \'bndbox\':\n                    tmp_box = []\n                    for node in child_item:\n                        tmp_box.append(int(node.text))\n                    assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    box_list.append(tmp_box)\n\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef convert_pascal_to_tfrecord():\n    xml_path = os.path.join(FLAGS.VOC_dir, FLAGS.xml_dir)\n    image_path = os.path.join(FLAGS.VOC_dir, FLAGS.image_dir)\n    save_path = os.path.join(FLAGS.save_dir, FLAGS.dataset + \'_\' + FLAGS.save_name + \'.tfrecord\')\n    mkdir(FLAGS.save_dir)\n\n    # writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n    # writer = tf.python_io.TFRecordWriter(path=save_path, options=writer_options)\n    writer = tf.python_io.TFRecordWriter(path=save_path)\n    for count, xml in enumerate(glob.glob(xml_path + \'/*.xml\')):\n        # to avoid path error in different development platform\n        xml = xml.replace(\'\\\\\', \'/\')\n\n        img_name = xml.split(\'/\')[-1].split(\'.\')[0] + FLAGS.img_format\n        img_path = image_path + \'/\' + img_name\n\n        if not os.path.exists(img_path):\n            print(\'{} is not exist!\'.format(img_path))\n            continue\n\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(xml)\n        # if img_height != 600 or img_width != 600:\n        #     continue\n\n        # img = np.array(Image.open(img_path))\n        img = cv2.imread(img_path)[:, :, ::-1]\n\n        feature = tf.train.Features(feature={\n            # do not need encode() in linux\n            \'img_name\': _bytes_feature(img_name.encode()),\n            # \'img_name\': _bytes_feature(img_name),\n            \'img_height\': _int64_feature(img_height),\n            \'img_width\': _int64_feature(img_width),\n            \'img\': _bytes_feature(img.tostring()),\n            \'gtboxes_and_label\': _bytes_feature(gtbox_label.tostring()),\n            \'num_objects\': _int64_feature(gtbox_label.shape[0])\n        })\n\n        example = tf.train.Example(features=feature)\n\n        writer.write(example.SerializeToString())\n\n        view_bar(\'Conversion progress\', count + 1, len(glob.glob(xml_path + \'/*.xml\')))\n\n    print(\'\\nConversion is complete!\')\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    # xml_path = \'../data/dataset/VOCdevkit/VOC2007/Annotations/000005.xml\'\n    # read_xml_gtbox_and_label(xml_path)\n\n    convert_pascal_to_tfrecord()\n'"
data/io/image_preprocess.py,18,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom libs.configs import cfgs\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 5].  gtboxes: [xmin, ymin, xmax, ymax]\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_xmin, new_ymin = xmin * new_w // img_w, ymin * new_h // img_h\n    new_xmax, new_ymax = xmax * new_w // img_w, ymax * new_h // img_h\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, new_ymin, new_xmax, new_ymax, label], axis=0))\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n        img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n        new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                               true_fn=lambda: (target_shortside_len,\n                                                max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                               false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                                 target_shortside_len))\n\n        img_tensor = tf.expand_dims(img_tensor, axis=0)\n        img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n        img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    xmin, ymin, xmax, ymax, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_xmax = w - xmin\n    new_xmin = w - xmax\n\n    return img_tensor, tf.transpose(tf.stack([new_xmin, ymin, new_xmax, ymax, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\n\n"""
data/io/image_preprocess_multi_gpu.py,50,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\n\n\ndef max_length_limitation(length, length_limitation):\n    return tf.cond(tf.less(length, length_limitation),\n                   true_fn=lambda: length,\n                   false_fn=lambda: length_limitation)\n\n\ndef short_side_resize(img_tensor, gtboxes_and_label, target_shortside_len, length_limitation=1200):\n    '''\n\n    :param img_tensor:[h, w, c], gtboxes_and_label:[-1, 9].\n    :param target_shortside_len:\n    :param length_limitation: set max length to avoid OUT OF MEMORY\n    :return:\n    '''\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                           true_fn=lambda: (target_shortside_len,\n                                            max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                           false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                             target_shortside_len))\n\n    img_tensor = tf.expand_dims(img_tensor, axis=0)\n    img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    x1, x2, x3, x4 = x1 * new_w // img_w, x2 * new_w // img_w, x3 * new_w // img_w, x4 * new_w // img_w\n    y1, y2, y3, y4 = y1 * new_h // img_h, y2 * new_h // img_h, y3 * new_h // img_h, y4 * new_h // img_h\n\n    img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n\n    return img_tensor, tf.transpose(tf.stack([x1, y1, x2, y2, x3, y3, x4, y4, label], axis=0)), new_h, new_w\n\n\ndef short_side_resize_for_inference_data(img_tensor, target_shortside_len, length_limitation=1200, is_resize=True):\n    if is_resize:\n      img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n      new_h, new_w = tf.cond(tf.less(img_h, img_w),\n                             true_fn=lambda: (target_shortside_len,\n                                              max_length_limitation(target_shortside_len * img_w // img_h, length_limitation)),\n                             false_fn=lambda: (max_length_limitation(target_shortside_len * img_h // img_w, length_limitation),\n                                               target_shortside_len))\n\n      img_tensor = tf.expand_dims(img_tensor, axis=0)\n      img_tensor = tf.image.resize_bilinear(img_tensor, [new_h, new_w])\n\n      img_tensor = tf.squeeze(img_tensor, axis=0)  # ensure image tensor rank is 3\n    return img_tensor\n\n\ndef flip_left_to_right(img_tensor, gtboxes_and_label):\n\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n\n    img_tensor = tf.image.flip_left_right(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n    new_x1 = w - x1\n    new_x2 = w - x2\n    new_x3 = w - x3\n    new_x4 = w - x4\n\n    return img_tensor, tf.transpose(tf.stack([new_x1, y1, new_x2, y2, new_x3, y3, new_x4, y4, label], axis=0))\n\n\ndef random_flip_left_right(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label= tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                           lambda: flip_left_to_right(img_tensor, gtboxes_and_label),\n                                           lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor,  gtboxes_and_label\n\n\ndef aspect_ratio_jittering(img_tensor, gtboxes_and_label, aspect_ratio=(0.8, 1.5)):\n    ratio_list = tf.range(aspect_ratio[0], aspect_ratio[1], delta=0.025)\n    ratio = tf.random_shuffle(ratio_list)[0]\n\n    img_h, img_w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    areas = img_h * img_w\n    areas = tf.cast(areas, tf.float32)\n    short_side = tf.sqrt(areas / ratio)\n    long_side = short_side * ratio\n    short_side = tf.cast(short_side, tf.int32)\n    long_side = tf.cast(long_side, tf.int32)\n\n    image, gtbox, new_h, new_w = tf.cond(tf.less(img_w, img_h),\n                                         true_fn=lambda: tf_resize_image(img_tensor, gtboxes_and_label, short_side,\n                                                                         long_side),\n                                         false_fn=lambda: tf_resize_image(img_tensor, gtboxes_and_label, long_side,\n                                                                          short_side))\n\n    return image, gtbox, new_h, new_w\n\n\ndef tf_resize_image(image, gtbox, rw, rh):\n    img_h, img_w = tf.shape(image)[0], tf.shape(image)[1]\n    image = tf.image.resize_bilinear(tf.expand_dims(image, axis=0), (rh, rw))\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtbox, axis=1)\n    new_x1 = x1 * rw // img_w\n    new_x2 = x2 * rw // img_w\n    new_x3 = x3 * rw // img_w\n    new_x4 = x4 * rw // img_w\n\n    new_y1 = y1 * rh // img_h\n    new_y2 = y2 * rh // img_h\n    new_y3 = y3 * rh // img_h\n    new_y4 = y4 * rh // img_h\n    gtbox = tf.transpose(tf.stack([new_x1, new_y1, new_x2, new_y2, new_x3, new_y3, new_x4, new_y4, label], axis=0))\n    return tf.squeeze(image, axis=0), gtbox, rh, rw\n\n\ndef flip_up_down(img_tensor, gtboxes_and_label):\n    h, w = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1]\n    img_tensor = tf.image.flip_up_down(img_tensor)\n\n    x1, y1, x2, y2, x3, y3, x4, y4, label = tf.unstack(gtboxes_and_label, axis=1)\n\n    new_y1 = h - y1\n    new_y2 = h - y2\n    new_y3 = h - y3\n    new_y4 = h - y4\n\n    return img_tensor, tf.transpose(tf.stack([x1, new_y1, x2, new_y2, x3, new_y3, x4, new_y4, label], axis=0))\n\n\ndef random_flip_up_down(img_tensor, gtboxes_and_label):\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.5),\n                                            lambda: flip_up_down(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor, gtboxes_and_label\n\n\ndef random_rgb2gray(img_tensor, gtboxes_and_label):\n    '''\n    :param img_tensor: tf.float32\n    :return:\n    '''\n    def rgb2gray(img, gtboxes_and_label):\n\n        label = gtboxes_and_label[:, -1]\n\n        if cfgs.DATASET_NAME.startswith('DOTA'):\n            if NAME_LABEL_MAP['swimming-pool'] in label:\n                # do not change color, because swimming-pool need color\n                return img\n\n        coin = np.random.rand()\n        if coin < 0.3:\n            img = np.asarray(img, dtype=np.float32)\n            r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n            gray = r * 0.299 + g * 0.587 + b * 0.114\n            img = np.stack([gray, gray, gray], axis=2)\n            return img\n        else:\n            return img\n\n    h, w, c = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1], tf.shape(img_tensor)[2]\n    img_tensor = tf.py_func(rgb2gray,\n                            inp=[img_tensor, gtboxes_and_label],\n                            Tout=tf.float32)\n    img_tensor = tf.reshape(img_tensor, shape=[h, w, c])\n\n    return img_tensor\n\n\ndef rotate_img_np(img, gtboxes_and_label, r_theta):\n    h, w, c = img.shape\n    center = (w // 2, h // 2)\n\n    M = cv2.getRotationMatrix2D(center, r_theta, 1.0)\n    cos, sin = np.abs(M[0, 0]), np.abs(M[0, 1])\n    nW, nH = int(h*sin + w*cos), int(h*cos + w*sin)  # new W and new H\n    M[0, 2] += (nW/2) - center[0]\n    M[1, 2] += (nH/2) - center[1]\n    rotated_img = cv2.warpAffine(img, M, (nW, nH))\n    # -------\n\n    new_points_list = []\n    obj_num = len(gtboxes_and_label)\n    for st in range(0, 7, 2):\n        points = gtboxes_and_label[:, st:st+2]\n        expand_points = np.concatenate((points, np.ones(shape=(obj_num, 1))), axis=1)\n        new_points = np.dot(M, expand_points.T)\n        new_points = new_points.T\n        new_points_list.append(new_points)\n    gtboxes = np.concatenate(new_points_list, axis=1)\n    gtboxes_and_label = np.concatenate((gtboxes, gtboxes_and_label[:, -1].reshape(-1, 1)), axis=1)\n    gtboxes_and_label = np.asarray(gtboxes_and_label, dtype=np.int32)\n\n    return rotated_img, gtboxes_and_label\n\n\ndef rotate_img(img_tensor, gtboxes_and_label):\n\n    # thetas = tf.constant([-30, -60, -90, 30, 60, 90])\n    thetas = tf.range(-90, 90+16, delta=15)\n    # -90, -75, -60, -45, -30, -15,   0,  15,  30,  45,  60,  75,  90\n\n    theta = tf.random_shuffle(thetas)[0]\n\n    img_tensor, gtboxes_and_label = tf.py_func(rotate_img_np,\n                                               inp=[img_tensor, gtboxes_and_label, theta],\n                                               Tout=[tf.float32, tf.int32])\n\n    h, w, c = tf.shape(img_tensor)[0], tf.shape(img_tensor)[1], tf.shape(img_tensor)[2]\n    img_tensor = tf.reshape(img_tensor, [h, w, c])\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n\n    return img_tensor, gtboxes_and_label\n\n\ndef random_rotate_img(img_tensor, gtboxes_and_label):\n\n    img_tensor, gtboxes_and_label = tf.cond(tf.less(tf.random_uniform(shape=[], minval=0, maxval=1), 0.6),\n                                            lambda: rotate_img(img_tensor, gtboxes_and_label),\n                                            lambda: (img_tensor, gtboxes_and_label))\n\n    return img_tensor, gtboxes_and_label\n\n\n"""
data/io/read_tfrecord.py,23,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom data.io import image_preprocess\nfrom libs.configs import cfgs\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 5])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n        img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                         gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                    target_shortside_len=shortside_len,\n                                                                    length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    if dataset_name not in [\'ship\', \'spacenet\', \'pascal\', \'coco\']:\n        raise ValueError(\'dataSet name must be in pascal, coco spacenet and ship\')\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    img_name, img, gtboxes_and_label, num_obs = read_and_prepocess_single_img(filename_queue, shortside_len,\n                                                                              is_training=is_training)\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs],\n                       batch_size=batch_size,\n                       capacity=1,\n                       num_threads=1,\n                       dynamic_pad=True)\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch\n'"
data/io/read_tfrecord_multi_gpu.py,32,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport os\nimport sys\nsys.path.append(\'../../\')\n\nfrom data.io import image_preprocess_multi_gpu as image_preprocess\nfrom libs.configs import cfgs\n\n\ndef read_single_example_and_decode(filename_queue):\n\n    # tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n\n    # reader = tf.TFRecordReader(options=tfrecord_options)\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n\n    features = tf.parse_single_example(\n        serialized=serialized_example,\n        features={\n            \'img_name\': tf.FixedLenFeature([], tf.string),\n            \'img_height\': tf.FixedLenFeature([], tf.int64),\n            \'img_width\': tf.FixedLenFeature([], tf.int64),\n            \'img\': tf.FixedLenFeature([], tf.string),\n            \'gtboxes_and_label\': tf.FixedLenFeature([], tf.string),\n            \'num_objects\': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    img_name = features[\'img_name\']\n    img_height = tf.cast(features[\'img_height\'], tf.int32)\n    img_width = tf.cast(features[\'img_width\'], tf.int32)\n    img = tf.decode_raw(features[\'img\'], tf.uint8)\n\n    img = tf.reshape(img, shape=[img_height, img_width, 3])\n\n    gtboxes_and_label = tf.decode_raw(features[\'gtboxes_and_label\'], tf.int32)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-1, 9])\n\n    num_objects = tf.cast(features[\'num_objects\'], tf.int32)\n    return img_name, img, gtboxes_and_label, num_objects\n\n\ndef read_and_prepocess_single_img(filename_queue, shortside_len, is_training):\n\n    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)\n\n    img = tf.cast(img, tf.float32)\n\n    if is_training:\n\n        if cfgs.RGB2GRAY:\n            # img, gtboxes_and_label = image_preprocess.aspect_ratio_jittering(img, gtboxes_and_label)\n            img = image_preprocess.random_rgb2gray(img_tensor=img, gtboxes_and_label=gtboxes_and_label)\n\n        if cfgs.IMG_ROTATE:\n            # rotate with 0.5 prob. and if rotate, if will random choose a theta from : tf.range(-90, 90+16, delta=15)\n            img, gtboxes_and_label = image_preprocess.random_rotate_img(img_tensor=img,\n                                                                        gtboxes_and_label=gtboxes_and_label)\n\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n        if cfgs.HORIZONTAL_FLIP:\n            img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,\n                                                                             gtboxes_and_label=gtboxes_and_label)\n        if cfgs.VERTICAL_FLIP:\n            img, gtboxes_and_label = image_preprocess.random_flip_up_down(img_tensor=img,\n                                                                          gtboxes_and_label=gtboxes_and_label)\n\n    else:\n        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,\n                                                                                  target_shortside_len=shortside_len,\n                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)\n    if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n        img = img / 255 - tf.constant([[cfgs.PIXEL_MEAN_]])\n    else:\n        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  # sub pixel mean at last\n    return img_name, img, gtboxes_and_label, num_objects, img_h, img_w\n\n\ndef next_batch(dataset_name, batch_size, shortside_len, is_training):\n    \'\'\'\n    :return:\n    img_name_batch: shape(1, 1)\n    img_batch: shape:(1, new_imgH, new_imgW, C)\n    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]\n    \'\'\'\n    # assert batch_size == 1, ""we only support batch_size is 1.We may support large batch_size in the future""\n\n    valid_dataset= [\'DOTA1.5\', \'ICDAR2015\', \'pascal\', \'coco\', \'bdd100k\', \'DOTA\', \'DOTA800\', \'DOTA600\', \'HRSC2016\', \'UCAS-AOD\']\n    if dataset_name not in valid_dataset:\n        raise ValueError(\'dataSet name must be in {}\'.format(valid_dataset))\n\n    if is_training:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_train*\')\n    else:\n        pattern = os.path.join(\'../data/tfrecord\', dataset_name + \'_test*\')\n\n    print(\'tfrecord path is -->\', os.path.abspath(pattern))\n\n    filename_tensorlist = tf.train.match_filenames_once(pattern)\n\n    filename_queue = tf.train.string_input_producer(filename_tensorlist)\n\n    img_name, img, gtboxes_and_label, num_obs, img_h, img_w = read_and_prepocess_single_img(filename_queue,\n                                                                                            shortside_len,\n                                                                                            is_training=is_training)\n\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch = \\\n        tf.train.batch(\n                       [img_name, img, gtboxes_and_label, num_obs, img_h, img_w],\n                       batch_size=batch_size,\n                       capacity=16,\n                       num_threads=16,\n                       dynamic_pad=True)\n\n    return img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch\n\n\nif __name__ == \'__main__\':\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0,1\'\n    num_gpu = len(cfgs.GPU_GROUP.strip().split(\',\'))\n    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \\\n        next_batch(dataset_name=cfgs.DATASET_NAME,  # \'pascal\', \'coco\'\n                   batch_size=cfgs.BATCH_SIZE * 8,\n                   shortside_len=cfgs.IMG_SHORT_SIDE_LEN,\n                   is_training=True)\n    gtboxes_and_label = tf.reshape(gtboxes_and_label_batch, [-1, 9])\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n\n    with tf.Session(config=config) as sess:\n        sess.run(init_op)\n\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess, coord)\n\n        img_name_batch_, img_batch_, gtboxes_and_label_batch_, num_objects_batch_, img_h_batch_, img_w_batch_ \\\n            = sess.run([img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch])\n\n        print(img_name_batch_.shape)\n        print(img_batch_.shape)\n        print(gtboxes_and_label_batch_.shape)\n        print(num_objects_batch_.shape)\n        print(img_h_batch_.shape)\n        print(\'debug\')\n\n        coord.request_stop()\n        coord.join(threads)\n'"
libs/box_utils/__init__.py,0,b''
libs/box_utils/anchor_utils.py,19,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nimport sys\nsys.path.append(\'../..\')\n\nfrom libs.configs import cfgs\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios,\n                 featuremap_height, featuremap_width,\n                 stride, name=\'make_anchors\'):\n    \'\'\'\n    :param base_anchor_size:256\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [x_center, y_center, w, h]\n\n        ws, hs = enum_ratios(enum_scales(base_anchor, anchor_scales),\n                             anchor_ratios)  # per locations ws and hs\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride\n\n        if cfgs.USE_CENTER_OFFSET:\n            x_centers += stride / 2.\n            y_centers += stride / 2.\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_sizes = tf.stack([ws, hs], axis=2)\n        box_sizes = tf.reshape(box_sizes, [-1, 2])\n        # anchors = tf.concat([anchor_centers, box_sizes], axis=1)\n        anchors = tf.concat([anchor_centers - 0.5*box_sizes,\n                             anchor_centers + 0.5*box_sizes], axis=1)\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios(anchors, anchor_ratios):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1, 1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1, 1])\n\n    return ws, hs\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    base_anchor_size = 32\n    anchor_scales = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n    anchor_ratios = [0.5, 2.0, 1.0]\n    anchors = make_anchors(base_anchor_size=base_anchor_size, anchor_ratios=anchor_ratios,\n                           anchor_scales=anchor_scales,\n                           featuremap_width=512,\n                           featuremap_height=512,\n                           stride=8)\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)\n        anchor_result = sess.run(anchors)\n    print(anchor_result[:10])\n\n'"
libs/box_utils/bbox_transform.py,6,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef bbox_transform_inv(boxes, deltas, scale_factors=None):\n    dx = deltas[:, 0]\n    dy = deltas[:, 1]\n    dw = deltas[:, 2]\n    dh = deltas[:, 3]\n\n    if scale_factors:\n        dx /= scale_factors[0]\n        dy /= scale_factors[1]\n        dw /= scale_factors[2]\n        dh /= scale_factors[3]\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    pred_ctr_x = dx * widths + ctr_x\n    pred_ctr_y = dy * heights + ctr_y\n    pred_w = tf.exp(dw) * widths\n    pred_h = tf.exp(dh) * heights\n\n    predict_xmin = pred_ctr_x - 0.5 * pred_w\n    predict_xmax = pred_ctr_x + 0.5 * pred_w\n    predict_ymin = pred_ctr_y - 0.5 * pred_h\n    predict_ymax = pred_ctr_y + 0.5 * pred_h\n\n    return tf.transpose(tf.stack([predict_xmin, predict_ymin,\n                                  predict_xmax, predict_ymax]))\n\n\ndef bbox_transform(ex_rois, gt_rois, scale_factors=None):\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths + 1e-5)\n    targets_dh = np.log(gt_heights / ex_heights + 1e-5)\n\n    if scale_factors:\n        targets_dx *= scale_factors[0]\n        targets_dy *= scale_factors[1]\n        targets_dw *= scale_factors[2]\n        targets_dh *= scale_factors[3]\n\n    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n\n    return targets\n\n\ndef rbbox_transform_inv(boxes, deltas, scale_factors=None):\n    dx = deltas[:, 0]\n    dy = deltas[:, 1]\n    dw = deltas[:, 2]\n    dh = deltas[:, 3]\n    dtheta = deltas[:, 4]\n\n    if scale_factors:\n        dx /= scale_factors[0]\n        dy /= scale_factors[1]\n        dw /= scale_factors[2]\n        dh /= scale_factors[3]\n        dtheta /= scale_factors[4]\n\n    pred_ctr_x = dx * boxes[:, 2] + boxes[:, 0]\n    pred_ctr_y = dy * boxes[:, 3] + boxes[:, 1]\n    pred_w = tf.exp(dw) * boxes[:, 2]\n    pred_h = tf.exp(dh) * boxes[:, 3]\n\n    pred_theta = dtheta * 180 / np.pi + boxes[:, 4]\n\n    return tf.transpose(tf.stack([pred_ctr_x, pred_ctr_y,\n                                  pred_w, pred_h, pred_theta]))\n\n\ndef rbbox_transform(ex_rois, gt_rois, scale_factors=None):\n\n    targets_dx = (gt_rois[:, 0] - ex_rois[:, 0]) / ex_rois[:, 2]\n    targets_dy = (gt_rois[:, 1] - ex_rois[:, 1]) / ex_rois[:, 3]\n    targets_dw = np.log(gt_rois[:, 2] / ex_rois[:, 2] + 1e-5)\n    targets_dh = np.log(gt_rois[:, 3] / ex_rois[:, 3] + 1e-5)\n\n    targets_dtheta = (gt_rois[:, 4] - ex_rois[:, 4]) * np.pi / 180\n\n    if scale_factors:\n        targets_dx *= scale_factors[0]\n        targets_dy *= scale_factors[1]\n        targets_dw *= scale_factors[2]\n        targets_dh *= scale_factors[3]\n        targets_dtheta *= scale_factors[4]\n\n    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh, targets_dtheta)).transpose()\n\n    return targets\n'"
libs/box_utils/boxes_utils.py,35,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef ious_calu(boxes_1, boxes_2):\n    '''\n\n    :param boxes_1: [N, 4] [xmin, ymin, xmax, ymax]\n    :param boxes_2: [M, 4] [xmin, ymin. xmax, ymax]\n    :return:\n    '''\n    boxes_1 = tf.cast(boxes_1, tf.float32)\n    boxes_2 = tf.cast(boxes_2, tf.float32)\n    xmin_1, ymin_1, xmax_1, ymax_1 = tf.split(boxes_1, 4, axis=1)  # xmin_1 shape is [N, 1]..\n    xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # xmin_2 shape is [M, ]..\n\n    max_xmin = tf.maximum(xmin_1, xmin_2)\n    min_xmax = tf.minimum(xmax_1, xmax_2)\n\n    max_ymin = tf.maximum(ymin_1, ymin_2)\n    min_ymax = tf.minimum(ymax_1, ymax_2)\n\n    overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    ious = overlaps / (area_1 + area_2 - overlaps)\n\n    return ious\n\n\ndef clip_boxes_to_img_boundaries(boxes, img_shape):\n    '''\n\n    :param decode_boxes:\n    :return: decode boxes, and already clip to boundaries\n    '''\n\n    with tf.name_scope('clip_boxes_to_img_boundaries'):\n\n        # xmin, ymin, xmax, ymax = tf.unstack(decode_boxes, axis=1)\n        xmin = boxes[:, 0]\n        ymin = boxes[:, 1]\n        xmax = boxes[:, 2]\n        ymax = boxes[:, 3]\n        img_h, img_w = img_shape[1], img_shape[2]\n\n        img_h, img_w = tf.cast(img_h, tf.float32), tf.cast(img_w, tf.float32)\n\n        xmin = tf.maximum(tf.minimum(xmin, img_w-1.), 0.)\n        ymin = tf.maximum(tf.minimum(ymin, img_h-1.), 0.)\n\n        xmax = tf.maximum(tf.minimum(xmax, img_w-1.), 0.)\n        ymax = tf.maximum(tf.minimum(ymax, img_h-1.), 0.)\n\n        return tf.transpose(tf.stack([xmin, ymin, xmax, ymax]))\n\n\ndef filter_outside_boxes(boxes, img_h, img_w):\n    '''\n    :param anchors:boxes with format [xmin, ymin, xmax, ymax]\n    :param img_h: height of image\n    :param img_w: width of image\n    :return: indices of anchors that inside the image boundary\n    '''\n\n    with tf.name_scope('filter_outside_boxes'):\n        xmin, ymin, xmax, ymax = tf.unstack(boxes, axis=1)\n\n        xmin_index = tf.greater_equal(xmin, 0)\n        ymin_index = tf.greater_equal(ymin, 0)\n        xmax_index = tf.less_equal(xmax, tf.cast(img_w, tf.float32))\n        ymax_index = tf.less_equal(ymax, tf.cast(img_h, tf.float32))\n\n        indices = tf.transpose(tf.stack([xmin_index, ymin_index, xmax_index, ymax_index]))\n        indices = tf.cast(indices, dtype=tf.int32)\n        indices = tf.reduce_sum(indices, axis=1)\n        indices = tf.where(tf.equal(indices, 4))\n        # indices = tf.equal(indices, 4)\n        return tf.reshape(indices, [-1])\n\n\ndef padd_boxes_with_zeros(boxes, scores, max_num_of_boxes):\n\n    '''\n    num of boxes less than max num of boxes, so it need to pad with zeros[0, 0, 0, 0]\n    :param boxes:\n    :param scores: [-1]\n    :param max_num_of_boxes:\n    :return:\n    '''\n\n    pad_num = tf.cast(max_num_of_boxes, tf.int32) - tf.shape(boxes)[0]\n\n    zero_boxes = tf.zeros(shape=[pad_num, 4], dtype=boxes.dtype)\n    zero_scores = tf.zeros(shape=[pad_num], dtype=scores.dtype)\n\n    final_boxes = tf.concat([boxes, zero_boxes], axis=0)\n\n    final_scores = tf.concat([scores, zero_scores], axis=0)\n\n    return final_boxes, final_scores"""
libs/box_utils/coordinate_convert.py,19,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\n\ndef forward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x_c, y_c, w, h, theta]\n    :return: format [x1, y1, x2, y2, x3, y3, x4, y4]\n    """"""\n\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7], rect[5]])\n    else:\n        for rect in coordinate:\n            box = cv2.boxPoints(((rect[0], rect[1]), (rect[2], rect[3]), rect[4]))\n            box = np.reshape(box, [-1, ])\n            boxes.append([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]])\n\n    return np.array(boxes, dtype=np.float32)\n\n\ndef backward_convert(coordinate, with_label=True):\n    """"""\n    :param coordinate: format [x1, y1, x2, y2, x3, y3, x4, y4, (label)]\n    :param with_label: default True\n    :return: format [x_c, y_c, w, h, theta, (label)]\n    """"""\n\n    boxes = []\n    if with_label:\n        for rect in coordinate:\n            box = np.int0(rect[:-1])\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta, rect[-1]])\n\n    else:\n        for rect in coordinate:\n            box = np.int0(rect)\n            box = box.reshape([4, 2])\n            rect1 = cv2.minAreaRect(box)\n\n            x, y, w, h, theta = rect1[0][0], rect1[0][1], rect1[1][0], rect1[1][1], rect1[2]\n            boxes.append([x, y, w, h, theta])\n\n    return np.array(boxes, dtype=np.float32)\n\n\ndef get_horizen_minAreaRectangle(boxes, with_label=True):\n\n    if with_label:\n        boxes = tf.reshape(boxes, [-1, 9])\n\n        boxes_shape = tf.shape(boxes)\n        x_list = tf.strided_slice(boxes, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(boxes, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1] - 1],\n                                  strides=[1, 2])\n\n        label = tf.unstack(boxes, axis=1)[-1]\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n        return tf.transpose(tf.stack([x_min, y_min, x_max, y_max, label], axis=0))\n    else:\n        boxes = tf.reshape(boxes, [-1, 8])\n\n        boxes_shape = tf.shape(boxes)\n        x_list = tf.strided_slice(boxes, begin=[0, 0], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n        y_list = tf.strided_slice(boxes, begin=[0, 1], end=[boxes_shape[0], boxes_shape[1]],\n                                  strides=[1, 2])\n\n        y_max = tf.reduce_max(y_list, axis=1)\n        y_min = tf.reduce_min(y_list, axis=1)\n        x_max = tf.reduce_max(x_list, axis=1)\n        x_min = tf.reduce_min(x_list, axis=1)\n\n    return tf.transpose(tf.stack([x_min, y_min, x_max, y_max], axis=0))\n\n\ndef coordinate_present_convert(coords, mode=1):\n    """"""\n    :param coords: shape [-1, 5]\n    :param mode: -1 convert coords range to [-90, 90), 1 convert coords range to [-90, 0)\n    :return: shape [-1, 5]\n    """"""\n    # angle range from [-90, 0) to [-180, 0)\n    if mode == -1:\n        w, h = coords[:, 2], coords[:, 3]\n\n        remain_mask = np.greater(w, h)\n        convert_mask = np.logical_not(remain_mask).astype(np.int32)\n        remain_mask = remain_mask.astype(np.int32)\n\n        remain_coords = coords * np.reshape(remain_mask, [-1, 1])\n\n        coords[:, [2, 3]] = coords[:, [3, 2]]\n        coords[:, 4] += 90\n\n        convert_coords = coords * np.reshape(convert_mask, [-1, 1])\n\n        coords_new = remain_coords + convert_coords\n\n        coords_new[:, 4] -= 90\n\n    # angle range from [-180, 0) to [-90, 0)\n    elif mode == 1:\n        coords[:, 4] += 90\n\n        # theta = coords[:, 4]\n        # remain_mask = np.logical_and(np.greater_equal(theta, -90), np.less(theta, 0))\n        # convert_mask = np.logical_not(remain_mask)\n        #\n        # remain_coords = coords * np.reshape(remain_mask, [-1, 1])\n        #\n        # coords[:, [2, 3]] = coords[:, [3, 2]]\n        # coords[:, 4] -= 90\n        #\n        # convert_coords = coords * np.reshape(convert_mask, [-1, 1])\n        #\n        # coords_new = remain_coords + convert_coords\n\n        xlt, ylt = -1 * coords[:, 2] / 2.0, coords[:, 3] / 2.0\n        xld, yld = -1 * coords[:, 2] / 2.0, -1 * coords[:, 3] / 2.0\n        xrd, yrd = coords[:, 2] / 2.0, -1 * coords[:, 3] / 2.0\n        xrt, yrt = coords[:, 2] / 2.0, coords[:, 3] / 2.0\n\n        theta = -coords[:, -1] / 180 * np.pi\n\n        xlt_ = np.cos(theta) * xlt + np.sin(theta) * ylt + coords[:, 0]\n        ylt_ = -np.sin(theta) * xlt + np.cos(theta) * ylt + coords[:, 1]\n\n        xrt_ = np.cos(theta) * xrt + np.sin(theta) * yrt + coords[:, 0]\n        yrt_ = -np.sin(theta) * xrt + np.cos(theta) * yrt + coords[:, 1]\n\n        xld_ = np.cos(theta) * xld + np.sin(theta) * yld + coords[:, 0]\n        yld_ = -np.sin(theta) * xld + np.cos(theta) * yld + coords[:, 1]\n\n        xrd_ = np.cos(theta) * xrd + np.sin(theta) * yrd + coords[:, 0]\n        yrd_ = -np.sin(theta) * xrd + np.cos(theta) * yrd + coords[:, 1]\n\n        convert_box = np.transpose(np.stack([xlt_, ylt_, xrt_, yrt_, xrd_, yrd_, xld_, yld_], axis=0))\n\n        coords_new = backward_convert(convert_box, False)\n\n\n    else:\n        raise Exception(\'mode error!\')\n\n    return np.array(coords_new, dtype=np.float32)\n\n\ndef coords_regular(coords):\n    # [-180, -90) -> [-90, 90)\n    theta = coords[:, 4]\n    convert_mask = np.logical_and(np.greater_equal(theta, -180), np.less(theta, -90))\n    remain_mask = np.logical_not(convert_mask)\n    remain_coords = coords * np.reshape(remain_mask, [-1, 1])\n\n    coords[:, [2, 3]] = coords[:, [3, 2]]\n    coords[:, 4] -= 90\n\n    convert_coords = coords * np.reshape(convert_mask, [-1, 1])\n    coords_new = remain_coords + convert_coords\n    return coords_new\n\n\nif __name__ == \'__main__\':\n    coord = np.array([[150, 150, 50, 100, -90, 1],\n                      [150, 150, 100, 50, -90, 1],\n                      [150, 150, 50, 100, -45, 1],\n                      [150, 150, 100, 50, -45, 1]])\n\n    coord1 = np.array([[150, 150, 100, 50, 0],\n                      [150, 150, 100, 50, -90],\n                      [150, 150, 100, 50, 45],\n                      [150, 150, 100, 50, -45]])\n\n    coord2 = forward_convert(coord)\n    # coord3 = forward_convert(coord1, mode=-1)\n    print(coord2)\n    # print(coord3-coord2)\n    # coord_label = np.array([[167., 203., 96., 132., 132., 96., 203., 167., 1.]])\n    #\n    # coord4 = back_forward_convert(coord_label, mode=1)\n    # coord5 = back_forward_convert(coord_label)\n\n    # print(coord4)\n    # print(coord5)\n\n    # coord3 = coordinate_present_convert(coord, -1)\n    # print(coord3)\n    # coord4 = coordinate_present_convert(coord3, mode=1)\n# print(coord4)'"
libs/box_utils/draw_box_in_img.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport numpy as np\n\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\n\nfrom libs.configs import cfgs\nfrom libs.label_name_dict.label_dict import LABEL_NAME_MAP\nfrom help_utils.tools import get_dota_short_names\nNOT_DRAW_BOXES = 0\nONLY_DRAW_BOXES = -1\nONLY_DRAW_BOXES_WITH_SCORES = -2\n\nSTANDARD_COLORS = [\n    \'AliceBlue\', \'Chartreuse\', \'Aqua\', \'Aquamarine\', \'Azure\', \'Beige\', \'Bisque\',\n    \'BlanchedAlmond\', \'BlueViolet\', \'BurlyWood\', \'CadetBlue\', \'AntiqueWhite\',\n    \'Chocolate\', \'Coral\', \'CornflowerBlue\', \'Cornsilk\', \'Crimson\', \'Cyan\',\n    \'DarkCyan\', \'DarkGoldenRod\', \'DarkGrey\', \'DarkKhaki\', \'DarkOrange\',\n    \'DarkOrchid\', \'DarkSalmon\', \'DarkSeaGreen\', \'DarkTurquoise\', \'DarkViolet\',\n    \'DeepPink\', \'DeepSkyBlue\', \'DodgerBlue\', \'FireBrick\', \'FloralWhite\',\n    \'ForestGreen\', \'Fuchsia\', \'Gainsboro\', \'GhostWhite\', \'Gold\', \'GoldenRod\',\n    \'Salmon\', \'Tan\', \'HoneyDew\', \'HotPink\', \'IndianRed\', \'Ivory\', \'Khaki\',\n    \'Lavender\', \'LavenderBlush\', \'LawnGreen\', \'LemonChiffon\', \'LightBlue\',\n    \'LightCoral\', \'LightCyan\', \'LightGoldenRodYellow\', \'LightGray\', \'LightGrey\',\n    \'LightGreen\', \'LightPink\', \'LightSalmon\', \'LightSeaGreen\', \'LightSkyBlue\',\n    \'LightSlateGray\', \'LightSlateGrey\', \'LightSteelBlue\', \'LightYellow\', \'Lime\',\n    \'LimeGreen\', \'Linen\', \'Magenta\', \'MediumAquaMarine\', \'MediumOrchid\',\n    \'MediumPurple\', \'MediumSeaGreen\', \'MediumSlateBlue\', \'MediumSpringGreen\',\n    \'MediumTurquoise\', \'MediumVioletRed\', \'MintCream\', \'MistyRose\', \'Moccasin\',\n    \'NavajoWhite\', \'OldLace\', \'Olive\', \'OliveDrab\', \'Orange\', \'OrangeRed\',\n    \'Orchid\', \'PaleGoldenRod\', \'PaleGreen\', \'PaleTurquoise\', \'PaleVioletRed\',\n    \'PapayaWhip\', \'PeachPuff\', \'Peru\', \'Pink\', \'Plum\', \'PowderBlue\', \'Purple\',\n    \'Red\', \'RosyBrown\', \'RoyalBlue\', \'SaddleBrown\', \'Green\', \'SandyBrown\',\n    \'SeaGreen\', \'SeaShell\', \'Sienna\', \'Silver\', \'SkyBlue\', \'SlateBlue\',\n    \'SlateGray\', \'SlateGrey\', \'Snow\', \'SpringGreen\', \'SteelBlue\', \'GreenYellow\',\n    \'Teal\', \'Thistle\', \'Tomato\', \'Turquoise\', \'Violet\', \'Wheat\', \'White\',\n    \'WhiteSmoke\', \'Yellow\', \'YellowGreen\', \'LightBlue\', \'LightGreen\'\n]\nFONT = ImageFont.load_default()\n\n\ndef draw_a_rectangel_in_img(draw_obj, box, color, width, method):\n    \'\'\'\n    use draw lines to draw rectangle. since the draw_rectangle func can not modify the width of rectangle\n    :param draw_obj:\n    :param box: [x1, y1, x2, y2]\n    :return:\n    \'\'\'\n    # color = (0, 255, 0)\n    if method == 0:\n        x1, y1, x2, y2 = box[0], box[1], box[2], box[3]\n        top_left, top_right = (x1, y1), (x2, y1)\n        bottom_left, bottom_right = (x1, y2), (x2, y2)\n\n        draw_obj.line(xy=[top_left, top_right],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[top_left, bottom_left],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[bottom_left, bottom_right],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[top_right, bottom_right],\n                      fill=color,\n                      width=width)\n    else:\n        x_c, y_c, w, h, theta = box[0], box[1], box[2], box[3], box[4]\n        rect = ((x_c, y_c), (w, h), theta)\n        rect = cv2.boxPoints(rect)\n        rect = np.int0(rect)\n        draw_obj.line(xy=[(rect[0][0], rect[0][1]), (rect[1][0], rect[1][1])],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[(rect[1][0], rect[1][1]), (rect[2][0], rect[2][1])],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[(rect[2][0], rect[2][1]), (rect[3][0], rect[3][1])],\n                      fill=color,\n                      width=width)\n        draw_obj.line(xy=[(rect[3][0], rect[3][1]), (rect[0][0], rect[0][1])],\n                      fill=color,\n                      width=width)\n\n\ndef only_draw_scores(draw_obj, box, score, color):\n\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x+60, y+10],\n                       fill=color)\n    draw_obj.text(xy=(x, y),\n                  text=""obj:"" + str(round(score, 2)),\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_label_with_scores(draw_obj, box, label, score, color):\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x + 60, y + 10],\n                       fill=color)\n\n    txt = LABEL_NAME_MAP[label] + \':\' + str(round(score, 2))\n    draw_obj.text(xy=(x, y),\n                  text=txt,\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_label_with_scores_csl(draw_obj, box, label, score, color):\n    x, y = box[0], box[1]\n    draw_obj.rectangle(xy=[x, y, x + 60, y + 20],\n                       fill=color)\n\n    if cfgs.DATASET_NAME == \'DOTA\':\n        label_name = get_dota_short_names(LABEL_NAME_MAP[label])\n    else:\n        label_name = LABEL_NAME_MAP[label]\n    txt = label_name + \':\' + str(round(score, 2))\n    # txt = \' \' + label_name\n    draw_obj.text(xy=(x, y),\n                  text=txt,\n                  fill=\'black\',\n                  font=FONT)\n    if cfgs.ANGLE_RANGE == 180:\n        if box[2] < box[3]:\n            angle = box[-1] + 90\n        else:\n            angle = box[-1]\n    else:\n        angle = box[-1]\n    txt_angle = \'angle:%.1f\' % angle\n    # txt_angle = \' %.1f\' % angle\n    draw_obj.text(xy=(x, y+10),\n                  text=txt_angle,\n                  fill=\'black\',\n                  font=FONT)\n\n\ndef draw_boxes_with_label_and_scores(img_array, boxes, labels, scores, method, is_csl=False, in_graph=True):\n    if in_graph:\n        if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n            img_array = (img_array * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n        else:\n            img_array = img_array + np.array(cfgs.PIXEL_MEAN)\n    img_array.astype(np.float32)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n\n    img_obj = Image.fromarray(img_array)\n    raw_img_obj = img_obj.copy()\n\n    draw_obj = ImageDraw.Draw(img_obj)\n    num_of_objs = 0\n\n    for box, a_label, a_score in zip(boxes, labels, scores):\n\n        if a_label != NOT_DRAW_BOXES:\n            num_of_objs += 1\n            draw_a_rectangel_in_img(draw_obj, box, color=STANDARD_COLORS[a_label], width=3, method=method)\n            if a_label == ONLY_DRAW_BOXES:  # -1\n                continue\n            elif a_label == ONLY_DRAW_BOXES_WITH_SCORES:  # -2\n                 only_draw_scores(draw_obj, box, a_score, color=\'White\')\n            else:\n                if is_csl:\n                    draw_label_with_scores_csl(draw_obj, box, a_label, a_score, color=\'White\')\n                else:\n                    draw_label_with_scores(draw_obj, box, a_label, a_score, color=\'White\')\n\n    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.7)\n\n    return np.array(out_img_obj)\n\n\ndef draw_boxes(img_array, boxes, labels, scores, color, method, is_csl=False, in_graph=True):\n    if in_graph:\n        if cfgs.NET_NAME in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n            img_array = (img_array * np.array(cfgs.PIXEL_STD) + np.array(cfgs.PIXEL_MEAN_)) * 255\n        else:\n            img_array = img_array + np.array(cfgs.PIXEL_MEAN)\n    img_array.astype(np.float32)\n    boxes = boxes.astype(np.int64)\n    labels = labels.astype(np.int32)\n    img_array = np.array(img_array * 255 / np.max(img_array), dtype=np.uint8)\n\n    img_obj = Image.fromarray(img_array)\n    raw_img_obj = img_obj.copy()\n\n    draw_obj = ImageDraw.Draw(img_obj)\n    num_of_objs = 0\n    for box, a_label, a_score in zip(boxes, labels, scores):\n\n        if a_label != NOT_DRAW_BOXES:\n            num_of_objs += 1\n            draw_a_rectangel_in_img(draw_obj, box, color=color, width=3, method=method)\n            # draw_a_rectangel_in_img(draw_obj, box, color=STANDARD_COLORS[1], width=3, method=method)\n            if a_label == ONLY_DRAW_BOXES:  # -1\n                continue\n            elif a_label == ONLY_DRAW_BOXES_WITH_SCORES:  # -2\n                 only_draw_scores(draw_obj, box, a_score, color=\'White\')\n            else:\n                if is_csl:\n                    draw_label_with_scores_csl(draw_obj, box, a_label, a_score, color=\'White\')\n                else:\n                    draw_label_with_scores(draw_obj, box, a_label, a_score, color=\'White\')\n\n    out_img_obj = Image.blend(raw_img_obj, img_obj, alpha=0.7)\n\n    return np.array(out_img_obj)\n\n\nif __name__ == \'__main__\':\n    img_array = cv2.imread(""/home/yjr/PycharmProjects/FPN_TF/tools/inference_image/2.jpg"")\n    img_array = np.array(img_array, np.float32) - np.array(cfgs.PIXEL_MEAN)\n    boxes = np.array(\n        [[200, 200, 500, 500],\n         [300, 300, 400, 400],\n         [200, 200, 400, 400]]\n    )\n\n    # test only draw boxes\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES\n    scores = np.zeros_like(labes)\n    imm = draw_boxes_with_label_and_scores(img_array, boxes, labes ,scores)\n    # imm = np.array(imm)\n\n    cv2.imshow(""te"", imm)\n\n    # test only draw scores\n    labes = np.ones(shape=[len(boxes), ], dtype=np.float32) * ONLY_DRAW_BOXES_WITH_SCORES\n    scores = np.random.rand((len(boxes))) * 10\n    imm2 = draw_boxes_with_label_and_scores(img_array, boxes, labes, scores)\n\n    cv2.imshow(""te2"", imm2)\n    # test draw label and scores\n\n    labels = np.arange(1, 4)\n    imm3 = draw_boxes_with_label_and_scores(img_array, boxes, labels, scores)\n    cv2.imshow(""te3"", imm3)\n\n    cv2.waitKey(0)\n\n\n\n\n\n\n\n'"
libs/box_utils/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef gereate_centering_anchor(\n        base_size=16, ratios=[0.5, 1, 2],\n        scales=2 ** np.arange(3, 6)):\n\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n    base_anchor = np.array([1, 1, base_size, base_size]) - (base_size // 2)\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors.astype(np.float32)\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in range(ratio_anchors.shape[0])])\n    return anchors.astype(np.float32)\n\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales=(8, 16, 32),\n                         anchor_ratios=(0.5, 1, 2), base_size=16):\n    """""" A wrapper function to generate anchors given different scales\n      Also return the number of anchors in variable \'length\'\n    """"""\n    anchors = generate_anchors(\n        base_size=base_size, ratios=np.array(anchor_ratios),\n        scales=np.array(anchor_scales))\n    A = anchors.shape[0]\n    shift_x = np.arange(0, width) * feat_stride\n    shift_y = np.arange(0, height) * feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n                        shift_y.ravel())).transpose()\n    K = shifts.shape[0]\n    # width changes faster, so here it is H, W, C\n    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose(\n        (1, 0, 2))\n    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n\n    return anchors\n\n\nif __name__ == \'__main__\':\n    anchors = generate_anchors_pre(64, 64, 8, anchor_scales=np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]) * 8,\n                                   anchor_ratios=(0.5, 1.0, 2.0), base_size=4)\n    print(anchors[:10])\n\n    x_c = (anchors[:, 2] - anchors[:, 0]) / 2\n    y_c = (anchors[:, 3] - anchors[:, 1]) / 2\n    h = anchors[:, 2] - anchors[:, 0] + 1\n    w = anchors[:, 3] - anchors[:, 1] + 1\n    theta = -90 * np.ones_like(x_c)\n    anchors = np.stack([x_c, y_c]).transpose()\n    print(anchors.shape)\n'"
libs/box_utils/generate_rotate_anchors.py,27,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport time\nfrom libs.box_utils.show_box_in_tensor import only_draw_boxes\n\n\ndef make_anchors(base_anchor_size, anchor_scales, anchor_ratios, anchor_angles,\n                 featuremap_height, featuremap_width, stride, name=\'make_ratate_anchors\'):\n\n\n    \'\'\'\n    :param base_anchor_size:\n    :param anchor_scales:\n    :param anchor_ratios:\n    :param anchor_thetas:\n    :param featuremap_height:\n    :param featuremap_width:\n    :param stride:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)  # [y_center, x_center, h, w]\n        ws, hs, angles = enum_ratios_and_thetas(enum_scales(base_anchor, anchor_scales),\n                                                anchor_ratios, anchor_angles)  # per locations ws and hs and thetas\n\n        x_centers = tf.range(featuremap_width, dtype=tf.float32) * stride + stride // 2\n        y_centers = tf.range(featuremap_height, dtype=tf.float32) * stride + stride // 2\n\n        x_centers, y_centers = tf.meshgrid(x_centers, y_centers)\n\n        angles, _ = tf.meshgrid(angles, x_centers)\n        ws, x_centers = tf.meshgrid(ws, x_centers)\n        hs, y_centers = tf.meshgrid(hs, y_centers)\n\n        anchor_centers = tf.stack([x_centers, y_centers], 2)\n        anchor_centers = tf.reshape(anchor_centers, [-1, 2])\n\n        box_parameters = tf.stack([ws, hs, angles], axis=2)\n        box_parameters = tf.reshape(box_parameters, [-1, 3])\n        anchors = tf.concat([anchor_centers, box_parameters], axis=1)\n\n        return anchors\n\n\ndef enum_scales(base_anchor, anchor_scales):\n    anchor_scales = base_anchor * tf.constant(anchor_scales, dtype=tf.float32, shape=(len(anchor_scales), 1))\n\n    return anchor_scales\n\n\ndef enum_ratios_and_thetas(anchors, anchor_ratios, anchor_angles):\n    \'\'\'\n    ratio = h /w\n    :param anchors:\n    :param anchor_ratios:\n    :return:\n    \'\'\'\n    ws = anchors[:, 2]  # for base anchor: w == h\n    hs = anchors[:, 3]\n    anchor_angles = tf.constant(anchor_angles, tf.float32)\n    sqrt_ratios = tf.sqrt(tf.constant(anchor_ratios))\n\n    ws = tf.reshape(ws / sqrt_ratios[:, tf.newaxis], [-1])\n    hs = tf.reshape(hs * sqrt_ratios[:, tf.newaxis], [-1])\n\n    ws, _ = tf.meshgrid(ws, anchor_angles)\n    hs, anchor_angles = tf.meshgrid(hs, anchor_angles)\n\n    anchor_angles = tf.reshape(anchor_angles, [-1, 1])\n    ws = tf.reshape(ws, [-1, 1])\n    hs = tf.reshape(hs, [-1, 1])\n\n    return ws, hs, anchor_angles\n\n\nif __name__ == \'__main__\':\n    import os\n    from libs.configs import cfgs\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    base_anchor_size = 256\n    anchor_scales = [1.]\n    anchor_ratios = [0.5, 2.0, 1/3, 3, 1/5, 5, 1/8, 8]\n    anchor_angles = [-90, -75, -60, -45, -30, -15]\n    base_anchor = tf.constant([0, 0, base_anchor_size, base_anchor_size], tf.float32)\n    tmp1 = enum_ratios_and_thetas(enum_scales(base_anchor, anchor_scales), anchor_ratios, anchor_angles)\n    anchors = make_anchors(32,\n                           [2.], [2.0, 1/2], anchor_angles,\n                           featuremap_height=800 // 8,\n                           featuremap_width=800 // 8,\n                           stride=8)\n\n    # anchors = make_anchors(base_anchor_size=cfgs.BASE_ANCHOR_SIZE_LIST[0],\n    #                        anchor_scales=cfgs.ANCHOR_SCALES,\n    #                        anchor_ratios=cfgs.ANCHOR_RATIOS,\n    #                        anchor_angles=cfgs.ANCHOR_ANGLES,\n    #                        featuremap_height=800 // 16,\n    #                        featuremap_width=800 // 16,\n    #                        stride=cfgs.ANCHOR_STRIDE[0],\n    #                        name=""make_anchors_forRPN"")\n\n    img = tf.zeros([800, 800, 3])\n    img = tf.expand_dims(img, axis=0)\n\n    img1 = only_draw_boxes(img, anchors[9100:9110], \'r\')\n\n    with tf.Session() as sess:\n        temp1, _img1 = sess.run([anchors, img1])\n\n        _img1 = _img1[0]\n\n        cv2.imwrite(\'rotate_anchors.jpg\', _img1)\n        cv2.waitKey(0)\n\n        print(temp1)\n        print(\'debug\')\n'"
libs/box_utils/iou.py,9,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef iou_calculate(boxes_1, boxes_2):\n\n    with tf.name_scope(\'iou_caculate\'):\n\n        xmin_1, ymin_1, xmax_1, ymax_1 = tf.unstack(boxes_1, axis=1)  # ymin_1 shape is [N, 1]..\n\n        xmin_2, ymin_2, xmax_2, ymax_2 = tf.unstack(boxes_2, axis=1)  # ymin_2 shape is [M, ]..\n\n        max_xmin = tf.maximum(xmin_1, xmin_2)\n        min_xmax = tf.minimum(xmax_1, xmax_2)\n\n        max_ymin = tf.maximum(ymin_1, ymin_2)\n        min_ymax = tf.minimum(ymax_1, ymax_2)\n\n        overlap_h = tf.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n        overlap_w = tf.maximum(0., min_xmax - max_xmin)\n\n        overlaps = overlap_h * overlap_w\n\n        area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n        area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n        iou = overlaps / (area_1 + area_2 - overlaps)\n\n        return iou\n\n\ndef iou_calculate_np(boxes_1, boxes_2):\n    xmin_1, ymin_1, xmax_1, ymax_1 = np.split(boxes_1, 4, axis=1)\n    # xmin_1, ymin_1, xmax_1, ymax_1 = boxes_1[:, 0], boxes_1[:, 1], boxes_1[:, 2], boxes_1[:, 3]\n\n    xmin_2, ymin_2, xmax_2, ymax_2 = boxes_2[:, 0], boxes_2[:, 1], boxes_2[:, 2], boxes_2[:, 3]\n\n    max_xmin = np.maximum(xmin_1, xmin_2)\n    min_xmax = np.minimum(xmax_1, xmax_2)\n\n    max_ymin = np.maximum(ymin_1, ymin_2)\n    min_ymax = np.minimum(ymax_1, ymax_2)\n\n    overlap_h = np.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = np.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    iou = overlaps / (area_1 + area_2 - overlaps)\n\n    return iou\n\n\ndef iou_calculate1(boxes_1, boxes_2):\n\n    xmin_1, ymin_1, xmax_1, ymax_1 = boxes_1[:, 0], boxes_1[:, 1], boxes_1[:, 2], boxes_1[:, 3]\n\n    xmin_2, ymin_2, xmax_2, ymax_2 = boxes_2[:, 0], boxes_2[:, 1], boxes_2[:, 2], boxes_2[:, 3]\n\n    max_xmin = np.maximum(xmin_1, xmin_2)\n    min_xmax = np.minimum(xmax_1, xmax_2)\n\n    max_ymin = np.maximum(ymin_1, ymin_2)\n    min_ymax = np.minimum(ymax_1, ymax_2)\n\n    overlap_h = np.maximum(0., min_ymax - max_ymin)  # avoid h < 0\n    overlap_w = np.maximum(0., min_xmax - max_xmin)\n\n    overlaps = overlap_h * overlap_w\n\n    area_1 = (xmax_1 - xmin_1) * (ymax_1 - ymin_1)  # [N, 1]\n    area_2 = (xmax_2 - xmin_2) * (ymax_2 - ymin_2)  # [M, ]\n\n    iou = overlaps / (area_1 + area_2 - overlaps)\n\n    return iou\n\n\nif __name__ == \'__main__\':\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'13\'\n    boxes1 = np.array([[50, 50, 100, 300],\n                       [60, 60, 100, 200]], np.float32)\n\n    boxes2 = np.array([[50, 50, 100, 300],\n                       [200, 200, 100, 200]], np.float32)\n\n    print(iou_calculate_np(boxes1, boxes2))\n\n\n\n'"
libs/box_utils/iou_rotate.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport tensorflow as tf\nimport math\nfrom libs.box_utils.coordinate_convert import *\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils.iou_cpu import get_iou_matrix\n\n\ndef iou_rotate_calculate(boxes1, boxes2, use_gpu=True, gpu_id=0):\n    \'\'\'\n\n    :param boxes_list1:[N, 8] tensor\n    :param boxes_list2: [M, 8] tensor\n    :return:\n    \'\'\'\n\n    boxes1 = tf.cast(boxes1, tf.float32)\n    boxes2 = tf.cast(boxes2, tf.float32)\n    if use_gpu:\n\n        iou_matrix = tf.py_func(rbbx_overlaps,\n                                inp=[boxes1, boxes2, gpu_id],\n                                Tout=tf.float32)\n    else:\n        iou_matrix = tf.py_func(get_iou_matrix, inp=[boxes1, boxes2],\n                                Tout=tf.float32)\n\n    iou_matrix = tf.reshape(iou_matrix, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n\n    return iou_matrix\n\n\ndef iou_rotate_calculate1(boxes1, boxes2, use_gpu=True, gpu_id=0):\n\n    # start = time.time()\n    if use_gpu:\n        ious = rbbx_overlaps(boxes1, boxes2, gpu_id)\n    else:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n        ious = []\n        for i, box1 in enumerate(boxes1):\n            temp_ious = []\n            r1 = ((box1[0], box1[1]), (box1[2], box1[3]), box1[4])\n            for j, box2 in enumerate(boxes2):\n                r2 = ((box2[0], box2[1]), (box2[2], box2[3]), box2[4])\n\n                int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n                if int_pts is not None:\n                    order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                    int_area = cv2.contourArea(order_pts)\n\n                    inter = int_area * 1.0 / (area1[i] + area2[j] - int_area)\n                    temp_ious.append(inter)\n                else:\n                    temp_ious.append(0.0)\n            ious.append(temp_ious)\n\n    # print(\'{}s\'.format(time.time() - start))\n\n    return np.array(ious, dtype=np.float32)\n\n\ndef iou_rotate_calculate2(boxes1, boxes2):\n    ious = []\n    if boxes1.shape[0] != 0:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n\n        for i in range(boxes1.shape[0]):\n            temp_ious = []\n            r1 = ((boxes1[i][0], boxes1[i][1]), (boxes1[i][2], boxes1[i][3]), boxes1[i][4])\n            r2 = ((boxes2[i][0], boxes2[i][1]), (boxes2[i][2], boxes2[i][3]), boxes2[i][4])\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                inter = int_area * 1.0 / (area1[i] + area2[i] - int_area)\n                temp_ious.append(inter)\n            else:\n                temp_ious.append(0.0)\n            ious.append(temp_ious)\n\n    return np.array(ious, dtype=np.float32)\n\n\ndef diou_rotate_calculate(boxes1, boxes2):\n\n    if boxes1.shape[0] != 0:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n        d = (boxes1[:, 0] - boxes2[:, 0]) ** 2 + (boxes1[:, 1] - boxes2[:, 1])\n\n        boxes1_ = forward_convert(boxes1, with_label=False)\n        boxes2_ = forward_convert(boxes2, with_label=False)\n\n        xmin = np.minimum(np.min(boxes1_[:, 0::2]), np.min(boxes2_[:, 0::2]))\n        xmax = np.maximum(np.max(boxes1_[:, 0::2]), np.max(boxes2_[:, 0::2]))\n        ymin = np.minimum(np.min(boxes1_[:, 1::2]), np.min(boxes2_[:, 1::2]))\n        ymax = np.maximum(np.max(boxes1_[:, 1::2]), np.max(boxes2_[:, 1::2]))\n\n        c = (xmax - xmin) ** 2 + (ymax - ymin) ** 2\n        ious = []\n        for i in range(boxes1.shape[0]):\n            r1 = ((boxes1[i][0], boxes1[i][1]), (boxes1[i][2], boxes1[i][3]), boxes1[i][4])\n            r2 = ((boxes2[i][0], boxes2[i][1]), (boxes2[i][2], boxes2[i][3]), boxes2[i][4])\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                iou = int_area * 1.0 / (area1[i] + area2[i] - int_area)\n            else:\n                iou = 0.0\n\n            ious.append(iou)\n        ious = np.array(ious)\n\n        dious = ious - d / c\n    else:\n        dious = []\n\n    return np.reshape(np.array(dious, dtype=np.float32), [-1, 1])\n\n\ndef adiou_rotate_calculate(boxes1, boxes2):\n\n    if boxes1.shape[0] != 0:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n        d = (boxes1[:, 0] - boxes2[:, 0]) ** 2 + (boxes1[:, 1] - boxes2[:, 1])\n\n        boxes1_ = forward_convert(boxes1, with_label=False)\n        boxes2_ = forward_convert(boxes2, with_label=False)\n\n        xmin = np.minimum(np.min(boxes1_[:, 0::2]), np.min(boxes2_[:, 0::2]))\n        xmax = np.maximum(np.max(boxes1_[:, 0::2]), np.max(boxes2_[:, 0::2]))\n        ymin = np.minimum(np.min(boxes1_[:, 1::2]), np.min(boxes2_[:, 1::2]))\n        ymax = np.maximum(np.max(boxes1_[:, 1::2]), np.max(boxes2_[:, 1::2]))\n\n        c = (xmax - xmin) ** 2 + (ymax - ymin) ** 2\n\n        # v = (4 / (math.pi ** 2)) * (np.arctan(boxes1[:, 2]/boxes1[:, 3]) - np.arctan(boxes2[:, 2]/boxes2[:, 3])) ** 2\n\n        ious = []\n        for i in range(boxes1.shape[0]):\n            r1 = ((boxes1[i][0], boxes1[i][1]), (boxes1[i][2], boxes1[i][3]), boxes1[i][4])\n            r2 = ((boxes2[i][0], boxes2[i][1]), (boxes2[i][2], boxes2[i][3]), boxes2[i][4])\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                iou = int_area * 1.0 / (area1[i] + area2[i] - int_area)\n            else:\n                iou = 0.0\n\n            ious.append(iou)\n        ious = np.array(ious)\n\n        # S = 1 - ious\n        # alpha = v / (S + v)\n        # w_temp = 2 * boxes1[:, 2]\n        # ar = (8 / (math.pi ** 2)) * (np.arctan(boxes1[:, 2]/boxes1[:, 3]) - np.arctan(boxes2[:, 2]/boxes2[:, 3])) \\\n        #      * ((boxes1[:, 2] - w_temp) * boxes1[:, 3])\n        # cious = ious - d / c - alpha * ar\n        cious = (ious - d / c) * np.abs(np.cos(boxes1[:, 4] - boxes2[:, 4]))\n    else:\n        cious = []\n\n    return np.reshape(np.array(cious, dtype=np.float32), [-1, 1])\n\n\nif __name__ == \'__main__\':\n    import os\n    # os.environ[""CUDA_VISIBLE_DEVICES""] = \'13\'\n    boxes1 = np.array([[50, 50, 10, 70, -45],\n                       [150, 150, 10, 50, -50]], np.float32)\n\n    boxes2 = np.array([[150, 150, 10, 70, -50],\n                       [150, 150, 10, 70, -50]], np.float32)\n\n    print(iou_rotate_calculate2(boxes1, boxes2))\n    print(diou_rotate_calculate(boxes1, boxes2))\n    print(adiou_rotate_calculate(boxes1, boxes2))\n\n    # start = time.time()\n    # with tf.Session() as sess:\n    #     ious = iou_rotate_calculate1(boxes1, boxes2, use_gpu=False)\n    #     print(sess.run(ious))\n    #     print(\'{}s\'.format(time.time() - start))\n\n    # start = time.time()\n    # for _ in range(10):\n    #     ious = rbbox_overlaps.rbbx_overlaps(boxes1, boxes2)\n    # print(\'{}s\'.format(time.time() - start))\n    # print(ious)\n\n    # print(ovr)\n\n\n\n'"
libs/box_utils/mask_utils.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport numpy as np\nimport tfplot as tfp\nimport cv2\n\n\ndef make_gt_mask(fet_h, fet_w, img_h, img_w, gtboxes):\n    '''\n    :param fet_h:\n    :param fet_w:\n    :param img_h:\n    :param img_w:\n    :param gtboxes: [xmin, ymin, xmax, ymax, label]. shape is (N, 5)\n    :return:\n    '''\n    gtboxes = np.reshape(gtboxes, [-1, 5])\n    # xmin, ymin, xmax, ymax, label = gtboxes[:, 0], gtboxes[:, 1], gtboxes[:, 2], gtboxes[:, 3], gtboxes[:, 4]\n\n    areas = (gtboxes[:, 2]-gtboxes[:, 0])*(gtboxes[:, 3]-gtboxes[:, 1])\n    arg_areas = np.argsort(-1*areas)  # sort from large to small\n    gtboxes = gtboxes[arg_areas]\n\n    fet_h, fet_w = int(fet_h), int(fet_w)\n    mask = np.zeros(shape=[fet_h, fet_w], dtype=np.int32)\n    for a_box in gtboxes:\n        xmin, ymin, xmax, ymax, label = a_box[0], a_box[1], a_box[2], a_box[3], a_box[4]\n\n        new_xmin, new_ymin, new_xmax, new_ymax = int(xmin*fet_w/float(img_w)), int(ymin*fet_h/float(img_h)),\\\n                                                 int(xmax*fet_w/float(img_w)), int(ymax*fet_h/float(img_h))\n\n        new_xmin, new_ymin = max(0, new_xmin), max(0, new_ymin)\n        new_xmax, new_ymax = min(fet_w, new_xmax), min(fet_h, new_ymax)\n\n        mask[new_ymin:new_ymax, new_xmin:new_xmax] = np.int32(label)\n    return mask\n\n\ndef make_r_gt_mask(fet_h, fet_w, img_h, img_w, gtboxes):\n    gtboxes = np.reshape(gtboxes, [-1, 6])  # [x, y, w, h, theta, label]\n\n    areas = gtboxes[:, 2] * gtboxes[:, 3]\n    arg_areas = np.argsort(-1 * areas)  # sort from large to small\n    gtboxes = gtboxes[arg_areas]\n\n    fet_h, fet_w = int(fet_h), int(fet_w)\n    mask = np.zeros(shape=[fet_h, fet_w], dtype=np.int32)\n    for a_box in gtboxes:\n        # print(a_box)\n        box = cv2.boxPoints(((a_box[0], a_box[1]), (a_box[2], a_box[3]), a_box[4]))\n        box = np.reshape(box, [-1, ])\n        label = a_box[-1]\n        new_box = []\n        for i in range(8):\n            if i % 2 == 0:\n                x = box[i]\n                new_x = int(x * fet_w / float(img_w))\n                new_box.append(new_x)\n            else:\n                y = box[i]\n                new_y = int(y*fet_h/float(img_h))\n                new_box.append(new_y)\n\n        new_box = np.int0(new_box).reshape([4, 2])\n        color = int(label)\n        # print(type(color), color)\n        cv2.fillConvexPoly(mask, new_box, color=color)\n    # print (mask.dtype)\n    return mask\n\n\ndef vis_mask_tfsmry(mask, name):\n    '''\n    :param mask:[H, W]. It's a tensor, not array\n    :return:\n    '''\n\n    def figure_attention(activation):\n        fig, ax = tfp.subplots()\n        im = ax.imshow(activation, cmap='jet')\n        fig.colorbar(im)\n        return fig\n\n    heatmap = mask*10\n\n    tfp.summary.plot(name, figure_attention, [heatmap])"""
libs/box_utils/nms_rotate.py,18,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nfrom libs.configs import cfgs\nimport tensorflow as tf\nfrom libs.box_utils.rotate_polygon_nms import rotate_gpu_nms\n\n\ndef nms_rotate(decode_boxes, scores, iou_threshold, max_output_size,\n               use_angle_condition=False, angle_threshold=0, use_gpu=True, gpu_id=0):\n    """"""\n    :param boxes: format [x_c, y_c, w, h, theta]\n    :param scores: scores of boxes\n    :param threshold: iou threshold (0.7 or 0.5)\n    :param max_output_size: max number of output\n    :return: the remaining index of boxes\n    """"""\n\n    if use_gpu:\n        keep = nms_rotate_gpu(boxes_list=decode_boxes,\n                              scores=scores,\n                              iou_threshold=iou_threshold,\n                              angle_gap_threshold=angle_threshold,\n                              use_angle_condition=use_angle_condition,\n                              device_id=gpu_id)\n\n        keep = tf.cond(\n            tf.greater(tf.shape(keep)[0], max_output_size),\n            true_fn=lambda: tf.slice(keep, [0], [max_output_size]),\n            false_fn=lambda: keep)\n\n    else:\n        keep = tf.py_func(nms_rotate_cpu,\n                          inp=[decode_boxes, scores, iou_threshold, max_output_size],\n                          Tout=tf.int64)\n    return keep\n\n\ndef nms_rotate_cpu(boxes, scores, iou_threshold, max_output_size):\n\n    keep = []\n\n    order = scores.argsort()[::-1]\n    num = boxes.shape[0]\n\n    suppressed = np.zeros((num), dtype=np.int)\n\n    for _i in range(num):\n        if len(keep) >= max_output_size:\n            break\n\n        i = order[_i]\n        if suppressed[i] == 1:\n            continue\n        keep.append(i)\n        r1 = ((boxes[i, 0], boxes[i, 1]), (boxes[i, 2], boxes[i, 3]), boxes[i, 4])\n        area_r1 = boxes[i, 2] * boxes[i, 3]\n        for _j in range(_i + 1, num):\n            j = order[_j]\n            if suppressed[i] == 1:\n                continue\n            r2 = ((boxes[j, 0], boxes[j, 1]), (boxes[j, 2], boxes[j, 3]), boxes[j, 4])\n            area_r2 = boxes[j, 2] * boxes[j, 3]\n            inter = 0.0\n\n            try:\n                int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n\n                if int_pts is not None:\n                    order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                    int_area = cv2.contourArea(order_pts)\n\n                    inter = int_area * 1.0 / (area_r1 + area_r2 - int_area + cfgs.EPSILON)\n\n            except:\n                """"""\n                  cv2.error: /io/opencv/modules/imgproc/src/intersection.cpp:247:\n                  error: (-215) intersection.size() <= 8 in function rotatedRectangleIntersection\n                """"""\n                # print(r1)\n                # print(r2)\n                inter = 0.9999\n\n            if inter >= iou_threshold:\n                suppressed[j] = 1\n\n    return np.array(keep, np.int64)\n\n\ndef nms_rotate_gpu(boxes_list, scores, iou_threshold, use_angle_condition=False, angle_gap_threshold=0, device_id=0):\n    if use_angle_condition:\n        x_c, y_c, w, h, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        return keep\n    else:\n        x_c, y_c, w, h, theta = tf.unstack(boxes_list, axis=1)\n        boxes_list = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n        det_tensor = tf.concat([boxes_list, tf.expand_dims(scores, axis=1)], axis=1)\n        keep = tf.py_func(rotate_gpu_nms,\n                          inp=[det_tensor, iou_threshold, device_id],\n                          Tout=tf.int64)\n        keep = tf.reshape(keep, [-1])\n        return keep\n\n\nif __name__ == \'__main__\':\n    boxes = np.array([[50, 50, 100, 100, 0],\n                      [60, 60, 100, 100, 0],\n                      [50, 50, 100, 100, -45.],\n                      [200, 200, 100, 100, 0.]])\n\n    scores = np.array([0.99, 0.88, 0.66, 0.77])\n\n    keep = nms_rotate(tf.convert_to_tensor(boxes, dtype=tf.float32), tf.convert_to_tensor(scores, dtype=tf.float32),\n                      0.7, 5)\n\n    import os\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'0\'\n    with tf.Session() as sess:\n        print(sess.run(keep))\n'"
libs/box_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport subprocess\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\'rbbox_overlaps\',\n              [\'rbbox_overlaps_kernel.cu\', \'rbbox_overlaps.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\'rotate_polygon_nms\',\n        [\'rotate_polygon_nms_kernel.cu\', \'rotate_polygon_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs=[numpy_include, CUDA[\'include\']]\n    ),\n    Extension(\'iou_cpu\',\n              [\'iou_cpu.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with\n              # gcc the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']])\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/box_utils/show_box_in_tensor.py,30,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.box_utils import draw_box_in_img\n\n\ndef only_draw_boxes(img_batch, boxes, method, is_csl=False):\n\n    boxes = tf.stop_gradient(boxes)\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0], ), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES\n    scores = tf.zeros_like(labels, dtype=tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores, method, is_csl],\n                                       Tout=tf.uint8)\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))  # [batch_size, h, w, c]\n\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_scores(img_batch, boxes, scores, method, is_csl=False):\n\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    labels = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.int32) * draw_box_in_img.ONLY_DRAW_BOXES_WITH_SCORES\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores, method, is_csl],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories(img_batch, boxes, labels, method, is_csl=False):\n    boxes = tf.stop_gradient(boxes)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    scores = tf.ones(shape=(tf.shape(boxes)[0],), dtype=tf.float32)\n\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores, method, is_csl],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\ndef draw_boxes_with_categories_and_scores(img_batch, boxes, labels, scores, method, is_csl=False):\n    boxes = tf.stop_gradient(boxes)\n    scores = tf.stop_gradient(scores)\n\n    img_tensor = tf.squeeze(img_batch, 0)\n    img_tensor = tf.cast(img_tensor, tf.float32)\n    img_tensor_with_boxes = tf.py_func(draw_box_in_img.draw_boxes_with_label_and_scores,\n                                       inp=[img_tensor, boxes, labels, scores, method, is_csl],\n                                       Tout=[tf.uint8])\n    img_tensor_with_boxes = tf.reshape(img_tensor_with_boxes, tf.shape(img_batch))\n    return img_tensor_with_boxes\n\n\nif __name__ == ""__main__"":\n    print (1)\n\n'"
libs/box_utils/tf_ops.py,7,"b'# -*- coding:utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nimport tensorflow as tf\n\n\'\'\'\nall of these ops are derived from tenosrflow Object Detection API\n\'\'\'\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific (the para ""indices_value"" ) and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])'"
libs/configs/__init__.py,0,b''
libs/configs/cfgs.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv7 + iou-smooth l1 loss\n\nThis is your result for task 1:\n\n    mAP: 0.6949672314782958\n    ap of each class:\n    plane:0.885386792983877,\n    baseball-diamond:0.7331968839644771,\n    bridge:0.4898628104022108,\n    ground-track-field:0.6400479115497535,\n    small-vehicle:0.6804863040286401,\n    large-vehicle:0.7272874790497282,\n    ship:0.7602937054410586,\n    tennis-court:0.9083344898088754,\n    basketball-court:0.784327582019443,\n    storage-tank:0.8013993994143976,\n    soccer-ball-field:0.5483354704539025,\n    roundabout:0.6171343622691822,\n    harbor:0.5801559667600483,\n    swimming-pool:0.6858516206184136,\n    helicopter:0.582407693410429\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200501_91.8w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200501\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\nALPHA = 1.0\nBETA = 1.0\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/detection_oprations/__init__.py,0,b''
libs/detection_oprations/anchor_target_layer_without_boxweight.py,0,"b""# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom libs.configs import cfgs\nimport numpy as np\nfrom libs.box_utils.cython_utils.cython_bbox import bbox_overlaps\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils.iou_cpu import get_iou_matrix\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert\n\n\ndef anchor_target_layer(gt_boxes_h, gt_boxes_r, anchors, gpu_id=0):\n\n    anchor_states = np.zeros((anchors.shape[0],))\n    labels = np.zeros((anchors.shape[0], cfgs.CLASS_NUM))\n    if gt_boxes_r.shape[0]:\n        # [N, M]\n\n        if cfgs.METHOD == 'H':\n            overlaps = bbox_overlaps(np.ascontiguousarray(anchors, dtype=np.float),\n                                     np.ascontiguousarray(gt_boxes_h, dtype=np.float))\n        else:\n            overlaps = rbbx_overlaps(np.ascontiguousarray(anchors, dtype=np.float32),\n                                     np.ascontiguousarray(gt_boxes_r[:, :-1], dtype=np.float32), gpu_id)\n\n            # overlaps = get_iou_matrix(np.ascontiguousarray(anchors, dtype=np.float32),\n            #                           np.ascontiguousarray(gt_boxes_r[:, :-1], dtype=np.float32))\n\n        argmax_overlaps_inds = np.argmax(overlaps, axis=1)\n        max_overlaps = overlaps[np.arange(overlaps.shape[0]), argmax_overlaps_inds]\n\n        # compute box regression targets\n        target_boxes = gt_boxes_r[argmax_overlaps_inds]\n\n        if cfgs.USE_ANGLE_COND:\n            if cfgs.METHOD == 'R':\n                delta_theta = np.abs(target_boxes[:, -2] - anchors[:, -1])\n                theta_indices = delta_theta < 15\n                positive_indices = (max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD) & theta_indices\n            else:\n                positive_indices = max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD\n\n            ignore_indices = (max_overlaps > cfgs.IOU_NEGATIVE_THRESHOLD) & (max_overlaps < cfgs.IOU_POSITIVE_THRESHOLD)\n\n        else:\n            positive_indices = max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD\n            ignore_indices = (max_overlaps > cfgs.IOU_NEGATIVE_THRESHOLD) & ~positive_indices\n\n        anchor_states[ignore_indices] = -1\n        anchor_states[positive_indices] = 1\n\n        # compute target class labels\n        labels[positive_indices, target_boxes[positive_indices, -1].astype(int) - 1] = 1\n    else:\n        # no annotations? then everything is background\n        target_boxes = np.zeros((anchors.shape[0], gt_boxes_r.shape[1]))\n\n    if cfgs.METHOD == 'H':\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * np.ones_like(x_c)\n        anchors = np.vstack([x_c, y_c, w, h, theta]).transpose()\n\n    if cfgs.ANGLE_RANGE == 180:\n        anchors = coordinate_present_convert(anchors, mode=-1)\n        target_boxes = coordinate_present_convert(target_boxes, mode=-1)\n    target_delta = bbox_transform.rbbox_transform(ex_rois=anchors, gt_rois=target_boxes)\n\n    return np.array(labels, np.float32), np.array(target_delta, np.float32), \\\n           np.array(anchor_states, np.float32), np.array(target_boxes, np.float32)\n\n\n\n\n"""
libs/detection_oprations/anchor_target_layer_without_boxweight_win.py,0,"b""# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom libs.configs import cfgs\nimport numpy as np\nfrom libs.box_utils.iou import iou_calculate_np\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert\n\n\ndef anchor_target_layer(gt_boxes_h, gt_boxes_r, anchors, gpu_id=0):\n\n    anchor_states = np.zeros((anchors.shape[0],))\n    labels = np.zeros((anchors.shape[0], cfgs.CLASS_NUM))\n    if gt_boxes_r.shape[0]:\n        # [N, M]\n\n        if cfgs.METHOD == 'H':\n            overlaps = iou_calculate_np(np.ascontiguousarray(anchors, dtype=np.float),\n                                        np.ascontiguousarray(gt_boxes_h, dtype=np.float))\n        else:\n            raise Exception('Do not support mode=R in windows version')\n        argmax_overlaps_inds = np.argmax(overlaps, axis=1)\n        max_overlaps = overlaps[np.arange(overlaps.shape[0]), argmax_overlaps_inds]\n\n        # compute box regression targets\n        target_boxes = gt_boxes_r[argmax_overlaps_inds]\n\n        if cfgs.USE_ANGLE_COND:\n            if cfgs.METHOD == 'R':\n                delta_theta = np.abs(target_boxes[:, -2] - anchors[:, -1])\n                theta_indices = delta_theta < 15\n                positive_indices = (max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD) & theta_indices\n            else:\n                positive_indices = max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD\n\n            ignore_indices = (max_overlaps > cfgs.IOU_NEGATIVE_THRESHOLD) & (max_overlaps < cfgs.IOU_POSITIVE_THRESHOLD)\n\n        else:\n            positive_indices = max_overlaps >= cfgs.IOU_POSITIVE_THRESHOLD\n            ignore_indices = (max_overlaps > cfgs.IOU_NEGATIVE_THRESHOLD) & ~positive_indices\n\n        anchor_states[ignore_indices] = -1\n        anchor_states[positive_indices] = 1\n\n        # compute target class labels\n        labels[positive_indices, target_boxes[positive_indices, -1].astype(int) - 1] = 1\n    else:\n        # no annotations? then everything is background\n        target_boxes = np.zeros((anchors.shape[0], gt_boxes_r.shape[1]))\n\n    if cfgs.METHOD == 'H':\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * np.ones_like(x_c)\n        anchors = np.vstack([x_c, y_c, w, h, theta]).transpose()\n\n    if cfgs.ANGLE_RANGE == 180:\n        anchors = coordinate_present_convert(anchors, mode=-1)\n        target_boxes = coordinate_present_convert(target_boxes, mode=-1)\n    target_delta = bbox_transform.rbbox_transform(ex_rois=anchors, gt_rois=target_boxes)\n\n    return np.array(labels, np.float32), np.array(target_delta, np.float32), \\\n           np.array(anchor_states, np.float32), np.array(target_boxes, np.float32)\n\n\n\n\n"""
libs/detection_oprations/proposal_opr.py,26,"b'# encoding: utf-8\nfrom libs.configs import cfgs\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils import nms_rotate\nimport tensorflow as tf\n\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert, coords_regular\n\n\ndef filter_detections(boxes, scores, is_training):\n    """"""\n    :param boxes: [-1, 4]\n    :param scores: [-1, ]\n    :param labels: [-1, ]\n    :return:\n    """"""\n    if is_training:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.VIS_SCORE)), [-1, ])\n    else:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.FILTERED_SCORE)), [-1, ])\n\n    if cfgs.NMS:\n        filtered_boxes = tf.gather(boxes, indices)\n        filtered_scores = tf.gather(scores, indices)\n\n        # perform NMS\n\n        nms_indices = nms_rotate.nms_rotate(decode_boxes=filtered_boxes,\n                                            scores=filtered_scores,\n                                            iou_threshold=cfgs.NMS_IOU_THRESHOLD,\n                                            max_output_size=100,\n                                            use_angle_condition=False,\n                                            angle_threshold=15,\n                                            use_gpu=False)\n\n        # filter indices based on NMS\n        indices = tf.gather(indices, nms_indices)\n\n    # add indices to list of all indices\n    return indices\n\n\ndef postprocess_detctions(rpn_bbox_pred, rpn_cls_prob, anchors, is_training):\n\n    if cfgs.METHOD == \'H\':\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    if cfgs.ANGLE_RANGE == 180:\n        anchors = tf.py_func(coordinate_present_convert,\n                             inp=[anchors, -1],\n                             Tout=[tf.float32])\n        anchors = tf.reshape(anchors, [-1, 5])\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=rpn_bbox_pred)\n\n    if cfgs.ANGLE_RANGE == 180:\n        # boxes_pred = tf.py_func(coords_regular,\n        #                         inp=[boxes_pred],\n        #                         Tout=[tf.float32])\n        # boxes_pred = tf.reshape(boxes_pred, [-1, 5])\n\n        _, _, _, _, theta = tf.unstack(boxes_pred, axis=1)\n        indx = tf.reshape(tf.where(tf.logical_and(tf.less(theta, 0), tf.greater_equal(theta, -180))), [-1, ])\n        boxes_pred = tf.gather(boxes_pred, indx)\n        rpn_cls_prob = tf.gather(rpn_cls_prob, indx)\n\n        boxes_pred = tf.py_func(coordinate_present_convert,\n                                inp=[boxes_pred, 1],\n                                Tout=[tf.float32])\n        boxes_pred = tf.reshape(boxes_pred, [-1, 5])\n\n    return_boxes_pred = []\n    return_scores = []\n    return_labels = []\n    for j in range(0, cfgs.CLASS_NUM):\n        indices = filter_detections(boxes_pred, rpn_cls_prob[:, j], is_training)\n        tmp_boxes_pred = tf.reshape(tf.gather(boxes_pred, indices), [-1, 5])\n        tmp_scores = tf.reshape(tf.gather(rpn_cls_prob[:, j], indices), [-1, ])\n\n        return_boxes_pred.append(tmp_boxes_pred)\n        return_scores.append(tmp_scores)\n        return_labels.append(tf.ones_like(tmp_scores)*(j+1))\n\n    return_boxes_pred = tf.concat(return_boxes_pred, axis=0)\n    return_scores = tf.concat(return_scores, axis=0)\n    return_labels = tf.concat(return_labels, axis=0)\n\n    return return_boxes_pred, return_scores, return_labels\n'"
libs/detection_oprations/proposal_opr_.py,23,"b""# encoding: utf-8\nfrom libs.configs import cfgs\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils import nms_rotate\nimport tensorflow as tf\n\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert, coords_regular\n\n\ndef postprocess_detctions(rpn_bbox_pred, rpn_cls_prob, anchors, is_training):\n\n    return_boxes_pred = []\n    return_scores = []\n    return_labels = []\n    for j in range(0, cfgs.CLASS_NUM):\n        scores = rpn_cls_prob[:, j]\n        if is_training:\n            indices = tf.reshape(tf.where(tf.greater(scores, cfgs.VIS_SCORE)), [-1, ])\n        else:\n            indices = tf.reshape(tf.where(tf.greater(scores, cfgs.FILTERED_SCORE)), [-1, ])\n\n        anchors_ = tf.gather(anchors, indices)\n        rpn_bbox_pred_ = tf.gather(rpn_bbox_pred, indices)\n        scores = tf.gather(scores, indices)\n\n        if cfgs.METHOD == 'H':\n            x_c = (anchors_[:, 2] + anchors_[:, 0]) / 2\n            y_c = (anchors_[:, 3] + anchors_[:, 1]) / 2\n            h = anchors_[:, 2] - anchors_[:, 0] + 1\n            w = anchors_[:, 3] - anchors_[:, 1] + 1\n            theta = -90 * tf.ones_like(x_c)\n            anchors_ = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n        if cfgs.ANGLE_RANGE == 180:\n            anchors_ = tf.py_func(coordinate_present_convert,\n                                 inp=[anchors_, -1],\n                                 Tout=[tf.float32])\n            anchors_ = tf.reshape(anchors_, [-1, 5])\n\n        boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors_, deltas=rpn_bbox_pred_)\n\n        if cfgs.ANGLE_RANGE == 180:\n\n            _, _, _, _, theta = tf.unstack(boxes_pred, axis=1)\n            indx = tf.reshape(tf.where(tf.logical_and(tf.less(theta, 0), tf.greater_equal(theta, -180))), [-1, ])\n            boxes_pred = tf.gather(boxes_pred, indx)\n            scores = tf.gather(scores, indx)\n\n            boxes_pred = tf.py_func(coordinate_present_convert,\n                                    inp=[boxes_pred, 1],\n                                    Tout=[tf.float32])\n            boxes_pred = tf.reshape(boxes_pred, [-1, 5])\n\n        nms_indices = nms_rotate.nms_rotate(decode_boxes=boxes_pred,\n                                            scores=scores,\n                                            iou_threshold=cfgs.NMS_IOU_THRESHOLD,\n                                            max_output_size=100,\n                                            use_angle_condition=False,\n                                            angle_threshold=15,\n                                            use_gpu=False)\n\n        tmp_boxes_pred = tf.reshape(tf.gather(boxes_pred, nms_indices), [-1, 5])\n        tmp_scores = tf.reshape(tf.gather(scores, nms_indices), [-1, ])\n\n        return_boxes_pred.append(tmp_boxes_pred)\n        return_scores.append(tmp_scores)\n        return_labels.append(tf.ones_like(tmp_scores)*(j+1))\n\n    return_boxes_pred = tf.concat(return_boxes_pred, axis=0)\n    return_scores = tf.concat(return_scores, axis=0)\n    return_labels = tf.concat(return_labels, axis=0)\n\n    return return_boxes_pred, return_scores, return_labels\n"""
libs/detection_oprations/proposal_opr_win.py,25,"b'# encoding: utf-8\nfrom libs.configs import cfgs\nfrom libs.box_utils import bbox_transform\nimport tensorflow as tf\nimport numpy as np\n\nfrom libs.box_utils.coordinate_convert import coordinate_present_convert, coords_regular\n\n\ndef nms_rotate_cpu(boxes, scores, iou_threshold, max_output_size):\n\n    keep = []\n\n    order = scores.argsort()[::-1]\n    num = boxes.shape[0]\n\n    suppressed = np.zeros((num), dtype=np.int)\n\n    for _i in range(num):\n        if len(keep) >= max_output_size:\n            break\n\n        i = order[_i]\n        if suppressed[i] == 1:\n            continue\n        keep.append(i)\n        r1 = ((boxes[i, 0], boxes[i, 1]), (boxes[i, 2], boxes[i, 3]), boxes[i, 4])\n        area_r1 = boxes[i, 2] * boxes[i, 3]\n        for _j in range(_i + 1, num):\n            j = order[_j]\n            if suppressed[i] == 1:\n                continue\n            r2 = ((boxes[j, 0], boxes[j, 1]), (boxes[j, 2], boxes[j, 3]), boxes[j, 4])\n            area_r2 = boxes[j, 2] * boxes[j, 3]\n            inter = 0.0\n\n            try:\n                int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n\n                if int_pts is not None:\n                    order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                    int_area = cv2.contourArea(order_pts)\n\n                    inter = int_area * 1.0 / (area_r1 + area_r2 - int_area + cfgs.EPSILON)\n\n            except:\n                """"""\n                  cv2.error: /io/opencv/modules/imgproc/src/intersection.cpp:247:\n                  error: (-215) intersection.size() <= 8 in function rotatedRectangleIntersection\n                """"""\n                # print(r1)\n                # print(r2)\n                inter = 0.9999\n\n            if inter >= iou_threshold:\n                suppressed[j] = 1\n\n    return np.array(keep, np.int64)\n\n\ndef nms_rotate(decode_boxes, scores, iou_threshold, max_output_size):\n    keep = tf.py_func(nms_rotate_cpu,\n                      inp=[decode_boxes, scores, iou_threshold, max_output_size],\n                      Tout=tf.int64)\n    return keep\n\n\ndef postprocess_detctions(rpn_bbox_pred, rpn_cls_prob, anchors, is_training):\n\n    return_boxes_pred = []\n    return_scores = []\n    return_labels = []\n    for j in range(0, cfgs.CLASS_NUM):\n        scores = rpn_cls_prob[:, j]\n        if is_training:\n            indices = tf.reshape(tf.where(tf.greater(scores, cfgs.VIS_SCORE)), [-1, ])\n        else:\n            indices = tf.reshape(tf.where(tf.greater(scores, cfgs.FILTERED_SCORE)), [-1, ])\n\n        anchors_ = tf.gather(anchors, indices)\n        rpn_bbox_pred_ = tf.gather(rpn_bbox_pred, indices)\n        scores = tf.gather(scores, indices)\n\n        if cfgs.METHOD == \'H\':\n            x_c = (anchors_[:, 2] + anchors_[:, 0]) / 2\n            y_c = (anchors_[:, 3] + anchors_[:, 1]) / 2\n            h = anchors_[:, 2] - anchors_[:, 0] + 1\n            w = anchors_[:, 3] - anchors_[:, 1] + 1\n            theta = -90 * tf.ones_like(x_c)\n            anchors_ = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n        if cfgs.ANGLE_RANGE == 180:\n            anchors_ = tf.py_func(coordinate_present_convert,\n                                 inp=[anchors_, -1],\n                                 Tout=[tf.float32])\n            anchors_ = tf.reshape(anchors_, [-1, 5])\n\n        boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors_, deltas=rpn_bbox_pred_)\n\n        if cfgs.ANGLE_RANGE == 180:\n\n            _, _, _, _, theta = tf.unstack(boxes_pred, axis=1)\n            indx = tf.reshape(tf.where(tf.logical_and(tf.less(theta, 0), tf.greater_equal(theta, -180))), [-1, ])\n            boxes_pred = tf.gather(boxes_pred, indx)\n            scores = tf.gather(scores, indx)\n\n            boxes_pred = tf.py_func(coordinate_present_convert,\n                                    inp=[boxes_pred, 1],\n                                    Tout=[tf.float32])\n            boxes_pred = tf.reshape(boxes_pred, [-1, 5])\n\n        nms_indices = nms_rotate(decode_boxes=boxes_pred,\n                                 scores=scores,\n                                 iou_threshold=cfgs.NMS_IOU_THRESHOLD,\n                                 max_output_size=100)\n\n        tmp_boxes_pred = tf.reshape(tf.gather(boxes_pred, nms_indices), [-1, 5])\n        tmp_scores = tf.reshape(tf.gather(scores, nms_indices), [-1, ])\n\n        return_boxes_pred.append(tmp_boxes_pred)\n        return_scores.append(tmp_scores)\n        return_labels.append(tf.ones_like(tmp_scores)*(j+1))\n\n    return_boxes_pred = tf.concat(return_boxes_pred, axis=0)\n    return_scores = tf.concat(return_scores, axis=0)\n    return_labels = tf.concat(return_labels, axis=0)\n\n    return return_boxes_pred, return_scores, return_labels\n'"
libs/detection_oprations/refine_proposal_opr.py,11,"b'# encoding: utf-8\nfrom libs.configs import cfgs\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils import nms_rotate\nimport tensorflow as tf\n\n\ndef filter_detections(boxes, scores, is_training):\n    """"""\n    :param boxes: [-1, 4]\n    :param scores: [-1, ]\n    :param labels: [-1, ]\n    :return:\n    """"""\n    if is_training:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.VIS_SCORE)), [-1, ])\n    else:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.FILTERED_SCORE)), [-1, ])\n\n    if cfgs.NMS:\n        filtered_boxes = tf.gather(boxes, indices)\n        filtered_scores = tf.gather(scores, indices)\n\n        # perform NMS\n\n        nms_indices = nms_rotate.nms_rotate(decode_boxes=filtered_boxes,\n                                            scores=filtered_scores,\n                                            iou_threshold=cfgs.NMS_IOU_THRESHOLD,\n                                            max_output_size=100,\n                                            use_angle_condition=False,\n                                            angle_threshold=15,\n                                            use_gpu=False)\n\n        # filter indices based on NMS\n        indices = tf.gather(indices, nms_indices)\n\n    # add indices to list of all indices\n    return indices\n\n\ndef postprocess_detctions(refine_bbox_pred, refine_cls_prob, anchors, is_training):\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=refine_bbox_pred,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    return_boxes_pred = []\n    return_scores = []\n    return_labels = []\n    for j in range(0, cfgs.CLASS_NUM):\n        indices = filter_detections(boxes_pred, refine_cls_prob[:, j], is_training)\n        tmp_boxes_pred = tf.reshape(tf.gather(boxes_pred, indices), [-1, 5])\n        tmp_scores = tf.reshape(tf.gather(refine_cls_prob[:, j], indices), [-1, ])\n\n        return_boxes_pred.append(tmp_boxes_pred)\n        return_scores.append(tmp_scores)\n        return_labels.append(tf.ones_like(tmp_scores)*(j+1))\n\n    return_boxes_pred = tf.concat(return_boxes_pred, axis=0)\n    return_scores = tf.concat(return_scores, axis=0)\n    return_labels = tf.concat(return_labels, axis=0)\n\n    return return_boxes_pred, return_scores, return_labels\n'"
libs/detection_oprations/refine_proposal_opr_csl.py,17,"b'# encoding: utf-8\nfrom libs.configs import cfgs\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils import nms_rotate\nimport tensorflow as tf\n\n\ndef filter_detections(boxes, scores, is_training):\n    """"""\n    :param boxes: [-1, 4]\n    :param scores: [-1, ]\n    :param labels: [-1, ]\n    :return:\n    """"""\n    if is_training:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.VIS_SCORE)), [-1, ])\n    else:\n        indices = tf.reshape(tf.where(tf.greater(scores, cfgs.FILTERED_SCORE)), [-1, ])\n\n    if cfgs.NMS:\n        filtered_boxes = tf.gather(boxes, indices)\n        filtered_scores = tf.gather(scores, indices)\n\n        # perform NMS\n\n        nms_indices = nms_rotate.nms_rotate(decode_boxes=filtered_boxes,\n                                            scores=filtered_scores,\n                                            iou_threshold=cfgs.NMS_IOU_THRESHOLD,\n                                            max_output_size=100,\n                                            use_angle_condition=False,\n                                            angle_threshold=15,\n                                            use_gpu=False)\n\n        # filter indices based on NMS\n        indices = tf.gather(indices, nms_indices)\n\n    # add indices to list of all indices\n    return indices\n\n\ndef postprocess_detctions(refine_bbox_pred, refine_cls_prob, refine_angle_prob, anchors, is_training):\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=refine_bbox_pred,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n    angle_cls = tf.cast(tf.argmax(refine_angle_prob, axis=1), tf.float32)\n    angle_cls = tf.reshape(angle_cls, [-1, ]) * -1 - 0.5\n    x, y, w, h, theta = tf.unstack(boxes_pred, axis=1)\n    boxes_pred_angle = tf.transpose(tf.stack([x, y, w, h, angle_cls]))\n\n    return_boxes_pred = []\n    return_boxes_pred_angle = []\n    return_scores = []\n    return_labels = []\n    for j in range(0, cfgs.CLASS_NUM):\n        indices = filter_detections(boxes_pred_angle, refine_cls_prob[:, j], is_training)\n        tmp_boxes_pred_angle = tf.reshape(tf.gather(boxes_pred_angle, indices), [-1, 5])\n        tmp_boxes_pred = tf.reshape(tf.gather(boxes_pred, indices), [-1, 5])\n        tmp_scores = tf.reshape(tf.gather(refine_cls_prob[:, j], indices), [-1, ])\n\n        return_boxes_pred.append(tmp_boxes_pred)\n        return_boxes_pred_angle.append(tmp_boxes_pred_angle)\n        return_scores.append(tmp_scores)\n        return_labels.append(tf.ones_like(tmp_scores)*(j+1))\n\n    return_boxes_pred = tf.concat(return_boxes_pred, axis=0)\n    return_boxes_pred_angle = tf.concat(return_boxes_pred_angle, axis=0)\n    return_scores = tf.concat(return_scores, axis=0)\n    return_labels = tf.concat(return_labels, axis=0)\n\n    return return_boxes_pred, return_scores, return_labels, return_boxes_pred_angle\n'"
libs/detection_oprations/refinebox_target_layer_without_boxweight.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom libs.configs import cfgs\nimport numpy as np\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils import bbox_transform\n\n\ndef refinebox_target_layer(gt_boxes_r, anchors, pos_threshold, neg_threshold, gpu_id=0):\n\n    anchor_states = np.zeros((anchors.shape[0],))\n    labels = np.zeros((anchors.shape[0], cfgs.CLASS_NUM))\n    if gt_boxes_r.shape[0]:\n        # [N, M]\n\n        overlaps = rbbx_overlaps(np.ascontiguousarray(anchors, dtype=np.float32),\n                                 np.ascontiguousarray(gt_boxes_r[:, :-1], dtype=np.float32), gpu_id)\n\n        argmax_overlaps_inds = np.argmax(overlaps, axis=1)\n        max_overlaps = overlaps[np.arange(overlaps.shape[0]), argmax_overlaps_inds]\n\n        # compute box regression targets\n        target_boxes = gt_boxes_r[argmax_overlaps_inds]\n\n        if cfgs.USE_ANGLE_COND:\n            delta_theta = np.abs(target_boxes[:, -2] - anchors[:, -1])\n            theta_indices = delta_theta < 15\n            positive_indices = (max_overlaps >= pos_threshold) & theta_indices\n            ignore_indices = (max_overlaps > neg_threshold) & (max_overlaps < pos_threshold)\n\n        else:\n            positive_indices = max_overlaps >= pos_threshold\n            ignore_indices = (max_overlaps > neg_threshold) & ~positive_indices\n        anchor_states[ignore_indices] = -1\n        anchor_states[positive_indices] = 1\n\n        # compute target class labels\n        labels[positive_indices, target_boxes[positive_indices, -1].astype(int) - 1] = 1\n    else:\n        # no annotations? then everything is background\n        target_boxes = np.zeros((anchors.shape[0], gt_boxes_r.shape[1]))\n\n    target_delta = bbox_transform.rbbox_transform(ex_rois=anchors, gt_rois=target_boxes,\n                                                  scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    return np.array(labels, np.float32), np.array(target_delta, np.float32), \\\n           np.array(anchor_states, np.float32), np.array(target_boxes, np.float32)\n\n\n\n\n'"
libs/detection_oprations/refinebox_target_layer_without_boxweight_csl.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom libs.configs import cfgs\nimport numpy as np\nfrom libs.box_utils.rbbox_overlaps import rbbx_overlaps\nfrom libs.box_utils import bbox_transform\n\n\ndef refinebox_target_layer(gt_boxes_r, gt_smooth_label, anchors, pos_threshold, neg_threshold, gpu_id=0):\n\n    anchor_states = np.zeros((anchors.shape[0],))\n    labels = np.zeros((anchors.shape[0], cfgs.CLASS_NUM))\n    if gt_boxes_r.shape[0]:\n        # [N, M]\n\n        overlaps = rbbx_overlaps(np.ascontiguousarray(anchors, dtype=np.float32),\n                                 np.ascontiguousarray(gt_boxes_r[:, :-1], dtype=np.float32), gpu_id)\n\n        # overlaps = np.clip(overlaps, 0.0, 1.0)\n\n        argmax_overlaps_inds = np.argmax(overlaps, axis=1)\n        max_overlaps = overlaps[np.arange(overlaps.shape[0]), argmax_overlaps_inds]\n\n        # compute box regression targets\n        target_boxes = gt_boxes_r[argmax_overlaps_inds]\n        target_smooth_label = gt_smooth_label[argmax_overlaps_inds]\n\n        if cfgs.USE_ANGLE_COND:\n            delta_theta = np.abs(target_boxes[:, -2] - anchors[:, -1])\n            theta_indices = delta_theta < 15\n            positive_indices = (max_overlaps >= pos_threshold) & theta_indices\n            ignore_indices = (max_overlaps > neg_threshold) & (max_overlaps < pos_threshold)\n\n        else:\n            positive_indices = max_overlaps >= pos_threshold\n            ignore_indices = (max_overlaps > neg_threshold) & ~positive_indices\n        anchor_states[ignore_indices] = -1\n        anchor_states[positive_indices] = 1\n\n        # compute target class labels\n        labels[positive_indices, target_boxes[positive_indices, -1].astype(int) - 1] = 1\n    else:\n        # no annotations? then everything is background\n        target_boxes = np.zeros((anchors.shape[0], gt_boxes_r.shape[1]))\n        target_smooth_label = np.zeros((anchors.shape[0], 90))\n\n    target_delta = bbox_transform.rbbox_transform(ex_rois=anchors, gt_rois=target_boxes,\n                                                  scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    return np.array(labels, np.float32), np.array(target_delta, np.float32), \\\n           np.array(anchor_states, np.float32), np.array(target_boxes, np.float32), \\\n           np.array(target_smooth_label, np.float32)\n\n\n\n\n'"
libs/label_name_dict/__init__.py,0,b''
libs/label_name_dict/coco_dict.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\nclass_names = [\n    'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n    'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n    'fire hydrant', 'stop sign', 'parking meter', 'bench',\n    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n    'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n    'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle',\n    'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n    'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n    'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear',\n    'hair drier', 'toothbrush']\n\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\noriginID_classes = {item: key for key, item in classes_originID.items()}\nNAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\nLABEL_NAME_MAP = dict(zip(range(len(class_names)), class_names))\n\n# print (originID_classes)\n\n\n\n"""
libs/label_name_dict/label_dict.py,0,"b""# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\n\nfrom libs.configs import cfgs\n\n\nclass_names = [\n        'back_ground', 'person', 'bicycle', 'car', 'motorcycle',\n        'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n        'fire hydrant', 'stop sign', 'parking meter', 'bench',\n        'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant',\n        'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n        'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n        'sports ball', 'kite', 'baseball bat', 'baseball glove',\n        'skateboard', 'surfboard', 'tennis racket', 'bottle',\n        'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n        'banana', 'apple', 'sandwich', 'orange', 'broccoli',\n        'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n        'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n        'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n        'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n        'book', 'clock', 'vase', 'scissors', 'teddy bear',\n        'hair drier', 'toothbrush']\n\nclasses_originID = {\n    'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4,\n    'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9,\n    'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13,\n    'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17,\n    'dog': 18, 'horse': 19, 'sheep': 20, 'cow': 21, 'elephant': 22,\n    'bear': 23, 'zebra': 24, 'giraffe': 25, 'backpack': 27,\n    'umbrella': 28, 'handbag': 31, 'tie': 32, 'suitcase': 33,\n    'frisbee': 34, 'skis': 35, 'snowboard': 36, 'sports ball': 37,\n    'kite': 38, 'baseball bat': 39, 'baseball glove': 40,\n    'skateboard': 41, 'surfboard': 42, 'tennis racket': 43,\n    'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48,\n    'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53,\n    'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57,\n    'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61,\n    'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65,\n    'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73,\n    'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77,\n    'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81,\n    'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86,\n    'scissors': 87, 'teddy bear': 88, 'hair drier': 89,\n    'toothbrush': 90}\n\n\ndef get_coco_label_dict():\n    originID_classes = {item: key for key, item in classes_originID.items()}\n    NAME_LABEL_MAP = dict(zip(class_names, range(len(class_names))))\n    return NAME_LABEL_MAP\n\n\nif cfgs.DATASET_NAME == 'WIDER':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'face': 1\n    }\nelif cfgs.DATASET_NAME == 'ICDAR2015':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'text': 1\n    }\nelif cfgs.DATASET_NAME == 'HRSC2016':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'ship': 1\n    }\nelif cfgs.DATASET_NAME.startswith('DOTA'):\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'roundabout': 1,\n        'tennis-court': 2,\n        'swimming-pool': 3,\n        'storage-tank': 4,\n        'soccer-ball-field': 5,\n        'small-vehicle': 6,\n        'ship': 7,\n        'plane': 8,\n        'large-vehicle': 9,\n        'helicopter': 10,\n        'harbor': 11,\n        'ground-track-field': 12,\n        'bridge': 13,\n        'basketball-court': 14,\n        'baseball-diamond': 15\n    }\nelif cfgs.DATASET_NAME == 'coco':\n    NAME_LABEL_MAP = get_coco_label_dict()\nelif cfgs.DATASET_NAME == 'pascal':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'aeroplane': 1,\n        'bicycle': 2,\n        'bird': 3,\n        'boat': 4,\n        'bottle': 5,\n        'bus': 6,\n        'car': 7,\n        'cat': 8,\n        'chair': 9,\n        'cow': 10,\n        'diningtable': 11,\n        'dog': 12,\n        'horse': 13,\n        'motorbike': 14,\n        'person': 15,\n        'pottedplant': 16,\n        'sheep': 17,\n        'sofa': 18,\n        'train': 19,\n        'tvmonitor': 20\n    }\nelif cfgs.DATASET_NAME == 'bdd100k':\n    NAME_LABEL_MAP = {\n        'back_ground': 0,\n        'bus': 1,\n        'traffic light': 2,\n        'traffic sign': 3,\n        'person': 4,\n        'bike': 5,\n        'truck': 6,\n        'motor': 7,\n        'car': 8,\n        'train': 9,\n        'rider': 10\n    }\nelse:\n    assert 'please set label dict!'\n\n\ndef get_label_name_map():\n    reverse_dict = {}\n    for name, label in NAME_LABEL_MAP.items():\n        reverse_dict[label] = name\n    return reverse_dict\n\n\nLABEL_NAME_MAP = get_label_name_map()"""
libs/losses/__init__.py,0,b''
libs/losses/losses.py,221,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils.iou_rotate import iou_rotate_calculate2, diou_rotate_calculate, adiou_rotate_calculate\nfrom libs.configs import cfgs\n\n\ndef focal_loss_(labels, pred, anchor_state, alpha=0.25, gamma=2.0):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n\n    logits = tf.cast(pred, tf.float32)\n    onehot_labels = tf.cast(labels, tf.float32)\n    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=onehot_labels, logits=logits)\n    predictions = tf.sigmoid(logits)\n    predictions_pt = tf.where(tf.equal(onehot_labels, 1), predictions, 1.-predictions)\n    alpha_t = tf.scalar_mul(alpha, tf.ones_like(onehot_labels, dtype=tf.float32))\n    alpha_t = tf.where(tf.equal(onehot_labels, 1.0), alpha_t, 1-alpha_t)\n    loss = ce * tf.pow(1-predictions_pt, gamma) * alpha_t\n    positive_mask = tf.cast(tf.greater(labels, 0), tf.float32)\n    return tf.reduce_sum(loss) / tf.maximum(tf.reduce_sum(positive_mask), 1)\n\n\ndef focal_loss(labels, pred, anchor_state, alpha=0.25, gamma=2.0):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=pred))\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss) / normalizer\n\n\ndef _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4] in RPN. [-1, cls_num+1, 4] in Fast-rcnn\n    :param bbox_targets: shape is same as bbox_pred\n    :param sigma:\n    :return:\n    \'\'\'\n    sigma_2 = sigma**2\n\n    box_diff = bbox_pred - bbox_targets\n\n    abs_box_diff = tf.abs(box_diff)\n\n    smoothL1_sign = tf.stop_gradient(\n        tf.to_float(tf.less(abs_box_diff, 1. / sigma_2)))\n    loss_box = tf.pow(box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n               + (abs_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    return loss_box\n\n\ndef smooth_l1_loss_rcnn(bbox_targets, bbox_pred, anchor_state, sigma=3.0):\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(anchor_state, 0)))\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, 1, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, 1, 4])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, 1])\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value, 1)*outside_mask) / normalizer\n\n    return bbox_loss\n\n\ndef smooth_l1_loss(targets, preds, anchor_state, sigma=3.0, weight=None):\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    if weight is not None:\n        regression_loss = tf.reduce_sum(regression_loss, axis=-1)\n        weight = tf.gather(weight, indices)\n        regression_loss *= weight\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss) / normalizer\n\n\ndef smooth_l1_loss_atan(targets, preds, anchor_state, sigma=3.0, weight=None):\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n\n    regression_diff = tf.reshape(regression_diff, [-1, 5])\n    dx, dy, dw, dh, dtheta = tf.unstack(regression_diff, axis=-1)\n    dtheta = tf.atan(dtheta)\n    regression_diff = tf.transpose(tf.stack([dx, dy, dw, dh, dtheta]))\n\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    if weight is not None:\n        regression_loss = tf.reduce_sum(regression_loss, axis=-1)\n        weight = tf.gather(weight, indices)\n        regression_loss *= weight\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss) / normalizer\n\n\ndef iou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # -ln(x)\n    iou_factor = tf.stop_gradient(-1 * tf.log(overlaps)) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef iou_smooth_l1_loss_(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, alpha=1.0, beta=1.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(alpha*(1-overlaps)**beta)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef diou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(diou_rotate_calculate,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(1-overlaps)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef adiou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(adiou_rotate_calculate,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(1-overlaps)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef angle_focal_loss(labels, pred, anchor_state, alpha=0.25, gamma=2.0):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = - labels * tf.log(tf.sigmoid(pred) + cfgs.EPSILON) \\\n                          - (1 - labels) * tf.log(1 - tf.sigmoid(pred) + cfgs.EPSILON)\n\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.greater(anchor_state, -2)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss) / normalizer\n\n\ndef scale_iou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0,\n                             is_refine=False, use_scale_factor=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(1-overlaps)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n\n    if use_scale_factor:\n        area = target_boxes[:, 2] * target_boxes[:, 3]\n        area = tf.reshape(area, [-1, 1])\n        scale_factor = tf.stop_gradient(tf.exp(-1 * area) + 1)\n    else:\n        scale_factor = 1.0\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor * scale_factor) / normalizer\n\n\ndef scale_focal_loss(labels, pred, anchor_state, target_boxes, alpha=0.25, gamma=2.0, use_scale_factor=False):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=pred))\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    if use_scale_factor:\n        area = target_boxes[:, 2] * target_boxes[:, 3]\n        area = tf.reshape(area, [-1, 1])\n        scale_factor = tf.stop_gradient(tf.exp(-1 * area) + 1)\n    else:\n        scale_factor = 1.0\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss * scale_factor) / normalizer'"
libs/losses/losses_win.py,160,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\n\nfrom libs.box_utils import bbox_transform\nfrom libs.configs import cfgs\n\n\ndef iou_rotate_calculate2(boxes1, boxes2):\n    ious = []\n    if boxes1.shape[0] != 0:\n        area1 = boxes1[:, 2] * boxes1[:, 3]\n        area2 = boxes2[:, 2] * boxes2[:, 3]\n\n        for i in range(boxes1.shape[0]):\n            temp_ious = []\n            r1 = ((boxes1[i][0], boxes1[i][1]), (boxes1[i][2], boxes1[i][3]), boxes1[i][4])\n            r2 = ((boxes2[i][0], boxes2[i][1]), (boxes2[i][2], boxes2[i][3]), boxes2[i][4])\n\n            int_pts = cv2.rotatedRectangleIntersection(r1, r2)[1]\n            if int_pts is not None:\n                order_pts = cv2.convexHull(int_pts, returnPoints=True)\n\n                int_area = cv2.contourArea(order_pts)\n\n                inter = int_area * 1.0 / (area1[i] + area2[i] - int_area)\n                temp_ious.append(inter)\n            else:\n                temp_ious.append(0.0)\n            ious.append(temp_ious)\n\n    return np.array(ious, dtype=np.float32)\n\n\ndef focal_loss(labels, pred, anchor_state, alpha=0.25, gamma=2.0):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=pred))\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss) / normalizer\n\n\ndef _smooth_l1_loss_base(bbox_pred, bbox_targets, sigma=1.0):\n    \'\'\'\n\n    :param bbox_pred: [-1, 4] in RPN. [-1, cls_num+1, 4] in Fast-rcnn\n    :param bbox_targets: shape is same as bbox_pred\n    :param sigma:\n    :return:\n    \'\'\'\n    sigma_2 = sigma**2\n\n    box_diff = bbox_pred - bbox_targets\n\n    abs_box_diff = tf.abs(box_diff)\n\n    smoothL1_sign = tf.stop_gradient(\n        tf.to_float(tf.less(abs_box_diff, 1. / sigma_2)))\n    loss_box = tf.pow(box_diff, 2) * (sigma_2 / 2.0) * smoothL1_sign \\\n               + (abs_box_diff - (0.5 / sigma_2)) * (1.0 - smoothL1_sign)\n    return loss_box\n\n\ndef smooth_l1_loss_rcnn(bbox_targets, bbox_pred, anchor_state, sigma=3.0):\n\n    outside_mask = tf.stop_gradient(tf.to_float(tf.greater(anchor_state, 0)))\n\n    bbox_pred = tf.reshape(bbox_pred, [-1, 1, 4])\n    bbox_targets = tf.reshape(bbox_targets, [-1, 1, 4])\n\n    value = _smooth_l1_loss_base(bbox_pred,\n                                 bbox_targets,\n                                 sigma=sigma)\n    value = tf.reduce_sum(value, 2)\n    value = tf.reshape(value, [-1, 1])\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    bbox_loss = tf.reduce_sum(\n        tf.reduce_sum(value, 1)*outside_mask) / normalizer\n\n    return bbox_loss\n\n\ndef smooth_l1_loss(targets, preds, anchor_state, sigma=3.0, weight=None):\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    if weight is not None:\n        regression_loss = tf.reduce_sum(regression_loss, axis=-1)\n        weight = tf.gather(weight, indices)\n        regression_loss *= weight\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss) / normalizer\n\n\ndef smooth_l1_loss_atan(targets, preds, anchor_state, sigma=3.0, weight=None):\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n\n    regression_diff = tf.reshape(regression_diff, [-1, 5])\n    dx, dy, dw, dh, dtheta = tf.unstack(regression_diff, axis=-1)\n    dtheta = tf.atan(dtheta)\n    regression_diff = tf.transpose(tf.stack([dx, dy, dw, dh, dtheta]))\n\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    if weight is not None:\n        regression_loss = tf.reduce_sum(regression_loss, axis=-1)\n        weight = tf.gather(weight, indices)\n        regression_loss *= weight\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss) / normalizer\n\n\ndef iou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # -ln(x)\n    iou_factor = tf.stop_gradient(-1 * tf.log(overlaps)) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef iou_smooth_l1_loss_(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0, alpha=1.0, beta=1.0, is_refine=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(alpha*(1-overlaps)**beta)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n    # iou_factor = tf.Print(iou_factor, [iou_factor], \'iou_factor\', summarize=50)\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor) / normalizer\n\n\ndef angle_focal_loss(labels, pred, anchor_state, alpha=0.25, gamma=2.0):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = - labels * tf.log(tf.sigmoid(pred) + cfgs.EPSILON) \\\n                          - (1 - labels) * tf.log(1 - tf.sigmoid(pred) + cfgs.EPSILON)\n\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.greater(anchor_state, -2)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss) / normalizer\n\n\ndef scale_iou_smooth_l1_loss(targets, preds, anchor_state, target_boxes, anchors, sigma=3.0,\n                             is_refine=False, use_scale_factor=False):\n    if cfgs.METHOD == \'H\' and not is_refine:\n        x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n        y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n        h = anchors[:, 2] - anchors[:, 0] + 1\n        w = anchors[:, 3] - anchors[:, 1] + 1\n        theta = -90 * tf.ones_like(x_c)\n        anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n    sigma_squared = sigma ** 2\n    indices = tf.reshape(tf.where(tf.equal(anchor_state, 1)), [-1, ])\n\n    preds = tf.gather(preds, indices)\n    targets = tf.gather(targets, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n    anchors = tf.gather(anchors, indices)\n\n    boxes_pred = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=preds,\n                                                    scale_factors=cfgs.ANCHOR_SCALE_FACTORS)\n\n    # compute smooth L1 loss\n    # f(x) = 0.5 * (sigma * x)^2          if |x| < 1 / sigma / sigma\n    #        |x| - 0.5 / sigma / sigma    otherwise\n    regression_diff = preds - targets\n    regression_diff = tf.abs(regression_diff)\n    regression_loss = tf.where(\n        tf.less(regression_diff, 1.0 / sigma_squared),\n        0.5 * sigma_squared * tf.pow(regression_diff, 2),\n        regression_diff - 0.5 / sigma_squared\n    )\n\n    overlaps = tf.py_func(iou_rotate_calculate2,\n                          inp=[tf.reshape(boxes_pred, [-1, 5]), tf.reshape(target_boxes[:, :-1], [-1, 5])],\n                          Tout=[tf.float32])\n\n    overlaps = tf.reshape(overlaps, [-1, 1])\n    regression_loss = tf.reshape(tf.reduce_sum(regression_loss, axis=1), [-1, 1])\n    # 1-exp(1-x)\n    iou_factor = tf.stop_gradient(tf.exp(1-overlaps)-1) / (tf.stop_gradient(regression_loss) + cfgs.EPSILON)\n\n    if use_scale_factor:\n        area = target_boxes[:, 2] * target_boxes[:, 3]\n        area = tf.reshape(area, [-1, 1])\n        scale_factor = tf.stop_gradient(tf.exp(-1 * area) + 1)\n    else:\n        scale_factor = 1.0\n\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(regression_loss * iou_factor * scale_factor) / normalizer\n\n\ndef scale_focal_loss(labels, pred, anchor_state, target_boxes, alpha=0.25, gamma=2.0, use_scale_factor=False):\n\n    # filter out ""ignore"" anchors\n    indices = tf.reshape(tf.where(tf.not_equal(anchor_state, -1)), [-1, ])\n    labels = tf.gather(labels, indices)\n    pred = tf.gather(pred, indices)\n    target_boxes = tf.gather(target_boxes, indices)\n\n    # compute the focal loss\n    per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=labels, logits=pred))\n    prediction_probabilities = tf.sigmoid(pred)\n    p_t = ((labels * prediction_probabilities) +\n           ((1 - labels) * (1 - prediction_probabilities)))\n    modulating_factor = 1.0\n    if gamma:\n        modulating_factor = tf.pow(1.0 - p_t, gamma)\n    alpha_weight_factor = 1.0\n    if alpha is not None:\n        alpha_weight_factor = (labels * alpha +\n                               (1 - labels) * (1 - alpha))\n    focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *\n                                per_entry_cross_ent)\n\n    if use_scale_factor:\n        area = target_boxes[:, 2] * target_boxes[:, 3]\n        area = tf.reshape(area, [-1, 1])\n        scale_factor = tf.stop_gradient(tf.exp(-1 * area) + 1)\n    else:\n        scale_factor = 1.0\n\n    # compute the normalizer: the number of positive anchors\n    normalizer = tf.stop_gradient(tf.where(tf.equal(anchor_state, 1)))\n    normalizer = tf.cast(tf.shape(normalizer)[0], tf.float32)\n    normalizer = tf.maximum(1.0, normalizer)\n\n    # normalizer = tf.stop_gradient(tf.cast(tf.equal(anchor_state, 1), tf.float32))\n    # normalizer = tf.maximum(tf.reduce_sum(normalizer), 1)\n\n    return tf.reduce_sum(focal_cross_entropy_loss * scale_factor) / normalizer'"
libs/networks/__init__.py,0,b''
libs/networks/build_whole_network.py,46,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv, mobilenet_v2, xception\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.proposal_opr_ import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                              is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'xception\'):\n            return xception.xception_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2 and xception\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=256,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=256,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(\'rpn_net\'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level], scope_list, reuse_flag, level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_all_delta_boxes, rpn_all_boxes_scores, rpn_all_boxes_probs\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    # tmp_anchors = anchor_utils.make_anchors(base_anchor_size=base_anchor_size,\n                    #                                         anchor_scales=cfgs.ANCHOR_SCALES,\n                    #                                         anchor_ratios=cfgs.ANCHOR_RATIOS,\n                    #                                         featuremap_height=featuremap_height,\n                    #                                         featuremap_width=featuremap_width,\n                    #                                         stride=stride,\n                    #                                         name=\'make_anchors_{}\'.format(level))\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n                    anchor_list.append(tmp_anchors)\n\n                all_level_anchors = tf.concat(anchor_list, axis=0)\n            return all_level_anchors\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred, rpn_cls_score, rpn_cls_prob = self.rpn_net(feature_pyramid)\n\n        # 3. generate_anchors\n        anchors = self.make_anchors(feature_pyramid)\n\n        # 4. postprocess rpn proposals. such as: decode, clip, filter\n        if not self.is_training:\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(rpn_bbox_pred=rpn_box_pred,\n                                                                rpn_cls_prob=rpn_cls_prob,\n                                                                anchors=anchors,\n                                                                is_training=self.is_training)\n                return boxes, scores, category\n\n        #  5. build loss\n        else:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors, gpu_id],\n                                                                               Tout=[tf.float32, tf.float32, tf.float32,\n                                                                                     tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n\n                if cfgs.REG_LOSS_MODE == 0:\n                    reg_loss = losses.iou_smooth_l1_loss(target_delta, rpn_box_pred, anchor_states, target_boxes,\n                                                         anchors)\n                elif cfgs.REG_LOSS_MODE == 1:\n                    reg_loss = losses.smooth_l1_loss_atan(target_delta, rpn_box_pred, anchor_states)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                losses_dict = {\'cls_loss\': cls_loss * cfgs.CLS_WEIGHT,\n                               \'reg_loss\': reg_loss * cfgs.REG_WEIGHT}\n\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(rpn_bbox_pred=rpn_box_pred,\n                                                                rpn_cls_prob=rpn_cls_prob,\n                                                                anchors=anchors,\n                                                                is_training=self.is_training)\n                boxes = tf.stop_gradient(boxes)\n                scores = tf.stop_gradient(scores)\n                category = tf.stop_gradient(category)\n\n                return boxes, scores, category, losses_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n'"
libs/networks/build_whole_network_r3det.py,108,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv_r3det, mobilenet_v2\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.refine_proposal_opr import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.refinebox_target_layer_without_boxweight import refinebox_target_layer\nfrom libs.box_utils import bbox_transform\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n        self.losses_dict = {}\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv_r3det.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                                    is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def refine_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'refine_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'refine_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.FPN_CHANNEL,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def refine_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.FPN_CHANNEL,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'refine_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid, name):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == cfgs.LEVEL[0] else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level],\n                                                                     scope_list, reuse_flag,\n                                                                     level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                # rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                # rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                # rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_delta_boxes_list, rpn_scores_list, rpn_probs_list\n\n    def refine_net(self, feature_pyramid, name):\n\n        refine_delta_boxes_list = []\n        refine_scores_list = []\n        refine_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == cfgs.LEVEL[0] else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'refine_classification\', \'refine_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'refine_classification_\' + level, \'refine_regression_\' + level]\n\n                    refine_box_scores, refine_box_probs = self.refine_cls_net(feature_pyramid[level],\n                                                                              scope_list, reuse_flag,\n                                                                              level)\n                    refine_delta_boxes = self.refine_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    refine_scores_list.append(refine_box_scores)\n                    refine_probs_list.append(refine_box_probs)\n                    refine_delta_boxes_list.append(refine_delta_boxes)\n\n                # refine_all_delta_boxes = tf.concat(refine_delta_boxes_list, axis=0)\n                # refine_all_boxes_scores = tf.concat(refine_scores_list, axis=0)\n                # refine_all_boxes_probs = tf.concat(refine_probs_list, axis=0)\n\n            return refine_delta_boxes_list, refine_scores_list, refine_probs_list\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n\n                    anchor_list.append(tmp_anchors)\n\n                # all_level_anchors = tf.concat(anchor_list, axis=0)\n            return anchor_list\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred_list, rpn_cls_score_list, rpn_cls_prob_list = self.rpn_net(feature_pyramid, \'rpn_net\')\n\n        # 3. generate_anchors\n        anchor_list = self.make_anchors(feature_pyramid)\n\n        rpn_box_pred = tf.concat(rpn_box_pred_list, axis=0)\n        rpn_cls_score = tf.concat(rpn_cls_score_list, axis=0)\n        # rpn_cls_prob = tf.concat(rpn_cls_prob_list, axis=0)\n        anchors = tf.concat(anchor_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors],\n                                                                               Tout=[tf.float32, tf.float32,\n                                                                                     tf.float32, tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n                if cfgs.USE_IOU_FACTOR:\n                    reg_loss = losses.iou_smooth_l1_loss_(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                    # reg_loss = losses.adiou_smooth_l1_loss(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                self.losses_dict[\'cls_loss\'] = cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'reg_loss\'] = reg_loss * cfgs.REG_WEIGHT\n\n        box_pred_list, cls_prob_list, proposal_list = rpn_box_pred_list, rpn_cls_prob_list, anchor_list\n\n        all_box_pred_list, all_cls_prob_list, all_proposal_list = [], [], []\n\n        for i in range(cfgs.NUM_REFINE_STAGE):\n            box_pred_list, cls_prob_list, proposal_list = self.refine_stage(input_img_batch,\n                                                                            gtboxes_batch_r,\n                                                                            box_pred_list,\n                                                                            cls_prob_list,\n                                                                            proposal_list,\n                                                                            feature_pyramid,\n                                                                            gpu_id,\n                                                                            pos_threshold=cfgs.REFINE_IOU_POSITIVE_THRESHOLD[i],\n                                                                            neg_threshold=cfgs.REFINE_IOU_NEGATIVE_THRESHOLD[i],\n                                                                            stage=\'\' if i == 0 else \'_stage{}\'.format(i + 2),\n                                                                            proposal_filter=True if i == 0 else False)\n\n            if not self.is_training:\n                all_box_pred_list.extend(box_pred_list)\n                all_cls_prob_list.extend(cls_prob_list)\n                all_proposal_list.extend(proposal_list)\n            else:\n                all_box_pred_list, all_cls_prob_list, all_proposal_list = box_pred_list, cls_prob_list, proposal_list\n\n        with tf.variable_scope(\'postprocess_detctions\'):\n            box_pred = tf.concat(all_box_pred_list, axis=0)\n            cls_prob = tf.concat(all_cls_prob_list, axis=0)\n            proposal = tf.concat(all_proposal_list, axis=0)\n\n            boxes, scores, category = postprocess_detctions(refine_bbox_pred=box_pred,\n                                                            refine_cls_prob=cls_prob,\n                                                            anchors=proposal,\n                                                            is_training=self.is_training)\n            boxes = tf.stop_gradient(boxes)\n            scores = tf.stop_gradient(scores)\n            category = tf.stop_gradient(category)\n\n        if self.is_training:\n            return boxes, scores, category, self.losses_dict\n        else:\n            return boxes, scores, category\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n    def refine_feature_op(self, points, feature_map, name):\n\n        h, w = tf.cast(tf.shape(feature_map)[1], tf.int32), tf.cast(tf.shape(feature_map)[2], tf.int32)\n\n        xmin = tf.maximum(0.0, tf.floor(points[:, 0]))\n        ymin = tf.maximum(0.0, tf.floor(points[:, 1]))\n        xmax = tf.minimum(tf.cast(w - 1, tf.float32), tf.ceil(points[:, 0]))\n        ymax = tf.minimum(tf.cast(h - 1, tf.float32), tf.ceil(points[:, 1]))\n\n        left_top = tf.cast(tf.transpose(tf.stack([xmin, ymin], axis=0)), tf.int32)\n        right_bottom = tf.cast(tf.transpose(tf.stack([xmax, ymax], axis=0)), tf.int32)\n        left_bottom = tf.cast(tf.transpose(tf.stack([xmin, ymax], axis=0)), tf.int32)\n        right_top = tf.cast(tf.transpose(tf.stack([xmax, ymin], axis=0)), tf.int32)\n\n        feature_1x5 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 5],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x5_{}\'.format(name))\n\n        feature5x1 = slim.conv2d(inputs=feature_1x5,\n                                 num_outputs=cfgs.FPN_CHANNEL,\n                                 kernel_size=[5, 1],\n                                 weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                 biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                 stride=1,\n                                 activation_fn=None,\n                                 scope=\'refine_5x1_{}\'.format(name))\n\n        feature_1x1 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 1],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x1_{}\'.format(name))\n\n        feature = feature5x1 + feature_1x1\n        # feature = feature_map\n\n        left_top_feature = tf.gather_nd(tf.squeeze(feature), left_top)\n        right_bottom_feature = tf.gather_nd(tf.squeeze(feature), right_bottom)\n        left_bottom_feature = tf.gather_nd(tf.squeeze(feature), left_bottom)\n        right_top_feature = tf.gather_nd(tf.squeeze(feature), right_top)\n\n        refine_feature = right_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_top_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + right_top_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL])\n\n        refine_feature = tf.reshape(refine_feature, [1, tf.cast(h, tf.int32), tf.cast(w, tf.int32), cfgs.FPN_CHANNEL])\n\n        # refine_feature = tf.reshape(refine_feature, [1, tf.cast(feature_size[1], tf.int32),\n        #                                              tf.cast(feature_size[0], tf.int32), 256])\n\n        return refine_feature + feature\n\n    def refine_stage(self, input_img_batch, gtboxes_batch_r, box_pred_list, cls_prob_list, proposal_list,\n                     feature_pyramid, gpu_id, pos_threshold, neg_threshold,\n                     stage, proposal_filter=False):\n        with tf.variable_scope(\'refine_feature_pyramid{}\'.format(stage)):\n            refine_feature_pyramid = {}\n            refine_boxes_list = []\n\n            for box_pred, cls_prob, proposal, stride, level in \\\n                    zip(box_pred_list, cls_prob_list, proposal_list,\n                        cfgs.ANCHOR_STRIDE, cfgs.LEVEL):\n\n                if proposal_filter:\n                    box_pred = tf.reshape(box_pred, [-1, self.num_anchors_per_location, 5])\n                    proposal = tf.reshape(proposal, [-1, self.num_anchors_per_location, 5 if self.method == \'R\' else 4])\n                    cls_prob = tf.reshape(cls_prob, [-1, self.num_anchors_per_location, cfgs.CLASS_NUM])\n\n                    cls_max_prob = tf.reduce_max(cls_prob, axis=-1)\n                    box_pred_argmax = tf.cast(tf.reshape(tf.argmax(cls_max_prob, axis=-1), [-1, 1]), tf.int32)\n                    indices = tf.cast(tf.cumsum(tf.ones_like(box_pred_argmax), axis=0), tf.int32) - tf.constant(1, tf.int32)\n                    indices = tf.concat([indices, box_pred_argmax], axis=-1)\n\n                    box_pred = tf.reshape(tf.gather_nd(box_pred, indices), [-1, 5])\n                    proposal = tf.reshape(tf.gather_nd(proposal, indices), [-1, 5 if self.method == \'R\' else 4])\n\n                    if cfgs.METHOD == \'H\':\n                        x_c = (proposal[:, 2] + proposal[:, 0]) / 2\n                        y_c = (proposal[:, 3] + proposal[:, 1]) / 2\n                        h = proposal[:, 2] - proposal[:, 0] + 1\n                        w = proposal[:, 3] - proposal[:, 1] + 1\n                        theta = -90 * tf.ones_like(x_c)\n                        proposal = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n                else:\n                    box_pred = tf.reshape(box_pred, [-1, 5])\n                    proposal = tf.reshape(proposal, [-1, 5])\n\n                bboxes = bbox_transform.rbbox_transform_inv(boxes=proposal, deltas=box_pred)\n                refine_boxes_list.append(bboxes)\n                center_point = bboxes[:, :2] / stride\n\n                refine_feature_pyramid[level] = self.refine_feature_op(points=center_point,\n                                                                       feature_map=feature_pyramid[level],\n                                                                       name=level)\n\n            refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list = self.refine_net(refine_feature_pyramid,\n                                                                                                \'refine_net{}\'.format(stage))\n\n            refine_box_pred = tf.concat(refine_box_pred_list, axis=0)\n            refine_cls_score = tf.concat(refine_cls_score_list, axis=0)\n            # refine_cls_prob = tf.concat(refine_cls_prob_list, axis=0)\n            refine_boxes = tf.concat(refine_boxes_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_refine_loss{}\'.format(stage)):\n                refine_labels, refine_target_delta, refine_box_states, refine_target_boxes = tf.py_func(\n                    func=refinebox_target_layer,\n                    inp=[gtboxes_batch_r, refine_boxes, pos_threshold, neg_threshold, gpu_id],\n                    Tout=[tf.float32, tf.float32,\n                          tf.float32, tf.float32])\n\n                self.add_anchor_img_smry(input_img_batch, refine_boxes, refine_box_states, 1)\n\n                refine_cls_loss = losses.focal_loss(refine_labels, refine_cls_score, refine_box_states)\n                if cfgs.USE_IOU_FACTOR:\n                    refine_reg_loss = losses.iou_smooth_l1_loss_(refine_target_delta, refine_box_pred,\n                                                                 refine_box_states, refine_target_boxes,\n                                                                 refine_boxes, is_refine=True)\n                    # refine_reg_loss = losses.adiou_smooth_l1_loss(refine_target_delta, refine_box_pred,\n                    #                                               refine_box_states, refine_target_boxes,\n                    #                                               refine_boxes, is_refine=True)\n                else:\n                    refine_reg_loss = losses.smooth_l1_loss(refine_target_delta, refine_box_pred, refine_box_states)\n\n                self.losses_dict[\'refine_cls_loss{}\'.format(stage)] = refine_cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'refine_reg_loss{}\'.format(stage)] = refine_reg_loss * cfgs.REG_WEIGHT\n\n        return refine_box_pred_list, refine_cls_prob_list, refine_boxes_list'"
libs/networks/build_whole_network_r3det_csl.py,121,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv_r3det, mobilenet_v2, xception\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.refine_proposal_opr_csl import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.refinebox_target_layer_without_boxweight_csl import refinebox_target_layer\nfrom libs.box_utils import bbox_transform\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n        self.losses_dict = {}\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\'] or \'efficientnet\' in cfgs.NET_NAME:\n\n            return resnet_gluoncv_r3det.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                                    is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'xception\'):\n            return xception.xception_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2 and xception\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def refine_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'refine_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'refine_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.FPN_CHANNEL,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def refine_angle_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         scope=\'{}_{}_angle\'.format(scope_list[1], i),\n                                         reuse=reuse_flag)\n\n        rpn_angle_cls = slim.conv2d(rpn_conv2d_3x3,\n                                    num_outputs=cfgs.ANGLE_RANGE,\n                                    kernel_size=[3, 3],\n                                    stride=1,\n                                    weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                    biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                    scope=scope_list[4],\n                                    activation_fn=None,\n                                    reuse=reuse_flag)\n\n        rpn_angle_cls = tf.reshape(rpn_angle_cls, [-1, cfgs.ANGLE_RANGE],\n                                   name=\'rpn_{}_angle_cls_reshape\'.format(level))\n        return rpn_angle_cls\n\n    def refine_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         scope=\'{}_{}\'.format(scope_list[1], i),\n                                         reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_conv2d_3x3,\n                                      num_outputs=5,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_angle_cls = slim.conv2d(rpn_conv2d_3x3,\n                                    num_outputs=cfgs.ANGLE_RANGE,\n                                    kernel_size=[3, 3],\n                                    stride=1,\n                                    weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                    biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                    scope=scope_list[4],\n                                    activation_fn=None,\n                                    reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'refine_{}_regression_reshape\'.format(level))\n        rpn_angle_cls = tf.reshape(rpn_angle_cls, [-1, cfgs.ANGLE_RANGE],\n                                   name=\'rpn_{}_angle_cls_reshape\'.format(level))\n        return rpn_delta_boxes, rpn_angle_cls\n\n    def rpn_net(self, feature_pyramid, name):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level],\n                                                                     scope_list, reuse_flag,\n                                                                     level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                # rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                # rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                # rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_delta_boxes_list, rpn_scores_list, rpn_probs_list\n\n    def refine_net(self, feature_pyramid, name):\n\n        refine_delta_boxes_list = []\n        refine_scores_list = []\n        refine_probs_list = []\n        refine_angle_cls_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'refine_classification\',\n                                      \'refine_regression\', \'rpn_angle_cls\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'refine_classification_\' + level, \'refine_regression_\' + level,\n                                      \'refine_angle_cls_\' + level]\n\n                    refine_box_scores, refine_box_probs = self.refine_cls_net(feature_pyramid[level],\n                                                                              scope_list, reuse_flag,\n                                                                              level)\n                    refine_delta_boxes, refine_angle_cls = self.refine_reg_net(feature_pyramid[level], scope_list,\n                                                                               reuse_flag, level)\n                    # refine_angle_cls = self.refine_angle_net(feature_pyramid[level], scope_list,\n                    #                                          reuse_flag, level)\n\n                    refine_scores_list.append(refine_box_scores)\n                    refine_probs_list.append(refine_box_probs)\n                    refine_delta_boxes_list.append(refine_delta_boxes)\n                    refine_angle_cls_list.append(refine_angle_cls)\n\n                # refine_all_delta_boxes = tf.concat(refine_delta_boxes_list, axis=0)\n                # refine_all_boxes_scores = tf.concat(refine_scores_list, axis=0)\n                # refine_all_boxes_probs = tf.concat(refine_probs_list, axis=0)\n\n            return refine_delta_boxes_list, refine_scores_list, refine_probs_list, refine_angle_cls_list\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n\n                    anchor_list.append(tmp_anchors)\n\n                # all_level_anchors = tf.concat(anchor_list, axis=0)\n            return anchor_list\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method,\n                                                        is_csl=True)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gt_smooth_label, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n            gt_smooth_label = tf.reshape(gt_smooth_label, [-1, cfgs.ANGLE_RANGE])\n            gt_smooth_label = tf.cast(gt_smooth_label, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred_list, rpn_cls_score_list, rpn_cls_prob_list = self.rpn_net(feature_pyramid, \'rpn_net\')\n\n        # 3. generate_anchors\n        anchor_list = self.make_anchors(feature_pyramid)\n\n        rpn_box_pred = tf.concat(rpn_box_pred_list, axis=0)\n        rpn_cls_score = tf.concat(rpn_cls_score_list, axis=0)\n        # rpn_cls_prob = tf.concat(rpn_cls_prob_list, axis=0)\n        anchors = tf.concat(anchor_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors],\n                                                                               Tout=[tf.float32, tf.float32,\n                                                                                     tf.float32, tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n                if cfgs.USE_IOU_FACTOR:\n                    reg_loss = losses.iou_smooth_l1_loss(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                self.losses_dict[\'cls_loss\'] = cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'reg_loss\'] = reg_loss * cfgs.REG_WEIGHT\n\n        box_pred_list, cls_prob_list, proposal_list, angle_cls_list = rpn_box_pred_list, rpn_cls_prob_list, anchor_list, [None, None, None, None, None]\n\n        all_box_pred_list, all_cls_prob_list, all_proposal_list, all_angle_cls_list = [], [], [], []\n\n        for i in range(cfgs.NUM_REFINE_STAGE):\n            box_pred_list, cls_prob_list, proposal_list, angle_cls_list = self.refine_stage(input_img_batch,\n                                                                                            gtboxes_batch_r,\n                                                                                            gt_smooth_label,\n                                                                                            box_pred_list,\n                                                                                            cls_prob_list,\n                                                                                            proposal_list,\n                                                                                            angle_cls_list,\n                                                                                            feature_pyramid,\n                                                                                            gpu_id,\n                                                                                            pos_threshold=cfgs.REFINE_IOU_POSITIVE_THRESHOLD[i],\n                                                                                            neg_threshold=cfgs.REFINE_IOU_NEGATIVE_THRESHOLD[i],\n                                                                                            stage=\'\' if i == 0 else \'_stage{}\'.format(i + 2),\n                                                                                            proposal_filter=True if i == 0 else False)\n\n            if not self.is_training:\n                all_box_pred_list.extend(box_pred_list)\n                all_cls_prob_list.extend(cls_prob_list)\n                all_proposal_list.extend(proposal_list)\n                all_angle_cls_list.extend(angle_cls_list)\n            else:\n                all_box_pred_list, all_cls_prob_list, all_proposal_list, all_angle_cls_list = box_pred_list, cls_prob_list, proposal_list, angle_cls_list\n\n        with tf.variable_scope(\'postprocess_detctions\'):\n            box_pred = tf.concat(all_box_pred_list, axis=0)\n            cls_prob = tf.concat(all_cls_prob_list, axis=0)\n            proposal = tf.concat(all_proposal_list, axis=0)\n            angle_cls = tf.concat(all_angle_cls_list, axis=0)\n\n            boxes, scores, category, boxes_angle = postprocess_detctions(refine_bbox_pred=box_pred,\n                                                                         refine_cls_prob=cls_prob,\n                                                                         refine_angle_prob=tf.sigmoid(angle_cls),\n                                                                         anchors=proposal,\n                                                                         is_training=self.is_training)\n\n            boxes = tf.stop_gradient(boxes)\n            scores = tf.stop_gradient(scores)\n            category = tf.stop_gradient(category)\n            boxes_angle = tf.stop_gradient(boxes_angle)\n\n        if self.is_training:\n            return boxes, scores, category, boxes_angle, self.losses_dict\n        else:\n            return boxes, scores, category, boxes_angle\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n    def refine_feature_op(self, points, feature_map, name):\n\n        h, w = tf.cast(tf.shape(feature_map)[1], tf.int32), tf.cast(tf.shape(feature_map)[2], tf.int32)\n\n        xmin = tf.maximum(0.0, tf.floor(points[:, 0]))\n        ymin = tf.maximum(0.0, tf.floor(points[:, 1]))\n        xmax = tf.minimum(tf.cast(w - 1, tf.float32), tf.ceil(points[:, 0]))\n        ymax = tf.minimum(tf.cast(h - 1, tf.float32), tf.ceil(points[:, 1]))\n\n        left_top = tf.cast(tf.transpose(tf.stack([xmin, ymin], axis=0)), tf.int32)\n        right_bottom = tf.cast(tf.transpose(tf.stack([xmax, ymax], axis=0)), tf.int32)\n        left_bottom = tf.cast(tf.transpose(tf.stack([xmin, ymax], axis=0)), tf.int32)\n        right_top = tf.cast(tf.transpose(tf.stack([xmax, ymin], axis=0)), tf.int32)\n\n        feature_1x5 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 5],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x5_{}\'.format(name))\n\n        feature5x1 = slim.conv2d(inputs=feature_1x5,\n                                 num_outputs=cfgs.FPN_CHANNEL,\n                                 kernel_size=[5, 1],\n                                 weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                 biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                 stride=1,\n                                 activation_fn=None,\n                                 scope=\'refine_5x1_{}\'.format(name))\n\n        feature_1x1 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 1],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x1_{}\'.format(name))\n\n        feature = feature5x1 + feature_1x1\n\n        left_top_feature = tf.gather_nd(tf.squeeze(feature), left_top)\n        right_bottom_feature = tf.gather_nd(tf.squeeze(feature), right_bottom)\n        left_bottom_feature = tf.gather_nd(tf.squeeze(feature), left_bottom)\n        right_top_feature = tf.gather_nd(tf.squeeze(feature), right_top)\n\n        refine_feature = right_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_top_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + right_top_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL])\n\n        refine_feature = tf.reshape(refine_feature, [1, tf.cast(h, tf.int32), tf.cast(w, tf.int32), cfgs.FPN_CHANNEL])\n\n        # refine_feature = tf.reshape(refine_feature, [1, tf.cast(feature_size[1], tf.int32),\n        #                                              tf.cast(feature_size[0], tf.int32), 256])\n\n        return refine_feature + feature\n\n    def refine_stage(self, input_img_batch, gtboxes_batch_r, gt_smooth_label, box_pred_list, cls_prob_list, proposal_list,\n                     angle_cls_list, feature_pyramid, gpu_id, pos_threshold, neg_threshold,\n                     stage, proposal_filter=False):\n        with tf.variable_scope(\'refine_feature_pyramid{}\'.format(stage)):\n            refine_feature_pyramid = {}\n            refine_boxes_list = []\n            refine_boxes_angle_list = []\n\n            for box_pred, cls_prob, proposal, angle_prob, stride, level in \\\n                    zip(box_pred_list, cls_prob_list, proposal_list, angle_cls_list,\n                        cfgs.ANCHOR_STRIDE, cfgs.LEVEL):\n\n                if proposal_filter:\n                    box_pred = tf.reshape(box_pred, [-1, self.num_anchors_per_location, 5])\n                    proposal = tf.reshape(proposal, [-1, self.num_anchors_per_location, 5 if self.method == \'R\' else 4])\n                    cls_prob = tf.reshape(cls_prob, [-1, self.num_anchors_per_location, cfgs.CLASS_NUM])\n\n                    cls_max_prob = tf.reduce_max(cls_prob, axis=-1)\n                    box_pred_argmax = tf.cast(tf.reshape(tf.argmax(cls_max_prob, axis=-1), [-1, 1]), tf.int32)\n                    indices = tf.cast(tf.cumsum(tf.ones_like(box_pred_argmax), axis=0), tf.int32) - tf.constant(1, tf.int32)\n                    indices = tf.concat([indices, box_pred_argmax], axis=-1)\n\n                    box_pred = tf.reshape(tf.gather_nd(box_pred, indices), [-1, 5])\n                    proposal = tf.reshape(tf.gather_nd(proposal, indices), [-1, 5 if self.method == \'R\' else 4])\n\n                    if cfgs.METHOD == \'H\':\n                        x_c = (proposal[:, 2] + proposal[:, 0]) / 2\n                        y_c = (proposal[:, 3] + proposal[:, 1]) / 2\n                        h = proposal[:, 2] - proposal[:, 0] + 1\n                        w = proposal[:, 3] - proposal[:, 1] + 1\n                        theta = -90 * tf.ones_like(x_c)\n                        proposal = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n                else:\n                    box_pred = tf.reshape(box_pred, [-1, 5])\n                    proposal = tf.reshape(proposal, [-1, 5])\n\n                bboxes = bbox_transform.rbbox_transform_inv(boxes=proposal, deltas=box_pred)\n\n                if angle_prob is not None:\n                    angle_cls = tf.cast(tf.argmax(tf.sigmoid(angle_prob), axis=1), tf.float32)\n                    angle_cls = tf.reshape(angle_cls, [-1, ]) * -1 - 0.5\n                    x, y, w, h, theta = tf.unstack(bboxes, axis=1)\n                    bboxes_angle = tf.transpose(tf.stack([x, y, w, h, angle_cls]))\n                    refine_boxes_angle_list.append(bboxes_angle)\n                    center_point = bboxes_angle[:, :2] / stride\n                else:\n                    center_point = bboxes[:, :2] / stride\n                refine_boxes_list.append(bboxes)\n\n                refine_feature_pyramid[level] = self.refine_feature_op(points=center_point,\n                                                                       feature_map=feature_pyramid[level],\n                                                                       name=level)\n\n            refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list, refine_angle_cls_list = self.refine_net(\n                refine_feature_pyramid,\n                \'refine_net{}\'.format(stage))\n\n            refine_box_pred = tf.concat(refine_box_pred_list, axis=0)\n            refine_cls_score = tf.concat(refine_cls_score_list, axis=0)\n            # refine_cls_prob = tf.concat(refine_cls_prob_list, axis=0)\n            refine_boxes = tf.concat(refine_boxes_list, axis=0)\n            refine_angle_cls = tf.concat(refine_angle_cls_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_refine_loss{}\'.format(stage)):\n                refine_labels, refine_target_delta, refine_box_states, refine_target_boxes, refine_target_smooth_label = tf.py_func(\n                    func=refinebox_target_layer,\n                    inp=[gtboxes_batch_r, gt_smooth_label, refine_boxes, pos_threshold, neg_threshold, gpu_id],\n                    Tout=[tf.float32, tf.float32, tf.float32,\n                          tf.float32, tf.float32])\n\n                self.add_anchor_img_smry(input_img_batch, refine_boxes, refine_box_states, 1)\n\n                refine_cls_loss = losses.focal_loss(refine_labels, refine_cls_score, refine_box_states)\n                if False:  # cfgs.USE_IOU_FACTOR:\n                    refine_reg_loss = losses.iou_smooth_l1_loss(refine_target_delta, refine_box_pred,\n                                                                refine_box_states, refine_target_boxes,\n                                                                refine_boxes, is_refine=True)\n                else:\n                    refine_reg_loss = losses.smooth_l1_loss(refine_target_delta, refine_box_pred, refine_box_states)\n\n                angle_cls_loss = losses.angle_focal_loss(refine_target_smooth_label, refine_angle_cls, refine_box_states)\n\n                self.losses_dict[\'refine_cls_loss{}\'.format(stage)] = refine_cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'refine_reg_loss{}\'.format(stage)] = refine_reg_loss * cfgs.REG_WEIGHT\n                self.losses_dict[\'angle_cls_loss{}\'.format(stage)] = angle_cls_loss * cfgs.ANGLE_CLS_WEIGHT\n\n        return refine_box_pred_list, refine_cls_prob_list, refine_boxes_list, refine_angle_cls_list'"
libs/networks/build_whole_network_r3det_efficientnet.py,109,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv_r3det, mobilenet_v2\nfrom libs.networks.efficientnet import efficientnet_builder, efficientnet_lite_builder\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.refine_proposal_opr import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.refinebox_target_layer_without_boxweight import refinebox_target_layer\nfrom libs.box_utils import bbox_transform\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n        self.losses_dict = {}\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv_r3det.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                                    is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        elif \'efficientnet-lite\' in self.base_network_name:\n            return efficientnet_lite_builder.build_model_fpn_base(input_img_batch, model_name=self.base_network_name,\n                                                                  training=True)\n\n        elif \'efficientnet\' in self.base_network_name:\n            return efficientnet_builder.build_model_fpn_base(input_img_batch, model_name=self.base_network_name,\n                                                             training=True)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'box_class_repeats\']):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def refine_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'box_class_repeats\']):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'refine_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'refine_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'box_class_repeats\']):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def refine_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'box_class_repeats\']):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'refine_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid, name):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == cfgs.LEVEL[0] else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level],\n                                                                     scope_list, reuse_flag,\n                                                                     level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                # rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                # rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                # rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_delta_boxes_list, rpn_scores_list, rpn_probs_list\n\n    def refine_net(self, feature_pyramid, name):\n\n        refine_delta_boxes_list = []\n        refine_scores_list = []\n        refine_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == cfgs.LEVEL[0] else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'refine_classification\', \'refine_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'refine_classification_\' + level, \'refine_regression_\' + level]\n\n                    refine_box_scores, refine_box_probs = self.refine_cls_net(feature_pyramid[level],\n                                                                              scope_list, reuse_flag,\n                                                                              level)\n                    refine_delta_boxes = self.refine_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    refine_scores_list.append(refine_box_scores)\n                    refine_probs_list.append(refine_box_probs)\n                    refine_delta_boxes_list.append(refine_delta_boxes)\n\n                # refine_all_delta_boxes = tf.concat(refine_delta_boxes_list, axis=0)\n                # refine_all_boxes_scores = tf.concat(refine_scores_list, axis=0)\n                # refine_all_boxes_probs = tf.concat(refine_probs_list, axis=0)\n\n            return refine_delta_boxes_list, refine_scores_list, refine_probs_list\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n\n                    anchor_list.append(tmp_anchors)\n\n                # all_level_anchors = tf.concat(anchor_list, axis=0)\n            return anchor_list\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred_list, rpn_cls_score_list, rpn_cls_prob_list = self.rpn_net(feature_pyramid, \'rpn_net\')\n\n        # 3. generate_anchors\n        anchor_list = self.make_anchors(feature_pyramid)\n\n        rpn_box_pred = tf.concat(rpn_box_pred_list, axis=0)\n        rpn_cls_score = tf.concat(rpn_cls_score_list, axis=0)\n        # rpn_cls_prob = tf.concat(rpn_cls_prob_list, axis=0)\n        anchors = tf.concat(anchor_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors],\n                                                                               Tout=[tf.float32, tf.float32,\n                                                                                     tf.float32, tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n                if cfgs.USE_IOU_FACTOR:\n                    reg_loss = losses.iou_smooth_l1_loss_(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                self.losses_dict[\'cls_loss\'] = cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'reg_loss\'] = reg_loss * cfgs.REG_WEIGHT\n\n        box_pred_list, cls_prob_list, proposal_list = rpn_box_pred_list, rpn_cls_prob_list, anchor_list\n\n        all_box_pred_list, all_cls_prob_list, all_proposal_list = [], [], []\n\n        for i in range(cfgs.NUM_REFINE_STAGE):\n            box_pred_list, cls_prob_list, proposal_list = self.refine_stage(input_img_batch,\n                                                                            gtboxes_batch_r,\n                                                                            box_pred_list,\n                                                                            cls_prob_list,\n                                                                            proposal_list,\n                                                                            feature_pyramid,\n                                                                            gpu_id,\n                                                                            pos_threshold=cfgs.REFINE_IOU_POSITIVE_THRESHOLD[i],\n                                                                            neg_threshold=cfgs.REFINE_IOU_NEGATIVE_THRESHOLD[i],\n                                                                            stage=\'\' if i == 0 else \'_stage{}\'.format(i + 2),\n                                                                            proposal_filter=True if i == 0 else False)\n\n            if not self.is_training:\n                all_box_pred_list.extend(box_pred_list)\n                all_cls_prob_list.extend(cls_prob_list)\n                all_proposal_list.extend(proposal_list)\n            else:\n                all_box_pred_list, all_cls_prob_list, all_proposal_list = box_pred_list, cls_prob_list, proposal_list\n\n        with tf.variable_scope(\'postprocess_detctions\'):\n            box_pred = tf.concat(all_box_pred_list, axis=0)\n            cls_prob = tf.concat(all_cls_prob_list, axis=0)\n            proposal = tf.concat(all_proposal_list, axis=0)\n\n            boxes, scores, category = postprocess_detctions(refine_bbox_pred=box_pred,\n                                                            refine_cls_prob=cls_prob,\n                                                            anchors=proposal,\n                                                            is_training=self.is_training)\n            boxes = tf.stop_gradient(boxes)\n            scores = tf.stop_gradient(scores)\n            category = tf.stop_gradient(category)\n\n        if self.is_training:\n            return boxes, scores, category, self.losses_dict\n        else:\n            return boxes, scores, category\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            if \'efficientnet\' in cfgs.NET_NAME:\n                model_variables = tf.global_variables()\n            else:\n                model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if \'efficientnet\' in cfgs.NET_NAME and \'/Momentum\' in var.name:\n                    continue\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n    def refine_feature_op(self, points, feature_map, name):\n\n        h, w = tf.cast(tf.shape(feature_map)[1], tf.int32), tf.cast(tf.shape(feature_map)[2], tf.int32)\n\n        xmin = tf.maximum(0.0, tf.floor(points[:, 0]))\n        ymin = tf.maximum(0.0, tf.floor(points[:, 1]))\n        xmax = tf.minimum(tf.cast(w - 1, tf.float32), tf.ceil(points[:, 0]))\n        ymax = tf.minimum(tf.cast(h - 1, tf.float32), tf.ceil(points[:, 1]))\n\n        left_top = tf.cast(tf.transpose(tf.stack([xmin, ymin], axis=0)), tf.int32)\n        right_bottom = tf.cast(tf.transpose(tf.stack([xmax, ymax], axis=0)), tf.int32)\n        left_bottom = tf.cast(tf.transpose(tf.stack([xmin, ymax], axis=0)), tf.int32)\n        right_top = tf.cast(tf.transpose(tf.stack([xmax, ymin], axis=0)), tf.int32)\n\n        feature_1x5 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                  kernel_size=[1, 5],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x5_{}\'.format(name))\n\n        feature5x1 = slim.conv2d(inputs=feature_1x5,\n                                 num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                 kernel_size=[5, 1],\n                                 weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                 biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                 stride=1,\n                                 activation_fn=None,\n                                 scope=\'refine_5x1_{}\'.format(name))\n\n        feature_1x1 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                  kernel_size=[1, 1],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x1_{}\'.format(name))\n\n        feature = feature5x1 + feature_1x1\n\n        left_top_feature = tf.gather_nd(tf.squeeze(feature), left_top)\n        right_bottom_feature = tf.gather_nd(tf.squeeze(feature), right_bottom)\n        left_bottom_feature = tf.gather_nd(tf.squeeze(feature), left_bottom)\n        right_top_feature = tf.gather_nd(tf.squeeze(feature), right_top)\n\n        refine_feature = right_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\']]) \\\n                         + left_top_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\']]) \\\n                         + right_top_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\']]) \\\n                         + left_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\']])\n\n        refine_feature = tf.reshape(refine_feature, [1, tf.cast(h, tf.int32), tf.cast(w, tf.int32), cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\']])\n\n        # refine_feature = tf.reshape(refine_feature, [1, tf.cast(feature_size[1], tf.int32),\n        #                                              tf.cast(feature_size[0], tf.int32), 256])\n\n        return refine_feature + feature\n\n    def refine_stage(self, input_img_batch, gtboxes_batch_r, box_pred_list, cls_prob_list, proposal_list,\n                     feature_pyramid, gpu_id, pos_threshold, neg_threshold,\n                     stage, proposal_filter=False):\n        with tf.variable_scope(\'refine_feature_pyramid{}\'.format(stage)):\n            refine_feature_pyramid = {}\n            refine_boxes_list = []\n\n            for box_pred, cls_prob, proposal, stride, level in \\\n                    zip(box_pred_list, cls_prob_list, proposal_list,\n                        cfgs.ANCHOR_STRIDE, cfgs.LEVEL):\n\n                if proposal_filter:\n                    box_pred = tf.reshape(box_pred, [-1, self.num_anchors_per_location, 5])\n                    proposal = tf.reshape(proposal, [-1, self.num_anchors_per_location, 5 if self.method == \'R\' else 4])\n                    cls_prob = tf.reshape(cls_prob, [-1, self.num_anchors_per_location, cfgs.CLASS_NUM])\n\n                    cls_max_prob = tf.reduce_max(cls_prob, axis=-1)\n                    box_pred_argmax = tf.cast(tf.reshape(tf.argmax(cls_max_prob, axis=-1), [-1, 1]), tf.int32)\n                    indices = tf.cast(tf.cumsum(tf.ones_like(box_pred_argmax), axis=0), tf.int32) - tf.constant(1, tf.int32)\n                    indices = tf.concat([indices, box_pred_argmax], axis=-1)\n\n                    box_pred = tf.reshape(tf.gather_nd(box_pred, indices), [-1, 5])\n                    proposal = tf.reshape(tf.gather_nd(proposal, indices), [-1, 5 if self.method == \'R\' else 4])\n\n                    if cfgs.METHOD == \'H\':\n                        x_c = (proposal[:, 2] + proposal[:, 0]) / 2\n                        y_c = (proposal[:, 3] + proposal[:, 1]) / 2\n                        h = proposal[:, 2] - proposal[:, 0] + 1\n                        w = proposal[:, 3] - proposal[:, 1] + 1\n                        theta = -90 * tf.ones_like(x_c)\n                        proposal = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n                else:\n                    box_pred = tf.reshape(box_pred, [-1, 5])\n                    proposal = tf.reshape(proposal, [-1, 5])\n\n                bboxes = bbox_transform.rbbox_transform_inv(boxes=proposal, deltas=box_pred)\n                refine_boxes_list.append(bboxes)\n                center_point = bboxes[:, :2] / stride\n\n                refine_feature_pyramid[level] = self.refine_feature_op(points=center_point,\n                                                                       feature_map=feature_pyramid[level],\n                                                                       name=level)\n\n            refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list = self.refine_net(refine_feature_pyramid,\n                                                                                                \'refine_net{}\'.format(stage))\n\n            refine_box_pred = tf.concat(refine_box_pred_list, axis=0)\n            refine_cls_score = tf.concat(refine_cls_score_list, axis=0)\n            # refine_cls_prob = tf.concat(refine_cls_prob_list, axis=0)\n            refine_boxes = tf.concat(refine_boxes_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_refine_loss{}\'.format(stage)):\n                refine_labels, refine_target_delta, refine_box_states, refine_target_boxes = tf.py_func(\n                    func=refinebox_target_layer,\n                    inp=[gtboxes_batch_r, refine_boxes, pos_threshold, neg_threshold, gpu_id],\n                    Tout=[tf.float32, tf.float32,\n                          tf.float32, tf.float32])\n\n                self.add_anchor_img_smry(input_img_batch, refine_boxes, refine_box_states, 1)\n\n                refine_cls_loss = losses.focal_loss(refine_labels, refine_cls_score, refine_box_states)\n                if cfgs.USE_IOU_FACTOR:\n                    refine_reg_loss = losses.iou_smooth_l1_loss_(refine_target_delta, refine_box_pred,\n                                                                 refine_box_states, refine_target_boxes,\n                                                                 refine_boxes, is_refine=True)\n                else:\n                    refine_reg_loss = losses.smooth_l1_loss(refine_target_delta, refine_box_pred, refine_box_states)\n\n                self.losses_dict[\'refine_cls_loss{}\'.format(stage)] = refine_cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'refine_reg_loss{}\'.format(stage)] = refine_reg_loss * cfgs.REG_WEIGHT\n\n        return refine_box_pred_list, refine_cls_prob_list, refine_boxes_list'"
libs/networks/build_whole_network_r3det_plusplus.py,118,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv_r3det_plusplus, mobilenet_v2, xception, mobilenet_v2_r3det_plusplus\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.refine_proposal_opr import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.refinebox_target_layer_without_boxweight import refinebox_target_layer\nfrom libs.box_utils import bbox_transform\nfrom libs.box_utils import mask_utils\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n        self.losses_dict = {}\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv_r3det_plusplus.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                                             is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2_r3det_plusplus.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'xception\'):\n            return xception.xception_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2 and xception\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def refine_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=cfgs.FPN_CHANNEL,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'refine_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'refine_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.FPN_CHANNEL,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def refine_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=cfgs.FPN_CHANNEL,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'refine_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid, name):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level],\n                                                                     scope_list, reuse_flag,\n                                                                     level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                # rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                # rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                # rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_delta_boxes_list, rpn_scores_list, rpn_probs_list\n\n    def refine_net(self, feature_pyramid, name):\n\n        refine_delta_boxes_list = []\n        refine_scores_list = []\n        refine_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'refine_classification\', \'refine_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'refine_classification_\' + level, \'refine_regression_\' + level]\n\n                    refine_box_scores, refine_box_probs = self.refine_cls_net(feature_pyramid[level],\n                                                                              scope_list, reuse_flag,\n                                                                              level)\n                    refine_delta_boxes = self.refine_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    refine_scores_list.append(refine_box_scores)\n                    refine_probs_list.append(refine_box_probs)\n                    refine_delta_boxes_list.append(refine_delta_boxes)\n\n                # refine_all_delta_boxes = tf.concat(refine_delta_boxes_list, axis=0)\n                # refine_all_boxes_scores = tf.concat(refine_scores_list, axis=0)\n                # refine_all_boxes_probs = tf.concat(refine_probs_list, axis=0)\n\n            return refine_delta_boxes_list, refine_scores_list, refine_probs_list\n\n    def generate_mask(self, mask_list, img_shape, gtboxes_batch_h, gtboxes_batch_r, feature_pyramid):\n        mask_gt_list = []\n\n        for i, l in enumerate(cfgs.LEVEL):\n            p_h, p_w = tf.shape(feature_pyramid[l])[1], tf.shape(feature_pyramid[l])[2]\n            if cfgs.USE_SUPERVISED_MASK and i < len(mask_list) and self.is_training:\n                if cfgs.MASK_TYPE.strip() == \'h\':\n                    mask = tf.py_func(mask_utils.make_gt_mask,\n                                      [p_h, p_w, img_shape[1], img_shape[2], gtboxes_batch_h],\n                                      Tout=tf.int32)\n                elif cfgs.MASK_TYPE.strip() == \'r\':\n                    mask = tf.py_func(mask_utils.make_r_gt_mask,\n                                      [p_h, p_w, img_shape[1], img_shape[2], gtboxes_batch_r],\n                                      Tout=tf.int32)\n                if cfgs.BINARY_MASK:\n                    mask = tf.where(tf.greater(mask, 0), tf.ones_like(mask), tf.zeros_like(mask))\n                mask_gt_list.append(mask)\n                mask_utils.vis_mask_tfsmry(mask, name=""MASK/%s"" % l)\n        return mask_gt_list\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n\n                    anchor_list.append(tmp_anchors)\n\n                # all_level_anchors = tf.concat(anchor_list, axis=0)\n            return anchor_list\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        img_shape = tf.shape(input_img_batch)\n\n        # 1. build base network\n        if cfgs.USE_SUPERVISED_MASK:\n            feature_pyramid, mask_list, dot_layer_list = self.build_base_network(input_img_batch)\n        else:\n            feature_pyramid = self.build_base_network(input_img_batch)\n            dot_layer_list = None\n            mask_list = []\n\n        # 2. build rpn\n        # if cfgs.USE_SUPERVISED_MASK:\n        #     for i, d in enumerate(dot_layer_list):\n        #         feature_pyramid[\'P{}\'.format(i + 3)] *= d\n        rpn_box_pred_list, rpn_cls_score_list, rpn_cls_prob_list = self.rpn_net(feature_pyramid, \'rpn_net\')\n\n        # 3. generate_anchors and mask\n        anchor_list = self.make_anchors(feature_pyramid)\n\n        if cfgs.USE_SUPERVISED_MASK:\n            mask_gt_list = self.generate_mask(mask_list, img_shape, gtboxes_batch_h, gtboxes_batch_r, feature_pyramid)\n\n        rpn_box_pred = tf.concat(rpn_box_pred_list, axis=0)\n        rpn_cls_score = tf.concat(rpn_cls_score_list, axis=0)\n        # rpn_cls_prob = tf.concat(rpn_cls_prob_list, axis=0)\n        anchors = tf.concat(anchor_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors],\n                                                                               Tout=[tf.float32, tf.float32,\n                                                                                     tf.float32, tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n                if cfgs.USE_IOU_FACTOR:\n                    reg_loss = losses.iou_smooth_l1_loss_(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                if cfgs.USE_SUPERVISED_MASK:\n                    with tf.variable_scope(""supervised_mask_loss""):\n                        mask_loss = 0.0\n                        for i in range(len(mask_list)):\n                            a_mask, a_mask_gt = mask_list[i], mask_gt_list[i]\n                            # b, h, w, c = a_mask.shape\n                            last_dim = 2 if cfgs.BINARY_MASK else cfgs.CLASS_NUM + 1\n                            a_mask = tf.reshape(a_mask, shape=[-1, last_dim])\n                            a_mask_gt = tf.reshape(a_mask_gt, shape=[-1])\n                            a_mask_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=a_mask,\n                                                                                                        labels=a_mask_gt))\n                            mask_loss += a_mask_loss\n                        self.losses_dict[\'mask_loss\'] = mask_loss * cfgs.SUPERVISED_MASK_LOSS_WEIGHT / float(len(mask_list))\n\n                self.losses_dict[\'cls_loss\'] = cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'reg_loss\'] = reg_loss * cfgs.REG_WEIGHT\n\n        box_pred_list, cls_prob_list, proposal_list = rpn_box_pred_list, rpn_cls_prob_list, anchor_list\n\n        all_box_pred_list, all_cls_prob_list, all_proposal_list = [], [], []\n\n        for i in range(cfgs.NUM_REFINE_STAGE):\n            box_pred_list, cls_prob_list, proposal_list = self.refine_stage(input_img_batch,\n                                                                            gtboxes_batch_r,\n                                                                            box_pred_list,\n                                                                            cls_prob_list,\n                                                                            proposal_list,\n                                                                            feature_pyramid,\n                                                                            dot_layer_list,\n                                                                            gpu_id,\n                                                                            pos_threshold=cfgs.REFINE_IOU_POSITIVE_THRESHOLD[i],\n                                                                            neg_threshold=cfgs.REFINE_IOU_NEGATIVE_THRESHOLD[i],\n                                                                            stage=\'\' if i == 0 else \'_stage{}\'.format(i + 2),\n                                                                            proposal_filter=True if i == 0 else False)\n\n            if not self.is_training:\n                all_box_pred_list.extend(box_pred_list)\n                all_cls_prob_list.extend(cls_prob_list)\n                all_proposal_list.extend(proposal_list)\n            else:\n                all_box_pred_list, all_cls_prob_list, all_proposal_list = box_pred_list, cls_prob_list, proposal_list\n\n        with tf.variable_scope(\'postprocess_detctions\'):\n            box_pred = tf.concat(all_box_pred_list, axis=0)\n            cls_prob = tf.concat(all_cls_prob_list, axis=0)\n            proposal = tf.concat(all_proposal_list, axis=0)\n\n            boxes, scores, category = postprocess_detctions(refine_bbox_pred=box_pred,\n                                                            refine_cls_prob=cls_prob,\n                                                            anchors=proposal,\n                                                            is_training=self.is_training)\n            boxes = tf.stop_gradient(boxes)\n            scores = tf.stop_gradient(scores)\n            category = tf.stop_gradient(category)\n\n        if self.is_training:\n            return boxes, scores, category, self.losses_dict\n        else:\n            return boxes, scores, category\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n\n    def refine_feature_op(self, points, feature_map, name):\n\n        h, w = tf.cast(tf.shape(feature_map)[1], tf.int32), tf.cast(tf.shape(feature_map)[2], tf.int32)\n\n        xmin = tf.maximum(0.0, tf.floor(points[:, 0]))\n        ymin = tf.maximum(0.0, tf.floor(points[:, 1]))\n        xmax = tf.minimum(tf.cast(w - 1, tf.float32), tf.ceil(points[:, 0]))\n        ymax = tf.minimum(tf.cast(h - 1, tf.float32), tf.ceil(points[:, 1]))\n\n        left_top = tf.cast(tf.transpose(tf.stack([xmin, ymin], axis=0)), tf.int32)\n        right_bottom = tf.cast(tf.transpose(tf.stack([xmax, ymax], axis=0)), tf.int32)\n        left_bottom = tf.cast(tf.transpose(tf.stack([xmin, ymax], axis=0)), tf.int32)\n        right_top = tf.cast(tf.transpose(tf.stack([xmax, ymin], axis=0)), tf.int32)\n\n        feature_1x5 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 5],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x5_{}\'.format(name))\n\n        feature5x1 = slim.conv2d(inputs=feature_1x5,\n                                 num_outputs=cfgs.FPN_CHANNEL,\n                                 kernel_size=[5, 1],\n                                 weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                 biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                 stride=1,\n                                 activation_fn=None,\n                                 scope=\'refine_5x1_{}\'.format(name))\n\n        feature_1x1 = slim.conv2d(inputs=feature_map,\n                                  num_outputs=cfgs.FPN_CHANNEL,\n                                  kernel_size=[1, 1],\n                                  weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                  biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                  stride=1,\n                                  activation_fn=None,\n                                  scope=\'refine_1x1_{}\'.format(name))\n\n        feature = feature5x1 + feature_1x1\n\n        left_top_feature = tf.gather_nd(tf.squeeze(feature), left_top)\n        right_bottom_feature = tf.gather_nd(tf.squeeze(feature), right_bottom)\n        left_bottom_feature = tf.gather_nd(tf.squeeze(feature), left_bottom)\n        right_top_feature = tf.gather_nd(tf.squeeze(feature), right_top)\n\n        refine_feature = right_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_top_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + right_top_feature * tf.tile(\n            tf.reshape((tf.abs((points[:, 0] - xmin) * (ymax - points[:, 1]))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL]) \\\n                         + left_bottom_feature * tf.tile(\n            tf.reshape((tf.abs((xmax - points[:, 0]) * (points[:, 1] - ymin))), [-1, 1]),\n            [1, cfgs.FPN_CHANNEL])\n\n        refine_feature = tf.reshape(refine_feature, [1, tf.cast(h, tf.int32), tf.cast(w, tf.int32), cfgs.FPN_CHANNEL])\n\n        # refine_feature = tf.reshape(refine_feature, [1, tf.cast(feature_size[1], tf.int32),\n        #                                              tf.cast(feature_size[0], tf.int32), 256])\n\n        return refine_feature + feature\n\n    def refine_stage(self, input_img_batch, gtboxes_batch_r, box_pred_list, cls_prob_list, proposal_list,\n                     feature_pyramid, dot_layer_list, gpu_id, pos_threshold, neg_threshold,\n                     stage, proposal_filter=False):\n        with tf.variable_scope(\'refine_feature_pyramid{}\'.format(stage)):\n            refine_feature_pyramid = {}\n            refine_boxes_list = []\n\n            for box_pred, cls_prob, proposal, stride, level in \\\n                    zip(box_pred_list, cls_prob_list, proposal_list,\n                        cfgs.ANCHOR_STRIDE, cfgs.LEVEL):\n\n                if proposal_filter:\n                    box_pred = tf.reshape(box_pred, [-1, self.num_anchors_per_location, 5])\n                    proposal = tf.reshape(proposal, [-1, self.num_anchors_per_location, 5 if self.method == \'R\' else 4])\n                    cls_prob = tf.reshape(cls_prob, [-1, self.num_anchors_per_location, cfgs.CLASS_NUM])\n\n                    cls_max_prob = tf.reduce_max(cls_prob, axis=-1)\n                    box_pred_argmax = tf.cast(tf.reshape(tf.argmax(cls_max_prob, axis=-1), [-1, 1]), tf.int32)\n                    indices = tf.cast(tf.cumsum(tf.ones_like(box_pred_argmax), axis=0), tf.int32) - tf.constant(1, tf.int32)\n                    indices = tf.concat([indices, box_pred_argmax], axis=-1)\n\n                    box_pred = tf.reshape(tf.gather_nd(box_pred, indices), [-1, 5])\n                    proposal = tf.reshape(tf.gather_nd(proposal, indices), [-1, 5 if self.method == \'R\' else 4])\n\n                    if cfgs.METHOD == \'H\':\n                        x_c = (proposal[:, 2] + proposal[:, 0]) / 2\n                        y_c = (proposal[:, 3] + proposal[:, 1]) / 2\n                        h = proposal[:, 2] - proposal[:, 0] + 1\n                        w = proposal[:, 3] - proposal[:, 1] + 1\n                        theta = -90 * tf.ones_like(x_c)\n                        proposal = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n                else:\n                    box_pred = tf.reshape(box_pred, [-1, 5])\n                    proposal = tf.reshape(proposal, [-1, 5])\n\n                bboxes = bbox_transform.rbbox_transform_inv(boxes=proposal, deltas=box_pred)\n                refine_boxes_list.append(bboxes)\n                center_point = bboxes[:, :2] / stride\n\n                refine_feature_pyramid[level] = self.refine_feature_op(points=center_point,\n                                                                       feature_map=feature_pyramid[level],\n                                                                       name=level)\n\n            if cfgs.USE_SUPERVISED_MASK and stage in [\'\', \'_stage2\']:\n                for i, d in enumerate(dot_layer_list):\n                    refine_feature_pyramid[\'P{}\'.format(i+3)] *= d\n\n            refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list = self.refine_net(refine_feature_pyramid,\n                                                                                                \'refine_net{}\'.format(stage))\n\n            refine_box_pred = tf.concat(refine_box_pred_list, axis=0)\n            refine_cls_score = tf.concat(refine_cls_score_list, axis=0)\n            # refine_cls_prob = tf.concat(refine_cls_prob_list, axis=0)\n            refine_boxes = tf.concat(refine_boxes_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_refine_loss{}\'.format(stage)):\n                refine_labels, refine_target_delta, refine_box_states, refine_target_boxes = tf.py_func(\n                    func=refinebox_target_layer,\n                    inp=[gtboxes_batch_r, refine_boxes, pos_threshold, neg_threshold, gpu_id],\n                    Tout=[tf.float32, tf.float32,\n                          tf.float32, tf.float32])\n\n                self.add_anchor_img_smry(input_img_batch, refine_boxes, refine_box_states, 1)\n\n                refine_cls_loss = losses.focal_loss(refine_labels, refine_cls_score, refine_box_states)\n                if cfgs.USE_IOU_FACTOR:\n                    refine_reg_loss = losses.iou_smooth_l1_loss_(refine_target_delta, refine_box_pred,\n                                                                 refine_box_states, refine_target_boxes,\n                                                                 refine_boxes, is_refine=True)\n                else:\n                    refine_reg_loss = losses.smooth_l1_loss(refine_target_delta, refine_box_pred, refine_box_states)\n\n                self.losses_dict[\'refine_cls_loss{}\'.format(stage)] = refine_cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'refine_reg_loss{}\'.format(stage)] = refine_reg_loss * cfgs.REG_WEIGHT\n\n        return refine_box_pred_list, refine_cls_prob_list, refine_boxes_list'"
libs/networks/build_whole_network_refine_retinanet.py,70,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv, mobilenet_v2, xception\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.refine_proposal_opr import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight import anchor_target_layer\nfrom libs.detection_oprations.refinebox_target_layer_without_boxweight import refinebox_target_layer\nfrom libs.box_utils import bbox_transform\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n        self.losses_dict = {}\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                              is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=256,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def refine_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=256,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'refine_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'refine_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=256,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def refine_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=256,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'refine_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid, name):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level],\n                                                                     scope_list, reuse_flag,\n                                                                     level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                # rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                # rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                # rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_delta_boxes_list, rpn_scores_list, rpn_probs_list\n\n    def refine_net(self, feature_pyramid, name):\n\n        refine_delta_boxes_list = []\n        refine_scores_list = []\n        refine_probs_list = []\n        with tf.variable_scope(name):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'refine_classification\', \'refine_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'refine_classification_\' + level, \'refine_regression_\' + level]\n\n                    refine_box_scores, refine_box_probs = self.refine_cls_net(feature_pyramid[level],\n                                                                              scope_list, reuse_flag,\n                                                                              level)\n                    refine_delta_boxes = self.refine_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    refine_scores_list.append(refine_box_scores)\n                    refine_probs_list.append(refine_box_probs)\n                    refine_delta_boxes_list.append(refine_delta_boxes)\n\n                # refine_all_delta_boxes = tf.concat(refine_delta_boxes_list, axis=0)\n                # refine_all_boxes_scores = tf.concat(refine_scores_list, axis=0)\n                # refine_all_boxes_probs = tf.concat(refine_probs_list, axis=0)\n\n            return refine_delta_boxes_list, refine_scores_list, refine_probs_list\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n                    anchor_list.append(tmp_anchors)\n\n                # all_level_anchors = tf.concat(anchor_list, axis=0)\n            return anchor_list\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred_list, rpn_cls_score_list, rpn_cls_prob_list = self.rpn_net(feature_pyramid, \'rpn_net\')\n\n        # 3. generate_anchors\n        anchor_list = self.make_anchors(feature_pyramid)\n        rpn_box_pred = tf.concat(rpn_box_pred_list, axis=0)\n        rpn_cls_score = tf.concat(rpn_cls_score_list, axis=0)\n        # rpn_cls_prob = tf.concat(rpn_cls_prob_list, axis=0)\n        anchors = tf.concat(anchor_list, axis=0)\n\n        if self.is_training:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors],\n                                                                               Tout=[tf.float32, tf.float32,\n                                                                                     tf.float32, tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses.focal_loss(labels, rpn_cls_score, anchor_states)\n                if cfgs.USE_IOU_FACTOR:\n                    reg_loss = losses.iou_smooth_l1_loss(target_delta, rpn_box_pred, anchor_states, target_boxes, anchors)\n                else:\n                    reg_loss = losses.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                self.losses_dict[\'cls_loss\'] = cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'reg_loss\'] = reg_loss * cfgs.REG_WEIGHT\n\n        with tf.variable_scope(\'refine_feature_pyramid\'):\n            refine_feature_pyramid = {}\n            for level in cfgs.LEVEL:\n                feature_1x5 = slim.conv2d(inputs=feature_pyramid[level],\n                                          num_outputs=256,\n                                          kernel_size=[1, 5],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=None,\n                                          scope=\'refine_1x5_{}\'.format(level))\n\n                feature5x1 = slim.conv2d(inputs=feature_1x5,\n                                         num_outputs=256,\n                                         kernel_size=[5, 1],\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         stride=1,\n                                         activation_fn=None,\n                                         scope=\'refine_5x1_{}\'.format(level))\n\n                feature_1x1 = slim.conv2d(inputs=feature_pyramid[level],\n                                          num_outputs=256,\n                                          kernel_size=[1, 1],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=None,\n                                          scope=\'refine_1x1_{}\'.format(level))\n                refine_feature_pyramid[level] = feature5x1 + feature_1x1\n\n        # refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list = self.refine_net(refine_feature_pyramid, \'refine_net\')\n        refine_box_pred_list, refine_cls_score_list, refine_cls_prob_list = self.refine_net(feature_pyramid, \'refine_net\')\n\n        refine_box_pred = tf.concat(refine_box_pred_list, axis=0)\n        refine_cls_score = tf.concat(refine_cls_score_list, axis=0)\n        refine_cls_prob = tf.concat(refine_cls_prob_list, axis=0)\n        # refine_boxes = tf.concat(refine_boxes_list, axis=0)\n\n        if cfgs.METHOD == \'H\':\n            x_c = (anchors[:, 2] + anchors[:, 0]) / 2\n            y_c = (anchors[:, 3] + anchors[:, 1]) / 2\n            h = anchors[:, 2] - anchors[:, 0] + 1\n            w = anchors[:, 3] - anchors[:, 1] + 1\n            theta = -90 * tf.ones_like(x_c)\n            anchors = tf.transpose(tf.stack([x_c, y_c, w, h, theta]))\n\n        refine_boxes = bbox_transform.rbbox_transform_inv(boxes=anchors, deltas=rpn_box_pred)\n\n        # 4. postprocess rpn proposals. such as: decode, clip, filter\n        if not self.is_training:\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(refine_bbox_pred=refine_box_pred,\n                                                                refine_cls_prob=refine_cls_prob,\n                                                                anchors=refine_boxes,\n                                                                is_training=self.is_training)\n                return boxes, scores, category\n\n        #  5. build loss\n        else:\n            with tf.variable_scope(\'build_refine_loss\'):\n                refine_labels, refine_target_delta, refine_box_states, refine_target_boxes = tf.py_func(\n                    func=refinebox_target_layer,\n                    inp=[gtboxes_batch_r, refine_boxes, cfgs.REFINE_IOU_POSITIVE_THRESHOLD[0], cfgs.REFINE_IOU_NEGATIVE_THRESHOLD[0], gpu_id],\n                    Tout=[tf.float32, tf.float32,\n                          tf.float32, tf.float32])\n\n                self.add_anchor_img_smry(input_img_batch, refine_boxes, refine_box_states, 1)\n\n                refine_cls_loss = losses.focal_loss(refine_labels, refine_cls_score, refine_box_states)\n                if cfgs.USE_IOU_FACTOR:\n                    refine_reg_loss = losses.iou_smooth_l1_loss(refine_target_delta, refine_box_pred,\n                                                                refine_box_states, refine_target_boxes, refine_boxes)\n                else:\n                    refine_reg_loss = losses.smooth_l1_loss(refine_target_delta, refine_box_pred, refine_box_states)\n\n                self.losses_dict[\'refine_cls_loss\'] = refine_cls_loss * cfgs.CLS_WEIGHT\n                self.losses_dict[\'refine_reg_loss\'] = refine_reg_loss * cfgs.REG_WEIGHT\n\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(refine_bbox_pred=refine_box_pred,\n                                                                refine_cls_prob=refine_cls_prob,\n                                                                anchors=refine_boxes,\n                                                                is_training=self.is_training)\n                boxes = tf.stop_gradient(boxes)\n                scores = tf.stop_gradient(scores)\n                category = tf.stop_gradient(category)\n\n                return boxes, scores, category, self.losses_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n'"
libs/networks/build_whole_network_win.py,46,"b'# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom libs.networks import resnet, resnet_gluoncv, mobilenet_v2, xception\nfrom libs.box_utils import anchor_utils, generate_anchors, generate_rotate_anchors\nfrom libs.configs import cfgs\nfrom libs.losses import losses_win\nfrom libs.box_utils import show_box_in_tensor\nfrom libs.detection_oprations.proposal_opr_win import postprocess_detctions\nfrom libs.detection_oprations.anchor_target_layer_without_boxweight_win import anchor_target_layer\n\n\nclass DetectionNetwork(object):\n\n    def __init__(self, base_network_name, is_training):\n\n        self.base_network_name = base_network_name\n        self.is_training = is_training\n        if cfgs.METHOD == \'H\':\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS)\n        else:\n            self.num_anchors_per_location = len(cfgs.ANCHOR_SCALES) * len(cfgs.ANCHOR_RATIOS) * len(cfgs.ANCHOR_ANGLES)\n        self.method = cfgs.METHOD\n\n    def build_base_network(self, input_img_batch):\n\n        if self.base_network_name.startswith(\'resnet_v1\'):\n            return resnet.resnet_base(input_img_batch, scope_name=self.base_network_name, is_training=self.is_training)\n\n        elif self.base_network_name in [\'resnet152_v1d\', \'resnet101_v1d\', \'resnet50_v1d\']:\n\n            return resnet_gluoncv.resnet_base(input_img_batch, scope_name=self.base_network_name,\n                                              is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'MobilenetV2\'):\n            return mobilenet_v2.mobilenetv2_base(input_img_batch, is_training=self.is_training)\n\n        elif self.base_network_name.startswith(\'xception\'):\n            return xception.xception_base(input_img_batch, is_training=self.is_training)\n\n        else:\n            raise ValueError(\'Sry, we only support resnet, mobilenet_v2 and xception\')\n\n    def rpn_cls_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_conv2d_3x3 = inputs\n        for i in range(4):\n            rpn_conv2d_3x3 = slim.conv2d(inputs=rpn_conv2d_3x3,\n                                         num_outputs=256,\n                                         kernel_size=[3, 3],\n                                         stride=1,\n                                         activation_fn=tf.nn.relu,\n                                         weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                         biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                         scope=\'{}_{}\'.format(scope_list[0], i),\n                                         reuse=reuse_flag)\n\n        rpn_box_scores = slim.conv2d(rpn_conv2d_3x3,\n                                     num_outputs=cfgs.CLASS_NUM * self.num_anchors_per_location,\n                                     kernel_size=[3, 3],\n                                     stride=1,\n                                     weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                     biases_initializer=cfgs.FINAL_CONV_BIAS_INITIALIZER,\n                                     scope=scope_list[2],\n                                     activation_fn=None,\n                                     reuse=reuse_flag)\n\n        rpn_box_scores = tf.reshape(rpn_box_scores, [-1, cfgs.CLASS_NUM],\n                                    name=\'rpn_{}_classification_reshape\'.format(level))\n        rpn_box_probs = tf.sigmoid(rpn_box_scores, name=\'rpn_{}_classification_sigmoid\'.format(level))\n\n        return rpn_box_scores, rpn_box_probs\n\n    def rpn_reg_net(self, inputs, scope_list, reuse_flag, level):\n        rpn_delta_boxes = inputs\n        for i in range(4):\n            rpn_delta_boxes = slim.conv2d(inputs=rpn_delta_boxes,\n                                          num_outputs=256,\n                                          kernel_size=[3, 3],\n                                          weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                          biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                          stride=1,\n                                          activation_fn=tf.nn.relu,\n                                          scope=\'{}_{}\'.format(scope_list[1], i),\n                                          reuse=reuse_flag)\n\n        rpn_delta_boxes = slim.conv2d(rpn_delta_boxes,\n                                      num_outputs=5 * self.num_anchors_per_location,\n                                      kernel_size=[3, 3],\n                                      stride=1,\n                                      weights_initializer=cfgs.SUBNETS_WEIGHTS_INITIALIZER,\n                                      biases_initializer=cfgs.SUBNETS_BIAS_INITIALIZER,\n                                      scope=scope_list[3],\n                                      activation_fn=None,\n                                      reuse=reuse_flag)\n\n        rpn_delta_boxes = tf.reshape(rpn_delta_boxes, [-1, 5],\n                                     name=\'rpn_{}_regression_reshape\'.format(level))\n        return rpn_delta_boxes\n\n    def rpn_net(self, feature_pyramid):\n\n        rpn_delta_boxes_list = []\n        rpn_scores_list = []\n        rpn_probs_list = []\n        with tf.variable_scope(\'rpn_net\'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY)):\n                for level in cfgs.LEVEL:\n\n                    if cfgs.SHARE_NET:\n                        reuse_flag = None if level == \'P3\' else True\n                        scope_list = [\'conv2d_3x3_cls\', \'conv2d_3x3_reg\', \'rpn_classification\', \'rpn_regression\']\n                    else:\n                        reuse_flag = None\n                        scope_list = [\'conv2d_3x3_cls_\' + level, \'conv2d_3x3_reg_\' + level,\n                                      \'rpn_classification_\' + level, \'rpn_regression_\' + level]\n\n                    rpn_box_scores, rpn_box_probs = self.rpn_cls_net(feature_pyramid[level], scope_list, reuse_flag, level)\n                    rpn_delta_boxes = self.rpn_reg_net(feature_pyramid[level], scope_list, reuse_flag, level)\n\n                    rpn_scores_list.append(rpn_box_scores)\n                    rpn_probs_list.append(rpn_box_probs)\n                    rpn_delta_boxes_list.append(rpn_delta_boxes)\n\n                rpn_all_delta_boxes = tf.concat(rpn_delta_boxes_list, axis=0)\n                rpn_all_boxes_scores = tf.concat(rpn_scores_list, axis=0)\n                rpn_all_boxes_probs = tf.concat(rpn_probs_list, axis=0)\n\n            return rpn_all_delta_boxes, rpn_all_boxes_scores, rpn_all_boxes_probs\n\n    def make_anchors(self, feature_pyramid):\n        with tf.variable_scope(\'make_anchors\'):\n            anchor_list = []\n            level_list = cfgs.LEVEL\n            with tf.name_scope(\'make_anchors_all_level\'):\n                for level, base_anchor_size, stride in zip(level_list, cfgs.BASE_ANCHOR_SIZE_LIST, cfgs.ANCHOR_STRIDE):\n                    \'\'\'\n                    (level, base_anchor_size) tuple:\n                    (P3, 32), (P4, 64), (P5, 128), (P6, 256), (P7, 512)\n                    \'\'\'\n                    featuremap_height, featuremap_width = tf.shape(feature_pyramid[level])[1], \\\n                                                          tf.shape(feature_pyramid[level])[2]\n\n                    featuremap_height = tf.cast(featuremap_height, tf.float32)\n                    featuremap_width = tf.cast(featuremap_width, tf.float32)\n\n                    # tmp_anchors = anchor_utils.make_anchors(base_anchor_size=base_anchor_size,\n                    #                                         anchor_scales=cfgs.ANCHOR_SCALES,\n                    #                                         anchor_ratios=cfgs.ANCHOR_RATIOS,\n                    #                                         featuremap_height=featuremap_height,\n                    #                                         featuremap_width=featuremap_width,\n                    #                                         stride=stride,\n                    #                                         name=\'make_anchors_{}\'.format(level))\n                    if self.method == \'H\':\n                        tmp_anchors = tf.py_func(generate_anchors.generate_anchors_pre,\n                                                 inp=[featuremap_height, featuremap_width, stride,\n                                                      np.array(cfgs.ANCHOR_SCALES) * stride, cfgs.ANCHOR_RATIOS, 4.0],\n                                                 Tout=[tf.float32])\n\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 4])\n                    else:\n                        tmp_anchors = generate_rotate_anchors.make_anchors(base_anchor_size=base_anchor_size,\n                                                                           anchor_scales=cfgs.ANCHOR_SCALES,\n                                                                           anchor_ratios=cfgs.ANCHOR_RATIOS,\n                                                                           anchor_angles=cfgs.ANCHOR_ANGLES,\n                                                                           featuremap_height=featuremap_height,\n                                                                           featuremap_width=featuremap_width,\n                                                                           stride=stride)\n                        tmp_anchors = tf.reshape(tmp_anchors, [-1, 5])\n                    anchor_list.append(tmp_anchors)\n\n                all_level_anchors = tf.concat(anchor_list, axis=0)\n            return all_level_anchors\n\n    def add_anchor_img_smry(self, img, anchors, labels, method):\n\n        positive_anchor_indices = tf.reshape(tf.where(tf.greater_equal(labels, 1)), [-1])\n        # negative_anchor_indices = tf.reshape(tf.where(tf.equal(labels, 0)), [-1])\n\n        positive_anchor = tf.gather(anchors, positive_anchor_indices)\n        # negative_anchor = tf.gather(anchors, negative_anchor_indices)\n\n        pos_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n                                                        boxes=positive_anchor,\n                                                        method=method)\n        # neg_in_img = show_box_in_tensor.only_draw_boxes(img_batch=img,\n        #                                                 boxes=negative_anchor)\n\n        tf.summary.image(\'positive_anchor\', pos_in_img)\n        # tf.summary.image(\'negative_anchors\', neg_in_img)\n\n    def build_whole_detection_network(self, input_img_batch, gtboxes_batch_h, gtboxes_batch_r, gpu_id=0):\n\n        if self.is_training:\n            gtboxes_batch_h = tf.reshape(gtboxes_batch_h, [-1, 5])\n            gtboxes_batch_h = tf.cast(gtboxes_batch_h, tf.float32)\n\n            gtboxes_batch_r = tf.reshape(gtboxes_batch_r, [-1, 6])\n            gtboxes_batch_r = tf.cast(gtboxes_batch_r, tf.float32)\n\n        # 1. build base network\n        feature_pyramid = self.build_base_network(input_img_batch)\n\n        # 2. build rpn\n        rpn_box_pred, rpn_cls_score, rpn_cls_prob = self.rpn_net(feature_pyramid)\n\n        # 3. generate_anchors\n        anchors = self.make_anchors(feature_pyramid)\n\n        # 4. postprocess rpn proposals. such as: decode, clip, filter\n        if not self.is_training:\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(rpn_bbox_pred=rpn_box_pred,\n                                                                rpn_cls_prob=rpn_cls_prob,\n                                                                anchors=anchors,\n                                                                is_training=self.is_training)\n                return boxes, scores, category\n\n        #  5. build loss\n        else:\n            with tf.variable_scope(\'build_loss\'):\n                labels, target_delta, anchor_states, target_boxes = tf.py_func(func=anchor_target_layer,\n                                                                               inp=[gtboxes_batch_h, gtboxes_batch_r,\n                                                                                    anchors, gpu_id],\n                                                                               Tout=[tf.float32, tf.float32, tf.float32,\n                                                                                     tf.float32])\n\n                if self.method == \'H\':\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 0)\n                else:\n                    self.add_anchor_img_smry(input_img_batch, anchors, anchor_states, 1)\n\n                cls_loss = losses_win.focal_loss(labels, rpn_cls_score, anchor_states)\n\n                if cfgs.REG_LOSS_MODE == 0:\n                    reg_loss = losses_win.iou_smooth_l1_loss(target_delta, rpn_box_pred, anchor_states, target_boxes,\n                                                         anchors)\n                elif cfgs.REG_LOSS_MODE == 1:\n                    reg_loss = losses_win.smooth_l1_loss_atan(target_delta, rpn_box_pred, anchor_states)\n                else:\n                    reg_loss = losses_win.smooth_l1_loss(target_delta, rpn_box_pred, anchor_states)\n\n                losses_dict = {\'cls_loss\': cls_loss * cfgs.CLS_WEIGHT,\n                               \'reg_loss\': reg_loss * cfgs.REG_WEIGHT}\n\n            with tf.variable_scope(\'postprocess_detctions\'):\n                boxes, scores, category = postprocess_detctions(rpn_bbox_pred=rpn_box_pred,\n                                                                rpn_cls_prob=rpn_cls_prob,\n                                                                anchors=anchors,\n                                                                is_training=self.is_training)\n                boxes = tf.stop_gradient(boxes)\n                scores = tf.stop_gradient(scores)\n                category = tf.stop_gradient(category)\n\n                return boxes, scores, category, losses_dict\n\n    def get_restorer(self):\n        checkpoint_path = tf.train.latest_checkpoint(os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION))\n\n        if checkpoint_path != None:\n            if cfgs.RESTORE_FROM_RPN:\n                print(\'___restore from rpn___\')\n                model_variables = slim.get_model_variables()\n                restore_variables = [var for var in model_variables if not var.name.startswith(\'FastRCNN_Head\')] + \\\n                                    [slim.get_or_create_global_step()]\n                for var in restore_variables:\n                    print(var.name)\n                restorer = tf.train.Saver(restore_variables)\n            else:\n                restorer = tf.train.Saver()\n            print(""model restore from :"", checkpoint_path)\n        else:\n            checkpoint_path = cfgs.PRETRAINED_CKPT\n            print(""model restore from pretrained mode, path is :"", checkpoint_path)\n\n            model_variables = slim.get_model_variables()\n\n            # for var in model_variables:\n            #     print(var.name)\n            # print(20*""__++__++__"")\n\n            def name_in_ckpt_rpn(var):\n                return var.op.name\n\n            def name_in_ckpt_fastrcnn_head(var):\n                \'\'\'\n                Fast-RCNN/resnet_v1_50/block4 -->resnet_v1_50/block4\n                Fast-RCNN/MobilenetV2/** -- > MobilenetV2 **\n                :param var:\n                :return:\n                \'\'\'\n                return \'/\'.join(var.op.name.split(\'/\')[1:])\n\n            nameInCkpt_Var_dict = {}\n            for var in model_variables:\n                if var.name.startswith(\'Fast-RCNN/\'+self.base_network_name):  # +\'/block4\'\n                    var_name_in_ckpt = name_in_ckpt_fastrcnn_head(var)\n                    nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                else:\n                    if var.name.startswith(self.base_network_name):\n                        var_name_in_ckpt = name_in_ckpt_rpn(var)\n                        nameInCkpt_Var_dict[var_name_in_ckpt] = var\n                    else:\n                        continue\n            restore_variables = nameInCkpt_Var_dict\n            for key, item in restore_variables.items():\n                print(""var_in_graph: "", item.name)\n                print(""var_in_ckpt: "", key)\n                print(20*""___"")\n            restorer = tf.train.Saver(restore_variables)\n            print(20 * ""****"")\n            print(""restore from pretrained_weighs in IMAGE_NET"")\n        return restorer, checkpoint_path\n\n    def get_gradients(self, optimizer, loss):\n        \'\'\'\n\n        :param optimizer:\n        :param loss:\n        :return:\n\n        return vars and grads that not be fixed\n        \'\'\'\n\n        # if cfgs.FIXED_BLOCKS > 0:\n        #     trainable_vars = tf.trainable_variables()\n        #     # trained_vars = slim.get_trainable_variables()\n        #     start_names = [cfgs.NET_NAME + \'/block%d\'%i for i in range(1, cfgs.FIXED_BLOCKS+1)] + \\\n        #                   [cfgs.NET_NAME + \'/conv1\']\n        #     start_names = tuple(start_names)\n        #     trained_var_list = []\n        #     for var in trainable_vars:\n        #         if not var.name.startswith(start_names):\n        #             trained_var_list.append(var)\n        #     # slim.learning.train()\n        #     grads = optimizer.compute_gradients(loss, var_list=trained_var_list)\n        #     return grads\n        # else:\n        #     return optimizer.compute_gradients(loss)\n        return optimizer.compute_gradients(loss)\n\n    def enlarge_gradients_for_bias(self, gradients):\n\n        final_gradients = []\n        with tf.variable_scope(""Gradient_Mult"") as scope:\n            for grad, var in gradients:\n                scale = 1.0\n                if cfgs.MUTILPY_BIAS_GRADIENT and \'./biases\' in var.name:\n                    scale = scale * cfgs.MUTILPY_BIAS_GRADIENT\n                if not np.allclose(scale, 1.0):\n                    grad = tf.multiply(grad, scale)\n                final_gradients.append((grad, var))\n        return final_gradients\n'"
libs/networks/mobilenet_v2.py,5,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import mobilenet_v2\nfrom libs.networks.mobilenet.mobilenet import training_scope\nfrom libs.networks.mobilenet.mobilenet_v2 import op\nfrom libs.networks.mobilenet.mobilenet_v2 import ops\nfrom libs.networks.resnet import fusion_two_layer\nfrom libs.configs import cfgs\nexpand_input = ops.expand_input_by_factor\n\nV2_BASE_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16, scope=\'expanded_conv\'),\n        op(ops.expanded_conv, stride=2, num_outputs=24, scope=\'expanded_conv_1\'),\n        op(ops.expanded_conv, stride=1, num_outputs=24, scope=\'expanded_conv_2\'),\n        op(ops.expanded_conv, stride=2, num_outputs=32, scope=\'expanded_conv_3\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_4\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_5\'),\n        op(ops.expanded_conv, stride=2, num_outputs=64, scope=\'expanded_conv_6\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_7\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_8\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_9\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_10\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_11\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_12\'),\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\n\n\nV2_HEAD_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\n\ndef mobilenetv2_scope(is_training=True,\n                      trainable=True,\n                      weight_decay=0.00004,\n                      stddev=0.09,\n                      dropout_keep_prob=0.8,\n                      bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  In default. We do not use BN\n  ReWrite the scope.\n  """"""\n  batch_norm_params = {\n      \'is_training\': False,\n      \'trainable\': False,\n      \'decay\': bn_decay,\n  }\n  with slim.arg_scope(training_scope(is_training=is_training, weight_decay=weight_decay)):\n      with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n                          trainable=trainable):\n          with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n              return sc\n\n\n\ndef mobilenetv2_base(img_batch, is_training=True):\n\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        feature_to_crop, endpoints = mobilenet_v2.mobilenet_base(input_tensor=img_batch,\n                                                                 num_classes=None,\n                                                                 is_training=False,\n                                                                 depth_multiplier=1.0,\n                                                                 scope=\'MobilenetV2\',\n                                                                 conv_defs=V2_BASE_DEF,\n                                                                 finegrain_classification_mode=False)\n\n        feature_dict = {""C3"": endpoints[\'layer_5\'],\n                        \'C4\': endpoints[\'layer_8\'],\n                        \'C5\': endpoints[\'layer_15\']}\n        pyramid_dict = {}\n        with tf.variable_scope(\'build_pyramid\'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                                activation_fn=None, normalizer_fn=None):\n\n                P5 = slim.conv2d(feature_dict[\'C5\'],\n                                 num_outputs=256,\n                                 kernel_size=[1, 1],\n                                 stride=1, scope=\'build_P5\')\n\n                pyramid_dict[\'P5\'] = P5\n\n                for level in range(4, 2, -1):  # build [P4, P3]\n\n                    pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                                   P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                                   scope=\'build_P%d\' % level)\n                for level in range(5, 2, -1):\n                    pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                              num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                                              stride=1, scope=""fuse_P%d"" % level)\n\n                p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                                 num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                 stride=2, scope=\'p6_conv\')\n                pyramid_dict[\'P6\'] = p6\n\n                p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n                p7 = slim.conv2d(p7,\n                                 num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                 stride=2, scope=\'p7_conv\')\n\n                pyramid_dict[\'P7\'] = p7\n\n        # for level in range(7, 1, -1):\n        #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n        return pyramid_dict\n\n\ndef mobilenetv2_head(inputs, is_training=True):\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        net, _ = mobilenet_v2.mobilenet(input_tensor=inputs,\n                                        num_classes=None,\n                                        is_training=False,\n                                        depth_multiplier=1.0,\n                                        scope=\'MobilenetV2\',\n                                        conv_defs=V2_HEAD_DEF,\n                                        finegrain_classification_mode=False)\n\n        net = tf.squeeze(net, [1, 2])\n\n        return net'"
libs/networks/mobilenet_v2_r3det_plusplus.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\nimport tensorflow.contrib.slim as slim\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import mobilenet_v2\nfrom libs.networks.mobilenet.mobilenet import training_scope\nfrom libs.networks.mobilenet.mobilenet_v2 import op\nfrom libs.networks.mobilenet.mobilenet_v2 import ops\nfrom libs.networks.resnet import fusion_two_layer\nfrom libs.configs import cfgs\nexpand_input = ops.expand_input_by_factor\n\nV2_BASE_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16, scope=\'expanded_conv\'),\n        op(ops.expanded_conv, stride=2, num_outputs=24, scope=\'expanded_conv_1\'),\n        op(ops.expanded_conv, stride=1, num_outputs=24, scope=\'expanded_conv_2\'),\n        op(ops.expanded_conv, stride=2, num_outputs=32, scope=\'expanded_conv_3\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_4\'),\n        op(ops.expanded_conv, stride=1, num_outputs=32, scope=\'expanded_conv_5\'),\n        op(ops.expanded_conv, stride=2, num_outputs=64, scope=\'expanded_conv_6\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_7\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_8\'),\n        op(ops.expanded_conv, stride=1, num_outputs=64, scope=\'expanded_conv_9\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_10\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_11\'),\n        op(ops.expanded_conv, stride=1, num_outputs=96, scope=\'expanded_conv_12\'),\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\n\n\nV2_HEAD_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(ops.expanded_conv, stride=2, num_outputs=160, scope=\'expanded_conv_13\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_14\'),\n        op(ops.expanded_conv, stride=1, num_outputs=160, scope=\'expanded_conv_15\'),\n        op(ops.expanded_conv, stride=1, num_outputs=320, scope=\'expanded_conv_16\'),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280, scope=\'Conv_1\')\n    ],\n)\n\ndef mobilenetv2_scope(is_training=True,\n                      trainable=True,\n                      weight_decay=0.00004,\n                      stddev=0.09,\n                      dropout_keep_prob=0.8,\n                      bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n  In default. We do not use BN\n  ReWrite the scope.\n  """"""\n  batch_norm_params = {\n      \'is_training\': False,\n      \'trainable\': False,\n      \'decay\': bn_decay,\n  }\n  with slim.arg_scope(training_scope(is_training=is_training, weight_decay=weight_decay)):\n      with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n                          trainable=trainable):\n          with slim.arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n              return sc\n\n\ndef mobilenetv2_base(img_batch, is_training=True):\n\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        feature_to_crop, endpoints = mobilenet_v2.mobilenet_base(input_tensor=img_batch,\n                                                                 num_classes=None,\n                                                                 is_training=False,\n                                                                 depth_multiplier=1.0,\n                                                                 scope=\'MobilenetV2\',\n                                                                 conv_defs=V2_BASE_DEF,\n                                                                 finegrain_classification_mode=False)\n\n        feature_dict = {""C3"": endpoints[\'layer_5\'],\n                        \'C4\': endpoints[\'layer_8\'],\n                        \'C5\': endpoints[\'layer_15\']}\n        pyramid_dict = {}\n        with tf.variable_scope(\'build_pyramid\'):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                                activation_fn=None, normalizer_fn=None):\n\n                P5 = slim.conv2d(feature_dict[\'C5\'],\n                                 num_outputs=256,\n                                 kernel_size=[1, 1],\n                                 stride=1, scope=\'build_P5\')\n\n                pyramid_dict[\'P5\'] = P5\n\n                for level in range(4, 2, -1):  # build [P4, P3]\n\n                    pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                                   P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                                   scope=\'build_P%d\' % level)\n                for level in range(5, 2, -1):\n                    pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                              num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                                              stride=1, scope=""fuse_P%d"" % level)\n\n                p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                                 num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                 stride=2, scope=\'p6_conv\')\n                pyramid_dict[\'P6\'] = p6\n\n                p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n                p7 = slim.conv2d(p7,\n                                 num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                 stride=2, scope=\'p7_conv\')\n\n                pyramid_dict[\'P7\'] = p7\n\n        print(""we are in Pyramid::-======>>>>"")\n        print(cfgs.LEVEL)\n        print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n        print(20 * ""__"")\n\n        if cfgs.USE_SUPERVISED_MASK:\n            mask_list = []\n            dot_layer_list = []\n            with tf.variable_scope(""enrich_semantics""):\n                with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                                    normalizer_fn=None):\n                    for i, l_name in enumerate(cfgs.GENERATE_MASK_LIST):\n                        G, mask, dot_layer = generate_mask(net=pyramid_dict[l_name],\n                                                           num_layer=cfgs.ADDITION_LAYERS[i],\n                                                           level_name=l_name)\n                        # add_heatmap(G, name=""MASK/G_%s"" % l_name)\n                        # add_heatmap(mask, name=""MASK/mask_%s"" % l_name)\n\n                        # if cfgs.MASK_ACT_FET:\n                        #     pyramid_dict[l_name] = pyramid_dict[l_name] * dot_layer\n                        dot_layer_list.append(dot_layer)\n                        mask_list.append(mask)\n\n            return pyramid_dict, mask_list, dot_layer_list\n        else:\n            return pyramid_dict\n\n\ndef mobilenetv2_head(inputs, is_training=True):\n    with slim.arg_scope(mobilenetv2_scope(is_training=is_training, trainable=True)):\n        net, _ = mobilenet_v2.mobilenet(input_tensor=inputs,\n                                        num_classes=None,\n                                        is_training=False,\n                                        depth_multiplier=1.0,\n                                        scope=\'MobilenetV2\',\n                                        conv_defs=V2_HEAD_DEF,\n                                        finegrain_classification_mode=False)\n\n        net = tf.squeeze(net, [1, 2])\n\n        return net\n\n\ndef generate_mask(net, num_layer, level_name):\n    G = enrich_semantics_supervised(net=net,\n                                    num_layer=num_layer,\n                                    channels=cfgs.FPN_CHANNEL, scope=""enrich_%s"" % level_name)\n\n    last_dim = 2 if cfgs.BINARY_MASK else cfgs.CLASS_NUM + 1\n    mask = slim.conv2d(G, num_outputs=last_dim, kernel_size=[1, 1], stride=1, padding=""SAME"",\n                       activation_fn=None,\n                       scope=\'gmask_%s\' % level_name)\n\n    act_fn = tf.nn.sigmoid if cfgs.SIGMOID_ON_DOT else None\n    dot_layer = slim.conv2d(G, num_outputs=cfgs.FPN_CHANNEL, kernel_size=[1, 1], stride=1, padding=""SAME"",\n                            activation_fn=act_fn,\n                            scope=\'gdot_%s\' % level_name)\n\n    return G, mask, dot_layer\n\n\ndef enrich_semantics_supervised(net, channels, num_layer, scope):\n    with tf.variable_scope(scope):\n        for _ in range(num_layer-1):\n            net = slim.conv2d(net, num_outputs=channels, kernel_size=[3, 3], stride=1, rate=2, padding=""SAME"")\n\n        net = slim.conv2d(net, num_outputs=channels, kernel_size=[3, 3], stride=1, rate=4, padding=""SAME"")\n        net = slim.conv2d(net, num_outputs=channels, kernel_size=[1, 1], stride=1, padding=""SAME"")\n        return net'"
libs/networks/opts.py,14,"b""# -*-coding: utf-8 -*-\n\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\n\n\ndef norm(x, norm_type, is_train, name, G=32, esp=1e-5):\n    with tf.variable_scope('{}_norm_{}'.format(norm_type, name)):\n        if norm_type == 'none':\n            output = x\n        elif norm_type == 'batch':\n            output = tf.contrib.layers.batch_norm(\n                x, center=True, scale=True, decay=0.999,\n                is_training=is_train, updates_collections=None\n            )\n        elif norm_type == 'group':\n            # normalize\n            # tranpose: [bs, h, w, c] to [bs, c, h, w] following the paper\n            x = tf.transpose(x, [0, 3, 1, 2])\n            N, C, H, W = x.get_shape().as_list()\n            G = min(G, C)\n            x = tf.reshape(x, [N, G, C // G, H, W])\n            mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n            x = (x - mean) / tf.sqrt(var + esp)\n            # per channel gamma and beta\n            gamma = tf.get_variable('gamma', [C],\n                                    initializer=tf.constant_initializer(1.0))\n            beta = tf.get_variable('beta', [C],\n                                   initializer=tf.constant_initializer(0.0))\n            gamma = tf.reshape(gamma, [1, C, 1, 1])\n            beta = tf.reshape(beta, [1, C, 1, 1])\n\n            output = tf.reshape(x, [N, C, H, W]) * gamma + beta\n            # tranpose: [bs, c, h, w, c] to [bs, h, w, c] following the paper\n            output = tf.transpose(output, [0, 2, 3, 1])\n        else:\n            raise NotImplementedError\n        return output\n"""
libs/networks/resnet.py,15,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import, print_function, division\n\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom libs.configs import cfgs\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.python.slim.nets.resnet_v1 import resnet_v1_block\n# import tfplot as tfp\n\n\ndef resnet_arg_scope(\n        is_training=True, weight_decay=cfgs.WEIGHT_DECAY, batch_norm_decay=0.997,\n        batch_norm_epsilon=1e-5, batch_norm_scale=True):\n    \'\'\'\n\n    In Default, we do not use BN to train resnet, since batch_size is too small.\n    So is_training is False and trainable is False in the batch_norm params.\n\n    \'\'\'\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n    }\n\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=cfgs.FPN_CHANNEL,\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\n# def add_heatmap(feature_maps, name):\n#     \'\'\'\n#\n#     :param feature_maps:[B, H, W, C]\n#     :return:\n#     \'\'\'\n#\n#     def figure_attention(activation):\n#         fig, ax = tfp.subplots()\n#         im = ax.imshow(activation, cmap=\'jet\')\n#         fig.colorbar(im)\n#         return fig\n#\n#     heatmap = tf.reduce_sum(feature_maps, axis=-1)\n#     heatmap = tf.squeeze(heatmap, axis=0)\n#     tfp.summary.plot(name, figure_attention, [heatmap])\n\n\ndef resnet_base(img_batch, scope_name, is_training=True):\n\n    if scope_name == \'resnet_v1_50\':\n        middle_num_units = 6\n    elif scope_name == \'resnet_v1_101\':\n        middle_num_units = 23\n    else:\n        raise NotImplementedError(\'We only support resnet_v1_50 or resnet_v1_101. \')\n\n    blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n              resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n              resnet_v1_block(\'block3\', base_depth=256, num_units=middle_num_units, stride=2),\n              resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n    # when use fpn . stride list is [1, 2, 2]\n\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):\n        with tf.variable_scope(scope_name, scope_name):\n            # Do the first few layers manually, because \'SAME\' padding can behave inconsistently\n            # for images of different sizes: sometimes 0, sometimes 1\n            net = resnet_utils.conv2d_same(\n                img_batch, 64, 7, stride=2, scope=\'conv1\')\n            net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\n            net = slim.max_pool2d(\n                net, [3, 3], stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    not_freezed = [False] * cfgs.FIXED_BLOCKS + (4-cfgs.FIXED_BLOCKS)*[True]\n    # Fixed_Blocks can be 1~3\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[0]))):\n        C2, end_points_C2 = resnet_v1.resnet_v1(net,\n                                                blocks[0:1],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C2 = tf.Print(C2, [tf.shape(C2)], summarize=10, message=\'C2_shape\')\n    # add_heatmap(C2, name=\'Layer2/C2_heat\')\n\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[1]))):\n        C3, end_points_C3 = resnet_v1.resnet_v1(C2,\n                                                blocks[1:2],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # C3 = tf.Print(C3, [tf.shape(C3)], summarize=10, message=\'C3_shape\')\n    # add_heatmap(C3, name=\'Layer3/C3_heat\')\n    with slim.arg_scope(resnet_arg_scope(is_training=(is_training and not_freezed[2]))):\n        C4, end_points_C4 = resnet_v1.resnet_v1(C3,\n                                                blocks[2:3],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n\n    # add_heatmap(C4, name=\'Layer4/C4_heat\')\n\n    # C4 = tf.Print(C4, [tf.shape(C4)], summarize=10, message=\'C4_shape\')\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        C5, end_points_C5 = resnet_v1.resnet_v1(C4,\n                                                blocks[3:4],\n                                                global_pool=False,\n                                                include_root_block=False,\n                                                scope=scope_name)\n    # C5 = tf.Print(C5, [tf.shape(C5)], summarize=10, message=\'C5_shape\')\n    # add_heatmap(C5, name=\'Layer5/C5_heat\')\n\n    feature_dict = {\'C2\': end_points_C2[\'{}/block1/unit_2/bottleneck_v1\'.format(scope_name)],\n                    \'C3\': end_points_C3[\'{}/block2/unit_3/bottleneck_v1\'.format(scope_name)],\n                    \'C4\': end_points_C4[\'{}/block3/unit_{}/bottleneck_v1\'.format(scope_name, middle_num_units - 1)],\n                    \'C5\': end_points_C5[\'{}/block4/unit_3/bottleneck_v1\'.format(scope_name)],\n                    # \'C5\': end_points_C5[\'{}/block4\'.format(scope_name)],\n                    }\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 2, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 2, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    # for level in range(7, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    return pyramid_dict\n'"
libs/networks/resnet_gluoncv.py,31,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet152_v1b\': [3, 8, 36, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3],\n    \'resnet152_v1d\': [3, 8, 36, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet152_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512],\n    \'resnet152_v1d\': [64, 128, 256, 512]\n}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) + 1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[0]) and is_training),\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True  # do not downsample in C2\n            with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[i-1]) and is_training),\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\n# -----------------------------------\ndef resnet_base(img_batch, scope_name, is_training=True):\n    if scope_name.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b_base\n    elif scope_name.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d_base\n    else:\n        raise ValueError(""scope Name erro...."")\n\n    _, feature_dict = get_resnet_fn(input_x=img_batch, scope=scope_name,\n                                    bottleneck_nums=BottleNeck_NUM_DICT[scope_name],\n                                    base_channels=BASE_CHANNELS_DICT[scope_name],\n                                    is_training=is_training, freeze_norm=True,\n                                    freeze=cfgs.FREEZE_BLOCKS)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 2, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 2, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    # for level in range(7, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    return pyramid_dict\n\n\n'"
libs/networks/resnet_gluoncv_r3det.py,32,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\n\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet152_v1b\': [3, 8, 36, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3],\n    \'resnet152_v1d\': [3, 8, 36, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet152_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512],\n    \'resnet152_v1d\': [64, 128, 256, 512]\n}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) + 1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[0]) and is_training),\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True  # do not downsample in C2\n            with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[i-1]) and is_training),\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\n# -----------------------------------\ndef resnet_base(img_batch, scope_name, is_training=True):\n    if scope_name.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b_base\n    elif scope_name.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d_base\n    else:\n        raise ValueError(""scope Name erro...."")\n\n    _, feature_dict = get_resnet_fn(input_x=img_batch, scope=scope_name,\n                                    bottleneck_nums=BottleNeck_NUM_DICT[scope_name],\n                                    base_channels=BASE_CHANNELS_DICT[scope_name],\n                                    is_training=is_training, freeze_norm=True,\n                                    freeze=cfgs.FREEZE_BLOCKS)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, int(cfgs.LEVEL[0][-1]) - 1, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, int(cfgs.LEVEL[0][-1]) - 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level,\n                                                          activation_fn=tf.nn.relu if cfgs.USE_RELU else None)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVEL)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n\n    return pyramid_dict\n\n\n'"
libs/networks/resnet_gluoncv_r3det_plusplus.py,35,"b'# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\nfrom libs.networks.resnet import fusion_two_layer\n\nDATA_FORMAT = ""NHWC""\nDEBUG = False\ndebug_dict = {}\nBottleNeck_NUM_DICT = {\n    \'resnet50_v1b\': [3, 4, 6, 3],\n    \'resnet101_v1b\': [3, 4, 23, 3],\n    \'resnet152_v1b\': [3, 8, 36, 3],\n    \'resnet50_v1d\': [3, 4, 6, 3],\n    \'resnet101_v1d\': [3, 4, 23, 3],\n    \'resnet152_v1d\': [3, 8, 36, 3]\n}\n\nBASE_CHANNELS_DICT = {\n    \'resnet50_v1b\': [64, 128, 256, 512],\n    \'resnet101_v1b\': [64, 128, 256, 512],\n    \'resnet152_v1b\': [64, 128, 256, 512],\n    \'resnet50_v1d\': [64, 128, 256, 512],\n    \'resnet101_v1d\': [64, 128, 256, 512],\n    \'resnet152_v1d\': [64, 128, 256, 512]\n}\n\n\ndef resnet_arg_scope(freeze_norm, is_training=True, weight_decay=0.0001,\n                     batch_norm_decay=0.9, batch_norm_epsilon=1e-5, batch_norm_scale=True):\n\n    batch_norm_params = {\n        \'is_training\': False, \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon, \'scale\': batch_norm_scale,\n        \'trainable\': False,\n        \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n        \'data_format\': DATA_FORMAT\n    }\n    with slim.arg_scope(\n            [slim.conv2d],\n            weights_regularizer=slim.l2_regularizer(weight_decay),\n            weights_initializer=slim.variance_scaling_initializer(),\n            trainable=is_training,\n            activation_fn=tf.nn.relu,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params=batch_norm_params):\n        with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n            return arg_sc\n\n\ndef stem_7x7(net, scope=""C1""):\n\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [3, 3], [3, 3], [0, 0]])  # pad for data\n        net = slim.conv2d(net, num_outputs=64, kernel_size=[7, 7], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=""conv0"")\n        if DEBUG:\n            debug_dict[\'conv_7x7_bn_relu\'] = tf.transpose(net, [0, 3, 1, 2])  # NHWC --> NCHW\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef stem_stack_3x3(net, input_channel=32, scope=""C1""):\n    with tf.variable_scope(scope):\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=2,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.conv2d(net, num_outputs=input_channel*2, kernel_size=[3, 3], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv2\')\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        net = slim.max_pool2d(net, kernel_size=[3, 3], stride=2, padding=""VALID"", data_format=DATA_FORMAT)\n        return net\n\n\ndef bottleneck_v1b(input_x, base_channel, scope, stride=1, projection=False, avg_down=True):\n    \'\'\'\n    for bottleneck_v1b: reduce spatial dim in conv_3x3 with stride 2.\n    \'\'\'\n    with tf.variable_scope(scope):\n        if DEBUG:\n            debug_dict[input_x.op.name] = tf.transpose(input_x, [0, 3, 1, 2])\n        net = slim.conv2d(input_x, num_outputs=base_channel, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv0\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = tf.pad(net, paddings=[[0, 0], [1, 1], [1, 1], [0, 0]])\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n\n        net = slim.conv2d(net, num_outputs=base_channel, kernel_size=[3, 3], stride=stride,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          scope=\'conv1\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = slim.conv2d(net, num_outputs=base_channel * 4, kernel_size=[1, 1], stride=1,\n                          padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                          activation_fn=None, scope=\'conv2\')\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        # Note that : gamma in the last conv should be init with 0.\n        # But we just reload params from mxnet, so don\'t specific batch norm initializer\n        if projection:\n\n            if avg_down:  # design for resnet_v1d\n                \'\'\'\n                In GluonCV, padding is ""ceil mode"". Here we use ""SAME"" to replace it, which may cause Erros.\n                And the erro will grow with depth of resnet. e.g. res101 erro > res50 erro\n                \'\'\'\n                shortcut = slim.avg_pool2d(input_x, kernel_size=[stride, stride], stride=stride, padding=""SAME"",\n                                           data_format=DATA_FORMAT)\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n                shortcut = slim.conv2d(shortcut, num_outputs=base_channel*4, kernel_size=[1, 1],\n                                       stride=1, padding=""VALID"", biases_initializer=None, data_format=DATA_FORMAT,\n                                       activation_fn=None,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n                # shortcut should have batch norm.\n            else:\n                shortcut = slim.conv2d(input_x, num_outputs=base_channel * 4, kernel_size=[1, 1],\n                                       stride=stride, padding=""VALID"", biases_initializer=None, activation_fn=None,\n                                       data_format=DATA_FORMAT,\n                                       scope=\'shortcut\')\n                if DEBUG:\n                    debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n        else:\n            shortcut = tf.identity(input_x, name=\'shortcut/Identity\')\n            if DEBUG:\n                debug_dict[shortcut.op.name] = tf.transpose(shortcut, [0, 3, 1, 2])\n\n        net = net + shortcut\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        net = tf.nn.relu(net)\n        if DEBUG:\n            debug_dict[net.op.name] = tf.transpose(net, [0, 3, 1, 2])\n        return net\n\n\ndef make_block(net, base_channel, bottleneck_nums, scope, avg_down=True, spatial_downsample=False):\n    with tf.variable_scope(scope):\n        first_stride = 2 if spatial_downsample else 1\n\n        net = bottleneck_v1b(input_x=net, base_channel=base_channel,scope=\'bottleneck_0\',\n                             stride=first_stride, avg_down=avg_down, projection=True)\n        for i in range(1, bottleneck_nums):\n            net = bottleneck_v1b(input_x=net, base_channel=base_channel, scope=""bottleneck_%d"" % i,\n                                 stride=1, avg_down=avg_down, projection=False)\n        return net\n\n\ndef get_resnet_v1_b_base(input_x, freeze_norm, scope=""resnet50_v1b"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) +1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[0]) and is_training,\n                                             freeze_norm=freeze_norm)):\n            net = stem_7x7(net=input_x, scope=""C1"")\n            feature_dict[""C1""] = net\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True\n            with slim.arg_scope(resnet_arg_scope(is_training=(not freeze[i-1]) and is_training,\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=False, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\ndef get_resnet_v1_d_base(input_x, freeze_norm, scope=""resnet50_v1d"", bottleneck_nums=[3, 4, 6, 3], base_channels=[64, 128, 256, 512],\n                    freeze=[True, False, False, False, False], is_training=True):\n\n    assert len(bottleneck_nums) == len(base_channels), ""bottleneck num should same as base_channels size""\n    assert len(freeze) == len(bottleneck_nums) + 1, ""should satisfy:: len(freeze) == len(bottleneck_nums) + 1""\n    feature_dict = {}\n    with tf.variable_scope(scope):\n        with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[0]) and is_training),\n                                             freeze_norm=freeze_norm)):\n            net = stem_stack_3x3(net=input_x, input_channel=32, scope=""C1"")\n            feature_dict[""C1""] = net\n            # print (net)\n        for i in range(2, len(bottleneck_nums)+2):\n            spatial_downsample = False if i == 2 else True  # do not downsample in C2\n            with slim.arg_scope(resnet_arg_scope(is_training=((not freeze[i-1]) and is_training),\n                                                 freeze_norm=freeze_norm)):\n                net = make_block(net=net, base_channel=base_channels[i-2],\n                                 bottleneck_nums=bottleneck_nums[i-2],\n                                 scope=""C%d"" % i,\n                                 avg_down=True, spatial_downsample=spatial_downsample)\n                feature_dict[""C%d"" % i] = net\n\n    return net, feature_dict\n\n\n# -----------------------------------\ndef resnet_base(img_batch, scope_name, is_training=True):\n    if scope_name.endswith(\'b\'):\n        get_resnet_fn = get_resnet_v1_b_base\n    elif scope_name.endswith(\'d\'):\n        get_resnet_fn = get_resnet_v1_d_base\n    else:\n        raise ValueError(""scope Name erro...."")\n\n    _, feature_dict = get_resnet_fn(input_x=img_batch, scope=scope_name,\n                                    bottleneck_nums=BottleNeck_NUM_DICT[scope_name],\n                                    base_channels=BASE_CHANNELS_DICT[scope_name],\n                                    is_training=is_training, freeze_norm=True,\n                                    freeze=cfgs.FREEZE_BLOCKS)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, int(cfgs.LEVEL[0][-1]) - 1, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, int(cfgs.LEVEL[0][-1]) - 1, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level,\n                                                          activation_fn=tf.nn.relu if cfgs.USE_RELU else None)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.FPN_CHANNEL, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    print(""we are in Pyramid::-======>>>>"")\n    print(cfgs.LEVEL)\n    print(""base_anchor_size are: "", cfgs.BASE_ANCHOR_SIZE_LIST)\n    print(20 * ""__"")\n\n    if cfgs.USE_SUPERVISED_MASK:\n        mask_list = []\n        dot_layer_list = []\n        with tf.variable_scope(""enrich_semantics""):\n            with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                                normalizer_fn=None):\n                for i, l_name in enumerate(cfgs.GENERATE_MASK_LIST):\n                    G, mask, dot_layer = generate_mask(net=pyramid_dict[l_name],\n                                                       num_layer=cfgs.ADDITION_LAYERS[i],\n                                                       level_name=l_name)\n                    # add_heatmap(G, name=""MASK/G_%s"" % l_name)\n                    # add_heatmap(mask, name=""MASK/mask_%s"" % l_name)\n\n                    # if cfgs.MASK_ACT_FET:\n                    #     pyramid_dict[l_name] = pyramid_dict[l_name] * dot_layer\n                    dot_layer_list.append(dot_layer)\n                    mask_list.append(mask)\n\n        return pyramid_dict, mask_list, dot_layer_list\n    else:\n        return pyramid_dict\n\n\ndef generate_mask(net, num_layer, level_name):\n    G = enrich_semantics_supervised(net=net,\n                                    num_layer=num_layer,\n                                    channels=cfgs.FPN_CHANNEL, scope=""enrich_%s"" % level_name)\n\n    last_dim = 2 if cfgs.BINARY_MASK else cfgs.CLASS_NUM + 1\n    mask = slim.conv2d(G, num_outputs=last_dim, kernel_size=[1, 1], stride=1, padding=""SAME"",\n                       activation_fn=None,\n                       scope=\'gmask_%s\' % level_name)\n\n    act_fn = tf.nn.sigmoid if cfgs.SIGMOID_ON_DOT else None\n    dot_layer = slim.conv2d(G, num_outputs=cfgs.FPN_CHANNEL, kernel_size=[1, 1], stride=1, padding=""SAME"",\n                            activation_fn=act_fn,\n                            scope=\'gdot_%s\' % level_name)\n\n    return G, mask, dot_layer\n\n\ndef enrich_semantics_supervised(net, channels, num_layer, scope):\n    with tf.variable_scope(scope):\n        for _ in range(num_layer-1):\n            net = slim.conv2d(net, num_outputs=channels, kernel_size=[3, 3], stride=1, rate=2, padding=""SAME"")\n\n        net = slim.conv2d(net, num_outputs=channels, kernel_size=[3, 3], stride=1, rate=4, padding=""SAME"")\n        net = slim.conv2d(net, num_outputs=channels, kernel_size=[1, 1], stride=1, padding=""SAME"")\n        return net\n\n\n'"
libs/networks/xception.bak.py,72,"b'import tensorflow as tf\n\nUSE_FUSED_BN = True\nBN_EPSILON = 0.001\nBN_MOMENTUM = 0.99\n\n\ndef reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n    shape = input_tensor.get_shape().as_list()\n    if shape[1] is None or shape[2] is None:\n        kernel_size_out = kernel_size\n    else:\n        kernel_size_out = [\n          min(shape[1], kernel_size[0]), min(shape[2], kernel_size[1])\n        ]\n    return kernel_size_out\n\n\ndef relu_separable_bn_block(inputs, filters, name_prefix, is_training, data_format):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    inputs = tf.nn.relu(inputs, name=name_prefix + \'_act\')\n\n    inputs = tf.layers.separable_conv2d(inputs, filters, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=name_prefix, reuse=None)\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=name_prefix + \'_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    return inputs\n\n\ndef XceptionModel(input_image, num_classes, is_training = False, data_format=\'channels_last\'):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    # Entry Flow\n    inputs = tf.layers.conv2d(input_image, 32, (3, 3), use_bias=False, name=\'block1_conv1\', strides=(2, 2),\n                              padding=\'valid\', data_format=data_format, activation=None,\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                              bias_initializer=tf.zeros_initializer())\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block1_conv1_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block1_conv1_act\')\n\n    inputs = tf.layers.conv2d(inputs, 64, (3, 3), use_bias=False, name=\'block1_conv2\', strides=(1, 1),\n                              padding=\'valid\', data_format=data_format, activation=None,\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                              bias_initializer=tf.zeros_initializer())\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block1_conv2_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block1_conv2_act\')\n\n    residual = tf.layers.conv2d(inputs, 128, (1, 1), use_bias=False, name=\'conv2d_1\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_1\', axis=bn_axis,\n                                             epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = tf.layers.separable_conv2d(inputs, 128, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=\'block2_sepconv1\', reuse=None)\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block2_sepconv1_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 128, \'block2_sepconv2\', is_training, data_format)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block2_pool\')\n\n    inputs = tf.add(inputs, residual, name=\'residual_add_0\')\n    residual = tf.layers.conv2d(inputs, 256, (1, 1), use_bias=False, name=\'conv2d_2\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_2\', axis=bn_axis,\n                                             epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 256, \'block3_sepconv1\', is_training, data_format)\n    inputs = relu_separable_bn_block(inputs, 256, \'block3_sepconv2\', is_training, data_format)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block3_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_1\')\n\n    residual = tf.layers.conv2d(inputs, 728, (1, 1), use_bias=False, name=\'conv2d_3\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_3\', axis=bn_axis,\n                                             epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 728, \'block4_sepconv1\', is_training, data_format)\n    inputs = relu_separable_bn_block(inputs, 728, \'block4_sepconv2\', is_training, data_format)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block4_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_2\')\n    # Middle Flow\n    for index in range(8):\n        residual = inputs\n        prefix = \'block\' + str(index + 5)\n\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv1\', is_training, data_format)\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv2\', is_training, data_format)\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv3\', is_training, data_format)\n        inputs = tf.add(inputs, residual, name=prefix + \'_residual_add\')\n    # Exit Flow\n    residual = tf.layers.conv2d(inputs, 1024, (1, 1), use_bias=False, name=\'conv2d_4\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_4\', axis=bn_axis,\n                                             epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 728, \'block13_sepconv1\', is_training, data_format)\n    inputs = relu_separable_bn_block(inputs, 1024, \'block13_sepconv2\', is_training, data_format)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block13_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_3\')\n\n    inputs = tf.layers.separable_conv2d(inputs, 1536, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=\'block14_sepconv1\', reuse=None)\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block14_sepconv1_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block14_sepconv1_act\')\n\n    inputs = tf.layers.separable_conv2d(inputs, 2048, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=\'block14_sepconv2\', reuse=None)\n    inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block14_sepconv2_bn\', axis=bn_axis,\n                                           epsilon=BN_EPSILON, training=is_training, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block14_sepconv2_act\')\n\n    if data_format == \'channels_first\':\n        channels_last_inputs = tf.transpose(inputs, [0, 2, 3, 1])\n    else:\n        channels_last_inputs = inputs\n\n    inputs = tf.layers.average_pooling2d(inputs, pool_size = reduced_kernel_size_for_small_input(channels_last_inputs, [10, 10]), strides = 1, padding=\'valid\', data_format=data_format, name=\'avg_pool\')\n\n    if data_format == \'channels_first\':\n        inputs = tf.squeeze(inputs, axis=[2, 3])\n    else:\n        inputs = tf.squeeze(inputs, axis=[1, 2])\n\n    outputs = tf.layers.dense(inputs, num_classes,\n                              activation=tf.nn.softmax, use_bias=True,\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                              bias_initializer=tf.zeros_initializer(),\n                              name=\'dense\', reuse=None)\n\n    return outputs\n\n\nif __name__ == \'__main__\':\n    \'\'\'model test samples\n    \'\'\'\n    import numpy as np\n    # from tensorflow.python.keras._impl.keras.applications.imagenet_utils import decode_predictions  # pylint: disable=unused-import\n    import scipy\n    import tensorflow.contrib.slim as slim\n\n    tf.reset_default_graph()\n\n    input_image = tf.placeholder(tf.float32,  shape = (None, 299, 299, 3), name = \'input_placeholder\')\n    outputs = XceptionModel(input_image, 1000, is_training = True, data_format=\'channels_last\')\n\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        model_variables = tf.trainable_variables()\n        print(model_variables)\n\n        saver.restore(sess, ""/data/RetinaNet_TF/data/pretrained_weights/xception_tf_model/xception_model.ckpt"")\n\n        image_file = [\'test_images/000013.jpg\', \'test_images/000018.jpg\', \'test_images/000031.jpg\', \'test_images/000038.jpg\', \'test_images/000045.jpg\']\n        image_array = []\n        for file in image_file:\n            np_image = scipy.misc.imread(file, mode=\'RGB\')\n            np_image = scipy.misc.imresize(np_image, (299, 299))\n            np_image = np.expand_dims(np_image, axis=0).astype(np.float32)\n            image_array.append(np_image)\n            np_image = np.concatenate(image_array, axis=0)\n            np_image /= 127.5\n            np_image -= 1.\n            #np_image = np.transpose(np_image, (0, 3, 1, 2))\n            predict = sess.run(outputs, feed_dict = {input_image : np_image})\n            #print(predict)\n            print(np.argmax(predict))\n            # print(\'Predicted:\', decode_predictions(predict, 1))\n\n\n'"
libs/networks/xception.py,71,"b'import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.configs import cfgs\n\nUSE_FUSED_BN = True\nBN_EPSILON = 0.001\nBN_MOMENTUM = 0.99\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=256,\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\ndef reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n    shape = input_tensor.get_shape().as_list()\n    if shape[1] is None or shape[2] is None:\n        kernel_size_out = kernel_size\n    else:\n        kernel_size_out = [\n          min(shape[1], kernel_size[0]), min(shape[2], kernel_size[1])\n        ]\n    return kernel_size_out\n\n\ndef relu_separable_bn_block(inputs, filters, name_prefix, is_training, data_format, has_bn=False):\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n\n    inputs = tf.nn.relu(inputs, name=name_prefix + \'_act\')\n    inputs = tf.layers.separable_conv2d(inputs, filters, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=name_prefix, reuse=None)\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=name_prefix + \'_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n    return inputs\n\n\ndef XceptionModel(input_image, num_classes, is_training=False, has_bn=False, data_format=\'channels_last\'):\n    feature_dict = {}\n\n    bn_axis = -1 if data_format == \'channels_last\' else 1\n    # Entry Flow\n    inputs = tf.layers.conv2d(input_image, 32, (3, 3), use_bias=False, name=\'block1_conv1\', strides=(2, 2),\n                              padding=\'valid\', data_format=data_format, activation=None,\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                              bias_initializer=tf.zeros_initializer())\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block1_conv1_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block1_conv1_act\')\n\n    inputs = tf.layers.conv2d(inputs, 64, (3, 3), use_bias=False, name=\'block1_conv2\', strides=(1, 1),\n                              padding=\'valid\', data_format=data_format, activation=None,\n                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                              bias_initializer=tf.zeros_initializer())\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block1_conv2_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n    inputs = tf.nn.relu(inputs, name=\'block1_conv2_act\')\n\n    residual = tf.layers.conv2d(inputs, 128, (1, 1), use_bias=False, name=\'conv2d_1\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    # residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_1\', axis=bn_axis,\n    #                                          epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = tf.layers.separable_conv2d(inputs, 128, (3, 3),\n                                        strides=(1, 1), padding=\'same\',\n                                        data_format=data_format,\n                                        activation=None, use_bias=False,\n                                        depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n                                        bias_initializer=tf.zeros_initializer(),\n                                        name=\'block2_sepconv1\', reuse=None)\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block2_sepconv1_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 128, \'block2_sepconv2\', is_training, data_format, has_bn)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block2_pool\')\n\n    feature_dict[\'C2\'] = inputs\n\n    inputs = tf.add(inputs, residual, name=\'residual_add_0\')\n    residual = tf.layers.conv2d(inputs, 256, (1, 1), use_bias=False, name=\'conv2d_2\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    # residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_2\', axis=bn_axis,\n    #                                          epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 256, \'block3_sepconv1\', is_training, data_format, has_bn)\n    inputs = relu_separable_bn_block(inputs, 256, \'block3_sepconv2\', is_training, data_format, has_bn)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block3_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_1\')\n\n    feature_dict[\'C3\'] = inputs\n\n    residual = tf.layers.conv2d(inputs, 728, (1, 1), use_bias=False, name=\'conv2d_3\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    # residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_3\', axis=bn_axis,\n    #                                          epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 728, \'block4_sepconv1\', is_training, data_format, has_bn)\n    inputs = relu_separable_bn_block(inputs, 728, \'block4_sepconv2\', is_training, data_format, has_bn)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block4_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_2\')\n\n    feature_dict[\'C4\'] = inputs\n\n    # Middle Flow\n    for index in range(8):\n        residual = inputs\n        prefix = \'block\' + str(index + 5)\n\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv1\', is_training, data_format, has_bn)\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv2\', is_training, data_format, has_bn)\n        inputs = relu_separable_bn_block(inputs, 728, prefix + \'_sepconv3\', is_training, data_format, has_bn)\n        inputs = tf.add(inputs, residual, name=prefix + \'_residual_add\')\n    # Exit Flow\n    residual = tf.layers.conv2d(inputs, 1024, (1, 1), use_bias=False, name=\'conv2d_4\', strides=(2, 2),\n                                padding=\'same\', data_format=data_format, activation=None,\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                bias_initializer=tf.zeros_initializer())\n    # residual = tf.layers.batch_normalization(residual, momentum=BN_MOMENTUM, name=\'batch_normalization_4\', axis=bn_axis,\n    #                                          epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n\n    inputs = relu_separable_bn_block(inputs, 728, \'block13_sepconv1\', is_training, data_format, has_bn)\n    inputs = relu_separable_bn_block(inputs, 1024, \'block13_sepconv2\', is_training, data_format, has_bn)\n\n    inputs = tf.layers.max_pooling2d(inputs, pool_size=(3, 3), strides=(2, 2),\n                                     padding=\'same\', data_format=data_format,\n                                     name=\'block13_pool\')\n    inputs = tf.add(inputs, residual, name=\'residual_add_3\')\n\n    feature_dict[\'C5\'] = inputs\n\n    # inputs = tf.layers.separable_conv2d(inputs, 1536, (3, 3),\n    #                                     strides=(1, 1), padding=\'same\',\n    #                                     data_format=data_format,\n    #                                     activation=None, use_bias=False,\n    #                                     depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n    #                                     pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n    #                                     bias_initializer=tf.zeros_initializer(),\n    #                                     name=\'block14_sepconv1\', reuse=None)\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block14_sepconv1_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n    # inputs = tf.nn.relu(inputs, name=\'block14_sepconv1_act\')\n    #\n    # inputs = tf.layers.separable_conv2d(inputs, 2048, (3, 3),\n    #                                     strides=(1, 1), padding=\'same\',\n    #                                     data_format=data_format,\n    #                                     activation=None, use_bias=False,\n    #                                     depthwise_initializer=tf.contrib.layers.xavier_initializer(),\n    #                                     pointwise_initializer=tf.contrib.layers.xavier_initializer(),\n    #                                     bias_initializer=tf.zeros_initializer(),\n    #                                     name=\'block14_sepconv2\', reuse=None)\n    # inputs = tf.layers.batch_normalization(inputs, momentum=BN_MOMENTUM, name=\'block14_sepconv2_bn\', axis=bn_axis,\n    #                                        epsilon=BN_EPSILON, training=has_bn, reuse=None, fused=USE_FUSED_BN)\n    # inputs = tf.nn.relu(inputs, name=\'block14_sepconv2_act\')\n    #\n    # if data_format == \'channels_first\':\n    #     channels_last_inputs = tf.transpose(inputs, [0, 2, 3, 1])\n    # else:\n    #     channels_last_inputs = inputs\n    #\n    # inputs = tf.layers.average_pooling2d(inputs, pool_size=reduced_kernel_size_for_small_input(channels_last_inputs, [10, 10]), strides = 1, padding=\'valid\', data_format=data_format, name=\'avg_pool\')\n    #\n    # if data_format == \'channels_first\':\n    #     inputs = tf.squeeze(inputs, axis=[2, 3])\n    # else:\n    #     inputs = tf.squeeze(inputs, axis=[1, 2])\n    #\n    # outputs = tf.layers.dense(inputs, num_classes,\n    #                           activation=tf.nn.softmax, use_bias=True,\n    #                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n    #                           bias_initializer=tf.zeros_initializer(),\n    #                           name=\'dense\', reuse=None)\n\n    return feature_dict\n\n\ndef xception_base(input_image, is_training=True):\n    feature_dict = XceptionModel(input_image, 1000, is_training=is_training, has_bn=False, data_format=\'channels_last\')\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'C5\'],\n                             num_outputs=256,\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 2, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""C%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 2, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'C5\'],\n                             num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=256, kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    # for level in range(7, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    return pyramid_dict\n\n'"
libs/val_libs/__init__.py,0,b''
libs/val_libs/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\nfrom help_utils.tools import mkdir\n\ndef write_voc_results_file(all_boxes, test_imgid_list, det_save_dir):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n  the detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n\n  :param test_imgid_list:\n  :param det_save_path:\n  :return:\n  \'\'\'\n  for cls, cls_id in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(""Writing {} VOC resutls file"".format(cls))\n\n    mkdir(det_save_dir)\n    det_save_path = os.path.join(det_save_dir, ""det_""+cls+"".txt"")\n    with open(det_save_path, \'wt\') as f:\n      for index, img_name in enumerate(test_imgid_list):\n        this_img_detections = all_boxes[index]\n\n        this_cls_detections = this_img_detections[this_img_detections[:, 0]==cls_id]\n        if this_cls_detections.shape[0] == 0:\n          continue # this cls has none detections in this img\n        for a_det in this_cls_detections:\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(img_name, a_det[1],\n                         a_det[2], a_det[3],\n                         a_det[4], a_det[5]))  # that is [img_name, score, xmin, ymin, xmax, ymax]\n\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath, annopath, test_imgid_list, cls_name, ovthresh=0.5,\n                 use_07_metric=False, use_diff=False):\n  \'\'\'\n\n  :param detpath:\n  :param annopath:\n  :param test_imgid_list: it \'s a list that contains the img_name of test_imgs\n  :param cls_name:\n  :param ovthresh:\n  :param use_07_metric:\n  :param use_diff:\n  :return:\n  \'\'\'\n  # 1. parse xml to get gtboxes\n\n  # read list of images\n  imagenames = test_imgid_list\n\n  recs = {}\n  for i, imagename in enumerate(imagenames):\n    recs[imagename] = parse_rec(os.path.join(annopath, imagename+\'.xml\'))\n    # if i % 100 == 0:\n    #   print(\'Reading annotation for {:d}/{:d}\'.format(\n    #     i + 1, len(imagenames)))\n\n  # 2. get gtboxes for this class.\n  class_recs = {}\n  num_pos = 0\n  # if cls_name == \'person\':\n  #   print (""aaa"")\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == cls_name]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    num_pos = num_pos + sum(~difficult)  # ignored the diffcult boxes\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det} # det means that gtboxes has already been detected\n\n  # 3. read the detection file\n  detfile = os.path.join(detpath, ""det_""+cls_name+"".txt"")\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  # for a line. that is [img_name, confidence, xmin, ymin, xmax, ymax]\n  splitlines = [x.strip().split(\' \') for x in lines]  # a list that include a list\n  image_ids = [x[0] for x in splitlines]  # img_id is img_name\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids) # num of detections. That, a line is a det_box.\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]  #reorder the img_name\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]  # img_id is img_name\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # 4. get recall, precison and AP\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(num_pos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n\n\ndef do_python_eval(test_imgid_list, test_annotation_path):\n  AP_list = []\n  # import matplotlib.pyplot as plt\n  # import matplotlib.colors as colors\n  # color_list = colors.cnames.keys()[::6]\n\n  for cls, index in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    recall, precision, AP = voc_eval(detpath=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION),\n                                     test_imgid_list=test_imgid_list,\n                                     cls_name=cls,\n                                     annopath=test_annotation_path,\n                                     use_07_metric=cfgs.USE_07_METRIC)\n    AP_list += [AP]\n    print(""cls : {}|| Recall: {} || Precison: {}|| AP: {}"".format(cls, recall[-1], precision[-1], AP))\n    # plt.plot(recall, precision, label=cls, color=color_list[index])\n    # plt.legend(loc=\'upper right\')\n    # print(10*""__"")\n  # plt.show()\n  # plt.savefig(cfgs.VERSION+\'.jpg\')\n  print(""mAP is : {}"".format(np.mean(AP_list)))\n\n\ndef voc_evaluate_detections(all_boxes, test_annotation_path, test_imgid_list):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n\n  The detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n  :return:\n  \'\'\'\n  test_imgid_list = [item.split(\'.\')[0] for item in test_imgid_list]\n\n  write_voc_results_file(all_boxes, test_imgid_list=test_imgid_list,\n                         det_save_dir=os.path.join(cfgs.EVALUATE_DIR, cfgs.VERSION))\n  do_python_eval(test_imgid_list, test_annotation_path=test_annotation_path)\n\n\n\n\n\n\n\n'"
libs/val_libs/voc_eval_r.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# import matplotlib.colors as colors\n# import matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\nfrom libs.label_name_dict.label_dict import NAME_LABEL_MAP\nfrom libs.configs import cfgs\nfrom libs.box_utils import iou_rotate\nfrom libs.box_utils import coordinate_convert\nfrom help_utils import tools\n\n\ndef _write_voc_results_file(all_boxes, test_imgid_list, det_save_path):\n  for cls, cls_ind in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(\'Writing {} VOC results file\'.format(cls))\n\n    with open(det_save_path, \'wt\') as f:\n      for im_ind, index in enumerate(test_imgid_list):\n        dets = all_boxes[cls_ind][im_ind]\n        if dets == []:\n          continue\n        # the VOCdevkit expects 1-based indices\n        for k in range(dets.shape[0]):\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(index, dets[k, -1],\n                         dets[k, 0] + 1, dets[k, 1] + 1,\n                         dets[k, 2] + 1, dets[k, 3] + 1, dets[k, 4] + 1))\n\n\ndef write_voc_results_file(all_boxes, test_imgid_list, det_save_dir):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n  the detections is a array. shape is [-1, 7]. [category, score, x, y, w, h, theta]\n  Note that: if none detections in this img. that the detetions is : []\n\n  :param test_imgid_list:\n  :param det_save_path:\n  :return:\n  \'\'\'\n  for cls, cls_id in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    print(""Writing {} VOC resutls file"".format(cls))\n\n    tools.mkdir(det_save_dir)\n    det_save_path = os.path.join(det_save_dir, ""det_""+cls+"".txt"")\n    with open(det_save_path, \'wt\') as f:\n      for index, img_name in enumerate(test_imgid_list):\n        this_img_detections = all_boxes[index]\n\n        this_cls_detections = this_img_detections[this_img_detections[:, 0] == cls_id]\n        if this_cls_detections.shape[0] == 0:\n          continue # this cls has none detections in this img\n        for a_det in this_cls_detections:\n          f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                  format(img_name, a_det[1],\n                         a_det[2], a_det[3],\n                         a_det[4], a_det[5], a_det[6]))  # that is [img_name, score, x, y, w, h, theta]\n\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    rbox = [int(bbox.find(\'x1\').text), int(bbox.find(\'y1\').text),\n            int(bbox.find(\'x2\').text), int(bbox.find(\'y2\').text),\n            int(bbox.find(\'x3\').text), int(bbox.find(\'y3\').text),\n            int(bbox.find(\'x4\').text), int(bbox.find(\'y4\').text)]\n    rbox = np.array([rbox], np.float32)\n    rbox = coordinate_convert.backward_convert(rbox, with_label=False)\n    obj_struct[\'bbox\'] = rbox\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath, annopath, test_imgid_list, cls_name, ovthresh=0.5,\n             use_07_metric=False, use_diff=False):\n  \'\'\'\n\n  :param detpath:\n  :param annopath:\n  :param test_imgid_list: it \'s a list that contains the img_name of test_imgs\n  :param cls_name:\n  :param ovthresh:\n  :param use_07_metric:\n  :param use_diff:\n  :return:\n  \'\'\'\n  # 1. parse xml to get gtboxes\n\n  # read list of images\n  imagenames = test_imgid_list\n\n  recs = {}\n  for i, imagename in enumerate(imagenames):\n    recs[imagename] = parse_rec(os.path.join(annopath, imagename+\'.xml\'))\n    # if i % 100 == 0:\n    #   print(\'Reading annotation for {:d}/{:d}\'.format(\n    #     i + 1, len(imagenames)))\n\n  # 2. get gtboxes for this class.\n  class_recs = {}\n  num_pos = 0\n  # if cls_name == \'person\':\n  #   print (""aaa"")\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == cls_name]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    num_pos = num_pos + sum(~difficult)  # ignored the diffcult boxes\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det} # det means that gtboxes has already been detected\n\n  # 3. read the detection file\n  detfile = os.path.join(detpath, ""det_""+cls_name+"".txt"")\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  # for a line. that is [img_name, confidence, xmin, ymin, xmax, ymax]\n  splitlines = [x.strip().split(\' \') for x in lines]  # a list that include a list\n  image_ids = [x[0] for x in splitlines]  # img_id is img_name\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids) # num of detections. That, a line is a det_box.\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]  #reorder the img_name\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]  # img_id is img_name\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        # ixmin = np.maximum(BBGT[:, 0], bb[0])\n        # iymin = np.maximum(BBGT[:, 1], bb[1])\n        # ixmax = np.minimum(BBGT[:, 2], bb[2])\n        # iymax = np.minimum(BBGT[:, 3], bb[3])\n        # iw = np.maximum(ixmax - ixmin + 1., 0.)\n        # ih = np.maximum(iymax - iymin + 1., 0.)\n        # inters = iw * ih\n        #\n        # # union\n        # uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n        #        (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n        #        (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n        #\n        # overlaps = inters / uni\n        overlaps = []\n        for i in range(len(BBGT)):\n          overlap = iou_rotate.iou_rotate_calculate1(np.array([bb]),\n                                                      BBGT[i],\n                                                      use_gpu=False)[0]\n          overlaps.append(overlap)\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # 4. get recall, precison and AP\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(num_pos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n\n\ndef do_python_eval(test_imgid_list, test_annotation_path):\n  # import matplotlib.colors as colors\n  # import matplotlib.pyplot as plt\n\n  AP_list = []\n  for cls, index in NAME_LABEL_MAP.items():\n    if cls == \'back_ground\':\n      continue\n    recall, precision, AP = voc_eval(detpath=cfgs.EVALUATE_R_DIR,\n                                     test_imgid_list=test_imgid_list,\n                                     cls_name=cls,\n                                     annopath=test_annotation_path,\n                                     use_07_metric=cfgs.USE_07_METRIC,\n                                     ovthresh=cfgs.EVAL_THRESHOLD)\n    AP_list += [AP]\n    print(""cls : {}|| Recall: {} || Precison: {}|| AP: {}"".format(cls, recall[-1], precision[-1], AP))\n    # print(""{}_ap: {}"".format(cls, AP))\n    # print(""{}_recall: {}"".format(cls, recall[-1]))\n    # print(""{}_precision: {}"".format(cls, precision[-1]))\n    r = np.array(recall)\n    p = np.array(precision)\n    F1 = 2 * r * p / (r + p)\n    max_ind = np.argmax(F1)\n    print(\'F1:{} P:{} R:{}\'.format(F1[max_ind], p[max_ind], r[max_ind]))\n\n    # c = colors.cnames.keys()\n    # c_dark = list(filter(lambda x: x.startswith(\'dark\'), c))\n    # c = [\'red\', \'orange\']\n    # plt.axis([0, 1.2, 0, 1])\n    # plt.plot(recall, precision, color=c_dark[index], label=cls)\n\n  # plt.legend(loc=\'upper right\')\n  # plt.xlabel(\'R\')\n  # plt.ylabel(\'P\')\n  # plt.savefig(\'./PR_R.png\')\n\n  print(""mAP is : {}"".format(np.mean(AP_list)))\n\n\ndef voc_evaluate_detections(all_boxes, test_imgid_list, test_annotation_path):\n  \'\'\'\n\n  :param all_boxes: is a list. each item reprensent the detections of a img.\n\n  The detections is a array. shape is [-1, 6]. [category, score, xmin, ymin, xmax, ymax]\n  Note that: if none detections in this img. that the detetions is : []\n  :return:\n  \'\'\'\n\n  write_voc_results_file(all_boxes, test_imgid_list=test_imgid_list,\n                         det_save_dir=cfgs.EVALUATE_R_DIR)\n  do_python_eval(test_imgid_list, test_annotation_path)\n\n'"
data/io/DOTA/data_crop.py,0,"b'import os\nimport scipy.misc as misc\nfrom xml.dom.minidom import Document\nimport numpy as np\nimport copy\nimport cv2\nimport sys\nsys.path.append(\'../../..\')\n\nfrom help_utils.tools import mkdir\n\n\ndef save_to_xml(save_path, im_height, im_width, objects_axis, label_name):\n    im_depth = 0\n    object_num = len(objects_axis)\n    doc = Document()\n\n    annotation = doc.createElement(\'annotation\')\n    doc.appendChild(annotation)\n\n    folder = doc.createElement(\'folder\')\n    folder_name = doc.createTextNode(\'VOC2007\')\n    folder.appendChild(folder_name)\n    annotation.appendChild(folder)\n\n    filename = doc.createElement(\'filename\')\n    filename_name = doc.createTextNode(\'000024.jpg\')\n    filename.appendChild(filename_name)\n    annotation.appendChild(filename)\n\n    source = doc.createElement(\'source\')\n    annotation.appendChild(source)\n\n    database = doc.createElement(\'database\')\n    database.appendChild(doc.createTextNode(\'The VOC2007 Database\'))\n    source.appendChild(database)\n\n    annotation_s = doc.createElement(\'annotation\')\n    annotation_s.appendChild(doc.createTextNode(\'PASCAL VOC2007\'))\n    source.appendChild(annotation_s)\n\n    image = doc.createElement(\'image\')\n    image.appendChild(doc.createTextNode(\'flickr\'))\n    source.appendChild(image)\n\n    flickrid = doc.createElement(\'flickrid\')\n    flickrid.appendChild(doc.createTextNode(\'322409915\'))\n    source.appendChild(flickrid)\n\n    owner = doc.createElement(\'owner\')\n    annotation.appendChild(owner)\n\n    flickrid_o = doc.createElement(\'flickrid\')\n    flickrid_o.appendChild(doc.createTextNode(\'knautia\'))\n    owner.appendChild(flickrid_o)\n\n    name_o = doc.createElement(\'name\')\n    name_o.appendChild(doc.createTextNode(\'yang\'))\n    owner.appendChild(name_o)\n\n    size = doc.createElement(\'size\')\n    annotation.appendChild(size)\n    width = doc.createElement(\'width\')\n    width.appendChild(doc.createTextNode(str(im_width)))\n    height = doc.createElement(\'height\')\n    height.appendChild(doc.createTextNode(str(im_height)))\n    depth = doc.createElement(\'depth\')\n    depth.appendChild(doc.createTextNode(str(im_depth)))\n    size.appendChild(width)\n    size.appendChild(height)\n    size.appendChild(depth)\n    segmented = doc.createElement(\'segmented\')\n    segmented.appendChild(doc.createTextNode(\'0\'))\n    annotation.appendChild(segmented)\n    for i in range(object_num):\n        objects = doc.createElement(\'object\')\n        annotation.appendChild(objects)\n        object_name = doc.createElement(\'name\')\n        object_name.appendChild(doc.createTextNode(label_name[int(objects_axis[i][-1])]))\n        objects.appendChild(object_name)\n        pose = doc.createElement(\'pose\')\n        pose.appendChild(doc.createTextNode(\'Unspecified\'))\n        objects.appendChild(pose)\n        truncated = doc.createElement(\'truncated\')\n        truncated.appendChild(doc.createTextNode(\'1\'))\n        objects.appendChild(truncated)\n        difficult = doc.createElement(\'difficult\')\n        difficult.appendChild(doc.createTextNode(\'0\'))\n        objects.appendChild(difficult)\n        bndbox = doc.createElement(\'bndbox\')\n        objects.appendChild(bndbox)\n\n        x0 = doc.createElement(\'x0\')\n        x0.appendChild(doc.createTextNode(str((objects_axis[i][0]))))\n        bndbox.appendChild(x0)\n        y0 = doc.createElement(\'y0\')\n        y0.appendChild(doc.createTextNode(str((objects_axis[i][1]))))\n        bndbox.appendChild(y0)\n\n        x1 = doc.createElement(\'x1\')\n        x1.appendChild(doc.createTextNode(str((objects_axis[i][2]))))\n        bndbox.appendChild(x1)\n        y1 = doc.createElement(\'y1\')\n        y1.appendChild(doc.createTextNode(str((objects_axis[i][3]))))\n        bndbox.appendChild(y1)\n\n        x2 = doc.createElement(\'x2\')\n        x2.appendChild(doc.createTextNode(str((objects_axis[i][4]))))\n        bndbox.appendChild(x2)\n        y2 = doc.createElement(\'y2\')\n        y2.appendChild(doc.createTextNode(str((objects_axis[i][5]))))\n        bndbox.appendChild(y2)\n\n        x3 = doc.createElement(\'x3\')\n        x3.appendChild(doc.createTextNode(str((objects_axis[i][6]))))\n        bndbox.appendChild(x3)\n        y3 = doc.createElement(\'y3\')\n        y3.appendChild(doc.createTextNode(str((objects_axis[i][7]))))\n        bndbox.appendChild(y3)\n\n    f = open(save_path, \'w\')\n    f.write(doc.toprettyxml(indent=\'\'))\n    f.close()\n\n\nclass_list = [\'plane\', \'baseball-diamond\', \'bridge\', \'ground-track-field\',\n              \'small-vehicle\', \'large-vehicle\', \'ship\',\n              \'tennis-court\', \'basketball-court\',\n              \'storage-tank\', \'soccer-ball-field\',\n              \'roundabout\', \'harbor\',\n              \'swimming-pool\', \'helicopter\', \'container-crane\']\n\n\ndef format_label(txt_list):\n    format_data = []\n    for i in txt_list:\n        if len(i.split(\' \')) < 9:\n            continue\n        format_data.append(\n            [float(xy) for xy in i.split(\' \')[:8]] + [class_list.index(i.split(\' \')[8])]\n        )\n\n        if i.split(\' \')[8] not in class_list:\n            print(\'warning found a new label :\', i.split(\' \')[8])\n            exit()\n    return np.array(format_data)\n\n\ndef clip_image(file_idx, image, boxes_all, width, height, stride_w, stride_h):\n    if len(boxes_all) > 0:\n        shape = image.shape\n        for start_h in range(0, shape[0], stride_h):\n            for start_w in range(0, shape[1], stride_w):\n                boxes = copy.deepcopy(boxes_all)\n                box = np.zeros_like(boxes_all)\n                start_h_new = start_h\n                start_w_new = start_w\n                if start_h + height > shape[0]:\n                    start_h_new = shape[0] - height\n                if start_w + width > shape[1]:\n                    start_w_new = shape[1] - width\n                top_left_row = max(start_h_new, 0)\n                top_left_col = max(start_w_new, 0)\n                bottom_right_row = min(start_h + height, shape[0])\n                bottom_right_col = min(start_w + width, shape[1])\n\n                subImage = image[top_left_row:bottom_right_row, top_left_col: bottom_right_col]\n\n                box[:, 0] = boxes[:, 0] - top_left_col\n                box[:, 2] = boxes[:, 2] - top_left_col\n                box[:, 4] = boxes[:, 4] - top_left_col\n                box[:, 6] = boxes[:, 6] - top_left_col\n\n                box[:, 1] = boxes[:, 1] - top_left_row\n                box[:, 3] = boxes[:, 3] - top_left_row\n                box[:, 5] = boxes[:, 5] - top_left_row\n                box[:, 7] = boxes[:, 7] - top_left_row\n                box[:, 8] = boxes[:, 8]\n                center_y = 0.25 * (box[:, 1] + box[:, 3] + box[:, 5] + box[:, 7])\n                center_x = 0.25 * (box[:, 0] + box[:, 2] + box[:, 4] + box[:, 6])\n\n                cond1 = np.intersect1d(np.where(center_y[:] >= 0)[0], np.where(center_x[:] >= 0)[0])\n                cond2 = np.intersect1d(np.where(center_y[:] <= (bottom_right_row - top_left_row))[0],\n                                       np.where(center_x[:] <= (bottom_right_col - top_left_col))[0])\n                idx = np.intersect1d(cond1, cond2)\n                if len(idx) > 0 and (subImage.shape[0] > 5 and subImage.shape[1] > 5):\n                    mkdir(os.path.join(save_dir, \'images\'))\n                    img = os.path.join(save_dir, \'images\',\n                                       ""%s_%04d_%04d.png"" % (file_idx, top_left_row, top_left_col))\n                    cv2.imwrite(img, subImage)\n\n                    mkdir(os.path.join(save_dir, \'labeltxt\'))\n                    xml = os.path.join(save_dir, \'labeltxt\',\n                                       ""%s_%04d_%04d.xml"" % (file_idx, top_left_row, top_left_col))\n                    save_to_xml(xml, subImage.shape[0], subImage.shape[1], box[idx, :], class_list)\n\n\nprint(\'class_list\', len(class_list))\nraw_data = \'/data/dataset/DOTA/train/\'\nraw_images_dir = os.path.join(raw_data, \'images\', \'images\')\nraw_label_dir = os.path.join(raw_data, \'labelTxt\', \'labelTxt\')\n\nsave_dir = \'/data/dataset/DOTA/DOTA1.0/trainval/\'\n\nimages = [i for i in os.listdir(raw_images_dir) if \'png\' in i]\nlabels = [i for i in os.listdir(raw_label_dir) if \'txt\' in i]\n\nprint(\'find image\', len(images))\nprint(\'find label\', len(labels))\n\nmin_length = 1e10\nmax_length = 1\n\nimg_h, img_w, stride_h, stride_w = 600, 600, 450, 450\n\nfor idx, img in enumerate(images):\n    print(idx, \'read image\', img)\n    img_data = cv2.imread(os.path.join(raw_images_dir, img))\n\n    txt_data = open(os.path.join(raw_label_dir, img.replace(\'png\', \'txt\')), \'r\').readlines()\n    box = format_label(txt_data)\n    clip_image(img.strip(\'.png\'), img_data, box, img_w, img_h, stride_w, stride_h)\n'"
data/io/FDDB/fddb.py,0,"b'import math, os\nimport shutil\nfrom data.io.FDDB.txt2xml import WriterXMLFiles\nimport numpy as np\nimport cv2\nimport random\nfrom PIL import Image\nimport glob\nimport sys\nrandom.seed(2018)\n\ndef convert_coord(coord):\n    """"""\n    :param coord:[major_axis_radius, minor_axis_radius, angle, center_x, center_y, detection_score]\n    :return:\n    """"""\n    x_c = coord[3]\n    y_c = coord[4]\n    w = coord[0]*2\n    h = coord[1]*2\n    theta = -(coord[2] / math.pi * 180)\n    up_l = (-w/2, h/2)\n    down_l = (-w/2, -h/2)\n    up_r = (w/2, h/2)\n    down_r = (w/2, -h/2)\n\n    theta = -theta\n\n    x1 = math.cos(theta/180*math.pi) * up_l[0] - math.sin(theta/180*math.pi) * up_l[1] + x_c\n    y1 = math.sin(theta/180*math.pi) * up_l[0] + math.cos(theta/180*math.pi) * up_l[1] + y_c\n\n    x2 = math.cos(theta / 180 * math.pi) * down_l[0] - math.sin(theta / 180 * math.pi) * down_l[1] + x_c\n    y2 = math.sin(theta / 180 * math.pi) * down_l[0] + math.cos(theta / 180 * math.pi) * down_l[1] + y_c\n\n    x3 = math.cos(theta / 180 * math.pi) * up_r[0] - math.sin(theta / 180 * math.pi) * up_r[1] + x_c\n    y3 = math.sin(theta / 180 * math.pi) * up_r[0] + math.cos(theta / 180 * math.pi) * up_r[1] + y_c\n\n    x4 = math.cos(theta / 180 * math.pi) * down_r[0] - math.sin(theta / 180 * math.pi) * down_r[1] + x_c\n    y4 = math.sin(theta / 180 * math.pi) * down_r[0] + math.cos(theta / 180 * math.pi) * down_r[1] + y_c\n\n    return [x1, y1, x2, y2, x3, y3, x4, y4]\ndef getFiles(file_dir):\n    return [file for file in os.listdir(file_dir) if os.path.isfile(os.path.join(file_dir,file))]\ndef view_bar(num,total):\n    """"""\n    \xe8\xbf\x9b\xe5\xba\xa6\xe6\x9d\xa1\n    :param num:\n    :param total:\n    :return:\n    """"""\n    ret = num / total\n    ag = int(ret * 50)\n    ab = ""\\r[%-50s]%3d%%%6d/%-6d"" % (\'=\' * ag, 100*ret, num, total)\n    sys.stdout.write(ab)\n    sys.stdout.flush()\ndef generateImageAndXml(image_dir,txt_dir,image_save_dir, xml_save_dir):\n    """"""\n    :param image_dir: \xe5\x9b\xbe\xe7\x89\x87\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\n    :param txt_dir: text\xe6\x96\x87\xe4\xbb\xb6\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\n    :return:\n    """"""\n    files = getFiles(txt_dir)  # \xe6\x89\x80\xe6\x9c\x89txt\n    files = [file for file in files if file.endswith(\'ellipseList.txt\')]  # \xe8\xbf\x87\xe6\xbb\xa4\xe5\x87\xbaannotation\xe6\x96\x87\xe4\xbb\xb6\n    for file in files:\n        print(file)\n        txt_path = os.path.join(txt_dir,file)\n        with open(txt_path, \'r\') as f:\n            line = \'init\'\n            while True:\n                line = f.readline()\n                if line == \'\':\n                    break\n                image_dir_tmp = line.strip()\n                # print("">>>{} is being processed..."".format(image_dir_tmp))\n                new_name = \'_\'.join(image_dir_tmp.split(\'/\'))\n                image_path = os.path.join(image_dir, image_dir_tmp+\'.jpg\')\n                img_shape = cv2.imread(image_path).shape\n                h = img_shape[0]\n                w = img_shape[1]\n                d = img_shape[2]\n                shutil.copyfile(image_path,os.path.join(image_save_dir,new_name+ \'.jpg\'))\n                line = f.readline().strip()\n                num = int(line)\n                box_list = []\n                for i in range(num):\n                    line = f.readline().strip()\n                    data = [float(val) for val in line.split(\' \') if val != \'\']\n                    coord = convert_coord(data)\n                    box_list.append(coord)\n                WriterXMLFiles(new_name+\'.xml\',xml_save_dir,box_list,w,h,d)\ndef rotateBox(box_list,rotate_matrix,h,w):\n    trans_box_list = []\n    for bbx in box_list:\n        bbx = [[bbx[0]-w//2,bbx[2]-w//2,bbx[4]-w//2,bbx[6]-w//2],\n               [bbx[1]-h//2,bbx[3]-h//2,bbx[5]-h//2,bbx[7]-h//2]]\n        trans_box_list.append(bbx)\n    if len(trans_box_list) == 0:\n        return []\n    else:\n        res_box_list = []\n        for bbx in trans_box_list:\n            bbx = np.matmul(rotate_matrix,np.array(bbx))\n            bbx = bbx + np.array([\n                [w//2,w//2,w//2,w//2],\n                [h//2,h//2,h//2,h//2]\n            ])\n            x_mean = np.mean(bbx[0])\n            y_mean = np.mean(bbx[1])\n            if 0 < x_mean < w and 0 < y_mean < h:\n                bbx = [bbx[0,0],bbx[1,0],bbx[0,1],bbx[1,1],bbx[0,2],bbx[1,2],bbx[0,3],bbx[1,3]]\n                res_box_list.append(bbx)\n        return res_box_list\n\ndef aug_data(image_dir,txt_dir,image_save_dir, xml_save_dir,n):\n    """"""\n    :param image_dir:\n    :param txt_dir:\n    :param image_save_dir:\n    :param xml_save_dir:\n    :param n:\xe5\xa2\x9e\xe5\xbc\xba\xe6\xac\xa1\xe6\x95\xb0\n    :return:\n    """"""\n    files = getFiles(txt_dir)  # \xe6\x89\x80\xe6\x9c\x89txt\n    files = [file for file in files if file.endswith(\'ellipseList.txt\')]  # \xe8\xbf\x87\xe6\xbb\xa4\xe5\x87\xbaannotation\xe6\x96\x87\xe4\xbb\xb6\n    for file in files:\n        print(file)\n        txt_path = os.path.join(txt_dir, file)\n        with open(txt_path, \'r\') as f:\n            while True:\n                line = f.readline()\n                if line == \'\':\n                    break\n                image_dir_tmp = line.strip()\n                print("">>>{} is being processed..."".format(image_dir_tmp))\n                new_name = \'_\'.join(image_dir_tmp.split(\'/\'))\n                image_path = os.path.join(image_dir, image_dir_tmp + \'.jpg\')\n                im = Image.open(image_path)\n                (w, h) = im.size\n                d = 3\n                center = (w//2,h//2)\n                line = f.readline().strip()\n                num = int(line)\n                box_list = []\n                for i in range(num):\n                    line = f.readline().strip()\n                    data = [float(val) for val in line.split(\' \') if val != \'\']\n                    coord = convert_coord(data)\n                    box_list.append(coord)\n                ii = 0\n                while ii < n:\n                    angle = random.randint(1,359)\n                    rotate_matrix = np.array([\n                        [np.cos(angle * np.pi / 180), np.sin(angle * np.pi / 180)],\n                        [-np.sin(angle * np.pi / 180), np.cos(angle * np.pi / 180)]\n                    ])\n                    box_list_new = rotateBox(box_list,rotate_matrix,h,w)\n                    if len(box_list_new) == 0:\n                        continue\n                    ii += 1\n                    new_im = im.rotate(angle, center=center)\n                    new_im.save(os.path.join(image_save_dir,\'{}_{}.jpg\'.format(new_name,angle)))\n                    WriterXMLFiles(\'{}_{}.xml\'.format(new_name,angle), xml_save_dir, box_list_new, w, h, d)\nif __name__ == \'__main__\':\n    image_dir = \'/data/yangxue/dataset/FDDB/originalPics\'\n    txt_dir = \'/data/yangxue/dataset/FDDB/FDDB-folds\'\n    image_save_dir = \'/data/yangxue/dataset/FDDB/images\'\n    xml_save_dir = \'/data/yangxue/dataset/FDDB/xml\'\n    generateImageAndXml(image_dir, txt_dir, image_save_dir, xml_save_dir)\n    # image_save_dir = \'/data/yangxue/dataset/FDDB/images_aug\'\n    # xml_save_dir = \'/data/yangxue/dataset/FDDB/xml_aug\'\n    # aug_data(image_dir, txt_dir, image_save_dir, xml_save_dir, 10)'"
data/io/FDDB/txt2xml.py,0,"b'import os\nfrom xml.dom.minidom import Document\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\nimport numpy as np\nimport csv\nimport cv2\n\n\ndef WriterXMLFiles(filename, path, box_list, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for box in box_list:\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(\'face\'))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(box[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(box[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(box[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(box[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(os.path.join(path,filename), \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\ndef load_annoataion(p):\n    \'\'\'\n    load annotation from the text file\n    :param p:\n    :return:\n    \'\'\'\n    text_polys = []\n    text_tags = []\n    if not os.path.exists(p):\n        return np.array(text_polys, dtype=np.float32)\n    with open(p, \'r\') as f:\n        reader = csv.reader(f)\n        for line in reader:\n            label = \'text\'\n            # strip BOM. \\ufeff for python3,  \\xef\\xbb\\bf for python2\n            line = [i.strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\') for i in line]\n\n            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n            text_polys.append([x1, y1, x2, y2, x3, y3, x4, y4])\n            text_tags.append(label)\n\n        return np.array(text_polys, dtype=np.int32), np.array(text_tags, dtype=np.str)\n\nif __name__ == ""__main__"":\n    txt_path = r\'C:\\Users\\yangxue\\Documents\\GitHub\\icdar2015\\ch4_training_localization_transcription_gt\'\n    xml_path = r\'C:\\Users\\yangxue\\Documents\\GitHub\\icdar2015\\ch4_xml\'\n    img_path = r\'C:\\Users\\yangxue\\Documents\\GitHub\\icdar2015\\ch4_training_images\'\n    print(os.path.exists(txt_path))\n    txts = os.listdir(txt_path)\n    for count, t in enumerate(txts):\n        boxes, labels = load_annoataion(os.path.join(txt_path, t))\n        xml_name = t.replace(\'.txt\', \'.xml\')\n        img_name = t.replace(\'.txt\', \'.jpg\')\n        img = cv2.imread(os.path.join(img_path, img_name.split(\'gt_\')[-1]))\n        if img != None:\n            h, w, d = img.shape\n            WriterXMLFiles(xml_name.split(\'gt_\')[-1], xml_path, boxes, labels, w, h, d)\n\n        if count % 1000 == 0:\n            print(count)'"
data/io/HRSC2016/__init__.py,0,b''
data/io/HRSC2016/make_test_xml.py,0,"b'import xml.etree.cElementTree as ET\nfrom xml.dom.minidom import Document\nimport xml.dom.minidom\nimport numpy as np\nimport os\nimport math\nimport sys\nsys.path.append(\'../../..\')\n\nfrom libs.label_name_dict.label_dict import LABEL_NAME_MAP\n\n\ndef coordinate_convert_r(box):\n    w, h = box[2:-1]\n    theta = -box[-1]\n    x_lu, y_lu = -w/2, h/2\n    x_ru, y_ru = w/2, h/2\n    x_ld, y_ld = -w/2, -h/2\n    x_rd, y_rd = w/2, -h/2\n\n    x_lu_ = math.cos(theta)*x_lu + math.sin(theta)*y_lu + box[0]\n    y_lu_ = -math.sin(theta)*x_lu + math.cos(theta)*y_lu + box[1]\n\n    x_ru_ = math.cos(theta) * x_ru + math.sin(theta) * y_ru + box[0]\n    y_ru_ = -math.sin(theta) * x_ru + math.cos(theta) * y_ru + box[1]\n\n    x_ld_ = math.cos(theta) * x_ld + math.sin(theta) * y_ld + box[0]\n    y_ld_ = -math.sin(theta) * x_ld + math.cos(theta) * y_ld + box[1]\n\n    x_rd_ = math.cos(theta) * x_rd + math.sin(theta) * y_rd + box[0]\n    y_rd_ = -math.sin(theta) * x_rd + math.cos(theta) * y_rd + box[1]\n\n    convert_box = [x_lu_, y_lu_, x_ru_, y_ru_, x_rd_, y_rd_, x_ld_, y_ld_]\n\n    return convert_box\n\n\ndef read_xml_gtbox_and_label(xml_path):\n    """"""\n    :param xml_path: the path of voc xml\n    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 9],\n           and has [x1, y1, x2, y2, x3, y3, x4, y4, label] in a per row\n    """"""\n\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    img_width = None\n    img_height = None\n    box_list = []\n    for child_of_root in root:\n        # if child_of_root.tag == \'filename\':\n        #     assert child_of_root.text == xml_path.split(\'/\')[-1].split(\'.\')[0] \\\n        #                                  + FLAGS.img_format, \'xml_name and img_name cannot match\'\n\n        # if child_of_root.tag == \'size\':\n        #     for child_item in child_of_root:\n        #         if child_item.tag == \'width\':\n        #             img_width = int(child_item.text)\n        #         if child_item.tag == \'height\':\n        #             img_height = int(child_item.text)\n        #\n        # if child_of_root.tag == \'object\':\n        #     label = None\n        #     for child_item in child_of_root:\n        #         if child_item.tag == \'name\':\n        #             label = NAME_LABEL_MAP[child_item.text]\n        #         if child_item.tag == \'bndbox\':\n        #             tmp_box = []\n        #             for node in child_item:\n        #                 tmp_box.append(float(node.text))\n        #             assert label is not None, \'label is none, error\'\n        #             tmp_box.append(label)\n        #             box_list.append(tmp_box)\n\n        # ship\n        if child_of_root.tag == \'Img_SizeWidth\':\n            img_width = int(child_of_root.text)\n        if child_of_root.tag == \'Img_SizeHeight\':\n            img_height = int(child_of_root.text)\n        if child_of_root.tag == \'HRSC_Objects\':\n            box_list = []\n            for child_item in child_of_root:\n                if child_item.tag == \'HRSC_Object\':\n                    label = 1\n                    # for child_object in child_item:\n                    #     if child_object.tag == \'Class_ID\':\n                    #         label = NAME_LABEL_MAP[child_object.text]\n                    tmp_box = [0., 0., 0., 0., 0.]\n                    for node in child_item:\n                        if node.tag == \'mbox_cx\':\n                            tmp_box[0] = float(node.text)\n                        if node.tag == \'mbox_cy\':\n                            tmp_box[1] = float(node.text)\n                        if node.tag == \'mbox_w\':\n                            tmp_box[2] = float(node.text)\n                        if node.tag == \'mbox_h\':\n                            tmp_box[3] = float(node.text)\n                        if node.tag == \'mbox_ang\':\n                            tmp_box[4] = float(node.text)\n\n                    tmp_box = coordinate_convert_r(tmp_box)\n                        # assert label is not None, \'label is none, error\'\n                    tmp_box.append(label)\n                    # if len(tmp_box) != 0:\n                    box_list.append(tmp_box)\n            # box_list = coordinate_convert(box_list)\n            # print(box_list)\n    gtbox_label = np.array(box_list, dtype=np.int32)\n\n    return img_height, img_width, gtbox_label\n\n\ndef WriterXMLFiles(filename, path, gtbox_label_list, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for gtbox_label in gtbox_label_list:\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(str(LABEL_NAME_MAP[gtbox_label[-1]])))\n        nodeobject.appendChild(nodename)\n\n        nodetruncated = doc.createElement(\'truncated\')\n        nodetruncated.appendChild(doc.createTextNode(str(0)))\n        nodeobject.appendChild(nodetruncated)\n\n        nodedifficult = doc.createElement(\'difficult\')\n        nodedifficult.appendChild(doc.createTextNode(str(0)))\n        nodeobject.appendChild(nodedifficult)\n\n        nodepose = doc.createElement(\'pose\')\n        nodepose.appendChild(doc.createTextNode(\'xxx\'))\n        nodeobject.appendChild(nodepose)\n\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(gtbox_label[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(gtbox_label[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(gtbox_label[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(gtbox_label[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(gtbox_label[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(gtbox_label[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(gtbox_label[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(gtbox_label[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(os.path.join(path, filename), \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\nif __name__ == \'__main__\':\n    src_xml_path = \'/data/HRSC2016/HRSC2016/Test/Annotations\'\n    xml_path = \'/data/HRSC2016/HRSC2016/Test/xmls\'\n\n    src_xmls = os.listdir(src_xml_path)\n\n    for x in src_xmls:\n        x_path = os.path.join(src_xml_path, x)\n        img_height, img_width, gtbox_label = read_xml_gtbox_and_label(x_path)\n        WriterXMLFiles(x, xml_path, gtbox_label, img_width, img_height, 3)\n'"
data/io/ICDAR2015/txt2xml.py,0,"b'import os\nfrom xml.dom.minidom import Document\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\nimport numpy as np\nimport csv\nimport cv2\nimport codecs\n\n\ndef WriterXMLFiles(filename, path, box_list, labels, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for box, label in zip(box_list, labels):\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(label))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(box[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(box[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(box[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(box[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(os.path.join(path,filename), \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\ndef load_annoataion(txt_path):\n    boxes, labels = [], []\n    fr = codecs.open(txt_path, \'r\', \'utf-8\')\n    lines = fr.readlines()\n\n    for line in lines:\n        b = line.strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\').strip(\'$\').split(\',\')[:8]\n        line = list(map(int, b))\n        boxes.append(line)\n        labels.append(\'text\')\n\n    return np.array(boxes), np.array(labels)\n\n\nif __name__ == ""__main__"":\n    txt_path = \'/root/userfolder/yx/idcar2015_txt\'\n    xml_path = \'/root/userfolder/yx/idcar2015_xml\'\n    img_path = \'/root/userfolder/yx/idcar2015_img\'\n    print(os.path.exists(txt_path))\n    txts = os.listdir(txt_path)\n    for count, t in enumerate(txts):\n        boxes, labels = load_annoataion(os.path.join(txt_path, t))\n        xml_name = t.replace(\'.txt\', \'.xml\')\n        img_name = t.replace(\'.txt\', \'.jpg\')\n        img = cv2.imread(os.path.join(img_path, img_name.split(\'gt_\')[-1]))\n        h, w, d = img.shape\n        WriterXMLFiles(xml_name.split(\'gt_\')[-1], xml_path, boxes, labels, w, h, d)\n\n        if count % 1000 == 0:\n            print(count)'"
data/io/MLT/txt2xml.py,0,"b'import os\nfrom xml.dom.minidom import Document\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\nimport numpy as np\nimport csv\nimport cv2\nimport codecs\n\n\ndef make_xml(filename, path, box_list, labels, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for box, label in zip(box_list, labels):\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(label))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(box[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(box[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(box[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(box[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(os.path.join(path,filename), \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\ndef load_annoataion(txt_path):\n    boxes, labels = [], []\n    fr = codecs.open(txt_path, \'r\', \'utf-8\')\n    lines = fr.readlines()\n\n    for line in lines:\n        b = line.strip(\'\\ufeff\').strip(\'\\xef\\xbb\\xbf\').strip(\'$\').split(\',\')[:8]\n        line = list(map(int, b))\n        boxes.append(line)\n        labels.append(\'text\')\n\n    return np.array(boxes), np.array(labels)\n\n\nif __name__ == ""__main__"":\n    txt_path = \'/data/yangxue/dataset/MLT/train/labels\'\n    xml_path = \'/data/yangxue/dataset/MLT/train/xmls\'\n    img_path = \'/data/yangxue/dataset/MLT/train/images\'\n    print(os.path.exists(txt_path))\n    imgs = os.listdir(img_path)\n    for count, i in enumerate(imgs):\n        t = os.path.join(txt_path, \'gt_{}.txt\'.format(i.split(\'.\')[0]))\n        boxes, labels = load_annoataion(t)\n        x = i.split(\'.\')[0] + \'.xml\'\n\n        img = cv2.imread(os.path.join(img_path, i))\n\n        if img is None:\n            print(i)\n            continue\n\n        if i.split(\'.\')[-1] in [\'png\', \'bmp\']:\n            print(i)\n            cv2.imwrite(os.path.join(img_path, i.split(\'.\')[0] + \'.jpg\'), img)\n\n        h, w, d = img.shape\n        make_xml(x, xml_path, boxes, labels, w, h, d)\n\n        if count % 1000 == 0:\n            print(count)'"
data/io/UCAS-AOD/split_data.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport sys\nsys.path.append(\'../../\')\nimport shutil\nimport os\nimport random\nimport math\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndivide_rate = 0.7351\n\nroot_path = \'/data/dataset/UCAS-AOD\'\n\nimage_path = root_path + \'/images\'\nxml_path = root_path + \'/label-xml\'\n\nimage_list = os.listdir(image_path)\n\nimage_name = [n.split(\'.\')[0] for n in image_list]\n\nrandom.shuffle(image_name)\n\ntrain_image = image_name[:int(math.ceil(len(image_name)) * divide_rate)]\ntest_image = image_name[int(math.ceil(len(image_name)) * divide_rate):]\n\nimage_output_train = os.path.join(root_path, \'VOCdevkit_train/JPEGImages\')\nmkdir(image_output_train)\nimage_output_test = os.path.join(root_path, \'VOCdevkit_test/JPEGImages\')\nmkdir(image_output_test)\n\nxml_train = os.path.join(root_path, \'VOCdevkit_train/Annotations\')\nmkdir(xml_train)\nxml_test = os.path.join(root_path, \'VOCdevkit_test/Annotations\')\nmkdir(xml_test)\n\n\ncount = 0\nfor i in train_image:\n    shutil.copy(os.path.join(image_path, i + \'.png\'), image_output_train)\n    shutil.copy(os.path.join(xml_path, i + \'.xml\'), xml_train)\n    if count % 100 == 0:\n        print(""process step {}"".format(count))\n    count += 1\n\nfor i in test_image:\n    shutil.copy(os.path.join(image_path, i + \'.png\'), image_output_test)\n    shutil.copy(os.path.join(xml_path, i + \'.xml\'), xml_test)\n    if count % 100 == 0:\n        print(""process step {}"".format(count))\n    count += 1\n'"
data/io/UCAS-AOD/txt2xml.py,0,"b'import os\nfrom xml.dom.minidom import Document\nfrom xml.dom.minidom import parse\nimport xml.dom.minidom\nimport numpy as np\nimport csv\nimport shutil\nimport cv2\nimport codecs\n\n\ndef WriterXMLFiles(filename, path, box_list, labels, w, h, d):\n\n    # dict_box[filename]=json_dict[filename]\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(\'annotation\')\n    doc.appendChild(root)\n\n    foldername = doc.createElement(""folder"")\n    foldername.appendChild(doc.createTextNode(""JPEGImages""))\n    root.appendChild(foldername)\n\n    nodeFilename = doc.createElement(\'filename\')\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n\n    pathname = doc.createElement(""path"")\n    pathname.appendChild(doc.createTextNode(""xxxx""))\n    root.appendChild(pathname)\n\n    sourcename=doc.createElement(""source"")\n\n    databasename = doc.createElement(""database"")\n    databasename.appendChild(doc.createTextNode(""Unknown""))\n    sourcename.appendChild(databasename)\n\n    annotationname = doc.createElement(""annotation"")\n    annotationname.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(annotationname)\n\n    imagename = doc.createElement(""image"")\n    imagename.appendChild(doc.createTextNode(""xxx""))\n    sourcename.appendChild(imagename)\n\n    flickridname = doc.createElement(""flickrid"")\n    flickridname.appendChild(doc.createTextNode(""0""))\n    sourcename.appendChild(flickridname)\n\n    root.appendChild(sourcename)\n\n    nodesize = doc.createElement(\'size\')\n    nodewidth = doc.createElement(\'width\')\n    nodewidth.appendChild(doc.createTextNode(str(w)))\n    nodesize.appendChild(nodewidth)\n    nodeheight = doc.createElement(\'height\')\n    nodeheight.appendChild(doc.createTextNode(str(h)))\n    nodesize.appendChild(nodeheight)\n    nodedepth = doc.createElement(\'depth\')\n    nodedepth.appendChild(doc.createTextNode(str(d)))\n    nodesize.appendChild(nodedepth)\n    root.appendChild(nodesize)\n\n    segname = doc.createElement(""segmented"")\n    segname.appendChild(doc.createTextNode(""0""))\n    root.appendChild(segname)\n\n    for box, label in zip(box_list, labels):\n\n        nodeobject = doc.createElement(\'object\')\n        nodename = doc.createElement(\'name\')\n        nodename.appendChild(doc.createTextNode(label))\n        nodeobject.appendChild(nodename)\n        nodebndbox = doc.createElement(\'bndbox\')\n        nodex1 = doc.createElement(\'x1\')\n        nodex1.appendChild(doc.createTextNode(str(box[0])))\n        nodebndbox.appendChild(nodex1)\n        nodey1 = doc.createElement(\'y1\')\n        nodey1.appendChild(doc.createTextNode(str(box[1])))\n        nodebndbox.appendChild(nodey1)\n        nodex2 = doc.createElement(\'x2\')\n        nodex2.appendChild(doc.createTextNode(str(box[2])))\n        nodebndbox.appendChild(nodex2)\n        nodey2 = doc.createElement(\'y2\')\n        nodey2.appendChild(doc.createTextNode(str(box[3])))\n        nodebndbox.appendChild(nodey2)\n        nodex3 = doc.createElement(\'x3\')\n        nodex3.appendChild(doc.createTextNode(str(box[4])))\n        nodebndbox.appendChild(nodex3)\n        nodey3 = doc.createElement(\'y3\')\n        nodey3.appendChild(doc.createTextNode(str(box[5])))\n        nodebndbox.appendChild(nodey3)\n        nodex4 = doc.createElement(\'x4\')\n        nodex4.appendChild(doc.createTextNode(str(box[6])))\n        nodebndbox.appendChild(nodex4)\n        nodey4 = doc.createElement(\'y4\')\n        nodey4.appendChild(doc.createTextNode(str(box[7])))\n        nodebndbox.appendChild(nodey4)\n\n        # ang = doc.createElement(\'angle\')\n        # ang.appendChild(doc.createTextNode(str(angle)))\n        # nodebndbox.appendChild(ang)\n        nodeobject.appendChild(nodebndbox)\n        root.appendChild(nodeobject)\n    fp = open(os.path.join(path,filename), \'w\')\n    doc.writexml(fp, indent=\'\\n\')\n    fp.close()\n\n\ndef load_annoataion(txt_path):\n    boxes, labels = [], []\n    fr = codecs.open(txt_path, \'r\', \'utf-8\')\n    lines = fr.readlines()\n\n    for line in lines:\n        b = line.split(\'\\n\')[0].split(\'\\t\')[:8]\n        line = list(map(float, b))\n        boxes.append(line)\n        labels.append(\'car\')\n\n    return np.array(boxes), np.array(labels)\n\n\nif __name__ == ""__main__"":\n    txt_path = \'/data/dataset/UCAS-AOD/CAR\'\n    xml_path = \'/data/dataset/UCAS-AOD/label-xml\'\n    scr_img_path = \'/data/dataset/UCAS-AOD/CAR\'\n    img_path = \'/data/dataset/UCAS-AOD/images\'\n    txts = os.listdir(txt_path)\n    for count, t in enumerate(txts):\n        if not t.endswith(\'.txt\'):\n            continue\n        boxes, labels = load_annoataion(os.path.join(txt_path, t))\n        xml_name = t.replace(\'.txt\', \'.xml\')\n        img_name = t.replace(\'.txt\', \'.png\')\n        img = cv2.imread(os.path.join(scr_img_path, img_name))\n        shutil.copy(os.path.join(scr_img_path, img_name), os.path.join(img_path, \'car_\' + img_name))\n        h, w, d = img.shape\n        WriterXMLFiles(\'car_\' + xml_name, xml_path, boxes, labels, w, h, d)\n\n        if count % 1000 == 0:\n            print(count)'"
libs/box_utils/cython_utils/__init__.py,0,b''
libs/box_utils/cython_utils/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""cython_bbox"",\n        [""bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""cython_nms"",\n        [""nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    )\n    # Extension(\n    #     ""cpu_nms"",\n    #     [""cpu_nms.pyx""],\n    #     extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n    #     include_dirs = [numpy_include]\n    # )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
libs/configs/DOTA1.0/__init__.py,0,b''
libs/configs/HRSC2019/__init__.py,0,b''
libs/configs/HRSC2019/cfgs_res152_hrsc2016_r3det_v1.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nanchor free\n\ncls : ship|| Recall: 0.9535830618892508 || Precison: 0.2536278969027507|| AP: 0.8938264420182536\nF1:0.9282455412691829 P:0.9459002535925612 R:0.9112377850162866\nmAP is : 0.8938264420182536\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_HRSC2016_R3Det_2x_20200312\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 10000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_R_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nEVAL_THRESHOLD = 0.5\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'HRSC2016\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 1\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/HRSC2019/cfgs_res152_hrsc2016_r3det_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nanchor free\ncls : ship|| Recall: 0.9511400651465798 || Precison: 0.4919966301600674|| AP: 0.8946540103361209\nF1:0.9219206680584551 P:0.9460154241645244 R:0.8990228013029316\nmAP is : 0.8946540103361209\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_HRSC2016_R3Det_2x_20200313\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 10000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_R_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\nEVAL_THRESHOLD = 0.5\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'HRSC2016\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 1\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/networks/efficientnet/__init__.py,0,b''
libs/networks/efficientnet/demo.py,62,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for EfficientNet model.\n[1] Mingxing Tan, Quoc V. Le\n  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n  ICML\'19, https://arxiv.org/abs/1905.11946\n""""""\nimport collections\nimport math\nimport numpy as np\nimport six\nimport tensorflow as tf\nimport re\nimport os\n\n# tf.enable_eager_execution()\nl = tf.keras.layers\n\n# defaults will be a public argument for namedtuple in Python 3.7\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\', \'data_format\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\',\n    \'depth_divisor\', \'min_depth\', \'drop_connect_rate\',\n])\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'strides\', \'se_ratio\'\n])\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef relu_fn(x):\n    return tf.keras.layers.Lambda(tf.nn.swish)(x)\n\n\ndef drop_connect(inputs, is_training, drop_connect_rate):\n    """"""Apply drop connect.""""""\n    if not is_training:\n        return inputs\n\n    # Compute keep_prob\n    # TODO(tanmingxing): add support for training progress.\n    keep_prob = 1.0 - drop_connect_rate\n\n    # Compute drop_connect tensor\n    batch_size = tf.shape(inputs)[0]\n    random_tensor = keep_prob\n    random_tensor += tf.random_uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n    binary_tensor = tf.floor(random_tensor)\n    output = tf.div(inputs, keep_prob) * binary_tensor\n    return output\n\n\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):\n    """"""Initialization for convolutional kernels.\n    The main difference with tf.variance_scaling_initializer is that\n    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n    standard deviation, whereas here we use a normal distribution. Similarly,\n    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n    a corrected standard deviation.\n    Args:\n      shape: shape of variable\n      dtype: dtype of variable\n      partition_info: unused\n    Returns:\n      an initialization for the variable\n    """"""\n    del partition_info\n    kernel_height, kernel_width, _, out_filters = shape\n    fan_out = int(kernel_height * kernel_width * out_filters)\n    return tf.random_normal(\n        shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n\n\ndef dense_kernel_initializer(shape, dtype=None, partition_info=None):\n    """"""Initialization for dense kernels.\n    This initialization is equal to\n      tf.variance_scaling_initializer(scale=1.0/3.0, mode=\'fan_out\',\n                                      distribution=\'uniform\').\n    It is written out explicitly here for clarity.\n    Args:\n      shape: shape of variable\n      dtype: dtype of variable\n      partition_info: unused\n    Returns:\n      an initialization for the variable\n    """"""\n    del partition_info\n    init_range = 1.0 / np.sqrt(shape[1])\n    return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n\n\ndef round_filters(filters, global_params):\n    """"""Round number of filters based on depth multiplier.""""""\n    orig_f = filters\n    multiplier = global_params.width_coefficient\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    if not multiplier:\n        return filters\n\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n        new_filters += divisor\n    tf.logging.info(\'round_filter input={} output={}\'.format(orig_f, new_filters))\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """"""Round number of filters based on depth multiplier.""""""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef _bn_layer(axis, momentum, epsilon, name=None):\n    return l.BatchNormalization(axis=axis,\n                                momentum=momentum,\n                                epsilon=epsilon,\n                                scale=True,\n                                center=True,\n                                name=name)\n\n\nclass MBConvBlock(object):\n    """"""A class of MBConv: Mobile Inveretd Residual Bottleneck.\n    Attributes:\n      has_se: boolean. Whether the block contains a Squeeze and Excitation layer\n        inside.\n      endpoints: dict. A list of internal tensors.\n    """"""\n\n    def __init__(self, block_args, global_params):\n        """"""Initializes a MBConv block.\n        Args:\n          block_args: BlockArgs, arguments to create a Block.\n          global_params: GlobalParams, a set of global parameters.\n        """"""\n        self._block_args = block_args\n        self._batch_norm_momentum = global_params.batch_norm_momentum\n        self._batch_norm_epsilon = global_params.batch_norm_epsilon\n        if global_params.data_format == \'channels_first\':\n            self._channel_axis = 1\n            self._spatial_dims = [2, 3]\n        else:\n            self._channel_axis = -1\n            self._spatial_dims = [1, 2]\n        self.has_se = (self._block_args.se_ratio is not None) and (\n            self._block_args.se_ratio > 0) and (self._block_args.se_ratio <= 1)\n\n        self.endpoints = None\n\n        # Builds the block accordings to arguments.\n        self._build()\n\n    def block_args(self):\n        return self._block_args\n\n    def _build(self):\n        """"""Builds block according to the arguments.""""""\n        filters = self._block_args.input_filters * self._block_args.expand_ratio\n        if self._block_args.expand_ratio != 1:\n            # Expansion phase:\n            self._expand_conv = l.Conv2D(\n                filters,\n                kernel_size=[1, 1],\n                strides=[1, 1],\n                kernel_initializer=conv_kernel_initializer,\n                padding=\'same\',\n                use_bias=False)\n            self._bn0 = _bn_layer(\n                axis=self._channel_axis,\n                momentum=self._batch_norm_momentum,\n                epsilon=self._batch_norm_epsilon)\n\n        kernel_size = self._block_args.kernel_size\n        # Depth-wise convolution phase:\n        self._depthwise_conv = l.DepthwiseConv2D(\n            kernel_size=[kernel_size, kernel_size],\n            strides=self._block_args.strides,\n            depthwise_initializer=conv_kernel_initializer,\n            padding=\'same\',\n            use_bias=False)\n        self._bn1 = _bn_layer(\n            axis=self._channel_axis,\n            momentum=self._batch_norm_momentum,\n            epsilon=self._batch_norm_epsilon)\n\n        if self.has_se:\n            num_reduced_filters = max(\n                1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            # Squeeze and Excitation layer.\n            self._se_reduce = l.Conv2D(\n                num_reduced_filters,\n                kernel_size=[1, 1],\n                strides=[1, 1],\n                kernel_initializer=conv_kernel_initializer,\n                padding=\'same\',\n                use_bias=True)\n            self._se_expand = l.Conv2D(\n                filters,\n                kernel_size=[1, 1],\n                strides=[1, 1],\n                kernel_initializer=conv_kernel_initializer,\n                padding=\'same\',\n                use_bias=True)\n\n        # Output phase:\n        filters = self._block_args.output_filters\n        self._project_conv = l.Conv2D(\n            filters,\n            kernel_size=[1, 1],\n            strides=[1, 1],\n            kernel_initializer=conv_kernel_initializer,\n            padding=\'same\',\n            use_bias=False)\n        self._bn2 = _bn_layer(\n            axis=self._channel_axis,\n            momentum=self._batch_norm_momentum,\n            epsilon=self._batch_norm_epsilon)\n\n    def _call_se(self, input_tensor):\n        """"""Call Squeeze and Excitation layer.\n        Args:\n          input_tensor: Tensor, a single input tensor for Squeeze/Excitation layer.\n        Returns:\n          A output tensor, which should have the same shape as input.\n        """"""\n        se_tensor = tf.reduce_mean(input_tensor, self._spatial_dims, keepdims=True)\n        se_tensor = self._se_expand(relu_fn(self._se_reduce(se_tensor)))\n        tf.logging.info(\'Built Squeeze and Excitation with tensor shape: %s\' %\n                        (se_tensor.shape))\n        return tf.sigmoid(se_tensor) * input_tensor\n\n    def call(self, inputs, training=True, drop_connect_rate=None, output_layer_name=None):\n        """"""Implementation of call().\n        Args:\n          inputs: the inputs tensor.\n          training: boolean, whether the model is constructed for training.\n          drop_connect_rate: float, between 0 to 1, drop connect rate.\n          output_layer_name: layer name for output layer\n        Returns:\n          A output tensor.\n        """"""\n        tf.logging.info(\'Block input: %s shape: %s\' % (inputs.name, inputs.shape))\n        if self._block_args.expand_ratio != 1:\n            x = relu_fn(self._bn0(self._expand_conv(inputs), training=training))\n        else:\n            x = inputs\n        tf.logging.info(\'Expand: %s shape: %s\' % (x.name, x.shape))\n\n        x = relu_fn(self._bn1(self._depthwise_conv(x), training=training))\n        tf.logging.info(\'DWConv: %s shape: %s\' % (x.name, x.shape))\n\n        if self.has_se:\n            with tf.variable_scope(\'se\'):\n                x = tf.keras.layers.Lambda(self._call_se)(x)\n\n        if self._block_args.id_skip:\n            if all(\n                            s == 1 for s in self._block_args.strides\n            ) and self._block_args.input_filters == self._block_args.output_filters:\n                # only apply drop_connect if skip presents.\n                x = self._bn2(self._project_conv(x), training=training)\n                if drop_connect_rate:\n                    x = tf.keras.layers.Lambda(drop_connect)(x, training, drop_connect_rate)\n                x = l.Add(name=output_layer_name)([x, inputs])\n            else:\n                x = _bn_layer(axis=self._channel_axis,\n                              momentum=self._batch_norm_momentum,\n                              epsilon=self._batch_norm_epsilon,\n                              name=output_layer_name)(self._project_conv(x), training=training)\n        else:\n            x = _bn_layer(axis=self._channel_axis,\n                          momentum=self._batch_norm_momentum,\n                          epsilon=self._batch_norm_epsilon,\n                          name=output_layer_name)(self._project_conv(x), training=training)\n        tf.logging.info(\'Project: %s shape: %s\' % (x.name, x.shape))\n        return x\n\n\nclass EfficientNet(tf.keras.Model):\n    """"""A class implements tf.keras.Model for MNAS-like model.\n      Reference: https://arxiv.org/abs/1807.11626\n    """"""\n\n    def __init__(self, blocks_args=None, global_params=None):\n        """"""Initializes an `Model` instance.\n        Args:\n          blocks_args: A list of BlockArgs to construct block modules.\n          global_params: GlobalParams, a set of global parameters.\n        Raises:\n          ValueError: when blocks_args is not specified as a list.\n        """"""\n        super(EfficientNet, self).__init__()\n        if not isinstance(blocks_args, list):\n            raise ValueError(\'blocks_args should be a list.\')\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n        self.endpoints = None\n        self._build()\n\n    def _build(self):\n        """"""Builds a model.""""""\n        self._blocks = []\n        # Builds blocks.\n        for block_args in self._blocks_args:\n            assert block_args.num_repeat > 0\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters,\n                                            self._global_params),\n                output_filters=round_filters(block_args.output_filters,\n                                             self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params))\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            if block_args.num_repeat > 1:\n                # pylint: disable=protected-access\n                block_args = block_args._replace(\n                    input_filters=block_args.output_filters, strides=[1, 1])\n                # pylint: enable=protected-access\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n\n        batch_norm_momentum = self._global_params.batch_norm_momentum\n        batch_norm_epsilon = self._global_params.batch_norm_epsilon\n        if self._global_params.data_format == \'channels_first\':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n\n        # Stem part.\n        self._conv_stem = l.Conv2D(\n            filters=round_filters(32, self._global_params),\n            kernel_size=[3, 3],\n            strides=[2, 2],\n            kernel_initializer=conv_kernel_initializer,\n            padding=\'same\',\n            use_bias=False,\n            name=""stem_conv"")\n        self._bn0 = _bn_layer(\n            axis=channel_axis,\n            momentum=batch_norm_momentum,\n            epsilon=batch_norm_epsilon)\n\n        # Head part.\n        self._conv_head = l.Conv2D(\n            filters=round_filters(1280, self._global_params),\n            kernel_size=[1, 1],\n            strides=[1, 1],\n            kernel_initializer=conv_kernel_initializer,\n            padding=\'same\',\n            use_bias=False)\n        self._bn1 = _bn_layer(\n            axis=channel_axis,\n            momentum=batch_norm_momentum,\n            epsilon=batch_norm_epsilon)\n\n        self._avg_pooling = l.GlobalAveragePooling2D(\n            data_format=self._global_params.data_format)\n        self._fc = l.Dense(\n            self._global_params.num_classes,\n            kernel_initializer=dense_kernel_initializer)\n\n        if self._global_params.dropout_rate > 0:\n            self._dropout = l.Dropout(self._global_params.dropout_rate)\n        else:\n            self._dropout = None\n\n    def call(self, inputs, training=True, features_only=None):\n        """"""Implementation of call().\n        Args:\n          inputs: input tensors.\n          training: boolean, whether the model is constructed for training.\n          features_only: build the base feature network only.\n        Returns:\n          output tensors.\n        """"""\n        outputs = None\n        self.endpoints = {}\n        # Calls Stem layers\n        with tf.variable_scope(\'stem\'):\n            outputs = relu_fn(\n                self._bn0(self._conv_stem(inputs), training=training))\n        tf.logging.info(\'Built stem layers with output shape: %s\' % outputs.shape)\n        self.endpoints[\'stem\'] = outputs\n\n        # Calls blocks.\n        reduction_idx = 0\n        for idx, block in enumerate(self._blocks):\n            is_reduction = False\n            if ((idx == len(self._blocks) - 1) or\n                        self._blocks[idx + 1].block_args().strides[0] > 1):\n                is_reduction = True\n                reduction_idx += 1\n\n            with tf.variable_scope(\'blocks_%s\' % idx):\n                drop_rate = self._global_params.drop_connect_rate\n                if drop_rate:\n                    drop_rate *= float(idx) / len(self._blocks)\n                    tf.logging.info(\'block_%s drop_connect_rate: %s\' % (idx, drop_rate))\n                outputs = block.call(outputs, training=training, output_layer_name=\'block_%s\' % idx)\n                self.endpoints[\'block_%s\' % idx] = outputs\n                if is_reduction:\n                    self.endpoints[\'reduction_%s\' % reduction_idx] = outputs\n                if block.endpoints:\n                    for k, v in six.iteritems(block.endpoints):\n                        self.endpoints[\'block_%s/%s\' % (idx, k)] = v\n                        if is_reduction:\n                            self.endpoints[\'reduction_%s/%s\' % (reduction_idx, k)] = v\n        self.endpoints[\'global_pool\'] = outputs\n        if not features_only:\n            # Calls final layers and returns logits.\n            with tf.variable_scope(\'head\'):\n                outputs = relu_fn(\n                    self._bn1(self._conv_head(outputs), training=training))\n                outputs = self._avg_pooling(outputs)\n                if self._dropout:\n                    outputs = self._dropout(outputs, training=training)\n                outputs = self._fc(outputs)\n                self.endpoints[\'head\'] = outputs\n        return outputs\n\n    def call_model(self, inputs, training=True, features_only=None):\n        """"""Implementation of call().\n        Args:\n          inputs: input tensors.\n          training: boolean, whether the model is constructed for training.\n          features_only: build the base feature network only.\n        Returns:\n          output tensors.\n        """"""\n        outputs = None\n        self.endpoints = {}\n        # Calls Stem layers\n        with tf.variable_scope(\'stem\'):\n            outputs = relu_fn(\n                self._bn0(self._conv_stem(inputs), training=training))\n        tf.logging.info(\'Built stem layers with output shape: %s\' % outputs.shape)\n\n        # Calls blocks.\n        reduction_idx = 0\n        for idx, block in enumerate(self._blocks):\n            is_reduction = False\n            if ((idx == len(self._blocks) - 1) or\n                        self._blocks[idx + 1].block_args().strides[0] > 1):\n                is_reduction = True\n                reduction_idx += 1\n\n            with tf.variable_scope(\'blocks_%s\' % idx):\n                drop_rate = self._global_params.drop_connect_rate\n                if drop_rate:\n                    drop_rate *= float(idx) / len(self._blocks)\n                    tf.logging.info(\'block_%s drop_connect_rate: %s\' % (idx, drop_rate))\n                outputs = block.call(outputs, training=training, output_layer_name=\'block_%s\' % idx)\n\n        if not features_only:\n            # Calls final layers and returns logits.\n            with tf.variable_scope(\'head\'):\n                outputs = relu_fn(\n                    self._bn1(self._conv_head(outputs), training=training))\n                outputs = self._avg_pooling(outputs)\n                if self._dropout:\n                    outputs = self._dropout(outputs, training=training)\n                outputs = self._fc(outputs)\n        # model\n        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n        return model\n\n\ndef efficientnet_params(model_name):\n    """"""Get efficientnet params based on model name.""""""\n    params_dict = {\n        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'noisy_student_efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    """"""Block Decoder for readability.""""""\n\n    def _decode_block_string(self, block_string):\n        """"""Gets a block through a string notation of arguments.""""""\n        assert isinstance(block_string, str)\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        if \'s\' not in options or len(options[\'s\']) != 2:\n            raise ValueError(\'Strides options should be a pair of integers.\')\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            strides=[int(options[\'s\'][0]), int(options[\'s\'][1])])\n\n    def _encode_block_string(self, block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if block.se_ratio > 0 and block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    def decode(self, string_list):\n        """"""Decodes a list of string notations to specify blocks inside the network.\n        Args:\n          string_list: a list of strings, each string is a notation of block.\n        Returns:\n          A list of namedtuples to represent blocks arguments.\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(self._decode_block_string(block_string))\n        return blocks_args\n\n    def encode(self, blocks_args):\n        """"""Encodes a list of Blocks to a list of strings.\n        Args:\n          blocks_args: A list of namedtuples to represent blocks arguments.\n        Returns:\n          a list of strings, each string is a notation of block.\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(self._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet_args(width_coefficient=None,\n                      depth_coefficient=None,\n                      dropout_rate=0.2,\n                      drop_connect_rate=0.2):\n    """"""Creates a efficientnet model args.""""""\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s11_e6_i192_o320_se0.25\',\n    ]\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        data_format=\'channels_last\',\n        num_classes=1000,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None)\n    decoder = BlockDecoder()\n    return decoder.decode(blocks_args), global_params\n\n\ndef get_model_params(model_name, override_params):\n    """"""Get the block args and global params for a given model.""""""\n    if model_name.startswith(\'efficientnet\'):\n        width_coefficient, depth_coefficient, _, dropout_rate = (\n            efficientnet_params(model_name))\n        blocks_args, global_params = efficientnet_args(\n            width_coefficient, depth_coefficient, dropout_rate)\n    else:\n        raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included\n        # in global_params.\n        global_params = global_params._replace(**override_params)\n\n    tf.logging.info(\'global_params= %s\', global_params)\n    tf.logging.info(\'blocks_args= %s\', blocks_args)\n    return blocks_args, global_params\n\n\ndef build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None):\n    """"""A helper functiion to creates a model and returns predicted logits.\n    Args:\n      images: input images tensor.\n      model_name: string, the predefined model name.\n      training: boolean, whether the model is constructed for training.\n      override_params: A dictionary of params for overriding. Fields must exist in\n        efficientnet_model.GlobalParams.\n      model_dir: string, optional model dir for saving configs.\n    Returns:\n      logits: the logits tensor of classes.\n      endpoints: the endpoints for each layer.\n    Raises:\n      When model_name specified an undefined model, raises NotImplementedError.\n      When override_params has invalid fields, raises ValueError.\n    """"""\n    assert isinstance(images, tf.Tensor)\n    blocks_args, global_params = get_model_params(model_name, override_params)\n\n    if model_dir:\n        param_file = os.path.join(model_dir, \'model_params.txt\')\n        if not tf.gfile.Exists(param_file):\n            with tf.gfile.GFile(param_file, \'w\') as f:\n                tf.logging.info(\'writing to %s\' % param_file)\n                f.write(\'model_name= %s\\n\\n\' % model_name)\n                f.write(\'global_params= %s\\n\\n\' % str(global_params))\n                f.write(\'blocks_args= %s\\n\\n\' % str(blocks_args))\n\n    with tf.variable_scope(model_name):\n        model = EfficientNet(blocks_args, global_params)\n        logits = model(images, training=training)\n\n    logits = tf.identity(logits, \'logits\')\n    return logits, model.endpoints\n\n\ndef build_model_base(images, model_name, training, override_params=None):\n    """"""A helper functiion to create a base model and return global_pool.\n    Args:\n      images: input images tensor.\n      model_name: string, the model name of a pre-defined MnasNet.\n      training: boolean, whether the model is constructed for training.\n      override_params: A dictionary of params for overriding. Fields must exist in\n        mnasnet_model.GlobalParams.\n    Returns:\n      features: global pool features.\n      endpoints: the endpoints for each layer.\n    Raises:\n      When model_name specified an undefined model, raises NotImplementedError.\n      When override_params has invalid fields, raises ValueError.\n    """"""\n    assert isinstance(images, tf.Tensor)\n    blocks_args, global_params = get_model_params(model_name, override_params)\n\n    with tf.variable_scope(model_name):\n        model = EfficientNet(blocks_args, global_params)\n        features = model(images, training=training, features_only=True)\n\n    # features = tf.identity(features, \'global_pool\')\n    return features, model.endpoints\n\n\ndef build_model_base_keras_model(input_shape, model_name, training, override_params=None):\n    """"""A helper functiion to create a base model and return global_pool.\n    Args:\n      images: input images tensor.\n      model_name: string, the model name of a pre-defined MnasNet.\n      training: boolean, whether the model is constructed for training.\n      override_params: A dictionary of params for overriding. Fields must exist in\n        mnasnet_model.GlobalParams.\n    Returns:\n      features: global pool features.\n      endpoints: the endpoints for each layer.\n    Raises:\n      When model_name specified an undefined model, raises NotImplementedError.\n      When override_params has invalid fields, raises ValueError.\n    """"""\n    blocks_args, global_params = get_model_params(model_name, override_params)\n\n    with tf.variable_scope(model_name):\n        inputs = tf.keras.layers.Input(shape=input_shape)\n        model = EfficientNet(blocks_args, global_params)\n        net = model.call_model(inputs, training=training, features_only=True)\n        return net\n\n\ndef restore_model(sess, ckpt_dir):\n    """"""Restore variables from checkpoint dir.""""""\n    checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n    # ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n    # ema_vars = tf.trainable_variables() + tf.get_collection(\'moving_vars\')\n    # for v in tf.global_variables():\n    #     if \'moving_mean\' in v.name or \'moving_variance\' in v.name:\n    #         ema_vars.append(v)\n    # ema_vars = list(set(ema_vars))\n    # var_dict = ema.variables_to_restore(ema_vars)\n    saver = tf.train.Saver(max_to_keep=1)\n    saver.restore(sess, checkpoint)\n\n\nif __name__ == ""__main__"":\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \'2\'\n    inputs = tf.ones((1, 640, 640, 3), dtype=tf.float32)\n    input_shape = [640, 640, 3]\n    model_name = ""efficientnet-b0""\n    features, endpoints = build_model_base(inputs, model_name, training=True)\n    # model = build_model_base_keras_model(input_shape, model_name, True)\n    # print(model.get_layer(""block_15"").output)\n    for k, v in endpoints.items():\n        print(k, v)\n\n        # b0: feature map keys: [""block_4"", ""block_10"", ""block_15""] (1/8, 1/16, 1/32)\n        # b1: feature map keys: [""block_7"", ""block_15"", ""block_22""]\n        # b2: feature map keys: [""block_7"", ""block_15"", ""block_22""]\n        # b3: feature map keys: [""block_7"", ""block_17"", ""block_25""]\n        # b4: feature map keys: [""block_9"", ""block_21"", ""block_31""]\n        # b5: feature map keys: [""block_12"", ""block_26"", ""block_38""]\n        # b6: feature map keys: [""block_14"", ""block_30"", ""block_44""]\n        # b7: feature map keys: [""block_17"", ""block_37"", ""block_54""]\n\n    init_op = tf.group(\n        tf.global_variables_initializer(),\n        tf.local_variables_initializer()\n    )\n    # saver = tf.train.Saver(max_to_keep=5)\n    tfconfig = tf.ConfigProto(\n        allow_soft_placement=True, log_device_placement=False)\n    tfconfig.gpu_options.allow_growth = True\n    with tf.Session(config=tfconfig) as sess:\n        sess.run(init_op)\n        restore_model(sess, \'/data/yangxue/code/R3Det_Tensorflow/libs/networks/efficientnet/efficientnet-b0\')\n        # saver.restore(sess, \'/data/yangxue/code/R3Det_Tensorflow/libs/networks/efficientnet/efficientnet-b0/model.ckpt\')\n\n        features_, endpoints_ = sess.run([features, endpoints])\n        print(features_.shape)'"
libs/networks/efficientnet/efficientnet_builder.py,26,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model Builder for EfficientNet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\nimport os\nimport re\nfrom absl import logging\nimport numpy as np\nimport six\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.networks.efficientnet import efficientnet_model\nfrom libs.networks.efficientnet import utils\nfrom libs.configs import cfgs\n\nMEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]\nSTDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n\n\ndef efficientnet_params(model_name):\n  """"""Get efficientnet params based on model name.""""""\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n      \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n      \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n      \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n      \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n      \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n      \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n  }\n  return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n  """"""Block Decoder for readability.""""""\n\n  def _decode_block_string(self, block_string):\n    """"""Gets a block through a string notation of arguments.""""""\n    if six.PY2:\n      assert isinstance(block_string, (str, unicode))\n    else:\n      assert isinstance(block_string, str)\n    ops = block_string.split(\'_\')\n    options = {}\n    for op in ops:\n      splits = re.split(r\'(\\d.*)\', op)\n      if len(splits) >= 2:\n        key, value = splits[:2]\n        options[key] = value\n\n    if \'s\' not in options or len(options[\'s\']) != 2:\n      raise ValueError(\'Strides options should be a pair of integers.\')\n\n    return efficientnet_model.BlockArgs(\n        kernel_size=int(options[\'k\']),\n        num_repeat=int(options[\'r\']),\n        input_filters=int(options[\'i\']),\n        output_filters=int(options[\'o\']),\n        expand_ratio=int(options[\'e\']),\n        id_skip=(\'noskip\' not in block_string),\n        se_ratio=float(options[\'se\']) if \'se\' in options else None,\n        strides=[int(options[\'s\'][0]),\n                 int(options[\'s\'][1])],\n        conv_type=int(options[\'c\']) if \'c\' in options else 0,\n        fused_conv=int(options[\'f\']) if \'f\' in options else 0,\n        super_pixel=int(options[\'p\']) if \'p\' in options else 0,\n        condconv=(\'cc\' in block_string))\n\n  def _encode_block_string(self, block):\n    """"""Encodes a block to a string.""""""\n    args = [\n        \'r%d\' % block.num_repeat,\n        \'k%d\' % block.kernel_size,\n        \'s%d%d\' % (block.strides[0], block.strides[1]),\n        \'e%s\' % block.expand_ratio,\n        \'i%d\' % block.input_filters,\n        \'o%d\' % block.output_filters,\n        \'c%d\' % block.conv_type,\n        \'f%d\' % block.fused_conv,\n        \'p%d\' % block.super_pixel,\n    ]\n    if block.se_ratio > 0 and block.se_ratio <= 1:\n      args.append(\'se%s\' % block.se_ratio)\n    if block.id_skip is False:  # pylint: disable=g-bool-id-comparison\n      args.append(\'noskip\')\n    if block.condconv:\n      args.append(\'cc\')\n    return \'_\'.join(args)\n\n  def decode(self, string_list):\n    """"""Decodes a list of string notations to specify blocks inside the network.\n    Args:\n      string_list: a list of strings, each string is a notation of block.\n    Returns:\n      A list of namedtuples to represent blocks arguments.\n    """"""\n    assert isinstance(string_list, list)\n    blocks_args = []\n    for block_string in string_list:\n      blocks_args.append(self._decode_block_string(block_string))\n    return blocks_args\n\n  def encode(self, blocks_args):\n    """"""Encodes a list of Blocks to a list of strings.\n    Args:\n      blocks_args: A list of namedtuples to represent blocks arguments.\n    Returns:\n      a list of strings, each string is a notation of block.\n    """"""\n    block_strings = []\n    for block in blocks_args:\n      block_strings.append(self._encode_block_string(block))\n    return block_strings\n\n\ndef swish(features, use_native=True, use_hard=False):\n  """"""Computes the Swish activation function.\n  We provide three alternnatives:\n    - Native tf.nn.swish, use less memory during training than composable swish.\n    - Quantization friendly hard swish.\n    - A composable swish, equivalant to tf.nn.swish, but more general for\n      finetuning and TF-Hub.\n  Args:\n    features: A `Tensor` representing preactivation values.\n    use_native: Whether to use the native swish from tf.nn that uses a custom\n      gradient to reduce memory usage, or to use customized swish that uses\n      default TensorFlow gradient computation.\n    use_hard: Whether to use quantization-friendly hard swish.\n  Returns:\n    The activation value.\n  """"""\n  if use_native and use_hard:\n    raise ValueError(\'Cannot specify both use_native and use_hard.\')\n\n  if use_native:\n    return tf.nn.swish(features)\n\n  if use_hard:\n    return features * tf.nn.relu6(features + np.float32(3)) * (1. / 6.)\n\n  features = tf.convert_to_tensor(features, name=\'features\')\n  return features * tf.nn.sigmoid(features)\n\n\n_DEFAULT_BLOCKS_ARGS = [\n    \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n    \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n    \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n    \'r1_k3_s11_e6_i192_o320_se0.25\',\n]\n\n\ndef efficientnet(width_coefficient=None,\n                 depth_coefficient=None,\n                 dropout_rate=0.2,\n                 survival_prob=0.8):\n  """"""Creates a efficientnet model.""""""\n  global_params = efficientnet_model.GlobalParams(\n      blocks_args=_DEFAULT_BLOCKS_ARGS,\n      batch_norm_momentum=0.99,\n      batch_norm_epsilon=1e-3,\n      dropout_rate=dropout_rate,\n      survival_prob=survival_prob,\n      data_format=\'channels_last\',\n      num_classes=1000,\n      width_coefficient=width_coefficient,\n      depth_coefficient=depth_coefficient,\n      depth_divisor=8,\n      min_depth=None,\n      relu_fn=tf.nn.swish,\n      # The default is TPU-specific batch norm.\n      # The alternative is tf.layers.BatchNormalization.\n      batch_norm=utils.BatchNormalization,  # TPU-specific requirement.\n      use_se=True,\n      clip_projection_output=False)\n  return global_params\n\n\ndef get_model_params(model_name, override_params):\n  """"""Get the block args and global params for a given model.""""""\n  if model_name.startswith(\'efficientnet\'):\n    width_coefficient, depth_coefficient, _, dropout_rate = (\n        efficientnet_params(model_name))\n    global_params = efficientnet(\n        width_coefficient, depth_coefficient, dropout_rate)\n  else:\n    raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n\n  if override_params:\n    # ValueError will be raised here if override_params has fields not included\n    # in global_params.\n    global_params = global_params._replace(**override_params)\n\n  decoder = BlockDecoder()\n  blocks_args = decoder.decode(global_params.blocks_args)\n\n  logging.info(\'global_params= %s\', global_params)\n  return blocks_args, global_params\n\n\ndef build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None,\n                fine_tuning=False,\n                features_only=False,\n                pooled_features_only=False):\n  """"""A helper function to create a model and return predicted logits.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n    model_dir: string, optional model dir for saving configs.\n    fine_tuning: boolean, whether the model is used for finetuning.\n    features_only: build the base feature network only (excluding final\n      1x1 conv layer, global pooling, dropout and fc head).\n    pooled_features_only: build the base network for features extraction (after\n      1x1 conv layer and global pooling, but before dropout and fc head).\n  Returns:\n    logits: the logits tensor of classes.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  assert not (features_only and pooled_features_only)\n\n  # For backward compatibility.\n  if override_params and override_params.get(\'drop_connect_rate\', None):\n    override_params[\'survival_prob\'] = 1 - override_params[\'drop_connect_rate\']\n\n  if not training or fine_tuning:\n    if not override_params:\n      override_params = {}\n    override_params[\'batch_norm\'] = utils.BatchNormalization\n    if fine_tuning:\n      override_params[\'relu_fn\'] = functools.partial(swish, use_native=False)\n  blocks_args, global_params = get_model_params(model_name, override_params)\n\n  if model_dir:\n    param_file = os.path.join(model_dir, \'model_params.txt\')\n    if not tf.gfile.Exists(param_file):\n      if not tf.gfile.Exists(model_dir):\n        tf.gfile.MakeDirs(model_dir)\n      with tf.gfile.GFile(param_file, \'w\') as f:\n        logging.info(\'writing to %s\', param_file)\n        f.write(\'model_name= %s\\n\\n\' % model_name)\n        f.write(\'global_params= %s\\n\\n\' % str(global_params))\n        f.write(\'blocks_args= %s\\n\\n\' % str(blocks_args))\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    outputs = model(\n        images,\n        training=training,\n        features_only=features_only,\n        pooled_features_only=pooled_features_only)\n  if features_only:\n    outputs = tf.identity(outputs, \'features\')\n  elif pooled_features_only:\n    outputs = tf.identity(outputs, \'pooled_features\')\n  else:\n    outputs = tf.identity(outputs, \'logits\')\n  return outputs, model.endpoints\n\n\ndef build_model_base(images, model_name, training, override_params=None):\n  """"""Create a base feature network and return the features before pooling.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n  Returns:\n    features: base features before pooling.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  # For backward compatibility.\n  if override_params and override_params.get(\'drop_connect_rate\', None):\n    override_params[\'survival_prob\'] = 1 - override_params[\'drop_connect_rate\']\n\n  blocks_args, global_params = get_model_params(model_name, override_params)\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    features = model(images, training=training, features_only=True)\n\n  features = tf.identity(features, \'features\')\n  return features, model.endpoints\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\ndef build_model_fpn_base(images, model_name, training, override_params=None):\n    _, feature_dict = build_model_base(images, model_name, training, override_params=override_params)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'reduction_5\'],\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 2, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""reduction_%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 2, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\n                                                              \'fpn_num_filters\'],\n                                                          kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'reduction_5\'],\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    # for level in range(7, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    return pyramid_dict'"
libs/networks/efficientnet/efficientnet_lite_builder.py,19,"b'# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model Builder for EfficientNet Edge Models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom absl import logging\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom libs.networks.efficientnet import efficientnet_builder\nfrom libs.networks.efficientnet import efficientnet_model\nfrom libs.networks.efficientnet import utils\nfrom libs.configs import cfgs\n\n# Edge models use inception-style MEAN and STDDEV for better post-quantization.\nMEAN_RGB = [127.0, 127.0, 127.0]\nSTDDEV_RGB = [128.0, 128.0, 128.0]\n\n\ndef efficientnet_lite_params(model_name):\n  """"""Get efficientnet params based on model name.""""""\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n      \'efficientnet-lite0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-lite1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-lite2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-lite3\': (1.2, 1.4, 280, 0.3),\n      \'efficientnet-lite4\': (1.4, 1.8, 300, 0.3),\n  }\n  return params_dict[model_name]\n\n\n_DEFAULT_BLOCKS_ARGS = [\n    \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n    \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n    \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n    \'r1_k3_s11_e6_i192_o320_se0.25\',\n]\n\n\ndef efficientnet_lite(width_coefficient=None,\n                      depth_coefficient=None,\n                      dropout_rate=0.2,\n                      survival_prob=0.8):\n  """"""Creates a efficientnet model.""""""\n  global_params = efficientnet_model.GlobalParams(\n      blocks_args=_DEFAULT_BLOCKS_ARGS,\n      batch_norm_momentum=0.99,\n      batch_norm_epsilon=1e-3,\n      dropout_rate=dropout_rate,\n      survival_prob=survival_prob,\n      data_format=\'channels_last\',\n      num_classes=1000,\n      width_coefficient=width_coefficient,\n      depth_coefficient=depth_coefficient,\n      depth_divisor=8,\n      min_depth=None,\n      relu_fn=tf.nn.relu6,  # Relu6 is for easier quantization.\n      # The default is TPU-specific batch norm.\n      # The alternative is tf.layers.BatchNormalization.\n      batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.\n      clip_projection_output=False,\n      fix_head_stem=True,  # Don\'t scale stem and head.\n      local_pooling=True,  # special cases for tflite issues.\n      use_se=False)  # SE is not well supported on many lite devices.\n  return global_params\n\n\ndef get_model_params(model_name, override_params):\n  """"""Get the block args and global params for a given model.""""""\n  if model_name.startswith(\'efficientnet-lite\'):\n    width_coefficient, depth_coefficient, _, dropout_rate = (\n        efficientnet_lite_params(model_name))\n    global_params = efficientnet_lite(\n        width_coefficient, depth_coefficient, dropout_rate)\n  else:\n    raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n\n  if override_params:\n    # ValueError will be raised here if override_params has fields not included\n    # in global_params.\n    global_params = global_params._replace(**override_params)\n\n  decoder = efficientnet_builder.BlockDecoder()\n  blocks_args = decoder.decode(global_params.blocks_args)\n\n  logging.info(\'global_params= %s\', global_params)\n  return blocks_args, global_params\n\n\ndef build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None,\n                fine_tuning=False,\n                features_only=False,\n                pooled_features_only=False):\n  """"""A helper function to create a model and return predicted logits.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n    model_dir: string, optional model dir for saving configs.\n    fine_tuning: boolean, whether the model is used for finetuning.\n    features_only: build the base feature network only (excluding final\n      1x1 conv layer, global pooling, dropout and fc head).\n    pooled_features_only: build the base network for features extraction (after\n      1x1 conv layer and global pooling, but before dropout and fc head).\n  Returns:\n    logits: the logits tensor of classes.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  assert not (features_only and pooled_features_only)\n\n  # For backward compatibility.\n  if override_params and override_params.get(\'drop_connect_rate\', None):\n    override_params[\'survival_prob\'] = 1 - override_params[\'drop_connect_rate\']\n\n  if not training or fine_tuning:\n    if not override_params:\n      override_params = {}\n    override_params[\'batch_norm\'] = utils.BatchNormalization\n  blocks_args, global_params = get_model_params(model_name, override_params)\n\n  if model_dir:\n    param_file = os.path.join(model_dir, \'model_params.txt\')\n    if not tf.gfile.Exists(param_file):\n      if not tf.gfile.Exists(model_dir):\n        tf.gfile.MakeDirs(model_dir)\n      with tf.gfile.GFile(param_file, \'w\') as f:\n        logging.info(\'writing to %s\', param_file)\n        f.write(\'model_name= %s\\n\\n\' % model_name)\n        f.write(\'global_params= %s\\n\\n\' % str(global_params))\n        f.write(\'blocks_args= %s\\n\\n\' % str(blocks_args))\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    outputs = model(\n        images,\n        training=training,\n        features_only=features_only,\n        pooled_features_only=pooled_features_only)\n  if features_only:\n    outputs = tf.identity(outputs, \'features\')\n  elif pooled_features_only:\n    outputs = tf.identity(outputs, \'pooled_features\')\n  else:\n    outputs = tf.identity(outputs, \'logits\')\n  return outputs, model.endpoints\n\n\ndef build_model_base(images, model_name, training, override_params=None):\n  """"""Create a base feature network and return the features before pooling.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n  Returns:\n    features: base features before pooling.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  # For backward compatibility.\n  if override_params and override_params.get(\'drop_connect_rate\', None):\n    override_params[\'survival_prob\'] = 1 - override_params[\'drop_connect_rate\']\n\n  blocks_args, global_params = get_model_params(model_name, override_params)\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    features = model(images, training=training, features_only=True)\n\n  features = tf.identity(features, \'features\')\n  return features, model.endpoints\n\n\ndef fusion_two_layer(C_i, P_j, scope):\n    \'\'\'\n    i = j+1\n    :param C_i: shape is [1, h, w, c]\n    :param P_j: shape is [1, h/2, w/2, 256]\n    :return:\n    P_i\n    \'\'\'\n    with tf.variable_scope(scope):\n        level_name = scope.split(\'_\')[1]\n\n        h, w = tf.shape(C_i)[1], tf.shape(C_i)[2]\n        upsample_p = tf.image.resize_bilinear(P_j,\n                                              size=[h, w],\n                                              name=\'up_sample_\'+level_name)\n\n        reduce_dim_c = slim.conv2d(C_i,\n                                   num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                                   kernel_size=[1, 1], stride=1,\n                                   scope=\'reduce_dim_\'+level_name)\n\n        add_f = 0.5*upsample_p + 0.5*reduce_dim_c\n\n        # P_i = slim.conv2d(add_f,\n        #                   num_outputs=256, kernel_size=[3, 3], stride=1,\n        #                   padding=\'SAME\',\n        #                   scope=\'fusion_\'+level_name)\n        return add_f\n\n\ndef build_model_fpn_base(images, model_name, training, override_params=None):\n    _, feature_dict = build_model_base(images, model_name, training, override_params=override_params)\n\n    pyramid_dict = {}\n    with tf.variable_scope(\'build_pyramid\'):\n        with slim.arg_scope([slim.conv2d], weights_regularizer=slim.l2_regularizer(cfgs.WEIGHT_DECAY),\n                            activation_fn=None, normalizer_fn=None):\n\n            P5 = slim.conv2d(feature_dict[\'reduction_5\'],\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[1, 1],\n                             stride=1, scope=\'build_P5\')\n\n            pyramid_dict[\'P5\'] = P5\n\n            for level in range(4, 2, -1):  # build [P4, P3]\n\n                pyramid_dict[\'P%d\' % level] = fusion_two_layer(C_i=feature_dict[""reduction_%d"" % level],\n                                                               P_j=pyramid_dict[""P%d"" % (level + 1)],\n                                                               scope=\'build_P%d\' % level)\n            for level in range(5, 2, -1):\n                pyramid_dict[\'P%d\' % level] = slim.conv2d(pyramid_dict[\'P%d\' % level],\n                                                          num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\n                                                              \'fpn_num_filters\'],\n                                                          kernel_size=[3, 3], padding=""SAME"",\n                                                          stride=1, scope=""fuse_P%d"" % level)\n\n            p6 = slim.conv2d(pyramid_dict[\'P5\'] if cfgs.USE_P5 else feature_dict[\'reduction_5\'],\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p6_conv\')\n            pyramid_dict[\'P6\'] = p6\n\n            p7 = tf.nn.relu(p6, name=\'p6_relu\')\n\n            p7 = slim.conv2d(p7,\n                             num_outputs=cfgs.efficientdet_model_param_dict[cfgs.NET_NAME][\'fpn_num_filters\'],\n                             kernel_size=[3, 3], padding=""SAME"",\n                             stride=2, scope=\'p7_conv\')\n\n            pyramid_dict[\'P7\'] = p7\n\n    # for level in range(7, 1, -1):\n    #     add_heatmap(pyramid_dict[\'P%d\' % level], name=\'Layer%d/P%d_heat\' % (level, level))\n\n    return pyramid_dict'"
libs/networks/efficientnet/efficientnet_model.py,41,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for EfficientNet model.\n[1] Mingxing Tan, Quoc V. Le\n  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n  ICML\'19, https://arxiv.org/abs/1905.11946\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport math\n\nfrom absl import logging\nimport numpy as np\nimport six\nimport tensorflow as tf\n\nfrom libs.networks.efficientnet import utils\nfrom libs.networks.efficientnet.condconv import condconv_layers\n\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\', \'data_format\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\', \'depth_divisor\',\n    \'min_depth\', \'survival_prob\', \'relu_fn\', \'batch_norm\', \'use_se\',\n    \'local_pooling\', \'condconv_num_experts\', \'clip_projection_output\',\n    \'blocks_args\', \'fix_head_stem\',\n])\n# Note: the default value of None is not necessarily valid. It is valid to leave\n# width_coefficient, depth_coefficient at None, which is treated as 1.0 (and\n# which also allows depth_divisor and min_depth to be left at None).\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'strides\', \'se_ratio\', \'conv_type\', \'fused_conv\',\n    \'super_pixel\', \'condconv\'\n])\n# defaults will be a public argument for namedtuple in Python 3.7\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef conv_kernel_initializer(shape, dtype=None, partition_info=None):\n  """"""Initialization for convolutional kernels.\n  The main difference with tf.variance_scaling_initializer is that\n  tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n  standard deviation, whereas here we use a normal distribution. Similarly,\n  tf.initializers.variance_scaling uses a truncated normal with\n  a corrected standard deviation.\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused\n  Returns:\n    an initialization for the variable\n  """"""\n  del partition_info\n  kernel_height, kernel_width, _, out_filters = shape\n  fan_out = int(kernel_height * kernel_width * out_filters)\n  return tf.random_normal(\n      shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)\n\n\ndef dense_kernel_initializer(shape, dtype=None, partition_info=None):\n  """"""Initialization for dense kernels.\n  This initialization is equal to\n    tf.variance_scaling_initializer(scale=1.0/3.0, mode=\'fan_out\',\n                                    distribution=\'uniform\').\n  It is written out explicitly here for clarity.\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused\n  Returns:\n    an initialization for the variable\n  """"""\n  del partition_info\n  init_range = 1.0 / np.sqrt(shape[1])\n  return tf.random_uniform(shape, -init_range, init_range, dtype=dtype)\n\n\ndef superpixel_kernel_initializer(shape, dtype=\'float32\', partition_info=None):\n  """"""Initializes superpixel kernels.\n  This is inspired by space-to-depth transformation that is mathematically\n  equivalent before and after the transformation. But we do the space-to-depth\n  via a convolution. Moreover, we make the layer trainable instead of direct\n  transform, we can initialization it this way so that the model can learn not\n  to do anything but keep it mathematically equivalent, when improving\n  performance.\n  Args:\n    shape: shape of variable\n    dtype: dtype of variable\n    partition_info: unused\n  Returns:\n    an initialization for the variable\n  """"""\n  del partition_info\n  #  use input depth to make superpixel kernel.\n  depth = shape[-2]\n  filters = np.zeros([2, 2, depth, 4 * depth], dtype=dtype)\n  i = np.arange(2)\n  j = np.arange(2)\n  k = np.arange(depth)\n  mesh = np.array(np.meshgrid(i, j, k)).T.reshape(-1, 3).T\n  filters[\n      mesh[0],\n      mesh[1],\n      mesh[2],\n      4 * mesh[2] + 2 * mesh[0] + mesh[1]] = 1\n  return filters\n\n\ndef round_filters(filters, global_params, skip=False):\n  """"""Round number of filters based on depth multiplier.""""""\n  orig_f = filters\n  multiplier = global_params.width_coefficient\n  divisor = global_params.depth_divisor\n  min_depth = global_params.min_depth\n  if skip or not multiplier:\n    return filters\n\n  filters *= multiplier\n  min_depth = min_depth or divisor\n  new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_filters < 0.9 * filters:\n    new_filters += divisor\n  logging.info(\'round_filter input=%s output=%s\', orig_f, new_filters)\n  return int(new_filters)\n\n\ndef round_repeats(repeats, global_params, skip=False):\n  """"""Round number of filters based on depth multiplier.""""""\n  multiplier = global_params.depth_coefficient\n  if skip or not multiplier:\n    return repeats\n  return int(math.ceil(multiplier * repeats))\n\n\nclass MBConvBlock(tf.keras.layers.Layer):\n  """"""A class of MBConv: Mobile Inverted Residual Bottleneck.\n  Attributes:\n    endpoints: dict. A list of internal tensors.\n  """"""\n\n  def __init__(self, block_args, global_params):\n    """"""Initializes a MBConv block.\n    Args:\n      block_args: BlockArgs, arguments to create a Block.\n      global_params: GlobalParams, a set of global parameters.\n    """"""\n    super(MBConvBlock, self).__init__()\n    self._block_args = block_args\n    self._local_pooling = global_params.local_pooling\n    self._batch_norm_momentum = global_params.batch_norm_momentum\n    self._batch_norm_epsilon = global_params.batch_norm_epsilon\n    self._batch_norm = global_params.batch_norm\n    self._condconv_num_experts = global_params.condconv_num_experts\n    self._data_format = global_params.data_format\n    if self._data_format == \'channels_first\':\n      self._channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      self._channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    self._relu_fn = global_params.relu_fn or tf.nn.swish\n    self._has_se = (\n        global_params.use_se and self._block_args.se_ratio is not None and\n        0 < self._block_args.se_ratio <= 1)\n\n    self._clip_projection_output = global_params.clip_projection_output\n\n    self.endpoints = None\n\n    self.conv_cls = tf.layers.Conv2D\n    self.depthwise_conv_cls = utils.DepthwiseConv2D\n    if self._block_args.condconv:\n      self.conv_cls = functools.partial(\n          condconv_layers.CondConv2D, num_experts=self._condconv_num_experts)\n      self.depthwise_conv_cls = functools.partial(\n          condconv_layers.DepthwiseCondConv2D,\n          num_experts=self._condconv_num_experts)\n\n    # Builds the block accordings to arguments.\n    self._build()\n\n  def block_args(self):\n    return self._block_args\n\n  def _build(self):\n    """"""Builds block according to the arguments.""""""\n    if self._block_args.super_pixel == 1:\n      self._superpixel = tf.layers.Conv2D(\n          self._block_args.input_filters,\n          kernel_size=[2, 2],\n          strides=[2, 2],\n          kernel_initializer=conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=False)\n      self._bnsp = self._batch_norm(\n          axis=self._channel_axis,\n          momentum=self._batch_norm_momentum,\n          epsilon=self._batch_norm_epsilon)\n\n    if self._block_args.condconv:\n      # Add the example-dependent routing function\n      self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n          data_format=self._data_format)\n      self._routing_fn = tf.layers.Dense(\n          self._condconv_num_experts, activation=tf.nn.sigmoid)\n\n    filters = self._block_args.input_filters * self._block_args.expand_ratio\n    kernel_size = self._block_args.kernel_size\n\n    # Fused expansion phase. Called if using fused convolutions.\n    self._fused_conv = self.conv_cls(\n        filters=filters,\n        kernel_size=[kernel_size, kernel_size],\n        strides=self._block_args.strides,\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n\n    # Expansion phase. Called if not using fused convolutions and expansion\n    # phase is necessary.\n    self._expand_conv = self.conv_cls(\n        filters=filters,\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n    # Depth-wise convolution phase. Called if not using fused convolutions.\n    self._depthwise_conv = self.depthwise_conv_cls(\n        kernel_size=[kernel_size, kernel_size],\n        strides=self._block_args.strides,\n        depthwise_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n\n    self._bn1 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n    if self._has_se:\n      num_reduced_filters = max(\n          1, int(self._block_args.input_filters * self._block_args.se_ratio))\n      # Squeeze and Excitation layer.\n      self._se_reduce = tf.layers.Conv2D(\n          num_reduced_filters,\n          kernel_size=[1, 1],\n          strides=[1, 1],\n          kernel_initializer=conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n      self._se_expand = tf.layers.Conv2D(\n          filters,\n          kernel_size=[1, 1],\n          strides=[1, 1],\n          kernel_initializer=conv_kernel_initializer,\n          padding=\'same\',\n          data_format=self._data_format,\n          use_bias=True)\n\n    # Output phase.\n    filters = self._block_args.output_filters\n    self._project_conv = self.conv_cls(\n        filters=filters,\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._data_format,\n        use_bias=False)\n    self._bn2 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n  def _call_se(self, input_tensor):\n    """"""Call Squeeze and Excitation layer.\n    Args:\n      input_tensor: Tensor, a single input tensor for Squeeze/Excitation layer.\n    Returns:\n      A output tensor, which should have the same shape as input.\n    """"""\n    if self._local_pooling:\n      shape = input_tensor.get_shape().as_list()\n      kernel_size = [\n          1, shape[self._spatial_dims[0]], shape[self._spatial_dims[1]], 1]\n      se_tensor = tf.nn.avg_pool(\n          input_tensor,\n          ksize=kernel_size,\n          strides=[1, 1, 1, 1],\n          padding=\'VALID\')\n    else:\n      se_tensor = tf.reduce_mean(\n          input_tensor, self._spatial_dims, keepdims=True)\n    se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))\n    logging.info(\'Built Squeeze and Excitation with tensor shape: %s\',\n                 (se_tensor.shape))\n    return tf.sigmoid(se_tensor) * input_tensor\n\n  def call(self, inputs, training=True, survival_prob=None):\n    """"""Implementation of call().\n    Args:\n      inputs: the inputs tensor.\n      training: boolean, whether the model is constructed for training.\n      survival_prob: float, between 0 to 1, drop connect rate.\n    Returns:\n      A output tensor.\n    """"""\n    logging.info(\'Block input: %s shape: %s\', inputs.name, inputs.shape)\n    logging.info(\'Block input depth: %s output depth: %s\',\n                 self._block_args.input_filters,\n                 self._block_args.output_filters)\n\n    x = inputs\n\n    fused_conv_fn = self._fused_conv\n    expand_conv_fn = self._expand_conv\n    depthwise_conv_fn = self._depthwise_conv\n    project_conv_fn = self._project_conv\n\n    if self._block_args.condconv:\n      pooled_inputs = self._avg_pooling(inputs)\n      routing_weights = self._routing_fn(pooled_inputs)\n      # Capture routing weights as additional input to CondConv layers\n      fused_conv_fn = functools.partial(\n          self._fused_conv, routing_weights=routing_weights)\n      expand_conv_fn = functools.partial(\n          self._expand_conv, routing_weights=routing_weights)\n      depthwise_conv_fn = functools.partial(\n          self._depthwise_conv, routing_weights=routing_weights)\n      project_conv_fn = functools.partial(\n          self._project_conv, routing_weights=routing_weights)\n\n    # creates conv 2x2 kernel\n    if self._block_args.super_pixel == 1:\n      with tf.variable_scope(\'super_pixel\'):\n        x = self._relu_fn(\n            self._bnsp(self._superpixel(x), training=training))\n      logging.info(\n          \'Block start with SuperPixel: %s shape: %s\', x.name, x.shape)\n\n    if self._block_args.fused_conv:\n      # If use fused mbconv, skip expansion and use regular conv.\n      x = self._relu_fn(self._bn1(fused_conv_fn(x), training=training))\n      logging.info(\'Conv2D: %s shape: %s\', x.name, x.shape)\n    else:\n      # Otherwise, first apply expansion and then apply depthwise conv.\n      if self._block_args.expand_ratio != 1:\n        x = self._relu_fn(self._bn0(expand_conv_fn(x), training=training))\n        logging.info(\'Expand: %s shape: %s\', x.name, x.shape)\n\n      x = self._relu_fn(self._bn1(depthwise_conv_fn(x), training=training))\n      logging.info(\'DWConv: %s shape: %s\', x.name, x.shape)\n\n    if self._has_se:\n      with tf.variable_scope(\'se\'):\n        x = self._call_se(x)\n\n    self.endpoints = {\'expansion_output\': x}\n\n    x = self._bn2(project_conv_fn(x), training=training)\n    # Add identity so that quantization-aware training can insert quantization\n    # ops correctly.\n    x = tf.identity(x)\n    if self._clip_projection_output:\n      x = tf.clip_by_value(x, -6, 6)\n    if self._block_args.id_skip:\n      if all(\n          s == 1 for s in self._block_args.strides\n      ) and self._block_args.input_filters == self._block_args.output_filters:\n        # Apply only if skip connection presents.\n        if survival_prob:\n          x = utils.drop_connect(x, training, survival_prob)\n        x = tf.add(x, inputs)\n    logging.info(\'Project: %s shape: %s\', x.name, x.shape)\n    return x\n\n\nclass MBConvBlockWithoutDepthwise(MBConvBlock):\n  """"""MBConv-like block without depthwise convolution and squeeze-and-excite.""""""\n\n  def _build(self):\n    """"""Builds block according to the arguments.""""""\n    filters = self._block_args.input_filters * self._block_args.expand_ratio\n    if self._block_args.expand_ratio != 1:\n      # Expansion phase:\n      self._expand_conv = tf.layers.Conv2D(\n          filters,\n          kernel_size=[3, 3],\n          strides=[1, 1],\n          kernel_initializer=conv_kernel_initializer,\n          padding=\'same\',\n          use_bias=False)\n      self._bn0 = self._batch_norm(\n          axis=self._channel_axis,\n          momentum=self._batch_norm_momentum,\n          epsilon=self._batch_norm_epsilon)\n\n    # Output phase:\n    filters = self._block_args.output_filters\n    self._project_conv = tf.layers.Conv2D(\n        filters,\n        kernel_size=[1, 1],\n        strides=self._block_args.strides,\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=self._channel_axis,\n        momentum=self._batch_norm_momentum,\n        epsilon=self._batch_norm_epsilon)\n\n  def call(self, inputs, training=True, survival_prob=None):\n    """"""Implementation of call().\n    Args:\n      inputs: the inputs tensor.\n      training: boolean, whether the model is constructed for training.\n      survival_prob: float, between 0 to 1, drop connect rate.\n    Returns:\n      A output tensor.\n    """"""\n    logging.info(\'Block input: %s shape: %s\', inputs.name, inputs.shape)\n    if self._block_args.expand_ratio != 1:\n      x = self._relu_fn(self._bn0(self._expand_conv(inputs), training=training))\n    else:\n      x = inputs\n    logging.info(\'Expand: %s shape: %s\', x.name, x.shape)\n\n    self.endpoints = {\'expansion_output\': x}\n\n    x = self._bn1(self._project_conv(x), training=training)\n    # Add identity so that quantization-aware training can insert quantization\n    # ops correctly.\n    x = tf.identity(x)\n    if self._clip_projection_output:\n      x = tf.clip_by_value(x, -6, 6)\n\n    if self._block_args.id_skip:\n      if all(\n          s == 1 for s in self._block_args.strides\n      ) and self._block_args.input_filters == self._block_args.output_filters:\n        # Apply only if skip connection presents.\n        if survival_prob:\n          x = utils.drop_connect(x, training, survival_prob)\n        x = tf.add(x, inputs)\n    logging.info(\'Project: %s shape: %s\', x.name, x.shape)\n    return x\n\n\nclass Model(tf.keras.Model):\n  """"""A class implements tf.keras.Model for MNAS-like model.\n    Reference: https://arxiv.org/abs/1807.11626\n  """"""\n\n  def __init__(self, blocks_args=None, global_params=None):\n    """"""Initializes an `Model` instance.\n    Args:\n      blocks_args: A list of BlockArgs to construct block modules.\n      global_params: GlobalParams, a set of global parameters.\n    Raises:\n      ValueError: when blocks_args is not specified as a list.\n    """"""\n    super(Model, self).__init__()\n    if not isinstance(blocks_args, list):\n      raise ValueError(\'blocks_args should be a list.\')\n    self._global_params = global_params\n    self._blocks_args = blocks_args\n    self._relu_fn = global_params.relu_fn or tf.nn.swish\n    self._batch_norm = global_params.batch_norm\n    self._fix_head_stem = global_params.fix_head_stem\n\n    self.endpoints = None\n\n    self._build()\n\n  def _get_conv_block(self, conv_type):\n    conv_block_map = {0: MBConvBlock, 1: MBConvBlockWithoutDepthwise}\n    return conv_block_map[conv_type]\n\n  def _build(self):\n    """"""Builds a model.""""""\n    self._blocks = []\n    batch_norm_momentum = self._global_params.batch_norm_momentum\n    batch_norm_epsilon = self._global_params.batch_norm_epsilon\n    if self._global_params.data_format == \'channels_first\':\n      channel_axis = 1\n      self._spatial_dims = [2, 3]\n    else:\n      channel_axis = -1\n      self._spatial_dims = [1, 2]\n\n    # Stem part.\n    self._conv_stem = tf.layers.Conv2D(\n        filters=round_filters(32, self._global_params, self._fix_head_stem),\n        kernel_size=[3, 3],\n        strides=[2, 2],\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._global_params.data_format,\n        use_bias=False)\n    self._bn0 = self._batch_norm(\n        axis=channel_axis,\n        momentum=batch_norm_momentum,\n        epsilon=batch_norm_epsilon)\n\n    # Builds blocks.\n    for i, block_args in enumerate(self._blocks_args):\n      assert block_args.num_repeat > 0\n      assert block_args.super_pixel in [0, 1, 2]\n      # Update block input and output filters based on depth multiplier.\n      input_filters = round_filters(block_args.input_filters,\n                                    self._global_params)\n\n      output_filters = round_filters(block_args.output_filters,\n                                     self._global_params)\n      kernel_size = block_args.kernel_size\n      if self._fix_head_stem and (i == 0 or i == len(self._blocks_args) - 1):\n        repeats = block_args.num_repeat\n      else:\n        repeats = round_repeats(block_args.num_repeat, self._global_params)\n      block_args = block_args._replace(\n          input_filters=input_filters,\n          output_filters=output_filters,\n          num_repeat=repeats)\n\n      # The first block needs to take care of stride and filter size increase.\n      conv_block = self._get_conv_block(block_args.conv_type)\n      if not block_args.super_pixel:  #  no super_pixel at all\n        self._blocks.append(conv_block(block_args, self._global_params))\n      else:\n        # if superpixel, adjust filters, kernels, and strides.\n        depth_factor = int(4 / block_args.strides[0] / block_args.strides[1])\n        block_args = block_args._replace(\n            input_filters=block_args.input_filters * depth_factor,\n            output_filters=block_args.output_filters * depth_factor,\n            kernel_size=((block_args.kernel_size + 1) // 2 if depth_factor > 1\n                         else block_args.kernel_size))\n        # if the first block has stride-2 and super_pixel trandformation\n        if (block_args.strides[0] == 2 and block_args.strides[1] == 2):\n          block_args = block_args._replace(strides=[1, 1])\n          self._blocks.append(conv_block(block_args, self._global_params))\n          block_args = block_args._replace(  # sp stops at stride-2\n              super_pixel=0,\n              input_filters=input_filters,\n              output_filters=output_filters,\n              kernel_size=kernel_size)\n        elif block_args.super_pixel == 1:\n          self._blocks.append(conv_block(block_args, self._global_params))\n          block_args = block_args._replace(super_pixel=2)\n        else:\n          self._blocks.append(conv_block(block_args, self._global_params))\n      if block_args.num_repeat > 1:  # rest of blocks with the same block_arg\n        # pylint: disable=protected-access\n        block_args = block_args._replace(\n            input_filters=block_args.output_filters, strides=[1, 1])\n        # pylint: enable=protected-access\n      for _ in range(block_args.num_repeat - 1):\n        self._blocks.append(conv_block(block_args, self._global_params))\n\n    # Head part.\n    self._conv_head = tf.layers.Conv2D(\n        filters=round_filters(1280, self._global_params, self._fix_head_stem),\n        kernel_size=[1, 1],\n        strides=[1, 1],\n        kernel_initializer=conv_kernel_initializer,\n        padding=\'same\',\n        data_format=self._global_params.data_format,\n        use_bias=False)\n    self._bn1 = self._batch_norm(\n        axis=channel_axis,\n        momentum=batch_norm_momentum,\n        epsilon=batch_norm_epsilon)\n\n    self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(\n        data_format=self._global_params.data_format)\n    if self._global_params.num_classes:\n      self._fc = tf.layers.Dense(\n          self._global_params.num_classes,\n          kernel_initializer=dense_kernel_initializer)\n    else:\n      self._fc = None\n\n    if self._global_params.dropout_rate > 0:\n      self._dropout = tf.keras.layers.Dropout(self._global_params.dropout_rate)\n    else:\n      self._dropout = None\n\n  def call(self,\n           inputs,\n           training=True,\n           features_only=None,\n           pooled_features_only=False):\n    """"""Implementation of call().\n    Args:\n      inputs: input tensors.\n      training: boolean, whether the model is constructed for training.\n      features_only: build the base feature network only.\n      pooled_features_only: build the base network for features extraction\n        (after 1x1 conv layer and global pooling, but before dropout and fc\n        head).\n    Returns:\n      output tensors.\n    """"""\n    outputs = None\n    self.endpoints = {}\n    reduction_idx = 0\n    # Calls Stem layers\n    with tf.variable_scope(\'stem\'):\n      outputs = self._relu_fn(\n          self._bn0(self._conv_stem(inputs), training=training))\n    logging.info(\'Built stem layers with output shape: %s\', outputs.shape)\n    self.endpoints[\'stem\'] = outputs\n\n    # Calls blocks.\n    for idx, block in enumerate(self._blocks):\n      is_reduction = False  # reduction flag for blocks after the stem layer\n      # If the first block has super-pixel (space-to-depth) layer, then stem is\n      # the first reduction point.\n      if (block.block_args().super_pixel == 1 and idx == 0):\n        reduction_idx += 1\n        self.endpoints[\'reduction_%s\' % reduction_idx] = outputs\n\n      elif ((idx == len(self._blocks) - 1) or\n            self._blocks[idx + 1].block_args().strides[0] > 1):\n        is_reduction = True\n        reduction_idx += 1\n\n      with tf.variable_scope(\'blocks_%s\' % idx):\n        survival_prob = self._global_params.survival_prob\n        if survival_prob:\n          drop_rate = 1.0 - survival_prob\n          survival_prob = 1.0 - drop_rate * float(idx) / len(self._blocks)\n          logging.info(\'block_%s survival_prob: %s\', idx, survival_prob)\n        outputs = block.call(\n            outputs, training=training, survival_prob=survival_prob)\n        self.endpoints[\'block_%s\' % idx] = outputs\n        if is_reduction:\n          self.endpoints[\'reduction_%s\' % reduction_idx] = outputs\n        if block.endpoints:\n          for k, v in six.iteritems(block.endpoints):\n            self.endpoints[\'block_%s/%s\' % (idx, k)] = v\n            if is_reduction:\n              self.endpoints[\'reduction_%s/%s\' % (reduction_idx, k)] = v\n    self.endpoints[\'features\'] = outputs\n\n    if not features_only:\n      # Calls final layers and returns logits.\n      with tf.variable_scope(\'head\'):\n        outputs = self._relu_fn(\n            self._bn1(self._conv_head(outputs), training=training))\n        self.endpoints[\'head_1x1\'] = outputs\n\n        if self._global_params.local_pooling:\n          shape = outputs.get_shape().as_list()\n          kernel_size = [\n              1, shape[self._spatial_dims[0]], shape[self._spatial_dims[1]], 1]\n          outputs = tf.nn.avg_pool(\n              outputs, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n          self.endpoints[\'pooled_features\'] = outputs\n          if not pooled_features_only:\n            if self._dropout:\n              outputs = self._dropout(outputs, training=training)\n            self.endpoints[\'global_pool\'] = outputs\n            if self._fc:\n              outputs = tf.squeeze(outputs, self._spatial_dims)\n              outputs = self._fc(outputs)\n            self.endpoints[\'head\'] = outputs\n        else:\n          outputs = self._avg_pooling(outputs)\n          self.endpoints[\'pooled_features\'] = outputs\n          if not pooled_features_only:\n            if self._dropout:\n              outputs = self._dropout(outputs, training=training)\n            self.endpoints[\'global_pool\'] = outputs\n            if self._fc:\n              outputs = self._fc(outputs)\n            self.endpoints[\'head\'] = outputs\n    return outputs'"
libs/networks/efficientnet/test.py,11,"b'import cv2\nimport tensorflow as tf\nimport os\nimport sys\n\nsys.path.append(\'../../..\')\nfrom libs.networks.efficientnet import efficientnet_builder\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = \'2\'\n\ndef restore_model(sess, ckpt_dir):\n    """"""Restore variables from checkpoint dir.""""""\n    checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n    ema_vars = tf.trainable_variables() + tf.get_collection(\'moving_vars\')\n    for v in tf.global_variables():\n        if \'moving_mean\' in v.name or \'moving_variance\' in v.name:\n            ema_vars.append(v)\n    ema_vars = list(set(ema_vars))\n    var_dict = ema.variables_to_restore(ema_vars)\n    saver = tf.train.Saver(max_to_keep=1)\n    saver.restore(sess, checkpoint)\n\n\nimages = cv2.imread(\'/data/yangxue/code/R3Det_Tensorflow/libs/networks/efficientnet/panda.jpg\')\nimages = cv2.resize(images, (112, 112))\nimages = tf.expand_dims(tf.constant(images, tf.float32), axis=0)\nfeatures, endpoints = efficientnet_builder.build_model_base(images, \'efficientnet-b0\', training=True)\nprint(endpoints.keys())\n\ninit_op = tf.group(\n            tf.global_variables_initializer(),\n            tf.local_variables_initializer()\n        )\n\ntfconfig = tf.ConfigProto(\n    allow_soft_placement=True, log_device_placement=False)\ntfconfig.gpu_options.allow_growth = True\nwith tf.Session(config=tfconfig) as sess:\n    sess.run(init_op)\n    restore_model(sess, \'/data/yangxue/code/R3Det_Tensorflow/libs/networks/efficientnet/efficientnet-b0\')\n    features_, endpoints_ = sess.run([features, endpoints])\n    print(endpoints[\'reduction_1\'])\n    print(endpoints[\'reduction_2\'])\n    print(endpoints[\'reduction_3\'])\n    print(endpoints[\'reduction_4\'])\n    print(endpoints[\'reduction_5\'])'"
libs/networks/efficientnet/utils.py,50,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport os\nimport sys\n\nfrom absl import logging\nimport numpy as np\nimport tensorflow as tf\n\n# from tensorflow.python.tpu import tpu_function  # pylint:disable=g-direct-tensorflow-import\n\n\ndef build_learning_rate(initial_lr,\n                        global_step,\n                        steps_per_epoch=None,\n                        lr_decay_type=\'exponential\',\n                        decay_factor=0.97,\n                        decay_epochs=2.4,\n                        total_steps=None,\n                        warmup_epochs=5):\n  """"""Build learning rate.""""""\n  if lr_decay_type == \'exponential\':\n    assert steps_per_epoch is not None\n    decay_steps = steps_per_epoch * decay_epochs\n    lr = tf.train.exponential_decay(\n        initial_lr, global_step, decay_steps, decay_factor, staircase=True)\n  elif lr_decay_type == \'cosine\':\n    assert total_steps is not None\n    lr = 0.5 * initial_lr * (\n        1 + tf.cos(np.pi * tf.cast(global_step, tf.float32) / total_steps))\n  elif lr_decay_type == \'constant\':\n    lr = initial_lr\n  else:\n    assert False, \'Unknown lr_decay_type : %s\' % lr_decay_type\n\n  if warmup_epochs:\n    logging.info(\'Learning rate warmup_epochs: %d\', warmup_epochs)\n    warmup_steps = int(warmup_epochs * steps_per_epoch)\n    warmup_lr = (\n        initial_lr * tf.cast(global_step, tf.float32) / tf.cast(\n            warmup_steps, tf.float32))\n    lr = tf.cond(global_step < warmup_steps, lambda: warmup_lr, lambda: lr)\n\n  return lr\n\n\ndef build_optimizer(learning_rate,\n                    optimizer_name=\'rmsprop\',\n                    decay=0.9,\n                    epsilon=0.001,\n                    momentum=0.9):\n  """"""Build optimizer.""""""\n  if optimizer_name == \'sgd\':\n    logging.info(\'Using SGD optimizer\')\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  elif optimizer_name == \'momentum\':\n    logging.info(\'Using Momentum optimizer\')\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate=learning_rate, momentum=momentum)\n  elif optimizer_name == \'rmsprop\':\n    logging.info(\'Using RMSProp optimizer\')\n    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay, momentum,\n                                          epsilon)\n  else:\n    logging.fatal(\'Unknown optimizer: %s\', optimizer_name)\n\n  return optimizer\n\n\n# class TpuBatchNormalization(tf.layers.BatchNormalization):\n#   # class TpuBatchNormalization(tf.layers.BatchNormalization):\n#   """"""Cross replica batch normalization.""""""\n#\n#   def __init__(self, fused=False, **kwargs):\n#     if fused in (True, None):\n#       raise ValueError(\'TpuBatchNormalization does not support fused=True.\')\n#     super(TpuBatchNormalization, self).__init__(fused=fused, **kwargs)\n#\n#   def _cross_replica_average(self, t, num_shards_per_group):\n#     """"""Calculates the average value of input tensor across TPU replicas.""""""\n#     num_shards = tpu_function.get_tpu_context().number_of_shards\n#     group_assignment = None\n#     if num_shards_per_group > 1:\n#       if num_shards % num_shards_per_group != 0:\n#         raise ValueError(\'num_shards: %d mod shards_per_group: %d, should be 0\'\n#                          % (num_shards, num_shards_per_group))\n#       num_groups = num_shards // num_shards_per_group\n#       group_assignment = [[\n#           x for x in range(num_shards) if x // num_shards_per_group == y\n#       ] for y in range(num_groups)]\n#     return tf.tpu.cross_replica_sum(t, group_assignment) / tf.cast(\n#         num_shards_per_group, t.dtype)\n#\n#   def _moments(self, inputs, reduction_axes, keep_dims):\n#     """"""Compute the mean and variance: it overrides the original _moments.""""""\n#     shard_mean, shard_variance = super(TpuBatchNormalization, self)._moments(\n#         inputs, reduction_axes, keep_dims=keep_dims)\n#\n#     num_shards = tpu_function.get_tpu_context().number_of_shards or 1\n#     if num_shards <= 8:  # Skip cross_replica for 2x2 or smaller slices.\n#       num_shards_per_group = 1\n#     else:\n#       num_shards_per_group = max(8, num_shards // 8)\n#     logging.info(\'TpuBatchNormalization with num_shards_per_group %s\',\n#                  num_shards_per_group)\n#     if num_shards_per_group > 1:\n#       # Compute variance using: Var[X]= E[X^2] - E[X]^2.\n#       shard_square_of_mean = tf.math.square(shard_mean)\n#       shard_mean_of_square = shard_variance + shard_square_of_mean\n#       group_mean = self._cross_replica_average(\n#           shard_mean, num_shards_per_group)\n#       group_mean_of_square = self._cross_replica_average(\n#           shard_mean_of_square, num_shards_per_group)\n#       group_variance = group_mean_of_square - tf.math.square(group_mean)\n#       return (group_mean, group_variance)\n#     else:\n#       return (shard_mean, shard_variance)\n\n\nclass BatchNormalization(tf.layers.BatchNormalization):\n  """"""Fixed default name of BatchNormalization to match TpuBatchNormalization.""""""\n\n  def __init__(self, name=\'tpu_batch_normalization\', **kwargs):\n    super(BatchNormalization, self).__init__(name=name, **kwargs)\n\n\ndef drop_connect(inputs, is_training, survival_prob):\n  """"""Drop the entire conv with given survival probability.""""""\n  # ""Deep Networks with Stochastic Depth"", https://arxiv.org/pdf/1603.09382.pdf\n  if not is_training:\n    return inputs\n\n  # Compute tensor.\n  batch_size = tf.shape(inputs)[0]\n  random_tensor = survival_prob\n  random_tensor += tf.random_uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)\n  binary_tensor = tf.floor(random_tensor)\n  # Unlike conventional way that multiply survival_prob at test time, here we\n  # divide survival_prob at training time, such that no addition compute is\n  # needed at test time.\n  output = tf.div(inputs, survival_prob) * binary_tensor\n  return output\n\n\ndef archive_ckpt(ckpt_eval, ckpt_objective, ckpt_path):\n  """"""Archive a checkpoint if the metric is better.""""""\n  ckpt_dir, ckpt_name = os.path.split(ckpt_path)\n\n  saved_objective_path = os.path.join(ckpt_dir, \'best_objective.txt\')\n  saved_objective = float(\'-inf\')\n  if tf.gfile.Exists(saved_objective_path):\n    with tf.gfile.GFile(saved_objective_path, \'r\') as f:\n      saved_objective = float(f.read())\n  if saved_objective > ckpt_objective:\n    logging.info(\'Ckpt %s is worse than %s\', ckpt_objective, saved_objective)\n    return False\n\n  filenames = tf.gfile.Glob(ckpt_path + \'.*\')\n  if filenames is None:\n    logging.info(\'No files to copy for checkpoint %s\', ckpt_path)\n    return False\n\n  # Clear the old folder.\n  dst_dir = os.path.join(ckpt_dir, \'archive\')\n  if tf.gfile.Exists(dst_dir):\n    tf.gfile.DeleteRecursively(dst_dir)\n  tf.gfile.MakeDirs(dst_dir)\n\n  # Write checkpoints.\n  for f in filenames:\n    dest = os.path.join(dst_dir, os.path.basename(f))\n    tf.gfile.Copy(f, dest, overwrite=True)\n  ckpt_state = tf.train.generate_checkpoint_state_proto(\n      dst_dir,\n      model_checkpoint_path=ckpt_name,\n      all_model_checkpoint_paths=[ckpt_name])\n  with tf.gfile.GFile(os.path.join(dst_dir, \'checkpoint\'), \'w\') as f:\n    f.write(str(ckpt_state))\n  with tf.gfile.GFile(os.path.join(dst_dir, \'best_eval.txt\'), \'w\') as f:\n    f.write(\'%s\' % ckpt_eval)\n\n  # Update the best objective.\n  with tf.gfile.GFile(saved_objective_path, \'w\') as f:\n    f.write(\'%f\' % ckpt_objective)\n\n  logging.info(\'Copying checkpoint %s to %s\', ckpt_path, dst_dir)\n  return True\n\n\ndef get_ema_vars():\n  """"""Get all exponential moving average (ema) variables.""""""\n  ema_vars = tf.trainable_variables() + tf.get_collection(\'moving_vars\')\n  for v in tf.global_variables():\n    # We maintain mva for batch norm moving mean and variance as well.\n    if \'moving_mean\' in v.name or \'moving_variance\' in v.name:\n      ema_vars.append(v)\n  return list(set(ema_vars))\n\n\nclass DepthwiseConv2D(tf.keras.layers.DepthwiseConv2D, tf.layers.Layer):\n  """"""Wrap keras DepthwiseConv2D to tf.layers.""""""\n\n  pass\n\n\nclass EvalCkptDriver(object):\n  """"""A driver for running eval inference.\n  Attributes:\n    model_name: str. Model name to eval.\n    batch_size: int. Eval batch size.\n    image_size: int. Input image size, determined by model name.\n    num_classes: int. Number of classes, default to 1000 for ImageNet.\n    include_background_label: whether to include extra background label.\n    advprop_preprocessing: whether to use advprop preprocessing.\n  """"""\n\n  def __init__(self,\n               model_name,\n               batch_size=1,\n               image_size=224,\n               num_classes=1000,\n               include_background_label=False,\n               advprop_preprocessing=False):\n    """"""Initialize internal variables.""""""\n    self.model_name = model_name\n    self.batch_size = batch_size\n    self.num_classes = num_classes\n    self.include_background_label = include_background_label\n    self.image_size = image_size\n    self.advprop_preprocessing = advprop_preprocessing\n\n  def restore_model(self, sess, ckpt_dir, enable_ema=True, export_ckpt=None):\n    """"""Restore variables from checkpoint dir.""""""\n    sess.run(tf.global_variables_initializer())\n    checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n    if enable_ema:\n      ema = tf.train.ExponentialMovingAverage(decay=0.0)\n      ema_vars = get_ema_vars()\n      var_dict = ema.variables_to_restore(ema_vars)\n      ema_assign_op = ema.apply(ema_vars)\n    else:\n      var_dict = get_ema_vars()\n      ema_assign_op = None\n\n    tf.train.get_or_create_global_step()\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver(var_dict, max_to_keep=1)\n    saver.restore(sess, checkpoint)\n\n    if export_ckpt:\n      if ema_assign_op is not None:\n        sess.run(ema_assign_op)\n      saver = tf.train.Saver(max_to_keep=1, save_relative_paths=True)\n      saver.save(sess, export_ckpt)\n\n  def build_model(self, features, is_training):\n    """"""Build model with input features.""""""\n    del features, is_training\n    raise ValueError(\'Must be implemented by subclasses.\')\n\n  def get_preprocess_fn(self):\n    raise ValueError(\'Must be implemented by subclsses.\')\n\n  def build_dataset(self, filenames, labels, is_training):\n    """"""Build input dataset.""""""\n    batch_drop_remainder = False\n    if \'condconv\' in self.model_name and not is_training:\n      # CondConv layers can only be called with known batch dimension. Thus, we\n      # must drop all remaining examples that do not make up one full batch.\n      # To ensure all examples are evaluated, use a batch size that evenly\n      # divides the number of files.\n      batch_drop_remainder = True\n      num_files = len(filenames)\n      if num_files % self.batch_size != 0:\n        tf.logging.warn(\'Remaining examples in last batch are not being \'\n                        \'evaluated.\')\n    filenames = tf.constant(filenames)\n    labels = tf.constant(labels)\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n\n    def _parse_function(filename, label):\n      image_string = tf.read_file(filename)\n      preprocess_fn = self.get_preprocess_fn()\n      image_decoded = preprocess_fn(\n          image_string, is_training, image_size=self.image_size)\n      image = tf.cast(image_decoded, tf.float32)\n      return image, label\n\n    dataset = dataset.map(_parse_function)\n    dataset = dataset.batch(self.batch_size,\n                            drop_remainder=batch_drop_remainder)\n\n    iterator = dataset.make_one_shot_iterator()\n    images, labels = iterator.get_next()\n    return images, labels\n\n  def run_inference(self,\n                    ckpt_dir,\n                    image_files,\n                    labels,\n                    enable_ema=True,\n                    export_ckpt=None):\n    """"""Build and run inference on the target images and labels.""""""\n    label_offset = 1 if self.include_background_label else 0\n    with tf.Graph().as_default(), tf.Session() as sess:\n      images, labels = self.build_dataset(image_files, labels, False)\n      probs = self.build_model(images, is_training=False)\n      if isinstance(probs, tuple):\n        probs = probs[0]\n\n      self.restore_model(sess, ckpt_dir, enable_ema, export_ckpt)\n\n      prediction_idx = []\n      prediction_prob = []\n      for _ in range(len(image_files) // self.batch_size):\n        out_probs = sess.run(probs)\n        idx = np.argsort(out_probs)[::-1]\n        prediction_idx.append(idx[:5] - label_offset)\n        prediction_prob.append([out_probs[pid] for pid in idx[:5]])\n\n      # Return the top 5 predictions (idx and prob) for each image.\n      return prediction_idx, prediction_prob\n\n  def eval_example_images(self,\n                          ckpt_dir,\n                          image_files,\n                          labels_map_file,\n                          enable_ema=True,\n                          export_ckpt=None):\n    """"""Eval a list of example images.\n    Args:\n      ckpt_dir: str. Checkpoint directory path.\n      image_files: List[str]. A list of image file paths.\n      labels_map_file: str. The labels map file path.\n      enable_ema: enable expotential moving average.\n      export_ckpt: export ckpt folder.\n    Returns:\n      A tuple (pred_idx, and pred_prob), where pred_idx is the top 5 prediction\n      index and pred_prob is the top 5 prediction probability.\n    """"""\n    classes = json.loads(tf.gfile.Open(labels_map_file).read())\n    pred_idx, pred_prob = self.run_inference(\n        ckpt_dir, image_files, [0] * len(image_files), enable_ema, export_ckpt)\n    for i in range(len(image_files)):\n      print(\'predicted class for image {}: \'.format(image_files[i]))\n      for j, idx in enumerate(pred_idx[i]):\n        print(\'  -> top_{} ({:4.2f}%): {}  \'.format(j, pred_prob[i][j] * 100,\n                                                    classes[str(idx)]))\n    return pred_idx, pred_prob\n\n  def eval_imagenet(self, ckpt_dir, imagenet_eval_glob,\n                    imagenet_eval_label, num_images, enable_ema, export_ckpt):\n    """"""Eval ImageNet images and report top1/top5 accuracy.\n    Args:\n      ckpt_dir: str. Checkpoint directory path.\n      imagenet_eval_glob: str. File path glob for all eval images.\n      imagenet_eval_label: str. File path for eval label.\n      num_images: int. Number of images to eval: -1 means eval the whole\n        dataset.\n      enable_ema: enable expotential moving average.\n      export_ckpt: export checkpoint folder.\n    Returns:\n      A tuple (top1, top5) for top1 and top5 accuracy.\n    """"""\n    imagenet_val_labels = [int(i) for i in tf.gfile.GFile(imagenet_eval_label)]\n    imagenet_filenames = sorted(tf.gfile.Glob(imagenet_eval_glob))\n    if num_images < 0:\n      num_images = len(imagenet_filenames)\n    image_files = imagenet_filenames[:num_images]\n    labels = imagenet_val_labels[:num_images]\n\n    pred_idx, _ = self.run_inference(\n        ckpt_dir, image_files, labels, enable_ema, export_ckpt)\n    top1_cnt, top5_cnt = 0.0, 0.0\n    for i, label in enumerate(labels):\n      top1_cnt += label in pred_idx[i][:1]\n      top5_cnt += label in pred_idx[i][:5]\n      if i % 100 == 0:\n        print(\'Step {}: top1_acc = {:4.2f}%  top5_acc = {:4.2f}%\'.format(\n            i, 100 * top1_cnt / (i + 1), 100 * top5_cnt / (i + 1)))\n        sys.stdout.flush()\n    top1, top5 = 100 * top1_cnt / num_images, 100 * top5_cnt / num_images\n    print(\'Final: top1_acc = {:4.2f}%  top5_acc = {:4.2f}%\'.format(top1, top5))\n    return top1, top5'"
libs/networks/mobilenet/__init__.py,0,b''
libs/networks/mobilenet/conv_blocks.py,14,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolution blocks for mobilenet.""""""\nimport contextlib\nimport functools\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\ndef _split_divisible(num, num_ways, divisible_by=8):\n  """"""Evenly splits num, num_ways so each piece is a multiple of divisible_by.""""""\n  assert num % divisible_by == 0\n  assert num / num_ways >= divisible_by\n  # Note: want to round down, we adjust each split to match the total.\n  base = num // num_ways // divisible_by * divisible_by\n  result = []\n  accumulated = 0\n  for i in range(num_ways):\n    r = base\n    while accumulated + r < num * (i + 1) / num_ways:\n      r += divisible_by\n    result.append(r)\n    accumulated += r\n  assert accumulated == num\n  return result\n\n\n@contextlib.contextmanager\ndef _v1_compatible_scope_naming(scope):\n  if scope is None:  # Create uniqified separable blocks.\n    with tf.variable_scope(None, default_name=\'separable\') as s, \\\n         tf.name_scope(s.original_name_scope):\n      yield \'\'\n  else:\n    # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.\n    # which provide numbered scopes.\n    scope += \'_\'\n    yield scope\n\n\n@slim.add_arg_scope\ndef split_separable_conv2d(input_tensor,\n                           num_outputs,\n                           scope=None,\n                           normalizer_fn=None,\n                           stride=1,\n                           rate=1,\n                           endpoints=None,\n                           use_explicit_padding=False):\n  """"""Separable mobilenet V1 style convolution.\n\n  Depthwise convolution, with default non-linearity,\n  followed by 1x1 depthwise convolution.  This is similar to\n  slim.separable_conv2d, but differs in tha it applies batch\n  normalization and non-linearity to depthwise. This  matches\n  the basic building of Mobilenet Paper\n  (https://arxiv.org/abs/1704.04861)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs\n    scope: optional name of the scope. Note if provided it will use\n    scope_depthwise for deptwhise, and scope_pointwise for pointwise.\n    normalizer_fn: which normalizer function to use for depthwise/pointwise\n    stride: stride\n    rate: output rate (also known as dilation rate)\n    endpoints: optional, if provided, will export additional tensors to it.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n\n  Returns:\n    output tesnor\n  """"""\n\n  with _v1_compatible_scope_naming(scope) as scope:\n    dw_scope = scope + \'depthwise\'\n    endpoints = endpoints if endpoints is not None else {}\n    kernel_size = [3, 3]\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n      input_tensor = _fixed_padding(input_tensor, kernel_size, rate)\n    net = slim.separable_conv2d(\n        input_tensor,\n        None,\n        kernel_size,\n        depth_multiplier=1,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=dw_scope)\n\n    endpoints[dw_scope] = net\n\n    pw_scope = scope + \'pointwise\'\n    net = slim.conv2d(\n        net,\n        num_outputs, [1, 1],\n        stride=1,\n        normalizer_fn=normalizer_fn,\n        scope=pw_scope)\n    endpoints[pw_scope] = net\n  return net\n\n\ndef expand_input_by_factor(n, divisible_by=8):\n  return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)\n\n\n@slim.add_arg_scope\ndef expanded_conv(input_tensor,\n                  num_outputs,\n                  expansion_size=expand_input_by_factor(6),\n                  stride=1,\n                  rate=1,\n                  kernel_size=(3, 3),\n                  residual=True,\n                  normalizer_fn=None,\n                  split_projection=1,\n                  split_expansion=1,\n                  expansion_transform=None,\n                  depthwise_location=\'expansion\',\n                  depthwise_channel_multiplier=1,\n                  endpoints=None,\n                  use_explicit_padding=False,\n                  scope=None):\n  """"""Depthwise Convolution Block with expansion.\n\n  Builds a composite convolution that has the following structure\n  expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)\n\n  Args:\n    input_tensor: input\n    num_outputs: number of outputs in the final layer.\n    expansion_size: the size of expansion, could be a constant or a callable.\n      If latter it will be provided \'num_inputs\' as an input. For forward\n      compatibility it should accept arbitrary keyword arguments.\n      Default will expand the input by factor of 6.\n    stride: depthwise stride\n    rate: depthwise rate\n    kernel_size: depthwise kernel\n    residual: whether to include residual connection between input\n      and output.\n    normalizer_fn: batchnorm or otherwise\n    split_projection: how many ways to split projection operator\n      (that is conv expansion->bottleneck)\n    split_expansion: how many ways to split expansion op\n      (that is conv bottleneck->expansion) ops will keep depth divisible\n      by this value.\n    expansion_transform: Optional function that takes expansion\n      as a single input and returns output.\n    depthwise_location: where to put depthwise covnvolutions supported\n      values None, \'input\', \'output\', \'expansion\'\n    depthwise_channel_multiplier: depthwise channel multiplier:\n    each input will replicated (with different filters)\n    that many times. So if input had c channels,\n    output will have c x depthwise_channel_multpilier.\n    endpoints: An optional dictionary into which intermediate endpoints are\n      placed. The keys ""expansion_output"", ""depthwise_output"",\n      ""projection_output"" and ""expansion_transform"" are always populated, even\n      if the corresponding functions are not invoked.\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional scope.\n\n  Returns:\n    Tensor of depth num_outputs\n\n  Raises:\n    TypeError: on inval\n  """"""\n  with tf.variable_scope(scope, default_name=\'expanded_conv\') as s, \\\n       tf.name_scope(s.original_name_scope):\n    prev_depth = input_tensor.get_shape().as_list()[3]\n    if  depthwise_location not in [None, \'input\', \'output\', \'expansion\']:\n      raise TypeError(\'%r is unknown value for depthwise_location\' %\n                      depthwise_location)\n    padding = \'SAME\'\n    if use_explicit_padding:\n      padding = \'VALID\'\n    depthwise_func = functools.partial(\n        slim.separable_conv2d,\n        num_outputs=None,\n        kernel_size=kernel_size,\n        depth_multiplier=depthwise_channel_multiplier,\n        stride=stride,\n        rate=rate,\n        normalizer_fn=normalizer_fn,\n        padding=padding,\n        scope=\'depthwise\')\n    # b1 -> b2 * r -> b2\n    #   i -> (o * r) (bottleneck) -> o\n    input_tensor = tf.identity(input_tensor, \'input\')\n    net = input_tensor\n\n    if depthwise_location == \'input\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(expansion_size):\n      inner_size = expansion_size(num_inputs=prev_depth)\n    else:\n      inner_size = expansion_size\n\n    if inner_size > net.shape[3]:\n      net = split_conv(\n          net,\n          inner_size,\n          num_ways=split_expansion,\n          scope=\'expand\',\n          stride=1,\n          normalizer_fn=normalizer_fn)\n      net = tf.identity(net, \'expansion_output\')\n    if endpoints is not None:\n      endpoints[\'expansion_output\'] = net\n\n    if depthwise_location == \'expansion\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net)\n\n    net = tf.identity(net, name=\'depthwise_output\')\n    if endpoints is not None:\n      endpoints[\'depthwise_output\'] = net\n    if expansion_transform:\n      net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)\n    # Note in contrast with expansion, we always have\n    # projection to produce the desired output size.\n    net = split_conv(\n        net,\n        num_outputs,\n        num_ways=split_projection,\n        stride=1,\n        scope=\'project\',\n        normalizer_fn=normalizer_fn,\n        activation_fn=tf.identity)\n    if endpoints is not None:\n      endpoints[\'projection_output\'] = net\n    if depthwise_location == \'output\':\n      if use_explicit_padding:\n        net = _fixed_padding(net, kernel_size, rate)\n      net = depthwise_func(net, activation_fn=None)\n\n    if callable(residual):  # custom residual\n      net = residual(input_tensor=input_tensor, output_tensor=net)\n    elif (residual and\n          # stride check enforces that we don\'t add residuals when spatial\n          # dimensions are None\n          stride == 1 and\n          # Depth matches\n          net.get_shape().as_list()[3] ==\n          input_tensor.get_shape().as_list()[3]):\n      net += input_tensor\n    return tf.identity(net, name=\'output\')\n\n\ndef split_conv(input_tensor,\n               num_outputs,\n               num_ways,\n               scope,\n               divisible_by=8,\n               **kwargs):\n  """"""Creates a split convolution.\n\n  Split convolution splits the input and output into\n  \'num_blocks\' blocks of approximately the same size each,\n  and only connects $i$-th input to $i$ output.\n\n  Args:\n    input_tensor: input tensor\n    num_outputs: number of output filters\n    num_ways: num blocks to split by.\n    scope: scope for all the operators.\n    divisible_by: make sure that every part is divisiable by this.\n    **kwargs: will be passed directly into conv2d operator\n  Returns:\n    tensor\n  """"""\n  b = input_tensor.get_shape().as_list()[3]\n\n  if num_ways == 1 or min(b // num_ways,\n                          num_outputs // num_ways) < divisible_by:\n    # Don\'t do any splitting if we end up with less than 8 filters\n    # on either side.\n    return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)\n\n  outs = []\n  input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)\n  output_splits = _split_divisible(\n      num_outputs, num_ways, divisible_by=divisible_by)\n  inputs = tf.split(input_tensor, input_splits, axis=3, name=\'split_\' + scope)\n  base = scope\n  for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):\n    scope = base + \'_part_%d\' % (i,)\n    n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)\n    n = tf.identity(n, scope + \'_output\')\n    outs.append(n)\n  return tf.concat(outs, 3, name=scope + \'_concat\')\n'"
libs/networks/mobilenet/mobilenet.py,17,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Mobilenet Base Class.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport collections\nimport contextlib\nimport copy\nimport os\n\nimport tensorflow as tf\n\n\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef apply_activation(x, name=None, activation_fn=None):\n  return activation_fn(x, name=name) if activation_fn else x\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Pads the input such that if it was used in a convolution with \'VALID\' padding,\n  the output would have the same dimensions as if the unpadded input was used\n  in a convolution with \'SAME\' padding.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = [kernel_size[0] + (kernel_size[0] - 1) * (rate - 1),\n                           kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)]\n  pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]\n  pad_beg = [pad_total[0] // 2, pad_total[1] // 2]\n  pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],\n                                  [pad_beg[1], pad_end[1]], [0, 0]])\n  return padded_inputs\n\n\ndef _make_divisible(v, divisor, min_value=None):\n  if min_value is None:\n    min_value = divisor\n  new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n  # Make sure that round down does not go down by more than 10%.\n  if new_v < 0.9 * v:\n    new_v += divisor\n  return new_v\n\n\n@contextlib.contextmanager\ndef _set_arg_scope_defaults(defaults):\n  """"""Sets arg scope defaults for all items present in defaults.\n\n  Args:\n    defaults: dictionary/list of pairs, containing a mapping from\n    function to a dictionary of default args.\n\n  Yields:\n    context manager where all defaults are set.\n  """"""\n  if hasattr(defaults, \'items\'):\n    items = defaults.items()\n  else:\n    items = defaults\n  if not items:\n    yield\n  else:\n    items = list(items)\n    func, default_arg = items[0]\n    with slim.arg_scope(func, **default_arg):\n      with _set_arg_scope_defaults(items[1:]):\n        yield\n\n\n@slim.add_arg_scope\ndef depth_multiplier(output_params,\n                     multiplier,\n                     divisible_by=8,\n                     min_depth=8,\n                     **unused_kwargs):\n  if \'num_outputs\' not in output_params:\n    return\n  d = output_params[\'num_outputs\']\n  output_params[\'num_outputs\'] = _make_divisible(d * multiplier, divisible_by,\n                                                 min_depth)\n\n\n_Op = collections.namedtuple(\'Op\', [\'op\', \'params\', \'multiplier_func\'])\n\n\ndef op(opfunc, **params):\n  multiplier = params.pop(\'multiplier_transorm\', depth_multiplier)\n  return _Op(opfunc, params=params, multiplier_func=multiplier)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(  # pylint: disable=invalid-name\n    inputs,\n    conv_defs,\n    multiplier=1.0,\n    final_endpoint=None,\n    output_stride=None,\n    use_explicit_padding=False,\n    scope=None,\n    is_training=False):\n  """"""Mobilenet base network.\n\n  Constructs a network from inputs to the given final endpoint. By default\n  the network is constructed in inference mode. To create network\n  in training mode use:\n\n  with slim.arg_scope(mobilenet.training_scope()):\n     logits, endpoints = mobilenet_base(...)\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    conv_defs: A list of op(...) layers specifying the net architecture.\n    multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    final_endpoint: The name of last layer, for early termination for\n    for V1-based networks: last layer is ""layer_14"", for V2: ""layer_20""\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 1 or any even number, excluding\n      zero. Typical values are 8 (accurate fully convolutional mode), 16\n      (fast fully convolutional mode), and 32 (classification mode).\n\n      NOTE- output_stride relies on all consequent operators to support dilated\n      operators via ""rate"" parameter. This might require wrapping non-conv\n      operators to operate properly.\n\n    use_explicit_padding: Use \'VALID\' padding for convolutions, but prepad\n      inputs so that the output dimensions are the same as if \'SAME\' padding\n      were used.\n    scope: optional variable scope.\n    is_training: How to setup batch_norm and other ops. Note: most of the time\n      this does not need be set directly. Use mobilenet.training_scope() to set\n      up training instead. This parameter is here for backward compatibility\n      only. It is safe to set it to the value matching\n      training_scope(is_training=...). It is also safe to explicitly set\n      it to False, even if there is outer training_scope set to to training.\n      (The network will be built in inference mode).\n  Returns:\n    tensor_out: output tensor.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  if multiplier <= 0:\n    raise ValueError(\'multiplier is not greater than zero.\')\n\n  # Set conv defs defaults and overrides.\n  conv_defs_defaults = conv_defs.get(\'defaults\', {})\n  conv_defs_overrides = conv_defs.get(\'overrides\', {})\n  if use_explicit_padding:\n    conv_defs_overrides = copy.deepcopy(conv_defs_overrides)\n    conv_defs_overrides[\n        (slim.conv2d, slim.separable_conv2d)] = {\'padding\': \'VALID\'}\n\n  if output_stride is not None:\n    if output_stride == 0 or (output_stride > 1 and output_stride % 2):\n      raise ValueError(\'Output stride must be None, 1 or a multiple of 2.\')\n\n  # a) Set the tensorflow scope\n  # b) set padding to default: note we might consider removing this\n  # since it is also set by mobilenet_scope\n  # c) set all defaults\n  # d) set all extra overrides.\n  with _scope_all(scope, default_scope=\'Mobilenet\'), \\\n      slim.arg_scope([slim.batch_norm], is_training=is_training), \\\n      _set_arg_scope_defaults(conv_defs_defaults), \\\n      _set_arg_scope_defaults(conv_defs_overrides):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    # Insert default parameters before the base scope which includes\n    # any custom overrides set in mobilenet.\n    end_points = {}\n    scopes = {}\n    for i, opdef in enumerate(conv_defs[\'spec\']):\n      params = dict(opdef.params)\n      opdef.multiplier_func(params, multiplier)\n      stride = params.get(\'stride\', 1)\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= stride\n      else:\n        layer_stride = stride\n        layer_rate = 1\n        current_stride *= stride\n      # Update params.\n      params[\'stride\'] = layer_stride\n      # Only insert rate to params if rate > 1.\n      if layer_rate > 1:\n        params[\'rate\'] = layer_rate\n      # Set padding\n      if use_explicit_padding:\n        if \'kernel_size\' in params:\n          net = _fixed_padding(net, params[\'kernel_size\'], layer_rate)\n        else:\n          params[\'use_explicit_padding\'] = True\n\n      end_point = \'layer_%d\' % (i + 1)\n      try:\n        net = opdef.op(net, **params)\n      except Exception:\n        print(\'Failed to create op %i: %r params: %r\' % (i, opdef, params))\n        raise\n      end_points[end_point] = net\n      scope = os.path.dirname(net.name)\n      scopes[scope] = end_point\n      if final_endpoint is not None and end_point == final_endpoint:\n        break\n\n    # Add all tensors that end with \'output\' to\n    # endpoints\n    for t in net.graph.get_operations():\n      scope = os.path.dirname(t.name)\n      bn = os.path.basename(t.name)\n      if scope in scopes and t.name.endswith(\'output\'):\n        end_points[scopes[scope] + \'/\' + bn] = t.outputs[0]\n    return net, end_points\n\n\n@contextlib.contextmanager\ndef _scope_all(scope, default_scope=None):\n  with tf.variable_scope(scope, default_name=default_scope) as s,\\\n       tf.name_scope(s.original_name_scope):\n    yield s\n\n\n@slim.add_arg_scope\ndef mobilenet(inputs,\n              num_classes=1001,\n              prediction_fn=slim.softmax,\n              reuse=None,\n              scope=\'Mobilenet\',\n              base_only=False,\n              **mobilenet_args):\n  """"""Mobilenet model for classification, supports both V1 and V2.\n\n  Note: default mode is inference, use mobilenet.training_scope to create\n  training network.\n\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    prediction_fn: a function to get predictions out of logits\n      (default softmax).\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    base_only: if True will only create the base of the network (no pooling\n    and no logits).\n    **mobilenet_args: passed to mobilenet_base verbatim.\n      - conv_defs: list of conv defs\n      - multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n      - output_stride: will ensure that the last layer has at most total stride.\n      If the architecture calls for more stride than that provided\n      (e.g. output_stride=16, but the architecture has 5 stride=2 operators),\n      it will replace output_stride with fractional convolutions using Atrous\n      Convolutions.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation tensor.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  is_training = mobilenet_args.get(\'is_training\', False)\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Expected rank 4 input, was: %d\' % len(input_shape))\n\n  with tf.variable_scope(scope, \'Mobilenet\', reuse=reuse) as scope:\n    inputs = tf.identity(inputs, \'input\')\n    net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)\n    if base_only:\n      return net, end_points\n\n    net = tf.identity(net, name=\'embedding\')\n\n    with tf.variable_scope(\'Logits\'):\n      net = global_pool(net)\n      end_points[\'global_pool\'] = net\n      if not num_classes:\n        return net, end_points\n      net = slim.dropout(net, scope=\'Dropout\', is_training=is_training)\n      # 1 x 1 x num_classes\n      # Note: legacy scope name.\n      logits = slim.conv2d(\n          net,\n          num_classes, [1, 1],\n          activation_fn=None,\n          normalizer_fn=None,\n          biases_initializer=tf.zeros_initializer(),\n          scope=\'Conv2d_1c_1x1\')\n\n      logits = tf.squeeze(logits, [1, 2])\n\n      logits = tf.identity(logits, name=\'output\')\n    end_points[\'Logits\'] = logits\n    if prediction_fn:\n      end_points[\'Predictions\'] = prediction_fn(logits, \'Predictions\')\n  return logits, end_points\n\n\ndef global_pool(input_tensor, pool_op=tf.nn.avg_pool):\n  """"""Applies avg pool to produce 1x1 output.\n\n  NOTE: This function is funcitonally equivalenet to reduce_mean, but it has\n  baked in average pool which has better support across hardware.\n\n  Args:\n    input_tensor: input tensor\n    pool_op: pooling op (avg pool is default)\n  Returns:\n    a tensor batch_size x 1 x 1 x depth.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size = tf.convert_to_tensor(\n        [1, tf.shape(input_tensor)[1],\n         tf.shape(input_tensor)[2], 1])\n  else:\n    kernel_size = [1, shape[1], shape[2], 1]\n  output = pool_op(\n      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding=\'VALID\')\n  # Recover output shape, for unknown shape.\n  output.set_shape([None, 1, 1, None])\n  return output\n\n\ndef training_scope(is_training=True,\n                   weight_decay=0.00004,\n                   stddev=0.09,\n                   dropout_keep_prob=0.8,\n                   bn_decay=0.997):\n  """"""Defines Mobilenet training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n     # the network created will be trainble with dropout/batch norm\n     # initialized appropriately.\n  Args:\n    is_training: if set to False this will ensure that all customizations are\n    set to non-training mode. This might be helpful for code that is reused\n    across both training/evaluation, but most of the time training_scope with\n    value False is not needed.\n\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: Standard deviation for initialization, if negative uses xavier.\n    dropout_keep_prob: dropout keep probability\n    bn_decay: decay for the batch norm moving averages.\n\n  Returns:\n    An argument scope to use via arg_scope.\n  """"""\n  # Note: do not introduce parameters that would change the inference\n  # model here (for example whether to use bias), modify conv_def instead.\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'decay\': bn_decay,\n  }\n\n  if stddev < 0:\n    weight_intitializer = slim.initializers.xavier_initializer()\n  else:\n    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)\n\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected, slim.separable_conv2d],\n      weights_initializer=weight_intitializer,\n      normalizer_fn=slim.batch_norm), \\\n      slim.arg_scope([mobilenet_base, mobilenet], is_training=is_training),\\\n      slim.arg_scope([slim.batch_norm], **batch_norm_params), \\\n      slim.arg_scope([slim.dropout], is_training=is_training,\n                     keep_prob=dropout_keep_prob), \\\n      slim.arg_scope([slim.conv2d], \\\n                     weights_regularizer=slim.l2_regularizer(weight_decay)), \\\n      slim.arg_scope([slim.separable_conv2d], weights_regularizer=None) as s:\n    return s\n'"
libs/networks/mobilenet/mobilenet_v2.py,4,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of Mobilenet V2.\n\nArchitecture: https://arxiv.org/abs/1801.04381\n\nThe base model gives 72.2% accuracy on ImageNet, with 300MMadds,\n3.4 M parameters.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nimport tensorflow as tf\n\nfrom libs.networks.mobilenet import conv_blocks as ops\nfrom libs.networks.mobilenet import mobilenet as lib\n\nslim = tf.contrib.slim\nop = lib.op\n\nexpand_input = ops.expand_input_by_factor\n\n# pyformat: disable\n# Architecture: https://arxiv.org/abs/1801.04381\nV2_DEF = dict(\n    defaults={\n        # Note: these parameters of batch norm affect the architecture\n        # that\'s why they are here and not in training_scope.\n        (slim.batch_norm,): {\'center\': True, \'scale\': True},\n        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {\n            \'normalizer_fn\': slim.batch_norm, \'activation_fn\': tf.nn.relu6\n        },\n        (ops.expanded_conv,): {\n            \'expansion_size\': expand_input(6),\n            \'split_expansion\': 1,\n            \'normalizer_fn\': slim.batch_norm,\n            \'residual\': True\n        },\n        (slim.conv2d, slim.separable_conv2d): {\'padding\': \'SAME\'}\n    },\n    spec=[\n        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),\n        op(ops.expanded_conv,\n           expansion_size=expand_input(1, divisible_by=1),\n           num_outputs=16),\n        op(ops.expanded_conv, stride=2, num_outputs=24),\n        op(ops.expanded_conv, stride=1, num_outputs=24),\n        op(ops.expanded_conv, stride=2, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=1, num_outputs=32),\n        op(ops.expanded_conv, stride=2, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=64),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=1, num_outputs=96),\n        op(ops.expanded_conv, stride=2, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=160),\n        op(ops.expanded_conv, stride=1, num_outputs=320),\n        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280)\n    ],\n)\n# pyformat: enable\n\n\n@slim.add_arg_scope\ndef mobilenet(input_tensor,\n              num_classes=1001,\n              depth_multiplier=1.0,\n              scope=\'MobilenetV2\',\n              conv_defs=None,\n              finegrain_classification_mode=False,\n              min_depth=None,\n              divisible_by=None,\n              **kwargs):\n  """"""Creates mobilenet V2 network.\n\n  Inference mode is created by default. To create training use training_scope\n  below.\n\n  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n     logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  Args:\n    input_tensor: The input tensor\n    num_classes: number of classes\n    depth_multiplier: The multiplier applied to scale number of\n    channels in each layer. Note: this is called depth multiplier in the\n    paper but the name is kept for consistency with slim\'s model builder.\n    scope: Scope of the operator\n    conv_defs: Allows to override default conv def.\n    finegrain_classification_mode: When set to True, the model\n    will keep the last layer large even for small multipliers. Following\n    https://arxiv.org/abs/1801.04381\n    suggests that it improves performance for ImageNet-type of problems.\n      *Note* ignored if final_endpoint makes the builder exit earlier.\n    min_depth: If provided, will ensure that all layers will have that\n    many channels after application of depth multiplier.\n    divisible_by: If provided will ensure that all layers # channels\n    will be divisible by this number.\n    **kwargs: passed directly to mobilenet.mobilenet:\n      prediciton_fn- what prediction function to use.\n      reuse-: whether to reuse variables (if reuse set to true, scope\n      must be given).\n  Returns:\n    logits/endpoints pair\n\n  Raises:\n    ValueError: On invalid arguments\n  """"""\n  if conv_defs is None:\n    conv_defs = V2_DEF\n  if \'multiplier\' in kwargs:\n    raise ValueError(\'mobilenetv2 doesn\\\'t support generic \'\n                     \'multiplier parameter use ""depth_multiplier"" instead.\')\n  if finegrain_classification_mode:\n    conv_defs = copy.deepcopy(conv_defs)\n    if depth_multiplier < 1:\n      conv_defs[\'spec\'][-1].params[\'num_outputs\'] /= depth_multiplier\n\n  depth_args = {}\n  # NB: do not set depth_args unless they are provided to avoid overriding\n  # whatever default depth_multiplier might have thanks to arg_scope.\n  if min_depth is not None:\n    depth_args[\'min_depth\'] = min_depth\n  if divisible_by is not None:\n    depth_args[\'divisible_by\'] = divisible_by\n\n  with slim.arg_scope((lib.depth_multiplier,), **depth_args):\n    return lib.mobilenet(\n        input_tensor,\n        num_classes=num_classes,\n        conv_defs=conv_defs,\n        scope=scope,\n        multiplier=depth_multiplier,\n        **kwargs)\n\n\n@slim.add_arg_scope\ndef mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):\n  """"""Creates base of the mobilenet (no pooling and no logits) .""""""\n  return mobilenet(input_tensor,\n                   depth_multiplier=depth_multiplier,\n                   base_only=True, **kwargs)\n\n\ndef training_scope(**kwargs):\n  """"""Defines MobilenetV2 training scope.\n\n  Usage:\n     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):\n       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)\n\n  with slim.\n\n  Args:\n    **kwargs: Passed to mobilenet.training_scope. The following parameters\n    are supported:\n      weight_decay- The weight decay to use for regularizing the model.\n      stddev-  Standard deviation for initialization, if negative uses xavier.\n      dropout_keep_prob- dropout keep probability\n      bn_decay- decay for the batch norm moving averages.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v2 model.\n  """"""\n  return lib.training_scope(**kwargs)\n\n\n__all__ = [\'training_scope\', \'mobilenet_base\', \'mobilenet\', \'V2_DEF\']\n'"
libs/networks/mobilenet/mobilenet_v2_test.py,25,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for mobilenet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport copy\nimport tensorflow as tf\nfrom nets.mobilenet import conv_blocks as ops\nfrom nets.mobilenet import mobilenet\nfrom nets.mobilenet import mobilenet_v2\n\n\nslim = tf.contrib.slim\n\n\ndef find_ops(optype):\n  """"""Find ops of a given type in graphdef or a graph.\n\n  Args:\n    optype: operation type (e.g. Conv2D)\n  Returns:\n     List of operations.\n  """"""\n  gd = tf.get_default_graph()\n  return [var for var in gd.get_operations() if var.type == optype]\n\n\nclass MobilenetV2Test(tf.test.TestCase):\n\n  def setUp(self):\n    tf.reset_default_graph()\n\n  def testCreation(self):\n    spec = dict(mobilenet_v2.V2_DEF)\n    _, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n\n    # This is mostly a sanity test. No deep reason for these particular\n    # constants.\n    #\n    # All but first 2 and last one have  two convolutions, and there is one\n    # extra conv that is not in the spec. (logits)\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 2 - 2)\n    # Check that depthwise are exposed.\n    for i in range(2, 17):\n      self.assertIn(\'layer_%d/depthwise_output\' % i, ep)\n\n  def testCreationNoClasses(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    net, ep = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,\n        num_classes=None)\n    self.assertIs(net, ep[\'global_pool\'])\n\n  def testImageSizes(self):\n    for input_size, output_size in [(224, 7), (192, 6), (160, 5),\n                                    (128, 4), (96, 3)]:\n      tf.reset_default_graph()\n      _, ep = mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))\n\n      self.assertEqual(ep[\'layer_18/output\'].get_shape().as_list()[1:3],\n                       [output_size] * 2)\n\n  def testWithSplits(self):\n    spec = copy.deepcopy(mobilenet_v2.V2_DEF)\n    spec[\'overrides\'] = {\n        (ops.expanded_conv,): dict(split_expansion=2),\n    }\n    _, _ = mobilenet.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)\n    num_convs = len(find_ops(\'Conv2D\'))\n    # All but 3 op has 3 conv operatore, the remainign 3 have one\n    # and there is one unaccounted.\n    self.assertEqual(num_convs, len(spec[\'spec\']) * 3 - 5)\n\n  def testWithOutputStride8(self):\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testDivisibleBy(self):\n    tf.reset_default_graph()\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        divisible_by=16,\n        min_depth=32)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    self.assertSameElements([32, 64, 96, 160, 192, 320, 384, 576, 960, 1280,\n                             1001], s)\n\n  def testDivisibleByWithArgScope(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      mobilenet_v2.mobilenet(\n          tf.placeholder(tf.float32, (10, 224, 224, 2)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n      s = set(s)\n      self.assertSameElements(s, [32, 192, 128, 1001])\n\n  def testFineGrained(self):\n    tf.reset_default_graph()\n    # Verifies that depth_multiplier arg scope actually works\n    # if no default min_depth is provided.\n\n    mobilenet_v2.mobilenet(\n        tf.placeholder(tf.float32, (10, 224, 224, 2)),\n        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,\n        finegrain_classification_mode=True)\n    s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops(\'Conv2D\')]\n    s = set(s)\n    # All convolutions will be 8->48, except for the last one.\n    self.assertSameElements(s, [8, 48, 1001, 1280])\n\n  def testMobilenetBase(self):\n    tf.reset_default_graph()\n    # Verifies that mobilenet_base returns pre-pooling layer.\n    with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):\n      net, _ = mobilenet_v2.mobilenet_base(\n          tf.placeholder(tf.float32, (10, 224, 224, 16)),\n          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)\n      self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])\n\n  def testWithOutputStride16(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n  def testWithOutputStride8AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=8,\n        use_explicit_padding=True,\n        scope=\'MobilenetV2\')\n    self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])\n\n  def testWithOutputStride16AndExplicitPadding(self):\n    tf.reset_default_graph()\n    out, _ = mobilenet.mobilenet_base(\n        tf.placeholder(tf.float32, (10, 224, 224, 16)),\n        conv_defs=mobilenet_v2.V2_DEF,\n        output_stride=16,\n        use_explicit_padding=True)\n    self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/__init__.py,0,b'\n'
libs/networks/slim_nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
libs/networks/slim_nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_resnet_v2 import inception_resnet_v2_base\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
libs/networks/slim_nets/inception_resnet_v2.py,41,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2_base(inputs,\n                             final_endpoint=\'Conv2d_7b_1x1\',\n                             output_stride=16,\n                             align_feature_maps=False,\n                             scope=None):\n  """"""Inception model from  http://arxiv.org/abs/1602.07261.\n\n  Constructs an Inception Resnet v2 network from inputs to the given final\n  endpoint. This method can construct the network up to the final inception\n  block Conv2d_7b_1x1.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_6a\', \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    output_stride: A scalar that specifies the requested ratio of input to\n      output spatial resolution. Only supports 8 and 16.\n    align_feature_maps: When true, changes all the VALID paddings in the network\n      to SAME padding so that the feature maps are aligned.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n      or if the output_stride is not 8 or 16, or if the output_stride is 8 and\n      we request an end point after \'PreAuxLogits\'.\n  """"""\n  if output_stride != 8 and output_stride != 16:\n    raise ValueError(\'output_stride must be 8 or 16.\')\n\n  padding = \'SAME\' if align_feature_maps else \'VALID\'\n\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 149 x 149 x 32\n      net = slim.conv2d(inputs, 32, 3, stride=2, padding=padding,\n                        scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 32, 3, padding=padding,\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 73 x 73 x 64\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_3a_3x3\')\n      if add_and_check_final(\'MaxPool_3a_3x3\', net): return net, end_points\n      # 73 x 73 x 80\n      net = slim.conv2d(net, 80, 1, padding=padding,\n                        scope=\'Conv2d_3b_1x1\')\n      if add_and_check_final(\'Conv2d_3b_1x1\', net): return net, end_points\n      # 71 x 71 x 192\n      net = slim.conv2d(net, 192, 3, padding=padding,\n                        scope=\'Conv2d_4a_3x3\')\n      if add_and_check_final(\'Conv2d_4a_3x3\', net): return net, end_points\n      # 35 x 35 x 192\n      net = slim.max_pool2d(net, 3, stride=2, padding=padding,\n                            scope=\'MaxPool_5a_3x3\')\n      if add_and_check_final(\'MaxPool_5a_3x3\', net): return net, end_points\n\n      # 35 x 35 x 320\n      with tf.variable_scope(\'Mixed_5b\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                      scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                      scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                       scope=\'AvgPool_0a_3x3\')\n          tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                     scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(\n            [tower_conv, tower_conv1_1, tower_conv2_2, tower_pool_1], 3)\n\n      if add_and_check_final(\'Mixed_5b\', net): return net, end_points\n      # TODO(alemi): Register intermediate endpoints\n      net = slim.repeat(net, 10, block35, scale=0.17)\n\n      # 17 x 17 x 1088 if output_stride == 8,\n      # 33 x 33 x 1088 if output_stride == 16\n      use_atrous = output_stride == 8\n\n      with tf.variable_scope(\'Mixed_6a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,\n                                   padding=padding,\n                                   scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                      stride=1 if use_atrous else 2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):\n        net = slim.repeat(net, 20, block17, scale=0.10)\n      if add_and_check_final(\'PreAuxLogits\', net): return net, end_points\n\n      if output_stride == 8:\n        # TODO(gpapan): Properly support output_stride for the rest of the net.\n        raise ValueError(\'output_stride==8 is only supported up to the \'\n                         \'PreAuxlogits end_point for now.\')\n\n      # 8 x 8 x 2080\n      with tf.variable_scope(\'Mixed_7a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                     padding=padding,\n                                     scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n          tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                      scope=\'Conv2d_0b_3x3\')\n          tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                      padding=padding,\n                                      scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          tower_pool = slim.max_pool2d(net, 3, stride=2,\n                                       padding=padding,\n                                       scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(\n            [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool], 3)\n\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # TODO(alemi): register intermediate endpoints\n      net = slim.repeat(net, 9, block8, scale=0.20)\n      net = block8(net, activation_fn=None)\n\n      # 8 x 8 x 1536\n      net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n      if add_and_check_final(\'Conv2d_7b_1x1\', net): return net, end_points\n\n    raise ValueError(\'final_endpoint (%s) not recognized\', final_endpoint)\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,#0.8\n                        reuse=None,\n                        scope=\'InceptionResnetV2\',\n                        create_aux_logits=True):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n\n      net, end_points = inception_resnet_v2_base(inputs, scope=scope)\n\n      if create_aux_logits:\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = end_points[\'PreAuxLogits\']\n          aux = slim.avg_pool2d(aux, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                              scope=\'AvgPool_1a_8x8\')\n        net = slim.flatten(net)\n\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'Dropout\')\n\n        end_points[\'PreLogitsFlatten\'] = net\n        # end_points[\'yjr_feature\'] = tf.squeeze(net, axis=0)\n\n        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                      scope=\'Logits\')\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
libs/networks/slim_nets/inception_resnet_v2_test.py,27,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'AuxLogits\' in endpoints)\n      auxlogits = endpoints[\'AuxLogits\']\n      self.assertTrue(\n          auxlogits.op.name.startswith(\'InceptionResnetV2/AuxLogits\'))\n      self.assertListEqual(auxlogits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,\n                                                        create_aux_logits=False)\n      self.assertTrue(\'AuxLogits\' not in endpoints)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_resnet_v2_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'InceptionResnetV2/Conv2d_7b_1x1\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 8, 8, 1536])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                          \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_6a\',\n                 \'PreAuxLogits\', \'Mixed_7a\', \'Conv2d_7b_1x1\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_resnet_v2_base(\n            inputs, final_endpoint=endpoint)\n        if endpoint != \'PreAuxLogits\':\n          self.assertTrue(out_tensor.op.name.startswith(\n              \'InceptionResnetV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 17, 17, 1088],\n                        \'PreAuxLogits\': [5, 17, 17, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithAlignedFeatureMaps(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', align_feature_maps=True)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2a_3x3\': [5, 150, 150, 32],\n                        \'Conv2d_2b_3x3\': [5, 150, 150, 64],\n                        \'MaxPool_3a_3x3\': [5, 75, 75, 64],\n                        \'Conv2d_3b_1x1\': [5, 75, 75, 80],\n                        \'Conv2d_4a_3x3\': [5, 75, 75, 192],\n                        \'MaxPool_5a_3x3\': [5, 38, 38, 192],\n                        \'Mixed_5b\': [5, 38, 38, 320],\n                        \'Mixed_6a\': [5, 19, 19, 1088],\n                        \'PreAuxLogits\': [5, 19, 19, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsUptoPreAuxLogitsWithOutputStrideEight(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_resnet_v2_base(\n        inputs, final_endpoint=\'PreAuxLogits\', output_stride=8)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [5, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [5, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [5, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [5, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [5, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [5, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [5, 35, 35, 192],\n                        \'Mixed_5b\': [5, 35, 35, 320],\n                        \'Mixed_6a\': [5, 33, 33, 1088],\n                        \'PreAuxLogits\': [5, 33, 33, 1088]\n                       }\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_7b_1x1\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
libs/networks/slim_nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim_nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
libs/networks/slim_nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
libs/networks/slim_nets/mobilenet_v1.py,9,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""MobileNet v1.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nhead (for example: embeddings, localization and classification).\n\nAs described in https://arxiv.org/abs/1704.04861.\n\n  MobileNets: Efficient Convolutional Neural Networks for\n    Mobile Vision Applications\n  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,\n    Tobias Weyand, Marco Andreetto, Hartwig Adam\n\n100% Mobilenet V1 (base) with input size 224x224:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112\nMobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224\nMobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224\n--------------------------------------------------------------------------------\nTotal:                                                 3,185,088     567,716,352\n\n\n75% Mobilenet V1 (base) with input size 128x128:\n\nLayer                                                     params           macs\n--------------------------------------------------------------------------------\nMobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208\nMobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736\nMobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592\nMobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368\nMobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592\nMobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736\nMobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184\nMobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184\nMobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592\nMobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368\nMobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184\nMobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592\nMobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592\nMobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184\nMobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184\nMobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184\nMobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184\nMobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296\nMobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592\nMobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592\nMobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184\n--------------------------------------------------------------------------------\nTotal:                                                 1,800,144     106,002,432\n\n""""""\n\n# Tensorflow mandates these.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import namedtuple\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=[3, 3], stride=2, depth=32),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=128),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=256),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=512),\n    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),\n    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)\n]\n\n\ndef mobilenet_v1_base(inputs,\n                      final_endpoint=\'Conv2d_13_pointwise\',\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      conv_defs=None,\n                      output_stride=None,\n                      scope=None):\n  """"""Mobilenet v1.\n\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_0\', \'Conv2d_1_pointwise\', \'Conv2d_2_pointwise\',\n      \'Conv2d_3_pointwise\', \'Conv2d_4_pointwise\', \'Conv2d_5\'_pointwise,\n      \'Conv2d_6_pointwise\', \'Conv2d_7_pointwise\', \'Conv2d_8_pointwise\',\n      \'Conv2d_9_pointwise\', \'Conv2d_10_pointwise\', \'Conv2d_11_pointwise\',\n      \'Conv2d_12_pointwise\', \'Conv2d_13_pointwise\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. Allowed values are 8 (accurate fully convolutional\n      mode), 16 (fast fully convolutional mode), 32 (classification mode).\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0, or the target output_stride is not\n                allowed.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  if conv_defs is None:\n    conv_defs = _CONV_DEFS\n\n  if output_stride is not None and output_stride not in [8, 16, 32]:\n    raise ValueError(\'Only allowed output_stride values are 8, 16, 32.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=\'SAME\'):\n      # The current_stride variable keeps track of the output stride of the\n      # activations, i.e., the running product of convolution strides up to the\n      # current network layer. This allows us to invoke atrous convolution\n      # whenever applying the next convolution would result in the activations\n      # having output stride larger than the target output_stride.\n      current_stride = 1\n\n      # The atrous convolution rate parameter.\n      rate = 1\n\n      net = inputs\n      for i, conv_def in enumerate(conv_defs):\n        end_point_base = \'Conv2d_%d\' % i\n\n        if output_stride is not None and current_stride == output_stride:\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          layer_stride = 1\n          layer_rate = rate\n          rate *= conv_def.stride\n        else:\n          layer_stride = conv_def.stride\n          layer_rate = 1\n          current_stride *= conv_def.stride\n\n        if isinstance(conv_def, Conv):\n          end_point = end_point_base\n          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,\n                            stride=conv_def.stride,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n        elif isinstance(conv_def, DepthSepConv):\n          end_point = end_point_base + \'_depthwise\'\n\n          # By passing filters=None\n          # separable_conv2d produces only a depthwise convolution layer\n          net = slim.separable_conv2d(net, None, conv_def.kernel,\n                                      depth_multiplier=1,\n                                      stride=layer_stride,\n                                      rate=layer_rate,\n                                      normalizer_fn=slim.batch_norm,\n                                      scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n\n          end_point = end_point_base + \'_pointwise\'\n\n          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                            stride=1,\n                            normalizer_fn=slim.batch_norm,\n                            scope=end_point)\n\n          end_points[end_point] = net\n          if end_point == final_endpoint:\n            return net, end_points\n        else:\n          raise ValueError(\'Unknown convolution type %s for layer %d\'\n                           % (conv_def.ltype, i))\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef mobilenet_v1(inputs,\n                 num_classes=1000,\n                 dropout_keep_prob=0.999,\n                 is_training=True,\n                 min_depth=8,\n                 depth_multiplier=1.0,\n                 conv_defs=None,\n                 prediction_fn=tf.contrib.layers.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'MobilenetV1\'):\n  """"""Mobilenet v1 model for classification.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    is_training: whether is training or not.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef namedtuples specifying the net architecture.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: Input rank is invalid.\n  """"""\n  input_shape = inputs.get_shape().as_list()\n  if len(input_shape) != 4:\n    raise ValueError(\'Invalid input tensor rank, expected 4, was: %d\' %\n                     len(input_shape))\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = mobilenet_v1_base(inputs, scope=scope,\n                                          min_depth=min_depth,\n                                          depth_multiplier=depth_multiplier,\n                                          conv_defs=conv_defs)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a\')\n        end_points[\'AvgPool_1a\'] = net\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      if prediction_fn:\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\n\nmobilenet_v1.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ndef mobilenet_v1_arg_scope(is_training=True,\n                           weight_decay=0.00004,\n                           stddev=0.09,\n                           regularize_depthwise=False):\n  """"""Defines the default MobilenetV1 arg scope.\n\n  Args:\n    is_training: Whether or not we\'re training the model.\n    weight_decay: The weight decay to use for regularizing the model.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    regularize_depthwise: Whether or not apply regularization on depthwise.\n\n  Returns:\n    An `arg_scope` to use for the mobilenet v1 model.\n  """"""\n  batch_norm_params = {\n      \'is_training\': is_training,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  if regularize_depthwise:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n'"
libs/networks/slim_nets/mobilenet_v1_test.py,32,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n""""""Tests for MobileNet v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import mobilenet_v1\n\nslim = tf.contrib.slim\n\n\nclass MobilenetV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_13\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                          \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                          \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                          \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                          \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                          \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                          \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                          \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                          \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                          \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                          \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_0\',\n                 \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                 \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                 \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\',\n                 \'Conv2d_4_depthwise\', \'Conv2d_4_pointwise\',\n                 \'Conv2d_5_depthwise\', \'Conv2d_5_pointwise\',\n                 \'Conv2d_6_depthwise\', \'Conv2d_6_pointwise\',\n                 \'Conv2d_7_depthwise\', \'Conv2d_7_pointwise\',\n                 \'Conv2d_8_depthwise\', \'Conv2d_8_pointwise\',\n                 \'Conv2d_9_depthwise\', \'Conv2d_9_pointwise\',\n                 \'Conv2d_10_depthwise\', \'Conv2d_10_pointwise\',\n                 \'Conv2d_11_depthwise\', \'Conv2d_11_pointwise\',\n                 \'Conv2d_12_depthwise\', \'Conv2d_12_pointwise\',\n                 \'Conv2d_13_depthwise\', \'Conv2d_13_pointwise\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'MobilenetV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildCustomNetworkUsingConvDefs(self):\n    batch_size = 5\n    height, width = 224, 224\n    conv_defs = [\n        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),\n        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)\n    ]\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = mobilenet_v1.mobilenet_v1_base(\n        inputs, final_endpoint=\'Conv2d_3_pointwise\', conv_defs=conv_defs)\n    self.assertTrue(net.op.name.startswith(\'MobilenetV1/Conv2d_3\'))\n    self.assertListEqual(net.get_shape().as_list(),\n                         [batch_size, 56, 56, 512])\n    expected_endpoints = [\'Conv2d_0\',\n                          \'Conv2d_1_depthwise\', \'Conv2d_1_pointwise\',\n                          \'Conv2d_2_depthwise\', \'Conv2d_2_pointwise\',\n                          \'Conv2d_3_depthwise\', \'Conv2d_3_pointwise\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 7, 7, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 7, 7, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 16\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 14, 14, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 14, 14, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 14, 14, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 14, 14, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):\n    batch_size = 5\n    height, width = 224, 224\n    output_stride = 8\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, output_stride=output_stride,\n          final_endpoint=\'Conv2d_13_pointwise\')\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_depthwise\': [batch_size, 112, 112, 32],\n                        \'Conv2d_1_pointwise\': [batch_size, 112, 112, 64],\n                        \'Conv2d_2_depthwise\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_depthwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_3_pointwise\': [batch_size, 56, 56, 128],\n                        \'Conv2d_4_depthwise\': [batch_size, 28, 28, 128],\n                        \'Conv2d_4_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_5_pointwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_depthwise\': [batch_size, 28, 28, 256],\n                        \'Conv2d_6_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_7_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_8_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_9_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_10_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_11_pointwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_depthwise\': [batch_size, 28, 28, 512],\n                        \'Conv2d_12_pointwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_depthwise\': [batch_size, 28, 28, 1024],\n                        \'Conv2d_13_pointwise\': [batch_size, 28, 28, 1024]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):\n    batch_size = 5\n    height, width = 128, 128\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      _, end_points = mobilenet_v1.mobilenet_v1_base(\n          inputs, final_endpoint=\'Conv2d_13_pointwise\', depth_multiplier=0.75)\n    # For the Conv2d_0 layer FaceNet has depth=16\n    endpoints_shapes = {\'Conv2d_0\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_depthwise\': [batch_size, 64, 64, 24],\n                        \'Conv2d_1_pointwise\': [batch_size, 64, 64, 48],\n                        \'Conv2d_2_depthwise\': [batch_size, 32, 32, 48],\n                        \'Conv2d_2_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_depthwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_3_pointwise\': [batch_size, 32, 32, 96],\n                        \'Conv2d_4_depthwise\': [batch_size, 16, 16, 96],\n                        \'Conv2d_4_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_depthwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_5_pointwise\': [batch_size, 16, 16, 192],\n                        \'Conv2d_6_depthwise\': [batch_size, 8, 8, 192],\n                        \'Conv2d_6_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_7_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_8_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_9_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_10_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_depthwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_11_pointwise\': [batch_size, 8, 8, 384],\n                        \'Conv2d_12_depthwise\': [batch_size, 4, 4, 384],\n                        \'Conv2d_12_pointwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_depthwise\': [batch_size, 4, 4, 768],\n                        \'Conv2d_13_pointwise\': [batch_size, 4, 4, 768]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name, expected_shape in endpoints_shapes.iteritems():\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                        normalizer_fn=slim.batch_norm):\n      mobilenet_v1.mobilenet_v1_base(inputs)\n      total_params, _ = slim.model_analyzer.analyze_vars(\n          slim.get_model_variables())\n      self.assertAlmostEqual(3217920L, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys() if key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = mobilenet_v1.mobilenet_v1(\n          inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Conv2d_13_pointwise\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Conv2d_13_pointwise\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'MobilenetV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,\n                                          reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = mobilenet_v1.mobilenet_v1(images,\n                                          num_classes=num_classes,\n                                          spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import mobilenet_v1\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet_v1\': mobilenet_v1.mobilenet_v1,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet_v1\': mobilenet_v1.mobilenet_v1_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
libs/networks/slim_nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
libs/networks/slim_nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))\n            rate *= unit.get(\'stride\', 1)\n\n          else:\n            net = block.unit_fn(net, rate=1, **unit)\n            current_stride *= unit.get(\'stride\', 1)\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997, #0.997\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
libs/networks/slim_nets/resnet_v1.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom libs.networks.slim_nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n          # yjr_feature = tf.squeeze(net, [0, 1, 2])\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n\n        ###\n        # end_points[\'yjr_feature\'] = yjr_feature\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v1 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=True,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v1_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
libs/networks/slim_nets/resnet_v1_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    blocks = [\n        resnet_v1.resnet_v1_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v1.resnet_v1_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    block = resnet_v1.resnet_v1_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\n\nTypical use:\n\n   from tensorflow.contrib.slim.slim_nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=False,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        else:\n          logits = net\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(\n            end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_block(scope, base_depth, num_units, stride):\n  """"""Helper function for creating a resnet_v2 bottleneck block.\n\n  Args:\n    scope: The scope of the block.\n    base_depth: The depth of the bottleneck layer for each unit.\n    num_units: The number of units in the block.\n    stride: The stride of the block, implemented as a stride in the last unit.\n      All other units have stride=1.\n\n  Returns:\n    A resnet_v2 bottleneck block.\n  """"""\n  return resnet_utils.Block(scope, bottleneck, [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': 1\n  }] * (num_units - 1) + [{\n      \'depth\': base_depth * 4,\n      \'depth_bottleneck\': base_depth,\n      \'stride\': stride\n  }])\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 spatial_squeeze=False,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=6, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=4, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=23, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=8, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  spatial_squeeze=False,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_v2_block(\'block1\', base_depth=64, num_units=3, stride=2),\n      resnet_v2_block(\'block2\', base_depth=128, num_units=24, stride=2),\n      resnet_v2_block(\'block3\', base_depth=256, num_units=36, stride=2),\n      resnet_v2_block(\'block4\', base_depth=512, num_units=3, stride=1),\n  ]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n                   reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
libs/networks/slim_nets/resnet_v2_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = slim.utils.convert_collection_to_dict(\'end_points\')\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    blocks = [\n        resnet_v2.resnet_v2_block(\n            \'block1\', base_depth=1, num_units=2, stride=2),\n        resnet_v2.resnet_v2_block(\n            \'block2\', base_depth=2, num_units=2, stride=1),\n    ]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net, rate=1, **unit)\n    return net\n\n  def testAtrousValuesBottleneck(self):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n    """"""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=2, stride=2),\n        block(\'block2\', base_depth=2, num_units=2, stride=2),\n        block(\'block3\', base_depth=4, num_units=2, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    block = resnet_v2.resnet_v2_block\n    blocks = [\n        block(\'block1\', base_depth=1, num_units=3, stride=2),\n        block(\'block2\', base_depth=2, num_units=3, stride=2),\n        block(\'block3\', base_depth=4, num_units=3, stride=2),\n        block(\'block4\', base_depth=8, num_units=2, stride=1),\n    ]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/networks/slim_nets/vgg.py,10,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      # yjr_feature = tf.squeeze(net)\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      # end_points[\'yjr_feature\'] = yjr_feature\n      end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
libs/networks/slim_nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.slim_nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
libs/configs/DOTA1.0/baseline/__init__.py,0,b''
libs/configs/DOTA1.0/baseline/cfgs_res101_dota_v9.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + resnet101\n\nThis is your result for task 1:\n\n    mAP: 0.6472587241348037\n    ap of each class: plane:0.8910930708194905,\n    baseball-diamond:0.7768921360180305,\n    bridge:0.4335815026850154,\n    ground-track-field:0.6585617606510761,\n    small-vehicle:0.5589154083405831,\n    large-vehicle:0.4339570933168862,\n    ship:0.5615646017168024,\n    tennis-court:0.9053942464812329,\n    basketball-court:0.7961219077003465,\n    storage-tank:0.794853988825955,\n    soccer-ball-field:0.5868168710684329,\n    roundabout:0.634320160727544,\n    harbor:0.5632418045851224,\n    swimming-pool:0.5785529846522851,\n    helicopter:0.535013324433251\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190604\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190604\'\nNET_NAME = \'resnet101_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res152_dota_v12.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv8 + resnet152\n\nThis is your evaluation result for task 1:\n\n    mAP: 0.6579429843928571\n    ap of each class:\n    plane:0.893503785252709,\n    baseball-diamond:0.7522990449456908,\n    bridge:0.4225903314724291,\n    ground-track-field:0.6176259567961212,\n    small-vehicle:0.665437258817552,\n    large-vehicle:0.5675280986867499,\n    ship:0.6577197671665636,\n    tennis-court:0.9081239749973251,\n    basketball-court:0.7968397419973662,\n    storage-tank:0.7810871388484488,\n    soccer-ball-field:0.5130668716025378,\n    roundabout:0.5987768566798091,\n    harbor:0.5408862301475073,\n    swimming-pool:0.6327828268456754,\n    helicopter:0.520876881636372\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190607_108w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190607\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res152_dota_v18.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nRetinaNet-H + data aug. + res152\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_4x_20200203\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 4\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v1.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.6276150206677873\n    ap of each class:\n    plane:0.8870014421735071,\n    baseball-diamond:0.705325371100668,\n    bridge:0.32269463142075816,\n    ground-track-field:0.6392568489085992,\n    small-vehicle:0.6464182762763873,\n    large-vehicle:0.7101301530966344,\n    ship:0.6862044731417514,\n    tennis-court:0.9032288696055735,\n    basketball-court:0.7489848430301138,\n    storage-tank:0.7666316830070047,\n    soccer-ball-field:0.47762755855141725,\n    roundabout:0.5789849516428702,\n    harbor:0.5351731784765689,\n    swimming-pool:0.5525642677310019,\n    helicopter:0.2539987618539514\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190527_54w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190527\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""2""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4  # * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v10.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv1 + 2x multi-gpu\n\nThis is your evaluation result for task 1:\n\n    mAP: 0.62254218416428\n    ap of each class:\n    plane:0.887945888226154,\n    baseball-diamond:0.6826947930118734,\n    bridge:0.31644198755436914,\n    ground-track-field:0.5657609524817373,\n    small-vehicle:0.6637626685250525,\n    large-vehicle:0.7274283353038937,\n    ship:0.7032336615561822,\n    tennis-court:0.9086156561884718,\n    basketball-court:0.7387389056593121,\n    storage-tank:0.755496208304846,\n    soccer-ball-field:0.44223268652715814,\n    roundabout:0.5790070316108199,\n    harbor:0.5195873812063292,\n    swimming-pool:0.5676670034124572,\n    helicopter:0.27951960289554134\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190605_108w\nUsername: DetectionTeamCSU\nInstitute: CSU\nEmailadress: yangxue@csu.edu.cn\nTeamMembers: YangXue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190605\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4  # * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v13.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv5 repeat\nThis is your result for task 1:\n\n    mAP: 0.6887560239475758\n    ap of each class:\n    plane:0.8940529652135962,\n    baseball-diamond:0.7474486698392037,\n    bridge:0.3643166649129069,\n    ground-track-field:0.662924268693916,\n    small-vehicle:0.6616918317735657,\n    large-vehicle:0.7637966336747759,\n    ship:0.7775309381402764,\n    tennis-court:0.907857598484631,\n    basketball-court:0.8105264053192953,\n    storage-tank:0.8012307433913947,\n    soccer-ball-field:0.5642786809836354,\n    roundabout:0.6258372702184064,\n    harbor:0.6188599073641682,\n    swimming-pool:0.6492325004864525,\n    helicopter:0.4817552807174126\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20191119_54w\nUsername: liuqingiqng\nInstitute: Central South University\nEmailadress: liuqingqing@csu.edu.cn\nTeamMembers: liuqingqing\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20191119\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = 0\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4  # * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v14.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv8 + 800->1000\n\nThis is your evaluation result for task 1:\n\n    mAP: 0.603201528435074\n    ap of each class:\n    plane:0.8846741775683061,\n    baseball-diamond:0.6713744125419954,\n    bridge:0.35893635519325867,\n    ground-track-field:0.530980271953119,\n    small-vehicle:0.592568526848601,\n    large-vehicle:0.49986747358947725,\n    ship:0.6365858633301433,\n    tennis-court:0.907508223147038,\n    basketball-court:0.7355568350525334,\n    storage-tank:0.7486639431388442,\n    soccer-ball-field:0.4675623782300745,\n    roundabout:0.5821237715321051,\n    harbor:0.48939859280789544,\n    swimming-pool:0.5354528941753168,\n    helicopter:0.4067692074174034\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190609_80w\nUsername: DetectionTeamCSU\nInstitute: CSU\nEmailadress: yangxue@csu.edu.cn\nTeamMembers: YangXue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190609\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 20000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA800\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 1024\nIMG_MAX_LENGTH = 1024\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v15.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + [-90, 0) --> [-180, 0)\nThis is your result for task 1:\n\nThis is your result for task 1:\n\n    mAP: 0.6410387162340515\n    ap of each class:\n    plane:0.8825002666630422,\n    baseball-diamond:0.7687908856808928,\n    bridge:0.384748290602071,\n    ground-track-field:0.664809884478965,\n    small-vehicle:0.5414514889058302,\n    large-vehicle:0.4788509185128642,\n    ship:0.608654819886516,\n    tennis-court:0.90013300277984,\n    basketball-court:0.7921189604715826,\n    storage-tank:0.7940181430812733,\n    soccer-ball-field:0.5191877296004171,\n    roundabout:0.6054245683222966,\n    harbor:0.5363073420071247,\n    swimming-pool:0.5913801884362678,\n    helicopter:0.547204254081788\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20191103_54w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20191102\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 180  # or 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v16.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + atan\nThis is your result for task 1:\n\n    mAP: 0.6511450549584724\n    ap of each class: plane:0.8877809042872433,\n    baseball-diamond:0.7735557833035908,\n    bridge:0.41432194690232593,\n    ground-track-field:0.6618291593702126,\n    small-vehicle:0.5552107195407914,\n    large-vehicle:0.4857169873949726,\n    ship:0.6483131307845054,\n    tennis-court:0.9085375803624285,\n    basketball-court:0.8140258362691197,\n    storage-tank:0.8026148657191843,\n    soccer-ball-field:0.5236893754620894,\n    roundabout:0.5980641773687223,\n    harbor:0.5526353497689492,\n    swimming-pool:0.6651580798734689,\n    helicopter:0.47572192796948004\n    The submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20191102_54w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20191103\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""1""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = 1\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v17.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv15 + [-90, 0) --> [-180, 0)\nThis is your result for task 1:\n\nmAP: 0.6331393707245438\nap of each class: plane:0.8855651416180137, baseball-diamond:0.7506170439764697, bridge:0.37820193165920807, ground-track-field:0.5876074736410675, small-vehicle:0.5398925524095497, large-vehicle:0.45488518825532054, ship:0.6143898556575647, tennis-court:0.898883079961541, basketball-court:0.7960688808112641, storage-tank:0.7836823036541495, soccer-ball-field:0.5483442146314041, roundabout:0.6054606429014591, harbor:0.540896997167814, swimming-pool:0.6177733132868135, helicopter:0.49482194123651807\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20191106_54w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20191106\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""1""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = 1\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 180  # or 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your evaluation result for task 1:\n\n    mAP: 0.5264166801822928\n    ap of each class:\n    plane:0.8802779571413935,\n    baseball-diamond:0.6155761401775353,\n    bridge:0.3123243596356521,\n    ground-track-field:0.4364411645165694,\n    small-vehicle:0.5935315012642928,\n    large-vehicle:0.32799594526914044,\n    ship:0.5479230660530156,\n    tennis-court:0.9068436114513042,\n    basketball-court:0.6243552210935295,\n    storage-tank:0.744040313971316,\n    soccer-ball-field:0.29824012466973393,\n    roundabout:0.54049987476996,\n    harbor:0.33001958517247376,\n    swimming-pool:0.49889578605932305,\n    helicopter:0.2392855514891531\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190528_54w\nUsername: DetectionTeamCSU\nInstitute: CSU\nEmailadress: yangxue@csu.edu.cn\nTeamMembers: YangXue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190528\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, True, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v3.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv2 + FREEZE_BLOCKS = [True, False, False, False, False]\nThis is your evaluation result for task 1:\n\n    mAP: 0.5316511018433303\n    ap of each class:\n    plane:0.8798016603321953,\n    baseball-diamond:0.6172287394402983,\n    bridge:0.3357909314108519,\n    ground-track-field:0.5021798660368012,\n    small-vehicle:0.6089171964981729,\n    large-vehicle:0.3852954113165628,\n    ship:0.5483806651464563,\n    tennis-court:0.9054463258911792,\n    basketball-court:0.627634764597444,\n    storage-tank:0.7480772972528041,\n    soccer-ball-field:0.26789687792832406,\n    roundabout:0.5473677567561206,\n    harbor:0.29099760970348104,\n    swimming-pool:0.5367161593858448,\n    helicopter:0.17303526595341798\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190529_54w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190529\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v4.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv3 + single gpu\nThis is your result for task 1:\n\n    mAP: 0.6317708176766469\n    ap of each class:\n    plane:0.8852653294283945,\n    baseball-diamond:0.7498262295051162,\n    bridge:0.41145566588430577,\n    ground-track-field:0.6776071064117907,\n    small-vehicle:0.5374548529638528,\n    large-vehicle:0.48302905195645396,\n    ship:0.5591965949336322,\n    tennis-court:0.9018964535303656,\n    basketball-court:0.8126298860759724,\n    storage-tank:0.7912348207016853,\n    soccer-ball-field:0.5282200549286691,\n    roundabout:0.6197750079280963,\n    harbor:0.55768062609549,\n    swimming-pool:0.5671700322865507,\n    helicopter:0.3941205525193267\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190530_54w\nUsername: DetectionTeamCSU\nInstitute: CSU\nEmailadress: yangxue@csu.edu.cn\nTeamMembers: YangXue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190530\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v5.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv1 + iou-smooth l1 loss\n\nThis is your result for task 1:\n\n    mAP: 0.6865042123662575\n    ap of each class:\n    plane:0.8927147077745269,\n    baseball-diamond:0.7492970898148702,\n    bridge:0.37007353028918566,\n    ground-track-field:0.6449465712810265,\n    small-vehicle:0.6599911133902283,\n    large-vehicle:0.7587068066355522,\n    ship:0.7775055465600569,\n    tennis-court:0.9075607666719274,\n    basketball-court:0.8034725262940876,\n    storage-tank:0.8031338766673758,\n    soccer-ball-field:0.5474562520832246,\n    roundabout:0.6117248850866749,\n    harbor:0.6106856141552955,\n    swimming-pool:0.6478476354811812,\n    helicopter:0.5124462633086478\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190531_56.7w\nUsername: liuqingiqng\nInstitute: Central South University\nEmailadress: liuqingqing@csu.edu.cn\nTeamMembers: liuqingqing\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190531\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = 0\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4  # * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v6.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv5 + multi-gpu\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190601\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4  # * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 0.5 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = [10.0, 10.0, 5.0, 5.0, 5.0]\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v7.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + iou-smooth l1 loss\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190602\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = None  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = None  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * NUM_GPU * BATCH_SIZE\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_v8.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv3 + 2x multi-gpu + REG_WEIGHT=1.0\nThis is your evaluation result for task 1:\n\n    mAP: 0.627914092842082\n    ap of each class:\n    plane:0.8858093578644306,\n    baseball-diamond:0.7232762696378151,\n    bridge:0.3925084013659035,\n    ground-track-field:0.575763198334753,\n    small-vehicle:0.6349664734733673,\n    large-vehicle:0.50675416508869,\n    ship:0.6592800401002049,\n    tennis-court:0.9080613133873843,\n    basketball-court:0.7453368679016705,\n    storage-tank:0.7663649319070647,\n    soccer-ball-field:0.4541366472525138,\n    roundabout:0.5924830757164129,\n    harbor:0.5193142587537795,\n    swimming-pool:0.6127297076624193,\n    helicopter:0.4419266841848219\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_1x_20190603_108w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20190603\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3,4,5,6,7""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/baseline/cfgs_res50_dota_win_v19.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + windows version\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_1x_20200607\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0 / 5.0\nREG_LOSS_MODE = None\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90  # or 180\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n\n'"
libs/configs/DOTA1.0/csl/cfgs_res50_dota_r3det_csl_v1.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\n\nThis is your result for task 1:\n\n    mAP: 0.6445421724301419\n    ap of each class:\n    plane:0.8842518610185887,\n    baseball-diamond:0.753306367437665,\n    bridge:0.42026037103816344,\n    ground-track-field:0.6093222742084081,\n    small-vehicle:0.6565618396165769,\n    large-vehicle:0.723733513024378,\n    ship:0.6961894316870988,\n    tennis-court:0.9071703959460727,\n    basketball-court:0.7448770952572709,\n    storage-tank:0.7995904917440592,\n    soccer-ball-field:0.4976769410919856,\n    roundabout:0.5789924933481052,\n    harbor:0.545409363050681,\n    swimming-pool:0.5678765289200234,\n    helicopter:0.2829136190630481\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_CSL_2x_20200528_70.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_CSL_2x_20200528\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nANGLE_CLS_WEIGHT = 0.5\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*10, SAVE_WEIGHTS_INTE*14, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\nLABEL_TYPE = 0\nRADUIUS = 6\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/csl/cfgs_res50_dota_r3det_csl_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\n90 range + gaussian + raduius=6\n\nThis is your result for task 1:\n\n    mAP: 0.648807997055781\n    ap of each class:\n    plane:0.8849371320282189,\n    baseball-diamond:0.7305201083200566,\n    bridge:0.43371809419631946,\n    ground-track-field:0.5721636933344062,\n    small-vehicle:0.6516437929376034,\n    large-vehicle:0.7387087737504823,\n    ship:0.6975100565174344,\n    tennis-court:0.9052789420814376,\n    basketball-court:0.6618856916194479,\n    storage-tank:0.8134047634196325,\n    soccer-ball-field:0.5238980406578602,\n    roundabout:0.6151349801316419,\n    harbor:0.5629363738718284,\n    swimming-pool:0.560387322493651,\n    helicopter:0.37999219047669164\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_CSL_2x_20200530_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_CSL_2x_20200530\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nANGLE_CLS_WEIGHT = 0.5\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*10, SAVE_WEIGHTS_INTE*14, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 8.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\nLABEL_TYPE = 0\nRADUIUS = 6\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1., 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b0_dota_r3det_v14.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b0\n\nThis is your result for task 1:\n\n    mAP: 0.618376191005381\n    ap of each class:\n    plane:0.8732304757139724,\n    baseball-diamond:0.6660871969254017,\n    bridge:0.4172897541358919,\n    ground-track-field:0.5465076015701161,\n    small-vehicle:0.6545278241906022,\n    large-vehicle:0.6966573950691419,\n    ship:0.6898909393540738,\n    tennis-court:0.9078228487852539,\n    basketball-court:0.7247693289371898,\n    storage-tank:0.769286170585402,\n    soccer-ball-field:0.3419552989359052,\n    roundabout:0.5814102875529091,\n    harbor:0.46423553225952596,\n    swimming-pool:0.57332721817521,\n    helicopter:0.3686449928901182\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200507_70.2w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200507\'\nNET_NAME = \'efficientnet-b0\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b1_dota_r3det_v17.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b1\n\nThis is your result for task 1:\n\n    mAP: 0.6261398342061525\n    ap of each class:\n    plane:0.8832782546627675,\n    baseball-diamond:0.6552088844384489,\n    bridge:0.43867518182814624,\n    ground-track-field:0.5643544069773578,\n    small-vehicle:0.660604206478718,\n    large-vehicle:0.6968619072601661,\n    ship:0.6948851308401217,\n    tennis-court:0.9086043670789434,\n    basketball-court:0.7108134290727005,\n    storage-tank:0.760367404414557,\n    soccer-ball-field:0.38886735671049283,\n    roundabout:0.583791348885809,\n    harbor:0.47698143089859335,\n    swimming-pool:0.6135408243338392,\n    helicopter:0.355263379211626\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200510_70.2w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200510\'\nNET_NAME = \'efficientnet-b1\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b2_dota_r3det_v18.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b2\n\nThis is your result for task 1:\n\n    mAP: 0.6299824729109861\n    ap of each class:\n    plane:0.8833355440387483,\n    baseball-diamond:0.6742786747049715,\n    bridge:0.42449129819585735,\n    ground-track-field:0.5966382729825184,\n    small-vehicle:0.6632172118388726,\n    large-vehicle:0.7100538138902036,\n    ship:0.6887131217971768,\n    tennis-court:0.9079011424904627,\n    basketball-court:0.7167400890691855,\n    storage-tank:0.7887144829722075,\n    soccer-ball-field:0.374910545711846,\n    roundabout:0.5524507032111146,\n    harbor:0.48929469233386214,\n    swimming-pool:0.6360122265750501,\n    helicopter:0.3429852738527127\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200511_70.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200511\'\nNET_NAME = \'efficientnet-b2\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b3_dota_r3det_v19.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b3\n\nThis is your result for task 1:\n\n    mAP: 0.6411528517923154\n    ap of each class:\n    plane:0.8871609727393316,\n    baseball-diamond:0.6651965166562268,\n    bridge:0.4437241725354731,\n    ground-track-field:0.5754256479197026,\n    small-vehicle:0.6570068801930949,\n    large-vehicle:0.7069192196938723,\n    ship:0.6939857710507807,\n    tennis-court:0.9082466960113107,\n    basketball-court:0.7565020477262299,\n    storage-tank:0.7743804095676698,\n    soccer-ball-field:0.44134762946690864,\n    roundabout:0.570446583741574,\n    harbor:0.5419281091569005,\n    swimming-pool:0.6427508797814565,\n    helicopter:0.35227124064419946\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200512_70.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200512\'\nNET_NAME = \'efficientnet-b3\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b4_dota_r3det_v20.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b4\n\nThis is your result for task 1:\n\n    mAP: 0.6471806894736125\n    ap of each class:\n    plane:0.8832049978885607,\n    baseball-diamond:0.6718830312545369,\n    bridge:0.44203815819879366,\n    ground-track-field:0.6235806332799368,\n    small-vehicle:0.6677947990073914,\n    large-vehicle:0.7229812143258793,\n    ship:0.6968474994529471,\n    tennis-court:0.9086093557323557,\n    basketball-court:0.7510364177648917,\n    storage-tank:0.7809092769716914,\n    soccer-ball-field:0.4611799105631519,\n    roundabout:0.5738398504666071,\n    harbor:0.5550984321074194,\n    swimming-pool:0.6416345237287131,\n    helicopter:0.32707224136131124\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200513_70.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200513\'\nNET_NAME = \'efficientnet-b4\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_efficientnet-b5_dota_r3det_v21.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + efficientnet-b5\n\nThis is your result for task 1:\n\n    mAP: 0.6523289929344229\n    ap of each class:\n    plane:0.8885050583446055,\n    baseball-diamond:0.6625306726435614,\n    bridge:0.45621360367429786,\n    ground-track-field:0.6074556327852775,\n    small-vehicle:0.6491801966284957,\n    large-vehicle:0.7218392995796338,\n    ship:0.7101694129752802,\n    tennis-court:0.9084793359853228,\n    basketball-court:0.7552903701077572,\n    storage-tank:0.7921155693611729,\n    soccer-ball-field:0.44388735471047697,\n    roundabout:0.5784766733874839,\n    harbor:0.5638510590057048,\n    swimming-pool:0.6500710592305314,\n    helicopter:0.39686959559674134\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200514_81w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200514\'\nNET_NAME = \'efficientnet-b5\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_noisy-student-efficientnet-b0_dota_r3det_v15.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + noisy student efficientnet-b0\n\nThis is your result for task 1:\n\n    mAP: 0.6202449460620075\n    ap of each class:\n    plane:0.8745462332694063,\n    baseball-diamond:0.6731179191378253,\n    bridge:0.41997937456796963,\n    ground-track-field:0.56475588968927,\n    small-vehicle:0.6602488947561846,\n    large-vehicle:0.6950836152669552,\n    ship:0.6909831920287621,\n    tennis-court:0.9078541030755255,\n    basketball-court:0.7147855897127402,\n    storage-tank:0.7750298406373529,\n    soccer-ball-field:0.35123461547386015,\n    roundabout:0.5313349548736945,\n    harbor:0.4615224287746767,\n    swimming-pool:0.6157147924802956,\n    helicopter:0.36748274718559654\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200515_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200515\'\nNET_NAME = \'efficientnet-b0\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/noisy_student_{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'noisy_student_efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_noisy-student-efficientnet-b7_dota_r3det_v22.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv12 + noisy student efficientnet-b7\n\nThis is your result for task 1:\n\n    mAP: 0.6743870733156078\n    ap of each class:\n    plane:0.8926371561943417,\n    baseball-diamond:0.6910511631792542,\n    bridge:0.463452106066626,\n    ground-track-field:0.6233406156343534,\n    small-vehicle:0.6702088282364265,\n    large-vehicle:0.7265800185432247,\n    ship:0.7521589298961135,\n    tennis-court:0.9083027809304589,\n    basketball-court:0.7617093610132986,\n    storage-tank:0.8062657602245793,\n    soccer-ball-field:0.48140700296646927,\n    roundabout:0.604866743612208,\n    harbor:0.6211852192176545,\n    swimming-pool:0.661394189871849,\n    helicopter:0.4512462241472583\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200516_70.2w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200516\'\nNET_NAME = \'efficientnet-b7\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelif \'efficientnet\' in NET_NAME:\n    weights_name = ""/efficientnet/noisy-student-{}/model"".format(NET_NAME)\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2, efficient]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4 * BATCH_SIZE * NUM_GPU\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nefficientdet_model_param_dict = {\'efficientnet-b0\': dict(fpn_num_filters=64, fpn_cell_repeats=3, box_class_repeats=3),\n                                 \'noisy_student_efficientnet-b1\': dict(fpn_num_filters=88, fpn_cell_repeats=4, box_class_repeats=3),\n                                 \'efficientnet-b2\': dict(fpn_num_filters=112, fpn_cell_repeats=5, box_class_repeats=3),\n                                 \'efficientnet-b3\': dict(fpn_num_filters=160, fpn_cell_repeats=6, box_class_repeats=4),\n                                 \'efficientnet-b4\': dict(fpn_num_filters=224, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b5\': dict(fpn_num_filters=288, fpn_cell_repeats=7, box_class_repeats=4),\n                                 \'efficientnet-b6\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 \'efficientnet-b7\': dict(fpn_num_filters=384, fpn_cell_repeats=8, box_class_repeats=5),\n                                 }\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res101_dota_r3det_v11.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\n\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_3x_20200318\'\nNET_NAME = \'resnet101_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 3\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res152_dota_r3det_v3.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv2 + res152 + data aug.\n\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_4x_201200320\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 4\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v1.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.663127560922365\n    ap of each class:\n    plane:0.8806183496692495,\n    baseball-diamond:0.7389137978157448,\n    bridge:0.42724634527039734,\n    ground-track-field:0.6727269206785866,\n    small-vehicle:0.6580587240161413,\n    large-vehicle:0.7276171243174071,\n    ship:0.7014447630845434,\n    tennis-court:0.8903381923579665,\n    basketball-court:0.7523897218732802,\n    storage-tank:0.6762073662790057,\n    soccer-ball-field:0.5463206107631667,\n    roundabout:0.589204294452238,\n    harbor:0.5607007040750659,\n    swimming-pool:0.6607208327231129,\n    helicopter:0.46440566645956777\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20191108_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20191108\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v12.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv7 + iou-smooth l1 loss\n\nThis is your result for task 1:\n\n    mAP: 0.6949672314782958\n    ap of each class:\n    plane:0.885386792983877,\n    baseball-diamond:0.7331968839644771,\n    bridge:0.4898628104022108,\n    ground-track-field:0.6400479115497535,\n    small-vehicle:0.6804863040286401,\n    large-vehicle:0.7272874790497282,\n    ship:0.7602937054410586,\n    tennis-court:0.9083344898088754,\n    basketball-court:0.784327582019443,\n    storage-tank:0.8013993994143976,\n    soccer-ball-field:0.5483354704539025,\n    roundabout:0.6171343622691822,\n    harbor:0.5801559667600483,\n    swimming-pool:0.6858516206184136,\n    helicopter:0.582407693410429\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200501_91.8w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200501\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\nALPHA = 1.0\nBETA = 1.0\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv1 + two refine stages\n\nThis is your result for task 1:\n\n    mAP: 0.6729130998778783\n    ap of each class: plane:0.8826149967762923,\n    baseball-diamond:0.7342284978013562,\n    bridge:0.4514498941155807,\n    ground-track-field:0.672091907176201,\n    small-vehicle:0.6709474430972782,\n    large-vehicle:0.7369526299825417,\n    ship:0.7021106544077973,\n    tennis-court:0.9064314756695351,\n    basketball-court:0.7657412068215571,\n    storage-tank:0.7984134345038421,\n    soccer-ball-field:0.5386417366552334,\n    roundabout:0.6093094275405536,\n    harbor:0.5695698766743563,\n    swimming-pool:0.6065833385927675,\n    helicopter:0.4486099783532847\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20191118_108w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n-----------------------------------------------------------------\n\nmerge all refine stages results\n\nThis is your result for task 1:\n\n    mAP: 0.6765553919838632\n    ap of each class:\n    plane:0.8842026956804583,\n    baseball-diamond:0.7385818759437492,\n    bridge:0.4508422918807599,\n    ground-track-field:0.6795929425489206,\n    small-vehicle:0.674452811091998,\n    large-vehicle:0.7398138561006364,\n    ship:0.7026655772347278,\n    tennis-court:0.9070912170899791,\n    basketball-court:0.7776849884962305,\n    storage-tank:0.8044493538968289,\n    soccer-ball-field:0.5477872349765581,\n    roundabout:0.6053087523424203,\n    harbor:0.5729741901030716,\n    swimming-pool:0.6047761287319094,\n    helicopter:0.45810696363969794\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20191118_merge_108w\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20191118\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""1,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v4.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv1 + iou-smooth l1\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20191128\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""1""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'R\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v5.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + less anchor\nThis is your result for task 1:\n\n    mAP: 0.6710988053029553\n    ap of each class:\n    plane:0.8794665560865303,\n    baseball-diamond:0.732530568626708,\n    bridge:0.4516388309809375,\n    ground-track-field:0.6663921484733175,\n    small-vehicle:0.653023100259904,\n    large-vehicle:0.7147901635635758,\n    ship:0.6971152392617997,\n    tennis-court:0.9068598197326132,\n    basketball-court:0.7897695625020531,\n    storage-tank:0.8223151646132263,\n    soccer-ball-field:0.549850031158259,\n    roundabout:0.6134846359014985,\n    harbor:0.557324432201045,\n    swimming-pool:0.5969762209759844,\n    helicopter:0.43494560520687725\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200307_108ww\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200307\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v6.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + anchor free + P2\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200309\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P2\', \'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [16, 32, 64, 128, 256, 512]\nANCHOR_STRIDE = [4, 8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v7.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + anchor free\n\nThis is your result for task 1:\n\n    mAP: 0.6731691687391387\n    ap of each class:\n    plane:0.8811404076391482,\n    baseball-diamond:0.7593301554008368,\n    bridge:0.4604224700014371,\n    ground-track-field:0.6178848196153522,\n    small-vehicle:0.6515426650754489,\n    large-vehicle:0.7212507472414388,\n    ship:0.6991033060387193,\n    tennis-court:0.9074987319365154,\n    basketball-court:0.7787462292917784,\n    storage-tank:0.8095742625412805,\n    soccer-ball-field:0.5545627798003411,\n    roundabout:0.6003136960483185,\n    harbor:0.5698769064992982,\n    swimming-pool:0.6412292041835665,\n    helicopter:0.4450611497736008\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200308_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200308\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v8.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv4 + anchor free\n\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200310\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 3\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6, 0.65]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5, 0.55]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det/cfgs_res50_dota_r3det_v9.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv2 + anchor free\nThis is your result for task 1:\n\n    mAP: 0.6588687200678148\n    ap of each class:\n    plane:0.8861431982498656,\n    baseball-diamond:0.7547149365145954,\n    bridge:0.44480613621086246,\n    ground-track-field:0.6127421718706352,\n    small-vehicle:0.6520361518601682,\n    large-vehicle:0.7144370031538592,\n    ship:0.6973570555762958,\n    tennis-court:0.9075172059512909,\n    basketball-court:0.7789952549852983,\n    storage-tank:0.8246835133491305,\n    soccer-ball-field:0.4517339440615033,\n    roundabout:0.5836332580037924,\n    harbor:0.5418945591510039,\n    swimming-pool:0.6517692855203618,\n    helicopter:0.3805671265585586\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_2x_20200311_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_2x_20200311\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 2\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = False\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/__init__.py,0,b''
libs/configs/DOTA1.0/r3det_plusplus/cfgs_mobilenetv2_dota_r3det_plusplus_v6.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_3x_20200321\'\nNET_NAME = \'MobilenetV2\'  # \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 3\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.5\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n\n\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res101_dota_r3det_plusplus_v4.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.6919112172827518\n    ap of each class:\n    plane:0.890655568233423,\n    baseball-diamond:0.7766018585845097,\n    bridge:0.4677310348715208,\n    ground-track-field:0.6261305010022019,\n    small-vehicle:0.6957884232789637,\n    large-vehicle:0.7486632989672312,\n    ship:0.7853664353253879,\n    tennis-court:0.9075759483322384,\n    basketball-court:0.7902459406623531,\n    storage-tank:0.8188501385092533,\n    soccer-ball-field:0.5848163485374883,\n    roundabout:0.5939983855186469,\n    harbor:0.58388396587549,\n    swimming-pool:0.6512104643190033,\n    helicopter:0.4571499472235661\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20200317_108w\nUsername: liuqingiqng\nInstitute: Central South University\nEmailadress: liuqingqing@csu.edu.cn\nTeamMembers: liuqingqing\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20200317\'\nNET_NAME = \'resnet101_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res101_dota_r3det_plusplus_v5.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.7297809625497856\n    ap of each class:\n    plane:0.8925271259337683,\n    baseball-diamond:0.8330257641742015,\n    bridge:0.499438389181998,\n    ground-track-field:0.6619523363364268,\n    small-vehicle:0.7182401864565915,\n    large-vehicle:0.7712028831686399,\n    ship:0.7952712512313664,\n    tennis-court:0.9064777371749804,\n    basketball-court:0.821407907052357,\n    storage-tank:0.8456621359273732,\n    soccer-ball-field:0.6533158137187599,\n    roundabout:0.6389498063678302,\n    harbor:0.6755997197880204,\n    swimming-pool:0.6847678535754258,\n    helicopter:0.5488755281590466\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_3x_20200319_145.8w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_3x_20200319\'\nNET_NAME = \'resnet101_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 3\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res152_dota_r3det_plusplus_v1.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\nmAP: 0.7548292480753233\nap of each class:\nplane:0.8841769208553147,\nbaseball-diamond:0.8525061802298446,\nbridge:0.5496029472952018,\nground-track-field:0.6796421046681694,\nsmall-vehicle:0.7188970715741156,\nlarge-vehicle:0.818256263589424,\nship:0.7925366329881341,\ntennis-court:0.9078981713689425,\nbasketball-court:0.8637118106046471,\nstorage-tank:0.856599000619501,\nsoccer-ball-field:0.6483777976769765,\nroundabout:0.5986952521009021,\nharbor:0.7274815724392929,\nswimming-pool:0.7200362479652318,\nhelicopter:0.7040207471541494\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_6x_20191111\nUsername: yangxue\nInstitute: DetectionTeamUCAS\nEmailadress: yangxue16@mails.ucas.ac.cn\nTeamMembers: yangxue, yangjirui\n\nMS\nR3Det + mask + res152 + data aug + ms + 6x + more anchor\nThis is your result for task 1:\n\n    mAP: 0.7655534541378967\n    ap of each class:\n    plane:0.8868158614223661,\n    baseball-diamond:0.852182400391936,\n    bridge:0.5470275130245766,\n    ground-track-field:0.7370799767628674,\n    small-vehicle:0.719177596577124,\n    large-vehicle:0.8413667342068726,\n    ship:0.7938870736446055,\n    tennis-court:0.9081598692117578,\n    basketball-court:0.8703578000343699,\n    storage-tank:0.8602037189671236,\n    soccer-ball-field:0.6790367021214336,\n    roundabout:0.6085818723541996,\n    harbor:0.7452311928028673,\n    swimming-pool:0.7075836633997086,\n    helicopter:0.726609837146643\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_6x_20191111_ms_275.4w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_6x_20191111\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 6\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = [800, 400, 600, 900, 1000, 1100, 1200]\nIMG_MAX_LENGTH = 1500\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = True\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [0.5, 0.75, 2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5., 7., 1 / 7., 9., 1 / 9.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.5\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n\n\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res152_dota_r3det_plusplus_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_4x_20200324\'\nNET_NAME = \'resnet152_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 4\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = [768, 512, 640, 896, 1024, 1152, 1280]\nIMG_MAX_LENGTH = 1408\nCLASS_NUM = 15\n\nIMG_ROTATE = True\nRGB2GRAY = True\nVERTICAL_FLIP = True\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = True\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [1.]\nANCHOR_RATIOS = [1.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.35\nIOU_NEGATIVE_THRESHOLD = 0.25\nREFINE_IOU_POSITIVE_THRESHOLD = [0.5, 0.6]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.4, 0.5]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.5\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n\n\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res50_dota_r3det_plusplus_v2.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.6906642765291946\n    ap of each class:\n    plane:0.887522875557551,\n    baseball-diamond:0.7507783749944515,\n    bridge:0.4763570953343346,\n    ground-track-field:0.6432005420994066,\n    small-vehicle:0.699281458680165,\n    large-vehicle:0.7433771949228654,\n    ship:0.7874251198260236,\n    tennis-court:0.9072158317822742,\n    basketball-court:0.7837059827292017,\n    storage-tank:0.8272106150571217,\n    soccer-ball-field:0.531805077030781,\n    roundabout:0.597540926011181,\n    harbor:0.5772609579320696,\n    swimming-pool:0.6500953433771086,\n    helicopter:0.49718675260338446\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20191122_108w\nUsername: DetectionTeamCSU\nInstitute: CSU\nEmailadress: yangxue@csu.edu.cn\nTeamMembers: YangXue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20191121\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [4, 4, 3, 2, 2]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res50_dota_r3det_plusplus_v3.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nThis is your result for task 1:\n\n    mAP: 0.6981048818606869\n    ap of each class:\n    plane:0.8863289890592082,\n    baseball-diamond:0.7598027174394937,\n    bridge:0.45875102843598325,\n    ground-track-field:0.6545483481987261,\n    small-vehicle:0.6973539596281901,\n    large-vehicle:0.7408841434070434,\n    ship:0.782970797083198,\n    tennis-court:0.9077667189483063,\n    basketball-court:0.7895722036504925,\n    storage-tank:0.8127897549341713,\n    soccer-ball-field:0.5627873453809793,\n    roundabout:0.6301325959163575,\n    harbor:0.5740450517785702,\n    swimming-pool:0.6845284957942532,\n    helicopter:0.5293110782553292\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20200315_97.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20200315\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res50_dota_r3det_plusplus_v7.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv3 + binary mask\n\nThis is your result for task 1:\n\n    mAP: 0.6812440846467152\n    ap of each class:\n    plane:0.8808139102474871,\n    baseball-diamond:0.7607691494462757,\n    bridge:0.4612578999851587,\n    ground-track-field:0.6209642422135865,\n    small-vehicle:0.7002786564366527,\n    large-vehicle:0.7464813082321828,\n    ship:0.7845885660352094,\n    tennis-court:0.9080666966596697,\n    basketball-court:0.7851163310191952,\n    storage-tank:0.8171048438782343,\n    soccer-ball-field:0.5444266008721775,\n    roundabout:0.596699464560922,\n    harbor:0.5765265596301736,\n    swimming-pool:0.6348189196681072,\n    helicopter:0.4007481208156915\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20200403_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20200403\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = True\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res50_dota_r3det_plusplus_v8.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv3 + weight\n\nThis is your result for task 1:\n\n    mAP: 0.694278910759864\n    ap of each class:\n    plane:0.88866159706304,\n    baseball-diamond:0.7860352276239824,\n    bridge:0.47338301497690105,\n    ground-track-field:0.6216372729671545,\n    small-vehicle:0.6994177931102508,\n    large-vehicle:0.7458671012655077,\n    ship:0.785294772102568,\n    tennis-court:0.9075708653156096,\n    basketball-court:0.7834021499469714,\n    storage-tank:0.8172385380195397,\n    soccer-ball-field:0.5645662115849255,\n    roundabout:0.6018272737599449,\n    harbor:0.5750654725229614,\n    swimming-pool:0.6652388936929979,\n    helicopter:0.49897747744560683\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20200405_108w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20200405\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = False\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/configs/DOTA1.0/r3det_plusplus/cfgs_res50_dota_r3det_plusplus_v9.py,3,"b'# -*- coding: utf-8 -*-\nfrom __future__ import division, print_function, absolute_import\nimport os\nimport tensorflow as tf\nimport math\n\n""""""\nv3 + iou-smooth l1 loss\n\nThis is your result for task 1:\n\n    mAP: 0.7007692803221528\n    ap of each class:\n    plane:0.8869085219087487,\n    baseball-diamond:0.7863564890420646,\n    bridge:0.4841934769081011,\n    ground-track-field:0.6553479412012608,\n    small-vehicle:0.7329533508847295,\n    large-vehicle:0.7478460844218014,\n    ship:0.7913431220606855,\n    tennis-court:0.9083852978140723,\n    basketball-court:0.7937235171900185,\n    storage-tank:0.8266521002347584,\n    soccer-ball-field:0.5340372839446457,\n    roundabout:0.6195198573302646,\n    harbor:0.581827275405277,\n    swimming-pool:0.6840022392917975,\n    helicopter:0.47844264719406826\n\nThe submitted information is :\n\nDescription: RetinaNet_DOTA_R3Det_plusplus_2x_20200502_70.2w\nUsername: SJTU-Det\nInstitute: SJTU\nEmailadress: yangxue-2019-sjtu@sjtu.edu.cn\nTeamMembers: yangxue\n\n""""""\n\n# ------------------------------------------------\nVERSION = \'RetinaNet_DOTA_R3Det_plusplus_2x_20200502\'\nNET_NAME = \'resnet50_v1d\'  # \'MobilenetV2\'\nADD_BOX_IN_TENSORBOARD = True\n\n# ---------------------------------------- System_config\nROOT_PATH = os.path.abspath(\'../\')\nprint(20*""++--"")\nprint(ROOT_PATH)\nGPU_GROUP = ""0,1,2,3""\nNUM_GPU = len(GPU_GROUP.strip().split(\',\'))\nSHOW_TRAIN_INFO_INTE = 20\nSMRY_ITER = 200\nSAVE_WEIGHTS_INTE = 27000 * 2\n\nSUMMARY_PATH = ROOT_PATH + \'/output/summary\'\nTEST_SAVE_PATH = ROOT_PATH + \'/tools/test_result\'\n\nif NET_NAME.startswith(""resnet""):\n    weights_name = NET_NAME\nelif NET_NAME.startswith(""MobilenetV2""):\n    weights_name = ""mobilenet/mobilenet_v2_1.0_224""\nelse:\n    raise Exception(\'net name must in [resnet_v1_101, resnet_v1_50, MobilenetV2]\')\n\nPRETRAINED_CKPT = ROOT_PATH + \'/data/pretrained_weights/\' + weights_name + \'.ckpt\'\nTRAINED_CKPT = os.path.join(ROOT_PATH, \'output/trained_weights\')\nEVALUATE_DIR = ROOT_PATH + \'/output/evaluate_result_pickle/\'\n\n# ------------------------------------------ Train config\nRESTORE_FROM_RPN = False\nFIXED_BLOCKS = 1  # allow 0~3\nFREEZE_BLOCKS = [True, False, False, False, False]  # for gluoncv backbone\nUSE_07_METRIC = True\n\nMUTILPY_BIAS_GRADIENT = 2.0  # if None, will not multipy\nGRADIENT_CLIPPING_BY_NORM = 10.0  # if None, will not clip\n\nCLS_WEIGHT = 1.0\nREG_WEIGHT = 1.0\nUSE_IOU_FACTOR = True\n\nBATCH_SIZE = 1\nEPSILON = 1e-5\nMOMENTUM = 0.9\nLR = 5e-4\nDECAY_STEP = [SAVE_WEIGHTS_INTE*12, SAVE_WEIGHTS_INTE*16, SAVE_WEIGHTS_INTE*20]\nMAX_ITERATION = SAVE_WEIGHTS_INTE*20\nWARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)\n\n# -------------------------------------------- Data_preprocess_config\nDATASET_NAME = \'DOTA\'  # \'pascal\', \'coco\'\nPIXEL_MEAN = [123.68, 116.779, 103.939]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nPIXEL_MEAN_ = [0.485, 0.456, 0.406]\nPIXEL_STD = [0.229, 0.224, 0.225]  # R, G, B. In tf, channel is RGB. In openCV, channel is BGR\nIMG_SHORT_SIDE_LEN = 800\nIMG_MAX_LENGTH = 800\nCLASS_NUM = 15\n\nIMG_ROTATE = False\nRGB2GRAY = False\nVERTICAL_FLIP = False\nHORIZONTAL_FLIP = True\nIMAGE_PYRAMID = False\n\n# --------------------------------------------- Network_config\nSUBNETS_WEIGHTS_INITIALIZER = tf.random_normal_initializer(mean=0.0, stddev=0.01, seed=None)\nSUBNETS_BIAS_INITIALIZER = tf.constant_initializer(value=0.0)\nPROBABILITY = 0.01\nFINAL_CONV_BIAS_INITIALIZER = tf.constant_initializer(value=-math.log((1.0 - PROBABILITY) / PROBABILITY))\nWEIGHT_DECAY = 1e-4\nUSE_GN = False\nNUM_SUBNET_CONV = 4\nNUM_REFINE_STAGE = 1\nUSE_RELU = False\nFPN_CHANNEL = 256\n\n# ---------------------------------------------Anchor config\nLEVEL = [\'P3\', \'P4\', \'P5\', \'P6\', \'P7\']\nBASE_ANCHOR_SIZE_LIST = [32, 64, 128, 256, 512]\nANCHOR_STRIDE = [8, 16, 32, 64, 128]\nANCHOR_SCALES = [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\nANCHOR_RATIOS = [1, 1 / 2, 2., 1 / 3., 3., 5., 1 / 5.]\nANCHOR_ANGLES = [-90, -75, -60, -45, -30, -15]\nANCHOR_SCALE_FACTORS = None\nUSE_CENTER_OFFSET = True\nMETHOD = \'H\'\nUSE_ANGLE_COND = False\nANGLE_RANGE = 90\n\n# --------------------------------------------RPN config\nSHARE_NET = True\nUSE_P5 = True\nIOU_POSITIVE_THRESHOLD = 0.5\nIOU_NEGATIVE_THRESHOLD = 0.4\nREFINE_IOU_POSITIVE_THRESHOLD = [0.6, 0.7]\nREFINE_IOU_NEGATIVE_THRESHOLD = [0.5, 0.6]\n\nNMS = True\nNMS_IOU_THRESHOLD = 0.1\nMAXIMUM_DETECTIONS = 100\nFILTERED_SCORE = 0.05\nVIS_SCORE = 0.4\n\n# --------------------------------------------MASK config\nUSE_SUPERVISED_MASK = True\nMASK_TYPE = \'r\'  # r or h\nBINARY_MASK = False\nSIGMOID_ON_DOT = False\nMASK_ACT_FET = True  # weather use mask generate 256 channels to dot feat.\nGENERATE_MASK_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nADDITION_LAYERS = [1, 1, 1, 1, 1]  # add 4 layer to generate P2_mask, 2 layer to generate P3_mask\nENLAEGE_RF_LIST = [""P3"", ""P4"", ""P5"", ""P6"", ""P7""]\nSUPERVISED_MASK_LOSS_WEIGHT = 1.0\n'"
libs/networks/efficientnet/condconv/condconv_layers.py,32,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""CondConv implementations in Tensorflow Layers.\n[1] Brandon Yang, Gabriel Bender, Quoc V. Le, Jiquan Ngiam\n  CondConv: Conditionally Parameterized Convolutions for Efficient Inference.\n  NeurIPS\'19, https://arxiv.org/abs/1904.04971\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n  """"""Wraps the initializer to correctly initialize CondConv variables.\n  CondConv initializes biases and kernels in a num_experts x num_params\n  matrix for efficient computation. This wrapper ensures that each expert\n  is correctly initialized with the given initializer before being flattened\n  into the correctly shaped CondConv variable.\n  Arguments:\n    initializer: The initializer to apply for each individual expert.\n    num_experts: The number of experts to be initialized.\n    expert_shape: The original shape of each individual expert.\n  Returns:\n    The initializer for the num_experts x num_params CondConv variable.\n  """"""\n  def condconv_initializer(expected_shape, dtype=None, partition=None):\n    """"""CondConv initializer function.""""""\n    num_params = np.prod(expert_shape)\n    if (len(expected_shape) != 2 or expected_shape[0] != num_experts or\n        expected_shape[1] != num_params):\n      raise (ValueError(\n          \'CondConv variables must have shape [num_experts, num_params]\'))\n    flattened_kernels = []\n    for _ in range(num_experts):\n      kernel = initializer(expert_shape, dtype, partition)\n      flattened_kernels.append(tf.reshape(kernel, [-1]))\n    return tf.stack(flattened_kernels)\n\n  return condconv_initializer\n\n\nclass CondConv2D(tf.keras.layers.Conv2D):\n  """"""2D conditional convolution layer (e.g. spatial convolution over images).\n  Attributes:\n    filters: Integer, the dimensionality of the output space (i.e. the number of\n      output filters in the convolution).\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n      and width of the 2D convolution window. Can be a single integer to specify\n      the same value for all spatial dimensions.\n    num_experts: The number of expert kernels and biases in the CondConv layer.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of\n      the convolution along the height and width. Can be a single integer to\n      specify the same value for all spatial dimensions. Specifying any stride\n      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: one of `""valid""` or `""same""` (case-insensitive).\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs. `channels_last` corresponds\n      to inputs with shape `(batch, height, width, channels)` while\n      `channels_first` corresponds to inputs with shape `(batch, channels,\n      height, width)`. It defaults to the `image_data_format` value found in\n      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n      it will be ""channels_last"".\n    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n      dilation rate to use for dilated convolution. Can be a single integer to\n      specify the same value for all spatial dimensions. Currently, specifying\n      any `dilation_rate` value != 1 is incompatible with specifying any stride\n      value != 1.\n    activation: Activation function to use. If you don\'t specify anything, no\n      activation is applied\n      (ie. ""linear"" activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    kernel_initializer: Initializer for the `kernel` weights matrix.\n    bias_initializer: Initializer for the bias vector.\n    kernel_regularizer: Regularizer function applied to the `kernel` weights\n      matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the\n      layer (its ""activation"")..\n    kernel_constraint: Constraint function applied to the kernel matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n  Input shape:\n    4D tensor with shape: `(samples, channels, rows, cols)` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `(samples, rows, cols, channels)` if\n      data_format=\'channels_last\'.\n  Output shape:\n    4D tensor with shape: `(samples, filters, new_rows, new_cols)` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `(samples, new_rows, new_cols, filters)` if\n      data_format=\'channels_last\'. `rows` and `cols` values might have changed\n      due to padding.\n  """"""\n\n  def __init__(self,\n               filters,\n               kernel_size,\n               num_experts,\n               strides=(1, 1),\n               padding=\'valid\',\n               data_format=None,\n               dilation_rate=(1, 1),\n               activation=None,\n               use_bias=True,\n               kernel_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               kernel_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               kernel_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(CondConv2D, self).__init__(\n        filters=filters,\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        data_format=data_format,\n        dilation_rate=dilation_rate,\n        activation=activation,\n        use_bias=use_bias,\n        kernel_initializer=kernel_initializer,\n        bias_initializer=bias_initializer,\n        kernel_regularizer=kernel_regularizer,\n        bias_regularizer=bias_regularizer,\n        activity_regularizer=activity_regularizer,\n        kernel_constraint=kernel_constraint,\n        bias_constraint=bias_constraint,\n        **kwargs)\n    if num_experts < 1:\n      raise ValueError(\'A CondConv layer must have at least one expert.\')\n    self.num_experts = num_experts\n    if self.data_format == \'channels_first\':\n      self.converted_data_format = \'NCHW\'\n    else:\n      self.converted_data_format = \'NHWC\'\n\n  def build(self, input_shape):\n    if len(input_shape) != 4:\n      raise ValueError(\n          \'Inputs to `CondConv2D` should have rank 4. \'\n          \'Received input shape:\', str(input_shape))\n    input_shape = tf.TensorShape(input_shape)\n    channel_axis = self._get_channel_axis()\n    if input_shape.dims[channel_axis].value is None:\n      raise ValueError(\'The channel dimension of the inputs \'\n                       \'should be defined. Found `None`.\')\n    input_dim = int(input_shape[channel_axis])\n\n    self.kernel_shape = self.kernel_size + (input_dim, self.filters)\n    kernel_num_params = 1\n    for kernel_dim in self.kernel_shape:\n      kernel_num_params *= kernel_dim\n    condconv_kernel_shape = (self.num_experts, kernel_num_params)\n    self.condconv_kernel = self.add_weight(\n        name=\'condconv_kernel\',\n        shape=condconv_kernel_shape,\n        initializer=get_condconv_initializer(self.kernel_initializer,\n                                             self.num_experts,\n                                             self.kernel_shape),\n        regularizer=self.kernel_regularizer,\n        constraint=self.kernel_constraint,\n        trainable=True,\n        dtype=self.dtype)\n\n    if self.use_bias:\n      self.bias_shape = (self.filters,)\n      condconv_bias_shape = (self.num_experts, self.filters)\n      self.condconv_bias = self.add_weight(\n          name=\'condconv_bias\',\n          shape=condconv_bias_shape,\n          initializer=get_condconv_initializer(self.bias_initializer,\n                                               self.num_experts,\n                                               self.bias_shape),\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n\n    self.input_spec = tf.layers.InputSpec(\n        ndim=self.rank + 2, axes={channel_axis: input_dim})\n\n    self.built = True\n\n  def call(self, inputs, routing_weights):\n    # Compute example dependent kernels\n    kernels = tf.matmul(routing_weights, self.condconv_kernel)\n    batch_size = inputs.shape[0].value\n    inputs = tf.split(inputs, batch_size, 0)\n    kernels = tf.split(kernels, batch_size, 0)\n    # Apply example-dependent convolution to each example in the batch\n    outputs_list = []\n    for input_tensor, kernel in zip(inputs, kernels):\n      kernel = tf.reshape(kernel, self.kernel_shape)\n      outputs_list.append(\n          tf.nn.convolution(\n              input_tensor,\n              kernel,\n              strides=self.strides,\n              padding=self._get_padding_op(),\n              dilations=self.dilation_rate,\n              data_format=self.converted_data_format))\n    outputs = tf.concat(outputs_list, 0)\n\n    if self.use_bias:\n      # Compute example-dependent biases\n      biases = tf.matmul(routing_weights, self.condconv_bias)\n      outputs = tf.split(outputs, batch_size, 0)\n      biases = tf.split(biases, batch_size, 0)\n      # Add example-dependent bias to each example in the batch\n      bias_outputs_list = []\n      for output, bias in zip(outputs, biases):\n        bias = tf.squeeze(bias, axis=0)\n        bias_outputs_list.append(\n            tf.nn.bias_add(output, bias,\n                           data_format=self.converted_data_format))\n      outputs = tf.concat(bias_outputs_list, 0)\n\n    if self.activation is not None:\n      return self.activation(outputs)\n    return outputs\n\n  def get_config(self):\n    config = {\'num_experts\': self.num_experts}\n    base_config = super(CondConv2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  def _get_channel_axis(self):\n    if self.data_format == \'channels_first\':\n      return 1\n    else:\n      return -1\n\n  def _get_padding_op(self):\n    if self.padding == \'causal\':\n      op_padding = \'valid\'\n    else:\n      op_padding = self.padding\n    if not isinstance(op_padding, (list, tuple)):\n      op_padding = op_padding.upper()\n    return op_padding\n\n\nclass DepthwiseCondConv2D(tf.keras.layers.DepthwiseConv2D):\n  """"""Depthwise separable 2D conditional convolution layer.\n  This layer extends the base depthwise 2D convolution layer to compute\n  example-dependent parameters. A DepthwiseCondConv2D layer has \'num_experts`\n  kernels and biases. It computes a kernel and bias for each example as a\n  weighted sum of experts using the input example-dependent routing weights,\n  then applies the depthwise convolution to each example.\n  Attributes:\n    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n      and width of the 2D convolution window. Can be a single integer to specify\n      the same value for all spatial dimensions.\n    num_experts: The number of expert kernels and biases in the\n      DepthwiseCondConv2D layer.\n    strides: An integer or tuple/list of 2 integers, specifying the strides of\n      the convolution along the height and width. Can be a single integer to\n      specify the same value for all spatial dimensions. Specifying any stride\n      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n    padding: one of `\'valid\'` or `\'same\'` (case-insensitive).\n    depth_multiplier: The number of depthwise convolution output channels for\n      each input channel. The total number of depthwise convolution output\n      channels will be equal to `filters_in * depth_multiplier`.\n    data_format: A string, one of `channels_last` (default) or `channels_first`.\n      The ordering of the dimensions in the inputs. `channels_last` corresponds\n      to inputs with shape `(batch, height, width, channels)` while\n      `channels_first` corresponds to inputs with shape `(batch, channels,\n      height, width)`. It defaults to the `image_data_format` value found in\n      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n      it will be \'channels_last\'.\n    activation: Activation function to use. If you don\'t specify anything, no\n      activation is applied\n      (ie. \'linear\' activation: `a(x) = x`).\n    use_bias: Boolean, whether the layer uses a bias vector.\n    depthwise_initializer: Initializer for the depthwise kernel matrix.\n    bias_initializer: Initializer for the bias vector.\n    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n      matrix.\n    bias_regularizer: Regularizer function applied to the bias vector.\n    activity_regularizer: Regularizer function applied to the output of the\n      layer (its \'activation\').\n    depthwise_constraint: Constraint function applied to the depthwise kernel\n      matrix.\n    bias_constraint: Constraint function applied to the bias vector.\n  Input shape:\n    4D tensor with shape: `[batch, channels, rows, cols]` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `[batch, rows, cols, channels]` if\n      data_format=\'channels_last\'.\n  Output shape:\n    4D tensor with shape: `[batch, filters, new_rows, new_cols]` if\n      data_format=\'channels_first\'\n    or 4D tensor with shape: `[batch, new_rows, new_cols, filters]` if\n      data_format=\'channels_last\'. `rows` and `cols` values might have changed\n      due to padding.\n  """"""\n\n  def __init__(self,\n               kernel_size,\n               num_experts,\n               strides=(1, 1),\n               padding=\'valid\',\n               depth_multiplier=1,\n               data_format=None,\n               activation=None,\n               use_bias=True,\n               depthwise_initializer=\'glorot_uniform\',\n               bias_initializer=\'zeros\',\n               depthwise_regularizer=None,\n               bias_regularizer=None,\n               activity_regularizer=None,\n               depthwise_constraint=None,\n               bias_constraint=None,\n               **kwargs):\n    super(DepthwiseCondConv2D, self).__init__(\n        kernel_size=kernel_size,\n        strides=strides,\n        padding=padding,\n        depth_multiplier=depth_multiplier,\n        data_format=data_format,\n        activation=activation,\n        use_bias=use_bias,\n        depthwise_initializer=depthwise_initializer,\n        bias_initializer=bias_initializer,\n        depthwise_regularizer=depthwise_regularizer,\n        bias_regularizer=bias_regularizer,\n        activity_regularizer=activity_regularizer,\n        depthwise_constraint=depthwise_constraint,\n        bias_constraint=bias_constraint,\n        **kwargs)\n    if num_experts < 1:\n      raise ValueError(\'A CondConv layer must have at least one expert.\')\n    self.num_experts = num_experts\n    if self.data_format == \'channels_first\':\n      self.converted_data_format = \'NCHW\'\n    else:\n      self.converted_data_format = \'NHWC\'\n\n  def build(self, input_shape):\n    if len(input_shape) < 4:\n      raise ValueError(\n          \'Inputs to `DepthwiseCondConv2D` should have rank 4. \'\n          \'Received input shape:\', str(input_shape))\n    input_shape = tf.TensorShape(input_shape)\n    if self.data_format == \'channels_first\':\n      channel_axis = 1\n    else:\n      channel_axis = 3\n    if input_shape.dims[channel_axis].value is None:\n      raise ValueError(\'The channel dimension of the inputs to \'\n                       \'`DepthwiseConv2D` \'\n                       \'should be defined. Found `None`.\')\n    input_dim = int(input_shape[channel_axis])\n    self.depthwise_kernel_shape = (self.kernel_size[0], self.kernel_size[1],\n                                   input_dim, self.depth_multiplier)\n\n    depthwise_kernel_num_params = 1\n    for dim in self.depthwise_kernel_shape:\n      depthwise_kernel_num_params *= dim\n    depthwise_condconv_kernel_shape = (self.num_experts,\n                                       depthwise_kernel_num_params)\n\n    self.depthwise_condconv_kernel = self.add_weight(\n        shape=depthwise_condconv_kernel_shape,\n        initializer=get_condconv_initializer(self.depthwise_initializer,\n                                             self.num_experts,\n                                             self.depthwise_kernel_shape),\n        name=\'depthwise_condconv_kernel\',\n        regularizer=self.depthwise_regularizer,\n        constraint=self.depthwise_constraint,\n        trainable=True)\n\n    if self.use_bias:\n      bias_dim = input_dim * self.depth_multiplier\n      self.bias_shape = (bias_dim,)\n      condconv_bias_shape = (self.num_experts, bias_dim)\n      self.condconv_bias = self.add_weight(\n          name=\'condconv_bias\',\n          shape=condconv_bias_shape,\n          initializer=get_condconv_initializer(self.bias_initializer,\n                                               self.num_experts,\n                                               self.bias_shape),\n          regularizer=self.bias_regularizer,\n          constraint=self.bias_constraint,\n          trainable=True,\n          dtype=self.dtype)\n    else:\n      self.bias = None\n    # Set input spec.\n    self.input_spec = tf.layers.InputSpec(\n        ndim=4, axes={channel_axis: input_dim})\n    self.built = True\n\n  def call(self, inputs, routing_weights):\n    # Compute example dependent depthwise kernels\n    depthwise_kernels = tf.matmul(routing_weights,\n                                  self.depthwise_condconv_kernel)\n    batch_size = inputs.shape[0].value\n    inputs = tf.split(inputs, batch_size, 0)\n    depthwise_kernels = tf.split(depthwise_kernels, batch_size, 0)\n    # Apply example-dependent depthwise convolution to each example in the batch\n    outputs_list = []\n    for input_tensor, depthwise_kernel in zip(inputs, depthwise_kernels):\n      depthwise_kernel = tf.reshape(depthwise_kernel,\n                                    self.depthwise_kernel_shape)\n      if self.data_format == \'channels_first\':\n        converted_strides = (1, 1) + self.strides\n      else:\n        converted_strides = (1,) + self.strides + (1,)\n      outputs_list.append(\n          tf.nn.depthwise_conv2d(\n              input_tensor,\n              depthwise_kernel,\n              strides=converted_strides,\n              padding=self.padding.upper(),\n              dilations=self.dilation_rate,\n              data_format=self.converted_data_format))\n    outputs = tf.concat(outputs_list, 0)\n\n    if self.use_bias:\n      # Compute example-dependent biases\n      biases = tf.matmul(routing_weights, self.condconv_bias)\n      outputs = tf.split(outputs, batch_size, 0)\n      biases = tf.split(biases, batch_size, 0)\n      # Add example-dependent bias to each example in the batch\n      bias_outputs_list = []\n      for output, bias in zip(outputs, biases):\n        bias = tf.squeeze(bias, axis=0)\n        bias_outputs_list.append(\n            tf.nn.bias_add(output, bias,\n                           data_format=self.converted_data_format))\n      outputs = tf.concat(bias_outputs_list, 0)\n\n    if self.activation is not None:\n      return self.activation(outputs)\n\n    return outputs\n\n  def get_config(self):\n    config = {\'num_experts\': self.num_experts}\n    base_config = super(DepthwiseCondConv2D, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))'"
libs/networks/efficientnet/condconv/efficientnet_condconv_builder.py,15,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Builder for EfficientNet-CondConv models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom libs.networks.efficientnet import efficientnet_builder\nfrom libs.networks.efficientnet import efficientnet_model\nfrom libs.networks.efficientnet import utils\n\n# The input tensor is in the range of [0, 255], we need to scale them to the\n# range of [0, 1]\nMEAN_RGB = [127.0, 127.0, 127.0]\nSTDDEV_RGB = [128.0, 128.0, 128.0]\n\n\ndef efficientnet_condconv_params(model_name):\n  """"""Get efficientnet-condconv params based on model name.""""""\n  params_dict = {\n      # (width_coefficient, depth_coefficient, resolution, dropout_rate,\n      #  condconv_num_experts)\n      \'efficientnet-condconv-b0-4e\': (1.0, 1.0, 224, 0.25, 4),\n      \'efficientnet-condconv-b0-8e\': (1.0, 1.0, 224, 0.25, 8),\n      \'efficientnet-condconv-b0-8e-depth\': (1.0, 1.1, 224, 0.25, 8)\n  }\n  return params_dict[model_name]\n\n\ndef efficientnet_condconv(width_coefficient=None,\n                          depth_coefficient=None,\n                          dropout_rate=0.2,\n                          survival_prob=0.8,\n                          condconv_num_experts=None):\n  """"""Creates an efficientnet-condconv model.""""""\n  blocks_args = [\n      \'r1_k3_s11_e1_i32_o16_se0.25\',\n      \'r2_k3_s22_e6_i16_o24_se0.25\',\n      \'r2_k5_s22_e6_i24_o40_se0.25\',\n      \'r3_k3_s22_e6_i40_o80_se0.25\',\n      \'r3_k5_s11_e6_i80_o112_se0.25_cc\',\n      \'r4_k5_s22_e6_i112_o192_se0.25_cc\',\n      \'r1_k3_s11_e6_i192_o320_se0.25_cc\',\n  ]\n  global_params = efficientnet_model.GlobalParams(\n      batch_norm_momentum=0.99,\n      batch_norm_epsilon=1e-3,\n      dropout_rate=dropout_rate,\n      survival_prob=survival_prob,\n      data_format=\'channels_last\',\n      num_classes=1000,\n      width_coefficient=width_coefficient,\n      depth_coefficient=depth_coefficient,\n      depth_divisor=8,\n      min_depth=None,\n      relu_fn=tf.nn.swish,\n      # The default is TPU-specific batch norm.\n      # The alternative is tf.layers.BatchNormalization.\n      batch_norm=utils.BatchNormalization,  # TPU-specific requirement.\n      use_se=True,\n      condconv_num_experts=condconv_num_experts)\n  decoder = efficientnet_builder.BlockDecoder()\n  return decoder.decode(blocks_args), global_params\n\n\ndef get_model_params(model_name, override_params):\n  """"""Get the block args and global params for a given model.""""""\n  if model_name.startswith(\'efficientnet-condconv\'):\n    (width_coefficient, depth_coefficient, _, dropout_rate,\n     condconv_num_experts) = (\n         efficientnet_condconv_params(model_name))\n    blocks_args, global_params = efficientnet_condconv(\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        dropout_rate=dropout_rate,\n        condconv_num_experts=condconv_num_experts)\n  else:\n    raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n\n  if override_params:\n    # ValueError will be raised here if override_params has fields not included\n    # in global_params.\n    global_params = global_params._replace(**override_params)\n\n  tf.logging.info(\'global_params= %s\', global_params)\n  tf.logging.info(\'blocks_args= %s\', blocks_args)\n  return blocks_args, global_params\n\n\ndef build_model(images,\n                model_name,\n                training,\n                override_params=None,\n                model_dir=None,\n                fine_tuning=False):\n  """"""A helper functiion to creates a model and returns predicted logits.\n  Args:\n    images: input images tensor.\n    model_name: string, the predefined model name.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      efficientnet_model.GlobalParams.\n    model_dir: string, optional model dir for saving configs.\n    fine_tuning: boolean, whether the model is used for finetuning.\n  Returns:\n    logits: the logits tensor of classes.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  if not training or fine_tuning:\n    if not override_params:\n      override_params = {}\n    override_params[\'batch_norm\'] = utils.BatchNormalization\n  blocks_args, global_params = get_model_params(model_name, override_params)\n  if not training or fine_tuning:\n    global_params = global_params._replace(batch_norm=utils.BatchNormalization)\n\n  if model_dir:\n    param_file = os.path.join(model_dir, \'model_params.txt\')\n    if not tf.gfile.Exists(param_file):\n      if not tf.gfile.Exists(model_dir):\n        tf.gfile.MakeDirs(model_dir)\n      with tf.gfile.GFile(param_file, \'w\') as f:\n        tf.logging.info(\'writing to %s\' % param_file)\n        f.write(\'model_name= %s\\n\\n\' % model_name)\n        f.write(\'global_params= %s\\n\\n\' % str(global_params))\n        f.write(\'blocks_args= %s\\n\\n\' % str(blocks_args))\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    logits = model(images, training=training)\n\n  logits = tf.identity(logits, \'logits\')\n  return logits, model.endpoints\n\n\ndef build_model_base(images, model_name, training, override_params=None):\n  """"""A helper functiion to create a base model and return global_pool.\n  Args:\n    images: input images tensor.\n    model_name: string, the model name of a pre-defined MnasNet.\n    training: boolean, whether the model is constructed for training.\n    override_params: A dictionary of params for overriding. Fields must exist in\n      mnasnet_model.GlobalParams.\n  Returns:\n    features: global pool features.\n    endpoints: the endpoints for each layer.\n  Raises:\n    When model_name specified an undefined model, raises NotImplementedError.\n    When override_params has invalid fields, raises ValueError.\n  """"""\n  assert isinstance(images, tf.Tensor)\n  blocks_args, global_params = get_model_params(model_name, override_params)\n\n  with tf.variable_scope(model_name):\n    model = efficientnet_model.Model(blocks_args, global_params)\n    features = model(images, training=training, features_only=True)\n\n  features = tf.identity(features, \'global_pool\')\n  return features, model.endpoints'"
