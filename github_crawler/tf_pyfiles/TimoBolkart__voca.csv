file_path,api_count,code
config_parser.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport os\nimport configparser\n\ndef set_default_paramters(config):\n    config.add_section(\'Input Output\')\n    config.set(\'Input Output\', \'checkpoint_dir\', \'./training\')\n    config.set(\'Input Output\', \'expression_basis_fname\', \'./training_data/init_expression_basis.npy\')\n    config.set(\'Input Output\', \'template_fname\', \'./template/FLAME_sample.ply\')\n    config.set(\'Input Output\', \'deepspeech_graph_fname\', \'./ds_graph/output_graph.pb\')\n\n    config.set(\'Input Output\', \'verts_mmaps_path\', \'./training_data/data_verts.npy\')\n    config.set(\'Input Output\', \'raw_audio_path\', \'./training_data/raw_audio_fixed.pkl\')\n    config.set(\'Input Output\', \'processed_audio_path\', \'\')\n    config.set(\'Input Output\', \'templates_path\', \'./training_data/templates.pkl\')\n    config.set(\'Input Output\', \'data2array_verts_path\', \'./training_data/subj_seq_to_idx.pkl\')\n\n    #Audio paramters\n    config.add_section(\'Audio Parameters\')\n    config.set(\'Audio Parameters\', \'audio_feature_type\', \'deepspeech\')       # deepspeech\n    config.set(\'Audio Parameters\', \'num_audio_features\', \'29\')               # 29\n    config.set(\'Audio Parameters\', \'audio_window_size\', \'16\')                # 16\n    config.set(\'Audio Parameters\', \'audio_window_stride\', \'1\')               # 1\n    config.set(\'Audio Parameters\', \'condition_speech_features\', \'True\')      # True\n    config.set(\'Audio Parameters\', \'speech_encoder_size_factor\', \'1.0\')      # 1\n\n    # Model paramters\n    config.add_section(\'Model Parameters\')\n    config.set(\'Model Parameters\', \'num_vertices\', \'5023\')                    # 5023\n    config.set(\'Model Parameters\', \'expression_dim\', \'50\')                    # 50\n    config.set(\'Model Parameters\', \'init_expression\', \'True\')                 # True\n\n    # Number of consecutive frames that are regressed in the same batch (must be >=2 if velocity is used)\n    config.set(\'Model Parameters\', \'num_consecutive_frames\', \'2\')             # 2\n    config.set(\'Model Parameters\', \'absolute_reconstruction_loss\', \'False\')   # False\n    config.set(\'Model Parameters\', \'velocity_weight\', \'10.0\')                 # 10.0\n    config.set(\'Model Parameters\', \'acceleration_weight\', \'0.0\')              # 0.0\n    config.set(\'Model Parameters\', \'verts_regularizer_weight\', \'0.0\')         # 0.0\n\n    config.add_section(\'Data Setup\')\n    config.set(\'Data Setup\', \'subject_for_training\',\n               ""FaceTalk_170728_03272_TA FaceTalk_170904_00128_TA FaceTalk_170725_00137_TA FaceTalk_170915_00223_TA""\n               "" FaceTalk_170811_03274_TA FaceTalk_170913_03279_TA FaceTalk_170904_03276_TA FaceTalk_170912_03278_TA "")\n    config.set(\'Data Setup\', \'sequence_for_training\',\n                ""sentence01 sentence02 sentence03 sentence04 sentence05 sentence06 sentence07 sentence08 sentence09 sentence10 ""\n                ""sentence11 sentence12 sentence13 sentence14 sentence15 sentence16 sentence17 sentence18 sentence19 sentence20 ""\n                ""sentence21 sentence22 sentence23 sentence24 sentence25 sentence26 sentence27 sentence28 sentence29 sentence30 ""\n                ""sentence31 sentence32 sentence33 sentence34 sentence35 sentence36 sentence37 sentence38 sentence39 sentence40"")\n    config.set(\'Data Setup\', \'subject_for_validation\', ""FaceTalk_170811_03275_TA FaceTalk_170908_03277_TA"")\n    config.set(\'Data Setup\', \'sequence_for_validation\',\n                ""sentence21 sentence22 sentence23 sentence24 sentence25 sentence26 sentence27 sentence28 sentence29 sentence30 ""\n                ""sentence31 sentence32 sentence33 sentence34 sentence35 sentence36 sentence37 sentence38 sentence39 sentence40"")\n    config.set(\'Data Setup\', \'subject_for_testing\', ""FaceTalk_170809_00138_TA FaceTalk_170731_00024_TA "")\n    config.set(\'Data Setup\', \'sequence_for_testing\',\n                ""sentence21 sentence22 sentence23 sentence24 sentence25 sentence26 sentence27 sentence28 sentence29 sentence30 ""\n                ""sentence31 sentence32 sentence33 sentence34 sentence35 sentence36 sentence37 sentence38 sentence39 sentence40"")\n\n    config.add_section(\'Learning Parameters\')\n    config.set(\'Learning Parameters\', \'batch_size\', \'64\')                     # 64\n    config.set(\'Learning Parameters\', \'learning_rate\', \'1e-4\')                # 1e-4\n    config.set(\'Learning Parameters\', \'decay_rate\', \'1.0\')                    # 1.0\n    config.set(\'Learning Parameters\', \'epoch_num\', \'100\')                     # 100\n    config.set(\'Learning Parameters\', \'adam_beta1_value\', \'0.9\')              # 0.9\n\n    config.add_section(\'Visualization Parameters\')\n    config.set(\'Visualization Parameters\', \'num_render_sequences\', \'3\')\n\n\ndef create_default_config(fname):\n    config = configparser.ConfigParser()\n    set_default_paramters(config)\n\n    with open(fname, \'w\') as configfile:\n        config.write(configfile)\n        configfile.close()\n\ndef read_config(fname):\n    if not os.path.exists(fname):\n        print(\'Config not found %s\' % fname)\n        return\n\n    config = configparser.RawConfigParser()\n    config.read(fname)\n\n    config_parms = {}\n    config_parms[\'checkpoint_dir\'] = config.get(\'Input Output\', \'checkpoint_dir\')\n    config_parms[\'expression_basis_fname\'] = config.get(\'Input Output\', \'expression_basis_fname\')\n    config_parms[\'template_fname\'] = config.get(\'Input Output\', \'template_fname\')\n    config_parms[\'deepspeech_graph_fname\'] = config.get(\'Input Output\', \'deepspeech_graph_fname\')\n\n    config_parms[\'verts_mmaps_path\'] = config.get(\'Input Output\', \'verts_mmaps_path\')\n    config_parms[\'raw_audio_path\'] = config.get(\'Input Output\', \'raw_audio_path\')\n    config_parms[\'processed_audio_path\'] = config.get(\'Input Output\', \'processed_audio_path\')\n    config_parms[\'templates_path\'] = config.get(\'Input Output\', \'templates_path\')\n    config_parms[\'data2array_verts_path\'] = config.get(\'Input Output\', \'data2array_verts_path\')\n\n    config_parms[\'audio_feature_type\'] = config.get(\'Audio Parameters\', \'audio_feature_type\')\n    config_parms[\'num_audio_features\'] = config.getint(\'Audio Parameters\', \'num_audio_features\')\n    config_parms[\'audio_window_size\'] = config.getint(\'Audio Parameters\', \'audio_window_size\')\n    config_parms[\'audio_window_stride\'] = config.getint(\'Audio Parameters\', \'audio_window_stride\')\n    config_parms[\'condition_speech_features\'] = config.getboolean(\'Audio Parameters\', \'condition_speech_features\')\n    config_parms[\'speech_encoder_size_factor\'] = config.getfloat(\'Audio Parameters\', \'speech_encoder_size_factor\')\n\n\n    config_parms[\'num_vertices\'] = config.getint(\'Model Parameters\', \'num_vertices\')\n    config_parms[\'expression_dim\'] = config.getint(\'Model Parameters\', \'expression_dim\')\n    config_parms[\'init_expression\'] = config.getboolean(\'Model Parameters\', \'init_expression\')\n\n    config_parms[\'num_consecutive_frames\'] = config.getint(\'Model Parameters\', \'num_consecutive_frames\')\n    config_parms[\'absolute_reconstruction_loss\'] = config.getboolean(\'Model Parameters\', \'absolute_reconstruction_loss\')\n    config_parms[\'velocity_weight\'] = config.getfloat(\'Model Parameters\', \'velocity_weight\')\n    config_parms[\'acceleration_weight\'] = config.getfloat(\'Model Parameters\', \'acceleration_weight\')\n    config_parms[\'verts_regularizer_weight\'] = config.getfloat(\'Model Parameters\', \'verts_regularizer_weight\')\n\n    config_parms[\'subject_for_training\'] = config.get(\'Data Setup\', \'subject_for_training\')\n    config_parms[\'sequence_for_training\'] = config.get(\'Data Setup\', \'sequence_for_training\')\n    config_parms[\'subject_for_validation\'] = config.get(\'Data Setup\', \'subject_for_validation\')\n    config_parms[\'sequence_for_validation\'] = config.get(\'Data Setup\', \'sequence_for_validation\')\n    config_parms[\'subject_for_testing\'] = config.get(\'Data Setup\', \'subject_for_testing\')\n    config_parms[\'sequence_for_testing\'] = config.get(\'Data Setup\', \'sequence_for_testing\')\n\n    config_parms[\'batch_size\'] = config.getint(\'Learning Parameters\', \'batch_size\')\n    config_parms[\'learning_rate\'] = config.getfloat(\'Learning Parameters\', \'learning_rate\')\n    config_parms[\'decay_rate\'] = config.getfloat(\'Learning Parameters\', \'decay_rate\')\n    config_parms[\'epoch_num\'] = config.getint(\'Learning Parameters\', \'epoch_num\')\n    config_parms[\'adam_beta1_value\'] = config.getfloat(\'Learning Parameters\', \'adam_beta1_value\')\n\n    config_parms[\'num_render_sequences\'] = config.getint(\'Visualization Parameters\', \'num_render_sequences\')\n    return config_parms\n\nif __name__ == \'__main__\':\n    pkg_path, _ = os.path.split(os.path.realpath(__file__))\n    config_fname = os.path.join(pkg_path, \'training_config.cfg\')\n\n    print(\'Writing default config file - %s\' % config_fname)\n    create_default_config(config_fname)\n'"
edit_sequences.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport os\nimport glob\nimport argparse\nimport numpy as np\nfrom psbody.mesh import Mesh\nfrom utils.inference import output_sequence_meshes\nfrom smpl_webuser.serialization import load_model\n\nparser = argparse.ArgumentParser(description=\'Edit VOCA motion sequences\')\n\nparser.add_argument(\'--source_path\', default=\'\', help=\'input sequence path\')\nparser.add_argument(\'--out_path\', default=\'\', help=\'output path\')\nparser.add_argument(\'--flame_model_path\', default=\'./flame/generic_model.pkl\', help=\'path to the FLAME model\')\nparser.add_argument(\'--mode\', default=\'shape\', help=\'edit shape, head pose, or add eye blinks\')\nparser.add_argument(\'--index\', type=int, default=0, help=\'parameter to be varied\')\nparser.add_argument(\'--max_variation\', type=float, default=1.0, help=\'maximum variation\')\nparser.add_argument(\'--num_blinks\', type=int, default=1, help=\'number of eye blinks\')\nparser.add_argument(\'--blink_duration\', type=int, default=15, help=\'blink_duration\')\n\nargs = parser.parse_args()\nsource_path = args.source_path\nout_path = args.out_path\nflame_model_fname = args.flame_model_path\n\ndef add_eye_blink(source_path, out_path, flame_model_fname, num_blinks, blink_duration):\n    \'\'\'\n    Load existing animation sequence in ""zero pose"" and add eye blinks over time\n    :param source_path:         path of the animation sequence (files must be provided in OBJ file format)\n    :param out_path:            output path of the altered sequence\n    :param flame_model_fname:   path of the FLAME model\n    :param num_blinks:          number of blinks within the sequence\n    :param blink_duration:      duration of a blink in number of frames\n    \'\'\'\n\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n\n    # Load sequence files\n    sequence_fnames = sorted(glob.glob(os.path.join(source_path, \'*.obj\')))\n    num_frames = len(sequence_fnames)\n    if num_frames == 0:\n        print(\'No sequence meshes found\')\n        return\n\n    # Load FLAME head model\n    model = load_model(flame_model_fname)\n\n    blink_exp_betas = np.array(\n        [0.04676158497927314, 0.03758675711005459, -0.8504121184951298, 0.10082324210507627, -0.574142329926028,\n         0.6440016589938355, 0.36403779939335984, 0.21642312586261656, 0.6754551784690193, 1.80958618462892,\n         0.7790133813372259, -0.24181691256476057, 0.826280685961679, -0.013525679499256753, 1.849393698014113,\n         -0.263035686247264, 0.42284248271332153, 0.10550891351425384, 0.6720993875023772, 0.41703592560736436,\n         3.308019065485072, 1.3358509602858895, 1.2997143108969278, -1.2463587328652894, -1.4818961382824924,\n         -0.6233880069345369, 0.26812528424728455, 0.5154889093160832, 0.6116267181402183, 0.9068826814583771,\n         -0.38869613253448576, 1.3311776710005476, -0.5802565274559162, -0.7920775624092143, -1.3278601781150017,\n         -1.2066425872386706, 0.34250140710360893, -0.7230686724732668, -0.6859285483325263, -1.524877347586566,\n         -1.2639479212965923, -0.019294228307535275, 0.2906175769381998, -1.4082782880837976, 0.9095436721066045,\n         1.6007365724960054, 2.0302381182163574, 0.5367600947801505, -0.12233184771794232, -0.506024823810769,\n         2.4312326730634783, 0.5622323258974669, 0.19022395712837198, -0.7729758559103581, -1.5624233513002923,\n         0.8275863297957926, 1.1661887586553132, 1.2299311381779416, -1.4146929897142397, -0.42980549225554004,\n         -1.4282801579740614, 0.26172301287347266, -0.5109318114918897, -0.6399495909195524, -0.733476856285442,\n         1.219652074726591, 0.08194907995352405, 0.4420398361785991, -1.184769973221183, 1.5126082924326332,\n         0.4442281271081217, -0.005079477284341147, 1.764084274265486, 0.2815940264026848, 0.2898827213634057,\n         -0.3686662696397026, 1.9125365942683656, 2.1801452989500274, -2.3915065327980467, 0.5794919897154226,\n         -1.777680085517591, 2.9015718628823604, -2.0516886588315777, 0.4146899057365943, -0.29917763685660903,\n         -0.5839240983516372, 2.1592457102697007, -0.8747902386178202, -0.5152943072876817, 0.12620001057735733,\n         1.3144109838803493, -0.5027032013330108, 1.2160353388774487, 0.7543834001473375, -3.512095548974531,\n         -0.9304382646186183, -0.30102930208709433, 0.9332135959962723, -0.52926196689098, 0.23509772959302958])\n\n    step = blink_duration//3\n    blink_weights = np.hstack((np.interp(np.arange(step), [0,step], [0,1]), np.ones(step), np.interp(np.arange(step), [0,step], [1,0])))\n\n    frequency = num_frames // (num_blinks+1)\n    weights = np.zeros(num_frames)\n    for i in range(num_blinks):\n        x1 = (i+1)*frequency-blink_duration//2\n        x2 = x1+3*step\n        if x1 >= 0 and x2 < weights.shape[0]:\n            weights[x1:x2] = blink_weights\n\n    predicted_vertices = np.zeros((num_frames, model.v_template.shape[0], model.v_template.shape[1]))\n\n    for frame_idx in range(num_frames):\n        model.v_template[:] = Mesh(filename=sequence_fnames[frame_idx]).v\n        model.betas[300:] = weights[frame_idx]*blink_exp_betas\n        predicted_vertices[frame_idx] = model.r\n\n    output_sequence_meshes(predicted_vertices, Mesh(model.v_template, model.f), out_path)\n\n\ndef alter_sequence_shape(source_path, out_path, flame_model_fname, pc_idx=0, pc_range=(0,3)):\n    \'\'\'\n    Load existing animation sequence in ""zero pose"" and change the identity dependent shape over time.\n    :param source_path:         path of the animation sequence (files must be provided in OBJ file format)\n    :param out_path:            output path of the altered sequence\n    :param flame_model_fname:   path of the FLAME model\n    :param pc_idx               Identity shape parameter to be varied in [0,300) as FLAME provides 300 shape paramters\n    :param pc_range             Tuple (start/end, max/min) defining the range of the shape variation.\n                                i.e. (0,3) varies the shape from 0 to 3 stdev and back to 0\n    \'\'\'\n\n    if pc_idx < 0 or pc_idx >= 300:\n        print(\'shape parameter index out of range [0,300)\')\n        return\n\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n\n    # Load sequence files\n    sequence_fnames = sorted(glob.glob(os.path.join(source_path, \'*.obj\')))\n    num_frames = len(sequence_fnames)\n    if num_frames == 0:\n        print(\'No sequence meshes found\')\n        return\n\n    # Load FLAME head model\n    model = load_model(flame_model_fname)\n    model_parms = np.zeros((num_frames, 300))\n\n    # Generate interpolated shape parameters for each frame\n    x1, y1 = [0, num_frames/2], pc_range\n    x2, y2 = [num_frames/2, num_frames], pc_range[::-1]\n\n    xsteps1 = np.arange(0, num_frames/2)\n    xsteps2 = np.arange(num_frames/2, num_frames)\n\n    model_parms[:, pc_idx] = np.hstack((np.interp(xsteps1, x1, y1), np.interp(xsteps2, x2, y2)))\n\n    predicted_vertices = np.zeros((num_frames, model.v_template.shape[0], model.v_template.shape[1]))\n\n    for frame_idx in range(num_frames):\n        model.v_template[:] = Mesh(filename=sequence_fnames[frame_idx]).v\n        model.betas[:300] = model_parms[frame_idx]\n        predicted_vertices[frame_idx] = model.r\n\n    output_sequence_meshes(predicted_vertices, Mesh(model.v_template, model.f), out_path)\n\ndef alter_sequence_head_pose(source_path, out_path, flame_model_fname, pose_idx=3, rot_angle=np.pi/6):\n    \'\'\'\n    Load existing animation sequence in ""zero pose"" and change the head pose (i.e. rotation around the neck) over time.\n    :param source_path:         path of the animation sequence (files must be provided in OBJ file format)\n    :param out_path:            output path of the altered sequence\n    :param flame_model_fname:   path of the FLAME model\n    :param pose_idx:            head pose parameter to be varied in [3,6)\n    :param rot_angle:           maximum rotation angle in [0,2pi)\n    \'\'\'\n\n    if pose_idx < 3 or pose_idx >= 6:\n        print(\'pose parameter index out of range [3,6)\')\n        return\n\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n\n    # Load sequence files\n    sequence_fnames = sorted(glob.glob(os.path.join(source_path, \'*.obj\')))\n    num_frames = len(sequence_fnames)\n    if num_frames == 0:\n        print(\'No sequence meshes found\')\n        return\n\n    # Load FLAME head model\n    model = load_model(flame_model_fname)\n    model_parms = np.zeros((num_frames, model.pose.shape[0]))\n\n    # Generate interpolated pose parameters for each frame\n    x1, y1 = [0, num_frames//4], [0, rot_angle]\n    x2, y2 = [num_frames//4, num_frames//2], [rot_angle, 0]\n    x3, y3 = [num_frames//2, 3*num_frames//4], [0, -rot_angle]\n    x4, y4 = [3*num_frames//4, num_frames], [-rot_angle, 0]\n\n    xsteps1 = np.arange(0, num_frames//4)\n    xsteps2 = np.arange(num_frames//4, num_frames/2)\n    xsteps3 = np.arange(num_frames//2, 3*num_frames//4)\n    xsteps4 = np.arange(3*num_frames//4, num_frames)\n\n    model_parms[:, pose_idx] = np.hstack((np.interp(xsteps1, x1, y1),\n                                   np.interp(xsteps2, x2, y2),\n                                   np.interp(xsteps3, x3, y3),\n                                   np.interp(xsteps4, x4, y4)))\n\n    predicted_vertices = np.zeros((num_frames, model.v_template.shape[0], model.v_template.shape[1]))\n\n    for frame_idx in range(num_frames):\n        model.v_template[:] = Mesh(filename=sequence_fnames[frame_idx]).v\n        model.pose[:] = model_parms[frame_idx]\n        predicted_vertices[frame_idx] = model.r\n\n    output_sequence_meshes(predicted_vertices, Mesh(model.v_template, model.f), out_path)\n\nif(args.mode == \'shape\'):\n    pc_idx = args.index\n    pc_range = (0, args.max_variation)\n    alter_sequence_shape(source_path, out_path, flame_model_fname, pc_idx=pc_idx, pc_range=pc_range)\nelif(args.mode == \'pose\'):\n    pose_idx = args.index\n    rot_angle = args.max_variation\n    alter_sequence_head_pose(source_path, out_path, flame_model_fname, pose_idx=pose_idx, rot_angle=rot_angle)\nelif(args.mode == \'blink\'):\n    num_blinks = args.num_blinks\n    blink_duration = args.blink_duration\n    add_eye_blink(source_path, out_path, flame_model_fname, num_blinks, blink_duration)\nelse:\n    print(\'Unknown mode\')'"
run_training.py,1,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport os\nimport stat\nimport glob\nimport shutil\nimport subprocess\nimport configparser\nimport tensorflow as tf\n\nfrom config_parser import read_config, create_default_config\nfrom utils.data_handler import DataHandler\nfrom utils.batcher import Batcher\nfrom utils.voca_model import VOCAModel as Model\n\n\ndef main():\n    # Prior to training, please adapt the hyper parameters in the config_parser.py and run the script to generate\n    # the training config file use to train your own VOCA model.\n\n    pkg_path, _ = os.path.split(os.path.realpath(__file__))\n    init_config_fname = os.path.join(pkg_path, \'training_config.cfg\')\n    if not os.path.exists(init_config_fname):\n        print(\'Config not found %s\' % init_config_fname)\n        create_default_config(init_config_fname)\n\n    config = configparser.ConfigParser()\n    config.read(init_config_fname)\n\n    # Path to cache the processed audio\n    config.set(\'Input Output\', \'processed_audio_path\', \'./training_data/processed_audio_%s.pkl\' % config.get(\'Audio Parameters\', \'audio_feature_type\'))\n\n    checkpoint_dir = config.get(\'Input Output\', \'checkpoint_dir\')\n    if os.path.exists(checkpoint_dir):\n        print(\'Checkpoint dir already exists %s\' % checkpoint_dir)\n        key = input(\'Press ""q"" to quit, ""x"" to erase existing folder, and any other key to continue training: \')\n        if key.lower() == \'q\':\n            return\n        elif key.lower() == \'x\':\n            try:\n                shutil.rmtree(checkpoint_dir, ignore_errors=True)\n            except:\n                print(\'Failed deleting checkpoint directory\')\n\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n\n    config_fname = os.path.join(checkpoint_dir, \'config.pkl\')\n    if os.path.exists(config_fname):\n        print(\'Use existing config %s\' % config_fname)\n    else:\n        with open(config_fname, \'w\') as fp:\n            config.write(fp)\n            fp.close()\n\n    config = read_config(config_fname)\n    data_handler = DataHandler(config)\n    batcher = Batcher(data_handler)\n\n    with tf.Session() as session:\n        model = Model(session=session, config=config, batcher=batcher)\n        model.build_graph()\n        model.load()\n        model.train()\n\nif __name__ == \'__main__\':\n    main()'"
run_voca.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\n\nimport os\nimport glob\nimport argparse\nfrom utils.inference import inference\n\ndef str2bool(val):\n    if isinstance(val, bool):\n        return val\n    elif isinstance(val, str):\n        if val.lower() in [\'true\', \'t\', \'yes\', \'y\']:\n            return True\n        elif val.lower() in [\'false\', \'f\', \'no\', \'n\']:\n            return False\n    return False\n\nparser = argparse.ArgumentParser(description=\'Voice operated character animation\')\nparser.add_argument(\'--tf_model_fname\', default=\'./model/gstep_52280.model\', help=\'Path to trained VOCA model\')\nparser.add_argument(\'--ds_fname\', default=\'./ds_graph/output_graph.pb\', help=\'Path to trained DeepSpeech model\')\nparser.add_argument(\'--audio_fname\', default=\'./audio/test_sentence.wav\', help=\'Path of input speech sequence\')\nparser.add_argument(\'--template_fname\', default=\'./template/FLAME_sample.ply\', help=\'Path of ""zero pose"" template mesh in"" FLAME topology to be animated\')\nparser.add_argument(\'--condition_idx\', type=int, default=3, help=\'Subject condition id in [1,8]\')\nparser.add_argument(\'--out_path\', default=\'./voca/animation_output\', help=\'Output path\')\nparser.add_argument(\'--visualize\', default=\'True\', help=\'Visualize animation\')\n\nargs = parser.parse_args()\ntf_model_fname = args.tf_model_fname\nds_fname = args.ds_fname\naudio_fname = args.audio_fname\ntemplate_fname = args.template_fname\ncondition_idx = args.condition_idx\nout_path = args.out_path\n\nif not os.path.exists(out_path):\n    os.makedirs(out_path)\n\ninference(tf_model_fname, ds_fname, audio_fname, template_fname, condition_idx, out_path, str2bool(args.visualize))\n\n'"
sample_templates.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\n\nimport os\nimport argparse\nimport numpy as np\nfrom psbody.mesh import Mesh\nfrom smpl_webuser.serialization import load_model\n\n\nparser = argparse.ArgumentParser(description=\'Sample templates from FLAME\')\nparser.add_argument(\'--flame_model_path\', default=\'./flame/generic_model.pkl\', help=\'path to the FLAME model\')\nparser.add_argument(\'--num_samples\', type=int, default=1, help=\'Number of samples\')\nparser.add_argument(\'--out_path\', default=\'./template\', help=\'Output path\')\n\nargs = parser.parse_args()\nflame_model_fname = args.flame_model_path\nnum_samples = args.num_samples\nout_path = args.out_path\n\nif not os.path.exists(out_path):\n    os.makedirs(out_path)\n\nout_fname = lambda num : os.path.join(out_path, \'FLAME_sample_%03d.ply\' % num)\nflame_model = load_model(flame_model_fname)\n\nfor i in range(num_samples):\n    # Assign random shape parameters.\n    # Beware changing expression parameters (i.e. betas[300:400]) or pose will result in meshes that cannot be used\n    # as template for VOCA as VOCA requires ""zero pose"" templates in neutral expression\n    flame_model.betas[:100] = np.random.randn(100) * 1.0\n\n    # Output mesh in ""zero pose""\n    Mesh(flame_model.r, flame_model.f).write_ply(out_fname(i+1))\n'"
visualize_sequence.py,0,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\n\nimport os\nimport glob\nimport argparse\nimport numpy as np\nfrom subprocess import call\nfrom psbody.mesh import Mesh\nfrom utils.inference import render_sequence_meshes\n\nparser = argparse.ArgumentParser(description='Sequence visualization')\nparser.add_argument('--sequence_path', default='./animation_output', help='Path to motion sequence')\nparser.add_argument('--audio_fname', default='', help='Path of speech sequence')\nparser.add_argument('--out_path', default='./animation_visualization', help='Output path')\n\n\nargs = parser.parse_args()\nsequence_path = args.sequence_path\naudio_fname = args.audio_fname\nout_path = args.out_path\n\nsequence_fnames = sorted(glob.glob(os.path.join(sequence_path, '*.obj')))\nif len(sequence_fnames) == 0:\n    print('No meshes found')\n\nsequence_vertices = []\nf = None\nfor frame_idx, mesh_fname in enumerate(sequence_fnames):\n    frame = Mesh(filename=mesh_fname)\n    sequence_vertices.append(frame.v)\n    if f is None:\n        f = frame.f\ntemplate = Mesh(sequence_vertices[0], f)\nsequence_vertices = np.stack(sequence_vertices)\nrender_sequence_meshes(audio_fname, sequence_vertices, template, out_path)"""
smpl_webuser/__init__.py,0,"b""'''\n'''"""
smpl_webuser/lbs.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines linear blend skinning for the SMPL loader which \ndefines the effect of bones and blendshapes on the vertices of the template mesh.\n\nModules included:\n- global_rigid_transformation: \n  computes global rotation & translation of the model\n- verts_core: [overloaded function inherited from verts.verts_core]\n  computes the blending of joint-influences for each vertex based on type of skinning\n\n'''\n\nfrom smpl_webuser.posemapper import posemap\nimport chumpy\nimport numpy as np\n\ndef global_rigid_transformation(pose, J, kintree_table, xp):\n    results = {}\n    pose = pose.reshape((-1,3))\n    id_to_col = {kintree_table[1,i] : i for i in range(kintree_table.shape[1])}\n    parent = {i : id_to_col[kintree_table[0,i]] for i in range(1, kintree_table.shape[1])}\n\n    if xp == chumpy:\n        from smpl_webuser.posemapper import Rodrigues\n        rodrigues = lambda x : Rodrigues(x)\n    else:\n        import cv2\n        rodrigues = lambda x : cv2.Rodrigues(x)[0]\n\n    with_zeros = lambda x : xp.vstack((x, xp.array([[0.0, 0.0, 0.0, 1.0]])))\n    results[0] = with_zeros(xp.hstack((rodrigues(pose[0,:]), J[0,:].reshape((3,1)))))        \n        \n    for i in range(1, kintree_table.shape[1]):\n        results[i] = results[parent[i]].dot(with_zeros(xp.hstack((\n            rodrigues(pose[i,:]),\n            ((J[i,:] - J[parent[i],:]).reshape((3,1)))\n            ))))\n\n    pack = lambda x : xp.hstack([np.zeros((4, 3)), x.reshape((4,1))])\n    \n    results = [results[i] for i in sorted(results.keys())]\n    results_global = results\n\n    if True:\n        results2 = [results[i] - (pack(\n            results[i].dot(xp.concatenate( ( (J[i,:]), 0 ) )))\n            ) for i in range(len(results))]\n        results = results2\n    result = xp.dstack(results)\n    return result, results_global\n\n\ndef verts_core(pose, v, J, weights, kintree_table, want_Jtr=False, xp=chumpy):\n    A, A_global = global_rigid_transformation(pose, J, kintree_table, xp)\n    T = A.dot(weights.T)\n\n    rest_shape_h = xp.vstack((v.T, np.ones((1, v.shape[0]))))\n        \n    v =(T[:,0,:] * rest_shape_h[0, :].reshape((1, -1)) + \n        T[:,1,:] * rest_shape_h[1, :].reshape((1, -1)) + \n        T[:,2,:] * rest_shape_h[2, :].reshape((1, -1)) + \n        T[:,3,:] * rest_shape_h[3, :].reshape((1, -1))).T\n\n    v = v[:,:3] \n    \n    if not want_Jtr:\n        return v\n    Jtr = xp.vstack([g[:3,3] for g in A_global])\n    return (v, Jtr)\n    \n"""
smpl_webuser/posemapper.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis module defines the mapping of joint-angles to pose-blendshapes. \n\nModules included:\n- posemap:\n  computes the joint-to-pose blend shape mapping given a mapping type as input\n\n'''\n\nimport chumpy as ch\nimport numpy as np\nimport cv2\n\n\nclass Rodrigues(ch.Ch):\n    dterms = 'rt'\n    \n    def compute_r(self):\n        return cv2.Rodrigues(self.rt.r)[0]\n    \n    def compute_dr_wrt(self, wrt):\n        if wrt is self.rt:\n            return cv2.Rodrigues(self.rt.r)[1].T\n\n\ndef lrotmin(p): \n    if isinstance(p, np.ndarray):\n        p = p.ravel()[3:]\n        return np.concatenate([(cv2.Rodrigues(np.array(pp))[0]-np.eye(3)).ravel() for pp in p.reshape((-1,3))]).ravel()        \n    if p.ndim != 2 or p.shape[1] != 3:\n        p = p.reshape((-1,3))\n    p = p[1:]\n    return ch.concatenate([(Rodrigues(pp)-ch.eye(3)).ravel() for pp in p]).ravel()\n\ndef posemap(s):\n    if s == 'lrotmin':\n        return lrotmin\n    else:\n        raise Exception('Unknown posemapping: %s' % (str(s),))\n"""
smpl_webuser/serialization.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines the serialization functions of the SMPL model. \n\nModules included:\n- save_model:\n  saves the SMPL model to a given file location as a .pkl file\n- load_model:\n  loads the SMPL model from a given file location (i.e. a .pkl file location), \n  or a dictionary object.\n\n'''\n\n__all__ = ['load_model', 'save_model']\n\nimport numpy as np\nimport _pickle as pickle\nimport chumpy as ch\nfrom chumpy.ch import MatVecMult\nfrom smpl_webuser.posemapper import posemap\nfrom smpl_webuser.verts import verts_core\n    \n# def save_model(model, fname):\n#     m0 = model\n#     trainer_dict = {'v_template': np.asarray(m0.v_template),'J': np.asarray(m0.J),'weights': np.asarray(m0.weights),'kintree_table': m0.kintree_table,'f': m0.f, 'bs_type': m0.bs_type, 'posedirs': np.asarray(m0.posedirs)}\n#     if hasattr(model, 'J_regressor'):\n#         trainer_dict['J_regressor'] = m0.J_regressor\n#     if hasattr(model, 'J_regressor_prior'):\n#         trainer_dict['J_regressor_prior'] = m0.J_regressor_prior\n#     if hasattr(model, 'weights_prior'):\n#         trainer_dict['weights_prior'] = m0.weights_prior\n#     if hasattr(model, 'shapedirs'):\n#         trainer_dict['shapedirs'] = m0.shapedirs\n#     if hasattr(model, 'vert_sym_idxs'):\n#         trainer_dict['vert_sym_idxs'] = m0.vert_sym_idxs\n#     if hasattr(model, 'bs_style'):\n#         trainer_dict['bs_style'] = model.bs_style\n#     else:\n#         trainer_dict['bs_style'] = 'lbs'\n#     pickle.dump(trainer_dict, open(fname, 'w'), -1)\n\n\ndef backwards_compatibility_replacements(dd):\n\n    # replacements\n    if 'default_v' in dd:\n        dd['v_template'] = dd['default_v']\n        del dd['default_v']\n    if 'template_v' in dd:\n        dd['v_template'] = dd['template_v']\n        del dd['template_v']\n    if 'joint_regressor' in dd:\n        dd['J_regressor'] = dd['joint_regressor']\n        del dd['joint_regressor']\n    if 'blendshapes' in dd:\n        dd['posedirs'] = dd['blendshapes']\n        del dd['blendshapes']\n    if 'J' not in dd:\n        dd['J'] = dd['joints']\n        del dd['joints']\n\n    # defaults\n    if 'bs_style' not in dd:\n        dd['bs_style'] = 'lbs'\n\n\n\ndef ready_arguments(fname_or_dict):\n\n    if not isinstance(fname_or_dict, dict):\n        dd = pickle.load(open(fname_or_dict, 'rb'), encoding='latin1')\n    else:\n        dd = fname_or_dict\n        \n    backwards_compatibility_replacements(dd)\n        \n    want_shapemodel = 'shapedirs' in dd\n    nposeparms = dd['kintree_table'].shape[1]*3\n\n    if 'trans' not in dd:\n        dd['trans'] = np.zeros(3)\n    if 'pose' not in dd:\n        dd['pose'] = np.zeros(nposeparms)\n    if 'shapedirs' in dd and 'betas' not in dd:\n        dd['betas'] = np.zeros(dd['shapedirs'].shape[-1])\n\n    for s in ['v_template', 'weights', 'posedirs', 'pose', 'trans', 'shapedirs', 'betas', 'J']:\n        if (s in dd) and not hasattr(dd[s], 'dterms'):\n            dd[s] = ch.array(dd[s])\n\n    if want_shapemodel:\n        dd['v_shaped'] = dd['shapedirs'].dot(dd['betas'])+dd['v_template']\n        v_shaped = dd['v_shaped']\n        J_tmpx = MatVecMult(dd['J_regressor'], v_shaped[:,0])        \n        J_tmpy = MatVecMult(dd['J_regressor'], v_shaped[:,1])        \n        J_tmpz = MatVecMult(dd['J_regressor'], v_shaped[:,2])        \n        dd['J'] = ch.vstack((J_tmpx, J_tmpy, J_tmpz)).T    \n        dd['v_posed'] = v_shaped + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n    else:    \n        dd['v_posed'] = dd['v_template'] + dd['posedirs'].dot(posemap(dd['bs_type'])(dd['pose']))\n            \n    return dd\n\n\n\ndef load_model(fname_or_dict):\n    dd = ready_arguments(fname_or_dict)\n    \n    args = {\n        'pose': dd['pose'],\n        'v': dd['v_posed'],\n        'J': dd['J'],\n        'weights': dd['weights'],\n        'kintree_table': dd['kintree_table'],\n        'xp': ch,\n        'want_Jtr': True,\n        'bs_style': dd['bs_style']\n    }\n    \n    result, Jtr = verts_core(**args)\n    result = result + dd['trans'].reshape((1,3))\n    result.J_transformed = Jtr + dd['trans'].reshape((1,3))\n\n    for k, v in dd.items():\n        setattr(result, k, v)\n        \n    return result\n\n"""
smpl_webuser/verts.py,0,"b""'''\nCopyright 2015 Matthew Loper, Naureen Mahmood and the Max Planck Gesellschaft.  All rights reserved.\nThis software is provided for research purposes only.\nBy using this software you agree to the terms of the SMPL Model license here http://smpl.is.tue.mpg.de/license\n\nMore information about SMPL is available here http://smpl.is.tue.mpg.\nFor comments or questions, please email us at: smpl@tuebingen.mpg.de\n\n\nAbout this file:\n================\nThis file defines the basic skinning modules for the SMPL loader which \ndefines the effect of bones and blendshapes on the vertices of the template mesh.\n\nModules included:\n- verts_decorated: \n  creates an instance of the SMPL model which inherits model attributes from another \n  SMPL model.\n- verts_core: [overloaded function inherited by lbs.verts_core]\n  computes the blending of joint-influences for each vertex based on type of skinning\n\n'''\n\nimport chumpy\nimport smpl_webuser.lbs as lbs\nfrom smpl_webuser.posemapper import posemap\nimport scipy.sparse as sp\nfrom chumpy.ch import MatVecMult\n\ndef ischumpy(x): return hasattr(x, 'dterms')\n\ndef verts_decorated(trans, pose, \n    v_template, J, weights, kintree_table, bs_style, f,\n    bs_type=None, posedirs=None, betas=None, shapedirs=None, want_Jtr=False):\n\n    for which in [trans, pose, v_template, weights, posedirs, betas, shapedirs]:\n        if which is not None:\n            assert ischumpy(which)\n\n    v = v_template\n\n    if shapedirs is not None:\n        if betas is None:\n            betas = chumpy.zeros(shapedirs.shape[-1])\n        v_shaped = v + shapedirs.dot(betas)\n    else:\n        v_shaped = v\n        \n    if posedirs is not None:\n        v_posed = v_shaped + posedirs.dot(posemap(bs_type)(pose))\n    else:\n        v_posed = v_shaped\n        \n    v = v_posed\n        \n    if sp.issparse(J):\n        regressor = J\n        J_tmpx = MatVecMult(regressor, v_shaped[:,0])        \n        J_tmpy = MatVecMult(regressor, v_shaped[:,1])        \n        J_tmpz = MatVecMult(regressor, v_shaped[:,2])        \n        J = chumpy.vstack((J_tmpx, J_tmpy, J_tmpz)).T            \n    else:    \n        assert(ischumpy(J))\n        \n    assert(bs_style=='lbs')\n    result, Jtr = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr=True, xp=chumpy)\n     \n    tr = trans.reshape((1,3))\n    result = result + tr\n    Jtr = Jtr + tr\n\n    result.trans = trans\n    result.f = f\n    result.pose = pose\n    result.v_template = v_template\n    result.J = J\n    result.weights = weights\n    result.kintree_table = kintree_table\n    result.bs_style = bs_style\n    result.bs_type =bs_type\n    if posedirs is not None:\n        result.posedirs = posedirs\n        result.v_posed = v_posed\n    if shapedirs is not None:\n        result.shapedirs = shapedirs\n        result.betas = betas\n        result.v_shaped = v_shaped\n    if want_Jtr:\n        result.J_transformed = Jtr\n    return result\n\ndef verts_core(pose, v, J, weights, kintree_table, bs_style, want_Jtr=False, xp=chumpy):\n    \n    if xp == chumpy:\n        assert(hasattr(pose, 'dterms'))\n        assert(hasattr(v, 'dterms'))\n        assert(hasattr(J, 'dterms'))\n        assert(hasattr(weights, 'dterms'))\n     \n    assert(bs_style=='lbs')\n    result = lbs.verts_core(pose, v, J, weights, kintree_table, want_Jtr, xp)\n\n    return result\n"""
utils/__init__.py,0,b''
utils/audio_handler.py,5,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport re\nimport copy\nimport resampy\nimport numpy as np\nimport tensorflow as tf\nfrom python_speech_features import mfcc\n\n\ndef interpolate_features(features, input_rate, output_rate, output_len=None):\n    num_features = features.shape[1]\n    input_len = features.shape[0]\n    seq_len = input_len / float(input_rate)\n    if output_len is None:\n        output_len = int(seq_len * output_rate)\n    input_timestamps = np.arange(input_len) / float(input_rate)\n    output_timestamps = np.arange(output_len) / float(output_rate)\n    output_features = np.zeros((output_len, num_features))\n    for feat in range(num_features):\n        output_features[:, feat] = np.interp(output_timestamps,\n                                             input_timestamps,\n                                             features[:, feat])\n    return output_features\n\nclass AudioHandler:\n    def __init__(self, config):\n        self.config = config\n        self.audio_feature_type = config[\'audio_feature_type\']\n        self.num_audio_features = config[\'num_audio_features\']\n        self.audio_window_size = config[\'audio_window_size\']\n        self.audio_window_stride = config[\'audio_window_stride\']\n\n    def process(self, audio):\n        if self.audio_feature_type.lower() == ""none"":\n            return None\n        elif self.audio_feature_type.lower() == \'deepspeech\':\n            return self.convert_to_deepspeech(audio)\n        else:\n            raise NotImplementedError(""Audio features not supported"")\n\n    def convert_to_deepspeech(self, audio):\n        def audioToInputVector(audio, fs, numcep, numcontext):\n            # Get mfcc coefficients\n            features = mfcc(audio, samplerate=fs, numcep=numcep)\n\n            # We only keep every second feature (BiRNN stride = 2)\n            features = features[::2]\n\n            # One stride per time step in the input\n            num_strides = len(features)\n\n            # Add empty initial and final contexts\n            empty_context = np.zeros((numcontext, numcep), dtype=features.dtype)\n            features = np.concatenate((empty_context, features, empty_context))\n\n            # Create a view into the array with overlapping strides of size\n            # numcontext (past) + 1 (present) + numcontext (future)\n            window_size = 2 * numcontext + 1\n            train_inputs = np.lib.stride_tricks.as_strided(\n                features,\n                (num_strides, window_size, numcep),\n                (features.strides[0], features.strides[0], features.strides[1]),\n                writeable=False)\n\n            # Flatten the second and third dimensions\n            train_inputs = np.reshape(train_inputs, [num_strides, -1])\n\n            train_inputs = np.copy(train_inputs)\n            train_inputs = (train_inputs - np.mean(train_inputs)) / np.std(train_inputs)\n\n            # Return results\n            return train_inputs\n\n        if type(audio) == dict:\n            pass\n        else:\n            raise ValueError(\'Wrong type for audio\')\n\n        # Load graph and place_holders\n        with tf.gfile.GFile(self.config[\'deepspeech_graph_fname\'], ""rb"") as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n        graph = tf.get_default_graph()\n        tf.import_graph_def(graph_def, name=""deepspeech"")\n        input_tensor = graph.get_tensor_by_name(\'deepspeech/input_node:0\')\n        seq_length = graph.get_tensor_by_name(\'deepspeech/input_lengths:0\')\n        layer_6 = graph.get_tensor_by_name(\'deepspeech/logits:0\')\n\n        n_input = 26\n        n_context = 9\n\n        processed_audio = copy.deepcopy(audio)\n        with tf.Session(graph=graph) as sess:\n            for subj in audio.keys():\n                for seq in audio[subj].keys():\n                    print(\'process audio: %s - %s\' % (subj, seq))\n\n                    audio_sample = audio[subj][seq][\'audio\']\n                    sample_rate = audio[subj][seq][\'sample_rate\']\n                    resampled_audio = resampy.resample(audio_sample.astype(float), sample_rate, 16000)\n                    input_vector = audioToInputVector(resampled_audio.astype(\'int16\'), 16000, n_input, n_context)\n\n                    network_output = sess.run(layer_6, feed_dict={input_tensor: input_vector[np.newaxis, ...],\n                                                                  seq_length: [input_vector.shape[0]]})\n\n                    # Resample network output from 50 fps to 60 fps\n                    audio_len_s = float(audio_sample.shape[0]) / sample_rate\n                    num_frames = int(round(audio_len_s * 60))\n                    network_output = interpolate_features(network_output[:, 0], 50, 60,\n                                                          output_len=num_frames)\n\n                    # Make windows\n                    zero_pad = np.zeros((int(self.audio_window_size / 2), network_output.shape[1]))\n                    network_output = np.concatenate((zero_pad, network_output, zero_pad), axis=0)\n                    windows = []\n                    for window_index in range(0, network_output.shape[0] - self.audio_window_size, self.audio_window_stride):\n                        windows.append(network_output[window_index:window_index + self.audio_window_size])\n\n                    processed_audio[subj][seq][\'audio\'] = np.array(windows)\n        return processed_audio\n\n'"
utils/base_model.py,4,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport os\nimport logging\nimport numpy as np\nimport tensorflow as tf\n\nclass BaseModel(object):\n    def __init__(self, session, batcher, config, scope=\'default\'):\n        if \'num_render_sequences\' in config:\n            self.num_render_sequences = config[\'num_render_sequences\']\n        if ""num_embedding_sequences"" in config:\n            self.num_embedding_sequences = config[\'num_embedding_sequences\']\n        if ""num_embedding_samples"" in config:\n            self.num_embedding_sequences = config[\'num_embedding_samples\']\n        self.session = session\n        self.batcher = batcher\n        self.config = config\n        self.scope = scope\n        self.end_points = {}\n        self.threads = []\n\n        self.save_path = os.path.join(self.config[\'checkpoint_dir\'], \'checkpoints\')\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n    def _build_savers(self, max_to_keep):\n        self.saver_forsave = tf.train.Saver(max_to_keep=max_to_keep)\n        self.saver_forrestore = tf.train.Saver()\n\n    def _save(self, step):\n        save_path = os.path.join(self.config[\'checkpoint_dir\'], \'checkpoints\')\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n        self.saver_forsave.save(self.session, os.path.join(self.save_path, \'gstep_%s.model\' % (step,)))\n\n    def _find_closest_path(self, ckpt, approx_g_step):\n        def find_nearest(array, value):\n            array = np.asarray(array)\n            idx = (np.abs(array - value)).argmin()\n            return idx, array[idx]\n\n        all_paths = ckpt.all_model_checkpoint_paths\n        possible_steps = []\n        for checkpoint in all_paths:\n            g_step = str(os.path.basename(checkpoint).split(\'.\')[0].split(\'/\')[-1])\n            g_step = int(filter(str.isdigit, g_step))\n            possible_steps.append(g_step)\n        idx, chosen_g_step = find_nearest(possible_steps, approx_g_step)\n        logging.warning(\'Loading %d step although %d was asked for\' % (int(chosen_g_step), approx_g_step))\n        return idx\n\n    def load(self, epoch=None):\n        if epoch is None:\n            #Load latest checkpoint file\n            ckpt_name = tf.train.latest_checkpoint(self.save_path)\n            if ckpt_name is not None:\n                self.saver_forrestore.restore(self.session, ckpt_name)\n                logging.warning(""Loading model %s - this will screw up the epoch number during training"" % ckpt_name)\n            else:\n                logging.warning(""Training model from scratch"")\n        else:\n            approx_gstep = int(epoch*self.batcher.get_training_size()/self.config[\'batch_size\'])\n            ckpt = tf.train.get_checkpoint_state(self.save_path)\n            chosen_checkpoint_index = self._find_closest_path(ckpt, approx_gstep)\n            if ckpt and ckpt.model_checkpoint_path:\n                ckpt_name = os.path.basename(ckpt.all_model_checkpoint_paths[chosen_checkpoint_index])\n                self.saver_forrestore.restore(self.session, os.path.join(self.save_path, ckpt_name))\n                logging.warning(""Loading model %s - this will screw up the epoch number during training"" % ckpt_name)\n            else:\n                logging.waring(""Failed loading model"")'"
utils/batcher.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nfrom __future__ import division\n\nimport random\nimport copy\n\nimport numpy as np\n\n\nclass Batcher:\n    def __init__(self, data_handler):\n\n        self.data_handler = data_handler\n\n        data_splits = data_handler.get_data_splits()\n        self.training_indices = copy.deepcopy(data_splits[0])\n        self.val_indices = copy.deepcopy(data_splits[1])\n        self.test_indices = copy.deepcopy(data_splits[2])\n\n        self.current_state = 0\n\n    def get_training_size(self):\n        return len(self.training_indices)\n\n    def get_num_training_subjects(self):\n        return self.data_handler.get_num_training_subjects()\n\n    def convert_training_idx2subj(self, idx):\n        return self.data_handler.convert_training_idx2subj(idx)\n\n    def convert_training_subj2idx(self, subj):\n        return self.data_handler.convert_training_subj2idx(subj)\n\n    def get_training_batch(self, batch_size):\n        """"""\n        Get batch for training\n        :param batch_size:\n        :return:\n        """"""\n        if self.current_state == 0:\n            random.shuffle(self.training_indices)\n\n        if (self.current_state + batch_size) > (len(self.training_indices) + 1):\n            self.current_state = 0\n            return self.get_training_batch(batch_size)\n        else:\n            self.current_state += batch_size\n            batch_indices = self.training_indices[self.current_state:(self.current_state + batch_size)]\n            if len(batch_indices) != batch_size:\n                self.current_state = 0\n                return self.get_training_batch(batch_size)\n            return self.data_handler.slice_data(batch_indices)\n\n    def get_validation_batch(self, batch_size):\n        """"""\n        Validation batch for randomize, quantitative evaluation\n        :param batch_size:\n        :return:\n        """"""\n        if batch_size > len(self.val_indices):\n            return self.data_handler.slice_data(self.val_indices)\n        else:\n            return self.data_handler.slice_data(list(np.random.choice(self.val_indices, size=batch_size)))\n\n    def get_test_batch(self, batch_size):\n        if batch_size > len(self.test_indices):\n            return self.data_handler.slice_data(self.test_indices)\n        else:\n            return self.data_handler.slice_data(list(np.random.choice(self.test_indices, size=batch_size)))\n\n    def get_num_batches(self, batch_size):\n        return int(len(self.training_indices) / batch_size)\n\n    def get_training_sequences_in_order(self, num_of_sequences):\n        return self.data_handler.get_training_sequences(num_of_sequences)\n\n    def get_validation_sequences_in_order(self, num_of_sequences):\n        return self.data_handler.get_validation_sequences(num_of_sequences)\n'"
utils/data_handler.py,0,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport os\nimport pickle\nimport random\nimport numpy as np\nfrom utils.audio_handler import AudioHandler\n\ndef load_from_config(config, key):\n    if key in config:\n        return config[key]\n    else:\n        raise ValueError(\'Key does not exist in config %s\' % key)\n\ndef invert_data2array(data2array):\n    array2data = {}\n    for sub in data2array.keys():\n        for seq in data2array[sub].keys():\n            for frame, array_idx in data2array[sub][seq].items():\n                array2data[array_idx] = (sub, seq, frame)\n    return array2data\n\ndef compute_window_array_idx(data2array, window_size):\n    def window_frame(frame_idx, window_size):\n        l0 = max(frame_idx + 1 - window_size, 0)\n        l1 = frame_idx + 1\n        window_frames = np.zeros(window_size, dtype=int)\n        window_frames[window_size - l1 + l0:] = np.arange(l0, l1)\n        return window_frames\n\n    array2window_ids = {}\n    for sub in data2array.keys():\n        for seq in data2array[sub].keys():\n            for frame, array_idx in data2array[sub][seq].items():\n                window_frames = window_frame(frame, window_size)\n                array2window_ids[array_idx] = [data2array[sub][seq][id] for id in window_frames]\n    return array2window_ids\n\nclass DataHandler:\n    def __init__(self, config):\n        subject_for_training = config[\'subject_for_training\'].split("" "")\n        sequence_for_training = config[\'sequence_for_training\'].split("" "")\n        subject_for_validation = config[\'subject_for_validation\'].split("" "")\n        sequence_for_validation = config[\'sequence_for_validation\'].split("" "")\n        subject_for_testing = config[\'subject_for_testing\'].split("" "")\n        sequence_for_testing = config[\'sequence_for_testing\'].split("" "")\n        self.num_consecutive_frames = config[\'num_consecutive_frames\']\n\n        self.audio_handler = AudioHandler(config)\n        print(""Loading data"")\n        self._load_data(config)\n        print(""Initialize data splits"")\n        self._init_data_splits(subject_for_training, sequence_for_training, subject_for_validation,\n                               sequence_for_validation, subject_for_testing, sequence_for_testing)\n        print(""Initialize training, validation, and test indices"")\n        self._init_indices()\n\n    def get_data_splits(self):\n        return self.training_indices, self.validation_indices, self.testing_indices\n\n    def slice_data(self, indices):\n        return self._slice_data(indices)\n\n    def get_training_sequences(self, num_sequences):\n        return self._get_random_sequences(self.training_subjects, self.training_sequences, num_sequences)\n\n    def get_validation_sequences(self, num_sequences):\n        return self._get_random_sequences(self.validation_subjects, self.validation_sequences, num_sequences)\n\n    def get_testing_sequences(self, num_sequences):\n        return self._get_random_sequences(self.testing_subjects, self.testing_sequences, num_sequences)\n\n    def get_num_training_subjects(self):\n        return len(self.training_subjects)\n\n    def convert_training_idx2subj(self, idx):\n        if idx in self.training_idx2subj:\n            return self.training_idx2subj[idx]\n        else:\n            return -1\n\n    def convert_training_subj2idx(self, subj):\n        if subj in self.training_subj2idx:\n            return self.training_subj2idx[subj]\n        else:\n            return -1\n\n    def _init_indices(self):\n        def get_indices(subjects, sequences):\n            indices = []\n            for subj in subjects:\n                if (subj not in self.raw_audio) or (subj not in self.data2array_verts):\n                    if subj != \'\':\n                        import pdb; pdb.set_trace()\n                        print(\'subject missing %s\' % subj)\n                    continue\n\n                for seq in sequences:\n                    if (seq not in self.raw_audio[subj]) or (seq not in self.data2array_verts[subj]):\n                        print(\'sequence data missing %s - %s\' % (subj, seq))\n                        continue\n\n                    num_data_frames = max(self.data2array_verts[subj][seq].keys())+1\n                    if self.processed_audio is not None:\n                        num_audio_frames = len(self.processed_audio[subj][seq][\'audio\'])\n                    else:\n                        num_audio_frames = num_data_frames\n\n                    try:\n                        for i in range(min(num_data_frames, num_audio_frames)):\n                            indexed_frame = self.data2array_verts[subj][seq][i]\n                            indices.append(indexed_frame)\n                    except KeyError:\n                        print(\'Key error with subject: %s and sequence: %s"" % (subj, seq)\')\n            return indices\n\n        self.training_indices = get_indices(self.training_subjects, self.training_sequences)\n        self.validation_indices = get_indices(self.validation_subjects, self.validation_sequences)\n        self.testing_indices = get_indices(self.testing_subjects, self.testing_sequences)\n\n        self.training_idx2subj = {idx: self.training_subjects[idx] for idx in np.arange(len(self.training_subjects))}\n        self.training_subj2idx = {self.training_idx2subj[idx]: idx for idx in self.training_idx2subj.keys()}\n\n    def _slice_data(self, indices):\n        if self.num_consecutive_frames == 1:\n            return self._slice_data_helper(indices)\n        else:\n            window_indices = []\n            for id in indices:\n                window_indices += self.array2window_ids[id]\n            return self._slice_data_helper(window_indices)\n\n    def _slice_data_helper(self, indices):\n        face_vertices = self.face_vert_mmap[indices]\n        face_templates = []\n        processed_audio = []\n        subject_idx = []\n        for idx in indices:\n            sub, sen, frame = self.array2data_verts[idx]\n            face_templates.append(self.templates_data[sub])\n            if self.processed_audio is not None:\n                processed_audio.append(self.processed_audio[sub][sen][\'audio\'][frame])\n            subject_idx.append(self.convert_training_subj2idx(sub))\n\n        face_templates = np.stack(face_templates)\n        subject_idx = np.hstack(subject_idx)\n        assert face_vertices.shape[0] == face_templates.shape[0]\n\n        if self.processed_audio is not None:\n            processed_audio = np.stack(processed_audio)\n            assert face_vertices.shape[0] == processed_audio.shape[0]\n        return processed_audio, face_vertices, face_templates, subject_idx\n\n    def _load_data(self, config):\n        face_verts_mmaps_path = load_from_config(config, \'verts_mmaps_path\')\n        face_templates_path = load_from_config(config, \'templates_path\')\n        raw_audio_path = load_from_config(config, \'raw_audio_path\')\n        processed_audio_path = load_from_config(config, \'processed_audio_path\')\n        data2array_verts_path = load_from_config(config, \'data2array_verts_path\')\n\n        print(""Loading face vertices"")\n        self.face_vert_mmap = np.load(face_verts_mmaps_path, mmap_mode=\'r\')\n\n        print(""Loading templates"")\n        self.templates_data = pickle.load(open(face_templates_path, \'rb\'), encoding=\'latin1\')\n\n        print(""Loading raw audio"")\n        self.raw_audio = pickle.load(open(raw_audio_path, \'rb\'), encoding=\'latin1\')\n\n        print(""Process audio"")\n        if os.path.exists(processed_audio_path):\n            self.processed_audio = pickle.load(open(processed_audio_path, \'rb\'), encoding=\'latin1\')\n        else:\n            self.processed_audio =  self._process_audio(self.raw_audio)\n            if processed_audio_path != \'\':\n                pickle.dump(self.processed_audio, open(processed_audio_path, \'wb\'))\n\n        print(""Loading index maps"")\n        self.data2array_verts = pickle.load(open(data2array_verts_path, \'rb\'))\n        self.array2data_verts = invert_data2array(self.data2array_verts)\n        self.array2window_ids = compute_window_array_idx(self.data2array_verts, self.num_consecutive_frames)\n\n    def _init_data_splits(self, subject_for_training, sequence_for_training, subject_for_validation,\n                          sequence_for_validation, subject_for_testing, sequence_for_testing):\n        def select_valid_subjects(subjects_list):\n            return [subj for subj in subjects_list]\n\n        def select_valid_sequences(sequences_list):\n            return [seq for seq in sequences_list]\n\n        self.training_subjects = select_valid_subjects(subject_for_training)\n        self.training_sequences = select_valid_sequences(sequence_for_training)\n\n        self.validation_subjects = select_valid_subjects(subject_for_validation)\n        self.validation_sequences = select_valid_sequences(sequence_for_validation)\n\n        self.testing_subjects = select_valid_subjects(subject_for_testing)\n        self.testing_sequences = select_valid_sequences(sequence_for_testing)\n\n        all_instances = []\n        for i in self.training_subjects:\n            for j in self.training_sequences:\n                all_instances.append((i, j))\n        for i in self.validation_subjects:\n            for j in self.validation_sequences:\n                all_instances.append((i, j))\n        for i in self.testing_subjects:\n            for j in self.testing_sequences:\n                all_instances.append((i, j))\n\n        # All instances should contain all unique elements, otherwise the arguments were passed wrongly, so assertion\n        if len(all_instances) != len(set(all_instances)):\n            raise ValueError(\'User-specified data split not disjoint\')\n\n    def _get_random_sequences(self, subjects, sequences, num_sequences):\n        if num_sequences == 0:\n            return\n\n        sub_seq_list = []\n        for subj in subjects:\n            if subj not in self.data2array_verts:\n                continue\n            for seq in sequences:\n                if(seq not in self.raw_audio[subj]) or (seq not in self.data2array_verts[subj]):\n                    continue\n                sub_seq_list.append((subj, seq))\n        st = random.getstate()\n        random.seed(777)\n        random.shuffle(sub_seq_list)\n        random.setstate(st)\n\n        if num_sequences > 0 and num_sequences < len(sub_seq_list):\n            sub_seq_list = sub_seq_list[:num_sequences]\n        return self._get_subject_sequences(sub_seq_list)\n\n    def _get_subject_sequences(self, subject_sequence_list):\n        face_vertices = []\n        face_templates = []\n        subject_idx = []\n\n        raw_audio = []\n        processed_audio = []\n        for subj, seq in subject_sequence_list:\n            frame_array_indices = []\n            try:\n                for frame, array_idx in self.data2array_verts[subj][seq].items():\n                    frame_array_indices.append(array_idx)\n            except KeyError:\n                continue\n            face_vertices.append(self.face_vert_mmap[frame_array_indices])\n            face_templates.append(self.templates_data[subj])\n            subject_idx.append(self.convert_training_subj2idx(subj))\n            raw_audio.append(self.raw_audio[subj][seq])\n            processed_seq_audio = []\n            if self.processed_audio is not None:\n                for frame, array_idx in self.data2array_verts[subj][seq].items():\n                    processed_seq_audio.append(self.processed_audio[subj][seq][\'audio\'][frame])\n            processed_audio.append(processed_seq_audio)\n        return raw_audio, processed_audio, face_vertices, face_templates, subject_idx\n\n    def _process_audio(self, raw_audio):\n        return self.audio_handler.process(raw_audio)'"
utils/expression_layer.py,3,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\nimport numpy as np\nimport tensorflow as tf\nfrom utils.ops import fc_layer\n\nclass ExpressionLayer:\n    def __init__(self, config, scope='ExpressionLayer'):\n        self.expression_basis_fname = config['expression_basis_fname']\n        self.init_expression = config['init_expression']\n\n        self.num_vertices = config['num_vertices']\n        self.expression_dim = config['expression_dim']\n        self.scope = scope\n\n    def __call__(self, parameters, if_reuse=False):\n        with tf.variable_scope(self.scope, reuse=if_reuse):\n\n            init_exp_basis = np.zeros((3*self.num_vertices, self.expression_dim))\n\n            if self.init_expression:\n                init_exp_basis[:, :min(self.expression_dim, 100)] = np.load(self.expression_basis_fname)[:, :min(self.expression_dim, 100)]\n\n            with tf.name_scope('expression_offset'):\n                exp_offset = fc_layer(parameters,\n                                    num_units_in=self.expression_dim,\n                                    num_units_out=3*self.num_vertices,\n                                    init_weights=init_exp_basis.T,\n                                    scope='expression_offset')\n            return tf.reshape(exp_offset, [-1, self.num_vertices, 3, 1])"""
utils/inference.py,7,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\n\nimport os\nimport cv2\nimport scipy\nimport tempfile\nimport numpy as np\nimport tensorflow as tf\nfrom subprocess import call\nfrom scipy.io import wavfile\n\nfrom psbody.mesh import Mesh\nfrom utils.audio_handler import  AudioHandler\nfrom utils.rendering import render_mesh_helper\n\ndef process_audio(ds_path, audio, sample_rate):\n    config = {}\n    config['deepspeech_graph_fname'] = ds_path\n    config['audio_feature_type'] = 'deepspeech'\n    config['num_audio_features'] = 29\n\n    config['audio_window_size'] = 16\n    config['audio_window_stride'] = 1\n\n    tmp_audio = {'subj': {'seq': {'audio': audio, 'sample_rate': sample_rate}}}\n    audio_handler = AudioHandler(config)\n    return audio_handler.process(tmp_audio)['subj']['seq']['audio']\n\n\ndef output_sequence_meshes(sequence_vertices, template, out_path):\n    mesh_out_path = os.path.join(out_path, 'meshes')\n    if not os.path.exists(mesh_out_path):\n        os.makedirs(mesh_out_path)\n\n    num_frames = sequence_vertices.shape[0]\n    for i_frame in range(num_frames):\n        out_fname = os.path.join(mesh_out_path, '%05d.obj' % i_frame)\n        Mesh(sequence_vertices[i_frame], template.f).write_obj(out_fname)\n\n\ndef render_sequence_meshes(audio_fname, sequence_vertices, template, out_path):\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n\n    tmp_video_file = tempfile.NamedTemporaryFile('w', suffix='.mp4', dir=out_path)\n    if int(cv2.__version__[0]) < 3:\n        writer = cv2.VideoWriter(tmp_video_file.name, cv2.cv.CV_FOURCC(*'mp4v'), 60, (800, 800), True)\n    else:\n        writer = cv2.VideoWriter(tmp_video_file.name, cv2.VideoWriter_fourcc(*'mp4v'), 60, (800, 800), True)\n\n    num_frames = sequence_vertices.shape[0]\n    center = np.mean(sequence_vertices[0], axis=0)\n    for i_frame in range(num_frames):\n        img = render_mesh_helper(Mesh(sequence_vertices[i_frame], template.f), center)\n        writer.write(img)\n    writer.release()\n\n    video_fname = os.path.join(out_path, 'video.mp4')\n    cmd = ('ffmpeg' + ' -i {0} -i {1} -vcodec h264 -ac 2 -channel_layout stereo -pix_fmt yuv420p {2}'.format(\n        audio_fname, tmp_video_file.name, video_fname)).split()\n    call(cmd)\n\n\ndef inference(tf_model_fname, ds_fname, audio_fname, template_fname, condition_idx, out_path, render_sequence=True):\n    template = Mesh(filename=template_fname)\n\n    sample_rate, audio = wavfile.read(audio_fname)\n    if audio.ndim != 1:\n        print('Audio has multiple channels, only first channel is considered')\n        audio = audio[:,0]\n\n    processed_audio = process_audio(ds_fname, audio, sample_rate)\n\n    # Load previously saved meta graph in the default graph\n    saver = tf.train.import_meta_graph(tf_model_fname + '.meta')\n    graph = tf.get_default_graph()\n\n    speech_features = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/speech_features:0')\n    condition_subject_id = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/condition_subject_id:0')\n    is_training = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/is_training:0')\n    input_template = graph.get_tensor_by_name(u'VOCA/Inputs_decoder/template_placeholder:0')\n    output_decoder = graph.get_tensor_by_name(u'VOCA/output_decoder:0')\n\n    num_frames = processed_audio.shape[0]\n    feed_dict = {speech_features: np.expand_dims(np.stack(processed_audio), -1),\n                 condition_subject_id: np.repeat(condition_idx-1, num_frames),\n                 is_training: False,\n                 input_template: np.repeat(template.v[np.newaxis, :, :, np.newaxis], num_frames, axis=0)}\n\n    with tf.Session() as session:\n        # Restore trained model\n        saver.restore(session, tf_model_fname)\n        predicted_vertices = np.squeeze(session.run(output_decoder, feed_dict))\n        output_sequence_meshes(predicted_vertices, template, out_path)\n        if(render_sequence):\n            render_sequence_meshes(audio_fname, predicted_vertices, template, out_path)\n    tf.reset_default_graph()\n\n\ndef inference_interpolate_styles(tf_model_fname, ds_fname, audio_fname, template_fname, condition_weights, out_path):\n    template = Mesh(filename=template_fname)\n\n    sample_rate, audio = wavfile.read(audio_fname)\n    if audio.ndim != 1:\n        print('Audio has multiple channels, only first channel is considered')\n        audio = audio[:, 0]\n\n    processed_audio = process_audio(ds_fname, audio, sample_rate)\n\n    # Load previously saved meta graph in the default graph\n    saver = tf.train.import_meta_graph(tf_model_fname + '.meta')\n    graph = tf.get_default_graph()\n\n    speech_features = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/speech_features:0')\n    condition_subject_id = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/condition_subject_id:0')\n    is_training = graph.get_tensor_by_name(u'VOCA/Inputs_encoder/is_training:0')\n    input_template = graph.get_tensor_by_name(u'VOCA/Inputs_decoder/template_placeholder:0')\n    output_decoder = graph.get_tensor_by_name(u'VOCA/output_decoder:0')\n\n    non_zeros = np.where(condition_weights > 0.0)[0]\n    condition_weights[non_zeros] /= sum(condition_weights[non_zeros])\n\n    num_frames = processed_audio.shape[0]\n    output_vertices = np.zeros((num_frames, template.v.shape[0], template.v.shape[1]))\n\n    with tf.Session() as session:\n        # Restore trained model\n        saver.restore(session, tf_model_fname)\n\n        for condition_id in non_zeros:\n            feed_dict = {speech_features: np.expand_dims(np.stack(processed_audio), -1),\n                         condition_subject_id: np.repeat(condition_id, num_frames),\n                         is_training: False,\n                         input_template: np.repeat(template.v[np.newaxis, :, :, np.newaxis], num_frames, axis=0)}\n            predicted_vertices = np.squeeze(session.run(output_decoder, feed_dict))\n            output_vertices += condition_weights[condition_id] * predicted_vertices\n\n        output_sequence_meshes(output_vertices, template, out_path)"""
utils/losses.py,13,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport tensorflow as tf\n\ndef reconstruction_loss(predicted, real, want_absolute_loss=True, want_in_mm=False, weights=None):\n    if weights is not None:\n        assert predicted.shape[1] == real.shape[1] == weights.shape[0]\n        tf_weights = tf.constant(weights, dtype=tf.float32)\n        predicted = tf.einsum('abcd,bd->abcd', predicted, tf_weights)\n        real = tf.einsum('abcd,bd->abcd', real, tf_weights)\n\n    if want_in_mm:\n        predicted, real = predicted * 1000, real * 1000\n    if want_absolute_loss:\n        return tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(predicted, real)), axis=2))\n    else:\n        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(predicted, real), axis=2))\n\ndef wing_reconstruction_loss(predicted, real):\n    pass\n\ndef edge_reconstruction_loss(predicted, real, num_vertices, mesh_f, want_absolute_loss=True):\n    from tfbody.mesh.utils import get_vertices_per_edge\n    vpe = get_vertices_per_edge(num_vertices, mesh_f)\n    edges_for = lambda x: tf.gather(x, vpe[:, 0], axis=1) - tf.gather(x, vpe[:, 1], axis=1)\n    if want_absolute_loss:\n        return tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(edges_for(predicted), edges_for(real))), axis=2))\n    else:\n        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(edges_for(predicted), edges_for(real)), axis=2))\n\ndef orthogonality_loss(W, want_absolute_loss=True, no_normalize=False):\n    '''Computes the loss to a column-wise orthogonality. If no_normalize is true, the loss reflects\n    the distance to orthogonalize the matrix columns <a,b> = 0 but the columnts are not orthonormal,\n    i.e. <a,a> != 1'''\n    wtw = tf.matmul(tf.transpose(W), W)\n\n    if no_normalize:\n        diag = tf.diag(tf.diag_part(wtw))\n    else:\n        num_rows = wtw.get_shape().as_list()[0]\n        diag = tf.eye(num_rows)\n\n    if want_absolute_loss:\n        return tf.reduce_sum(tf.abs(tf.subtract(wtw, diag)))\n    else:\n        return tf.reduce_sum(tf.squared_difference((wtw, diag)))"""
utils/ops.py,28,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nimport tensorflow as tf\n\ndef fc_layer(inputs,\n             num_units_in,\n             num_units_out,\n             init_weights=None,\n             weightini=0.1,\n             bias=0.,\n             trainable=True,\n             scope=None,\n             regularisation_constant=0.0):\n    with tf.variable_scope(scope):\n        weights_shape = [num_units_in, num_units_out]\n\n        if init_weights is not None:\n            weights_initializer = tf.constant(init_weights, dtype=tf.float32)\n            weights_shape = None\n        elif weightini == 0.:\n            weights_initializer = tf.constant_initializer(weightini)\n        else:\n            weights_initializer = tf.truncated_normal_initializer(stddev=weightini)\n\n        weights = tf.get_variable(\'weights\',\n                                  shape=weights_shape,\n                                  initializer=weights_initializer,\n                                  trainable=trainable,\n                                  regularizer=tf.contrib.layers.l2_regularizer(scale=regularisation_constant))\n\n        bias_shape = [num_units_out, ]\n        bias_initializer = tf.constant_initializer(bias)\n        biases = tf.get_variable(\'biases\',\n                                 shape=bias_shape,\n                                 initializer=bias_initializer,\n                                 trainable=trainable)\n        outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    return outputs\n\ndef custom_fc_layer(inputs,\n             num_units_in,\n             num_units_out,\n             init_weights=None,\n             weightini=0.1,\n             bias=0.,\n             trainable_weights=True,\n             trainable_bias=True,\n             regularisation_constant=0.0,\n             output_weights=False,\n              scope=None):\n    with tf.variable_scope(scope):\n        weights_shape = [num_units_in, num_units_out]\n\n        if init_weights is not None:\n            weights_initializer = tf.constant(init_weights, dtype=tf.float32)\n            weights_shape = None\n        elif weightini == 0.:\n            weights_initializer = tf.constant_initializer(weightini)\n        else:\n            weights_initializer = tf.truncated_normal_initializer(stddev=weightini)\n\n        weights = tf.get_variable(\'weights\',\n                                  shape=weights_shape,\n                                  initializer=weights_initializer,\n                                  trainable=trainable_weights,\n                                  regularizer=tf.contrib.layers.l2_regularizer(scale=regularisation_constant))\n\n        bias_shape = [num_units_out, ]\n        bias_initializer = tf.constant_initializer(bias)\n        biases = tf.get_variable(\'biases\',\n                                 shape=bias_shape,\n                                 initializer=bias_initializer,\n                                 trainable=trainable_bias)\n        outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    if not output_weights:\n        return outputs\n    else:\n        return outputs, weights\n\n\ndef conv2d(inputs, n_filters,\n           k_h=5, k_w=5,\n           stride_h=2, stride_w=2,\n           stddev=0.02,\n           activation=None,\n           bias=True,\n           padding=\'SAME\',\n           scope=None,\n           regularisation_constant=0.0):\n    """"""2D Convolution with options for kernel size, stride, and init deviation.\n    Parameters\n    ----------\n    inputs : Tensor\n        Input tensor to convolve.\n    n_filters : int\n        Number of filters to apply.\n    k_h : int, optional\n        Kernel height.\n    k_w : int, optional\n        Kernel width.\n    stride_h : int, optional\n        Stride in rows.\n    stride_w : int, optional\n        Stride in cols.\n    stddev : float, optional\n        Initialization\'s standard deviation.\n    activation : arguments, optional\n        Function which applies a nonlinearity\n    padding : str, optional\n        \'SAME\' or \'VALID\'\n    scope : str, optional\n        Variable scope to use.\n    Returns\n    -------\n    x : Tensor\n        Convolved input.\n    """"""\n    with tf.variable_scope(scope):\n        w = tf.get_variable(\'weights\',\n                            [k_h, k_w, inputs.get_shape()[-1], n_filters],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev),\n                            regularizer=tf.contrib.layers.l2_regularizer(scale=regularisation_constant))\n        conv = tf.nn.conv2d(inputs,\n                            w,\n                            strides=[1, stride_h, stride_w, 1],\n                            padding=padding)\n        if bias:\n            b = tf.get_variable(\'biases\',\n                                [n_filters],\n                                initializer=tf.truncated_normal_initializer(stddev=stddev))\n            conv = tf.nn.bias_add(conv, b)\n        if activation:\n            conv = activation(conv)\n        return conv\n\n\nclass BatchNorm(object):\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=""batch_norm""):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n            self.name = name\n\n    def __call__(self, x, reuse=False, is_training=True):\n        return tf.contrib.layers.batch_norm(x,\n                                            decay=self.momentum,\n                                            updates_collections=None,\n                                            epsilon=self.epsilon,\n                                            center=True,\n                                            scale=True,\n                                            is_training=is_training,\n                                            reuse=reuse,\n                                            scope=self.name)'"
utils/rendering.py,0,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\nfrom __future__ import division\n\nimport cv2\nimport pyrender\nimport trimesh\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nfrom psbody.mesh import Mesh\n\ndef get_unit_factor(unit):\n    if unit == 'mm':\n        return 1000.0\n    elif unit == 'cm':\n        return 100.0\n    elif unit == 'm':\n        return 1.0\n    else:\n        raise ValueError('Unit not supported')\n\ndef render_mesh_helper(mesh, t_center, rot=np.zeros(3), v_colors=None, errors=None, error_unit='m', min_dist_in_mm=0.0, max_dist_in_mm=3.0, z_offset=0):\n    camera_params = {'c': np.array([400, 400]),\n                     'k': np.array([-0.19816071, 0.92822711, 0, 0, 0]),\n                     'f': np.array([4754.97941935 / 2, 4754.97941935 / 2])}\n\n    frustum = {'near': 0.01, 'far': 3.0, 'height': 800, 'width': 800}\n\n    mesh_copy = Mesh(mesh.v, mesh.f)\n    mesh_copy.v[:] = cv2.Rodrigues(rot)[0].dot((mesh_copy.v-t_center).T).T+t_center\n\n    if errors is not None:\n        intensity = 0.5\n        unit_factor = get_unit_factor('mm')/get_unit_factor(error_unit)\n        errors = unit_factor*errors\n\n        norm = mpl.colors.Normalize(vmin=min_dist_in_mm, vmax=max_dist_in_mm)\n        cmap = cm.get_cmap(name='jet')\n        colormapper = cm.ScalarMappable(norm=norm, cmap=cmap)\n        rgba_per_v = colormapper.to_rgba(errors)\n        rgb_per_v = rgba_per_v[:, 0:3]\n    elif v_colors is not None:\n        intensity = 0.5\n        rgb_per_v = v_colors\n    else:\n        intensity = 1.5\n        rgb_per_v = None\n\n    tri_mesh = trimesh.Trimesh(vertices=mesh_copy.v, faces=mesh_copy.f, vertex_colors=rgb_per_v)\n    render_mesh = pyrender.Mesh.from_trimesh(tri_mesh, smooth=True)\n\n    scene = pyrender.Scene(ambient_light=[.2, .2, .2], bg_color=[255, 255, 255])\n    camera = pyrender.IntrinsicsCamera(fx=camera_params['f'][0],\n                                      fy=camera_params['f'][1],\n                                      cx=camera_params['c'][0],\n                                      cy=camera_params['c'][1],\n                                      znear=frustum['near'],\n                                      zfar=frustum['far'])\n\n    scene.add(render_mesh, pose=np.eye(4))\n\n    camera_pose = np.eye(4)\n    camera_pose[:3,3] = np.array([0, 0, 1.0-z_offset])\n    scene.add(camera, pose=[[1, 0, 0, 0],\n                            [0, 1, 0, 0],\n                            [0, 0, 1, 1],\n                            [0, 0, 0, 1]])\n\n    angle = np.pi / 6.0\n    pos = camera_pose[:3,3]\n    light_color = np.array([1., 1., 1.])\n    light = pyrender.PointLight(color=light_color, intensity=intensity)\n\n    light_pose = np.eye(4)\n    light_pose[:3,3] = pos\n    scene.add(light, pose=light_pose.copy())\n\n    light_pose[:3,3] = cv2.Rodrigues(np.array([angle, 0, 0]))[0].dot(pos)\n    scene.add(light, pose=light_pose.copy())\n\n    light_pose[:3,3] =  cv2.Rodrigues(np.array([-angle, 0, 0]))[0].dot(pos)\n    scene.add(light, pose=light_pose.copy())\n\n    light_pose[:3,3] = cv2.Rodrigues(np.array([0, -angle, 0]))[0].dot(pos)\n    scene.add(light, pose=light_pose.copy())\n\n    light_pose[:3,3] = cv2.Rodrigues(np.array([0, angle, 0]))[0].dot(pos)\n    scene.add(light, pose=light_pose.copy())\n\n    flags = pyrender.RenderFlags.SKIP_CULL_FACES\n    try:\n        r = pyrender.OffscreenRenderer(viewport_width=frustum['width'], viewport_height=frustum['height'])\n        color, _ = r.render(scene, flags=flags)\n    except:\n        print('pyrender: Failed rendering frame')\n        color = np.zeros((frustum['height'], frustum['width'], 3), dtype='uint8')\n\n    return color[..., ::-1]"""
utils/speech_encoder.py,25,"b""'''\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n'''\n\nimport tensorflow as tf\nfrom utils.ops import fc_layer, conv2d, BatchNorm\n\nclass SpeechEncoder:\n    def __init__(self, config, scope='SpeechEncoder'):\n        self.scope = scope\n        self._speech_encoding_dim = config['expression_dim']\n        self._condition_speech_features = config['condition_speech_features']\n        self._speech_encoder_size_factor = config['speech_encoder_size_factor']\n\n    def __call__(self, speech_features, condition, is_training, reuse=False):\n        with tf.variable_scope(self.scope, reuse=reuse):\n            if reuse == True:\n                tf.get_variable_scope().reuse_variables()\n\n            batch_norm = BatchNorm(epsilon=1e-5, momentum=0.9)\n            speech_features = batch_norm(speech_features, reuse=reuse, is_training=is_training)\n\n            speech_feature_shape = speech_features.get_shape().as_list()\n            speech_features_reshaped = tf.reshape(tensor=speech_features, shape=[-1, speech_feature_shape[1], 1, speech_feature_shape[2]])\n\n            condition_shape = condition.get_shape().as_list()\n            condition_reshaped = tf.reshape(tensor=condition, shape=[-1, condition_shape[1]])\n\n            if self._condition_speech_features:\n                #Condition input speech feature windows\n                speech_feature_condition = tf.transpose(tf.reshape(tensor=condition_reshaped, shape=[-1, condition_shape[1], 1, 1]), perm=[0,2,3,1])\n                speech_feature_condition = tf.tile(speech_feature_condition, [1, speech_feature_shape[1], 1, 1])\n                speech_features_reshaped = tf.concat((speech_features_reshaped, speech_feature_condition), axis=-1, name='conditioning_speech_features')\n\n            factor = self._speech_encoder_size_factor\n\n            with tf.name_scope('conv1_time'):\n                conv1_time = tf.nn.relu(conv2d(inputs=speech_features_reshaped,\n                                                n_filters=int(32*factor),\n                                                k_h=3, k_w=1,\n                                                stride_h=2, stride_w=1,\n                                                activation=tf.identity,\n                                                scope='conv1'))\n            with tf.name_scope('conv2_time'):\n                conv2_time = tf.nn.relu(conv2d(inputs=conv1_time,\n                                                n_filters=int(32*factor),\n                                                k_h=3, k_w=1,\n                                                stride_h=2, stride_w=1,\n                                                activation=tf.identity,\n                                                scope='conv2'))\n            with tf.name_scope('conv3_time'):\n                conv3_time = tf.nn.relu(conv2d(inputs=conv2_time,\n                                                n_filters=int(64*factor),\n                                                k_h=3, k_w=1,\n                                                stride_h=2, stride_w=1,\n                                                activation=tf.identity,\n                                                scope='conv3'))\n            with tf.name_scope('conv4_time'):\n                conv4_time = tf.nn.relu(conv2d(inputs=conv3_time,\n                                                n_filters=int(64*factor),\n                                                k_h=3, k_w=1,\n                                                stride_h=2, stride_w=1,\n                                                activation=tf.identity,\n                                                scope='conv4'))\n\n            previous_shape = conv4_time.get_shape().as_list()\n            time_conv_flattened = tf.reshape(conv4_time, [-1, previous_shape[1] * previous_shape[2] * previous_shape[3]])\n\n            #Condition audio encoding on speaker style\n            with tf.name_scope('concat_audio_embedding'):\n                concatenated = tf.concat((time_conv_flattened, condition_reshaped), axis=1, name='conditioning_audio_embedding')\n            # concatenated = time_conv_flattened\n\n            units_in = concatenated.get_shape().as_list()[1]\n\n            with tf.name_scope('fc1'):\n                fc1 = tf.nn.tanh(fc_layer(concatenated, num_units_in=units_in, num_units_out=128, scope='fc1'))\n            with tf.name_scope('fc2'):\n                fc2 = fc_layer(fc1, num_units_in=128, num_units_out=self._speech_encoding_dim, scope='fc2')\n            return fc2\n"""
utils/voca_model.py,52,"b'\'\'\'\nMax-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights on this\ncomputer program.\n\nYou can only use this computer program if you have closed a license agreement with MPG or you get the right to use\nthe computer program from someone who is authorized to grant you that right.\n\nAny use of the computer program without a valid license is prohibited and liable to prosecution.\n\nCopyright 2019 Max-Planck-Gesellschaft zur Foerderung der Wissenschaften e.V. (MPG). acting on behalf of its\nMax Planck Institute for Intelligent Systems and the Max Planck Institute for Biological Cybernetics.\nAll rights reserved.\n\nMore information about VOCA is available at http://voca.is.tue.mpg.de.\nFor comments or questions, please email us at voca@tue.mpg.de\n\'\'\'\n\nfrom utils.base_model import BaseModel\nimport cv2\nimport os\nimport sys\nimport logging\nimport tempfile\nimport threading\nfrom subprocess import call\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.io import wavfile\nfrom sklearn.manifold import TSNE\n\nfrom psbody.mesh import Mesh\nfrom utils.rendering import render_mesh_helper\nfrom utils.losses import *\nfrom utils.speech_encoder import SpeechEncoder\nfrom utils.expression_layer import ExpressionLayer\n\nlogging.basicConfig(stream=sys.stdout, format=\'%(asctime)s %(message)s\', datefmt=\'%Y-%m-%d %H:%M:%S\')\n\nclass VOCAModel(BaseModel):\n    def __init__(self, session, batcher, config, scope=\'VOCA\'):\n        BaseModel.__init__(self, session=session, batcher=batcher, config=config, scope=scope)\n        self.template_mesh = Mesh(filename=config[\'template_fname\'])\n\n        self.init_paceholders = getattr(self, \'_init_placeholders\')\n        self.build_encoder = getattr(self, \'_build_encoder\')\n        self.build_decoder = getattr(self, \'_build_decoder\')\n\n    def build_graph(self):\n        with tf.variable_scope(self.scope):\n            self.init_paceholders()\n            self.build_encoder()\n            self.build_decoder()\n            self._build_losses()\n            self._init_training()\n            self._build_savers(max_to_keep=10)\n\n    def _init_placeholders(self):\n        with tf.name_scope(\'Inputs_encoder\'):\n            self.speech_features = tf.placeholder(tf.float32, [None, self.config[\'audio_window_size\'], self.config[\'num_audio_features\'], 1], name=\'speech_features\')\n            self.condition_subject_id = tf.placeholder(tf.int32, [None], name=\'condition_subject_id\')\n            self.is_training = tf.placeholder(tf.bool, name=\'is_training\')\n        with tf.name_scope(\'Target\'):\n            self.target_vertices = tf.placeholder(tf.float32, [None, self.config[\'num_vertices\'], 3, 1], name=\'target_vertices\')\n        with tf.name_scope(\'Inputs_decoder\'):\n            self.input_template = tf.placeholder(tf.float32, [None, self.config[\'num_vertices\'], 3, 1], name=\'template_placeholder\')\n\n    def _build_encoder(self):\n        self.output_encoder = self._build_audio_encoder()\n\n    def _build_audio_encoder(self):\n        audio_encoder = SpeechEncoder(self.config)\n        condition = tf.one_hot(indices=self.condition_subject_id, depth=self.batcher.get_num_training_subjects())\n        return audio_encoder(self.speech_features, condition, self.is_training)\n        # return audio_encoder(self.speech_features)\n\n    def _build_decoder(self):\n        expression_decoder = ExpressionLayer(self.config)\n        self.expression_offset = expression_decoder(self.output_encoder)\n        self.output_decoder = tf.add(self.expression_offset, self.input_template, name=\'output_decoder\')\n\n    def _build_losses(self):\n        self.rec_loss = self._reconstruction_loss()\n        self.velocity_loss = self._velocity_loss()\n        self.acceleration_loss = self._acceleration_loss()\n        self.verts_reg_loss = self._verts_regularizer_loss()\n        self.loss = self.rec_loss + self.velocity_loss + self.acceleration_loss + self.verts_reg_loss\n\n        tf.summary.scalar(\'loss_training\', self.loss, collections=[\'train\'])\n        tf.summary.scalar(\'loss_validation\', self.loss, collections=[\'validation\'])\n        self.t_vars = tf.trainable_variables()\n\n    def _reconstruction_loss(self):\n        with tf.name_scope(\'Reconstruction_loss\'):\n            rec_loss = reconstruction_loss(predicted=self.output_decoder, real=self.target_vertices,\n                                                           want_absolute_loss=self.config[\n                                                               \'absolute_reconstruction_loss\'])\n        tf.summary.scalar(\'reconstruction_loss_training\', rec_loss, collections=[\'train\'])\n        tf.summary.scalar(\'reconstruction_loss_validation\', rec_loss, collections=[\'validation\'])\n        return rec_loss\n\n    def _velocity_loss(self):\n        if self.config[\'velocity_weight\'] > 0.0:\n            assert(self.config[\'num_consecutive_frames\'] >= 2)\n            verts_predicted = tf.reshape(self.output_decoder, [-1, self.config[\'num_consecutive_frames\'], self.config[\'num_vertices\'], 3])\n            x1_pred = tf.reshape(verts_predicted[:, -1, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x2_pred = tf.reshape(verts_predicted[:, -2, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            velocity_pred = x1_pred-x2_pred\n\n            verts_target = tf.reshape(self.target_vertices, [-1, self.config[\'num_consecutive_frames\'], self.config[\'num_vertices\'], 3])\n            x1_target = tf.reshape(verts_target[:, -1, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x2_target = tf.reshape(verts_target[:, -2, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            velocity_target = x1_target-x2_target\n\n            with tf.name_scope(\'Velocity_loss\'):\n                velocity_loss = self.config[\'velocity_weight\']*reconstruction_loss(predicted=velocity_pred, real=velocity_target,\n                                                        want_absolute_loss=self.config[\'absolute_reconstruction_loss\'])\n            tf.summary.scalar(\'velocity_loss_training\', velocity_loss, collections=[\'train\'])\n            tf.summary.scalar(\'velocity_loss_validation\', velocity_loss, collections=[\'validation\'])\n            return velocity_loss\n        else:\n            return 0.0\n\n    def _acceleration_loss(self):\n        if self.config[\'acceleration_weight\'] > 0.0:\n            assert(self.config[\'num_consecutive_frames\'] >= 3)\n            verts_predicted = tf.reshape(self.output_decoder, [-1, self.config[\'num_consecutive_frames\'], self.config[\'num_vertices\'], 3])\n            x1_pred = tf.reshape(verts_predicted[:, -1, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x2_pred = tf.reshape(verts_predicted[:, -2, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x3_pred = tf.reshape(verts_predicted[:, -3, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            acc_pred = x1_pred-2*x2_pred+x3_pred\n\n            verts_target = tf.reshape(self.target_vertices, [-1, self.config[\'num_consecutive_frames\'], self.config[\'num_vertices\'], 3])\n            x1_target = tf.reshape(verts_target[:, -1, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x2_target = tf.reshape(verts_target[:, -2, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            x3_target = tf.reshape(verts_target[:, -3, :], [-1, self.config[\'num_vertices\'], 3, 1])\n            acc_target = x1_target-2*x2_target+x3_target\n\n            with tf.name_scope(\'Acceleration_loss\'):\n                acceleration_loss = self.config[\'acceleration_weight\']*reconstruction_loss(predicted=acc_pred, real=acc_target,\n                                                        want_absolute_loss=self.config[\'absolute_reconstruction_loss\'])\n            tf.summary.scalar(\'acceleration_loss_training\', acceleration_loss, collections=[\'train\'])\n            tf.summary.scalar(\'acceleration_loss_validation\', acceleration_loss, collections=[\'validation\'])\n            return acceleration_loss\n        else:\n            return 0.0\n\n    def _verts_regularizer_loss(self):\n        if self.config[\'verts_regularizer_weight\'] > 0.0:\n            with tf.name_scope(\'Verts_regularizer_loss\'):\n                verts_regularizer_loss = self.config[\'verts_regularizer_weight\']*tf.reduce_mean(tf.reduce_sum(tf.abs(self.expression_offset), axis=2))\n            tf.summary.scalar(\'verts_regularizer_losss_training\', verts_regularizer_loss, collections=[\'train\'])\n            tf.summary.scalar(\'verts_regularizer_loss_validation\', verts_regularizer_loss, collections=[\'validation\'])\n            return verts_regularizer_loss\n        else:\n            return 0.0\n\n    def _init_training(self):\n        self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n        decay_steps = self.batcher.get_training_size()//self.config[\'batch_size\']\n        decay_rate = self.config[\'decay_rate\']\n        if decay_rate < 1:\n            self.global_learning_rate = tf.train.exponential_decay(self.config[\'learning_rate\'], self.global_step,\n                                                                   decay_steps, decay_rate, staircase=True)\n        else:\n            self.global_learning_rate = tf.constant(self.config[\'learning_rate\'], dtype=tf.float32)\n        tf.summary.scalar(\'learning_rate_training\', self.global_learning_rate, collections=[\'train\'])\n        tf.summary.scalar(\'learning_rate_validation\', self.global_learning_rate, collections=[\'validation\'])\n\n        self.optim = tf.train.AdamOptimizer(self.global_learning_rate, self.config[\'adam_beta1_value\']).\\\n            minimize(self.loss, var_list=self.t_vars, global_step=self.global_step)\n\n        self._init_summaries()\n        tf.global_variables_initializer().run()\n\n    def _init_summaries(self):\n        self.train_summary = tf.summary.merge_all(\'train\')\n        self.validation_summary = tf.summary.merge_all(\'validation\')\n        self.train_writer = tf.summary.FileWriter(os.path.join(self.config[\'checkpoint_dir\'], \'summaries\', \'train\'))\n        self.validation_writer = tf.summary.FileWriter(os.path.join(self.config[\'checkpoint_dir\'], \'summaries\', \'validation\'))\n\n    def train(self):\n        num_train_batches = self.batcher.get_num_batches(self.config[\'batch_size\'])\n        for epoch in range(1, self.config[\'epoch_num\']+1):\n            for iter in range(num_train_batches):\n                loss, g_step, summary, g_lr = self._training_step()\n                if iter % 50 == 0:\n                    logging.warning(""Epoch: %d | Iter: %d | Global Step: %d | Loss: %.6f | Learning Rate: %.6f"" % (epoch, iter, g_step, loss, g_lr))\n                    self.train_writer.add_summary(summary, g_step)\n                if iter % 100 == 0:\n                    val_loss, val_summary = self._validation_step()\n                    logging.warning(""Validation loss: %.6f"" % val_loss)\n                    self.validation_writer.add_summary(val_summary, g_step)\n\n            if epoch % 10 == 0:\n                self._save(g_step)\n\n            if epoch % 25 == 0:\n                self._render_sequences(out_folder=os.path.join(self.config[\'checkpoint_dir\'], \'videos\', \'training_epoch_%d_iter_%d\' % (epoch, iter))\n                                       , data_specifier=\'training\')\n                self._render_sequences(out_folder=os.path.join(self.config[\'checkpoint_dir\'], \'videos\', \'validation_epoch_%d_iter_%d\' % (epoch, iter))\n                                       , data_specifier=\'validation\')\n\n    def _training_step(self):\n        processed_audio, vertices, templates, subject_idx = self.batcher.get_training_batch(self.config[\'batch_size\'])\n\n        feed_dict = {self.speech_features: np.expand_dims(processed_audio, -1),\n                     self.condition_subject_id: np.array(subject_idx),\n                     self.is_training: True,\n                     self.input_template: np.expand_dims(templates, -1),\n                     self.target_vertices: np.expand_dims(vertices, -1)}\n\n        loss, g_step, summary, g_lr, _ = self.session.run([self.loss, self.global_step, self.train_summary, self.global_learning_rate, self.optim], feed_dict)\n        return loss, g_step, summary, g_lr\n\n    def _validation_step(self):\n        processed_audio, vertices, templates, _ = self.batcher.get_validation_batch(self.config[\'batch_size\'])\n\n        #Compute validation error conditioned on all training subjects and return mean over all\n        num_training_subjects = self.batcher.get_num_training_subjects()\n        conditions = np.reshape(np.repeat(np.arange(num_training_subjects)[:,np.newaxis],\n                                          repeats=self.config[\'num_consecutive_frames\']*self.config[\'batch_size\'], axis=-1), [-1,])\n\n        feed_dict = {self.speech_features: np.expand_dims(np.tile(processed_audio, (num_training_subjects, 1, 1)), -1),\n                    self.condition_subject_id: conditions,\n                    self.is_training: False,\n                    self.input_template: np.expand_dims(np.tile(templates, (num_training_subjects, 1, 1)), -1),\n                    self.target_vertices: np.expand_dims(np.tile(vertices, (num_training_subjects, 1, 1)), -1)}\n        loss, summary = self.session.run([self.loss, self.validation_summary], feed_dict)\n        return loss, summary\n\n    def _render_sequences(self, out_folder, run_in_parallel=True, data_specifier=\'validation\'):\n        print(\'Render %s sequences\' % data_specifier)\n        if run_in_parallel:\n            # self.threads.append(threading.Thread(target=self._render_helper, args=(out_folder, data_specifier)))\n            # self.threads[-1].start()\n            thread = threading.Thread(target=self._render_helper, args=(out_folder, data_specifier))\n            thread.start()\n            thread.join()\n        else:\n            self._render_helper(out_folder, data_specifier)\n\n    def _render_helper(self, out_folder, data_specifier):\n        if not os.path.exists(out_folder):\n            os.makedirs(out_folder)\n\n        if data_specifier == \'training\':\n            raw_audio, processed_audio, vertices, templates, subject_idx = self.batcher.get_training_sequences_in_order(\n                self.num_render_sequences)\n            #Render each training sequence with the corresponding condition\n            condition_subj_idx = [[idx] for idx in subject_idx]\n        elif data_specifier == \'validation\':\n            raw_audio, processed_audio, vertices, templates, subject_idx = self.batcher.get_validation_sequences_in_order(\n                self.num_render_sequences)\n            #Render each validation sequence with all training conditions\n            num_training_subjects = self.batcher.get_num_training_subjects()\n            condition_subj_idx = [range(num_training_subjects) for idx in subject_idx]\n        else:\n            raise NotImplementedError(\'Unknown data specifier %s\' % data_specifier)\n\n        for i_seq in range(len(raw_audio)):\n            conditions = condition_subj_idx[i_seq]\n            for condition_idx in conditions:\n                condition_subj = self.batcher.convert_training_idx2subj(condition_idx)\n                video_fname = os.path.join(out_folder, \'%s_%03d_condition_%s.mp4\' % (data_specifier, i_seq, condition_subj))\n                self._render_sequences_helper(video_fname, raw_audio[i_seq], processed_audio[i_seq], templates[i_seq], vertices[i_seq], condition_idx)\n\n    def _render_sequences_helper(self, video_fname, seq_raw_audio, seq_processed_audio, seq_template, seq_verts, condition_idx):\n        def add_image_text(img, text):\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            textsize = cv2.getTextSize(text, font, 1, 2)[0]\n            textX = (img.shape[1] - textsize[0]) // 2\n            textY = textsize[1] + 10\n            cv2.putText(img, \'%s\' % (text), (textX, textY), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n\n        num_frames = seq_verts.shape[0]\n        tmp_audio_file = tempfile.NamedTemporaryFile(\'w\', suffix=\'.wav\', dir=os.path.dirname(video_fname))\n        wavfile.write(tmp_audio_file.name, seq_raw_audio[\'sample_rate\'], seq_raw_audio[\'audio\'])\n\n        tmp_video_file = tempfile.NamedTemporaryFile(\'w\', suffix=\'.mp4\', dir=os.path.dirname(video_fname))\n        if int(cv2.__version__[0]) < 3:\n            print(\'cv2 < 3\')\n            writer = cv2.VideoWriter(tmp_video_file.name, cv2.cv.CV_FOURCC(*\'mp4v\'), 60, (1600, 800), True)\n        else:\n            print(\'cv2 >= 3\')\n            writer = cv2.VideoWriter(tmp_video_file.name, cv2.VideoWriter_fourcc(*\'mp4v\'), 60, (1600, 800), True)\n\n        feed_dict = {self.speech_features: np.expand_dims(np.stack(seq_processed_audio), -1),\n                     self.condition_subject_id: np.repeat(condition_idx, num_frames),\n                     self.is_training: False,\n                     self.input_template: np.repeat(seq_template[np.newaxis,:,:,np.newaxis], num_frames, axis=0)}\n\n        predicted_vertices, predicted_offset = self.session.run([self.output_decoder, self.expression_offset], feed_dict)\n        predicted_vertices = np.squeeze(predicted_vertices)\n        center = np.mean(seq_verts[0], axis=0)\n\n        for i_frame in range(num_frames):\n            gt_img = render_mesh_helper(Mesh(seq_verts[i_frame], self.template_mesh.f), center)\n            add_image_text(gt_img, \'Captured data\')\n            pred_img = render_mesh_helper(Mesh(predicted_vertices[i_frame], self.template_mesh.f), center)\n            add_image_text(pred_img, \'VOCA prediction\')\n            img = np.hstack((gt_img, pred_img))\n            writer.write(img)\n        writer.release()\n\n        cmd = (\'ffmpeg\' + \' -i {0} -i {1} -vcodec h264 -ac 2 -channel_layout stereo -pix_fmt yuv420p {2}\'.format(\n            tmp_audio_file.name, tmp_video_file.name, video_fname)).split()\n        call(cmd)\n'"
