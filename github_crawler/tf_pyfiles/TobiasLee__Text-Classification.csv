file_path,api_count,code
models/RMDL.py,1,"b'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\nRMDL: Random Multimodel Deep Learning for Classification\n\n * Copyright (C) 2018  Kamran Kowsari <kk7nc@virginia.edu>\n * Last Update: May 3rd, 2018\n * This file is part of  RMDL project, University of Virginia.\n * Free to use, change, share and distribute source code of RMDL\n * Refrenced paper : RMDL: Random Multimodel Deep Learning for Classification\n * Refrenced paper : An Improvement of Data Classification using Random Multimodel Deep Learning (RMDL)\n * Comments and Error: email: kk7nc@virginia.edu\n\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\'\n\n\nimport os\nfrom RMDL import text_feature_extraction as txt\nfrom sklearn.model_selection import train_test_split\nfrom RMDL.Download import Download_WOS as WOS\nimport numpy as np\nfrom RMDL import RMDL_Text as RMDL\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.utils import shuffle\n\ndef load_data(file_name, sample_ratio=1, n_class=15, one_hot=True):\n    \'\'\'load data from .csv file\'\'\'\n    names = [""class"", ""title"", ""content""]\n    csv_file = pd.read_csv(file_name, names=names)\n    shuffle_csv = csv_file.sample(frac=sample_ratio)\n    x = pd.Series(shuffle_csv[""content""])\n    y = pd.Series(shuffle_csv[""class""])\n    if one_hot:\n        y = to_one_hot(y, n_class)\n    return x, y\n\n\n\nif __name__ == ""__main__"":\n    dbpedia = tf.contrib.learn.datasets.load_dataset(\'dbpedia\')\n\n\n    X_train, y_train = load_data(""dbpedia_data/dbpedia_csv/train.csv"", sample_ratio=1e-2, one_hot=False)\n    X_test, y_test = load_data(""dbpedia_data/dbpedia_csv/test.csv"", one_hot=False)\n\n\n    batch_size = 100\n    sparse_categorical = 0\n    n_epochs = [500, 500, 500]  ## DNN--RNN-CNN\n    Random_Deep = [3, 3, 3]  ## DNN--RNN-CNN\n\n    RMDL.Text_Classification(X_train, y_train, X_test, y_test,\n                             batch_size=batch_size,\n                             sparse_categorical=True,\n                             random_deep=Random_Deep,\n                             epochs=n_epochs)\n'"
models/adversarial_abblstm.py,42,"b'from tensorflow.contrib.rnn import BasicLSTMCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nimport time\nfrom utils.prepare_data import *\nfrom utils.model_helper import *\n\n\ndef scale_l2(x, norm_length):\n    # shape(x) = (batch, num_timesteps, d)\n    # Divide x by max(abs(x)) for a numerically stable L2 norm.\n    # 2norm(x) = a * 2norm(x/a)\n    # Scale over the full sequence, dims (1, 2)\n    alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n    l2_norm = alpha * tf.sqrt(\n        tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n    x_unit = x / l2_norm\n    return norm_length * x_unit\n\n\ndef normalize(emb, weights):\n    # weights = vocab_freqs / tf.reduce_sum(vocab_freqs) ?? \xe8\xbf\x99\xe4\xb8\xaa\xe5\xae\x9e\xe7\x8e\xb0\xe6\xb2\xa1\xe9\x97\xae\xe9\xa2\x98\xe5\x90\x97\n    print(""Weights: "", weights)\n    mean = tf.reduce_sum(weights * emb, 0, keep_dims=True)\n    var = tf.reduce_sum(weights * tf.pow(emb - mean, 2.), 0, keep_dims=True)\n    stddev = tf.sqrt(1e-6 + var)\n    return (emb - mean) / stddev\n\n\nclass AdversarialClassifier(object):\n    def __init__(self, config):\n        self.max_len = config[""max_len""]\n        self.hidden_size = config[""hidden_size""]\n        self.vocab_size = config[""vocab_size""]\n        self.embedding_size = config[""embedding_size""]\n        self.n_class = config[""n_class""]\n        self.learning_rate = config[""learning_rate""]\n        self.epsilon = config[""epsilon""]\n\n        # placeholder\n        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n        self.label = tf.placeholder(tf.int32, [None])\n        self.keep_prob = tf.placeholder(tf.float32)\n\n    def _add_perturbation(self, embedded, loss):\n        """"""Adds gradient to embedding and recomputes classification loss.""""""\n        grad, = tf.gradients(\n            loss,\n            embedded,\n            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n        grad = tf.stop_gradient(grad)\n        perturb = scale_l2(grad, self.epsilon)\n        return embedded + perturb\n\n    def _get_freq(self, vocab_freq, word2idx):\n        """"""get a frequency dict format as {word_idx: word_freq}""""""\n        words = vocab_freq.keys()\n        freq = [0] * self.vocab_size\n        for word in words:\n            word_idx = word2idx.get(word)\n            word_freq = vocab_freq[word]\n            freq[word_idx] = word_freq\n        return freq\n\n    def build_graph(self, vocab_freq, word2idx):\n        vocab_freqs = tf.constant(self._get_freq(vocab_freq, word2idx),\n                                  dtype=tf.float32, shape=(self.vocab_size, 1))\n        weights = vocab_freqs / tf.reduce_sum(vocab_freqs)\n        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n                                     trainable=True, name=""embedding_var"")\n        embedding_norm = normalize(embeddings_var, weights)\n        batch_embedded = tf.nn.embedding_lookup(embedding_norm, self.x)\n\n        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n        W_fc = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n        b_fc = tf.Variable(tf.constant(0., shape=[self.n_class]))\n\n        def cal_loss_logit(embedded, keep_prob, reuse=True, scope=""loss""):\n            with tf.variable_scope(scope, reuse=reuse) as scope:\n                rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n                                        BasicLSTMCell(self.hidden_size),\n                                        inputs=embedded, dtype=tf.float32)\n\n                # Attention\n                H = tf.add(rnn_outputs[0], rnn_outputs[1])  # fw + bw\n                M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n                # alpha (bs * sl, 1)\n                alpha = tf.nn.softmax(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n                                                tf.reshape(W, [-1, 1])))\n                r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(alpha, [-1, self.max_len,\n                                                                             1]))  # supposed to be (batch_size * HIDDEN_SIZE, 1)\n                r = tf.squeeze(r)\n                h_star = tf.tanh(r)\n                drop = tf.nn.dropout(h_star, keep_prob)\n\n                # Fully connected layer\xef\xbc\x88dense layer)\n                y_hat = tf.nn.xw_plus_b(drop, W_fc, b_fc)\n\n            return y_hat, tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n\n        logits, self.cls_loss = cal_loss_logit(batch_embedded, self.keep_prob, reuse=False)\n        embedding_perturbated = self._add_perturbation(batch_embedded, self.cls_loss)\n        adv_logits, self.adv_loss = cal_loss_logit(embedding_perturbated, self.keep_prob, reuse=True)\n        self.loss = self.cls_loss + self.adv_loss\n\n        # optimization\n        loss_to_minimize = self.loss\n        tvars = tf.trainable_variables()\n        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n                                                       name=\'train_step\')\n        self.prediction = tf.argmax(tf.nn.softmax(logits), 1)\n\n        print(""graph built successfully!"")\n\n\nif __name__ == \'__main__\':\n    # load data\n    x_train, y_train = load_data(""../dbpedia_data/dbpedia_csv/train.csv"", sample_ratio=1e-2, one_hot=False)\n    x_test, y_test = load_data(""../dbpedia_data/dbpedia_csv/test.csv"", one_hot=False)\n\n    # data preprocessing\n    x_train, x_test, vocab_freq, word2idx, vocab_size = \\\n        data_preprocessing_with_dict(x_train, x_test, max_len=32)\n    print(""train size: "", len(x_train))\n    print(""vocab size: "", vocab_size)\n\n    # split dataset to test and dev\n    x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n        split_dataset(x_test, y_test, 0.1)\n    print(""Validation Size: "", dev_size)\n\n    config = {\n        ""max_len"": 32,\n        ""hidden_size"": 64,\n        ""vocab_size"": vocab_size,\n        ""embedding_size"": 128,\n        ""n_class"": 15,\n        ""learning_rate"": 1e-3,\n        ""batch_size"": 32,\n        ""train_epoch"": 10,\n        ""epsilon"": 5,\n    }\n\n    classifier = AdversarialClassifier(config)\n    classifier.build_graph(vocab_freq, word2idx)\n\n    # auto GPU growth, avoid occupy all GPU memory\n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=tf_config)\n\n    sess.run(tf.global_variables_initializer())\n    dev_batch = (x_dev, y_dev)\n    start = time.time()\n    for e in range(config[""train_epoch""]):\n\n        t0 = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[""batch_size""]):\n            return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n\n        t1 = time.time()\n\n        print(""Train Epoch time:  %.3f s"" % (t1 - t0))\n        dev_acc = run_eval_step(classifier, sess, dev_batch)\n        print(""validation accuracy: %.3f "" % dev_acc)\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[""batch_size""]):\n        acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n        test_acc += acc\n        cnt += 1\n\n    print(""Test accuracy : %f %%"" % (test_acc / cnt * 100))\n'"
models/attn_bi_lstm.py,28,"b'from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nfrom tensorflow.contrib.rnn import BasicLSTMCell\nfrom utils.prepare_data import *\nimport time\nfrom utils.model_helper import *\n\n\nclass ABLSTM(object):\n    def __init__(self, config):\n        self.max_len = config[""max_len""]\n        self.hidden_size = config[""hidden_size""]\n        self.vocab_size = config[""vocab_size""]\n        self.embedding_size = config[""embedding_size""]\n        self.n_class = config[""n_class""]\n        self.learning_rate = config[""learning_rate""]\n\n        # placeholder\n        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n        self.label = tf.placeholder(tf.int32, [None])\n        self.keep_prob = tf.placeholder(tf.float32)\n\n    def build_graph(self):\n        print(""building graph"")\n        # Word embedding\n        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n                                     trainable=True)\n        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n\n        rnn_outputs, _ = bi_rnn(BasicLSTMCell(self.hidden_size),\n                                BasicLSTMCell(self.hidden_size),\n                                inputs=batch_embedded, dtype=tf.float32)\n\n        fw_outputs, bw_outputs = rnn_outputs\n\n        W = tf.Variable(tf.random_normal([self.hidden_size], stddev=0.1))\n        H = fw_outputs + bw_outputs  # (batch_size, seq_len, HIDDEN_SIZE)\n        M = tf.tanh(H)  # M = tanh(H)  (batch_size, seq_len, HIDDEN_SIZE)\n\n        self.alpha = tf.nn.softmax(tf.reshape(tf.matmul(tf.reshape(M, [-1, self.hidden_size]),\n                                                        tf.reshape(W, [-1, 1])),\n                                              (-1, self.max_len)))  # batch_size x seq_len\n        r = tf.matmul(tf.transpose(H, [0, 2, 1]),\n                      tf.reshape(self.alpha, [-1, self.max_len, 1]))\n        r = tf.squeeze(r)\n        h_star = tf.tanh(r)  # (batch , HIDDEN_SIZE\n\n        h_drop = tf.nn.dropout(h_star, self.keep_prob)\n\n        # Fully connected layer\xef\xbc\x88dense layer)\n        FC_W = tf.Variable(tf.truncated_normal([self.hidden_size, self.n_class], stddev=0.1))\n        FC_b = tf.Variable(tf.constant(0., shape=[self.n_class]))\n        y_hat = tf.nn.xw_plus_b(h_drop, FC_W, FC_b)\n\n        self.loss = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=self.label))\n\n        # prediction\n        self.prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n\n        # optimization\n        loss_to_minimize = self.loss\n        tvars = tf.trainable_variables()\n        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n                                                       name=\'train_step\')\n        print(""graph built successfully!"")\n\n\nif __name__ == \'__main__\':\n    # load data\n    x_train, y_train = load_data(""../dbpedia_data/dbpedia_csv/train.csv"", sample_ratio=1e-2, one_hot=False)\n    x_test, y_test = load_data(""../dbpedia_data/dbpedia_csv/test.csv"", one_hot=False)\n\n    # data preprocessing\n    x_train, x_test, vocab_size = \\\n        data_preprocessing_v2(x_train, x_test, max_len=32)\n    print(""train size: "", len(x_train))\n    print(""vocab size: "", vocab_size)\n\n    # split dataset to test and dev\n    x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n        split_dataset(x_test, y_test, 0.1)\n    print(""Validation Size: "", dev_size)\n\n    config = {\n        ""max_len"": 32,\n        ""hidden_size"": 64,\n        ""vocab_size"": vocab_size,\n        ""embedding_size"": 128,\n        ""n_class"": 15,\n        ""learning_rate"": 1e-3,\n        ""batch_size"": 4,\n        ""train_epoch"": 20\n    }\n\n    classifier = ABLSTM(config)\n    classifier.build_graph()\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    dev_batch = (x_dev, y_dev)\n    start = time.time()\n    for e in range(config[""train_epoch""]):\n\n        t0 = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[""batch_size""]):\n            return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n            attn = get_attn_weight(classifier, sess, (x_batch, y_batch))\n            # plot the attention weight\n            # print(np.reshape(attn, (config[""batch_size""], config[""max_len""])))\n        t1 = time.time()\n\n        print(""Train Epoch time:  %.3f s"" % (t1 - t0))\n        dev_acc = run_eval_step(classifier, sess, dev_batch)\n        print(""validation accuracy: %.3f "" % dev_acc)\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[""batch_size""]):\n        acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n        test_acc += acc\n        cnt += 1\n\n    print(""Test accuracy : %f %%"" % (test_acc / cnt * 100))\n'"
models/attn_lstm_hierarchical.py,17,"b'from modules.attention import attention\nfrom tensorflow.contrib.rnn import BasicLSTMCell\nfrom tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\nimport time\nfrom utils.prepare_data import *\n\n# Hyperparameter\nMAX_DOCUMENT_LENGTH = 256\nEMBEDDING_SIZE = 128\nHIDDEN_SIZE = 64\nATTENTION_SIZE = 64\nlr = 1e-3\nBATCH_SIZE = 1024\nKEEP_PROB = 0.5\nLAMBDA = 0.0001\nMAX_LABEL = 15\nepochs = 10\n\n\n# load data\nx_train, y_train = load_data(""../dbpedia_csv/train.csv"", sample_ratio=1)\nx_test, y_test = load_data(""../dbpedia_csv/test.csv"", sample_ratio=1)\n\n# data preprocessing\nx_train, x_test, vocab_size = \\\n    data_preprocessing_v2(x_train, x_test, MAX_DOCUMENT_LENGTH)\nprint(vocab_size)\n\n# split dataset to test and dev\nx_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n    split_dataset(x_test, y_test, 0.1)\nprint(""Validation size: "", dev_size)\n\ngraph = tf.Graph()\nwith graph.as_default():\n\n    batch_x = tf.placeholder(tf.int32, [None, MAX_DOCUMENT_LENGTH])\n    batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n    keep_prob = tf.placeholder(tf.float32)\n\n    embeddings_var = tf.Variable(tf.random_uniform([vocab_size, EMBEDDING_SIZE], -1.0, 1.0), trainable=True)\n    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_x)\n    # print(batch_embedded.shape)  # (?, 256, 100)\n    rnn_outputs, _ = tf.nn.dynamic_rnn(BasicLSTMCell(HIDDEN_SIZE), batch_embedded, dtype=tf.float32)\n\n    # Attention\n    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n    drop = tf.nn.dropout(attention_output, keep_prob)\n    shape = drop.get_shape()\n\n    # Fully connected layer\xef\xbc\x88dense layer)\n    W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n    b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n    y_hat = tf.nn.xw_plus_b(drop, W, b)\n\n\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\n    # Accuracy metric\n    prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    print(""Initialized! "")\n\n    print(""Start trainning"")\n    start = time.time()\n    for e in range(epochs):\n\n        epoch_start = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, BATCH_SIZE):\n            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: KEEP_PROB}\n            l, _, acc = sess.run([loss, optimizer, accuracy], feed_dict=fd)\n\n        epoch_finish = time.time()\n        print(""Validation accuracy: "", sess.run([accuracy, loss], feed_dict={\n            batch_x: x_dev,\n            batch_y: y_dev,\n            keep_prob: 1.0\n        }))\n        print(""epoch finished, time consumed : "", time.time() - epoch_start, "" s"")\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, BATCH_SIZE):\n            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: 1.0}\n            acc = sess.run(accuracy, feed_dict=fd)\n            test_acc += acc\n            cnt += 1        \n    \n    print(""Test accuracy : %f %%"" % ( test_acc / cnt * 100))\n\n\n\n'"
models/cnn.py,47,"b'from utils.prepare_data import *\nimport time\nfrom utils.model_helper import *\n\n\ndef linear(input_, output_size, scope=None):\n    """"""\n    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n    Args:\n    input_: a tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    scope: VariableScope for the created subgraph; defaults to ""Linear"".\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n\n    shape = input_.get_shape().as_list()\n    if len(shape) != 2:\n        raise ValueError(""Linear is expecting 2D arguments: %s"" % str(shape))\n    if not shape[1]:\n        raise ValueError(""Linear expects shape[1] of arguments: %s"" % str(shape))\n    input_size = shape[1]\n\n    # Now the computation.\n    with tf.variable_scope(scope or ""SimpleLinear""):\n        matrix = tf.get_variable(""Matrix"", [output_size, input_size], dtype=input_.dtype)\n        bias_term = tf.get_variable(""Bias"", [output_size], dtype=input_.dtype)\n\n    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n\n\ndef highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope=\'Highway\'):\n    """"""Highway Network (cf. http://arxiv.org/abs/1505.00387).\n    t = sigmoid(Wy + b)\n    z = t * g(Wy + b) + (1 - t) * y\n    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n    """"""\n\n    with tf.variable_scope(scope):\n        for idx in range(num_layers):\n            g = f(linear(input_, size, scope=\'highway_lin_%d\' % idx))\n\n            t = tf.sigmoid(linear(input_, size, scope=\'highway_gate_%d\' % idx) + bias)\n\n            output = t * g + (1. - t) * input_\n            input_ = output\n\n    return output\n\n\nclass CNNClassfier(object):\n    def __init__(self, config):\n        # configuration\n        self.max_len = config[""max_len""]\n        # topic nums + 1\n        self.num_classes = config[""n_class""]\n        self.vocab_size = config[""vocab_size""]\n        self.embedding_size = config[""embedding_size""]\n        self.filter_sizes = config[""filter_sizes""]\n        self.num_filters = config[""num_filters""]\n        self.l2_reg_lambda = config[""l2_reg_lambda""]\n        self.learning_rate = config[""learning_rate""]\n\n        # placeholder\n        self.x = tf.placeholder(tf.int32, [None, self.max_len], name=""input_x"")\n        self.label = tf.placeholder(tf.int32, [None], name=""input_y"")\n        self.keep_prob = tf.placeholder(tf.float32, name=""keep_prob"")\n\n    def build_graph(self):\n        print(""building graph"")\n        l2_loss = tf.constant(0.0)\n        with tf.variable_scope(""discriminator""):\n            # Embedding:\n            with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n                self.W = tf.Variable(\n                    tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n                    name=""W"")\n                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.x)  # batch_size * seq * embedding_size\n                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1) # expand dims for conv operation\n            pooled_outputs = list()\n            # Create a convolution + max-pool layer for each filter size\n            for filter_size, filter_num in zip(self.filter_sizes, self.num_filters):\n                with tf.name_scope(""cov2d-maxpool%s"" % filter_size):\n                    filter_shape = [filter_size, self.embedding_size, 1, filter_num]\n                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                    b = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=""b"")\n                    conv = tf.nn.conv2d(\n                        self.embedded_chars_expanded,\n                        W,\n                        strides=[1, 1, 1, 1],\n                        padding=""VALID"",\n                        name=""conv"")\n                    # print(conv.name, "": "", conv.shape) batch * (seq - filter_shape) + 1 * 1(output channel) *\n                    # filter_num\n                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                    pooled = tf.nn.max_pool(\n                        h,\n                        ksize=[1, self.max_len - filter_size + 1, 1, 1],\n                        strides=[1, 1, 1, 1],\n                        padding=\'VALID\',\n                        name=""pool"")  # \xe5\x85\xa8\xe9\x83\xa8\xe6\xb1\xa0\xe5\x8c\x96\xe5\x88\xb0 1x1\n                    # print(conv.name, "": "", conv.shape , ""----"", pooled.name, "" : "" ,pooled.shape)\n                    pooled_outputs.append(pooled)\n            total_filters_num = sum(self.num_filters)\n\n            self.h_pool = tf.concat(pooled_outputs, 3)\n            self.h_pool_flat = tf.reshape(self.h_pool, [-1, total_filters_num])  # batch * total_num\n\n            # highway network\n            with tf.name_scope(""highway""):\n                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n\n            # add droppout\n            with tf.name_scope(""dropout""):\n                self.h_drop = tf.nn.dropout(self.h_highway, self.keep_prob)\n\n            with tf.name_scope(""output""):\n                W = tf.Variable(tf.truncated_normal([total_filters_num, self.num_classes], stddev=0.1), name=""W"")\n                b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=""b"")\n                l2_loss += tf.nn.l2_loss(W)\n                l2_loss += tf.nn.l2_loss(b)\n                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")\n                self.ypred_for_auc = tf.nn.softmax(self.scores)\n                self.prediction = tf.cast(tf.argmax(self.ypred_for_auc, 1), dtype=tf.int32)\n\n            with tf.name_scope(""loss""):\n                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.label)\n                self.loss = losses + self.l2_reg_lambda * l2_loss\n            with tf.name_scope(""accuracy""):\n                self.accuracy = tf.reduce_mean(\n                    tf.cast(tf.equal(self.prediction, self.label), tf.float32))\n\n        self.params = [param for param in tf.trainable_variables() if \'discriminator\' in param.name]\n        d_optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        # aggregation_method =2 \xe8\x83\xbd\xe5\xa4\x9f\xe5\xb8\xae\xe5\x8a\xa9\xe5\x87\x8f\xe5\xb0\x91\xe5\x86\x85\xe5\xad\x98\xe5\x8d\xa0\xe7\x94\xa8\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n        self.train_op = d_optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n        print(""graph built successfully!"")\n\n\nif __name__ == \'__main__\':\n    # load data\n    x_train, y_train = load_data(""../dbpedia_data/dbpedia_csv/train.csv"", sample_ratio=1, one_hot=False)\n    x_test, y_test = load_data(""../dbpedia_data/dbpedia_csv/test.csv"", one_hot=False)\n\n    # data preprocessing\n    x_train, x_test, vocab_size = \\\n        data_preprocessing_v2(x_train, x_test, max_len=120)\n    print(""train size: "", len(x_train))\n    print(""vocab size: "", vocab_size)\n\n    # split dataset to test and dev\n    x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n        split_dataset(x_test, y_test, 0.1)\n    print(""Validation Size: "", dev_size)\n\n    config = {\n        ""max_len"": 120,\n        ""vocab_size"": vocab_size,\n        ""embedding_size"": 32,\n        ""learning_rate"": 1e-3,\n        ""l2_reg_lambda"": 1e-3,\n        ""batch_size"": 256,\n        ""n_class"": 15,\n\n        # random setting, may need fine-tune\n        ""filter_sizes"": [1, 2, 3, 4, 5, 10, 20, 50, 100, 120],\n        ""num_filters"": [128, 256, 256, 256, 256, 128, 128, 128, 128, 256],\n        ""train_epoch"": 10,\n    }\n\n    classifier = CNNClassfier(config)\n    classifier.build_graph()\n\n    # auto GPU growth, avoid occupy all GPU memory\n    tf_config = tf.ConfigProto()\n    tf_config.gpu_options.allow_growth = True\n    sess = tf.Session(config=tf_config)\n\n    sess.run(tf.global_variables_initializer())\n    dev_batch = (x_dev, y_dev)\n    start = time.time()\n    for e in range(config[""train_epoch""]):\n\n        t0 = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[""batch_size""]):\n            return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n\n        t1 = time.time()\n\n        print(""Train Epoch time:  %.3f s"" % (t1 - t0))\n        dev_acc = run_eval_step(classifier, sess, dev_batch)\n        print(""validation accuracy: %.3f "" % dev_acc)\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[""batch_size""]):\n        acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n        test_acc += acc\n        cnt += 1\n\n    print(""Test accuracy : %f %%"" % (test_acc / cnt * 100))\n'"
models/ind_rnn_tc.py,17,"b'from modules.indRNN import IndRNNCell\nfrom modules.attention import attention\nimport time\nfrom utils.prepare_data import *\n\n# Hyperparameter\nMAX_DOCUMENT_LENGTH = 256\nEMBEDDING_SIZE = 128\nHIDDEN_SIZE = 64\nATTENTION_SIZE = 64\nlr = 1e-3\nBATCH_SIZE = 1024\nKEEP_PROB = 0.5\nLAMBDA = 1e-3\nMAX_LABEL = 15\nepochs = 10\n\n# load data\nx_train, y_train = load_data(""../dbpedia_csv/train.csv"", sample_ratio=1)\nx_test, y_test = load_data(""../dbpedia_csv/test.csv"", sample_ratio=1)\n\n# data preprocessing\nx_train, x_test,  vocab_size = \\\n    data_preprocessing_v2(x_train, x_test, MAX_DOCUMENT_LENGTH)\nprint(vocab_size)\n\n# split dataset to test and dev\nx_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n    split_dataset(x_test, y_test, 0.1)\nprint(""Validation size: "", dev_size)\n\ngraph = tf.Graph()\nwith graph.as_default():\n\n    batch_x = tf.placeholder(tf.int32, [None, MAX_DOCUMENT_LENGTH])\n    batch_y = tf.placeholder(tf.float32, [None, MAX_LABEL])\n    keep_prob = tf.placeholder(tf.float32)\n\n    embeddings_var = tf.Variable(tf.random_uniform([vocab_size, EMBEDDING_SIZE], -1.0, 1.0), trainable=True)\n    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_x)\n    print(batch_embedded.shape)  # (?, 256, 100)\n\n    cell = IndRNNCell(HIDDEN_SIZE)\n    rnn_outputs, _ = tf.nn.dynamic_rnn(cell, batch_embedded, dtype=tf.float32)\n\n    # Attention\n    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n    drop = tf.nn.dropout(attention_output, keep_prob)\n    shape = drop.get_shape()\n\n    # Fully connected layer\xef\xbc\x88dense layer)\n    W = tf.Variable(tf.truncated_normal([shape[1].value, MAX_LABEL], stddev=0.1))\n    b = tf.Variable(tf.constant(0., shape=[MAX_LABEL]))\n    y_hat = tf.nn.xw_plus_b(drop, W, b)\n\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=batch_y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\n    # Accuracy metric\n    prediction = tf.argmax(tf.nn.softmax(y_hat), 1)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(batch_y, 1)), tf.float32))\n\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    print(""Initialized! "")\n\n    print(""Start trainning"")\n    start = time.time()\n    for e in range(epochs):\n\n        epoch_start = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, BATCH_SIZE):\n            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: KEEP_PROB}\n            l, _, acc = sess.run([loss, optimizer, accuracy], feed_dict=fd)\n\n        epoch_finish = time.time()\n        print(""Validation accuracy: "", sess.run([accuracy, loss], feed_dict={\n            batch_x: x_dev,\n            batch_y: y_dev,\n            keep_prob: 1.0\n        }))\n        print(""Epoch time: "", time.time() - epoch_start, ""s"")\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, BATCH_SIZE):\n            fd = {batch_x: x_batch, batch_y: y_batch, keep_prob: 1.0}\n            acc = sess.run(accuracy, feed_dict=fd)\n            test_acc += acc\n            cnt += 1        \n    \n    print(""Test accuracy : %f %%"" % ( test_acc / cnt * 100))\n\n\n\n\n\n'"
models/multi_head.py,17,"b'from modules.multihead import *\nfrom utils.model_helper import *\nimport time\nfrom utils.prepare_data import *\n\n\nclass AttentionClassifier(object):\n    def __init__(self, config):\n        self.max_len = config[""max_len""]\n        self.hidden_size = config[""hidden_size""]\n        self.vocab_size = config[""vocab_size""]\n        self.embedding_size = config[""embedding_size""]\n        self.n_class = config[""n_class""]\n        self.learning_rate = config[""learning_rate""]\n\n        # placeholder\n        self.x = tf.placeholder(tf.int32, [None, self.max_len])\n        self.label = tf.placeholder(tf.int32, [None])\n        self.keep_prob = tf.placeholder(tf.float32)\n\n    def build_graph(self):\n        print(""building graph..."")\n        embeddings_var = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),\n                                     trainable=True)\n        batch_embedded = tf.nn.embedding_lookup(embeddings_var, self.x)\n        # multi-head attention\n        ma = multihead_attention(queries=batch_embedded, keys=batch_embedded)\n        # FFN(x) = LN(x + point-wisely NN(x))\n        outputs = feedforward(ma, [self.hidden_size, self.embedding_size])\n        outputs = tf.reshape(outputs, [-1, self.max_len * self.embedding_size])\n        logits = tf.layers.dense(outputs, units=self.n_class)\n\n        self.loss = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.label))\n        self.prediction = tf.argmax(tf.nn.softmax(logits), 1)\n\n        # optimization\n        loss_to_minimize = self.loss\n        tvars = tf.trainable_variables()\n        gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n        grads, global_norm = tf.clip_by_global_norm(gradients, 1.0)\n\n        self.global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.train_op = self.optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step,\n                                                       name=\'train_step\')\n        print(""graph built successfully!"")\n\n\nif __name__ == \'__main__\':\n    # load data\n    x_train, y_train = load_data(""../dbpedia_data/dbpedia_csv/train.csv"", sample_ratio=1e-2, one_hot=False)\n    x_test, y_test = load_data(""../dbpedia_data/dbpedia_csv/test.csv"", one_hot=False)\n\n    # data preprocessing\n    x_train, x_test, vocab_size = \\\n        data_preprocessing_v2(x_train, x_test, max_len=32)\n    print(""train size: "", len(x_train))\n    print(""vocab size: "", vocab_size)\n\n    # split dataset to test and dev\n    x_test, x_dev, y_test, y_dev, dev_size, test_size = \\\n        split_dataset(x_test, y_test, 0.1)\n    print(""Validation Size: "", dev_size)\n\n    config = {\n        ""max_len"": 32,\n        ""hidden_size"": 64,\n        ""vocab_size"": vocab_size,\n        ""embedding_size"": 128,\n        ""n_class"": 15,\n        ""learning_rate"": 1e-3,\n        ""batch_size"": 32,\n        ""train_epoch"": 20\n    }\n\n    classifier = AttentionClassifier(config)\n    classifier.build_graph()\n\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    dev_batch = (x_dev, y_dev)\n    start = time.time()\n    for e in range(config[""train_epoch""]):\n\n        t0 = time.time()\n        print(""Epoch %d start !"" % (e + 1))\n        for x_batch, y_batch in fill_feed_dict(x_train, y_train, config[""batch_size""]):\n            return_dict = run_train_step(classifier, sess, (x_batch, y_batch))\n\n        t1 = time.time()\n\n        print(""Train Epoch time:  %.3f s"" % (t1 - t0))\n        dev_acc = run_eval_step(classifier, sess, dev_batch)\n        print(""validation accuracy: %.3f "" % dev_acc)\n\n    print(""Training finished, time consumed : "", time.time() - start, "" s"")\n    print(""Start evaluating:  \\n"")\n    cnt = 0\n    test_acc = 0\n    for x_batch, y_batch in fill_feed_dict(x_test, y_test, config[""batch_size""]):\n        acc = run_eval_step(classifier, sess, (x_batch, y_batch))\n        test_acc += acc\n        cnt += 1\n\n    print(""Test accuracy : %f %%"" % (test_acc / cnt * 100))\n\n'"
utils/__init__.py,0,b''
utils/model_helper.py,0,"b'import numpy as np\n\n\ndef make_train_feed_dict(model, batch):\n    """"""make train feed dict for training""""""\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: .5}\n    return feed_dict\n\n\ndef make_test_feed_dict(model, batch):\n    feed_dict = {model.x: batch[0],\n                 model.label: batch[1],\n                 model.keep_prob: 1.0}\n    return feed_dict\n\n\ndef run_train_step(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    to_return = {\n        \'train_op\': model.train_op,\n        \'loss\': model.loss,\n        \'global_step\': model.global_step,\n    }\n    return sess.run(to_return, feed_dict)\n\n\ndef run_eval_step(model, sess, batch):\n    feed_dict = make_test_feed_dict(model, batch)\n    prediction = sess.run(model.prediction, feed_dict)\n    acc = np.sum(np.equal(prediction, batch[1])) / len(prediction)\n    return acc\n\n\ndef get_attn_weight(model, sess, batch):\n    feed_dict = make_train_feed_dict(model, batch)\n    return sess.run(model.alpha, feed_dict)\n'"
utils/prepare_data.py,3,"b'import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.utils import shuffle\nnames = [""class"", ""title"", ""content""]\n\n\ndef to_one_hot(y, n_class):\n    return np.eye(n_class)[y.astype(int)]\n\n\ndef load_data(file_name, sample_ratio=1, n_class=15, names=names, one_hot=True):\n    \'\'\'load data from .csv file\'\'\'\n    csv_file = pd.read_csv(file_name, names=names)\n    shuffle_csv = csv_file.sample(frac=sample_ratio)\n    x = pd.Series(shuffle_csv[""content""])\n    y = pd.Series(shuffle_csv[""class""])\n    if one_hot:\n        y = to_one_hot(y, n_class)\n    return x, y\n\n\ndef data_preprocessing(train, test, max_len):\n    """"""transform to one-hot idx vector by VocabularyProcessor""""""\n    """"""VocabularyProcessor is deprecated, use v2 instead""""""\n    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_len)\n    x_transform_train = vocab_processor.fit_transform(train)\n    x_transform_test = vocab_processor.transform(test)\n    vocab = vocab_processor.vocabulary_\n    vocab_size = len(vocab)\n    x_train_list = list(x_transform_train)\n    x_test_list = list(x_transform_test)\n    x_train = np.array(x_train_list)\n    x_test = np.array(x_test_list)\n\n    return x_train, x_test, vocab, vocab_size\n\n\ndef data_preprocessing_v2(train, test, max_len, max_words=50000):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(train)\n    train_idx = tokenizer.texts_to_sequences(train)\n    test_idx = tokenizer.texts_to_sequences(test)\n    train_padded = pad_sequences(train_idx, maxlen=max_len, padding=\'post\', truncating=\'post\')\n    test_padded = pad_sequences(test_idx, maxlen=max_len, padding=\'post\', truncating=\'post\')\n    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n    return train_padded, test_padded, max_words + 2\n\n\ndef data_preprocessing_with_dict(train, test, max_len):\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\'<UNK>\')\n    tokenizer.fit_on_texts(train)\n    train_idx = tokenizer.texts_to_sequences(train)\n    test_idx = tokenizer.texts_to_sequences(test)\n    train_padded = pad_sequences(train_idx, maxlen=max_len, padding=\'post\', truncating=\'post\')\n    test_padded = pad_sequences(test_idx, maxlen=max_len, padding=\'post\', truncating=\'post\')\n    # vocab size = len(word_docs) + 2  (<UNK>, <PAD>)\n    return train_padded, test_padded, tokenizer.word_docs, tokenizer.word_index, len(tokenizer.word_docs) + 2\n\n\ndef split_dataset(x_test, y_test, dev_ratio):\n    """"""split test dataset to test and dev set with ratio """"""\n    test_size = len(x_test)\n    print(test_size)\n    dev_size = (int)(test_size * dev_ratio)\n    print(dev_size)\n    x_dev = x_test[:dev_size]\n    x_test = x_test[dev_size:]\n    y_dev = y_test[:dev_size]\n    y_test = y_test[dev_size:]\n    return x_test, x_dev, y_test, y_dev, dev_size, test_size - dev_size\n\n\ndef fill_feed_dict(data_X, data_Y, batch_size):\n    """"""Generator to yield batches""""""\n    # Shuffle data first.\n    shuffled_X, shuffled_Y = shuffle(data_X, data_Y)\n    # print(""before shuffle: "", data_Y[:10])\n    # print(data_X.shape[0])\n    # perm = np.random.permutation(data_X.shape[0])\n    # data_X = data_X[perm]\n    # shuffled_Y = data_Y[perm]\n    # print(""after shuffle: "", shuffled_Y[:10])\n    for idx in range(data_X.shape[0] // batch_size):\n        x_batch = shuffled_X[batch_size * idx: batch_size * (idx + 1)]\n        y_batch = shuffled_Y[batch_size * idx: batch_size * (idx + 1)]\n        yield x_batch, y_batch\n'"
models/modules/attention.py,10,"b'import tensorflow as tf\n\n\ndef attention(inputs, attention_size, time_major=False, return_alphas=False):\n    """"""\n    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n    The idea was proposed in the article by Z. Yang et al., ""Hierarchical Attention Networks\n     for Document Classification"", 2016: http://www.aclweb.org/anthology/N16-1174.\n    Variables notation is also inherited from the article\n\n    Args:\n        inputs: The Attention inputs.\n            Matches outputs of RNN/Bi-RNN layer (not final state):\n                In case of RNN, this must be RNN outputs `Tensor`:\n                    If time_major == False (default), this must be a tensor of shape:\n                        `[batch_size, max_time, cell.output_size]`.\n                    If time_major == True, this must be a tensor of shape:\n                        `[max_time, batch_size, cell.output_size]`.\n                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n                the backward RNN outputs `Tensor`.\n                    If time_major == False (default),\n                        outputs_fw is a `Tensor` shaped:\n                        `[batch_size, max_time, cell_fw.output_size]`\n                        and outputs_bw is a `Tensor` shaped:\n                        `[batch_size, max_time, cell_bw.output_size]`.\n                    If time_major == True,\n                        outputs_fw is a `Tensor` shaped:\n                        `[max_time, batch_size, cell_fw.output_size]`\n                        and outputs_bw is a `Tensor` shaped:\n                        `[max_time, batch_size, cell_bw.output_size]`.\n        attention_size: Linear size of the Attention weights.\n        time_major: The shape format of the `inputs` Tensors.\n            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n            Using `time_major = True` is a bit more efficient because it avoids\n            transposes at the beginning and end of the RNN calculation.  However,\n            most TensorFlow data is batch-major, so by default this function\n            accepts input and emits output in batch-major form.\n        return_alphas: Whether to return attention coefficients variable along with layer\'s output.\n            Used for visualization purpose.\n    Returns:\n        The Attention output `Tensor`.\n        In case of RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell.output_size]`.\n        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n    """"""\n\n    if isinstance(inputs, tuple):\n        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n        inputs = tf.concat(inputs, 2)\n\n    if time_major:\n        # (T,B,D) => (B,T,D)\n        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n\n    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n\n    # Trainable parameters\n    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n\n    with tf.name_scope(\'v\'):\n        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n\n    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n    vu = tf.tensordot(v, u_omega, axes=1, name=\'vu\')  # (B,T) shape\n    alphas = tf.nn.softmax(vu, name=\'alphas\')  # (B,T) shape\n\n    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n\n    if not return_alphas:\n        return output\n    else:\n        return output, alphas'"
models/modules/indRNN.py,0,"b'from tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.layers import base as base_layer\nfrom tensorflow.python.ops.rnn_cell_impl import LayerRNNCell\n\n\nclass IndRNNCell(LayerRNNCell):  # \xe7\xbb\xa7\xe6\x89\xbf LayerRNNCell\n\n    def __init__(self,\n                 num_units,\n                 recurrent_min_abs=0,\n                 recurrent_max_abs=None,\n                 recurrent_kernel_initializer=None,\n                 input_kernel_initializer=None,\n                 activation=None,\n                 reuse=None,\n                 name=None):\n        super(IndRNNCell, self).__init__(_reuse=reuse, name=name)\n\n        self.input_spec = base_layer.InputSpec(ndim=2)\n\n        # initialization\n        self._num_units = num_units\n        self._recurrent_min_abs = recurrent_min_abs\n\n        self._recurrent_max_abs = recurrent_max_abs\n        self._recurrent_recurrent_kernel_initializer = recurrent_kernel_initializer\n        self._input_kernel_initializer = input_kernel_initializer\n        self._activation = activation or nn_ops.relu\n\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def build(self, inputs_shape):\n        \'\'\'construct the IndRNN Cell\'\'\'\n        if inputs_shape[1].value is None:\n            raise ValueError(""Expected input shape[1] is known"")\n\n        input_depth = inputs_shape[1]\n        if self._input_kernel_initializer is None:\n            self._input_kernel_initializer = init_ops.random_normal_initializer(mean=0,\n                                                                                stddev=1e-3)\n        # matrix W\n        self._input_kernel = self.add_variable(\n            ""input_kernel"",\n            shape=[input_depth, self._num_units],\n            initializer=self._input_kernel_initializer\n        )\n\n        if self._recurrent_recurrent_kernel_initializer is None:\n            self._recurrent_recurrent_kernel_initializer = init_ops.constant_initializer(1.)\n\n        # matrix U\n        self._recurrent_kernel = self.add_variable(\n            ""recurrent_kernel"",\n            shape=[self._num_units],\n            initializer=self._recurrent_recurrent_kernel_initializer\n        )\n\n        # Clip the U to min - max\n        if self._recurrent_min_abs:\n            abs_kernel = math_ops.abs(self._recurrent_kernel)\n            min_abs_kernel = math_ops.maximum(abs_kernel, self._recurrent_min_abs)\n            self._recurrent_kernel = math_ops.multiply(\n                math_ops.sign(self._recurrent_kernel),\n                min_abs_kernel\n            )\n        if self._recurrent_max_abs:\n            self._recurrent_kernel = clip_ops.clip_by_value(\n                self._recurrent_kernel,\n                -self._recurrent_max_abs,\n                self._recurrent_max_abs\n            )\n\n        self._bias = self.add_variable(\n            ""bias"",\n            shape=[self._num_units],\n            initializer=init_ops.zeros_initializer(dtype=self.dtype)\n        )\n        # built finished\n        self.built = True\n\n\n    def call(self, inputs, state):\n        \'\'\'output = new state = activation(W * x + U (*) h_t-1 + b)\'\'\'\n\n        gate_inputs = math_ops.matmul(inputs, self._input_kernel)\n        # (*)\n        state_update = math_ops.multiply(state, self._recurrent_kernel)\n        gate_inputs = math_ops.add(gate_inputs, state_update)\n        gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)\n        output = self._activation(gate_inputs)\n        return output, output\n\n\n\n\n\n\n\n\n'"
models/modules/multihead.py,33,"b'import tensorflow as tf\n\n\ndef layer_normalization(inputs,\n                        epsilon=1e-8,\n                        scope=""ln"",\n                        reuse=None):\n    with tf.variable_scope(scope, reuse=reuse):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** .5)\n        outputs = gamma * normalized + beta\n\n    return outputs\n\n\ndef multihead_attention(queries,\n                        keys,\n                        num_units=None,\n                        num_heads=8,\n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False,\n                        scope=""multihead_attention"",\n                        reuse=None):\n    with tf.variable_scope(scope, reuse=reuse):\n        if num_units is None:  # set default size for attention size C\n            num_units = queries.get_shape().as_list()[-1]\n\n        # Linear Projections\n        Q = tf.layers.dense(queries, num_units, activation=tf.nn.relu)  # [N, T_q, C]\n        K = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n        V = tf.layers.dense(keys, num_units, activation=tf.nn.relu)  # [N, T_k, C]\n\n        # Split and concat\n        Q_ = tf.concat(tf.split(Q, num_heads, axis=-1), axis=0)  # [num_heads * N, T_q, C/num_heads]\n        K_ = tf.concat(tf.split(K, num_heads, axis=-1), axis=0)  # [num_heads * N, T_k, C/num_heads]\n        V_ = tf.concat(tf.split(V, num_heads, axis=-1), axis=0)  # [num_heads * N, T_k, C/num_heads]\n\n        # Attention\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (num_heads * N, T_q, T_k)\n\n        # Scale : outputs = outputs / sqrt( d_k)\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n\n        # Key Masking\n        # see : https://github.com/Kyubyong/transformer/issues/3\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n\n        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)  # -infinity\n        outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n\n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense()  # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n            outputs = tf.where(tf.equal(masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n\n        # Activation: outputs is a weight matrix\n        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n        outputs *= query_masks  # broadcasting. (N, T_q, C)\n\n        # dropouts\n        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))\n\n        # weighted sum\n        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n\n        # reshape\n        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # (N, T_q, C)\n\n        # residual connection\n        outputs += queries\n\n        # layer normaliztion\n        outputs = layer_normalization(outputs)\n        return outputs\n\n\ndef feedforward(inputs,\n                num_units=[2048, 512],\n                scope=""multihead_attention"",\n                reuse=None):\n    with tf.variable_scope(scope, reuse=reuse):\n        # Inner layer\n        params = {""inputs"": inputs, ""filters"": num_units[0], ""kernel_size"": 1,\n                  ""activation"": tf.nn.relu, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Readout layer\n        params = {""inputs"": outputs, ""filters"": num_units[1], ""kernel_size"": 1,\n                  ""activation"": None, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        print(""Conv ret:"", outputs.shape)\n        # Residual connection\n        outputs += inputs\n\n        # Normalize\n        outputs = layer_normalization(outputs)\n\n    return outputs'"
