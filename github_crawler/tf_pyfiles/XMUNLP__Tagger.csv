file_path,api_count,code
tagger/__init__.py,0,b''
tagger/bin/predictor.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport logging\nimport os\nimport six\nimport time\nimport torch\n\nimport tagger.data as data\nimport tagger.models as models\nimport tagger.utils as utils\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=""Predict using SRL models"",\n        usage=""translator.py [<args>] [-h | --help]""\n    )\n\n    # input files\n    parser.add_argument(""--input"", type=str, required=True,\n                        help=""Path of input file"")\n    parser.add_argument(""--output"", type=str, required=True,\n                        help=""Path of output file"")\n    parser.add_argument(""--checkpoint"", type=str, required=True,\n                        help=""Path of trained models"")\n    parser.add_argument(""--vocabulary"", type=str, nargs=2, required=True,\n                        help=""Path of source and target vocabulary"")\n\n    # model and configuration\n    parser.add_argument(""--model"", type=str, required=True,\n                        help=""Name of the model"")\n    parser.add_argument(""--parameters"", type=str, default="""",\n                        help=""Additional hyper parameters"")\n    parser.add_argument(""--half"", action=""store_true"",\n                        help=""Use half precision for decoding"")\n\n    return parser.parse_args()\n\n\ndef default_params():\n    params = utils.HParams(\n        input=None,\n        output=None,\n        vocabulary=None,\n        embedding="""",\n        # vocabulary specific\n        pad=""<pad>"",\n        bos=""<bos>"",\n        eos=""<eos>"",\n        unk=""<unk>"",\n        device=0,\n        decode_batch_size=128\n    )\n\n    return params\n\n\ndef merge_params(params1, params2):\n    params = utils.HParams()\n\n    for (k, v) in six.iteritems(params1.values()):\n        params.add_hparam(k, v)\n\n    params_dict = params.values()\n\n    for (k, v) in six.iteritems(params2.values()):\n        if k in params_dict:\n            # Override\n            setattr(params, k, v)\n        else:\n            params.add_hparam(k, v)\n\n    return params\n\n\ndef import_params(model_dir, model_name, params):\n    model_dir = os.path.abspath(model_dir)\n    m_name = os.path.join(model_dir, model_name + "".json"")\n\n    if not os.path.exists(m_name):\n        return params\n\n    with open(m_name) as fd:\n        logging.info(""Restoring model parameters from %s"" % m_name)\n        json_str = fd.readline()\n        params.parse_json(json_str)\n\n    return params\n\n\ndef override_params(params, args):\n    params.parse(args.parameters)\n\n    src_vocab, src_w2idx, src_idx2w = data.load_vocabulary(args.vocabulary[0])\n    tgt_vocab, tgt_w2idx, tgt_idx2w = data.load_vocabulary(args.vocabulary[1])\n\n    params.vocabulary = {\n        ""source"": src_vocab, ""target"": tgt_vocab\n    }\n    params.lookup = {\n        ""source"": src_w2idx, ""target"": tgt_w2idx\n    }\n    params.mapping = {\n        ""source"": src_idx2w, ""target"": tgt_idx2w\n    }\n\n    return params\n\n\ndef convert_to_string(inputs, tensor, params):\n    inputs = torch.squeeze(inputs)\n    inputs = inputs.tolist()\n    tensor = torch.squeeze(tensor, dim=1)\n    tensor = tensor.tolist()\n    decoded = []\n\n    for wids, lids in zip(inputs, tensor):\n        output = []\n        for wid, lid in zip(wids, lids):\n            if wid == 0:\n                break\n            output.append(params.mapping[""target""][lid])\n        decoded.append(b"" "".join(output))\n\n    return decoded\n\n\ndef main(args):\n    # Load configs\n    model_cls = models.get_model(args.model)\n    params = default_params()\n    params = merge_params(params, model_cls.default_params())\n    params = import_params(args.checkpoint, args.model, params)\n    params = override_params(params, args)\n    torch.cuda.set_device(params.device)\n    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    # Create model\n    with torch.no_grad():\n        model = model_cls(params).cuda()\n\n        if args.half:\n            model = model.half()\n            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n\n        model.eval()\n        model.load_state_dict(\n            torch.load(utils.best_checkpoint(args.checkpoint),\n                       map_location=""cpu"")[""model""])\n\n        # Decoding\n        dataset = data.get_dataset(args.input, ""infer"", params)\n        fd = open(args.output, ""wb"")\n        counter = 0\n\n        if params.embedding is not None:\n            embedding = data.load_glove_embedding(params.embedding)\n        else:\n            embedding = None\n\n        for features in dataset:\n            t = time.time()\n            counter += 1\n            features = data.lookup(features, ""infer"", params, embedding)\n\n            labels = model.argmax_decode(features)\n            batch = convert_to_string(features[""inputs""], labels, params)\n\n            for seq in batch:\n                fd.write(seq)\n                fd.write(b""\\n"")\n\n            t = time.time() - t\n            print(""Finished batch: %d (%.3f sec)"" % (counter, t))\n\n        fd.close()\n\n\nif __name__ == ""__main__"":\n    main(parse_args())\n'"
tagger/bin/trainer.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport copy\nimport glob\nimport logging\nimport os\nimport re\nimport six\nimport socket\nimport threading\nimport time\nimport torch\n\nimport tagger.data as data\nimport torch.distributed as dist\nimport tagger.models as models\nimport tagger.optimizers as optimizers\nimport tagger.utils as utils\nimport tagger.utils.summary as summary\nfrom tagger.utils.validation import ValidationWorker\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser(\n        description=""Training SRL tagger"",\n        usage=""trainer.py [<args>] [-h | --help]""\n    )\n\n    # input files\n    parser.add_argument(""--input"", type=str,\n                        help=""Path of the training corpus"")\n    parser.add_argument(""--output"", type=str, default=""train"",\n                        help=""Path to saved models"")\n    parser.add_argument(""--vocabulary"", type=str, nargs=2,\n                        help=""Path of source and target vocabulary"")\n    parser.add_argument(""--checkpoint"", type=str,\n                        help=""Path to pre-trained checkpoint"")\n    parser.add_argument(""--distributed"", action=""store_true"",\n                        help=""Enable distributed training mode"")\n    parser.add_argument(""--local_rank"", type=int,\n                        help=""Local rank of this process"")\n    parser.add_argument(""--half"", action=""store_true"",\n                        help=""Enable mixed precision training"")\n    parser.add_argument(""--hparam_set"", type=str,\n                        help=""Name of pre-defined hyper parameter set"")\n\n    # model and configuration\n    parser.add_argument(""--model"", type=str, required=True,\n                        help=""Name of the model"")\n    parser.add_argument(""--parameters"", type=str, default="""",\n                        help=""Additional hyper parameters"")\n\n    return parser.parse_args(args)\n\n\ndef default_params():\n    params = utils.HParams(\n        input="""",\n        output="""",\n        model=""transformer"",\n        vocab=["""", """"],\n        pad=""<pad>"",\n        bos=""<eos>"",\n        eos=""<eos>"",\n        unk=""<unk>"",\n        # Dataset\n        batch_size=4096,\n        fixed_batch_size=False,\n        min_length=1,\n        max_length=256,\n        buffer_size=10000,\n        # Initialization\n        initializer_gain=1.0,\n        initializer=""uniform_unit_scaling"",\n        # Regularization\n        scale_l1=0.0,\n        scale_l2=0.0,\n        # Training\n        script="""",\n        warmup_steps=4000,\n        train_steps=100000,\n        update_cycle=1,\n        optimizer=""Adam"",\n        adam_beta1=0.9,\n        adam_beta2=0.999,\n        adam_epsilon=1e-8,\n        adadelta_rho=0.95,\n        adadelta_epsilon=1e-6,\n        clipping=""global_norm"",\n        clip_grad_norm=5.0,\n        learning_rate=1.0,\n        learning_rate_schedule=""linear_warmup_rsqrt_decay"",\n        learning_rate_boundaries=[0],\n        learning_rate_values=[0.0],\n        device_list=[0],\n        embedding="""",\n        # Validation\n        keep_top_k=50,\n        frequency=10,\n        # Checkpoint Saving\n        keep_checkpoint_max=20,\n        keep_top_checkpoint_max=5,\n        save_summary=True,\n        save_checkpoint_secs=0,\n        save_checkpoint_steps=1000,\n    )\n\n    return params\n\n\ndef import_params(model_dir, model_name, params):\n    model_dir = os.path.abspath(model_dir)\n    p_name = os.path.join(model_dir, ""params.json"")\n    m_name = os.path.join(model_dir, model_name + "".json"")\n\n    if not os.path.exists(p_name) or not os.path.exists(m_name):\n        return params\n\n    with open(p_name) as fd:\n        logging.info(""Restoring hyper parameters from %s"" % p_name)\n        json_str = fd.readline()\n        params.parse_json(json_str)\n\n    with open(m_name) as fd:\n        logging.info(""Restoring model parameters from %s"" % m_name)\n        json_str = fd.readline()\n        params.parse_json(json_str)\n\n    return params\n\n\ndef export_params(output_dir, name, params):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Save params as params.json\n    filename = os.path.join(output_dir, name)\n\n    with open(filename, ""w"") as fd:\n        fd.write(params.to_json())\n\n\ndef merge_params(params1, params2):\n    params = utils.HParams()\n\n    for (k, v) in six.iteritems(params1.values()):\n        params.add_hparam(k, v)\n\n    params_dict = params.values()\n\n    for (k, v) in six.iteritems(params2.values()):\n        if k in params_dict:\n            # Override\n            setattr(params, k, v)\n        else:\n            params.add_hparam(k, v)\n\n    return params\n\n\ndef override_params(params, args):\n    params.model = args.model or params.model\n    params.input = args.input or params.input\n    params.output = args.output or params.output\n    params.vocab = args.vocabulary or params.vocab\n    params.parse(args.parameters)\n\n    src_vocab, src_w2idx, src_idx2w = data.load_vocabulary(params.vocab[0])\n    tgt_vocab, tgt_w2idx, tgt_idx2w = data.load_vocabulary(params.vocab[1])\n\n    params.vocabulary = {\n        ""source"": src_vocab, ""target"": tgt_vocab\n    }\n    params.lookup = {\n        ""source"": src_w2idx, ""target"": tgt_w2idx\n    }\n    params.mapping = {\n        ""source"": src_idx2w, ""target"": tgt_idx2w\n    }\n\n    return params\n\n\ndef collect_params(all_params, params):\n    collected = utils.HParams()\n\n    for k in six.iterkeys(params.values()):\n        collected.add_hparam(k, getattr(all_params, k))\n\n    return collected\n\n\ndef print_variables(model):\n    weights = {v[0]: v[1] for v in model.named_parameters()}\n    total_size = 0\n\n    for name in sorted(list(weights)):\n        v = weights[name]\n        print(""%s %s"" % (name.ljust(60), str(list(v.shape)).rjust(15)))\n        total_size += v.nelement()\n\n    print(""Total trainable variables size: %d"" % total_size)\n\n\ndef save_checkpoint(step, epoch, model, optimizer, params):\n    if dist.get_rank() == 0:\n        state = {\n            ""step"": step,\n            ""epoch"": epoch,\n            ""model"": model.state_dict(),\n            ""optimizer"": optimizer.state_dict()\n        }\n        utils.save(state, params.output, params.keep_checkpoint_max)\n\n\ndef infer_gpu_num(param_str):\n    result = re.match(r"".*device_list=\\[(.*?)\\].*"", param_str)\n\n    if not result:\n        return 1\n    else:\n        dev_str = result.groups()[-1]\n        return len(dev_str.split("",""))\n\n\ndef get_clipper(params):\n    if params.clipping.lower() == ""none"":\n        clipper = None\n    elif params.clipping.lower() == ""adaptive"":\n        clipper = optimizers.adaptive_clipper(0.95)\n    elif params.clipping.lower() == ""global_norm"":\n        clipper = optimizers.global_norm_clipper(params.clip_grad_norm)\n    else:\n        raise ValueError(""Unknown clipper %s"" % params.clipping)\n\n    return clipper\n\n\ndef get_learning_rate_schedule(params):\n    if params.learning_rate_schedule == ""linear_warmup_rsqrt_decay"":\n        schedule = optimizers.LinearWarmupRsqrtDecay(params.learning_rate,\n                                                     params.warmup_steps)\n    elif params.learning_rate_schedule == ""piecewise_constant_decay"":\n        schedule = optimizers.PiecewiseConstantDecay(\n            params.learning_rate_boundaries, params.learning_rate_values)\n    elif params.learning_rate_schedule == ""linear_exponential_decay"":\n        schedule = optimizers.LinearExponentialDecay(params.learning_rate,\n            params.warmup_steps, params.start_decay_step,\n            params.end_decay_step,\n            dist.get_world_size())\n    else:\n        raise ValueError(""Unknown schedule %s"" % params.learning_rate_schedule)\n\n    return schedule\n\n\ndef broadcast(model):\n    for var in model.parameters():\n        dist.broadcast(var.data, 0)\n\n\ndef main(args):\n    model_cls = models.get_model(args.model)\n\n    # Import and override parameters\n    # Priorities (low -> high):\n    # default -> saved -> command\n    params = default_params()\n    params = merge_params(params, model_cls.default_params(args.hparam_set))\n    params = import_params(args.output, args.model, params)\n    params = override_params(params, args)\n\n    # Initialize distributed utility\n    if args.distributed:\n        dist.init_process_group(""nccl"")\n        torch.cuda.set_device(args.local_rank)\n    else:\n        dist.init_process_group(""nccl"", init_method=args.url,\n                                rank=args.local_rank,\n                                world_size=len(params.device_list))\n        torch.cuda.set_device(params.device_list[args.local_rank])\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    # Export parameters\n    if dist.get_rank() == 0:\n        export_params(params.output, ""params.json"", params)\n        export_params(params.output, ""%s.json"" % params.model,\n                      collect_params(params, model_cls.default_params()))\n\n    model = model_cls(params).cuda()\n    model.load_embedding(params.embedding)\n\n    if args.half:\n        model = model.half()\n        torch.set_default_dtype(torch.half)\n        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n\n    model.train()\n\n    # Init tensorboard\n    summary.init(params.output, params.save_summary)\n    schedule = get_learning_rate_schedule(params)\n    clipper = get_clipper(params)\n\n    if params.optimizer.lower() == ""adam"":\n        optimizer = optimizers.AdamOptimizer(learning_rate=schedule,\n                                             beta_1=params.adam_beta1,\n                                             beta_2=params.adam_beta2,\n                                             epsilon=params.adam_epsilon,\n                                             clipper=clipper)\n    elif params.optimizer.lower() == ""adadelta"":\n        optimizer = optimizers.AdadeltaOptimizer(\n            learning_rate=schedule, rho=params.adadelta_rho,\n            epsilon=params.adadelta_epsilon, clipper=clipper)\n    else:\n        raise ValueError(""Unknown optimizer %s"" % params.optimizer)\n\n    if args.half:\n        optimizer = optimizers.LossScalingOptimizer(optimizer)\n\n    optimizer = optimizers.MultiStepOptimizer(optimizer, params.update_cycle)\n\n    if dist.get_rank() == 0:\n        print_variables(model)\n\n    dataset = data.get_dataset(params.input, ""train"", params)\n\n    # Load checkpoint\n    checkpoint = utils.latest_checkpoint(params.output)\n\n    if checkpoint is not None:\n        state = torch.load(checkpoint, map_location=""cpu"")\n        step = state[""step""]\n        epoch = state[""epoch""]\n        model.load_state_dict(state[""model""])\n\n        if ""optimizer"" in state:\n            optimizer.load_state_dict(state[""optimizer""])\n    else:\n        step = 0\n        epoch = 0\n        broadcast(model)\n\n    def train_fn(inputs):\n        features, labels = inputs\n        loss = model(features, labels)\n        return loss\n\n    counter = 0\n    should_save = False\n\n    if params.script:\n        thread = ValidationWorker(daemon=True)\n        thread.init(params)\n        thread.start()\n    else:\n        thread = None\n\n    def step_fn(features, step):\n        t = time.time()\n        features = data.lookup(features, ""train"", params)\n        loss = train_fn(features)\n        gradients = optimizer.compute_gradients(loss,\n                                                list(model.parameters()))\n        if params.clip_grad_norm:\n            torch.nn.utils.clip_grad_norm_(model.parameters(),\n                                            params.clip_grad_norm)\n\n        optimizer.apply_gradients(zip(gradients,\n                                      list(model.named_parameters())))\n\n        t = time.time() - t\n\n        summary.scalar(""loss"", loss, step, write_every_n_steps=1)\n        summary.scalar(""global_step/sec"", t, step)\n\n        print(""epoch = %d, step = %d, loss = %.3f (%.3f sec)"" %\n              (epoch + 1, step, float(loss), t))\n\n    try:\n        while True:\n            for features in dataset:\n                if counter % params.update_cycle == 0:\n                    step += 1\n                    utils.set_global_step(step)\n                    should_save = True\n\n                counter += 1\n                step_fn(features, step)\n\n                if step % params.save_checkpoint_steps == 0:\n                    if should_save:\n                        save_checkpoint(step, epoch, model, optimizer, params)\n                        should_save = False\n\n                if step >= params.train_steps:\n                    if should_save:\n                        save_checkpoint(step, epoch, model, optimizer, params)\n\n                    if dist.get_rank() == 0:\n                        summary.close()\n\n                    return\n\n            epoch += 1\n    finally:\n        if thread is not None:\n            thread.stop()\n            thread.join()\n\n\n# Wrap main function\ndef process_fn(rank, args):\n    local_args = copy.copy(args)\n    local_args.local_rank = rank\n    main(local_args)\n\n\nif __name__ == ""__main__"":\n    parsed_args = parse_args()\n\n    if parsed_args.distributed:\n        main(parsed_args)\n    else:\n        # Pick a free port\n        with socket.socket() as s:\n            s.bind((""localhost"", 0))\n            port = s.getsockname()[1]\n            url = ""tcp://localhost:"" + str(port)\n            parsed_args.url = url\n\n        world_size = infer_gpu_num(parsed_args.parameters)\n\n        if world_size > 1:\n            torch.multiprocessing.spawn(process_fn, args=(parsed_args,),\n                                        nprocs=world_size)\n        else:\n            process_fn(0, parsed_args)\n'"
tagger/data/__init__.py,0,"b'from tagger.data.dataset import get_dataset\nfrom tagger.data.vocab import load_vocabulary, lookup\nfrom tagger.data.embedding import load_glove_embedding\n'"
tagger/data/dataset.py,36,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport queue\nimport torch\nimport threading\nimport tensorflow as tf\n\n\n_QUEUE = None\n_THREAD = None\n_LOCK = threading.Lock()\n\n\ndef build_input_fn(filename, mode, params):\n    def train_input_fn():\n        dataset = tf.data.TextLineDataset(filename)\n        dataset = dataset.prefetch(params.buffer_size)\n        dataset = dataset.shuffle(params.buffer_size)\n\n        # Split ""|||""\n        dataset = dataset.map(\n            lambda x: tf.strings.split([x], sep=""|||"", maxsplit=2),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x: (x.values[0], x.values[1]),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: (tf.strings.split([x]).values,\n                          tf.strings.split([y]).values),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: ({\n                ""preds"": tf.strings.to_number(x[0], tf.int32),\n                ""inputs"": tf.strings.lower(x[1:])\n            }, y),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: ({\n                ""preds"": tf.one_hot(x[""preds""], tf.shape(x[""inputs""])[0],\n                                    dtype=tf.int32),\n                ""inputs"": x[""inputs""]\n            }, y),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        def bucket_boundaries(max_length, min_length=8, step=8):\n            x = min_length\n            boundaries = []\n\n            while x <= max_length:\n                boundaries.append(x + 1)\n                x += step\n\n            return boundaries\n\n        batch_size = params.batch_size\n        max_length = (params.max_length // 8) * 8\n        min_length = params.min_length\n        boundaries = bucket_boundaries(max_length)\n        batch_sizes = [max(1, batch_size // (x - 1))\n                       if not params.fixed_batch_size else batch_size\n                       for x in boundaries] + [1]\n\n        def element_length_func(x, y):\n            return tf.shape(x[""inputs""])[0]\n\n        def valid_size(x, y):\n            size = element_length_func(x, y)\n            return tf.logical_and(size >= min_length, size <= max_length)\n\n        transformation_fn = tf.data.experimental.bucket_by_sequence_length(\n            element_length_func,\n            boundaries,\n            batch_sizes,\n            padded_shapes=({\n                ""inputs"": tf.TensorShape([None]),\n                ""preds"": tf.TensorShape([None]),\n                }, tf.TensorShape([None])),\n            padding_values=({\n                ""inputs"": params.pad,\n                ""preds"": 0,\n                }, params.pad),\n            pad_to_bucket_boundary=True)\n\n        dataset = dataset.filter(valid_size)\n        dataset = dataset.apply(transformation_fn)\n\n        return dataset\n\n\n    def infer_input_fn():\n        dataset = tf.data.TextLineDataset(filename)\n\n        # Split ""|||""\n        dataset = dataset.map(\n            lambda x: tf.strings.split([x], sep=""|||"", maxsplit=2),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x: (x.values[0], x.values[1]),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: (tf.strings.split([x]).values,\n                          tf.strings.split([y]).values),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: ({\n                ""preds"": tf.strings.to_number(x[0], tf.int32),\n                ""inputs"": tf.strings.lower(x[1:])\n            }, y),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        dataset = dataset.map(\n            lambda x, y: ({\n                ""preds"": tf.one_hot(x[""preds""], tf.shape(x[""inputs""])[0],\n                                    dtype=tf.int32),\n                ""inputs"": x[""inputs""]\n            }, y),\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n        dataset = dataset.padded_batch(\n            params.decode_batch_size,\n            padded_shapes=({\n                ""inputs"": tf.TensorShape([None]),\n                ""preds"": tf.TensorShape([None]),\n                }, tf.TensorShape([None])),\n            padding_values=({\n                ""inputs"": params.pad,\n                ""preds"": 0,\n                }, params.pad),\n            )\n\n        return dataset\n\n    if mode == ""train"":\n        return train_input_fn\n    else:\n        return infer_input_fn\n\n\nclass DatasetWorker(threading.Thread):\n\n    def init(self, dataset):\n        self._dataset = dataset\n        self._stop = False\n\n    def run(self):\n        global _QUEUE\n        global _LOCK\n\n        while not self._stop:\n            for feature in self._dataset:\n                _QUEUE.put(feature)\n\n    def stop(self):\n        self._stop = True\n\n\nclass Dataset(object):\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        global _QUEUE\n        return _QUEUE.get()\n\n    def stop(self):\n        global _THREAD\n        _THREAD.stop()\n        _THREAD.join()\n\n\ndef get_dataset(filenames, mode, params):\n    global _QUEUE\n    global _THREAD\n\n    input_fn = build_input_fn(filenames, mode, params)\n\n    with tf.device(""/cpu:0""):\n        dataset = input_fn()\n\n    if mode != ""train"":\n        return dataset\n    else:\n        _QUEUE = queue.Queue(100)\n        thread = DatasetWorker(daemon=True)\n        thread.init(dataset)\n        thread.start()\n        _THREAD = thread\n        return Dataset()\n'"
tagger/data/embedding.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef load_glove_embedding(filename, vocab=None):\n    fd = open(filename, ""r"")\n    emb = {}\n    fan_out = 0\n\n    for line in fd:\n        items = line.strip().split()\n        word = items[0].encode(""utf-8"")\n        value = [float(item) for item in items[1:]]\n        fan_out = len(value)\n        emb[word] = np.array(value, ""float32"")\n\n    if not vocab:\n        return emb\n\n    ivoc = {}\n\n    for item in vocab:\n        ivoc[vocab[item]] = item\n\n    new_emb = np.zeros([len(ivoc), fan_out], ""float32"")\n\n    for i in ivoc:\n        word = ivoc[i]\n        if word not in emb:\n            fan_in = len(ivoc)\n            scale = 3.0 / max(1.0, (fan_in + fan_out) / 2.0)\n            new_emb[i] = np.random.uniform(-scale, scale, [fan_out])\n        else:\n            new_emb[i] = emb[word]\n\n    return new_emb\n'"
tagger/data/vocab.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport numpy as np\n\n\ndef _lookup(x, vocab, embedding=None, feature_size=0):\n    x = x.tolist()\n    y = []\n    unk_mask = []\n    embeddings = []\n\n    for _, batch in enumerate(x):\n        ids = []\n        mask = []\n        emb = []\n\n        for _, v in enumerate(batch):\n            if v in vocab:\n                ids.append(vocab[v])\n                mask.append(1.0)\n\n                if embedding is not None:\n                    emb.append(np.zeros([feature_size]))\n            else:\n                ids.append(2)\n\n                if embedding is not None and v in embedding:\n                    mask.append(0.0)\n                    emb.append(embedding[v])\n                else:\n                    mask.append(1.0)\n                    emb.append(np.zeros([feature_size]))\n\n        y.append(ids)\n        unk_mask.append(mask)\n        embeddings.append(emb)\n\n    ids = torch.LongTensor(np.array(y, dtype=""int32"")).cuda()\n    mask = torch.Tensor(np.array(unk_mask, dtype=""float32"")).cuda()\n\n    if embedding is not None:\n        emb = torch.Tensor(np.array(embeddings, dtype=""float32"")).cuda()\n    else:\n        emb = None\n\n    return ids, mask, emb\n\n\ndef load_vocabulary(filename):\n    vocab = []\n    with open(filename, ""rb"") as fd:\n        for line in fd:\n            vocab.append(line.strip())\n\n    word2idx = {}\n    idx2word = {}\n\n    for idx, word in enumerate(vocab):\n        word2idx[word] = idx\n        idx2word[idx] = word\n\n    return vocab, word2idx, idx2word\n\n\ndef lookup(inputs, mode, params, embedding=None):\n    if mode == ""train"":\n        features, labels = inputs\n        preds, seqs = features[""preds""], features[""inputs""]\n        preds = torch.LongTensor(preds.numpy()).cuda()\n        seqs = seqs.numpy()\n        labels = labels.numpy()\n\n        seqs, _, _ = _lookup(seqs, params.lookup[""source""])\n        labels, _, _ = _lookup(labels, params.lookup[""target""])\n\n        features = {\n            ""preds"": preds,\n            ""inputs"": seqs\n        }\n\n        return features, labels\n    else:\n        features, _ = inputs\n        preds, seqs = features[""preds""], features[""inputs""]\n        preds = torch.LongTensor(preds.numpy()).cuda()\n        seqs = seqs.numpy()\n\n        seqs, unk_mask, emb = _lookup(seqs, params.lookup[""source""], embedding,\n                                      params.feature_size)\n\n        features = {\n            ""preds"": preds,\n            ""inputs"": seqs,\n            ""mask"": unk_mask\n        }\n\n        if emb is not None:\n            features[""embedding""] = emb\n\n        return features\n'"
tagger/models/__init__.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tagger.models.deepatt\n\n\ndef get_model(name):\n    name = name.lower()\n\n    if name == ""deepatt"":\n        return tagger.models.deepatt.DeepAtt\n    else:\n        raise LookupError(""Unknown model %s"" % name)\n'"
tagger/models/deepatt.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\nimport torch.nn as nn\n\nimport tagger.utils as utils\nimport tagger.modules as modules\n\nfrom tagger.data import load_glove_embedding\n\n\nclass AttentionSubLayer(modules.Module):\n\n    def __init__(self, params, name=""attention""):\n        super(AttentionSubLayer, self).__init__(name=name)\n\n        with utils.scope(name):\n            self.attention = modules.MultiHeadAttention(\n                params.hidden_size, params.num_heads, params.attention_dropout)\n            self.layer_norm = modules.LayerNorm(params.hidden_size)\n\n        self.dropout = params.residual_dropout\n\n    def forward(self, x, bias):\n        y = self.attention(x, bias)\n        y = nn.functional.dropout(y, self.dropout, self.training)\n\n        return self.layer_norm(x + y)\n\n\nclass FFNSubLayer(modules.Module):\n\n    def __init__(self, params, dtype=None, name=""ffn_layer""):\n        super(FFNSubLayer, self).__init__(name=name)\n\n        with utils.scope(name):\n            self.ffn_layer = modules.FeedForward(params.hidden_size,\n                                                 params.filter_size,\n                                                 dropout=params.relu_dropout)\n            self.layer_norm = modules.LayerNorm(params.hidden_size)\n        self.dropout = params.residual_dropout\n\n    def forward(self, x):\n        y = self.ffn_layer(x)\n        y = nn.functional.dropout(y, self.dropout, self.training)\n\n        return self.layer_norm(x + y)\n\n\nclass DeepAttEncoderLayer(modules.Module):\n\n    def __init__(self, params, name=""layer""):\n        super(DeepAttEncoderLayer, self).__init__(name=name)\n\n        with utils.scope(name):\n            self.self_attention = AttentionSubLayer(params)\n            self.feed_forward = FFNSubLayer(params)\n\n    def forward(self, x, bias):\n        x = self.feed_forward(x)\n        x = self.self_attention(x, bias)\n        return x\n\n\nclass DeepAttEncoder(modules.Module):\n\n    def __init__(self, params, name=""encoder""):\n        super(DeepAttEncoder, self).__init__(name=name)\n\n        with utils.scope(name):\n            self.layers = nn.ModuleList([\n                DeepAttEncoderLayer(params, name=""layer_%d"" % i)\n                for i in range(params.num_hidden_layers)])\n\n    def forward(self, x, bias):\n        for layer in self.layers:\n            x = layer(x, bias)\n        return x\n\n\nclass DeepAtt(modules.Module):\n\n    def __init__(self, params, name=""deepatt""):\n        super(DeepAtt, self).__init__(name=name)\n        self.params = params\n\n        with utils.scope(name):\n            self.build_embedding(params)\n            self.encoding = modules.PositionalEmbedding()\n            self.encoder = DeepAttEncoder(params)\n            self.classifier = modules.Affine(params.hidden_size,\n                                             len(params.vocabulary[""target""]),\n                                             name=""softmax"")\n\n        self.criterion = modules.SmoothedCrossEntropyLoss(\n            params.label_smoothing)\n        self.dropout = params.residual_dropout\n        self.hidden_size = params.hidden_size\n        self.reset_parameters()\n\n    def build_embedding(self, params):\n        vocab_size = len(params.vocabulary[""source""])\n\n        self.embedding = torch.nn.Parameter(\n            torch.empty([vocab_size, params.feature_size]))\n        self.weights = torch.nn.Parameter(\n            torch.empty([2, params.feature_size]))\n        self.bias = torch.nn.Parameter(torch.zeros([params.hidden_size]))\n        self.add_name(self.embedding, ""embedding"")\n        self.add_name(self.weights, ""weights"")\n        self.add_name(self.bias, ""bias"")\n\n    def reset_parameters(self):\n        nn.init.normal_(self.embedding, mean=0.0,\n                        std=self.params.feature_size ** -0.5)\n        nn.init.normal_(self.weights, mean=0.0,\n                        std=self.params.feature_size ** -0.5)\n        nn.init.normal_(self.classifier.weight, mean=0.0,\n                        std=self.params.hidden_size ** -0.5)\n        nn.init.zeros_(self.classifier.bias)\n\n    def encode(self, features):\n        seq = features[""inputs""]\n        pred = features[""preds""]\n        mask = torch.ne(seq, 0).float().cuda()\n        enc_attn_bias = self.masking_bias(mask)\n\n        inputs = torch.nn.functional.embedding(seq, self.embedding)\n\n        if ""embedding"" in features and not self.training:\n            embedding = features[""embedding""]\n            unk_mask = features[""mask""].to(mask)[:, :, None]\n            inputs = inputs * unk_mask + (1.0 - unk_mask) * embedding\n\n        preds = torch.nn.functional.embedding(pred, self.weights)\n        inputs = torch.cat([inputs, preds], axis=-1)\n        inputs = inputs * (self.hidden_size ** 0.5)\n        inputs = inputs + self.bias\n\n        inputs = nn.functional.dropout(self.encoding(inputs), self.dropout,\n                                       self.training)\n\n        enc_attn_bias = enc_attn_bias.to(inputs)\n        encoder_output = self.encoder(inputs, enc_attn_bias)\n        logits = self.classifier(encoder_output)\n\n        return logits\n\n    def argmax_decode(self, features):\n        logits = self.encode(features)\n        return torch.argmax(logits, -1)\n\n    def forward(self, features, labels):\n        mask = torch.ne(features[""inputs""], 0).float().cuda()\n        logits = self.encode(features)\n        loss = self.criterion(logits, labels)\n        mask = mask.to(logits)\n\n        return torch.sum(loss * mask) / torch.sum(mask)\n\n    def load_embedding(self, path):\n        if not path:\n            return\n        emb = load_glove_embedding(path, self.params.lookup[""source""])\n\n        with torch.no_grad():\n            self.embedding.copy_(torch.tensor(emb))\n\n    @staticmethod\n    def masking_bias(mask, inf=-1e9):\n        ret = (1.0 - mask) * inf\n        return torch.unsqueeze(torch.unsqueeze(ret, 1), 1)\n\n    @staticmethod\n    def base_params():\n        params = utils.HParams(\n            pad=""<pad>"",\n            bos=""<eos>"",\n            eos=""<eos>"",\n            unk=""<unk>"",\n            feature_size=100,\n            hidden_size=200,\n            filter_size=800,\n            num_heads=8,\n            num_hidden_layers=10,\n            attention_dropout=0.0,\n            residual_dropout=0.1,\n            relu_dropout=0.0,\n            label_smoothing=0.1,\n            clip_grad_norm=0.0\n        )\n\n        return params\n\n    @staticmethod\n    def default_params(name=None):\n        return DeepAtt.base_params()\n'"
tagger/modules/__init__.py,0,"b'from tagger.modules.attention import MultiHeadAttention\nfrom tagger.modules.embedding import PositionalEmbedding\nfrom tagger.modules.feed_forward import FeedForward\nfrom tagger.modules.layer_norm import LayerNorm\nfrom tagger.modules.losses import SmoothedCrossEntropyLoss\nfrom tagger.modules.module import Module\nfrom tagger.modules.affine import Affine\nfrom tagger.modules.recurrent import LSTMCell, GRUCell, HighwayLSTMCell, DynamicLSTMCell\n'"
tagger/modules/affine.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\nimport torch.nn as nn\n\nimport tagger.utils as utils\nfrom tagger.modules.module import Module\n\n\nclass Affine(Module):\n\n    def __init__(self, in_features, out_features, bias=True, name=""affine""):\n        super(Affine, self).__init__(name=name)\n        self.in_features = in_features\n        self.out_features = out_features\n\n        with utils.scope(name):\n            self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n            self.add_name(self.weight, ""weight"")\n            if bias:\n                self.bias = nn.Parameter(torch.Tensor(out_features))\n                self.add_name(self.bias, ""bias"")\n            else:\n                self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(self.bias, -bound, bound)\n\n    def orthogonal_initialize(self, gain=1.0):\n        nn.init.orthogonal_(self.weight, gain)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input):\n        return nn.functional.linear(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        return \'in_features={}, out_features={}, bias={}\'.format(\n            self.in_features, self.out_features, self.bias is not None\n        )\n'"
tagger/modules/attention.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport tagger.utils as utils\n\nfrom tagger.modules.module import Module\nfrom tagger.modules.affine import Affine\n\n\nclass MultiHeadAttention(Module):\n\n    def __init__(self, hidden_size, num_heads, dropout=0.0,\n                 name=""multihead_attention""):\n        super(MultiHeadAttention, self).__init__(name=name)\n\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.dropout = dropout\n\n        with utils.scope(name):\n            self.qkv_transform = Affine(hidden_size, 3 * hidden_size,\n                                        name=""qkv_transform"")\n            self.o_transform = Affine(hidden_size, hidden_size,\n                                      name=""o_transform"")\n\n        self.reset_parameters()\n\n    def forward(self, query, bias):\n        qkv = self.qkv_transform(query)\n        q, k, v = torch.split(qkv, self.hidden_size, dim=-1)\n\n        # split heads\n        qh = self.split_heads(q, self.num_heads)\n        kh = self.split_heads(k, self.num_heads)\n        vh = self.split_heads(v, self.num_heads)\n\n        # scale query\n        qh = qh * (self.hidden_size // self.num_heads) ** -0.5\n\n        # dot-product attention\n        kh = torch.transpose(kh, -2, -1)\n        logits = torch.matmul(qh, kh)\n\n        if bias is not None:\n            logits = logits + bias\n\n        weights = torch.nn.functional.dropout(torch.softmax(logits, dim=-1),\n                                              p=self.dropout,\n                                              training=self.training)\n\n        x = torch.matmul(weights, vh)\n\n        # combine heads\n        output = self.o_transform(self.combine_heads(x))\n\n        return output\n\n    def reset_parameters(self, initializer=""orthogonal""):\n        if initializer == ""orthogonal"":\n            self.qkv_transform.orthogonal_initialize()\n            self.o_transform.orthogonal_initialize()\n        else:\n            # 6 / (4 * hidden_size) -> 6 / (2 * hidden_size)\n            nn.init.xavier_uniform_(self.qkv_transform.weight)\n            nn.init.xavier_uniform_(self.o_transform.weight)\n            nn.init.constant_(self.qkv_transform.bias, 0.0)\n            nn.init.constant_(self.o_transform.bias, 0.0)\n\n    @staticmethod\n    def split_heads(x, heads):\n        batch = x.shape[0]\n        length = x.shape[1]\n        channels = x.shape[2]\n\n        y = torch.reshape(x, [batch, length, heads, channels // heads])\n        return torch.transpose(y, 2, 1)\n\n    @staticmethod\n    def combine_heads(x):\n        batch = x.shape[0]\n        heads = x.shape[1]\n        length = x.shape[2]\n        channels = x.shape[3]\n\n        y = torch.transpose(x, 2, 1)\n\n        return torch.reshape(y, [batch, length, heads * channels])\n'"
tagger/modules/embedding.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\n\n\nclass PositionalEmbedding(torch.nn.Module):\n\n    def __init__(self):\n        super(PositionalEmbedding, self).__init__()\n\n    def forward(self, inputs):\n        if inputs.dim() != 3:\n            raise ValueError(""The rank of input must be 3."")\n\n        length = inputs.shape[1]\n        channels = inputs.shape[2]\n        half_dim = channels // 2\n\n        positions = torch.arange(length, dtype=inputs.dtype,\n                                 device=inputs.device)\n        dimensions = torch.arange(half_dim, dtype=inputs.dtype,\n                                  device=inputs.device)\n\n        scale = math.log(10000.0) / float(half_dim - 1)\n        dimensions.mul_(-scale).exp_()\n\n        scaled_time = positions.unsqueeze(1) * dimensions.unsqueeze(0)\n        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)],\n                           dim=1)\n\n        if channels % 2 == 1:\n            pad = torch.zeros([signal.shape[0], 1], dtype=inputs.dtype,\n                              device=inputs.device)\n            signal = torch.cat([signal, pad], axis=1)\n\n        return inputs + torch.reshape(signal, [1, -1, channels]).to(inputs)\n'"
tagger/modules/feed_forward.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport tagger.utils as utils\n\nfrom tagger.modules.module import Module\nfrom tagger.modules.affine import Affine\n\n\nclass FeedForward(Module):\n\n    def __init__(self, input_size, hidden_size, output_size=None, dropout=0.0,\n                 name=""feed_forward""):\n        super(FeedForward, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size or input_size\n        self.dropout = dropout\n\n        with utils.scope(name):\n            self.input_transform = Affine(input_size, hidden_size,\n                                          name=""input_transform"")\n            self.output_transform = Affine(hidden_size, self.output_size,\n                                           name=""output_transform"")\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        h = nn.functional.relu(self.input_transform(x))\n        h = nn.functional.dropout(h, self.dropout, self.training)\n        return self.output_transform(h)\n\n    def reset_parameters(self, initializer=""orthogonal""):\n        if initializer == ""orthogonal"":\n            self.input_transform.orthogonal_initialize()\n            self.output_transform.orthogonal_initialize()\n        else:\n            nn.init.xavier_uniform_(self.input_transform.weight)\n            nn.init.xavier_uniform_(self.output_transform.weight)\n            nn.init.constant_(self.input_transform.bias, 0.0)\n            nn.init.constant_(self.output_transform.bias, 0.0)\n'"
tagger/modules/layer_norm.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numbers\nimport torch\nimport torch.nn as nn\nimport tagger.utils as utils\n\nfrom tagger.modules.module import Module\n\n\nclass LayerNorm(Module):\n\n    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True,\n                 name=""layer_norm""):\n        super(LayerNorm, self).__init__(name=name)\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        self.normalized_shape = tuple(normalized_shape)\n        self.eps = eps\n        self.elementwise_affine = elementwise_affine\n\n        with utils.scope(name):\n            if self.elementwise_affine:\n                self.weight = nn.Parameter(torch.Tensor(*normalized_shape))\n                self.bias = nn.Parameter(torch.Tensor(*normalized_shape))\n                self.add_name(self.weight, ""weight"")\n                self.add_name(self.bias, ""bias"")\n            else:\n                self.register_parameter(\'weight\', None)\n                self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.elementwise_affine:\n            nn.init.ones_(self.weight)\n            nn.init.zeros_(self.bias)\n\n    def forward(self, input):\n        return nn.functional.layer_norm(\n            input, self.normalized_shape, self.weight, self.bias, self.eps)\n\n    def extra_repr(self):\n        return \'{normalized_shape}, eps={eps}, \' \\\n            \'elementwise_affine={elementwise_affine}\'.format(**self.__dict__)\n'"
tagger/modules/losses.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\n\n\nclass SmoothedCrossEntropyLoss(torch.nn.Module):\n\n    def __init__(self, smoothing=0.0, normalize=True):\n        super(SmoothedCrossEntropyLoss, self).__init__()\n        self.smoothing = smoothing\n        self.normalize = normalize\n\n    def forward(self, logits, labels):\n        shape = labels.shape\n        logits = torch.reshape(logits, [-1, logits.shape[-1]])\n        labels = torch.reshape(labels, [-1])\n\n        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n        batch_idx = torch.arange(labels.shape[0], device=logits.device)\n        loss = log_probs[batch_idx, labels]\n\n        if not self.smoothing:\n            return -torch.reshape(loss, shape)\n\n        n = logits.shape[-1] - 1.0\n        p = 1.0 - self.smoothing\n        q = self.smoothing / n\n\n        if log_probs.dtype != torch.float16:\n            sum_probs = torch.sum(log_probs, dim=-1)\n            loss = p * loss + q * (sum_probs - loss)\n        else:\n            # Prevent FP16 overflow\n            sum_probs = torch.sum(log_probs.to(torch.float32), dim=-1)\n            loss = loss.to(torch.float32)\n            loss = p * loss + q * (sum_probs - loss)\n            loss = loss.to(torch.float16)\n\n        loss = -torch.reshape(loss, shape)\n\n        if self.normalize:\n            normalizing = -(p * math.log(p) + n * q * math.log(q + 1e-20))\n            return loss - normalizing\n        else:\n            return loss\n'"
tagger/modules/module.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\nimport tagger.utils as utils\n\n\nclass Module(nn.Module):\n\n    def __init__(self, name=""""):\n        super(Module, self).__init__()\n        scope = utils.get_scope()\n        self._name = scope + ""/"" + name if scope else name\n\n    def add_name(self, tensor, name):\n        tensor.tensor_name = utils.unique_name(name)\n\n    @property\n    def name(self):\n        return self._name\n'"
tagger/modules/recurrent.py,0,"b'# coding=utf-8\n# Copyright 2017-2020 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\n\nimport tagger.utils as utils\n\nfrom tagger.modules.module import Module\nfrom tagger.modules.affine import Affine\nfrom tagger.modules.layer_norm import LayerNorm\n\n\nclass GRUCell(Module):\n\n    def __init__(self, input_size, output_size, normalization=False,\n                 name=""gru""):\n        super(GRUCell, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n\n        with utils.scope(name):\n            self.reset_gate = Affine(input_size + output_size, output_size,\n                                     bias=False, name=""reset_gate"")\n            self.update_gate = Affine(input_size + output_size, output_size,\n                                      bias=False, name=""update_gate"")\n            self.transform = Affine(input_size + output_size, output_size,\n                                    name=""transform"")\n\n    def forward(self, x, h):\n        r = torch.sigmoid(self.reset_gate(torch.cat([x, h], -1)))\n        u = torch.sigmoid(self.update_gate(torch.cat([x, h], -1)))\n        c = self.transform(torch.cat([x, r * h], -1))\n\n        new_h = (1.0 - u) * h + u * torch.tanh(h)\n\n        return new_h, new_h\n\n    def init_state(self, batch_size, dtype, device):\n        h = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        return h\n\n    def mask_state(self, h, prev_h, mask):\n        mask = mask[:, None]\n        new_h = mask * h + (1.0 - mask) * prev_h\n        return new_h\n\n    def reset_parameters(self, initializer=""uniform""):\n        if initializer == ""uniform_scaling"":\n            nn.init.xavier_uniform_(self.gates.weight)\n            nn.init.constant_(self.gates.bias, 0.0)\n        elif initializer == ""uniform"":\n            nn.init.uniform_(self.gates.weight, -0.08, 0.08)\n            nn.init.uniform_(self.gates.bias, -0.08, 0.08)\n        else:\n            raise ValueError(""Unknown initializer %d"" % initializer)\n\n\nclass LSTMCell(Module):\n\n    def __init__(self, input_size, output_size, normalization=False,\n                 activation=torch.tanh, name=""lstm""):\n        super(LSTMCell, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.activation = activation\n\n        with utils.scope(name):\n            self.gates = Affine(input_size + output_size, 4 * output_size,\n                                name=""gates"")\n            if normalization:\n                self.layer_norm = LayerNorm([4, output_size])\n            else:\n                self.layer_norm = None\n\n        self.reset_parameters()\n\n    def forward(self, x, state):\n        c, h = state\n\n        gates = self.gates(torch.cat([x, h], 1))\n\n        if self.layer_norm is not None:\n            combined = self.layer_norm(\n                torch.reshape(gates, [-1, 4, self.output_size]))\n        else:\n            combined = torch.reshape(gates, [-1, 4, self.output_size])\n\n        i, j, f, o = torch.unbind(combined, 1)\n        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n\n        new_c = f * c + i * torch.tanh(j)\n\n        if self.activation is None:\n            # Do not use tanh activation\n            new_h = o * new_c\n        else:\n            new_h = o * self.activation(new_c)\n\n        return new_h, (new_c, new_h)\n\n    def init_state(self, batch_size, dtype, device):\n        c = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        h = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        return c, h\n\n    def mask_state(self, state, prev_state, mask):\n        c, h = state\n        prev_c, prev_h = prev_state\n        mask = mask[:, None]\n        new_c = mask * c + (1.0 - mask) * prev_c\n        new_h = mask * h + (1.0 - mask) * prev_h\n        return new_c, new_h\n\n    def reset_parameters(self, initializer=""orthogonal""):\n        if initializer == ""uniform_scaling"":\n            nn.init.xavier_uniform_(self.gates.weight)\n            nn.init.constant_(self.gates.bias, 0.0)\n        elif initializer == ""uniform"":\n            nn.init.uniform_(self.gates.weight, -0.04, 0.04)\n            nn.init.uniform_(self.gates.bias, -0.04, 0.04)\n        elif initializer == ""orthogonal"":\n            self.gates.orthogonal_initialize()\n        else:\n            raise ValueError(""Unknown initializer %d"" % initializer)\n\n\n\nclass HighwayLSTMCell(Module):\n\n    def __init__(self, input_size, output_size, name=""lstm""):\n        super(HighwayLSTMCell, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n\n        with utils.scope(name):\n            self.gates = Affine(input_size + output_size, 5 * output_size,\n                                name=""gates"")\n            self.trans = Affine(input_size, output_size, name=""trans"")\n\n        self.reset_parameters()\n\n    def forward(self, x, state):\n        c, h = state\n\n        gates = self.gates(torch.cat([x, h], 1))\n        combined = torch.reshape(gates, [-1, 5, self.output_size])\n        i, j, f, o, t = torch.unbind(combined, 1)\n        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n        t = torch.sigmoid(t)\n\n        new_c = f * c + i * torch.tanh(j)\n        tmp_h = o * torch.tanh(new_c)\n        new_h = t * tmp_h + (1.0 - t) * self.trans(x)\n\n        return new_h, (new_c, new_h)\n\n    def init_state(self, batch_size, dtype, device):\n        c = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        h = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        return c, h\n\n    def mask_state(self, state, prev_state, mask):\n        c, h = state\n        prev_c, prev_h = prev_state\n        mask = mask[:, None]\n        new_c = mask * c + (1.0 - mask) * prev_c\n        new_h = mask * h + (1.0 - mask) * prev_h\n        return new_c, new_h\n\n    def reset_parameters(self, initializer=""orthogonal""):\n        if initializer == ""uniform_scaling"":\n            nn.init.xavier_uniform_(self.gates.weight)\n            nn.init.constant_(self.gates.bias, 0.0)\n        elif initializer == ""uniform"":\n            nn.init.uniform_(self.gates.weight, -0.04, 0.04)\n            nn.init.uniform_(self.gates.bias, -0.04, 0.04)\n        elif initializer == ""orthogonal"":\n            self.gates.orthogonal_initialize()\n            self.trans.orthogonal_initialize()\n        else:\n            raise ValueError(""Unknown initializer %d"" % initializer)\n\n\nclass DynamicLSTMCell(Module):\n\n    def __init__(self, input_size, output_size, k=2, num_cells=4, name=""lstm""):\n        super(DynamicLSTMCell, self).__init__(name=name)\n\n        self.input_size = input_size\n        self.output_size = output_size\n        self.num_cells = num_cells\n        self.k = k\n\n        with utils.scope(name):\n            self.gates = Affine(input_size + output_size,\n                                4 * output_size * num_cells,\n                                name=""gates"")\n            self.topk_gate = Affine(input_size + output_size,\n                                    num_cells, name=""controller"")\n\n\n        self.reset_parameters()\n\n    @staticmethod\n    def top_k_softmax(logits, k, n):\n        top_logits, top_indices = torch.topk(logits, k=min(k + 1, n))\n\n        top_k_logits = top_logits[:, :k]\n        top_k_indices = top_indices[:, :k]\n\n        probs = torch.softmax(top_k_logits, dim=-1)\n        batch = top_k_logits.shape[0]\n        k = top_k_logits.shape[1]\n\n        # Flat to 1D\n        indices_flat = torch.reshape(top_k_indices, [-1])\n        indices_flat = indices_flat + torch.div(\n            torch.arange(batch * k, device=logits.device), k) * n\n\n        tensor = torch.zeros([batch * n], dtype=logits.dtype,\n                             device=logits.device)\n        tensor = tensor.scatter_add(0, indices_flat.long(),\n                                    torch.reshape(probs, [-1]))\n\n        return torch.reshape(tensor, [batch, n])\n\n    def forward(self, x, state):\n        c, h = state\n        feats = torch.cat([x, h], dim=-1)\n\n        logits = self.topk_gate(feats)\n        # [batch, num_cells]\n        gate = self.top_k_softmax(logits, self.k, self.num_cells)\n\n        # [batch, 4 * num_cells * dim]\n        combined = self.gates(feats)\n        combined = torch.reshape(combined,\n                                 [-1, self.num_cells, 4, self.output_size])\n\n        i, j, f, o = torch.unbind(combined, 2)\n        i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n\n        # [batch, num_cells, dim]\n        new_c = f * c[:, None, :] + i * torch.tanh(j)\n        new_h = o * torch.tanh(new_c)\n\n        gate = gate[:, None, :]\n        new_c = torch.matmul(gate, new_c)\n        new_h = torch.matmul(gate, new_h)\n\n        new_c = torch.squeeze(new_c, 1)\n        new_h = torch.squeeze(new_h, 1)\n\n        return new_h, (new_c, new_h)\n\n    def init_state(self, batch_size, dtype, device):\n        c = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        h = torch.zeros([batch_size, self.output_size], dtype=dtype,\n                        device=device)\n        return c, h\n\n    def mask_state(self, state, prev_state, mask):\n        c, h = state\n        prev_c, prev_h = prev_state\n        mask = mask[:, None]\n        new_c = mask * c + (1.0 - mask) * prev_c\n        new_h = mask * h + (1.0 - mask) * prev_h\n        return new_c, new_h\n\n    def reset_parameters(self, initializer=""orthogonal""):\n        if initializer == ""uniform_scaling"":\n            nn.init.xavier_uniform_(self.gates.weight)\n            nn.init.constant_(self.gates.bias, 0.0)\n        elif initializer == ""uniform"":\n            nn.init.uniform_(self.gates.weight, -0.04, 0.04)\n            nn.init.uniform_(self.gates.bias, -0.04, 0.04)\n        elif initializer == ""orthogonal"":\n            weight = self.gates.weight.view(\n                [self.input_size + self.output_size, self.num_cells,\n                 4 * self.output_size])\n            nn.init.orthogonal_(weight, 1.0)\n            nn.init.constant_(self.gates.bias, 0.0)\n        else:\n            raise ValueError(""Unknown initializer %d"" % initializer)\n'"
tagger/optimizers/__init__.py,0,"b'from tagger.optimizers.optimizers import AdamOptimizer\nfrom tagger.optimizers.optimizers import AdadeltaOptimizer\nfrom tagger.optimizers.optimizers import MultiStepOptimizer\nfrom tagger.optimizers.optimizers import LossScalingOptimizer\nfrom tagger.optimizers.schedules import LinearWarmupRsqrtDecay\nfrom tagger.optimizers.schedules import PiecewiseConstantDecay\nfrom tagger.optimizers.clipping import (\n    adaptive_clipper, global_norm_clipper, value_clipper)\n'"
tagger/optimizers/clipping.py,0,"b'# coding=utf-8\n# Copyright 2017-2020 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\n\ndef global_norm_clipper(value):\n    def clip_fn(gradients, grad_norm):\n        if not float(value) or grad_norm < value:\n            return False, gradients\n\n        scale = value / grad_norm\n\n        gradients = [grad.data.mul_(scale)\n            if grad is not None else None for grad in gradients]\n\n        return False, gradients\n\n    return clip_fn\n\n\ndef value_clipper(clip_min, clip_max):\n    def clip_fn(gradients, grad_norm):\n        gradients = [\n            grad.data.clamp_(clip_min, clip_max)\n            if grad is not None else None for grad in gradients]\n\n        return False, None\n\n    return clip_fn\n\n\ndef adaptive_clipper(rho):\n    norm_avg = 0.0\n    norm_stddev = 0.0\n    log_norm_avg = 0.0\n    log_norm_sqr = 0.0\n\n    def clip_fn(gradients, grad_norm):\n        nonlocal norm_avg\n        nonlocal norm_stddev\n        nonlocal log_norm_avg\n        nonlocal log_norm_sqr\n\n        norm = grad_norm\n        log_norm = math.log(norm)\n\n        avg = rho * norm_avg + (1.0 - rho) * norm\n        log_avg = rho * log_norm_avg + (1.0 - rho) * log_norm\n        log_sqr = rho * log_norm_sqr + (1.0 - rho) * (log_norm ** 2)\n        stddev = (log_sqr - (log_avg ** 2)) ** -0.5\n\n        norm_avg = avg\n        log_norm_avg = log_avg\n        log_norm_sqr = log_sqr\n        norm_stddev = rho * stddev + (1.0 - rho) * stddev\n\n        reject = False\n\n        if norm > norm_avg + 4 * math.exp(norm_stddev):\n            reject = True\n\n        return reject, gradients\n\n    return clip_fn\n'"
tagger/optimizers/optimizers.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport torch\nimport torch.distributed as dist\nimport tagger.utils as utils\nimport tagger.utils.summary as summary\n\nfrom tagger.optimizers.schedules import LearningRateSchedule\n\n\ndef _save_summary(grads_and_vars):\n    total_norm = 0.0\n\n    for grad, var in grads_and_vars:\n        if grad is None:\n            continue\n\n        _, var = var\n        grad_norm = grad.data.norm()\n        total_norm += grad_norm ** 2\n        summary.histogram(var.tensor_name, var,\n                          utils.get_global_step())\n        summary.scalar(""norm/"" + var.tensor_name, var.norm(),\n                       utils.get_global_step())\n        summary.scalar(""grad_norm/"" + var.tensor_name, grad_norm,\n                       utils.get_global_step())\n\n    total_norm = total_norm ** 0.5\n    summary.scalar(""grad_norm"", total_norm, utils.get_global_step())\n\n    return float(total_norm)\n\n\ndef _compute_grad_norm(gradients):\n    total_norm = 0.0\n\n    for grad in gradients:\n        total_norm += float(grad.data.norm() ** 2)\n\n    return float(total_norm ** 0.5)\n\n\nclass Optimizer(object):\n\n    def __init__(self, name, **kwargs):\n        self._name = name\n        self._iterations = 0\n        self._slots = {}\n\n    def detach_gradients(self, gradients):\n        for grad in gradients:\n            if grad is not None:\n                grad.detach_()\n\n    def scale_gradients(self, gradients, scale):\n        for grad in gradients:\n            if grad is not None:\n                grad.mul_(scale)\n\n    def sync_gradients(self, gradients, compress=True):\n        grad_vec = torch.nn.utils.parameters_to_vector(gradients)\n\n        if compress:\n            grad_vec_half = grad_vec.half()\n            dist.all_reduce(grad_vec_half)\n            grad_vec = grad_vec_half.to(grad_vec)\n        else:\n            dist.all_reduce(grad_vec)\n\n        torch.nn.utils.vector_to_parameters(grad_vec, gradients)\n\n    def zero_gradients(self, gradients):\n        for grad in gradients:\n            if grad is not None:\n                grad.zero_()\n\n    def compute_gradients(self, loss, var_list, aggregate=False):\n        var_list = list(var_list)\n        grads = [v.grad if v is not None else None for v in var_list]\n\n        self.detach_gradients(grads)\n\n        if not aggregate:\n            self.zero_gradients(grads)\n\n        loss.backward()\n        return [v.grad if v is not None else None for v in var_list]\n\n    def apply_gradients(self, grads_and_vars):\n        raise NotImplementedError(""Not implemented"")\n\n    @property\n    def iterations(self):\n        return self._iterations\n\n    def state_dict(self):\n        raise NotImplementedError(""Not implemented"")\n\n    def load_state_dict(self):\n        raise NotImplementedError(""Not implemented"")\n\n\nclass SGDOptimizer(Optimizer):\n\n    def __init__(self, learning_rate, summaries=True, name=""SGD"", **kwargs):\n        super(SGDOptimizer, self).__init__(name, **kwargs)\n        self._learning_rate = learning_rate\n        self._summaries = summaries\n        self._clipper = None\n\n        if ""clipper"" in kwargs and kwargs[""clipper""] is not None:\n            self._clipper = kwargs[""clipper""]\n\n    def apply_gradients(self, grads_and_vars):\n        self._iterations += 1\n        lr = self._learning_rate\n        grads, var_list = list(zip(*grads_and_vars))\n\n        if self._summaries:\n            grad_norm = _save_summary(zip(grads, var_list))\n        else:\n            grad_norm = _compute_grad_norm(grads)\n\n        if self._clipper is not None:\n            reject, grads = self._clipper(grads, grad_norm)\n\n            if reject:\n                return\n\n        for grad, var in zip(grads, var_list):\n            if grad is None:\n                continue\n\n            # Convert if grad is not FP32\n            grad = grad.data.float()\n            _, var = var\n            step_size = lr\n\n            if var.dtype == torch.float32:\n                var.data.add_(-step_size, grad)\n            else:\n                fp32_var = var.data.float()\n                fp32_var.add_(-step_size, grad)\n                var.data.copy_(fp32_var)\n\n    def state_dict(self):\n        state = {\n            ""iterations"": self._iterations,\n        }\n\n        if not isinstance(self._learning_rate, LearningRateSchedule):\n            state[""learning_rate""] = self._learning_rate\n\n        return state\n\n    def load_state_dict(self, state):\n        self._learning_rate = state.get(""learning_rate"", self._learning_rate)\n        self._iterations = state.get(""iterations"", self._iterations)\n\n\nclass AdamOptimizer(Optimizer):\n\n    def __init__(self, learning_rate=0.01, beta_1=0.9, beta_2=0.999,\n                 epsilon=1e-7, name=""Adam"", **kwargs):\n        super(AdamOptimizer, self).__init__(name, **kwargs)\n        self._learning_rate = learning_rate\n        self._beta_1 = beta_1\n        self._beta_2 = beta_2\n        self._epsilon = epsilon\n        self._summaries = True\n        self._clipper = None\n\n        if ""summaries"" in kwargs and not kwargs[""summaries""]:\n            self._summaries = False\n\n        if ""clipper"" in kwargs and kwargs[""clipper""] is not None:\n            self._clipper = kwargs[""clipper""]\n\n    def apply_gradients(self, grads_and_vars):\n        self._iterations += 1\n        lr = self._learning_rate\n        beta_1 = self._beta_1\n        beta_2 = self._beta_2\n        epsilon = self._epsilon\n        grads, var_list = list(zip(*grads_and_vars))\n\n        if self._summaries:\n            grad_norm = _save_summary(zip(grads, var_list))\n        else:\n            grad_norm = _compute_grad_norm(grads)\n\n        if self._clipper is not None:\n            reject, grads = self._clipper(grads, grad_norm)\n\n            if reject:\n                return\n\n        for grad, var in zip(grads, var_list):\n            if grad is None:\n                continue\n\n            # Convert if grad is not FP32\n            grad = grad.data.float()\n            name, var = var\n\n            if self._slots.get(name, None) is None:\n                self._slots[name] = {}\n                self._slots[name][""m""] = torch.zeros_like(var.data,\n                                                          dtype=torch.float32)\n                self._slots[name][""v""] = torch.zeros_like(var.data,\n                                                          dtype=torch.float32)\n\n            m, v = self._slots[name][""m""], self._slots[name][""v""]\n\n            bias_corr_1 = 1 - beta_1 ** self._iterations\n            bias_corr_2 = 1 - beta_2 ** self._iterations\n\n            m.mul_(beta_1).add_(1 - beta_1, grad)\n            v.mul_(beta_2).addcmul_(1 - beta_2, grad, grad)\n            denom = (v.sqrt() / math.sqrt(bias_corr_2)).add_(epsilon)\n\n            if isinstance(lr, LearningRateSchedule):\n                lr = lr(self._iterations)\n\n            step_size = lr / bias_corr_1\n\n            if var.dtype == torch.float32:\n                var.data.addcdiv_(-step_size, m, denom)\n            else:\n                fp32_var = var.data.float()\n                fp32_var.addcdiv_(-step_size, m, denom)\n                var.data.copy_(fp32_var)\n\n    def state_dict(self):\n        state = {\n            ""beta_1"": self._beta_1,\n            ""beta_2"": self._beta_2,\n            ""epsilon"": self._epsilon,\n            ""iterations"": self._iterations,\n            ""slot"": self._slots\n        }\n\n        if not isinstance(self._learning_rate, LearningRateSchedule):\n            state[""learning_rate""] = self._learning_rate\n\n        return state\n\n    def load_state_dict(self, state):\n        self._learning_rate = state.get(""learning_rate"", self._learning_rate)\n        self._beta_1 = state.get(""beta_1"", self._beta_1)\n        self._beta_2 = state.get(""beta_2"", self._beta_2)\n        self._epsilon = state.get(""epsilon"", self._epsilon)\n        self._iterations = state.get(""iterations"", self._iterations)\n\n        slots = state.get(""slot"", {})\n        self._slots = {}\n\n        for key in slots:\n            m, v = slots[key][""m""], slots[key][""v""]\n            self._slots[key] = {}\n            self._slots[key][""m""] = torch.zeros(m.shape, dtype=torch.float32)\n            self._slots[key][""v""] = torch.zeros(v.shape, dtype=torch.float32)\n            self._slots[key][""m""].copy_(m)\n            self._slots[key][""v""].copy_(v)\n\n\nclass AdadeltaOptimizer(Optimizer):\n\n    def __init__(self, learning_rate=0.001, rho=0.95, epsilon=1e-07,\n                 name=""Adadelta"", **kwargs):\n        super(AdadeltaOptimizer, self).__init__(name, **kwargs)\n        self._learning_rate = learning_rate\n        self._rho = rho\n        self._epsilon = epsilon\n        self._summaries = True\n\n        if ""summaries"" in kwargs and not kwargs[""summaries""]:\n            self._summaries = False\n\n        if ""clipper"" in kwargs and kwargs[""clipper""] is not None:\n            self._clipper = kwargs[""clipper""]\n\n    def apply_gradients(self, grads_and_vars):\n        self._iterations += 1\n        lr = self._learning_rate\n        rho = self._rho\n        epsilon = self._epsilon\n\n        grads, var_list = list(zip(*grads_and_vars))\n\n        if self._summaries:\n            grad_norm = _save_summary(zip(grads, var_list))\n        else:\n            grad_norm = _compute_grad_norm(grads)\n\n        if self._clipper is not None:\n            reject, grads = self._clipper(grads, grad_norm)\n\n            if reject:\n                return\n\n        for grad, var in zip(grads, var_list):\n            if grad is None:\n                continue\n\n            # Convert if grad is not FP32\n            grad = grad.data.float()\n            name, var = var\n\n            if self._slots.get(name, None) is None:\n                self._slots[name] = {}\n                self._slots[name][""m""] = torch.zeros_like(var.data,\n                                                          dtype=torch.float32)\n                self._slots[name][""v""] = torch.zeros_like(var.data,\n                                                          dtype=torch.float32)\n\n            square_avg = self._slots[name][""m""]\n            acc_delta = self._slots[name][""v""]\n\n            if isinstance(lr, LearningRateSchedule):\n                lr = lr(self._iterations)\n\n            square_avg.mul_(rho).addcmul_(1 - rho, grad, grad)\n            std = square_avg.add(epsilon).sqrt_()\n            delta = acc_delta.add(epsilon).sqrt_().div_(std).mul_(grad)\n            acc_delta.mul_(rho).addcmul_(1 - rho, delta, delta)\n\n            if var.dtype == torch.float32:\n                var.data.add_(-lr, delta)\n            else:\n                fp32_var = var.data.float()\n                fp32_var.add_(-lr, delta)\n                var.data.copy_(fp32_var)\n\n    def state_dict(self):\n        state = {\n            ""rho"": self._rho,\n            ""epsilon"": self._epsilon,\n            ""iterations"": self._iterations,\n            ""slot"": self._slots\n        }\n\n        if not isinstance(self._learning_rate, LearningRateSchedule):\n            state[""learning_rate""] = self._learning_rate\n\n        return state\n\n    def load_state_dict(self, state):\n        self._learning_rate = state.get(""learning_rate"", self._learning_rate)\n        self._rho = state.get(""rho"", self._rho)\n        self._epsilon = state.get(""epsilon"", self._epsilon)\n        self._iterations = state.get(""iterations"", self._iterations)\n\n        slots = state.get(""slot"", {})\n        self._slots = {}\n\n        for key in slots:\n            m, v = slots[key][""m""], slots[key][""v""]\n            self._slots[key] = {}\n            self._slots[key][""m""] = torch.zeros(m.shape, dtype=torch.float32)\n            self._slots[key][""v""] = torch.zeros(v.shape, dtype=torch.float32)\n            self._slots[key][""m""].copy_(m)\n            self._slots[key][""v""].copy_(v)\n\n\nclass LossScalingOptimizer(Optimizer):\n\n    def __init__(self, optimizer, scale=2.0**7, increment_period=2000,\n                 multiplier=2.0, name=""LossScalingOptimizer"", **kwargs):\n        super(LossScalingOptimizer, self).__init__(name, **kwargs)\n        self._optimizer = optimizer\n        self._scale = scale\n        self._increment_period = increment_period\n        self._multiplier = multiplier\n        self._num_good_steps = 0\n        self._summaries = True\n\n        if ""summaries"" in kwargs and not kwargs[""summaries""]:\n            self._summaries = False\n\n    def _update_if_finite_grads(self):\n        if self._num_good_steps + 1 > self._increment_period:\n            self._scale *= self._multiplier\n            self._scale = min(self._scale, 2.0**16)\n            self._num_good_steps = 0\n        else:\n            self._num_good_steps += 1\n\n    def _update_if_not_finite_grads(self):\n        self._scale = max(self._scale / self._multiplier, 1)\n\n    def compute_gradients(self, loss, var_list, aggregate=False):\n        var_list = list(var_list)\n        grads = [v.grad if v is not None else None for v in var_list]\n\n        self.detach_gradients(grads)\n\n        if not aggregate:\n            self.zero_gradients(grads)\n\n        loss = loss * self._scale\n        loss.backward()\n\n        return [v.grad if v is not None else None for v in var_list]\n\n    def apply_gradients(self, grads_and_vars):\n        self._iterations += 1\n        grads, var_list = list(zip(*grads_and_vars))\n        new_grads = []\n\n        if self._summaries:\n            summary.scalar(""optimizer/scale"", self._scale,\n                           utils.get_global_step())\n\n        for grad in grads:\n            if grad is None:\n                new_grads.append(None)\n                continue\n\n            norm = grad.data.norm()\n\n            if not torch.isfinite(norm):\n                self._update_if_not_finite_grads()\n                return\n            else:\n                # Rescale gradients\n                new_grads.append(grad.data.float().mul_(1.0 / self._scale))\n\n        self._update_if_finite_grads()\n        self._optimizer.apply_gradients(zip(new_grads, var_list))\n\n    def state_dict(self):\n        state = {\n            ""scale"": self._scale,\n            ""increment_period"": self._increment_period,\n            ""multiplier"": self._multiplier,\n            ""num_good_steps"": self._num_good_steps,\n            ""optimizer"": self._optimizer.state_dict()\n        }\n        return state\n\n    def load_state_dict(self, state):\n        self._scale = state.get(""scale"", self._scale)\n        self._increment_period = state.get(""increment_period"",\n                                           self._increment_period)\n        self._multiplier = state.get(""multiplier"", self._multiplier)\n        self._num_good_steps = state.get(""num_good_steps"",\n                                         self._num_good_steps)\n        self._optimizer.load_state_dict(state.get(""optimizer"", {}))\n\n\nclass MultiStepOptimizer(Optimizer):\n\n    def __init__(self, optimizer, n=1, compress=True,\n                 name=""MultiStepOptimizer"", **kwargs):\n        super(MultiStepOptimizer, self).__init__(name, **kwargs)\n        self._n = n\n        self._optimizer = optimizer\n        self._compress = compress\n\n    def compute_gradients(self, loss, var_list, aggregate=False):\n        if self._iterations % self._n == 0:\n            return self._optimizer.compute_gradients(loss, var_list, aggregate)\n        else:\n            return self._optimizer.compute_gradients(loss, var_list, True)\n\n    def apply_gradients(self, grads_and_vars):\n        size = dist.get_world_size()\n        grads, var_list = list(zip(*grads_and_vars))\n        self._iterations += 1\n\n        if self._n == 1:\n            if size > 1:\n                self.sync_gradients(grads, compress=self._compress)\n                self.scale_gradients(grads, 1.0 / size)\n\n            self._optimizer.apply_gradients(zip(grads, var_list))\n        else:\n            if self._iterations % self._n != 0:\n                return\n\n            if size > 1:\n                self.sync_gradients(grads, compress=self._compress)\n\n            self.scale_gradients(grads, 1.0 / (self._n * size))\n            self._optimizer.apply_gradients(zip(grads, var_list))\n\n    def state_dict(self):\n        state = {\n            ""n"": self._n,\n            ""iterations"": self._iterations,\n            ""compress"": self._compress,\n            ""optimizer"": self._optimizer.state_dict()\n        }\n        return state\n\n    def load_state_dict(self, state):\n        self._n = state.get(""n"", self._n)\n        self._iterations = state.get(""iterations"", self._iterations)\n        self._compress = state.get(""compress"", self._iterations)\n        self._optimizer.load_state_dict(state.get(""optimizer"", {}))\n'"
tagger/optimizers/schedules.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tagger.utils as utils\nimport tagger.utils.summary as summary\n\n\nclass LearningRateSchedule(object):\n\n    def __call__(self, step):\n        raise NotImplementedError(""Not implemented."")\n\n    def get_config(self):\n      raise NotImplementedError(""Not implemented."")\n\n    @classmethod\n    def from_config(cls, config):\n      return cls(**config)\n\n\n\nclass LinearWarmupRsqrtDecay(LearningRateSchedule):\n\n    def __init__(self, learning_rate, warmup_steps, initial_learning_rate=0.0,\n                 summary=True):\n        super(LinearWarmupRsqrtDecay, self).__init__()\n\n        if not initial_learning_rate:\n            initial_learning_rate = learning_rate / warmup_steps\n\n        self._initial_learning_rate = initial_learning_rate\n        self._maximum_learning_rate = learning_rate\n        self._warmup_steps = warmup_steps\n        self._summary = summary\n\n    def __call__(self, step):\n        if step <= self._warmup_steps:\n            lr_step = self._maximum_learning_rate - self._initial_learning_rate\n            lr_step /= self._warmup_steps\n            lr = self._initial_learning_rate + lr_step * step\n        else:\n            step = step / self._warmup_steps\n            lr = self._maximum_learning_rate * (step ** -0.5)\n\n        if self._summary:\n            summary.scalar(""learning_rate"", lr, utils.get_global_step())\n\n        return lr\n\n    def get_config(self):\n        return {\n            ""learning_rate"": self._maximum_learning_rate,\n            ""initial_learning_rate"": self._initial_learning_rate,\n            ""warmup_steps"": self._warmup_steps\n        }\n\n\nclass PiecewiseConstantDecay(LearningRateSchedule):\n\n    def __init__(self, boundaries, values, summary=True, name=None):\n        super(PiecewiseConstantDecay, self).__init__()\n\n        if len(boundaries) != len(values) - 1:\n            raise ValueError(""The length of boundaries should be 1""\n                             "" less than the length of values"")\n\n        self._boundaries = boundaries\n        self._values = values\n        self._summary = summary\n\n    def __call__(self, step):\n        boundaries = self._boundaries\n        values = self._values\n        learning_rate = values[0]\n\n        if step <= boundaries[0]:\n            learning_rate = values[0]\n        elif step > boundaries[-1]:\n            learning_rate = values[-1]\n        else:\n            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n                                    values[1:-1]):\n\n                if step > low and step <= high:\n                    learning_rate = v\n                    break\n\n        if self._summary:\n            summary.scalar(""learning_rate"", learning_rate,\n                           utils.get_global_step())\n\n        return learning_rate\n\n    def get_config(self):\n        return {\n            ""boundaries"": self._boundaries,\n            ""values"": self._values,\n        }\n\n\nclass LinearExponentialDecay(LearningRateSchedule):\n\n    def __init__(self, learning_rate, warmup_steps, start_decay_step,\n                 end_decay_step, n, summary=True):\n        super(LinearExponentialDecay, self).__init__()\n\n        self._learning_rate = learning_rate\n        self._warmup_steps = warmup_steps\n        self._start_decay_step = start_decay_step\n        self._end_decay_step = end_decay_step\n        self._n = n\n        self._summary = summary\n\n    def __call__(self, step):\n        # See reference: The Best of Both Worlds: Combining Recent Advances\n        # in Neural Machine Translation\n        n = self._n\n        p = self._warmup_steps / n\n        s = n * self._start_decay_step\n        e = n * self._end_decay_step\n\n        learning_rate = self._learning_rate\n\n        learning_rate *= min(\n            1.0 + (n - 1) * step / float(n * p),\n            n,\n            n * ((2 * n) ** (float(s - n * step) / float(e - s))))\n\n        if self._summary:\n            summary.scalar(""learning_rate"", learning_rate,\n                           utils.get_global_step())\n\n        return learning_rate\n\n    def get_config(self):\n        return {\n            ""learning_rate"": self._learning_rate,\n            ""warmup_steps"": self._warmup_steps,\n            ""start_decay_step"": self._start_decay_step,\n            ""end_decay_step"": self._end_decay_step,\n        }\nclass LearningRateSchedule(object):\n\n    def __call__(self, step):\n        raise NotImplementedError(""Not implemented."")\n\n    def get_config(self):\n      raise NotImplementedError(""Not implemented."")\n\n    @classmethod\n    def from_config(cls, config):\n      return cls(**config)\n\n\n\nclass LinearWarmupRsqrtDecay(LearningRateSchedule):\n\n    def __init__(self, learning_rate, warmup_steps, initial_learning_rate=0.0,\n                 summary=True):\n        super(LinearWarmupRsqrtDecay, self).__init__()\n\n        if not initial_learning_rate:\n            initial_learning_rate = learning_rate / warmup_steps\n\n        self._initial_learning_rate = initial_learning_rate\n        self._maximum_learning_rate = learning_rate\n        self._warmup_steps = warmup_steps\n        self._summary = summary\n\n    def __call__(self, step):\n        if step <= self._warmup_steps:\n            lr_step = self._maximum_learning_rate - self._initial_learning_rate\n            lr_step /= self._warmup_steps\n            lr = self._initial_learning_rate + lr_step * step\n        else:\n            step = step / self._warmup_steps\n            lr = self._maximum_learning_rate * (step ** -0.5)\n\n        if self._summary:\n            summary.scalar(""learning_rate"", lr, utils.get_global_step())\n\n        return lr\n\n    def get_config(self):\n        return {\n            ""learning_rate"": self._maximum_learning_rate,\n            ""initial_learning_rate"": self._initial_learning_rate,\n            ""warmup_steps"": self._warmup_steps\n        }\n\n\nclass PiecewiseConstantDecay(LearningRateSchedule):\n\n    def __init__(self, boundaries, values, summary=True, name=None):\n        super(PiecewiseConstantDecay, self).__init__()\n\n        if len(boundaries) != len(values) - 1:\n            raise ValueError(""The length of boundaries should be 1""\n                             "" less than the length of values"")\n\n        self._boundaries = boundaries\n        self._values = values\n        self._summary = summary\n\n    def __call__(self, step):\n        boundaries = self._boundaries\n        values = self._values\n        learning_rate = values[0]\n\n        if step <= boundaries[0]:\n            learning_rate = values[0]\n        elif step > boundaries[-1]:\n            learning_rate = values[-1]\n        else:\n            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n                                    values[1:-1]):\n\n                if step > low and step <= high:\n                    learning_rate = v\n                    break\n\n        if self._summary:\n            summary.scalar(""learning_rate"", learning_rate,\n                           utils.get_global_step())\n\n        return learning_rate\n\n    def get_config(self):\n        return {\n            ""boundaries"": self._boundaries,\n            ""values"": self._values,\n        }\n\n\nclass LinearExponentialDecay(LearningRateSchedule):\n\n    def __init__(self, learning_rate, warmup_steps, start_decay_step,\n                 end_decay_step, n, summary=True):\n        super(LinearExponentialDecay, self).__init__()\n\n        self._learning_rate = learning_rate\n        self._warmup_steps = warmup_steps\n        self._start_decay_step = start_decay_step\n        self._end_decay_step = end_decay_step\n        self._n = n\n        self._summary = summary\n\n    def __call__(self, step):\n        # See reference: The Best of Both Worlds: Combining Recent Advances\n        # in Neural Machine Translation\n        n = self._n\n        p = self._warmup_steps / n\n        s = n * self._start_decay_step\n        e = n * self._end_decay_step\n\n        learning_rate = self._learning_rate\n\n        learning_rate *= min(\n            1.0 + (n - 1) * step / float(n * p),\n            n,\n            n * ((2 * n) ** (float(s - n * step) / float(e - s))))\n\n        if self._summary:\n            summary.scalar(""learning_rate"", learning_rate,\n                           utils.get_global_step())\n\n        return learning_rate\n\n    def get_config(self):\n        return {\n            ""learning_rate"": self._learning_rate,\n            ""warmup_steps"": self._warmup_steps,\n            ""start_decay_step"": self._start_decay_step,\n            ""end_decay_step"": self._end_decay_step,\n        }\n'"
tagger/scripts/build_vocab.py,0,"b'# build_vocab.py\n# author: Playinf\n# email: playinf@stu.xmu.edu.cn\n\nimport argparse\nimport collections\n\n\ndef count_items(filename, lower=False):\n    counter = collections.Counter()\n    label_counter = collections.Counter()\n\n    with open(filename, ""r"") as fd:\n        for line in fd:\n            words, labels = line.strip().split(""|||"")\n            words = words.strip().split()\n            labels = labels.strip().split()\n\n            if lower:\n                words = [item.lower() for item in words[1:]]\n            else:\n                words = words[1:]\n\n            counter.update(words)\n            label_counter.update(labels)\n\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n    words, counts = list(zip(*count_pairs))\n    count_pairs = sorted(label_counter.items(), key=lambda x: (-x[1], x[0]))\n    labels, _ = list(zip(*count_pairs))\n\n    return words, labels, counts\n\n\ndef special_tokens(string):\n    if not string:\n        return []\n    else:\n        return string.strip().split("":"")\n\n\ndef save_vocab(name, vocab):\n    if name.split(""."")[-1] != ""txt"":\n        name = name + "".txt""\n\n    pairs = sorted(vocab.items(), key=lambda x: (x[1], x[0]))\n    words, ids = list(zip(*pairs))\n\n    with open(name, ""w"") as f:\n        for word in words:\n            f.write(word + ""\\n"")\n\n\ndef write_vocab(name, vocab):\n    with open(name, ""w"") as f:\n        for word in vocab:\n            f.write(word + ""\\n"")\n\n\ndef parse_args():\n    msg = ""build vocabulary""\n    parser = argparse.ArgumentParser(description=msg)\n\n    msg = ""input corpus""\n    parser.add_argument(""corpus"", help=msg)\n    msg = ""output vocabulary name""\n    parser.add_argument(""output"", default=""vocab.txt"", help=msg)\n    msg = ""limit""\n    parser.add_argument(""--limit"", default=0, type=int, help=msg)\n    msg = ""add special token, separated by colon""\n    parser.add_argument(""--special"", type=str, default=""<pad>:<unk>"",\n                        help=msg)\n    msg = ""use lowercase""\n    parser.add_argument(""--lower"", action=""store_true"", help=msg)\n\n    return parser.parse_args()\n\n\ndef main(args):\n    vocab = {}\n    limit = args.limit\n    count = 0\n\n    words, labels, counts = count_items(args.corpus, args.lower)\n    special = special_tokens(args.special)\n\n    for token in special:\n        vocab[token] = len(vocab)\n\n    for word, freq in zip(words, counts):\n        if limit and len(vocab) >= limit:\n            break\n\n        if word in vocab:\n            print(""warning: found duplicate token %s, ignored"" % word)\n            continue\n\n        vocab[word] = len(vocab)\n        count += freq\n\n    save_vocab(args.output + ""/vocab.txt"", vocab)\n    write_vocab(args.output + ""/label.txt"", labels)\n\n    print(""total words: %d"" % sum(counts))\n    print(""unique words: %d"" % len(words))\n    print(""vocabulary coverage: %4.2f%%"" % (100.0 * count / sum(counts)))\n\n\nif __name__ == ""__main__"":\n    main(parse_args())\n'"
tagger/scripts/convert_to_conll.py,0,"b'# convert_to_conll.py\n# author: Playinf\n# email: playinf@stu.xmu.edu.cn\n\nimport sys\n\n\ndef convert_bio(labels):\n    n = len(labels)\n    tags = []\n\n    tag = []\n    count = 0\n\n    # B I*\n    for label in labels:\n        count += 1\n\n        if count == n:\n            next_l = None\n        else:\n            next_l = labels[count]\n\n        if label == ""O"":\n            if tag:\n                tags.append(tag)\n                tag = []\n            tags.append([label])\n            continue\n\n        tag.append(label[2:])\n\n        if not next_l or next_l[0] == ""B"":\n            tags.append(tag)\n            tag = []\n\n    new_tag = []\n\n    for tag in tags:\n        if len(tag) == 1:\n            if tag[0] == ""O"":\n                new_tag.append(""*"")\n            else:\n                new_tag.append(""("" + tag[0] + ""*)"")\n            continue\n\n        label = tag[0]\n        n = len(tag)\n\n        for i in range(n):\n            if i == 0:\n                new_tag.append(""("" + label + ""*"")\n            elif i == n - 1:\n                new_tag.append(""*)"")\n            else:\n                new_tag.append(""*"")\n\n    return new_tag\n\n\ndef print_sentence_to_conll(fout, tokens, labels):\n    for label_column in labels:\n        assert len(label_column) == len(tokens)\n    for i in range(len(tokens)):\n        fout.write(tokens[i].ljust(15))\n        for label_column in labels:\n            fout.write(label_column[i].rjust(15))\n        fout.write(""\\n"")\n    fout.write(""\\n"")\n\n\ndef print_to_conll(pred_labels, gold_props_file, output_filename):\n    fout = open(output_filename, \'w\')\n    seq_ptr = 0\n    num_props_for_sentence = 0\n    tokens_buf = []\n\n    for line in open(gold_props_file, \'r\'):\n        line = line.strip()\n        if line == """" and len(tokens_buf) > 0:\n            print_sentence_to_conll(\n                fout,\n                tokens_buf,\n                pred_labels[seq_ptr:seq_ptr+num_props_for_sentence]\n            )\n            seq_ptr += num_props_for_sentence\n            tokens_buf = []\n            num_props_for_sentence = 0\n        else:\n            info = line.split()\n            num_props_for_sentence = len(info) - 1\n            tokens_buf.append(info[0])\n\n    # Output last sentence.\n    if len(tokens_buf) > 0:\n        print_sentence_to_conll(\n            fout,\n            tokens_buf,\n            pred_labels[seq_ptr:seq_ptr+num_props_for_sentence]\n        )\n\n    fout.close()\n\n\nif __name__ == ""__main__"":\n    all_labels = []\n    with open(sys.argv[1]) as fd:\n        for text_line in fd:\n            labs = text_line.strip().split()\n            labs = convert_bio(labs)\n            all_labels.append(labs)\n\n    print_to_conll(all_labels, sys.argv[2], sys.argv[3])\n'"
tagger/utils/__init__.py,0,"b'from tagger.utils.hparams import HParams\nfrom tagger.utils.checkpoint import save, latest_checkpoint, best_checkpoint\nfrom tagger.utils.scope import scope, get_scope, unique_name\nfrom tagger.utils.misc import get_global_step, set_global_step\n'"
tagger/utils/checkpoint.py,0,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport glob\nimport torch\n\n\ndef oldest_checkpoint(path):\n    names = glob.glob(os.path.join(path, ""*.pt""))\n\n    if not names:\n        return None\n\n    oldest_counter = 10000000\n    checkpoint_name = names[0]\n\n    for name in names:\n        counter = name.rstrip("".pt"").split(""-"")[-1]\n\n        if not counter.isdigit():\n            continue\n        else:\n            counter = int(counter)\n\n        if counter < oldest_counter:\n            checkpoint_name = name\n            oldest_counter = counter\n\n    return checkpoint_name\n\n\ndef best_checkpoint(path):\n    if not os.path.exists(os.path.join(path, ""checkpoint"")):\n        return latest_checkpoint(path)\n\n    with open(os.path.join(path, ""checkpoint"")) as fd:\n        line = fd.readline()\n        name = line.strip().split()[-1][1:-1]\n\n        return os.path.join(path, name)\n\n\ndef latest_checkpoint(path):\n    names = glob.glob(os.path.join(path, ""*.pt""))\n\n    if not names:\n        return None\n\n    latest_counter = 0\n    checkpoint_name = names[0]\n\n    for name in names:\n        counter = name.rstrip("".pt"").split(""-"")[-1]\n\n        if not counter.isdigit():\n            continue\n        else:\n            counter = int(counter)\n\n        if counter > latest_counter:\n            checkpoint_name = name\n            latest_counter = counter\n\n    return checkpoint_name\n\n\ndef save(state, path, max_to_keep=None):\n    checkpoints = glob.glob(os.path.join(path, ""*.pt""))\n\n    if max_to_keep and len(checkpoints) >= max_to_keep:\n        checkpoint = oldest_checkpoint(path)\n        os.remove(checkpoint)\n\n    if not checkpoints:\n        counter = 1\n    else:\n        checkpoint = latest_checkpoint(path)\n        counter = int(checkpoint.rstrip("".pt"").split(""-"")[-1]) + 1\n\n    checkpoint = os.path.join(path, ""model-%d.pt"" % counter)\n    print(""Saving checkpoint: %s"" % checkpoint)\n    torch.save(state, checkpoint)\n'"
tagger/utils/hparams.py,1,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n# Modified from TensorFlow (tf.contrib.training.HParams)\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport logging\nimport re\nimport six\n\n\ndef parse_values(values, type_map):\n    ret = {}\n    param_re = re.compile(r""(?P<name>[a-zA-Z][\\w]*)\\s*=\\s*""\n                          r""((?P<val>[^,\\[]*)|\\[(?P<vals>[^\\]]*)\\])($|,)"")\n    pos = 0\n\n    while pos < len(values):\n        m = param_re.match(values, pos)\n\n        if not m:\n            raise ValueError(\n                ""Malformed hyperparameter value: %s"" % values[pos:])\n\n        # Check that there is a comma between parameters and move past it.\n        pos = m.end()\n        # Parse the values.\n        m_dict = m.groupdict()\n        name = m_dict[""name""]\n\n        if name not in type_map:\n            raise ValueError(""Unknown hyperparameter type for %s"" % name)\n\n        def parse_fail():\n            raise ValueError(""Could not parse hparam %s in %s"" % (name, values))\n\n        if type_map[name] == bool:\n            def parse_bool(value):\n                if value == ""true"":\n                    return True\n                elif value == ""false"":\n                    return False\n                else:\n                    try:\n                        return bool(int(value))\n                    except ValueError:\n                        parse_fail()\n            parse = parse_bool\n        else:\n            parse = type_map[name]\n\n\n        if m_dict[""val""] is not None:\n            try:\n                ret[name] = parse(m_dict[""val""])\n            except ValueError:\n                parse_fail()\n        elif m_dict[""vals""] is not None:\n            elements = filter(None, re.split(""[ ,]"", m_dict[""vals""]))\n            try:\n                ret[name] = [parse(e) for e in elements]\n            except ValueError:\n                parse_fail()\n        else:\n            parse_fail()\n\n    return ret\n\n\nclass HParams(object):\n\n    def __init__(self, **kwargs):\n        self._hparam_types = {}\n\n        for name, value in six.iteritems(kwargs):\n            self.add_hparam(name, value)\n\n    def add_hparam(self, name, value):\n        if getattr(self, name, None) is not None:\n            raise ValueError(""Hyperparameter name is reserved: %s"" % name)\n        if isinstance(value, (list, tuple)):\n            if not value:\n                raise ValueError(""Multi-valued hyperparameters cannot be""\n                                 "" empty: %s"" % name)\n            self._hparam_types[name] = (type(value[0]), True)\n        else:\n            self._hparam_types[name] = (type(value), False)\n        setattr(self, name, value)\n\n    def parse(self, values):\n        type_map = dict()\n\n        for name, t in six.iteritems(self._hparam_types):\n            param_type, _ = t\n            type_map[name] = param_type\n\n        values_map = parse_values(values, type_map)\n        return self._set_from_map(values_map)\n\n    def _set_from_map(self, values_map):\n        for name, value in six.iteritems(values_map):\n            if name not in self._hparam_types:\n                logging.debug(""%s not found in hparams."" % name)\n                continue\n\n            _, is_list = self._hparam_types[name]\n\n            if isinstance(value, list):\n                if not is_list:\n                    raise ValueError(""Must not pass a list for single-valued ""\n                                     ""parameter: %s"" % name)\n                setattr(self, name, value)\n            else:\n                if is_list:\n                    raise ValueError(""Must pass a list for multi-valued ""\n                                     ""parameter: %s"" % name)\n                setattr(self, name, value)\n        return self\n\n    def to_json(self):\n        return json.dumps(self.values())\n\n    def parse_json(self, values_json):\n        values_map = json.loads(values_json)\n        return self._set_from_map(values_map)\n\n    def values(self):\n        return {n: getattr(self, n) for n in six.iterkeys(self._hparam_types)}\n\n    def __str__(self):\n        return str(sorted(six.iteritems(self.values)))\n'"
tagger/utils/misc.py,0,b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n_GLOBAL_STEP = 0\n\n\ndef get_global_step():\n    return _GLOBAL_STEP\n\n\ndef set_global_step(step):\n    global _GLOBAL_STEP\n    _GLOBAL_STEP = step\n'
tagger/utils/scope.py,1,"b'# coding=utf-8\n# Copyright 2017-2019 The THUMT Authors\n# Modified from TensorFlow (tf.name_scope)\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport contextlib\n\n# global variable\n_NAME_STACK = """"\n_NAMES_IN_USE = {}\n_VALID_OP_NAME_REGEX = re.compile(""^[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*$"")\n_VALID_SCOPE_NAME_REGEX = re.compile(""^[A-Za-z0-9_.\\\\-/]*$"")\n\n\ndef unique_name(name, mark_as_used=True):\n    global _NAME_STACK\n\n    if _NAME_STACK:\n        name = _NAME_STACK + ""/"" + name\n\n    i = _NAMES_IN_USE.get(name, 0)\n\n    if mark_as_used:\n        _NAMES_IN_USE[name] = i + 1\n\n    if i > 0:\n        base_name = name\n\n        while name in _NAMES_IN_USE:\n            name = ""%s_%d"" % (base_name, i)\n            i += 1\n\n        if mark_as_used:\n            _NAMES_IN_USE[name] = 1\n\n    return name\n\n\n@contextlib.contextmanager\ndef scope(name):\n    global _NAME_STACK\n\n    if name:\n        if _NAME_STACK:\n            # check name\n            if not _VALID_SCOPE_NAME_REGEX.match(name):\n                raise ValueError(""\'%s\' is not a valid scope name"" % name)\n        else:\n            # check name strictly\n            if not _VALID_OP_NAME_REGEX.match(name):\n                raise ValueError(""\'%s\' is not a valid scope name"" % name)\n\n    try:\n        old_stack = _NAME_STACK\n\n        if not name:\n            new_stack = None\n        elif name and name[-1] == ""/"":\n            new_stack = name[:-1]\n        else:\n            new_stack = unique_name(name)\n\n        _NAME_STACK = new_stack\n\n        yield """" if new_stack is None else new_stack + ""/""\n    finally:\n        _NAME_STACK = old_stack\n\n\ndef get_scope():\n    return _NAME_STACK\n'"
tagger/utils/summary.py,0,"b'# coding=utf-8\n# Copyright 2017-2020 The THUMT Authors\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport queue\nimport threading\nimport torch\n\nimport torch.distributed as dist\nimport torch.utils.tensorboard as tensorboard\n\n_SUMMARY_WRITER = None\n_QUEUE = None\n_THREAD = None\n\n\nclass SummaryWorker(threading.Thread):\n\n    def run(self):\n        global _QUEUE\n\n        while True:\n            item = _QUEUE.get()\n            name, kwargs = item\n\n            if name == ""stop"":\n                break\n\n            self.write_summary(name, **kwargs)\n\n    def write_summary(self, name, **kwargs):\n        if name == ""scalar"":\n            _SUMMARY_WRITER.add_scalar(**kwargs)\n        elif name == ""histogram"":\n            _SUMMARY_WRITER.add_histogram(**kwargs)\n\n    def stop(self):\n        global _QUEUE\n        _QUEUE.put((""stop"", None))\n        self.join()\n\n\ndef init(log_dir, enable=True):\n    global _SUMMARY_WRITER\n    global _QUEUE\n    global _THREAD\n\n    if enable and dist.get_rank() == 0:\n        _SUMMARY_WRITER = tensorboard.SummaryWriter(log_dir)\n        _QUEUE = queue.Queue()\n        thread = SummaryWorker(daemon=True)\n        thread.start()\n        _THREAD = thread\n\n\ndef scalar(tag, scalar_value, global_step=None, walltime=None,\n           write_every_n_steps=100):\n\n    if _SUMMARY_WRITER is not None:\n        if global_step % write_every_n_steps == 0:\n            scalar_value = float(scalar_value)\n            kwargs = dict(tag=tag, scalar_value=scalar_value,\n                          global_step=global_step, walltime=walltime)\n            _QUEUE.put((""scalar"", kwargs))\n\n\ndef histogram(tag, values, global_step=None, bins=""tensorflow"", walltime=None,\n              max_bins=None, write_every_n_steps=100):\n\n    if _SUMMARY_WRITER is not None:\n        if global_step % write_every_n_steps == 0:\n            values = values.detach().cpu()\n            kwargs = dict(tag=tag, values=values, global_step=global_step,\n                          bins=bins, walltime=walltime, max_bins=max_bins)\n            _QUEUE.put((""histogram"", kwargs))\n\n\ndef close():\n    if _SUMMARY_WRITER is not None:\n        _THREAD.stop()\n        _SUMMARY_WRITER.close()\n'"
tagger/utils/validation.py,0,"b'# validation.py\n# author: Playinf\n# email: playinf@stu.xmu.edu.cn\n\nimport os\nimport time\nimport threading\nimport subprocess\n\nfrom tagger.utils.checkpoint import latest_checkpoint\n\n\ndef get_current_model(filename):\n    try:\n        with open(filename) as fd:\n            line = fd.readline()\n            if not line:\n                return None\n\n            name = line.strip().split("":"")[1]\n            return name.strip()[1:-1]\n    except:\n        return None\n\n\ndef read_record(filename):\n    record = []\n\n    try:\n        with open(filename) as fd:\n            for line in fd:\n                line = line.strip().split("":"")\n                val = float(line[0])\n                name = line[1].strip()[1:-1]\n                record.append((val, name))\n    except:\n        pass\n\n    return record\n\n\ndef write_record(filename, record):\n    # sort\n    sorted_record = sorted(record, key=lambda x: -x[0])\n\n    with open(filename, ""w"") as fd:\n        for item in sorted_record:\n            val, name = item\n            fd.write(""%f: \\""%s\\""\\n"" % (val, name))\n\n\ndef write_checkpoint(filename, record):\n    # sort\n    sorted_record = sorted(record, key=lambda x: -x[0])\n\n    with open(filename, ""w"") as fd:\n        fd.write(""model_checkpoint_path: \\""%s\\""\\n"" % sorted_record[0][1])\n        for item in sorted_record:\n            val, name = item\n            fd.write(""all_model_checkpoint_paths: \\""%s\\""\\n"" % name)\n\n\ndef add_to_record(record, item, capacity):\n    added = None\n    removed = None\n    models = {}\n\n    for (val, name) in record:\n        models[name] = val\n\n    if len(record) < capacity:\n        if item[1] not in models:\n            added = item[1]\n            record.append(item)\n    else:\n        sorted_record = sorted(record, key=lambda x: -x[0])\n        worst_score = sorted_record[-1][0]\n        current_score = item[0]\n\n        if current_score >= worst_score:\n            if item[1] not in models:\n                added = item[1]\n                removed = sorted_record[-1][1]\n                record = sorted_record[:-1] + [item]\n\n    return added, removed, record\n\n\nclass ValidationWorker(threading.Thread):\n\n    def init(self, params):\n        self._params = params\n        self._stop = False\n\n    def run(self):\n        params = self._params\n        best_dir = params.output + ""/best""\n        last_checkpoint = None\n\n        # create directory\n        if not os.path.exists(best_dir):\n            os.mkdir(best_dir)\n            record = []\n        else:\n            record = read_record(best_dir + ""/top"")\n\n        while not self._stop:\n            try:\n                time.sleep(params.frequency)\n                model_name = latest_checkpoint(params.output)\n\n                if model_name is None:\n                    continue\n\n                if model_name == last_checkpoint:\n                    continue\n\n                last_checkpoint = model_name\n\n                model_name = model_name.split(""/"")[-1]\n                # prediction and evaluation\n                child = subprocess.Popen(""bash %s"" % params.script,\n                                        shell=True, stdout=subprocess.PIPE,\n                                        stderr=subprocess.PIPE)\n                info = child.communicate()[0]\n\n                if not info:\n                    continue\n\n                info = info.strip().split(b""\\n"")\n                overall = None\n\n                for line in info[::-1]:\n                    if line.find(b""Overall"") > 0:\n                        overall = line\n                        break\n\n                if not overall:\n                    continue\n\n                f_score = float(overall.strip().split()[-1])\n\n                # save best model\n                item = (f_score, model_name)\n                added, removed, record = add_to_record(record, item,\n                                                    params.keep_top_k)\n                log_fd = open(best_dir + ""/log"", ""a"")\n                log_fd.write(""%s: %f\\n"" % (model_name, f_score))\n                log_fd.close()\n\n                if added is not None:\n                    model_path = params.output + ""/"" + model_name + ""*""\n                    # copy model\n                    os.system(""cp %s %s"" % (model_path, best_dir))\n                    # update checkpoint\n                    write_record(best_dir + ""/top"", record)\n                    write_checkpoint(best_dir + ""/checkpoint"", record)\n\n                if removed is not None:\n                    # remove old model\n                    model_name = params.output + ""/best/"" + removed + ""*""\n                    os.system(""rm %s"" % model_name)\n            except Exception as e:\n                print(e)\n\n    def stop(self):\n        self._stop = True\n'"
