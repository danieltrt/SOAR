file_path,api_count,code
common.py,1,"b""from enum import Enum\n\nimport tensorflow as tf\nimport cv2\n\n\nregularizer_conv = 0.004\nregularizer_dsconv = 0.0004\nbatchnorm_fused = True\nactivation_fn = tf.nn.relu\n\n\nclass CocoPart(Enum):\n    Nose = 0\n    Neck = 1\n    RShoulder = 2\n    RElbow = 3\n    RWrist = 4\n    LShoulder = 5\n    LElbow = 6\n    LWrist = 7\n    RHip = 8\n    RKnee = 9\n    RAnkle = 10\n    LHip = 11\n    LKnee = 12\n    LAnkle = 13\n    REye = 14\n    LEye = 15\n    REar = 16\n    LEar = 17\n    Background = 18\n\n\nclass MPIIPart(Enum):\n    RAnkle = 0\n    RKnee = 1\n    RHip = 2\n    LHip = 3\n    LKnee = 4\n    LAnkle = 5\n    RWrist = 6\n    RElbow = 7\n    RShoulder = 8\n    LShoulder = 9\n    LElbow = 10\n    LWrist = 11\n    Neck = 12\n    Head = 13\n\n    @staticmethod\n    def from_coco(human):\n        # t = {\n        #     MPIIPart.RAnkle: CocoPart.RAnkle,\n        #     MPIIPart.RKnee: CocoPart.RKnee,\n        #     MPIIPart.RHip: CocoPart.RHip,\n        #     MPIIPart.LHip: CocoPart.LHip,\n        #     MPIIPart.LKnee: CocoPart.LKnee,\n        #     MPIIPart.LAnkle: CocoPart.LAnkle,\n        #     MPIIPart.RWrist: CocoPart.RWrist,\n        #     MPIIPart.RElbow: CocoPart.RElbow,\n        #     MPIIPart.RShoulder: CocoPart.RShoulder,\n        #     MPIIPart.LShoulder: CocoPart.LShoulder,\n        #     MPIIPart.LElbow: CocoPart.LElbow,\n        #     MPIIPart.LWrist: CocoPart.LWrist,\n        #     MPIIPart.Neck: CocoPart.Neck,\n        #     MPIIPart.Nose: CocoPart.Nose,\n        # }\n\n        t = [\n            (MPIIPart.Head, CocoPart.Nose),\n            (MPIIPart.Neck, CocoPart.Neck),\n            (MPIIPart.RShoulder, CocoPart.RShoulder),\n            (MPIIPart.RElbow, CocoPart.RElbow),\n            (MPIIPart.RWrist, CocoPart.RWrist),\n            (MPIIPart.LShoulder, CocoPart.LShoulder),\n            (MPIIPart.LElbow, CocoPart.LElbow),\n            (MPIIPart.LWrist, CocoPart.LWrist),\n            (MPIIPart.RHip, CocoPart.RHip),\n            (MPIIPart.RKnee, CocoPart.RKnee),\n            (MPIIPart.RAnkle, CocoPart.RAnkle),\n            (MPIIPart.LHip, CocoPart.LHip),\n            (MPIIPart.LKnee, CocoPart.LKnee),\n            (MPIIPart.LAnkle, CocoPart.LAnkle),\n        ]\n\n        pose_2d_mpii = []\n        visibilty = []\n        for mpi, coco in t:\n            if coco.value not in human.body_parts.keys():\n                pose_2d_mpii.append((0, 0))\n                visibilty.append(False)\n                continue\n            pose_2d_mpii.append((human.body_parts[coco.value].x, human.body_parts[coco.value].y))\n            visibilty.append(True)\n        return pose_2d_mpii, visibilty\n\nCocoPairs = [\n    (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),\n    (11, 12), (12, 13), (1, 0), (0, 14), (14, 16), (0, 15), (15, 17), (2, 16), (5, 17)\n]   # = 19\nCocoPairsRender = CocoPairs[:-2]\n# CocoPairsNetwork = [\n#     (12, 13), (20, 21), (14, 15), (16, 17), (22, 23), (24, 25), (0, 1), (2, 3), (4, 5),\n#     (6, 7), (8, 9), (10, 11), (28, 29), (30, 31), (34, 35), (32, 33), (36, 37), (18, 19), (26, 27)\n#  ]  # = 19\n\nCocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n\n\ndef read_imgfile(path, width=None, height=None):\n    val_image = cv2.imread(path, cv2.IMREAD_COLOR)\n    if width is not None and height is not None:\n        val_image = cv2.resize(val_image, (width, height))\n    return val_image\n\n\ndef get_sample_images(w, h):\n    val_image = [\n        read_imgfile('../images/p1.jpg', w, h),\n        read_imgfile('../images/p2.jpg', w, h),\n        read_imgfile('../images/p3.jpg', w, h),\n        read_imgfile('../images/golf.jpg', w, h),\n        read_imgfile('../images/hand1.jpg', w, h),\n        read_imgfile('../images/hand2.jpg', w, h),\n        read_imgfile('../images/apink1_crop.jpg', w, h),\n        read_imgfile('../images/ski.jpg', w, h),\n        read_imgfile('../images/apink2.jpg', w, h),\n        read_imgfile('../images/apink3.jpg', w, h),\n        read_imgfile('../images/handsup1.jpg', w, h),\n        read_imgfile('../images/p3_dance.png', w, h),\n    ]\n    return val_image\n"""
cpm.py,79,"b'import tensorflow as tf\n\n\nclass PafNet:\n    def __init__(self, inputs_x, use_bn=False, mask_paf=None, mask_hm=None, gt_hm=None, gt_paf=None, stage_num=6, hm_channel_num=19, paf_channel_num=38):\n        self.inputs_x = inputs_x\n        self.mask_paf = mask_paf\n        self.mask_hm = mask_hm\n        self.gt_hm = gt_hm\n        self.gt_paf = gt_paf\n        self.stage_num = stage_num\n        self.paf_channel_num = paf_channel_num\n        self.hm_channel_num = hm_channel_num\n        self.use_bn = use_bn\n\n    def add_layers(self, inputs):\n        net = self.conv2(inputs=inputs, filters=256, padding=\'SAME\', kernel_size=3, normalization=self.use_bn, name=\'cpm_1\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=3, normalization=self.use_bn, name=\'cpm_2\')\n        # net = tf.layers.conv2d(inputs=inputs,\n        #                              filters=256,\n        #                              padding=""same"",\n        #                              kernel_size=3,\n        #                              activation=""relu"",\n        #                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                              bias_initializer=tf.truncated_normal_initializer(stddev=0.1),)\n        # net = tf.layers.conv2d(inputs=net,\n        #                              filters=128,\n        #                              padding=""same"",\n        #                              kernel_size=3,\n        #                              activation=""relu"",\n        #                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                              bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        return net\n\n    def stage_1(self, inputs, out_channel_num, name):\n        # net = tf.layers.conv2d(inputs=inputs,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=3,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # # net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=3,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # # net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=3,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # # net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=512,\n        #                        padding=""same"",\n        #                        kernel_size=1,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=out_channel_num,\n        #                        padding=""same"",\n        #                        kernel_size=1,\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        net = self.conv2(inputs=inputs, filters=128, padding=\'SAME\', kernel_size=3, normalization=self.use_bn, name=name+\'_conv1\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=3, normalization=self.use_bn, name=name+\'_conv2\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=3, normalization=self.use_bn, name=name+\'_conv3\')\n        net = self.conv2(inputs=net, filters=512, padding=\'SAME\', kernel_size=1, normalization=self.use_bn, name=name+\'_conv4\')\n        net = self.conv2(inputs=net, filters=out_channel_num, padding=\'SAME\', kernel_size=1, act=False, normalization=self.use_bn, name=name+\'_conv5\')\n        return net\n\n    def stage_t(self, inputs, out_channel_num, name):\n        # net = tf.layers.conv2d(inputs=inputs,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=7,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=7,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=7,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=7,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=7,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=128,\n        #                        padding=""same"",\n        #                        kernel_size=1,\n        #                        activation=""relu"",\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        # net = tf.layers.conv2d(inputs=net,\n        #                        filters=out_channel_num,\n        #                        padding=""same"",\n        #                        kernel_size=1,\n        #                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.1),\n        #                        bias_initializer=tf.truncated_normal_initializer(stddev=0.1))\n        net = self.conv2(inputs=inputs, filters=128, padding=\'SAME\', kernel_size=7, normalization=self.use_bn, name=name+\'_conv1\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=7, normalization=self.use_bn, name=name+\'_conv2\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=7, normalization=self.use_bn, name=name+\'_conv3\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=7, normalization=self.use_bn, name=name+\'_conv4\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=7, normalization=self.use_bn, name=name+\'_conv5\')\n        net = self.conv2(inputs=net, filters=128, padding=\'SAME\', kernel_size=1, normalization=self.use_bn, name=name+\'_conv6\')\n        net = self.conv2(inputs=net, filters=out_channel_num, padding=\'SAME\', kernel_size=1, act=False, name=name+\'_conv7\')\n        return net\n\n    def conv2(self, inputs, filters, padding, kernel_size, name, act=True, normalization=False):\n        channels_in = inputs[0, 0, 0, :].get_shape().as_list()[0]\n        with tf.variable_scope(name) as scope:\n            w = tf.get_variable(\'weights\', shape=[kernel_size, kernel_size, channels_in, filters], trainable=True, initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.get_variable(\'biases\', shape=[filters], trainable=True, initializer=tf.contrib.layers.xavier_initializer())\n            conv = tf.nn.conv2d(inputs, w, strides=[1, 1, 1, 1], padding=padding)\n            output = tf.nn.bias_add(conv, b)\n            if normalization:\n                axis = list(range(len(output.get_shape()) - 1))\n                mean, variance = tf.nn.moments(conv, axes=axis)\n                scale = tf.Variable(tf.ones([filters]), name=\'scale\')\n                beta = tf.Variable(tf.zeros([filters]), name=\'beta\')\n                output = tf.nn.batch_normalization(output, mean, variance, offset=beta, scale=scale, variance_epsilon=0.0001)\n            if act:\n                output = tf.nn.relu(output, name=scope.name)\n        tf.summary.histogram(\'conv\', conv)\n        tf.summary.histogram(\'weights\', w)\n        tf.summary.histogram(\'biases\', b)\n        tf.summary.histogram(\'output\', output)\n\n        return output\n\n    def gen_net(self):\n        paf_pre = []\n        hm_pre = []\n        with tf.variable_scope(\'openpose_layers\'):\n            with tf.variable_scope(\'cpm_layers\'):\n                added_layers_out = self.add_layers(inputs=self.inputs_x)\n\n            with tf.variable_scope(\'stage1\'):\n                paf_net = self.stage_1(inputs=added_layers_out, out_channel_num=self.paf_channel_num, name=\'stage1_paf\')\n                hm_net = self.stage_1(inputs=added_layers_out, out_channel_num=self.hm_channel_num, name=\'stage1_hm\')\n                paf_pre.append(paf_net)\n                hm_pre.append(hm_net)\n                net = tf.concat([hm_net, paf_net, added_layers_out], 3)\n\n            with tf.variable_scope(\'staget\'):\n                for i in range(self.stage_num - 1):\n                    hm_net = self.stage_t(inputs=net, out_channel_num=self.hm_channel_num, name=\'stage%d_hm\' % (i + 2))\n                    paf_net = self.stage_t(inputs=net, out_channel_num=self.paf_channel_num, name=\'stage%d_paf\' % (i + 2))\n                    paf_pre.append(paf_net)\n                    hm_pre.append(hm_net)\n                    if i < self.stage_num - 2:\n                        net = tf.concat([hm_net, paf_net, added_layers_out], 3)\n\n        return hm_pre, paf_pre, added_layers_out\n\n    # test code\n    # def gen_net(self):\n    #     paf_loss = []\n    #     hm_loss = []\n    #     paf_pre = []\n    #     hm_pre = []\n    #     with tf.variable_scope(\'add_layers\'):\n    #       added_layers_out = self.add_layers(inputs=self.inputs_x)\n    #\n    #     with tf.variable_scope(\'stage1\'):\n    #         # paf_net = self.stage_1(inputs=self.inputs_x, out_channel_num=self.paf_channel_num)\n    #         hm_net = self.stage_1(inputs=added_layers_out, out_channel_num=self.hm_channel_num)\n    #         # paf_pre.append(paf_net)\n    #         hm_pre.append(hm_net)\n    #         # paf_loss.append(self.get_loss(paf_net, self.gt_paf, mask_type=\'paf\'))\n    #         hm_loss.append(self.get_loss(hm_net, self.gt_hm, mask_type=\'hm\'))\n    #         net = tf.concat([hm_net, self.inputs_x], 3)\n    #\n    #     with tf.variable_scope(\'staget\'):\n    #         for i in range(self.stage_num - 1):\n    #             with tf.name_scope(""staget""):\n    #                 hm_net = self.stage_t(inputs=net, out_channel_num=self.hm_channel_num)\n    #                 paf_net = self.stage_t(inputs=net, out_channel_num=self.paf_channel_num)\n    #                 paf_pre.append(paf_net)\n    #                 hm_pre.append(hm_net)\n    #                 hm_loss.append(self.get_loss(hm_net, self.gt_hm, mask_type=\'hm\'))\n    #                 paf_loss.append(self.get_loss(paf_net, self.gt_paf, mask_type=\'paf\'))\n    #                 if i < self.stage_num - 2:\n    #                     net = tf.concat([hm_net, self.inputs_x], 3)\n    #\n    #     # with tf.name_scope(""loss""):\n    #     #     total_loss = tf.reduce_sum(hm_loss)#  + tf.reduce_sum(paf_loss)\n    #     # tf.summary.scalar(""loss"", total_loss)\n    #     # tf.summary.image(\'hm_gt\', self.gt_hm)\n    #     return hm_pre, paf_pre\n\n    # def get_loss(self, pre_y, gt_y, mask_type):\n    #     if mask_type == \'paf\':\n    #         return tf.reduce_mean(tf.reduce_sum(tf.square(gt_y - pre_y) * self.mask_paf, axis=[1, 2, 3]))\n    #     if mask_type == \'hm\':\n    #         # return tf.reduce_mean(tf.reduce_sum(tf.square(gt_y - pre_y) * self.mask_hm, axis=[1, 2, 3]))\n    #         # return tf.losses.sigmoid_cross_entropy(gt_y, pre_y)\n    #         return tf.reduce_sum(tf.nn.l2_loss(gt_y - pre_y,))\n    #             #(pre_y, gt_y)\n'"
estimator.py,16,"b'import logging\nimport math\n\nimport slidingwindow as sw\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nimport common\nfrom common import CocoPart\nfrom tensblur.smoother import Smoother\n\ntry:\n    from pafprocess import pafprocess\nexcept ModuleNotFoundError as e:\n    print(e)\n    print(\'you need to build c++ library for pafprocess. See : https://github.com/ildoonet/tf-pose-estimation/tree/master/tf_pose/pafprocess\')\n    exit(-1)\n\nlogger = logging.getLogger(\'TfPoseEstimator\')\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n\ndef _round(v):\n    return int(round(v))\n\n\ndef _include_part(part_list, part_idx):\n    for part in part_list:\n        if part_idx == part.part_idx:\n            return True, part\n    return False, None\n\n\nclass Human:\n    """"""\n    body_parts: list of BodyPart\n    """"""\n    __slots__ = (\'body_parts\', \'pairs\', \'uidx_list\', \'score\')\n\n    def __init__(self, pairs):\n        self.pairs = []\n        self.uidx_list = set()\n        self.body_parts = {}\n        for pair in pairs:\n            self.add_pair(pair)\n        self.score = 0.0\n\n    @staticmethod\n    def _get_uidx(part_idx, idx):\n        return \'%d-%d\' % (part_idx, idx)\n\n    def add_pair(self, pair):\n        self.pairs.append(pair)\n        self.body_parts[pair.part_idx1] = BodyPart(Human._get_uidx(pair.part_idx1, pair.idx1),\n                                                   pair.part_idx1,\n                                                   pair.coord1[0], pair.coord1[1], pair.score)\n        self.body_parts[pair.part_idx2] = BodyPart(Human._get_uidx(pair.part_idx2, pair.idx2),\n                                                   pair.part_idx2,\n                                                   pair.coord2[0], pair.coord2[1], pair.score)\n        self.uidx_list.add(Human._get_uidx(pair.part_idx1, pair.idx1))\n        self.uidx_list.add(Human._get_uidx(pair.part_idx2, pair.idx2))\n\n    def is_connected(self, other):\n        return len(self.uidx_list & other.uidx_list) > 0\n\n    def merge(self, other):\n        for pair in other.pairs:\n            self.add_pair(pair)\n\n    def part_count(self):\n        return len(self.body_parts.keys())\n\n    def get_max_score(self):\n        return max([x.score for _, x in self.body_parts.items()])\n\n    def get_face_box(self, img_w, img_h, mode=0):\n        """"""\n        Get Face box compared to img size (w, h)\n        :param img_w:\n        :param img_h:\n        :param mode:\n        :return:\n        """"""\n        # SEE : https://github.com/ildoonet/tf-pose-estimation/blob/master/tf_pose/common.py#L13\n        _NOSE = CocoPart.Nose.value\n        _NECK = CocoPart.Neck.value\n        _REye = CocoPart.REye.value\n        _LEye = CocoPart.LEye.value\n        _REar = CocoPart.REar.value\n        _LEar = CocoPart.LEar.value\n\n        _THRESHOLD_PART_CONFIDENCE = 0.2\n        parts = [part for idx, part in self.body_parts.items() if part.score > _THRESHOLD_PART_CONFIDENCE]\n\n        is_nose, part_nose = _include_part(parts, _NOSE)\n        if not is_nose:\n            return None\n\n        size = 0\n        is_neck, part_neck = _include_part(parts, _NECK)\n        if is_neck:\n            size = max(size, img_h * (part_neck.y - part_nose.y) * 0.8)\n\n        is_reye, part_reye = _include_part(parts, _REye)\n        is_leye, part_leye = _include_part(parts, _LEye)\n        if is_reye and is_leye:\n            size = max(size, img_w * (part_reye.x - part_leye.x) * 2.0)\n            size = max(size,\n                       img_w * math.sqrt((part_reye.x - part_leye.x) ** 2 + (part_reye.y - part_leye.y) ** 2) * 2.0)\n\n        if mode == 1:\n            if not is_reye and not is_leye:\n                return None\n\n        is_rear, part_rear = _include_part(parts, _REar)\n        is_lear, part_lear = _include_part(parts, _LEar)\n        if is_rear and is_lear:\n            size = max(size, img_w * (part_rear.x - part_lear.x) * 1.6)\n\n        if size <= 0:\n            return None\n\n        if not is_reye and is_leye:\n            x = part_nose.x * img_w - (size // 3 * 2)\n        elif is_reye and not is_leye:\n            x = part_nose.x * img_w - (size // 3)\n        else:  # is_reye and is_leye:\n            x = part_nose.x * img_w - size // 2\n\n        x2 = x + size\n        if mode == 0:\n            y = part_nose.y * img_h - size // 3\n        else:\n            y = part_nose.y * img_h - _round(size / 2 * 1.2)\n        y2 = y + size\n\n        # fit into the image frame\n        x = max(0, x)\n        y = max(0, y)\n        x2 = min(img_w - x, x2 - x) + x\n        y2 = min(img_h - y, y2 - y) + y\n\n        if _round(x2 - x) == 0.0 or _round(y2 - y) == 0.0:\n            return None\n        if mode == 0:\n            return {""x"": _round((x + x2) / 2),\n                    ""y"": _round((y + y2) / 2),\n                    ""w"": _round(x2 - x),\n                    ""h"": _round(y2 - y)}\n        else:\n            return {""x"": _round(x),\n                    ""y"": _round(y),\n                    ""w"": _round(x2 - x),\n                    ""h"": _round(y2 - y)}\n\n    def get_upper_body_box(self, img_w, img_h):\n        """"""\n        Get Upper body box compared to img size (w, h)\n        :param img_w:\n        :param img_h:\n        :return:\n        """"""\n\n        if not (img_w > 0 and img_h > 0):\n            raise Exception(""img size should be positive"")\n\n        _NOSE = CocoPart.Nose.value\n        _NECK = CocoPart.Neck.value\n        _RSHOULDER = CocoPart.RShoulder.value\n        _LSHOULDER = CocoPart.LShoulder.value\n        _THRESHOLD_PART_CONFIDENCE = 0.3\n        parts = [part for idx, part in self.body_parts.items() if part.score > _THRESHOLD_PART_CONFIDENCE]\n        part_coords = [(img_w * part.x, img_h * part.y) for part in parts if\n                       part.part_idx in [0, 1, 2, 5, 8, 11, 14, 15, 16, 17]]\n\n        if len(part_coords) < 5:\n            return None\n\n        # Initial Bounding Box\n        x = min([part[0] for part in part_coords])\n        y = min([part[1] for part in part_coords])\n        x2 = max([part[0] for part in part_coords])\n        y2 = max([part[1] for part in part_coords])\n\n        # # ------ Adjust heuristically +\n        # if face points are detcted, adjust y value\n\n        is_nose, part_nose = _include_part(parts, _NOSE)\n        is_neck, part_neck = _include_part(parts, _NECK)\n        torso_height = 0\n        if is_nose and is_neck:\n            y -= (part_neck.y * img_h - y) * 0.8\n            torso_height = max(0, (part_neck.y - part_nose.y) * img_h * 2.5)\n        #\n        # # by using shoulder position, adjust width\n        is_rshoulder, part_rshoulder = _include_part(parts, _RSHOULDER)\n        is_lshoulder, part_lshoulder = _include_part(parts, _LSHOULDER)\n        if is_rshoulder and is_lshoulder:\n            half_w = x2 - x\n            dx = half_w * 0.15\n            x -= dx\n            x2 += dx\n        elif is_neck:\n            if is_lshoulder and not is_rshoulder:\n                half_w = abs(part_lshoulder.x - part_neck.x) * img_w * 1.15\n                x = min(part_neck.x * img_w - half_w, x)\n                x2 = max(part_neck.x * img_w + half_w, x2)\n            elif not is_lshoulder and is_rshoulder:\n                half_w = abs(part_rshoulder.x - part_neck.x) * img_w * 1.15\n                x = min(part_neck.x * img_w - half_w, x)\n                x2 = max(part_neck.x * img_w + half_w, x2)\n\n        # ------ Adjust heuristically -\n\n        # fit into the image frame\n        x = max(0, x)\n        y = max(0, y)\n        x2 = min(img_w - x, x2 - x) + x\n        y2 = min(img_h - y, y2 - y) + y\n\n        if _round(x2 - x) == 0.0 or _round(y2 - y) == 0.0:\n            return None\n        return {""x"": _round((x + x2) / 2),\n                ""y"": _round((y + y2) / 2),\n                ""w"": _round(x2 - x),\n                ""h"": _round(y2 - y)}\n\n    def __str__(self):\n        return \' \'.join([str(x) for x in self.body_parts.values()])\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass BodyPart:\n    """"""\n    part_idx : part index(eg. 0 for nose)\n    x, y: coordinate of body part\n    score : confidence score\n    """"""\n    __slots__ = (\'uidx\', \'part_idx\', \'x\', \'y\', \'score\')\n\n    def __init__(self, uidx, part_idx, x, y, score):\n        self.uidx = uidx\n        self.part_idx = part_idx\n        self.x, self.y = x, y\n        self.score = score\n\n    def get_part_name(self):\n        return CocoPart(self.part_idx)\n\n    def __str__(self):\n        return \'BodyPart:%d-(%.2f, %.2f) score=%.2f\' % (self.part_idx, self.x, self.y, self.score)\n\n    def __repr__(self):\n        return self.__str__()\n\n\nclass PoseEstimator:\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def estimate_paf(peaks, heat_mat, paf_mat):\n        pafprocess.process_paf(peaks, heat_mat, paf_mat)\n\n        humans = []\n        for human_id in range(pafprocess.get_num_humans()):\n            human = Human([])\n            is_added = False\n\n            for part_idx in range(18):\n                c_idx = int(pafprocess.get_part_cid(human_id, part_idx))\n                if c_idx < 0:\n                    continue\n\n                is_added = True\n                human.body_parts[part_idx] = BodyPart(\n                    \'%d-%d\' % (human_id, part_idx), part_idx,\n                    float(pafprocess.get_part_x(c_idx)) / heat_mat.shape[1],\n                    float(pafprocess.get_part_y(c_idx)) / heat_mat.shape[0],\n                    pafprocess.get_part_score(c_idx)\n                )\n\n            if is_added:\n                score = pafprocess.get_score(human_id)\n                human.score = score\n                humans.append(human)\n\n        return humans\n\n\nclass TfPoseEstimator:\n    # TODO : multi-scale\n\n    def __init__(self, graph_path, target_size=(320, 240), tf_config=None):\n        self.target_size = target_size\n\n        # load graph\n        logger.info(\'loading graph from %s(default size=%dx%d)\' % (graph_path, target_size[0], target_size[1]))\n        with tf.gfile.GFile(graph_path, \'rb\') as f:\n            graph_def = tf.GraphDef()\n            graph_def.ParseFromString(f.read())\n\n        self.graph = tf.get_default_graph()\n        tf.import_graph_def(graph_def, name=\'TfPoseEstimator\')\n        self.persistent_sess = tf.Session(graph=self.graph, config=tf_config)\n\n        # for op in self.graph.get_operations():\n        #     print(op.name)\n        # for ts in [n.name for n in tf.get_default_graph().as_graph_def().node]:\n        #     print(ts)\n\n        self.tensor_image = self.graph.get_tensor_by_name(\'TfPoseEstimator/image:0\')\n        self.tensor_output = self.graph.get_tensor_by_name(\'TfPoseEstimator/Openpose/concat_stage7:0\')\n        self.tensor_heatMat = self.tensor_output[:, :, :, :19]\n        self.tensor_pafMat = self.tensor_output[:, :, :, 19:]\n        self.upsample_size = tf.placeholder(dtype=tf.int32, shape=(2,), name=\'upsample_size\')\n        self.tensor_heatMat_up = tf.image.resize_area(self.tensor_output[:, :, :, :19], self.upsample_size,\n                                                      align_corners=False, name=\'upsample_heatmat\')\n        self.tensor_pafMat_up = tf.image.resize_area(self.tensor_output[:, :, :, 19:], self.upsample_size,\n                                                     align_corners=False, name=\'upsample_pafmat\')\n        smoother = Smoother({\'data\': self.tensor_heatMat_up}, 25, 3.0)\n        gaussian_heatMat = smoother.get_output()\n\n        max_pooled_in_tensor = tf.nn.pool(gaussian_heatMat, window_shape=(3, 3), pooling_type=\'MAX\', padding=\'SAME\')\n        self.tensor_peaks = tf.where(tf.equal(gaussian_heatMat, max_pooled_in_tensor), gaussian_heatMat,\n                                     tf.zeros_like(gaussian_heatMat))\n\n        self.heatMat = self.pafMat = None\n\n        # warm-up\n        self.persistent_sess.run(tf.variables_initializer(\n            [v for v in tf.global_variables() if\n             v.name.split(\':\')[0] in [x.decode(\'utf-8\') for x in\n                                      self.persistent_sess.run(tf.report_uninitialized_variables())]\n             ])\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1], target_size[0]]\n            }\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1] // 2, target_size[0] // 2]\n            }\n        )\n        self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up],\n            feed_dict={\n                self.tensor_image: [np.ndarray(shape=(target_size[1], target_size[0], 3), dtype=np.float32)],\n                self.upsample_size: [target_size[1] // 4, target_size[0] // 4]\n            }\n        )\n\n    def __del__(self):\n        # self.persistent_sess.close()\n        pass\n\n    @staticmethod\n    def _quantize_img(npimg):\n        npimg_q = npimg + 1.0\n        npimg_q /= (2.0 / 2 ** 8)\n        # npimg_q += 0.5\n        npimg_q = npimg_q.astype(np.uint8)\n        return npimg_q\n\n    @staticmethod\n    def draw_humans(npimg, humans, imgcopy=False):\n        if imgcopy:\n            npimg = np.copy(npimg)\n        image_h, image_w = npimg.shape[:2]\n        centers = {}\n        for human in humans:\n            # draw point\n            for i in range(common.CocoPart.Background.value):\n                if i not in human.body_parts.keys():\n                    continue\n\n                body_part = human.body_parts[i]\n                center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n                centers[i] = center\n                cv2.circle(npimg, center, 3, common.CocoColors[i], thickness=3, lineType=8, shift=0)\n\n            # draw line\n            for pair_order, pair in enumerate(common.CocoPairsRender):\n                if pair[0] not in human.body_parts.keys() or pair[1] not in human.body_parts.keys():\n                    continue\n\n                # npimg = cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)\n                cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)\n\n        return npimg\n\n    def _get_scaled_img(self, npimg, scale):\n        get_base_scale = lambda s, w, h: max(self.target_size[0] / float(h), self.target_size[1] / float(w)) * s\n        img_h, img_w = npimg.shape[:2]\n\n        if scale is None:\n            if npimg.shape[:2] != (self.target_size[1], self.target_size[0]):\n                # resize\n                npimg = cv2.resize(npimg, self.target_size, interpolation=cv2.INTER_CUBIC)\n            return [npimg], [(0.0, 0.0, 1.0, 1.0)]\n        elif isinstance(scale, float):\n            # scaling with center crop\n            base_scale = get_base_scale(scale, img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n\n            o_size_h, o_size_w = npimg.shape[:2]\n            if npimg.shape[0] < self.target_size[1] or npimg.shape[1] < self.target_size[0]:\n                newimg = np.zeros(\n                    (max(self.target_size[1], npimg.shape[0]), max(self.target_size[0], npimg.shape[1]), 3),\n                    dtype=np.uint8)\n                newimg[:npimg.shape[0], :npimg.shape[1], :] = npimg\n                npimg = newimg\n\n            windows = sw.generate(npimg, sw.DimOrder.HeightWidthChannel, self.target_size[0], self.target_size[1], 0.2)\n\n            rois = []\n            ratios = []\n            for window in windows:\n                indices = window.indices()\n                roi = npimg[indices]\n                rois.append(roi)\n                ratio_x, ratio_y = float(indices[1].start) / o_size_w, float(indices[0].start) / o_size_h\n                ratio_w, ratio_h = float(indices[1].stop - indices[1].start) / o_size_w, float(\n                    indices[0].stop - indices[0].start) / o_size_h\n                ratios.append((ratio_x, ratio_y, ratio_w, ratio_h))\n\n            return rois, ratios\n        elif isinstance(scale, tuple) and len(scale) == 2:\n            # scaling with sliding window : (scale, step)\n            base_scale = get_base_scale(scale[0], img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n            o_size_h, o_size_w = npimg.shape[:2]\n            if npimg.shape[0] < self.target_size[1] or npimg.shape[1] < self.target_size[0]:\n                newimg = np.zeros(\n                    (max(self.target_size[1], npimg.shape[0]), max(self.target_size[0], npimg.shape[1]), 3),\n                    dtype=np.uint8)\n                newimg[:npimg.shape[0], :npimg.shape[1], :] = npimg\n                npimg = newimg\n\n            window_step = scale[1]\n\n            windows = sw.generate(npimg, sw.DimOrder.HeightWidthChannel, self.target_size[0], self.target_size[1],\n                                  window_step)\n\n            rois = []\n            ratios = []\n            for window in windows:\n                indices = window.indices()\n                roi = npimg[indices]\n                rois.append(roi)\n                ratio_x, ratio_y = float(indices[1].start) / o_size_w, float(indices[0].start) / o_size_h\n                ratio_w, ratio_h = float(indices[1].stop - indices[1].start) / o_size_w, float(\n                    indices[0].stop - indices[0].start) / o_size_h\n                ratios.append((ratio_x, ratio_y, ratio_w, ratio_h))\n\n            return rois, ratios\n        elif isinstance(scale, tuple) and len(scale) == 3:\n            # scaling with ROI : (want_x, want_y, scale_ratio)\n            base_scale = get_base_scale(scale[2], img_w, img_h)\n            npimg = cv2.resize(npimg, dsize=None, fx=base_scale, fy=base_scale, interpolation=cv2.INTER_CUBIC)\n            ratio_w = self.target_size[0] / float(npimg.shape[1])\n            ratio_h = self.target_size[1] / float(npimg.shape[0])\n\n            want_x, want_y = scale[:2]\n            ratio_x = want_x - ratio_w / 2.\n            ratio_y = want_y - ratio_h / 2.\n            ratio_x = max(ratio_x, 0.0)\n            ratio_y = max(ratio_y, 0.0)\n            if ratio_x + ratio_w > 1.0:\n                ratio_x = 1. - ratio_w\n            if ratio_y + ratio_h > 1.0:\n                ratio_y = 1. - ratio_h\n\n            roi = self._crop_roi(npimg, ratio_x, ratio_y)\n            return [roi], [(ratio_x, ratio_y, ratio_w, ratio_h)]\n\n    def _crop_roi(self, npimg, ratio_x, ratio_y):\n        target_w, target_h = self.target_size\n        h, w = npimg.shape[:2]\n        x = max(int(w * ratio_x - .5), 0)\n        y = max(int(h * ratio_y - .5), 0)\n        cropped = npimg[y:y + target_h, x:x + target_w]\n\n        cropped_h, cropped_w = cropped.shape[:2]\n        if cropped_w < target_w or cropped_h < target_h:\n            npblank = np.zeros((self.target_size[1], self.target_size[0], 3), dtype=np.uint8)\n\n            copy_x, copy_y = (target_w - cropped_w) // 2, (target_h - cropped_h) // 2\n            npblank[copy_y:copy_y + cropped_h, copy_x:copy_x + cropped_w] = cropped\n        else:\n            return cropped\n\n    def inference(self, npimg, resize_to_default=True, upsample_size=1.0):\n        if npimg is None:\n            raise Exception(\'The image is not valid. Please check your image exists.\')\n\n        if resize_to_default:\n            upsample_size = [int(self.target_size[1] / 8 * upsample_size), int(self.target_size[0] / 8 * upsample_size)]\n        else:\n            upsample_size = [int(npimg.shape[0] / 8 * upsample_size), int(npimg.shape[1] / 8 * upsample_size)]\n\n        if self.tensor_image.dtype == tf.quint8:\n            # quantize input image\n            npimg = TfPoseEstimator._quantize_img(npimg)\n            pass\n\n        logger.debug(\'inference+ original shape=%dx%d\' % (npimg.shape[1], npimg.shape[0]))\n        img = npimg\n        if resize_to_default:\n            img = self._get_scaled_img(npimg, None)[0][0]\n        peaks, heatMat_up, pafMat_up = self.persistent_sess.run(\n            [self.tensor_peaks, self.tensor_heatMat_up, self.tensor_pafMat_up], feed_dict={\n                self.tensor_image: [img], self.upsample_size: upsample_size\n            })\n        peaks = peaks[0]\n        self.heatMat = heatMat_up[0]\n        self.pafMat = pafMat_up[0]\n        logger.debug(\'inference- heatMat=%dx%d pafMat=%dx%d\' % (\n            self.heatMat.shape[1], self.heatMat.shape[0], self.pafMat.shape[1], self.pafMat.shape[0]))\n\n        t = time.time()\n        humans = PoseEstimator.estimate_paf(peaks, self.heatMat, self.pafMat)\n        logger.debug(\'estimate time=%.5f\' % (time.time() - t))\n        return humans\n\n\nif __name__ == \'__main__\':\n    import pickle\n\n    f = open(\'./etcs/heatpaf1.pkl\', \'rb\')\n    data = pickle.load(f)\n    logger.info(\'size={}\'.format(data[\'heatMat\'].shape))\n    f.close()\n\n    t = time.time()\n    humans = PoseEstimator.estimate_paf(data[\'peaks\'], data[\'heatMat\'], data[\'pafMat\'])\n    dt = time.time() - t\n    t = time.time()\n    logger.info(\'elapsed #humans=%d time=%.8f\' % (len(humans), dt))\n'"
network.py,24,"b""import tensorflow as tf\n\n\ndef conv2(inputs, filters, kernel_size, name, padding='SAME', act=True, normalization=True):\n    channels_in = inputs[0, 0, 0, :].get_shape().as_list()[0]\n    with tf.variable_scope(name) as scope:\n        # w = tf.Variable(tf.truncated_normal(shape=[kernel_size, kernel_size, channels_in, filters], stddev=0.1))\n        w = tf.get_variable('weights', shape=[kernel_size, kernel_size, channels_in, filters], trainable=True,\n                            initializer=tf.contrib.layers.xavier_initializer())\n        # b = tf.Variable(tf.truncated_normal(shape=[filters], stddev=0.1))\n        b = tf.get_variable('biases', shape=[filters], trainable=True,\n                            initializer=tf.contrib.layers.xavier_initializer())\n\n        conv = tf.nn.conv2d(inputs, w, strides=[1, 1, 1, 1], padding=padding)\n        output = tf.nn.bias_add(conv, b)\n        # if normalization:\n        #     axis = list(range(len(output.get_shape()) - 1))\n        #     mean, variance = tf.nn.moments(conv, axes=axis)\n        #     output = tf.nn.batch_normalization(output, mean, variance, offset=None, scale=None, variance_epsilon=0.0001)\n        if act:\n            output = tf.nn.relu(output, name=scope.name)\n    tf.summary.histogram('conv', conv)\n    tf.summary.histogram('weights', w)\n    tf.summary.histogram('biases', b)\n    tf.summary.histogram('output', output)\n    return output\n\ndef gen_network(input):\n    cpm_out = []\n    hm_out = []\n    with tf.variable_scope('train_layers'):\n        add_layer = conv2(inputs=input, filters=256, kernel_size=3, name='add_layer_1')\n        add_layer = conv2(inputs=add_layer, filters=128, kernel_size=3, name='add_layer_2')\n\n        stage1_l1 = conv2(inputs=add_layer, filters=128, kernel_size=3, name='stage1_conv1_l1')\n        stage1_l1 = conv2(inputs=stage1_l1, filters=128, kernel_size=3, name='stage1_conv2_l1')\n        stage1_l1 = conv2(inputs=stage1_l1, filters=128, kernel_size=3, name='stage1_conv3_l1')\n        stage1_l1 = conv2(inputs=stage1_l1, filters=512, kernel_size=1, name='stage1_conv4_l1')\n        stage1_l1 = conv2(inputs=stage1_l1, filters=38, kernel_size=1, act=False, name='stage1_conv5_l1')\n\n        stage1_l2 = conv2(inputs=add_layer, filters=128, kernel_size=3, name='stage1_conv1_l2')\n        stage1_l2 = conv2(inputs=stage1_l2, filters=128, kernel_size=3, name='stage1_conv2_l2')\n        stage1_l2 = conv2(inputs=stage1_l2, filters=128, kernel_size=3, name='stage1_conv3_l2')\n        stage1_l2 = conv2(inputs=stage1_l2, filters=512, kernel_size=1, name='stage1_conv4_l2')\n        stage1_l2 = conv2(inputs=stage1_l2, filters=19, kernel_size=1, act=False, name='stage1_conv5_l2')\n\n        concat_stage_1 = tf.concat([stage1_l1, stage1_l2, add_layer], axis=3, name='concat_stage_1')\n\n        stage2_l1 = conv2(inputs=concat_stage_1, filters=128, kernel_size=7, name='stage2_conv1_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=128, kernel_size=7, name='stage2_conv2_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=128, kernel_size=7, name='stage2_conv3_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=128, kernel_size=7, name='stage2_conv4_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=128, kernel_size=7, name='stage2_conv5_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=128, kernel_size=1, name='stage2_conv6_l1')\n        stage2_l1 = conv2(inputs=stage2_l1, filters=38, kernel_size=1, act=False, name='stage2_conv7_l1')\n\n        stage2_l2 = conv2(inputs=concat_stage_1, filters=128, kernel_size=7, name='stage2_conv1_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=128, kernel_size=7, name='stage2_conv2_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=128, kernel_size=7, name='stage2_conv3_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=128, kernel_size=7, name='stage2_conv4_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=128, kernel_size=7, name='stage2_conv5_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=128, kernel_size=1, name='stage2_conv6_l2')\n        stage2_l2 = conv2(inputs=stage2_l2, filters=19, kernel_size=1, act=False, name='stage2_conv7_l2')\n        cpm_out.append(stage2_l1)\n        hm_out.append(stage2_l2)\n        concat_stage_2 = tf.concat([stage2_l1, stage2_l2, add_layer], axis=3, name='concat_stage_2')\n\n        stage3_l1 = conv2(inputs=concat_stage_2, filters=128, kernel_size=7, name='stage3_conv1_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=128, kernel_size=7, name='stage3_conv2_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=128, kernel_size=7, name='stage3_conv3_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=128, kernel_size=7, name='stage3_conv4_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=128, kernel_size=7, name='stage3_conv5_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=128, kernel_size=1, name='stage3_conv6_l1')\n        stage3_l1 = conv2(inputs=stage3_l1, filters=38, kernel_size=1, act=False, name='stage3_conv7_l1')\n\n        stage3_l2 = conv2(inputs=concat_stage_2, filters=128, kernel_size=7, name='stage3_conv1_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=128, kernel_size=7, name='stage3_conv2_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=128, kernel_size=7, name='stage3_conv3_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=128, kernel_size=7, name='stage3_conv4_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=128, kernel_size=7, name='stage3_conv5_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=128, kernel_size=1, name='stage3_conv6_l2')\n        stage3_l2 = conv2(inputs=stage3_l2, filters=19, kernel_size=1, act=False, name='stage3_conv7_l2')\n        cpm_out.append(stage3_l1)\n        hm_out.append(stage3_l2)\n        concat_stage_3 = tf.concat([stage3_l1, stage3_l2, add_layer], axis=3, name='concat_stage_3')\n\n        stage4_l1 = conv2(inputs=concat_stage_3, filters=128, kernel_size=7, name='stage4_conv1_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=128, kernel_size=7, name='stage4_conv2_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=128, kernel_size=7, name='stage4_conv3_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=128, kernel_size=7, name='stage4_conv4_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=128, kernel_size=7, name='stage4_conv5_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=128, kernel_size=1, name='stage4_conv6_l1')\n        stage4_l1 = conv2(inputs=stage4_l1, filters=38, kernel_size=1, act=False, name='stage4_conv7_l1')\n\n        stage4_l2 = conv2(inputs=concat_stage_3, filters=128, kernel_size=7, name='stage4_conv1_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=128, kernel_size=7, name='stage4_conv2_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=128, kernel_size=7, name='stage4_conv3_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=128, kernel_size=7, name='stage4_conv4_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=128, kernel_size=7, name='stage4_conv5_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=128, kernel_size=1, name='stage4_conv6_l2')\n        stage4_l2 = conv2(inputs=stage4_l2, filters=19, kernel_size=1, act=False, name='stage4_conv7_l2')\n        cpm_out.append(stage4_l1)\n        hm_out.append(stage4_l2)\n        concat_stage_4 = tf.concat([stage4_l1, stage4_l2, add_layer], axis=3, name='concat_stage_4')\n\n        stage5_l1 = conv2(inputs=concat_stage_4, filters=128, kernel_size=7, name='stage5_conv1_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=128, kernel_size=7, name='stage5_conv2_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=128, kernel_size=7, name='stage5_conv3_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=128, kernel_size=7, name='stage5_conv4_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=128, kernel_size=7, name='stage5_conv5_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=128, kernel_size=1, name='stage5_conv6_l1')\n        stage5_l1 = conv2(inputs=stage5_l1, filters=38, kernel_size=1, act=False, name='stage5_conv7_l1')\n\n        stage5_l2 = conv2(inputs=concat_stage_4, filters=128, kernel_size=7, name='stage5_conv1_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=128, kernel_size=7, name='stage5_conv2_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=128, kernel_size=7, name='stage5_conv3_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=128, kernel_size=7, name='stage5_conv4_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=128, kernel_size=7, name='stage5_conv5_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=128, kernel_size=1, name='stage5_conv6_l2')\n        stage5_l2 = conv2(inputs=stage5_l2, filters=19, kernel_size=1, act=False, name='stage5_conv7_l2')\n        cpm_out.append(stage5_l1)\n        hm_out.append(stage5_l2)\n        concat_stage_5 = tf.concat([stage5_l1, stage5_l2, add_layer], axis=3, name='concat_stage_5')\n\n        stage6_l1 = conv2(inputs=concat_stage_5, filters=128, kernel_size=7, name='stage6_conv1_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=128, kernel_size=7, name='stage6_conv2_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=128, kernel_size=7, name='stage6_conv3_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=128, kernel_size=7, name='stage6_conv4_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=128, kernel_size=7, name='stage6_conv5_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=128, kernel_size=1, name='stage6_conv6_l1')\n        stage6_l1 = conv2(inputs=stage6_l1, filters=38, kernel_size=1, act=False, name='stage6_conv7_l1')\n\n        stage6_l2 = conv2(inputs=concat_stage_5, filters=128, kernel_size=7, name='stage6_conv1_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=128, kernel_size=7, name='stage6_conv2_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=128, kernel_size=7, name='stage6_conv3_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=128, kernel_size=7, name='stage6_conv4_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=128, kernel_size=7, name='stage6_conv5_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=128, kernel_size=1, name='stage6_conv6_l2')\n        stage6_l2 = conv2(inputs=stage6_l2, filters=19, kernel_size=1, act=False, name='stage6_conv7_l2')\n        cpm_out.append(stage6_l1)\n        hm_out.append(stage6_l2)\n        concat_stage_6 = tf.concat([stage6_l1, stage6_l2, add_layer], axis=3, name='concat_stage_6')\n\n        stage7_l1 = conv2(inputs=concat_stage_6, filters=128, kernel_size=7, name='stage7_conv1_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=128, kernel_size=7, name='stage7_conv2_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=128, kernel_size=7, name='stage7_conv3_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=128, kernel_size=7, name='stage7_conv4_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=128, kernel_size=7, name='stage7_conv5_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=128, kernel_size=1, name='stage7_conv6_l1')\n        stage7_l1 = conv2(inputs=stage7_l1, filters=38, kernel_size=1, act=False, name='stage7_conv7_l1')\n\n        stage7_l2 = conv2(inputs=concat_stage_6, filters=128, kernel_size=7, name='stage7_conv1_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=128, kernel_size=7, name='stage7_conv2_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=128, kernel_size=7, name='stage7_conv3_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=128, kernel_size=7, name='stage7_conv4_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=128, kernel_size=7, name='stage7_conv5_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=128, kernel_size=1, name='stage7_conv6_l2')\n        stage7_l2 = conv2(inputs=stage7_l2, filters=19, kernel_size=1, act=False, name='stage7_conv7_l2')\n        cpm_out.append(stage7_l1)\n        hm_out.append(stage7_l2)\n        openpose = tf.concat([stage7_l1, stage7_l2], axis=3, name='concat_stage_7')\n\n    return hm_out, cpm_out, add_layer\n"""
pose_augment.py,0,"b'import math\nimport random\n\nimport cv2\nimport numpy as np\nfrom tensorpack.dataflow.imgaug.geometry import RotationAndCropValid\n\nfrom common import CocoPart\n\n_network_w = 368\n_network_h = 368\n_scale = 2\n\n\ndef set_network_input_wh(w, h):\n    global _network_w, _network_h\n    _network_w, _network_h = w, h\n\n\ndef set_network_scale(scale):\n    global _scale\n    _scale = scale\n\n\ndef pose_random_scale(meta):\n    scalew = random.uniform(0.8, 1.2)\n    scaleh = random.uniform(0.8, 1.2)\n    neww = int(meta.width * scalew)\n    newh = int(meta.height * scaleh)\n    dst = cv2.resize(meta.img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0 or int(point[0] * scalew + 0.5) > neww or int(\n            #                         point[1] * scaleh + 0.5) > newh:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = dst\n    return meta\n\n\ndef pose_resize_shortestedge_fixed(meta):\n    ratio_w = _network_w / meta.width\n    ratio_h = _network_h / meta.height\n    ratio = max(ratio_w, ratio_h)\n    return pose_resize_shortestedge(meta, int(min(meta.width * ratio + 0.5, meta.height * ratio + 0.5)))\n\n\ndef pose_resize_shortestedge_random(meta):\n    ratio_w = _network_w / meta.width\n    ratio_h = _network_h / meta.height\n    ratio = min(ratio_w, ratio_h)\n    target_size = int(min(meta.width * ratio + 0.5, meta.height * ratio + 0.5))\n    target_size = int(target_size * random.uniform(0.95, 1.6))\n    # target_size = int(min(_network_w, _network_h) * random.uniform(0.7, 1.5))\n    return pose_resize_shortestedge(meta, target_size)\n\n\ndef pose_resize_shortestedge(meta, target_size):\n    global _network_w, _network_h\n    img = meta.img\n\n    # adjust image\n    scale = target_size / min(meta.height, meta.width)\n    if meta.height < meta.width:\n        newh, neww = target_size, int(scale * meta.width + 0.5)\n    else:\n        newh, neww = int(scale * meta.height + 0.5), target_size\n\n    dst = cv2.resize(img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    pw = ph = 0\n    if neww < _network_w or newh < _network_h:\n        pw = max(0, (_network_w - neww) // 2)\n        ph = max(0, (_network_h - newh) // 2)\n        mw = (_network_w - neww) % 2\n        mh = (_network_h - newh) % 2\n        color = random.randint(0, 255)\n        dst = cv2.copyMakeBorder(dst, ph, ph+mh, pw, pw+mw, cv2.BORDER_CONSTANT, value=(color, 0, 0))\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0 or int(point[0]*scale+0.5) > neww or int(point[1]*scale+0.5) > newh:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((int(point[0]*scale+0.5) + pw, int(point[1]*scale+0.5) + ph))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww + pw * 2, newh + ph * 2\n    meta.img = dst\n    return meta\n\n\ndef pose_crop_center(meta):\n    global _network_w, _network_h\n    target_size = (_network_w, _network_h)\n    x = (meta.width - target_size[0]) // 2 if meta.width > target_size[0] else 0\n    y = (meta.height - target_size[1]) // 2 if meta.height > target_size[1] else 0\n\n    return pose_crop(meta, x, y, target_size[0], target_size[1])\n\n\ndef pose_crop_random(meta):\n    global _network_w, _network_h\n    target_size = (_network_w, _network_h)\n\n    for _ in range(50):\n        x = random.randrange(0, meta.width - target_size[0]) if meta.width > target_size[0] else 0\n        y = random.randrange(0, meta.height - target_size[1]) if meta.height > target_size[1] else 0\n\n        # check whether any face is inside the box to generate a reasonably-balanced datasets\n        for joint in meta.joint_list:\n            if x <= joint[CocoPart.Nose.value][0] < x + target_size[0] and y <= joint[CocoPart.Nose.value][1] < y + target_size[1]:\n                break\n\n    return pose_crop(meta, x, y, target_size[0], target_size[1])\n\n\ndef pose_crop(meta, x, y, w, h):\n    # adjust image\n    target_size = (w, h)\n\n    img = meta.img\n    resized = img[y:y+target_size[1], x:x+target_size[0], :]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1000, -1000))\n            #     continue\n            new_x, new_y = point[0] - x, point[1] - y\n            # if new_x <= 0 or new_y <= 0 or new_x > target_size[0] or new_y > target_size[1]:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((new_x, new_y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = target_size\n    meta.img = resized\n    return meta\n\n\ndef pose_flip(meta):\n    r = random.uniform(0, 1.0)\n    if r > 0.5:\n        return meta\n\n    img = meta.img\n    img = cv2.flip(img, 1)\n\n    # flip meta\n    flip_list = [CocoPart.Nose, CocoPart.Neck, CocoPart.LShoulder, CocoPart.LElbow, CocoPart.LWrist, CocoPart.RShoulder, CocoPart.RElbow, CocoPart.RWrist,\n                 CocoPart.LHip, CocoPart.LKnee, CocoPart.LAnkle, CocoPart.RHip, CocoPart.RKnee, CocoPart.RAnkle,\n                 CocoPart.LEye, CocoPart.REye, CocoPart.LEar, CocoPart.REar, CocoPart.Background]\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for cocopart in flip_list:\n            point = joint[cocopart.value]\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((meta.width - point[0], point[1]))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n\n    meta.img = img\n    return meta\n\n\ndef pose_rotation(meta):\n    deg = random.uniform(-15.0, 15.0)\n    img = meta.img\n\n    center = (img.shape[1] * 0.5, img.shape[0] * 0.5)       # x, y\n    rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n    ret = cv2.warpAffine(img, rot_m, img.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n    if img.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    neww, newh = RotationAndCropValid.largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n    neww = min(neww, ret.shape[1])\n    newh = min(newh, ret.shape[0])\n    newx = int(center[0] - neww * 0.5)\n    newy = int(center[1] - newh * 0.5)\n    # print(ret.shape, deg, newx, newy, neww, newh)\n    img = ret[newy:newy + newh, newx:newx + neww]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            x, y = _rotate_coord((meta.width, meta.height), (newx, newy), point, deg)\n            adjust_joint.append((x, y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = img\n\n    return meta\n\n\ndef _rotate_coord(shape, newxy, point, angle):\n    angle = -1 * angle / 180.0 * math.pi\n\n    ox, oy = shape\n    px, py = point\n\n    ox /= 2\n    oy /= 2\n\n    qx = math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n    qy = math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n\n    new_x, new_y = newxy\n\n    qx += ox - new_x\n    qy += oy - new_y\n\n    return int(qx + 0.5), int(qy + 0.5)\n\n\ndef pose_to_img(meta_l):\n    global _network_w, _network_h, _scale\n    return [\n        meta_l[0].img.astype(np.float16),\n        meta_l[0].get_heatmap(target_size=(_network_w // _scale, _network_h // _scale)),\n        meta_l[0].get_vectormap(target_size=(_network_w // _scale, _network_h // _scale))\n    ]\n'"
pose_dataset.py,4,"b'import logging\nimport math\nimport multiprocessing\nimport struct\nimport sys\nimport threading\n\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\nfrom contextlib import contextmanager\n\nimport os\nimport random\nimport requests\nimport cv2\nimport numpy as np\nimport time\n\nimport tensorflow as tf\n\nfrom tensorpack.dataflow import MultiThreadMapData\nfrom tensorpack.dataflow.image import MapDataComponent\nfrom tensorpack.dataflow.common import BatchData, MapData\nfrom tensorpack.dataflow.parallel import PrefetchData\nfrom tensorpack.dataflow.base import RNGDataFlow, DataFlowTerminated\n\nfrom pycocotools.coco import COCO\nfrom pose_augment import pose_flip, pose_rotation, pose_to_img, pose_crop_random, \\\n    pose_resize_shortestedge_random, pose_resize_shortestedge_fixed, pose_crop_center, pose_random_scale\n\nlogging.getLogger(""requests"").setLevel(logging.WARNING)\nlogger = logging.getLogger(\'pose_dataset\')\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\nmplset = False\n\n\nclass CocoMetadata:\n    # __coco_parts = 57\n    __coco_parts = 19\n    __coco_vecs = list(zip(\n        [2, 9,  10, 2,  12, 13, 2, 3, 4, 3,  2, 6, 7, 6,  2, 1,  1,  15, 16],\n        [9, 10, 11, 12, 13, 14, 3, 4, 5, 17, 6, 7, 8, 18, 1, 15, 16, 17, 18]\n    ))\n\n    @staticmethod\n    def parse_float(four_np):\n        assert len(four_np) == 4\n        return struct.unpack(\'<f\', bytes(four_np))[0]\n\n    @staticmethod\n    def parse_floats(four_nps, adjust=0):\n        assert len(four_nps) % 4 == 0\n        return [(CocoMetadata.parse_float(four_nps[x*4:x*4+4]) + adjust) for x in range(len(four_nps) // 4)]\n\n    def __init__(self, idx, img_url, img_meta, annotations, sigma):\n        self.idx = idx\n        self.img_url = img_url\n        self.img = None\n        self.sigma = sigma\n\n        self.height = int(img_meta[\'height\'])\n        self.width = int(img_meta[\'width\'])\n\n        joint_list = []\n        for ann in annotations:\n            if ann.get(\'num_keypoints\', 0) == 0:\n                continue\n\n            kp = np.array(ann[\'keypoints\'])\n            xs = kp[0::3]\n            ys = kp[1::3]\n            vs = kp[2::3]\n\n            joint_list.append([(x, y) if v >= 1 else (-1000, -1000) for x, y, v in zip(xs, ys, vs)])\n\n        self.joint_list = []\n        transform = list(zip(\n            [1, 6, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4],\n            [1, 7, 7, 9, 11, 6, 8, 10, 13, 15, 17, 12, 14, 16, 3, 2, 5, 4]\n        ))\n        for prev_joint in joint_list:\n            new_joint = []\n            for idx1, idx2 in transform:\n                j1 = prev_joint[idx1-1]\n                j2 = prev_joint[idx2-1]\n\n                if j1[0] <= 0 or j1[1] <= 0 or j2[0] <= 0 or j2[1] <= 0:\n                    new_joint.append((-1000, -1000))\n                else:\n                    new_joint.append(((j1[0] + j2[0]) / 2, (j1[1] + j2[1]) / 2))\n\n            new_joint.append((-1000, -1000))\n            self.joint_list.append(new_joint)\n\n        # logger.debug(\'joint size=%d\' % len(self.joint_list))\n\n    def get_heatmap(self, target_size):\n        heatmap = np.zeros((CocoMetadata.__coco_parts, self.height, self.width), dtype=np.float32)\n\n        for joints in self.joint_list:\n            for idx, point in enumerate(joints):\n                if point[0] < 0 or point[1] < 0:\n                    continue\n                CocoMetadata.put_heatmap(heatmap, idx, point, self.sigma)\n\n        heatmap = heatmap.transpose((1, 2, 0))\n\n        # background\n        heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n\n        if target_size:\n            heatmap = cv2.resize(heatmap, target_size, interpolation=cv2.INTER_AREA)\n\n        return heatmap.astype(np.float16)\n\n    @staticmethod\n    def put_heatmap(heatmap, plane_idx, center, sigma):\n        center_x, center_y = center\n        _, height, width = heatmap.shape[:3]\n\n        th = 4.6052\n        delta = math.sqrt(th * 2)\n\n        x0 = int(max(0, center_x - delta * sigma))\n        y0 = int(max(0, center_y - delta * sigma))\n\n        x1 = int(min(width, center_x + delta * sigma))\n        y1 = int(min(height, center_y + delta * sigma))\n\n        for y in range(y0, y1):\n            for x in range(x0, x1):\n                d = (x - center_x) ** 2 + (y - center_y) ** 2\n                exp = d / 2.0 / sigma / sigma\n                if exp > th:\n                    continue\n                heatmap[plane_idx][y][x] = max(heatmap[plane_idx][y][x], math.exp(-exp))\n                heatmap[plane_idx][y][x] = min(heatmap[plane_idx][y][x], 1.0)\n\n    def get_vectormap(self, target_size):\n        vectormap = np.zeros((CocoMetadata.__coco_parts*2, self.height, self.width), dtype=np.float32)\n        countmap = np.zeros((CocoMetadata.__coco_parts, self.height, self.width), dtype=np.int16)\n        for joints in self.joint_list:\n            for plane_idx, (j_idx1, j_idx2) in enumerate(CocoMetadata.__coco_vecs):\n                j_idx1 -= 1\n                j_idx2 -= 1\n\n                center_from = joints[j_idx1]\n                center_to = joints[j_idx2]\n\n                if center_from[0] < -100 or center_from[1] < -100 or center_to[0] < -100 or center_to[1] < -100:\n                    continue\n\n                CocoMetadata.put_vectormap(vectormap, countmap, plane_idx, center_from, center_to)\n\n        vectormap = vectormap.transpose((1, 2, 0))\n        nonzeros = np.nonzero(countmap)\n        for p, y, x in zip(nonzeros[0], nonzeros[1], nonzeros[2]):\n            if countmap[p][y][x] <= 0:\n                continue\n            vectormap[y][x][p*2+0] /= countmap[p][y][x]\n            vectormap[y][x][p*2+1] /= countmap[p][y][x]\n\n        if target_size:\n            vectormap = cv2.resize(vectormap, target_size, interpolation=cv2.INTER_AREA)\n\n        return vectormap.astype(np.float16)\n\n    @staticmethod\n    def put_vectormap(vectormap, countmap, plane_idx, center_from, center_to, threshold=8):\n        _, height, width = vectormap.shape[:3]\n\n        vec_x = center_to[0] - center_from[0]\n        vec_y = center_to[1] - center_from[1]\n\n        min_x = max(0, int(min(center_from[0], center_to[0]) - threshold))\n        min_y = max(0, int(min(center_from[1], center_to[1]) - threshold))\n\n        max_x = min(width, int(max(center_from[0], center_to[0]) + threshold))\n        max_y = min(height, int(max(center_from[1], center_to[1]) + threshold))\n\n        norm = math.sqrt(vec_x ** 2 + vec_y ** 2)\n        if norm == 0:\n            return\n\n        vec_x /= norm\n        vec_y /= norm\n\n        for y in range(min_y, max_y):\n            for x in range(min_x, max_x):\n                bec_x = x - center_from[0]\n                bec_y = y - center_from[1]\n                dist = abs(bec_x * vec_y - bec_y * vec_x)\n\n                if dist > threshold:\n                    continue\n\n                countmap[plane_idx][y][x] += 1\n\n                vectormap[plane_idx*2+0][y][x] = vec_x\n                vectormap[plane_idx*2+1][y][x] = vec_y\n\n\nclass CocoPose(RNGDataFlow):\n    @staticmethod\n    def display_image(inp, heatmap, vectmap, as_numpy=False):\n        global mplset\n        # if as_numpy and not mplset:\n        #     import matplotlib as mpl\n        #     mpl.use(\'Agg\')\n        mplset = True\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure()\n        a = fig.add_subplot(2, 2, 1)\n        a.set_title(\'Image\')\n        plt.imshow(CocoPose.get_bgimg(inp))\n\n        a = fig.add_subplot(2, 2, 2)\n        a.set_title(\'Heatmap\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(heatmap.shape[1], heatmap.shape[0])), alpha=0.5)\n        tmp = np.amax(heatmap, axis=2)\n        plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        tmp2 = vectmap.transpose((2, 0, 1))\n        tmp2_odd = np.amax(np.absolute(tmp2[::2, :, :]), axis=0)\n        tmp2_even = np.amax(np.absolute(tmp2[1::2, :, :]), axis=0)\n\n        a = fig.add_subplot(2, 2, 3)\n        a.set_title(\'Vectormap-x\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_odd, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        a = fig.add_subplot(2, 2, 4)\n        a.set_title(\'Vectormap-y\')\n        plt.imshow(CocoPose.get_bgimg(inp, target_size=(vectmap.shape[1], vectmap.shape[0])), alpha=0.5)\n        plt.imshow(tmp2_even, cmap=plt.cm.gray, alpha=0.5)\n        plt.colorbar()\n\n        if not as_numpy:\n            plt.show()\n        else:\n            fig.canvas.draw()\n            data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            fig.clear()\n            plt.close()\n            return data\n\n    @staticmethod\n    def get_bgimg(inp, target_size=None):\n        inp = cv2.cvtColor(inp.astype(np.uint8), cv2.COLOR_BGR2RGB)\n        if target_size:\n            inp = cv2.resize(inp, target_size, interpolation=cv2.INTER_AREA)\n        return inp\n\n    def __init__(self, path, img_path=None, is_train=True, decode_img=True, only_idx=-1):\n        self.is_train = is_train\n        self.decode_img = decode_img\n        self.only_idx = only_idx\n\n        if is_train:\n            whole_path = os.path.join(path, \'person_keypoints_train2017.json\')\n        else:\n            whole_path = os.path.join(path, \'person_keypoints_val2017.json\')\n        self.img_path = (img_path if img_path is not None else \'\') + (\'train2017/\' if is_train else \'val2017/\')\n        self.coco = COCO(whole_path)\n\n        logger.info(\'%s dataset %d\' % (path, self.size()))\n\n    def size(self):\n        return len(self.coco.imgs)\n\n    def get_data(self):\n        idxs = np.arange(self.size())\n        if self.is_train:\n            self.rng.shuffle(idxs)\n        else:\n            pass\n\n        keys = list(self.coco.imgs.keys())\n        for idx in idxs:\n            img_meta = self.coco.imgs[keys[idx]]\n            img_idx = img_meta[\'id\']\n            ann_idx = self.coco.getAnnIds(imgIds=img_idx)\n\n            if \'http://\' in self.img_path:\n                img_url = self.img_path + img_meta[\'file_name\']\n            else:\n                img_url = os.path.join(self.img_path, img_meta[\'file_name\'])\n\n            anns = self.coco.loadAnns(ann_idx)\n            meta = CocoMetadata(idx, img_url, img_meta, anns, sigma=8.0)\n\n            total_keypoints = sum([ann.get(\'num_keypoints\', 0) for ann in anns])\n            if total_keypoints == 0 and random.uniform(0, 1) > 0.2:\n                continue\n\n            yield [meta]\n\n\nclass MPIIPose(RNGDataFlow):\n    def __init__(self):\n        pass\n\n    def size(self):\n        pass\n\n    def get_data(self):\n        pass\n\n\ndef read_image_url(metas):\n    for meta in metas:\n        img_str = None\n        if \'http://\' in meta.img_url:\n            # print(meta.img_url)\n            for _ in range(10):\n                try:\n                    resp = requests.get(meta.img_url)\n                    if resp.status_code // 100 != 2:\n                        logger.warning(\'request failed code=%d url=%s\' % (resp.status_code, meta.img_url))\n                        time.sleep(1.0)\n                        continue\n                    img_str = resp.content\n                    break\n                except Exception as e:\n                    logger.warning(\'request failed url=%s, err=%s\' % (meta.img_url, str(e)))\n        else:\n            img_str = open(meta.img_url, \'rb\').read()\n\n        if not img_str:\n            logger.warning(\'image not read, path=%s\' % meta.img_url)\n            raise Exception()\n\n        nparr = np.fromstring(img_str, np.uint8)\n        meta.img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n    return metas\n\n\ndef get_dataflow(path, is_train, img_path=None):\n    ds = CocoPose(path, img_path, is_train)       # read data from lmdb\n    if is_train:\n        ds = MapData(ds, read_image_url)\n        ds = MapDataComponent(ds, pose_random_scale)\n        ds = MapDataComponent(ds, pose_rotation)\n        ds = MapDataComponent(ds, pose_flip)\n        ds = MapDataComponent(ds, pose_resize_shortestedge_random)\n        ds = MapDataComponent(ds, pose_crop_random)\n        ds = MapData(ds, pose_to_img)\n        # augs = [\n        #     imgaug.RandomApplyAug(imgaug.RandomChooseAug([\n        #         imgaug.GaussianBlur(max_size=3)\n        #     ]), 0.7)\n        # ]\n        # ds = AugmentImageComponent(ds, augs)\n        ds = PrefetchData(ds, 1000, multiprocessing.cpu_count()-1)\n    else:\n        ds = MultiThreadMapData(ds, nr_thread=16, map_func=read_image_url, buffer_size=1000)\n        ds = MapDataComponent(ds, pose_resize_shortestedge_fixed)\n        ds = MapDataComponent(ds, pose_crop_center)\n        ds = MapData(ds, pose_to_img)\n        ds = PrefetchData(ds, 100, multiprocessing.cpu_count() // 4)\n\n    return ds\n\n\ndef _get_dataflow_onlyread(path, is_train, img_path=None):\n    ds = CocoPose(path, img_path, is_train)  # read data from lmdb\n    ds = MapData(ds, read_image_url)\n    ds = MapData(ds, pose_to_img)\n    # ds = PrefetchData(ds, 1000, multiprocessing.cpu_count() * 4)\n    return ds\n\n\ndef get_dataflow_batch(path, is_train, batchsize, img_path=None):\n    logger.info(\'dataflow img_path=%s\' % img_path)\n    ds = get_dataflow(path, is_train, img_path=img_path)\n    ds = BatchData(ds, batchsize)\n    if is_train:\n        ds = PrefetchData(ds, 10, 2)\n    else:\n        ds = PrefetchData(ds, 50, 2)\n\n    return ds\n\n\nclass DataFlowToQueue(threading.Thread):\n    def __init__(self, ds, placeholders, queue_size=5):\n        super().__init__()\n        self.daemon = True\n\n        self.ds = ds\n        self.placeholders = placeholders\n        self.queue = tf.FIFOQueue(queue_size, [ph.dtype for ph in placeholders], shapes=[ph.get_shape() for ph in placeholders])\n        self.op = self.queue.enqueue(placeholders)\n        self.close_op = self.queue.close(cancel_pending_enqueues=True)\n\n        self._coord = None\n        self._sess = None\n\n        self.last_dp = None\n\n    @contextmanager\n    def default_sess(self):\n        if self._sess:\n            with self._sess.as_default():\n                yield\n        else:\n            logger.warning(""DataFlowToQueue {} wasn\'t under a default session!"".format(self.name))\n            yield\n\n    def size(self):\n        return self.queue.size()\n\n    def start(self):\n        self._sess = tf.get_default_session()\n        super().start()\n\n    def set_coordinator(self, coord):\n        self._coord = coord\n\n    def run(self):\n        with self.default_sess():\n            try:\n                while not self._coord.should_stop():\n                    try:\n                        self.ds.reset_state()\n                        while True:\n                            for dp in self.ds.get_data():\n                                feed = dict(zip(self.placeholders, dp))\n                                self.op.run(feed_dict=feed)\n                                self.last_dp = dp\n                    except (tf.errors.CancelledError, tf.errors.OutOfRangeError, DataFlowTerminated):\n                        logger.error(\'err type1, placeholders={}\'.format(self.placeholders))\n                        sys.exit(-1)\n                    except Exception as e:\n                        logger.error(\'err type2, err={}, placeholders={}\'.format(str(e), self.placeholders))\n                        if isinstance(e, RuntimeError) and \'closed Session\' in str(e):\n                            pass\n                        else:\n                            logger.exception(""Exception in {}:{}"".format(self.name, str(e)))\n                        sys.exit(-1)\n            except Exception as e:\n                logger.exception(""Exception in {}:{}"".format(self.name, str(e)))\n            finally:\n                try:\n                    self.close_op.run()\n                except Exception:\n                    pass\n                logger.info(""{} Exited."".format(self.name))\n\n    def dequeue(self):\n        return self.queue.dequeue()\n\n\nif __name__ == \'__main__\':\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\n    from pose_augment import set_network_input_wh, set_network_scale\n    # set_network_input_wh(368, 368)\n    set_network_input_wh(480, 320)\n    set_network_scale(8)\n\n    # df = get_dataflow(\'/data/public/rw/coco/annotations\', True, \'/data/public/rw/coco/\')\n    df = _get_dataflow_onlyread(\'/data/public/rw/coco/annotations\', True, \'/data/public/rw/coco/\')\n    # df = get_dataflow(\'/root/coco/annotations\', False, img_path=\'http://gpu-twg.kakaocdn.net/braincloud/COCO/\')\n\n    from tensorpack.dataflow.common import TestDataSpeed\n    TestDataSpeed(df).start()\n    sys.exit(0)\n\n    with tf.Session() as sess:\n        df.reset_state()\n        t1 = time.time()\n        for idx, dp in enumerate(df.get_data()):\n            if idx == 0:\n                for d in dp:\n                    logger.info(\'%d dp shape={}\'.format(d.shape))\n            print(time.time() - t1)\n            t1 = time.time()\n            CocoPose.display_image(dp[0], dp[1].astype(np.float32), dp[2].astype(np.float32))\n            print(dp[1].shape, dp[2].shape)\n            pass\n\n    logger.info(\'done\')\n'"
run.py,17,"b'import argparse\nimport tensorflow as tf\nimport sys\nimport time\nimport logging\nimport cv2\nimport numpy as np\nfrom tensorflow.contrib import slim\nimport vgg\nfrom cpm import PafNet\nimport common\nfrom tensblur.smoother import Smoother\nfrom estimator import PoseEstimator, TfPoseEstimator\n\nlogger = logging.getLogger(\'run\')\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Training codes for Openpose using Tensorflow\')\n    parser.add_argument(\'--checkpoint_path\', type=str, default=\'checkpoints/train/2018-12-13-16-56-49/\')\n    parser.add_argument(\'--backbone_net_ckpt_path\', type=str, default=\'checkpoints/vgg/vgg_19.ckpt\')\n    parser.add_argument(\'--image\', type=str, default=None)\n    # parser.add_argument(\'--run_model\', type=str, default=\'img\')\n    parser.add_argument(\'--video\', type=str, default=None)\n    parser.add_argument(\'--train_vgg\', type=bool, default=True)\n    parser.add_argument(\'--use_bn\', type=bool, default=False)\n    parser.add_argument(\'--save_video\', type=str, default=\'result/our.mp4\')\n\n    args = parser.parse_args()\n\n    checkpoint_path = args.checkpoint_path\n    logger.info(\'checkpoint_path: \' + checkpoint_path)\n\n    with tf.name_scope(\'inputs\'):\n        raw_img = tf.placeholder(tf.float32, shape=[None, None, None, 3])\n        img_size = tf.placeholder(dtype=tf.int32, shape=(2,), name=\'original_image_size\')\n\n    img_normalized = raw_img / 255 - 0.5\n\n    # define vgg19\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        vgg_outputs, end_points = vgg.vgg_19(img_normalized)\n\n    # get net graph\n    logger.info(\'initializing model...\')\n    net = PafNet(inputs_x=vgg_outputs, use_bn=args.use_bn)\n    hm_pre, cpm_pre, added_layers_out = net.gen_net()\n\n    hm_up = tf.image.resize_area(hm_pre[5], img_size)\n    cpm_up = tf.image.resize_area(cpm_pre[5], img_size)\n    # hm_up = hm_pre[5]\n    # cpm_up = cpm_pre[5]\n    smoother = Smoother({\'data\': hm_up}, 25, 3.0)\n    gaussian_heatMat = smoother.get_output()\n\n    max_pooled_in_tensor = tf.nn.pool(gaussian_heatMat, window_shape=(3, 3), pooling_type=\'MAX\', padding=\'SAME\')\n    tensor_peaks = tf.where(tf.equal(gaussian_heatMat, max_pooled_in_tensor), gaussian_heatMat,\n                                 tf.zeros_like(gaussian_heatMat))\n\n    logger.info(\'initialize saver...\')\n    # trainable_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'openpose_layers\')\n    # trainable_var_list = []\n    trainable_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'openpose_layers\')\n    if args.train_vgg:\n        trainable_var_list = trainable_var_list + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'vgg_19\')\n\n    restorer = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'vgg_19\'), name=\'vgg_restorer\')\n    saver = tf.train.Saver(trainable_var_list)\n    logger.info(\'initialize session...\')\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        sess.run(tf.group(tf.global_variables_initializer()))\n        logger.info(\'restoring vgg weights...\')\n        restorer.restore(sess, args.backbone_net_ckpt_path)\n        logger.info(\'restoring from checkpoint...\')\n        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir=checkpoint_path))\n        # saver.restore(sess, args.checkpoint_path + \'model-55000.ckpt\')\n        logger.info(\'initialization done\')\n        if args.image is None:\n            if args.video is not None:\n                cap = cv2.VideoCapture(args.video)\n            else:\n                cap = cv2.VideoCapture(0)\n                cap = cv2.VideoCapture(\'http://admin:admin@192.168.1.52:8081\')\n            _, image = cap.read()\n            if image is None:\n                logger.error(""Can\'t read video"")\n                sys.exit(-1)\n            fps = cap.get(cv2.CAP_PROP_FPS)\n            ori_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n            ori_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            if args.save_video is not None:\n                fourcc = cv2.VideoWriter_fourcc(*\'MP4V\')\n                video_saver = cv2.VideoWriter(\'result/our.mp4\', fourcc, fps, (ori_w, ori_h))\n                logger.info(\'record vide to %s\' % args.save_video)\n            logger.info(\'fps@%f\' % fps)\n            size = [int(654 * (ori_h / ori_w)), 654]\n            h = int(654 * (ori_h / ori_w))\n            time_n = time.time()\n            while True:\n                _, image = cap.read()\n                img = np.array(cv2.resize(image, (654, h)))\n                cv2.imshow(\'raw\', img)\n                img_corner = np.array(cv2.resize(image, (360, int(360*(ori_h/ori_w)))))\n                img = img[np.newaxis, :]\n                peaks, heatmap, vectormap = sess.run([tensor_peaks, hm_up, cpm_up],\n                                                     feed_dict={raw_img: img, img_size: size})\n                bodys = PoseEstimator.estimate_paf(peaks[0], heatmap[0], vectormap[0])\n                image = TfPoseEstimator.draw_humans(image, bodys, imgcopy=False)\n                fps = round(1 / (time.time() - time_n), 2)\n                image = cv2.putText(image, str(fps)+\'fps\', (10, 15), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255))\n                time_n = time.time()\n                if args.video is not None:\n                    image[27:img_corner.shape[0]+27, :img_corner.shape[1]] = img_corner  # [3:-10, :]\n                cv2.imshow(\' \', image)\n                if args.save_video is not None:\n                    video_saver.write(image)\n                cv2.waitKey(1)\n        else:\n            image = common.read_imgfile(args.image)\n            size = [image.shape[0], image.shape[1]]\n            if image is None:\n                logger.error(\'Image can not be read, path=%s\' % args.image)\n                sys.exit(-1)        \n            h = int(654 * (size[0] / size[1]))\n            img = np.array(cv2.resize(image, (654, h)))\n            cv2.imshow(\'ini\', img)\n            img = img[np.newaxis, :]\n            peaks, heatmap, vectormap = sess.run([tensor_peaks, hm_up, cpm_up], feed_dict={raw_img: img, img_size: size})\n            cv2.imshow(\'in\', vectormap[0, :, :, 0])\n            bodys = PoseEstimator.estimate_paf(peaks[0], heatmap[0], vectormap[0])\n            image = TfPoseEstimator.draw_humans(image, bodys, imgcopy=False)\n            cv2.imshow(\' \', image)\n            cv2.waitKey(0)\n'"
test_vgg19.py,5,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\nimport vgg\nimport cv2\nimport numpy as np\n\nimg = cv2.imread(""images/1.png"")\nimg = cv2.resize(img, (368, 368))\nimg = np.resize(img, [1, 368, 368, 3])\nvgg19_ckpt_path = ""checkpoints/vgg/vgg_19.ckpt""\n\nwith tf.name_scope(\'inputs\'):\n    inputs = tf.placeholder(tf.float32, shape=(None, None, None, 3))\nwith slim.arg_scope(vgg.vgg_arg_scope()):\n    vgg_outputs, end_points = vgg.vgg_19(inputs)\n\nrestorer = tf.train.Saver()\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n\n\n\n\nwith tf.Session(config=config) as sess:\n    restorer.restore(sess, vgg19_ckpt_path)\n    print(""model restored"")\n    predict = sess.run(vgg_outputs, feed_dict={inputs: img})\n    for i in range(512):\n        cv2.imshow("" "", predict[0, :, :, i-1])\n        cv2.waitKey(0)\n    # print(vgg_outputs.shape)\n\n'"
train.py,42,"b'import os\nimport time\nimport logging\nfrom tqdm import tqdm\nimport argparse\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nimport vgg\nfrom cpm import PafNet\nfrom pose_dataset import get_dataflow_batch, DataFlowToQueue, CocoPose\nfrom pose_augment import set_network_input_wh, set_network_scale\n\n\ndef train():\n    parser = argparse.ArgumentParser(description=\'Training codes for Openpose using Tensorflow\')\n    parser.add_argument(\'--batch_size\', type=int, default=10)\n    parser.add_argument(\'--continue_training\', type=bool, default=False)\n    parser.add_argument(\'--checkpoint_path\', type=str, default=\'checkpoints/train/\')\n    parser.add_argument(\'--backbone_net_ckpt_path\', type=str, default=\'checkpoints/vgg/vgg_19.ckpt\')\n    parser.add_argument(\'--train_vgg\', type=bool, default=True)\n    parser.add_argument(\'--annot_path\', type=str,\n                        default=\'/run/user/1000/gvfs/smb-share:server=server,share=data/yzy/dataset/\'\n                                \'Realtime_Multi-Person_Pose_Estimation-master/training/dataset/COCO/annotations/\')\n    parser.add_argument(\'--img_path\', type=str,\n                        default=\'/run/user/1000/gvfs/smb-share:server=server,share=data/yzy/dataset/\'\n                                \'Realtime_Multi-Person_Pose_Estimation-master/training/dataset/COCO/images/\')\n    # parser.add_argument(\'--annot_path_val\', type=str,\n    #                     default=\'/run/user/1000/gvfs/smb-share:server=192.168.1.2,share=data/yzy/dataset/\'\n    #                             \'Realtime_Multi-Person_Pose_Estimation-master/training/dataset/COCO/annotations/\'\n    #                             \'person_keypoints_val2017.json\')\n    # parser.add_argument(\'--img_path_val\', type=str,\n    #                     default=\'/run/user/1000/gvfs/smb-share:server=192.168.1.2,share=data/yzy/dataset/\'\n    #                             \'Realtime_Multi-Person_Pose_Estimation-master/training/dataset/COCO/images/val2017/\')\n    parser.add_argument(\'--save_checkpoint_frequency\', type=int, default=1000)\n    parser.add_argument(\'--save_summary_frequency\', type=int, default=100)\n    parser.add_argument(\'--stage_num\', type=int, default=6)\n    parser.add_argument(\'--hm_channels\', type=int, default=19)\n    parser.add_argument(\'--paf_channels\', type=int, default=38)\n    parser.add_argument(\'--input-width\', type=int, default=368)\n    parser.add_argument(\'--input-height\', type=int, default=368)\n    parser.add_argument(\'--max_echos\', type=int, default=5)\n    parser.add_argument(\'--use_bn\', type=bool, default=False)\n    parser.add_argument(\'--loss_func\', type=str, default=\'l2\')\n    args = parser.parse_args()\n\n    if not args.continue_training:\n        start_time = time.localtime(time.time())\n        checkpoint_path = args.checkpoint_path + (\'%d-%d-%d-%d-%d-%d\' % start_time[0:6])\n        os.mkdir(checkpoint_path)\n    else:\n        checkpoint_path = args.checkpoint_path\n\n    logger = logging.getLogger(\'train\')\n    logger.setLevel(logging.DEBUG)\n    fh = logging.FileHandler(checkpoint_path + \'/train_log.log\')\n    fh.setLevel(logging.DEBUG)\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s\')\n    fh.setFormatter(formatter)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.addHandler(fh)\n    logger.info(args)\n    logger.info(\'checkpoint_path: \' + checkpoint_path)\n\n    # define input placeholder\n    with tf.name_scope(\'inputs\'):\n        raw_img = tf.placeholder(tf.float32, shape=[args.batch_size, 368, 368, 3])\n        # mask_hm = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, 46, 46, args.hm_channels])\n        # mask_paf = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, 46, 46, args.paf_channels])\n        hm = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, 46, 46, args.hm_channels])\n        paf = tf.placeholder(dtype=tf.float32, shape=[args.batch_size, 46, 46, args.paf_channels])\n\n    # defien data loader\n    logger.info(\'initializing data loader...\')\n    set_network_input_wh(args.input_width, args.input_height)\n    scale = 8\n    set_network_scale(scale)\n    df = get_dataflow_batch(args.annot_path, True, args.batch_size, img_path=args.img_path)\n    steps_per_echo = df.size()\n    enqueuer = DataFlowToQueue(df, [raw_img, hm, paf], queue_size=100)\n    q_inp, q_heat, q_vect = enqueuer.dequeue()\n    q_inp_split, q_heat_split, q_vect_split = tf.split(q_inp, 1), tf.split(q_heat, 1), tf.split(q_vect, 1)\n    img_normalized = q_inp_split[0] / 255 - 0.5  # [-0.5, 0.5]\n\n    df_valid = get_dataflow_batch(args.annot_path, False, args.batch_size, img_path=args.img_path)\n    df_valid.reset_state()\n    validation_cache = []\n\n    logger.info(\'initializing model...\')\n    # define vgg19\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        vgg_outputs, end_points = vgg.vgg_19(img_normalized)\n\n    # get net graph\n    net = PafNet(inputs_x=vgg_outputs, stage_num=args.stage_num, hm_channel_num=args.hm_channels, use_bn=args.use_bn)\n    hm_pre, paf_pre, added_layers_out = net.gen_net()\n\n    # two kinds of loss\n    losses = []\n    with tf.name_scope(\'loss\'):\n        for idx, (l1, l2), in enumerate(zip(hm_pre, paf_pre)):\n            if args.loss_func == \'square\':\n                hm_loss = tf.reduce_sum(tf.square(tf.concat(l1, axis=0) - q_heat_split[0]))\n                paf_loss = tf.reduce_sum(tf.square(tf.concat(l2, axis=0) - q_vect_split[0]))\n                losses.append(tf.reduce_sum([hm_loss, paf_loss]))\n                logger.info(\'use square loss\')\n            else:\n                hm_loss = tf.nn.l2_loss(tf.concat(l1, axis=0) - q_heat_split[0])\n                paf_loss = tf.nn.l2_loss(tf.concat(l2, axis=0) - q_vect_split[0])\n                losses.append(tf.reduce_mean([hm_loss, paf_loss]))\n                logger.info(\'use l2 loss\')\n        loss = tf.reduce_sum(losses) / args.batch_size\n\n    global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    learning_rate = tf.train.exponential_decay(1e-4, global_step, steps_per_echo, 0.5, staircase=True)\n    trainable_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'openpose_layers\')\n    if args.train_vgg:\n        trainable_var_list = trainable_var_list + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'vgg_19\')\n    with tf.name_scope(\'train\'):\n        train = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=1e-8).minimize(loss=loss,\n                                                                                           global_step=global_step,\n                                                                                           var_list=trainable_var_list)\n    logger.info(\'initialize saver...\')\n    restorer = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\'vgg_19\'), name=\'vgg_restorer\')\n    saver = tf.train.Saver(trainable_var_list)\n\n    logger.info(\'initialize tensorboard\')\n    tf.summary.scalar(""lr"", learning_rate)\n    tf.summary.scalar(""loss2"", loss)\n    tf.summary.histogram(\'img_normalized\', img_normalized)\n    tf.summary.histogram(\'vgg_outputs\', vgg_outputs)\n    tf.summary.histogram(\'added_layers_out\', added_layers_out)\n    tf.summary.image(\'vgg_out\', tf.transpose(vgg_outputs[0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=512)\n    tf.summary.image(\'added_layers_out\', tf.transpose(added_layers_out[0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=128)\n    tf.summary.image(\'paf_gt\', tf.transpose(q_vect_split[0][0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=38)\n    tf.summary.image(\'hm_gt\', tf.transpose(q_heat_split[0][0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=19)\n    for i in range(args.stage_num):\n        tf.summary.image(\'hm_pre_stage_%d\' % i, tf.transpose(hm_pre[i][0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=19)\n        tf.summary.image(\'paf_pre_stage_%d\' % i, tf.transpose(paf_pre[i][0:1, :, :, :], perm=[3, 1, 2, 0]), max_outputs=38)\n    tf.summary.image(\'input\', img_normalized, max_outputs=4)\n\n    logger.info(\'initialize session...\')\n    merged = tf.summary.merge_all()\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        writer = tf.summary.FileWriter(checkpoint_path, sess.graph)\n        sess.run(tf.group(tf.global_variables_initializer()))\n        if args.backbone_net_ckpt_path is not None:\n            logger.info(\'restoring vgg weights from %s\' % args.backbone_net_ckpt_path)\n            restorer.restore(sess, args.backbone_net_ckpt_path)\n        if args.continue_training:\n            saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir=checkpoint_path))\n            logger.info(\'restoring from checkpoint...\')\n        logger.info(\'start training...\')\n        coord = tf.train.Coordinator()\n        enqueuer.set_coordinator(coord)\n        enqueuer.start()\n        while True:\n            best_checkpoint = float(\'inf\')\n            for _ in tqdm(range(steps_per_echo),):\n                total_loss, _, gs_num = sess.run([loss, train, global_step])\n                echo = gs_num / steps_per_echo\n\n                if gs_num % args.save_summary_frequency == 0:\n                    total_loss, gs_num, summary, lr = sess.run([loss, global_step, merged, learning_rate])\n                    writer.add_summary(summary, gs_num)\n                    logger.info(\'echos=%f, setp=%d, total_loss=%f, lr=%f\' % (echo, gs_num, total_loss, lr))\n\n                if gs_num % args.save_checkpoint_frequency == 0:\n                    valid_loss = 0\n                    if len(validation_cache) == 0:\n                        for images_test, heatmaps, vectmaps in tqdm(df_valid.get_data()):\n                            validation_cache.append((images_test, heatmaps, vectmaps))\n                        df_valid.reset_state()\n                        del df_valid\n                        df_valid = None\n\n                    for images_test, heatmaps, vectmaps in validation_cache:\n                        valid_loss += sess.run(loss, feed_dict={q_inp: images_test, q_vect: vectmaps, q_heat: heatmaps})\n\n                    if valid_loss / len(validation_cache) <= best_checkpoint:\n                        best_checkpoint = valid_loss / len(validation_cache)\n                        saver.save(sess, save_path=checkpoint_path + \'/\' + \'model\', global_step=gs_num)\n                        logger.info(\'best_checkpoint = %f, saving checkpoint to \' % best_checkpoint + checkpoint_path + \'/\' + \'model-%d\' % gs_num)\n\n                    else:\n                        logger.info(\'loss = %f drop\' % (valid_loss / len(validation_cache)))\n\n                if echo >= args.max_echos:\n                    sess.close()\n                    return 0\n\n\nif __name__ == \'__main__\':\n    train()\n'"
vgg.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\nThese model definitions were introduced in the following technical report:\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n  Args:\n    weight_decay: The l2 regularization coefficient.\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\',\n          global_pool=False):\n  """"""Oxford Net VGG 11-Layers version A Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer is\n      omitted and the input features to the logits layer are returned instead.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output.\n      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n      \'VALID\' padding.\n    global_pool: Optional boolean flag. If True, the input to the classification\n      layer is avgpooled to size 1x1, for any input size. (This is not part\n      of the original VGG architecture.)\n  Returns:\n    net: the output of the logits layer (if num_classes is a non-zero integer),\n      or the input to the logits layer (if num_classes is 0 or None).\n    end_points: a dict of tensors with intermediate activations.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if global_pool:\n        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n        end_points[\'global_pool\'] = net\n      if num_classes:\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          scope=\'fc8\')\n        if spatial_squeeze:\n          net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\',\n           global_pool=False):\n  """"""Oxford Net VGG 16-Layers version D Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer is\n      omitted and the input features to the logits layer are returned instead.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output.\n      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n      \'VALID\' padding.\n    global_pool: Optional boolean flag. If True, the input to the classification\n      layer is avgpooled to size 1x1, for any input size. (This is not part\n      of the original VGG architecture.)\n  Returns:\n    net: the output of the logits layer (if num_classes is a non-zero integer),\n      or the input to the logits layer (if num_classes is 0 or None).\n    end_points: a dict of tensors with intermediate activations.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if global_pool:\n        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n        end_points[\'global_pool\'] = net\n      if num_classes:\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          scope=\'fc8\')\n        if spatial_squeeze:\n          net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\',\n           global_pool=False):\n  """"""Oxford Net VGG 19-Layers version E Example.\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer is\n      omitted and the input features to the logits layer are returned instead.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output.\n      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n      \'VALID\' padding.\n    global_pool: Optional boolean flag. If True, the input to the classification\n      layer is avgpooled to size 1x1, for any input size. (This is not part\n      of the original VGG architecture.)\n  Returns:\n    net: the output of the logits layer (if num_classes is a non-zero integer),\n      or the non-dropped-out input to the logits layer (if num_classes is 0 or\n      None).\n    end_points: a dict of tensors with intermediate activations.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.original_name_scope + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      layer10 = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(layer10, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if global_pool:\n        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'global_pool\')\n        end_points[\'global_pool\'] = net\n      if num_classes:\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          scope=\'fc8\')\n        # if spatial_squeeze:\n        #   net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n  # regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n  # with tf.variable_scope(\'add_layers\'):\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                              filters=256,\n  #                              padding=""same"",\n  #                              kernel_size=3,\n  #                              activation=""relu"",\n  #                              kernel_initializer=tf.random_normal_initializer(),\n  #                              kernel_regularizer=regularizer)\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                              filters=128,\n  #                              padding=""same"",\n  #                              kernel_size=3,\n  #                              activation=""relu"",\n  #                              kernel_initializer=tf.random_normal_initializer(),\n  #                              kernel_regularizer=regularizer)\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                          filters=128,\n  #                          padding=""same"",\n  #                          kernel_size=3,\n  #                          activation=""relu"",\n  #                          kernel_initializer=tf.random_normal_initializer(stddev=0.1))\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                          filters=128,\n  #                          padding=""same"",\n  #                          kernel_size=3,\n  #                          activation=""relu"",\n  #                          kernel_initializer=tf.random_normal_initializer(stddev=0.1))\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                          filters=128,\n  #                          padding=""same"",\n  #                          kernel_size=3,\n  #                          activation=""relu"",\n  #                          kernel_initializer=tf.random_normal_initializer(stddev=0.1))\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                          filters=512,\n  #                          padding=""same"",\n  #                          kernel_size=1,\n  #                          activation=""relu"",\n  #                          kernel_initializer=tf.random_normal_initializer(stddev=0.1))\n  #   layer10 = tf.layers.conv2d(inputs=layer10,\n  #                          filters=1,\n  #                          padding=""same"",\n  #                          kernel_size=1,\n  #                          activation=""relu"",\n  #                          kernel_initializer=tf.random_normal_initializer(stddev=0.1))\n  return layer10, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19'"
pafprocess/__init__.py,0,b''
pafprocess/pafprocess.py,0,"b'# This file was automatically generated by SWIG (http://www.swig.org).\n# Version 3.0.12\n#\n# Do not make changes to this file unless you know what you are doing--modify\n# the SWIG interface file instead.\n\nfrom sys import version_info as _swig_python_version_info\nif _swig_python_version_info >= (2, 7, 0):\n    def swig_import_helper():\n        import importlib\n        pkg = __name__.rpartition(\'.\')[0]\n        mname = \'.\'.join((pkg, \'_pafprocess\')).lstrip(\'.\')\n        try:\n            return importlib.import_module(mname)\n        except ImportError:\n            return importlib.import_module(\'_pafprocess\')\n    _pafprocess = swig_import_helper()\n    del swig_import_helper\nelif _swig_python_version_info >= (2, 6, 0):\n    def swig_import_helper():\n        from os.path import dirname\n        import imp\n        fp = None\n        try:\n            fp, pathname, description = imp.find_module(\'_pafprocess\', [dirname(__file__)])\n        except ImportError:\n            import _pafprocess\n            return _pafprocess\n        try:\n            _mod = imp.load_module(\'_pafprocess\', fp, pathname, description)\n        finally:\n            if fp is not None:\n                fp.close()\n        return _mod\n    _pafprocess = swig_import_helper()\n    del swig_import_helper\nelse:\n    import _pafprocess\ndel _swig_python_version_info\n\ntry:\n    _swig_property = property\nexcept NameError:\n    pass  # Python < 2.2 doesn\'t have \'property\'.\n\ntry:\n    import builtins as __builtin__\nexcept ImportError:\n    import __builtin__\n\ndef _swig_setattr_nondynamic(self, class_type, name, value, static=1):\n    if (name == ""thisown""):\n        return self.this.own(value)\n    if (name == ""this""):\n        if type(value).__name__ == \'SwigPyObject\':\n            self.__dict__[name] = value\n            return\n    method = class_type.__swig_setmethods__.get(name, None)\n    if method:\n        return method(self, value)\n    if (not static):\n        if _newclass:\n            object.__setattr__(self, name, value)\n        else:\n            self.__dict__[name] = value\n    else:\n        raise AttributeError(""You cannot add attributes to %s"" % self)\n\n\ndef _swig_setattr(self, class_type, name, value):\n    return _swig_setattr_nondynamic(self, class_type, name, value, 0)\n\n\ndef _swig_getattr(self, class_type, name):\n    if (name == ""thisown""):\n        return self.this.own()\n    method = class_type.__swig_getmethods__.get(name, None)\n    if method:\n        return method(self)\n    raise AttributeError(""\'%s\' object has no attribute \'%s\'"" % (class_type.__name__, name))\n\n\ndef _swig_repr(self):\n    try:\n        strthis = ""proxy of "" + self.this.__repr__()\n    except __builtin__.Exception:\n        strthis = """"\n    return ""<%s.%s; %s >"" % (self.__class__.__module__, self.__class__.__name__, strthis,)\n\ntry:\n    _object = object\n    _newclass = 1\nexcept __builtin__.Exception:\n    class _object:\n        pass\n    _newclass = 0\n\nclass Peak(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, Peak, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, Peak, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""x""] = _pafprocess.Peak_x_set\n    __swig_getmethods__[""x""] = _pafprocess.Peak_x_get\n    if _newclass:\n        x = _swig_property(_pafprocess.Peak_x_get, _pafprocess.Peak_x_set)\n    __swig_setmethods__[""y""] = _pafprocess.Peak_y_set\n    __swig_getmethods__[""y""] = _pafprocess.Peak_y_get\n    if _newclass:\n        y = _swig_property(_pafprocess.Peak_y_get, _pafprocess.Peak_y_set)\n    __swig_setmethods__[""score""] = _pafprocess.Peak_score_set\n    __swig_getmethods__[""score""] = _pafprocess.Peak_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.Peak_score_get, _pafprocess.Peak_score_set)\n    __swig_setmethods__[""id""] = _pafprocess.Peak_id_set\n    __swig_getmethods__[""id""] = _pafprocess.Peak_id_get\n    if _newclass:\n        id = _swig_property(_pafprocess.Peak_id_get, _pafprocess.Peak_id_set)\n\n    def __init__(self):\n        this = _pafprocess.new_Peak()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_Peak\n    __del__ = lambda self: None\nPeak_swigregister = _pafprocess.Peak_swigregister\nPeak_swigregister(Peak)\ncvar = _pafprocess.cvar\nTHRESH_HEAT = cvar.THRESH_HEAT\nTHRESH_VECTOR_SCORE = cvar.THRESH_VECTOR_SCORE\nTHRESH_VECTOR_CNT1 = cvar.THRESH_VECTOR_CNT1\nTHRESH_PART_CNT = cvar.THRESH_PART_CNT\nTHRESH_HUMAN_SCORE = cvar.THRESH_HUMAN_SCORE\nNUM_PART = cvar.NUM_PART\nSTEP_PAF = cvar.STEP_PAF\nCOCOPAIRS_SIZE = cvar.COCOPAIRS_SIZE\nCOCOPAIRS_NET = cvar.COCOPAIRS_NET\nCOCOPAIRS = cvar.COCOPAIRS\n\nclass VectorXY(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, VectorXY, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, VectorXY, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""x""] = _pafprocess.VectorXY_x_set\n    __swig_getmethods__[""x""] = _pafprocess.VectorXY_x_get\n    if _newclass:\n        x = _swig_property(_pafprocess.VectorXY_x_get, _pafprocess.VectorXY_x_set)\n    __swig_setmethods__[""y""] = _pafprocess.VectorXY_y_set\n    __swig_getmethods__[""y""] = _pafprocess.VectorXY_y_get\n    if _newclass:\n        y = _swig_property(_pafprocess.VectorXY_y_get, _pafprocess.VectorXY_y_set)\n\n    def __init__(self):\n        this = _pafprocess.new_VectorXY()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_VectorXY\n    __del__ = lambda self: None\nVectorXY_swigregister = _pafprocess.VectorXY_swigregister\nVectorXY_swigregister(VectorXY)\n\nclass ConnectionCandidate(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, ConnectionCandidate, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, ConnectionCandidate, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""idx1""] = _pafprocess.ConnectionCandidate_idx1_set\n    __swig_getmethods__[""idx1""] = _pafprocess.ConnectionCandidate_idx1_get\n    if _newclass:\n        idx1 = _swig_property(_pafprocess.ConnectionCandidate_idx1_get, _pafprocess.ConnectionCandidate_idx1_set)\n    __swig_setmethods__[""idx2""] = _pafprocess.ConnectionCandidate_idx2_set\n    __swig_getmethods__[""idx2""] = _pafprocess.ConnectionCandidate_idx2_get\n    if _newclass:\n        idx2 = _swig_property(_pafprocess.ConnectionCandidate_idx2_get, _pafprocess.ConnectionCandidate_idx2_set)\n    __swig_setmethods__[""score""] = _pafprocess.ConnectionCandidate_score_set\n    __swig_getmethods__[""score""] = _pafprocess.ConnectionCandidate_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.ConnectionCandidate_score_get, _pafprocess.ConnectionCandidate_score_set)\n    __swig_setmethods__[""etc""] = _pafprocess.ConnectionCandidate_etc_set\n    __swig_getmethods__[""etc""] = _pafprocess.ConnectionCandidate_etc_get\n    if _newclass:\n        etc = _swig_property(_pafprocess.ConnectionCandidate_etc_get, _pafprocess.ConnectionCandidate_etc_set)\n\n    def __init__(self):\n        this = _pafprocess.new_ConnectionCandidate()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_ConnectionCandidate\n    __del__ = lambda self: None\nConnectionCandidate_swigregister = _pafprocess.ConnectionCandidate_swigregister\nConnectionCandidate_swigregister(ConnectionCandidate)\n\nclass Connection(_object):\n    __swig_setmethods__ = {}\n    __setattr__ = lambda self, name, value: _swig_setattr(self, Connection, name, value)\n    __swig_getmethods__ = {}\n    __getattr__ = lambda self, name: _swig_getattr(self, Connection, name)\n    __repr__ = _swig_repr\n    __swig_setmethods__[""cid1""] = _pafprocess.Connection_cid1_set\n    __swig_getmethods__[""cid1""] = _pafprocess.Connection_cid1_get\n    if _newclass:\n        cid1 = _swig_property(_pafprocess.Connection_cid1_get, _pafprocess.Connection_cid1_set)\n    __swig_setmethods__[""cid2""] = _pafprocess.Connection_cid2_set\n    __swig_getmethods__[""cid2""] = _pafprocess.Connection_cid2_get\n    if _newclass:\n        cid2 = _swig_property(_pafprocess.Connection_cid2_get, _pafprocess.Connection_cid2_set)\n    __swig_setmethods__[""score""] = _pafprocess.Connection_score_set\n    __swig_getmethods__[""score""] = _pafprocess.Connection_score_get\n    if _newclass:\n        score = _swig_property(_pafprocess.Connection_score_get, _pafprocess.Connection_score_set)\n    __swig_setmethods__[""peak_id1""] = _pafprocess.Connection_peak_id1_set\n    __swig_getmethods__[""peak_id1""] = _pafprocess.Connection_peak_id1_get\n    if _newclass:\n        peak_id1 = _swig_property(_pafprocess.Connection_peak_id1_get, _pafprocess.Connection_peak_id1_set)\n    __swig_setmethods__[""peak_id2""] = _pafprocess.Connection_peak_id2_set\n    __swig_getmethods__[""peak_id2""] = _pafprocess.Connection_peak_id2_get\n    if _newclass:\n        peak_id2 = _swig_property(_pafprocess.Connection_peak_id2_get, _pafprocess.Connection_peak_id2_set)\n\n    def __init__(self):\n        this = _pafprocess.new_Connection()\n        try:\n            self.this.append(this)\n        except __builtin__.Exception:\n            self.this = this\n    __swig_destroy__ = _pafprocess.delete_Connection\n    __del__ = lambda self: None\nConnection_swigregister = _pafprocess.Connection_swigregister\nConnection_swigregister(Connection)\n\n\ndef process_paf(p1, h1, f1):\n    return _pafprocess.process_paf(p1, h1, f1)\nprocess_paf = _pafprocess.process_paf\n\ndef get_num_humans():\n    return _pafprocess.get_num_humans()\nget_num_humans = _pafprocess.get_num_humans\n\ndef get_part_cid(human_id, part_id):\n    return _pafprocess.get_part_cid(human_id, part_id)\nget_part_cid = _pafprocess.get_part_cid\n\ndef get_score(human_id):\n    return _pafprocess.get_score(human_id)\nget_score = _pafprocess.get_score\n\ndef get_part_x(cid):\n    return _pafprocess.get_part_x(cid)\nget_part_x = _pafprocess.get_part_x\n\ndef get_part_y(cid):\n    return _pafprocess.get_part_y(cid)\nget_part_y = _pafprocess.get_part_y\n\ndef get_part_score(cid):\n    return _pafprocess.get_part_score(cid)\nget_part_score = _pafprocess.get_part_score\n# This file is compatible with both classic and new-style classes.\n\n\n'"
pafprocess/setup.py,0,"b'from distutils.core import setup, Extension\nimport numpy\nimport os\n\n# os.environ[\'CC\'] = \'g++\';\nsetup(name=\'pafprocess_ext\', version=\'1.0\',\n    ext_modules=[\n        Extension(\'_pafprocess\', [\'pafprocess.cpp\', \'pafprocess.i\'],\n                  swig_opts=[\'-c++\'],\n                  depends=[""pafprocess.h""],\n                  include_dirs=[numpy.get_include(), \'.\'])\n    ],\n    py_modules=[\n        ""pafprocess""\n    ]\n)\n'"
tensblur/__init__.py,0,b''
tensblur/smoother.py,4,"b'# vim: sta:et:sw=2:ts=2:sts=2\n# Written by Antonio Loquercio\n\nimport numpy as np\nimport scipy.stats as st\nimport pdb\n\nimport tensorflow as tf\n\n\ndef layer(op):\n    def layer_decorated(self, *args, **kwargs):\n        # Automatically set a name if not provided.\n        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n        # Figure out the layer inputs.\n        if len(self.terminals) == 0:\n            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n        elif len(self.terminals) == 1:\n            layer_input = self.terminals[0]\n        else:\n            layer_input = list(self.terminals)\n        # Perform the operation and get the output.\n        layer_output = op(self, layer_input, *args, **kwargs)\n        # Add to layer LUT.\n        self.layers[name] = layer_output\n        # This output is now the input for the next layer.\n        self.feed(layer_output)\n        # Return self for chained calls.\n        return self\n\n    return layer_decorated\n\n\nclass Smoother(object):\n    def __init__(self, inputs, filter_size, sigma):\n        self.inputs = inputs\n        self.terminals = []\n        self.layers = dict(inputs)\n        self.filter_size = filter_size\n        self.sigma = sigma\n        self.setup()\n\n    def setup(self):\n        self.feed(\'data\').conv(name=\'smoothing\')\n\n    def get_unique_name(self, prefix):\n        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n        return \'%s_%d\' % (prefix, ident)\n\n    def feed(self, *args):\n        assert len(args) != 0\n        self.terminals = []\n        for fed_layer in args:\n            if isinstance(fed_layer, str):\n                try:\n                    fed_layer = self.layers[fed_layer]\n                except KeyError:\n                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n            self.terminals.append(fed_layer)\n        return self\n\n    def gauss_kernel(self, kernlen=21, nsig=3, channels=1):\n        interval = (2*nsig+1.)/(kernlen)\n        x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)\n        kern1d = np.diff(st.norm.cdf(x))\n        kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n        kernel = kernel_raw/kernel_raw.sum()\n        out_filter = np.array(kernel, dtype = np.float32)\n        out_filter = out_filter.reshape((kernlen, kernlen, 1, 1))\n        out_filter = np.repeat(out_filter, channels, axis = 2)\n        return out_filter\n\n    def make_gauss_var(self, name, size, sigma, c_i):\n        # with tf.device(""/cpu:0""):\n        kernel = self.gauss_kernel(size, sigma, c_i)\n        var = tf.Variable(tf.convert_to_tensor(kernel), name=name)\n        return var\n\n    def get_output(self):\n        \'\'\'Returns the smoother output.\'\'\'\n        return self.terminals[-1]\n\n    @layer\n    def conv(self,\n             input,\n             name,\n             padding=\'SAME\'):\n        # Get the number of channels in the input\n        c_i = input.get_shape().as_list()[3]\n        # Convolution for a given input and kernel\n        convolve = lambda i, k: tf.nn.depthwise_conv2d(i, k, [1, 1, 1, 1], padding=padding)\n        with tf.variable_scope(name) as scope:\n            kernel = self.make_gauss_var(\'gauss_weight\', self.filter_size, self.sigma, c_i)\n            output = convolve(input, kernel)\n        return output\n'"
