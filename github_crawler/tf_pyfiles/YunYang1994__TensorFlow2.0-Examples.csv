file_path,api_count,code
1-Introduction/GradientTape.py,7,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : GradientTape.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 13:50:49\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n# tf.GradientTape is an API for automatic differentiation - computing the gradient of\n# a computation with respect to its input variables. Tensorflow ""records"" all operations\n# executed inside the context of a tf.GradientTape onto a ""tape""\n\n## Automatic differentiation\n\nx = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as t:\n    t.watch(x)       # Ensures that `tensor` is being traced by this tape.\n    y = x * x\n    z = y * y\ndz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\ndy_dx = t.gradient(y, x)  # 6.0\nprint(""dz/dx="", dz_dx.numpy())\nprint(""dy/dx="", dy_dx.numpy())\ndel t  # Drop the reference to the tape\n\n\n## Recording control flow\n\n# Because tapes record operations as they are executed,\n# Python control flow (using ifs and whiles for example) is naturally handled\ndef f(x, y):\n    output = 1.0\n    for i in range(y):\n        if i > 1 and i < 5:\n            output = tf.multiply(output, x)\n    return output\n\ndef grad(x, y):\n    with tf.GradientTape() as t:\n        t.watch(x)\n        out = f(x, y)\n    return t.gradient(out, x)\n\nx = tf.convert_to_tensor(2.0)\n\nassert grad(x, 6).numpy() == 12.0\nassert grad(x, 5).numpy() == 12.0\nassert grad(x, 4).numpy() == 4.0\n\n\n'"
1-Introduction/activation.py,5,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : activation.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 22:05:51\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# fake data\nx = np.linspace(-5, 5, 100)\n\n# following are popular activation functions\ny_relu = tf.nn.relu(x)\ny_sigmoid = tf.nn.sigmoid(x)\ny_tanh = tf.nn.tanh(x)\ny_softplus = tf.nn.softplus(x)\n# y_softmax = tf.nn.softmax(x)  softmax is a special kind of activation function, it is about probability\n\n# plt to visualize these activation function\nplt.figure(1, figsize=(8, 6))\nplt.subplot(221)\nplt.plot(x, y_relu, c='red', label='relu')\nplt.ylim((-1, 5))\nplt.legend(loc='best')\n\nplt.subplot(222)\nplt.plot(x, y_sigmoid, c='red', label='sigmoid')\nplt.ylim((-0.2, 1.2))\nplt.legend(loc='best')\n\nplt.subplot(223)\nplt.plot(x, y_tanh, c='red', label='tanh')\nplt.ylim((-1.2, 1.2))\nplt.legend(loc='best')\n\nplt.subplot(224)\nplt.plot(x, y_softplus, c='red', label='softplus')\nplt.ylim((-0.2, 6))\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n"""
1-Introduction/basic_operations.py,13,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : basic_operations.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 14:32:57\n#   Description :\n#\n#================================================================\n\n""""""\nBasic Operations example using TensorFlow library.\n""""""\n\nimport tensorflow as tf\n\n###======================================= assign value  ===================================#\n\na = tf.ones([2,3])\nprint(a)\n\n# a[0,0] = 10 => TypeError: \'tensorflow.python.framework.ops.EagerTensor\' object does not support item assignment\n\na = tf.Variable(a)\na[0,0].assign(10)\nb = a.read_value()\nprint(b)\n\n###======================================= add, multiply, div. etc ===================================#\n\na = tf.constant(2)\nb = tf.constant(3)\n\nprint(""a + b :"" , a.numpy() + b.numpy())\nprint(""Addition with constants: "", a+b)\nprint(""Addition with constants: "", tf.add(a, b))\nprint(""a * b :"" , a.numpy() * b.numpy())\nprint(""Multiplication with constants: "", a*b)\nprint(""Multiplication with constants: "", tf.multiply(a, b))\n\n\n# ----------------\n# More in details:\n# Matrix Multiplication from TensorFlow official tutorial\n\n# Create a Constant op that produces a 1x2 matrix.  The op is\n# added as a node to the default graph.\n#\n# The value returned by the constructor represents the output\n# of the Constant op.\nmatrix1 = tf.constant([[3., 3.]])\n\n# Create another Constant that produces a 2x1 matrix.\nmatrix2 = tf.constant([[2.],[2.]])\n\n# Create a Matmul op that takes \'matrix1\' and \'matrix2\' as inputs.\n# The returned value, \'product\', represents the result of the matrix\n# multiplication.\nproduct = tf.matmul(matrix1, matrix2)\nprint(""Multiplication with matrixes:"", product)\n\n# broadcast matrix in Multiplication\n\nprint(""broadcast matrix in Multiplication:"", matrix1 * matrix2)\n\n\n###===================================== cast operations =====================================#\n\na = tf.convert_to_tensor(2.)\nb = tf.cast(a, tf.int32)\nprint(a, b)\n\n###===================================== shape operations ===================================#\n\na = tf.ones([2,3])\nprint(a.shape[0], a.shape[1]) # 2, 3\nshape = tf.shape(a)           # a tensor\nprint(shape[0], shape[1])\n\n\n'"
1-Introduction/helloworld.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : helloworld.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 00:21:22\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n# Simple hello world using TensorFlow\n\n# Create a Constant op\n# The op is added as a node to the default graph.\n#\n# The value returned by the constructor represents the output\n# of the Constant op.\n\nhelloworld = tf.constant(""hello, TensorFlow"")\nprint(""Tensor:"", helloworld)\nprint(""Value :"", helloworld.numpy())\n\n'"
1-Introduction/variable.py,6,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : variable.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 00:36:23\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n# Variables are manipulated via the tf.Variable class.\n# A tf.Variable represents a tensor whose value can be changed by running ops on it.\n# Specific ops allow you to read and modify the values of this tensor.\n\n## Creaging a Variable\n\nwith tf.name_scope(""my""):\n    variable = tf.Variable(1)\n\nprint(""value:"", variable.numpy())\n\n## Using Variables\n\n# To use the value of a tf.Variable in a TensorFlow graph, simply treat it like a normal tf.Tensor\nvariable = variable + 1\nprint(""value:"", variable.numpy())\n\n# To assign a value to a variable, use the methods assign, assign_add\nvariable = tf.Variable(2)\nvariable.assign_add(1)\nprint(""value:"", variable.numpy())\n'"
2-Basical_Models/CNN.py,17,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : CNN.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-10 20:18:48\n#   Description :\n#\n#================================================================\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\n\n\n# Load and prepare the MNIST dataset.\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# # Add a channels dimension\nx_train = x_train[..., tf.newaxis]\nx_test  = x_test[..., tf.newaxis]\n\n# Use tf.data to batch and shuffle the dataset\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n        (x_train, y_train)).shuffle(10000).batch(32)\ntest_ds = tf.data.Dataset.from_tensor_slices(\n        (x_test, y_test)).batch(32)\n\n# Build the tf.keras model using the Keras model subclassing API\nclass MyModel(Model):\n  def __init__(self):\n      super(MyModel, self).__init__()\n      self.conv1 = Conv2D(32, 3, activation='relu')\n      self.flatten = Flatten()\n      self.d1 = Dense(128, activation='relu')\n      self.d2 = Dense(10, activation='softmax')\n\n  def call(self, x):\n      x = self.conv1(x)\n      x = self.flatten(x)\n      x = self.d1(x)\n      return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()\n\n# Choose an optimizer and loss function for training\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\n\n# Select metrics to measure the loss and the accuracy of the model\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n\n# Use tf.GradientTape to train the model.\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n\n@tf.function\ndef test_step(images, labels):\n    predictions = model(images)\n    t_loss = loss_object(labels, predictions)\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n    for images, labels in train_ds:\n        train_step(images, labels)\n\n    for test_images, test_labels in test_ds:\n        test_step(test_images, test_labels)\n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch+1,\n                          train_loss.result(),\n                          train_accuracy.result()*100,\n                          test_loss.result(),\n                          test_accuracy.result()*100))\n    # Reset the metrics for the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n\n\n"""
2-Basical_Models/Linear_Regression.py,6,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : Linear_Regression.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 17:33:48\n#   Description :\n#\n#================================================================\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Define model and Loss\n\nclass Model(object):\n    def __init__(self):\n        self.W = tf.Variable(10.0)\n        self.b = tf.Variable(-5.0)\n\n    def __call__(self, inputs):\n        return self.W * inputs + self.b\n\ndef compute_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true-y_pred))\n\nmodel = Model()\n\n# Define True weight and bias\n\nTRUE_W = 3.0\nTRUE_b = 2.0\n\n# Obtain training data, Let\'s synthesize the training data with some noise.\n\nNUM_EXAMPLES = 1000\ninputs  = tf.random.normal(shape=[NUM_EXAMPLES])\nnoise   = tf.random.normal(shape=[NUM_EXAMPLES])\noutputs = inputs * TRUE_W + TRUE_b + noise\n\n# Before we train the model let\'s visualize where the model stands right now.\n# We\'ll plot the model\'s predictions in red and the training data in blue.\n\ndef plot(epoch):\n    plt.scatter(inputs, outputs, c=\'b\')\n    plt.scatter(inputs, model(inputs), c=\'r\')\n    plt.title(""epoch %2d, loss = %s"" %(epoch, str(compute_loss(outputs, model(inputs)).numpy())))\n    plt.legend()\n    plt.draw()\n    plt.ion()   # replacing plt.show()\n    plt.pause(1)\n    plt.close()\n\n# Define a training loop\nlearning_rate = 0.1\nfor epoch in range(30):\n    with tf.GradientTape() as tape:\n        loss = compute_loss(outputs, model(inputs))\n\n    dW, db = tape.gradient(loss, [model.W, model.b])\n\n    model.W.assign_sub(learning_rate * dW)\n    model.b.assign_sub(learning_rate * db)\n\n    print(""=> epoch %2d: w_true= %.2f, w_pred= %.2f; b_true= %.2f, b_pred= %.2f, loss= %.2f"" %(\n          epoch+1, TRUE_W, model.W.numpy(), TRUE_b, model.b.numpy(), loss.numpy()))\n    plot(epoch + 1)\n\n'"
2-Basical_Models/Logistic_Regression.py,11,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : Logistic_Regression.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-08 22:28:21\n#   Description :\n#\n#================================================================\n\nimport numpy as np\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\ntraining_epochs = 6\nbatch_size = 600\n\n# Import MNIST data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\ntrain_dataset = (\n    tf.data.Dataset.from_tensor_slices((tf.reshape(x_train, [-1, 784]), y_train))\n    .batch(batch_size)\n    .shuffle(1000)\n)\n\ntrain_dataset = (\n    train_dataset.map(lambda x, y:\n                      (tf.divide(tf.cast(x, tf.float32), 255.0),\n                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n)\n\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# Construct model\nmodel = lambda x: tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n# Minimize error using cross entropy\ncompute_loss = lambda true, pred: tf.reduce_mean(tf.reduce_sum(tf.losses.binary_crossentropy(true, pred), axis=-1))\n# caculate accuracy\ncompute_accuracy = lambda true, pred: tf.reduce_mean(tf.keras.metrics.categorical_accuracy(true, pred))\n# Gradient Descent\noptimizer = tf.optimizers.Adam(learning_rate)\n\nfor epoch in range(training_epochs):\n    for i, (x_, y_) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            pred = model(x_)\n            loss = compute_loss(y_, pred)\n        acc = compute_accuracy(y_, pred)\n        grads = tape.gradient(loss, [W, b])\n        optimizer.apply_gradients(zip(grads, [W, b]))\n        print(""=> loss %.2f acc %.2f"" %(loss.numpy(), acc.numpy()))\n\n'"
2-Basical_Models/Multilayer_Perceptron.py,22,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# # A Brief History of Perceptrons\n\n# # Multilayer Perceptron Layer\n\n# Subsequent work with multilayer perceptrons has shown that they are capable of\n# approximating an XOR operator as well as many other non-linear functions.\n#\n# A multilayer perceptron (MLP) is a deep, artificial neural network.\n# It is composed of more than one perceptron. They are composed of an input layer to receive the signal,\n# an output layer that makes a decision or prediction about the input, and in between those two,\n# an arbitrary number of hidden layers that are the true computational engine of the MLP.\n# MLPs with one hidden layer are capable of approximating any continuous function.\n#\n# Multilayer perceptrons are often applied to supervised learning problems3:\n# they train on a set of input-output pairs and learn to model the correlation (or dependencies)\n# between those inputs and outputs. Training involves adjusting the parameters,\n# or the weights and biases, of the model in order to minimize error. Backpropagation\n# is used to make those weigh and bias adjustments relative to the error, id the error\n# itself can be measured in a variety of ways, including by root mean squared error (RMSE).\n\n\nimport numpy as np\nimport tensorflow as tf\n\n# Parameters\nlearning_rate = 0.001\ntraining_steps = 3000\nbatch_size = 100\ndisplay_step = 300\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n# Convert to float32.\nx_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n# Flatten images to 1-D vector of 784 features (28*28).\nx_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n# Normalize images value from [0, 255] to [0, 1].\nx_train, x_test = x_train / 255., x_test / 255.\n\n# Use tf.data API to shuffle and batch data.\ntrain_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n\n\n# Network Parameters\nn_hidden_1 = 256 # 1st layer number of neurons\nn_hidden_2 = 256 # 2nd layer number of neurons\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\n\n# Store layers weight & bias\nweights = {\n    \'h1\': tf.Variable(tf.random.normal([n_input, n_hidden_1])),\n    \'h2\': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2])),\n    \'out\': tf.Variable(tf.random.normal([n_hidden_2, n_classes]))\n}\nbiases = {\n    \'b1\': tf.Variable(tf.random.normal([n_hidden_1])),\n    \'b2\': tf.Variable(tf.random.normal([n_hidden_2])),\n    \'out\': tf.Variable(tf.random.normal([n_classes]))\n}\n\n\n# Create model\ndef multilayer_perceptron(x):\n    # Hidden fully connected layer with 256 neurons\n    layer_1 = tf.add(tf.matmul(x, weights[\'h1\']), biases[\'b1\'])\n    layer_1 = tf.nn.sigmoid(layer_1)\n    # Hidden fully connected layer with 256 neurons\n    layer_2 = tf.add(tf.matmul(layer_1, weights[\'h2\']), biases[\'b2\'])\n    # Output fully connected layer with a neuron for each class\n    layer_2 = tf.nn.sigmoid(layer_2)\n    output = tf.matmul(layer_2, weights[\'out\']) + biases[\'out\']\n    return tf.nn.softmax(output)\n\n# Cross-Entropy loss function.\ndef cross_entropy(y_pred, y_true):\n    # Encode label to a one hot vector.\n    y_true = tf.one_hot(y_true, depth=10)\n    # Clip prediction values to avoid log(0) error.\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n    # Compute cross-entropy.\n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n\n# Accuracy metric.\ndef accuracy(y_pred, y_true):\n    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n\n# Stochastic gradient descent optimizer.\noptimizer = tf.optimizers.SGD(learning_rate)\n\n\n# Optimization process.\ndef train_step(x, y):\n    # Wrap computation inside a GradientTape for automatic differentiation.\n    with tf.GradientTape() as tape:\n        pred = multilayer_perceptron(x)\n        loss = cross_entropy(pred, y)\n\n    # Variables to update, i.e. trainable variables.\n    trainable_variables = list(weights.values()) + list(biases.values())\n\n    # Compute gradients.\n    gradients = tape.gradient(loss, trainable_variables)\n\n    # Update W and b following gradients.\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n\n# Run training for the given number of steps.\nfor step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n    # Run the optimization to update W and b values.\n    train_step(batch_x, batch_y)\n\n    if (step+1) % display_step == 0:\n        pred = multilayer_perceptron(batch_x)\n        loss  = cross_entropy(pred, batch_y)\n        acc  = accuracy(pred, batch_y)\n        print(""step: %i, loss: %f, accuracy: %f"" % (step+1, loss, acc))\n\n'"
3-Neural_Network_Architecture/autoencoder.py,15,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : autoencoder.py\n#   Author      : YunYang1994\n#   Created date: 2019-11-05 19:53:58\n#   Description :\n#\n#================================================================\n\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\n\nclass Encoder(tf.keras.Model):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.dense_1 = tf.keras.layers.Dense(128, activation=\'tanh\')\n        self.dense_2 = tf.keras.layers.Dense(64 , activation=\'tanh\')\n        self.dense_3 = tf.keras.layers.Dense(32 , activation=\'tanh\')\n        self.dense_4 = tf.keras.layers.Dense(2  , activation=\'sigmoid\')\n    def call(self, x, training=False):\n        out = self.dense_1(x)\n        out = self.dense_2(out)\n        out = self.dense_3(out)\n        out = self.dense_4(out)\n        return out\n\n\nclass Decoder(tf.keras.Model):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.dense_1 = tf.keras.layers.Dense(16 , activation=\'tanh\')\n        self.dense_2 = tf.keras.layers.Dense(64 , activation=\'tanh\')\n        self.dense_3 = tf.keras.layers.Dense(128, activation=\'tanh\')\n        self.dense_4 = tf.keras.layers.Dense(784, activation=\'sigmoid\')\n    def call(self, x, training=False):\n        out = self.dense_1(x)\n        out = self.dense_2(out)\n        out = self.dense_3(out)\n        out = self.dense_4(out)\n        return out\n\nclass Autoencoder(tf.keras.Model):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n    def call(self, x, training=False):\n        out = self.encoder(x, training)\n        out = self.decoder(out, training)\n        return out\n\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = np.reshape(X_train, (-1, 784)) / 255.\nbatch_size = 512\noptimizer = tf.keras.optimizers.Adam(lr=0.001)\nmodel = Autoencoder()\nsample = np.reshape(X_test[:5], (5, 784))\n\nfor step in range(10000):\n    true_image = X_train[np.random.choice(X_train.shape[0], batch_size)]\n    with tf.GradientTape() as tape:\n        pred_image = model(true_image, training=True)\n        loss = tf.reduce_mean(tf.square(pred_image-true_image))\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    if step % 200  == 0: print(""=> %4d loss %.4f"" %(step, loss))\n    if step % 1000 == 0:\n        pred_image = model(sample, training=False)\n        pred_image = np.reshape(pred_image, (5, 28, 28))\n        show_image = np.concatenate(pred_image[:5], -1)\n        plt.tight_layout()\n        plt.imshow(show_image)\n        plt.savefig(""%d.png"" %((step+1) // 2000))\n\n""""""\nvisualize embedding in 2D\n""""""\nsample = np.reshape(X_test[:5000], (5000, 784))\nlabel = y_test[:5000]\nembeddings = model.encoder(sample, training=False)\n\nfig,ax = plt.subplots()\nX, Y = embeddings[:,0].numpy(), embeddings[:,1].numpy()\nax.set_xlim(X.min(), X.max())\nax.set_ylim(Y.min(), Y.max())\nfor x,y,l in zip(X,Y,label):\n    c = cm.rainbow(int(255 *l/ 9))\n    ax.text(x, y, l, color=c)\n    # plt.plot(x,y, \'.\', c=c)\nplt.axis(\'off\')\nplt.legend()\nplt.tight_layout()\nplt.savefig(""embedding.png"")\nplt.show()\n\n\n'"
3-Neural_Network_Architecture/fpn.py,25,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : fpn.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-25 17:37:35\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n""""""\nImplements a ResNet version of the FPN introduced in\nhttps://arxiv.org/pdf/1612.03144.pdf\n""""""\n\nclass BasicBlock(tf.keras.Model):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, strides=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=strides,\n                                            padding=""same"", use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n\n        self.conv2 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=1,\n                                            padding=""same"", use_bias=False)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n\n        """"""\n        Adds a shortcut between input and residual block and merges them with ""sum""\n        """"""\n        if strides != 1 or in_channels != self.expansion * out_channels:\n            self.shortcut = tf.keras.Sequential([\n                    tf.keras.layers.Conv2D(self.expansion*out_channels, kernel_size=1,\n                                           strides=strides, use_bias=False),\n                    tf.keras.layers.BatchNormalization()]\n                    )\n        else:\n            self.shortcut = lambda x,_: x\n\n    def call(self, x, training=False):\n        # if training: print(""=> training network ... "")\n        out = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n        out = self.bn2(self.conv2(out), training=training)\n        out += self.shortcut(x, training)\n        return tf.nn.relu(out)\n\nclass FPN(tf.keras.Model):\n    """""" use ResNet as backbone\n    """"""\n    def __init__(self, block, num_blocks):\n        super(FPN, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = tf.keras.layers.Conv2D(64, 7, 2, padding=""same"", use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n\n        # Bottom --> up layers\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n\n        # Top layer\n        self.top_layer = tf.keras.layers.Conv2D(256, 1, 1, padding=""valid"")\n\n        # Smooth layers\n        self.smooth1   = tf.keras.layers.Conv2D(256, 3, 1, padding=""same"")\n        self.smooth2   = tf.keras.layers.Conv2D(256, 3, 1, padding=""same"")\n        self.smooth3   = tf.keras.layers.Conv2D(256, 3, 1, padding=""same"")\n\n        # Lateral layers\n        self.lateral_layer1 = tf.keras.layers.Conv2D(256, 1, 1, padding=""valid"")\n        self.lateral_layer2 = tf.keras.layers.Conv2D(256, 1, 1, padding=""valid"")\n        self.lateral_layer3 = tf.keras.layers.Conv2D(256, 1, 1, padding=""valid"")\n\n    def _make_layer(self, block, out_channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * block.expansion\n        return tf.keras.Sequential(layers)\n\n    def _upsample_add(self, x, y):\n        """"""Upsample and add two feature maps.\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n        Returns:\n          (Variable) added feature map.\n          """"""\n        _, H, W, C = y.shape\n        return tf.image.resize(x, size=(H, W), method=""bilinear"")\n\n    def call(self, x, training=False):\n        p1 = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n        p1 = tf.nn.max_pool2d(p1, ksize=3, strides=2, padding=""SAME"")\n\n        # Bottom --> up\n        p2 = self.layer1(p1, training=training)\n        p3 = self.layer2(p2, training=training)\n        p4 = self.layer3(p3, training=training)\n        p5 = self.layer4(p4, training=training)\n\n        # Top-down\n        d5 = self.top_layer(p5)\n        d4 = self._upsample_add(d5, self.lateral_layer1(p4))\n        d3 = self._upsample_add(d4, self.lateral_layer2(p3))\n        d2 = self._upsample_add(d3, self.lateral_layer3(p2))\n\n        # Smooth\n        d4 = self.smooth1(d4)\n        d3 = self.smooth2(d3)\n        d2 = self.smooth3(d2)\n\n        return d2, d3, d4, d5\n\ndef ResNet18_fpn():\n    return FPN(BasicBlock, [2, 2, 2, 2])\n\ndef ResNet34_fpn():\n    return FPN(BasicBlock, [3, 4, 6, 3])\n\nif __name__ == ""__main__"":\n    ## Test model\n    data = tf.ones(shape=[1, 416, 416, 3])\n    # model = ResNet18_fpn()\n    model = ResNet34_fpn()\n    fms = model(data)\n    for fm in fms:\n        print(fm.shape)\n\n\n'"
3-Neural_Network_Architecture/main.py,15,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : main.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-25 15:18:10\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\nfrom resnet import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n\nlr = 0.0001\nbatch_size = 32\nEPOCHS = 50\n\n# Build your model here\nmodel = ResNet18()\noptimizer = tf.keras.optimizers.Adam(lr)\n\n# Load and prepare the cifar10 dataset.\ncifar10 = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\ny_train, y_test = tf.reshape(y_train, (-1,)), tf.reshape(y_test, (-1,))\n\n# Use tf.data to batch and shuffle the dataset\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n        (x_train, y_train)).shuffle(100).batch(batch_size)\ntest_ds = tf.data.Dataset.from_tensor_slices(\n        (x_test, y_test)).batch(batch_size)\n\n# Choose an optimizer and loss function for training\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# Select metrics to measure the loss and the accuracy of the model\ntrain_loss = tf.keras.metrics.Mean(name=\'train_loss\')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\'train_accuracy\')\n\ntest_loss = tf.keras.metrics.Mean(name=\'test_loss\')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\'test_accuracy\')\n\n# Use tf.GradientTape to train the model.\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        predictions = model(images, training=True)\n        # print(""=> label shape: "", labels.shape, ""pred shape"", predictions.shape)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n\n@tf.function\ndef test_step(images, labels):\n    predictions = model(images)\n    t_loss = loss_object(labels, predictions)\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n\nfor epoch in range(EPOCHS):\n    for images, labels in train_ds:\n        train_step(images, labels)\n\n    for test_images, test_labels in test_ds:\n        test_step(test_images, test_labels)\n\n    template = \'=> Epoch {}, Loss: {:.4}, Accuracy: {:.2%}, Test Loss: {:.4}, Test Accuracy: {:.2%}\'\n    print(template.format(epoch+1,\n                          train_loss.result(),\n                          train_accuracy.result(),\n                          test_loss.result(),\n                          test_accuracy.result()))\n    # Reset the metrics for the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n'"
3-Neural_Network_Architecture/resnet.py,31,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : resnet.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-11 19:16:55\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\nclass BasicBlock(tf.keras.Model):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, strides=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=strides,\n                                            padding=""same"", use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n\n        self.conv2 = tf.keras.layers.Conv2D(out_channels, kernel_size=3, strides=1,\n                                            padding=""same"", use_bias=False)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n\n        """"""\n        Adds a shortcut between input and residual block and merges them with ""sum""\n        """"""\n        if strides != 1 or in_channels != self.expansion * out_channels:\n            self.shortcut = tf.keras.Sequential([\n                    tf.keras.layers.Conv2D(self.expansion*out_channels, kernel_size=1,\n                                           strides=strides, use_bias=False),\n                    tf.keras.layers.BatchNormalization()]\n                    )\n        else:\n            self.shortcut = lambda x,_: x\n\n    def call(self, x, training=False):\n        # if training: print(""=> training network ... "")\n        out = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n        out = self.bn2(self.conv2(out), training=training)\n        out += self.shortcut(x, training)\n        return tf.nn.relu(out)\n\n\nclass Bottleneck(tf.keras.Model):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, strides=1):\n        super(Bottleneck, self).__init__()\n\n        self.conv1 = tf.keras.layers.Conv2D(out_channels, 1, 1, use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.conv2 = tf.keras.layers.Conv2D(out_channels, 3, strides, padding=""same"", use_bias=False)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.conv3 = tf.keras.layers.Conv2D(out_channels*self.expansion, 1, 1, use_bias=False)\n        self.bn3 = tf.keras.layers.BatchNormalization()\n\n        if strides != 1 or in_channels != self.expansion * out_channels:\n            self.shortcut = tf.keras.Sequential([\n                    tf.keras.layers.Conv2D(self.expansion*out_channels, kernel_size=1,\n                                           strides=strides, use_bias=False),\n                    tf.keras.layers.BatchNormalization()]\n                    )\n        else:\n            self.shortcut = lambda x,_: x\n\n    def call(self, x, training=False):\n        out = tf.nn.relu(self.bn1(self.conv1(x), training))\n        out = tf.nn.relu(self.bn2(self.conv2(out), training))\n        out = self.bn3(self.conv3(out), training)\n        out += self.shortcut(x, training)\n        return tf.nn.relu(out)\n\n\nclass ResNet(tf.keras.Model):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 64\n\n        self.conv1 = tf.keras.layers.Conv2D(64, 3, 1, padding=""same"", use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n\n        self.avg_pool2d = tf.keras.layers.AveragePooling2D(4)\n        self.linear = tf.keras.layers.Dense(units=num_classes, activation=""softmax"")\n\n    def _make_layer(self, block, out_channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_channels, out_channels, stride))\n            self.in_channels = out_channels * block.expansion\n        return tf.keras.Sequential(layers)\n\n    def call(self, x, training=False):\n        out = tf.nn.relu(self.bn1(self.conv1(x), training))\n        out = self.layer1(out, training=training)\n        out = self.layer2(out, training=training)\n        out = self.layer3(out, training=training)\n        out = self.layer4(out, training=training)\n\n        # For classification\n        out = self.avg_pool2d(out)\n        out = tf.reshape(out, (out.shape[0], -1))\n        out = self.linear(out)\n        return out\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3,4,14,3])\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3,8,36,3])\n\n'"
3-Neural_Network_Architecture/vgg16.py,27,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : vgg16.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-16 10:09:17\n#   Description :\n#\n#================================================================\n\nimport skimage\nimport numpy as np\nimport tensorflow as tf\n\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\n# define input layer\ninput_layer = tf.keras.layers.Input([224, 224, 3])\n\nred, green, blue = tf.split(axis=3, num_or_size_splits=3, value=input_layer)\nbgr = tf.concat(axis=3, values=[blue - VGG_MEAN[0], green - VGG_MEAN[1], red - VGG_MEAN[2]])\n\n# Block 1\nconv1_1 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv1_1\')(bgr)\n\nconv1_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv1_2\')(conv1_1)\npool1_1 = tf.nn.max_pool(conv1_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool1_1\')\n\n# Block 2\nconv2_1 = tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv2_1\')(pool1_1)\nconv2_2 = tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv2_2\')(conv2_1)\npool2_1 = tf.nn.max_pool(conv2_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool2_1\')\n\n# Block 3\nconv3_1 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv3_1\')(pool2_1)\nconv3_2 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv3_2\')(conv3_1)\nconv3_3 = tf.keras.layers.Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv3_3\')(conv3_2)\npool3_1 = tf.nn.max_pool(conv3_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool3_1\')\n\n# Block 4\nconv4_1 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv4_1\')(pool3_1)\nconv4_2 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv4_2\')(conv4_1)\nconv4_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv4_3\')(conv4_2)\npool4_1 = tf.nn.max_pool(conv4_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool4_1\')\n\n# Block 4\nconv5_1 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv5_1\')(pool4_1)\nconv5_2 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv5_2\')(conv5_1)\nconv5_3 = tf.keras.layers.Conv2D(filters=512, kernel_size=[3, 3], strides=[1, 1], padding=\'same\',\n                                 use_bias=True, activation=\'relu\', name=\'conv5_3\')(conv5_2)\npool5_1 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\', name=\'pool5_1\')\n\nflatten = tf.keras.layers.Flatten()(pool5_1)\nfc6 = tf.keras.layers.Dense(units=4096, use_bias=True, name=\'fc6\', activation=\'relu\')(flatten)\nfc7 = tf.keras.layers.Dense(units=4096, use_bias=True, name=\'fc7\', activation=\'relu\')(fc6)\nfc8 = tf.keras.layers.Dense(units=1000, use_bias=True, name=\'fc8\', activation=None)(fc7)\n\nprob = tf.nn.softmax(fc8)\n\n# Build model\nmodel = tf.keras.Model(input_layer, prob)\n\n# # Load weighs\n# weighs = np.load(""./vgg16.npy"", encoding=\'latin1\').item()\n# for layer_name in weighs.keys():\n    # layer = model.get_layer(layer_name)\n    # layer.set_weights(weighs[layer_name])\n\n# # Load image\n# image_data = skimage.io.imread(""./docs/cat.jpg"").astype(np.float32)\n\n# # Load labels\n# labels = open(""./docs/synset_words.txt"", ""r"").readlines()\n\n# # Print result\n# print(labels[np.argmax(model(np.expand_dims(image_data, 0)))])\n\n\n\n\n\n'"
6-Generative_Adversarial_Networks/Pix2Pix.py,55,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : Pix2Pix.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-09 13:42:33\n#   Description :\n#\n#================================================================\n\nimport os\nimport time\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\n""""""\nThis notebook demonstrates image to image translation using conditional GAN\'s,\nas described in Image-to-Image Translation with Conditional Adversarial Networks.\n""""""\n\n_URL = \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\'\n\npath_to_zip = tf.keras.utils.get_file(\'facades.tar.gz\', origin=_URL, extract=True)\n\nPATH = os.path.join(os.path.dirname(path_to_zip), \'facades/\')\n\n\nBUFFER_SIZE = 400\nEPOCHS = 200\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nOUTPUT_CHANNELS = 3\n\n\ndef load(image_file):\n\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n\n    w = tf.shape(image)[1]\n    w = w // 2\n    real_image  = image[:, :w, :]\n    input_image = image[:, w:, :]\n    input_image = tf.cast(input_image, tf.float32)\n    real_image  = tf.cast(real_image, tf.float32)\n\n    return input_image, real_image\n\ndef resize(input_image, real_image, height, width):\n\n    input_image = tf.image.resize(input_image, [height, width],\n                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    real_image  = tf.image.resize(real_image, [height, width],\n                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    return input_image, real_image\n\ndef random_crop(input_image, real_image):\n\n    stacked_image = tf.stack([input_image, real_image], axis=0)\n    cropped_image = tf.image.random_crop(\n        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_image[0], cropped_image[1]\n\n# normalizing the images to [-1, 1]\ndef normalize(input_image, real_image):\n\n    input_image = (input_image / 127.5) - 1\n    real_image  = (real_image  / 127.5) - 1\n\n    return input_image, real_image\n\n@tf.function()\ndef random_jitter(input_image, real_image):\n    # resizing to 286 x 286 x 3\n    input_image, real_image = resize(input_image, real_image, 286, 286)\n    # randomly cropping to 256 x 256 x 3\n    input_image, real_image = random_crop(input_image, real_image)\n\n    # random mirroring\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        real_image = tf.image.flip_left_right(real_image)\n\n    return input_image, real_image\n\n\ndef load_image_train(image_file):\n\n    input_image, real_image = load(image_file)\n    input_image, real_image = random_jitter(input_image, real_image)\n    input_image, real_image = normalize(input_image, real_image)\n\n    return input_image, real_image\n\ndef load_image_test(image_file):\n\n    input_image, real_image = load(image_file)\n    input_image, real_image = resize(input_image, real_image,  IMG_HEIGHT, IMG_WIDTH)\n    input_image, real_image = normalize(input_image, real_image)\n\n    return input_image, real_image\n\n\ntrain_dataset = tf.data.Dataset.list_files(PATH+\'train/*.jpg\')\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.map(load_image_train,\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntrain_dataset = train_dataset.batch(1)\n\n\n# Build the Generator\n\ndef downsample(filters, size, apply_batchnorm=True):\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2D(filters, size, strides=2, padding=\'same\',\n                             kernel_initializer=initializer, use_bias=False))\n    if apply_batchnorm:\n        result.add(tf.keras.layers.BatchNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding=\'same\',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n    result.add(tf.keras.layers.BatchNormalization())\n\n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n\n    result.add(tf.keras.layers.ReLU())\n    return result\n\ndef Generator():\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding=\'same\',\n                                         kernel_initializer=initializer,\n                                         activation=\'tanh\') # (bs, 256, 256, 3)\n    concat = tf.keras.layers.Concatenate()\n\n    inputs = tf.keras.layers.Input(shape=[None,None,3])\n    x = inputs\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = concat([x, skip])\n    x = last(x)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\ngenerator = Generator()\n\n# Build the Discriminator\n\ndef Discriminator():\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    inp = tf.keras.layers.Input(shape=[None, None, 3], name=\'input_image\')\n    tar = tf.keras.layers.Input(shape=[None, None, 3], name=\'target_image\')\n\n    x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n\ndiscriminator = Discriminator()\n\n# Define the loss functions and the optimizer\n\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    LAMBDA = 100\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss\n\ngenerator_optimizer     = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# Checkpoints (Object-based saving)\ncheckpoint_dir = \'./training_checkpoints\'\ncheckpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n\ndef generate_images(model, test_input, tar):\n    # the training=True is intentional here since\n    # we want the batch statistics while running the model\n    # on the test dataset. If we use training=False, we will get\n    # the accumulated statistics learned from the training dataset\n    # (which we don\'t want)\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = [\'Input Image\', \'Ground Truth\', \'Predicted Image\']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis(\'off\')\n    plt.show()\n\n@tf.function\ndef train_step(input_image, target):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n        disc_real_output = discriminator([input_image, target], training=True)\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))\n\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    for input_image, target in train_dataset:\n        train_step(input_image, target)\n\n    clear_output(wait=True)\n    for inp, tar in test_dataset.take(1):\n        generate_images(generator, inp, tar)\n    # saving (checkpoint) the model every 20 epochs\n    if (epoch + 1) % 20 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n    print (\'=> Time taken for epoch {} is {} sec\\n\'.format(epoch + 1, time.time()-start))\n'"
6-Generative_Adversarial_Networks/dcgan.py,39,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# # Deep Convolutional Generative Adversarial Network\n\n# This tutorial demonstrates how to generate images of handwritten digits using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop.\n\n# ## What are GANs?\n# [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (""the artist"") learns to create images that look real, while a *discriminator* (""the art critic"") learns to tell real images apart from fakes.\n#\n# ![A diagram of a generator and discriminator](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/gan1.png?raw=1)\n#\n# During training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes.\n#\n# ![A second diagram of a generator and discriminator](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/images/gan2.png?raw=1)\n#\n# This notebook demonstrate this process on the MNIST dataset. The following animation shows a series of images produced by the *generator* as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time.\n#\n# To learn more about GANs, we recommend MIT\'s [Intro to Deep Learning](http://introtodeeplearning.com/) course.\n\n# ### Import TensorFlow and other libraries\n\n# In[1]:\n\n\nimport glob\nimport imageio\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport time\n\nfrom IPython import display\n\n\n# ### Load and prepare the dataset\n#\n# You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data.\n\n\nBUFFER_SIZE = 60000\nBATCH_SIZE  = 256\n\n(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\'float32\')\ntrain_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n# Batch and shuffle the data\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\n\n# ## Create the models\n#\n# Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model).\n\n# ### The Generator\n#\n# The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh.\n\ndef make_generator_model():\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n\n    model.add(tf.keras.layers.Reshape((7, 7, 256)))\n    assert model.output_shape == (None, 7, 7, 256)                  # Note: None is the batch size\n\n    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\'same\', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 128)\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n\n    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False))\n    assert model.output_shape == (None, 14, 14, 64)\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n\n    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False, activation=\'tanh\'))\n    assert model.output_shape == (None, 28, 28, 1)\n\n    return model\n\n\n# Use the (as yet untrained) generator to create an image.\n\ngenerator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap=\'gray\')\n\n\n# ### The Discriminator\n#\n# The discriminator is a CNN-based image classifier.\n\ndef make_discriminator_model():\n\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\'same\', input_shape=[28, 28, 1]))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n\n    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\'same\'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1))\n\n    return model\n\n\n# Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images.\n\ndiscriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)\n\n\n# ## Define the loss and optimizers\n# Define loss functions and optimizers for both models.\n\n# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\n# ### Discriminator loss\n#\n# This method quantifies how well the discriminator is able to distinguish real images from fakes.\n# It compares the disciminator\'s predictions on real images to an array of 1s,\n# and the disciminator\'s predictions on fake (generated) images to an array of 0s.\n\ndef discriminator_loss(real_output, fake_output):\n\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n\n    return total_loss\n\n\n# ### Generator loss\n# The generator\'s loss quantifies how well it was able to trick the discriminator.\n# Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).\n# Here, we will compare the disciminators decisions on the generated images to an array of 1s.\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n\n# The discriminator and the generator optimizers are different since we will train two networks separately.\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n\n# ### Save checkpoints\n# This notebook also demonstrates how to save and restore models,\n# which can be helpful in case a long running training task is interrupted.\n\n\ncheckpoint_dir = \'./training_checkpoints\'\ncheckpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n\n\n# ## Define the training loop\n\nEPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\n# We will reuse this seed overtime (so it\'s easier)\n# to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])\n\n\n# The training loop begins with generator receiving a random seed as input.\n# That seed is used to produce an image.\n# The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator).\n# The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n\n# Notice the use of `tf.function`\n# This annotation causes the function to be ""compiled"".\n@tf.function\ndef train_step(images):\n\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n\ndef train(dataset, epochs):\n\n    for epoch in range(epochs):\n        start = time.time()\n        for image_batch in dataset:\n            train_step(image_batch)\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epoch + 1, seed)\n\n    # Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print (\'Time for epoch {} is {} sec\'.format(epoch + 1, time.time()-start))\n    # Generate after the final epoch\n    display.clear_output(wait=True)\n    generate_and_save_images(generator, epochs, seed)\n\n\n# **Generate and save images**\ndef generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(4,4))\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\'gray\')\n        plt.axis(\'off\')\n    plt.savefig(\'image_at_epoch_{:04d}.png\'.format(epoch))\n    plt.show()\n\n\n# ## Train the model\n# Call the `train()` method defined above to train the generator and discriminator simultaneously.\n# Note, training GANs can be tricky. It\'s important that the generator and discriminator do not\n# overpower each other (e.g., that they train at a similar rate).\n# At the beginning of the training, the generated images look like random noise.\n# As training progresses, the generated digits will look increasingly real.\n# After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab.\n\nget_ipython().run_cell_magic(\'time\', \'\', \'train(train_dataset, EPOCHS)\')\n\n\n# Restore the latest checkpoint.\n\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n\n\n# ## Create a GIF\n\n\n# Display a single image using the epoch number\ndef display_image(epoch_no):\n  return PIL.Image.open(\'image_at_epoch_{:04d}.png\'.format(epoch_no))\n\ndisplay_image(EPOCHS)\n\n\n# Use `imageio` to create an animated gif using the images saved during training.\n\nwith imageio.get_writer(\'dcgan.gif\', mode=\'I\') as writer:\n    filenames = glob.glob(\'image*.png\')\n    filenames = sorted(filenames)\n    last = -1\n    for i,filename in enumerate(filenames):\n        frame = 2*(i**0.5)\n        if round(frame) > round(last):\n            last = frame\n        else:\n            continue\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    image = imageio.imread(filename)\n    writer.append_data(image)\n\n# A hack to display the GIF inside this notebook\nos.rename(\'dcgan.gif\', \'dcgan.gif.png\')\n\n\n# Display the animated gif with all the mages generated during the training of GANs.\n\ndisplay.Image(filename=""dcgan.gif.png"")\n\n\n'"
7-Utils/multi_gpu_train.py,22,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2020 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : multi_gpu_train.py\n#   Author      : YunYang1994\n#   Created date: 2020-02-02 22:14:30\n#   Description :\n#\n#================================================================\n\nimport os\nimport shutil\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import applications\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0,1,2,3""\n\nEPOCHS       = 40\nSCORE_THRESH = 0.8\nNUM_CLASS    = 10\nEMB_SIZE     = 2     # Embedding Size\nIMG_SIZE     = 112   # Input Image Size\nBATCH_SIZE   = 512   # Total 4 GPU, 128 batch per GPU\nGPU_SIZE     = 30    # (G)  MemorySIZE per GPU\n\n#------------------------------------ Prepare Dataset ------------------------------------#\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=False)\n\ntrain_generator = train_datagen.flow_from_directory(\n        \'./mnist/train\',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=BATCH_SIZE,\n        class_mode=\'categorical\')\n\ntest_datagen = ImageDataGenerator(\n        rescale=1./255,\n        horizontal_flip=False)\n\ntest_generator = test_datagen.flow_from_directory(\n        \'./mnist/test\',\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=2,\n        class_mode=\'categorical\')\n\n#------------------------------------ Build Mode -----------------------------------#\n\ntf.debugging.set_log_device_placement(True)\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\n\nfor gpu in gpus:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=GPU_SIZE*1024)]\n    )\nlogical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\nprint(len(gpus), ""Physical GPU,"", len(logical_gpus), ""Logical GPUs"")\n\ntf.debugging.set_log_device_placement(True)\nstrategy = tf.distribute.MirroredStrategy()\n\n# Defining Model\nwith strategy.scope():\n    backbone = applications.mobilenet_v2.MobileNetV2(include_top=False,\n                                                    weights=\'imagenet\', input_shape=(IMG_SIZE,IMG_SIZE,3))\n    x = tf.keras.layers.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n    y = backbone(x)\n    y = tf.keras.layers.AveragePooling2D()(y)\n    y = tf.keras.layers.Flatten()(y)\n    y = tf.keras.layers.Dense(EMB_SIZE,  activation=None)(y)\n    featureExtractor = tf.keras.models.Model(inputs=x, outputs=y)\n    model = tf.keras.Sequential([\n        featureExtractor,\n        tf.keras.layers.Dense(NUM_CLASS, activation=\'softmax\')\n    ])\n\n    model.build(input_shape=[1, IMG_SIZE, IMG_SIZE, 3])\n    optimizer = tf.keras.optimizers.Adam(0.001)\n\n# Defining Loss and Metrics\nwith strategy.scope():\n    loss_object = tf.keras.losses.CategoricalCrossentropy(\n        reduction=tf.keras.losses.Reduction.NONE\n    )\n    def compute_loss(labels, predictions):\n        per_example_loss = loss_object(labels, predictions)\n        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\n\n    train_accuracy = tf.keras.metrics.CategoricalAccuracy(\n        name=\'train_accuracy\'\n    )\n\n# Defining Training Step\nwith strategy.scope():\n    def train_step(inputs):\n        images, labels = inputs\n\n        with tf.GradientTape() as tape:\n            predictions = model(images, training=True)\n            loss = compute_loss(labels, predictions)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        train_accuracy.update_state(labels, predictions)\n        return loss\n\n#------------------------------------ Training Loop -----------------------------------#\n\n# Defining Training Loops\nwith strategy.scope():\n    @tf.function\n    def distributed_train_step(dataset_inputs):\n        per_replica_losses = strategy.experimental_run_v2(train_step,\n                                                          args=(dataset_inputs,))\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                               axis=None)\n    for epoch in range(1, EPOCHS+1):\n        if epoch == 30: optimizer.lr.assign(0.0001)\n\n        batchs_per_epoch = len(train_generator)\n        train_dataset    = iter(train_generator)\n        test_dataset     = iter(test_generator)\n\n        with tqdm(total=batchs_per_epoch,\n                  desc=""Epoch %2d/%2d"" %(epoch, EPOCHS)) as pbar:\n            loss_value = 0.\n            acc_value  = 0.\n            num_batch  = 0\n\n            for _ in range(batchs_per_epoch):\n\n                num_batch  += 1\n                batch_loss = distributed_train_step(next(train_dataset))\n                batch_acc  = train_accuracy.result()\n\n                loss_value += batch_loss\n                acc_value  += batch_acc\n\n                pbar.set_postfix({\'loss\' : \'%.4f\'     %(loss_value / num_batch),\n                                  \'accuracy\' : \'%.6f\' %(acc_value  / num_batch)})\n                train_accuracy.reset_states()\n                pbar.update(1)\n\n        model_path = ""./models/weights_%02d"" %epoch\n        if not os.path.exists(model_path):\n            os.makedirs(model_path)\n\n        model.save(os.path.join(model_path, ""model.h5""))\n        featureExtractor.save(os.path.join(model_path, ""featureExtractor.h5""))\n\n\n'"
4-Object_Detection/MTCNN/main.py,3,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : main.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-27 12:51:40\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom mtcnn import PNet, RNet, ONet\nfrom PIL import Image, ImageDraw\nfrom utils import detect_face\n\ndef load_weights(model, weights_file):\n    weights_dict = np.load(weights_file, encoding=\'latin1\').item()\n    for layer_name in weights_dict.keys():\n        layer = model.get_layer(layer_name)\n        if ""conv"" in layer_name:\n            layer.set_weights([weights_dict[layer_name][""weights""], weights_dict[layer_name][""biases""]])\n        else:\n            prelu_weight = weights_dict[layer_name][\'alpha\']\n            try:\n                layer.set_weights([prelu_weight])\n            except:\n                layer.set_weights([prelu_weight[np.newaxis, np.newaxis, :]])\n    return True\n\npnet, rnet, onet = PNet(), RNet(), ONet()\npnet(tf.ones(shape=[1,  12,  12, 3]))\nrnet(tf.ones(shape=[1,  24,  24 ,3]))\nonet(tf.ones(shape=[1,  48,  48, 3]))\nload_weights(pnet, ""./det1.npy""), load_weights(rnet, ""./det2.npy""), load_weights(onet, ""./det3.npy"")\n\nimage = cv2.cvtColor(cv2.imread(""./multiface.jpg""), cv2.COLOR_BGR2RGB)\ntotal_boxes, points = detect_face(image, 20, pnet, rnet, onet, [0.6, 0.7, 0.7], 0.709)\n\nfor bounding_box, keypoints in zip(total_boxes, points.T):\n    bounding_boxes = {\n            \'box\': [int(bounding_box[0]), int(bounding_box[1]),\n                    int(bounding_box[2]-bounding_box[0]), int(bounding_box[3]-bounding_box[1])],\n            \'confidence\': bounding_box[-1],\n            \'keypoints\': {\n                \'left_eye\': (int(keypoints[0]), int(keypoints[5])),\n                \'right_eye\': (int(keypoints[1]), int(keypoints[6])),\n                \'nose\': (int(keypoints[2]), int(keypoints[7])),\n                \'mouth_left\': (int(keypoints[3]), int(keypoints[8])),\n                \'mouth_right\': (int(keypoints[4]), int(keypoints[9])),\n            }\n        }\n    bounding_box = bounding_boxes[\'box\']\n    keypoints = bounding_boxes[\'keypoints\']\n    cv2.rectangle(image,\n                (bounding_box[0], bounding_box[1]),\n                (bounding_box[0]+bounding_box[2], bounding_box[1] + bounding_box[3]),\n                (0,155,255), 2)\n    cv2.circle(image,(keypoints[\'left_eye\']), 2, (0,155,255), 2)\n    cv2.circle(image,(keypoints[\'right_eye\']), 2, (0,155,255), 2)\n    cv2.circle(image,(keypoints[\'nose\']), 2, (0,155,255), 2)\n    cv2.circle(image,(keypoints[\'mouth_left\']), 2, (0,155,255), 2)\n    cv2.circle(image,(keypoints[\'mouth_right\']), 2, (0,155,255), 2)\n\nImage.fromarray(image).show()\n'"
4-Object_Detection/MTCNN/mtcnn.py,45,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : mtcnn.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-26 19:22:04\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n\nclass PNet(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(10, 3, 1, name=\'conv1\')\n        self.prelu1 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""PReLU1"")\n\n        self.conv2 = tf.keras.layers.Conv2D(16, 3, 1, name=\'conv2\')\n        self.prelu2 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""PReLU2"")\n\n        self.conv3 = tf.keras.layers.Conv2D(32, 3, 1, name=\'conv3\')\n        self.prelu3 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""PReLU3"")\n\n        self.conv4_1 = tf.keras.layers.Conv2D(2, 1, 1, name=\'conv4-1\')\n        self.conv4_2 = tf.keras.layers.Conv2D(4, 1, 1, name=\'conv4-2\')\n\n    def call(self, x, training=False):\n        out = self.prelu1(self.conv1(x))\n        out = tf.nn.max_pool2d(out, 2, 2, padding=""SAME"")\n        out = self.prelu2(self.conv2(out))\n        out = self.prelu3(self.conv3(out))\n        score = tf.nn.softmax(self.conv4_1(out), axis=-1)\n        boxes = self.conv4_2(out)\n        return boxes, score\n\nclass RNet(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(28, 3, 1, name=\'conv1\')\n        self.prelu1 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu1"")\n\n        self.conv2 = tf.keras.layers.Conv2D(48, 3, 1, name=\'conv2\')\n        self.prelu2 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu2"")\n\n        self.conv3 = tf.keras.layers.Conv2D(64, 2, 1, name=\'conv3\')\n        self.prelu3 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu3"")\n\n        self.dense4 = tf.keras.layers.Dense(128, name=\'conv4\')\n        self.prelu4 = tf.keras.layers.PReLU(shared_axes=None, name=""prelu4"")\n\n        self.dense5_1 = tf.keras.layers.Dense(2, name=""conv5-1"")\n        self.dense5_2 = tf.keras.layers.Dense(4, name=""conv5-2"")\n\n        self.flatten = tf.keras.layers.Flatten()\n\n    def call(self, x, training=False):\n        out = self.prelu1(self.conv1(x))\n        out = tf.nn.max_pool2d(out, 3, 2, padding=""SAME"")\n        out = self.prelu2(self.conv2(out))\n        out = tf.nn.max_pool2d(out, 3, 2, padding=""VALID"")\n        out = self.prelu3(self.conv3(out))\n        out = self.flatten(out)\n        out = self.prelu4(self.dense4(out))\n        score = tf.nn.softmax(self.dense5_1(out), -1)\n        boxes = self.dense5_2(out)\n        return boxes, score\n\nclass ONet(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = tf.keras.layers.Conv2D(32, 3, 1, name=""conv1"")\n        self.prelu1 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu1"")\n\n        self.conv2 = tf.keras.layers.Conv2D(64, 3, 1, name=""conv2"")\n        self.prelu2 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu2"")\n\n        self.conv3 = tf.keras.layers.Conv2D(64, 3, 1, name=""conv3"")\n        self.prelu3 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu3"")\n\n        self.conv4 = tf.keras.layers.Conv2D(128, 2, 1, name=""conv4"")\n        self.prelu4 = tf.keras.layers.PReLU(shared_axes=[1,2], name=""prelu4"")\n\n        self.dense5 = tf.keras.layers.Dense(256, name=""conv5"")\n        self.prelu5 = tf.keras.layers.PReLU(shared_axes=None, name=""prelu5"")\n\n        self.dense6_1 = tf.keras.layers.Dense(2  , name=""conv6-1"")\n        self.dense6_2 = tf.keras.layers.Dense(4  , name=""conv6-2"")\n        self.dense6_3 = tf.keras.layers.Dense(10 , name=""conv6-3"")\n\n        self.flatten = tf.keras.layers.Flatten()\n\n    def call(self, x, training=False):\n        out = self.prelu1(self.conv1(x))\n        out = tf.nn.max_pool2d(out, 3, 2, padding=""SAME"")\n        out = self.prelu2(self.conv2(out))\n        out = tf.nn.max_pool2d(out, 3, 2, padding=""VALID"")\n        out = self.prelu3(self.conv3(out))\n        out = tf.nn.max_pool2d(out, 2, 2, padding=""SAME"")\n        out = self.prelu4(self.conv4(out))\n\n\n        out = self.dense5(self.flatten(out))\n        out = self.prelu5(out)\n        score = tf.nn.softmax(self.dense6_1(out))\n        boxes = self.dense6_2(out)\n        lamks = self.dense6_3(out)\n        return boxes, lamks, score\n\n\n'"
4-Object_Detection/MTCNN/utils.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : utils.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-27 01:18:15\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\n\n\ndef detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n    """"""Detects faces in an image, and returns bounding boxes and points for them.\n    img: input image\n    minsize: minimum faces\' size\n    pnet, rnet, onet: caffemodel\n    threshold: threshold=[th1, th2, th3], th1-3 are three steps\'s threshold\n    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n    """"""\n    factor_count=0\n    total_boxes=np.empty((0,9))\n    points=np.empty(0)\n    h=img.shape[0]\n    w=img.shape[1]\n    minl=np.amin([h, w])\n    m=12.0/minsize\n    minl=minl*m\n    # create scale pyramid\n    scales=[]\n    while minl>=12:\n        scales += [m*np.power(factor, factor_count)]\n        minl = minl*factor\n        factor_count += 1\n\n    # first stage\n    for scale in scales:\n        hs=int(np.ceil(h*scale))\n        ws=int(np.ceil(w*scale))\n        im_data = imresample(img, (hs, ws))\n        im_data = (im_data-127.5)*0.0078125\n        img_x = np.expand_dims(im_data, 0)\n        img_y = np.transpose(img_x, (0,2,1,3))\n        out = pnet(img_y)\n        out0 = np.transpose(out[0], (0,2,1,3))\n        out1 = np.transpose(out[1], (0,2,1,3))\n\n        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n\n        # inter-scale nms\n        pick = nms(boxes.copy(), 0.5, \'Union\')\n        if boxes.size>0 and pick.size>0:\n            boxes = boxes[pick,:]\n            total_boxes = np.append(total_boxes, boxes, axis=0)\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n        total_boxes = total_boxes[pick,:]\n        regw = total_boxes[:,2]-total_boxes[:,0]\n        regh = total_boxes[:,3]-total_boxes[:,1]\n        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n        total_boxes = rerec(total_boxes.copy())\n        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        # second stage\n        tempimg = np.zeros((24,24,3,numbox))\n        for k in range(0,numbox):\n            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n            else:\n                return np.empty()\n        tempimg = (tempimg-127.5)*0.0078125\n        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n        out = rnet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        score = out1[1,:]\n        ipass = np.where(score>threshold[1])\n        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n        mv = out0[:,ipass[0]]\n        if total_boxes.shape[0]>0:\n            pick = nms(total_boxes, 0.7, \'Union\')\n            total_boxes = total_boxes[pick,:]\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n            total_boxes = rerec(total_boxes.copy())\n\n    numbox = total_boxes.shape[0]\n    if numbox>0:\n        # third stage\n        total_boxes = np.fix(total_boxes).astype(np.int32)\n        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n        tempimg = np.zeros((48,48,3,numbox))\n        for k in range(0,numbox):\n            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n            else:\n                return np.empty()\n        tempimg = (tempimg-127.5)*0.0078125\n        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n        out = onet(tempimg1)\n        out0 = np.transpose(out[0])\n        out1 = np.transpose(out[1])\n        out2 = np.transpose(out[2])\n        score = out2[1,:]\n        points = out1\n        ipass = np.where(score>threshold[2])\n        points = points[:,ipass[0]]\n        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n        mv = out0[:,ipass[0]]\n\n        w = total_boxes[:,2]-total_boxes[:,0]+1\n        h = total_boxes[:,3]-total_boxes[:,1]+1\n        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n        if total_boxes.shape[0]>0:\n            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n            total_boxes = total_boxes[pick,:]\n            points = points[:,pick]\n\n    return total_boxes, points\n\ndef bbreg(boundingbox,reg):\n    """"""Calibrate bounding boxes""""""\n    if reg.shape[1]==1:\n        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n    w = boundingbox[:,2]-boundingbox[:,0]+1\n    h = boundingbox[:,3]-boundingbox[:,1]+1\n    b1 = boundingbox[:,0]+reg[:,0]*w\n    b2 = boundingbox[:,1]+reg[:,1]*h\n    b3 = boundingbox[:,2]+reg[:,2]*w\n    b4 = boundingbox[:,3]+reg[:,3]*h\n    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n    return boundingbox\n\ndef generateBoundingBox(imap, reg, scale, t):\n    """"""Use heatmap to generate bounding boxes""""""\n    stride=2\n    cellsize=12\n\n    imap = np.transpose(imap)\n    dx1 = np.transpose(reg[:,:,0])\n    dy1 = np.transpose(reg[:,:,1])\n    dx2 = np.transpose(reg[:,:,2])\n    dy2 = np.transpose(reg[:,:,3])\n    y, x = np.where(imap >= t)\n    if y.shape[0]==1:\n        dx1 = np.flipud(dx1)\n        dy1 = np.flipud(dy1)\n        dx2 = np.flipud(dx2)\n        dy2 = np.flipud(dy2)\n    score = imap[(y,x)]\n    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n    if reg.size==0:\n        reg = np.empty((0,3))\n    bb = np.transpose(np.vstack([y,x]))\n    q1 = np.fix((stride*bb+1)/scale)\n    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n    return boundingbox, reg\n\n# function pick = nms(boxes,threshold,type)\ndef nms(boxes, threshold, method):\n    if boxes.size==0:\n        return np.empty((0,3))\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    x2 = boxes[:,2]\n    y2 = boxes[:,3]\n    s = boxes[:,4]\n    area = (x2-x1+1) * (y2-y1+1)\n    I = np.argsort(s)\n    pick = np.zeros_like(s, dtype=np.int16)\n    counter = 0\n    while I.size>0:\n        i = I[-1]\n        pick[counter] = i\n        counter += 1\n        idx = I[0:-1]\n        xx1 = np.maximum(x1[i], x1[idx])\n        yy1 = np.maximum(y1[i], y1[idx])\n        xx2 = np.minimum(x2[i], x2[idx])\n        yy2 = np.minimum(y2[i], y2[idx])\n        w = np.maximum(0.0, xx2-xx1+1)\n        h = np.maximum(0.0, yy2-yy1+1)\n        inter = w * h\n        if method is \'Min\':\n            o = inter / np.minimum(area[i], area[idx])\n        else:\n            o = inter / (area[i] + area[idx] - inter)\n        I = I[np.where(o<=threshold)]\n    pick = pick[0:counter]\n    return pick\n\n# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\ndef pad(total_boxes, w, h):\n    """"""Compute the padding coordinates (pad the bounding boxes to square)""""""\n    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n    numbox = total_boxes.shape[0]\n\n    dx = np.ones((numbox), dtype=np.int32)\n    dy = np.ones((numbox), dtype=np.int32)\n    edx = tmpw.copy().astype(np.int32)\n    edy = tmph.copy().astype(np.int32)\n\n    x = total_boxes[:,0].copy().astype(np.int32)\n    y = total_boxes[:,1].copy().astype(np.int32)\n    ex = total_boxes[:,2].copy().astype(np.int32)\n    ey = total_boxes[:,3].copy().astype(np.int32)\n\n    tmp = np.where(ex>w)\n    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n    ex[tmp] = w\n\n    tmp = np.where(ey>h)\n    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n    ey[tmp] = h\n\n    tmp = np.where(x<1)\n    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n    x[tmp] = 1\n\n    tmp = np.where(y<1)\n    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n    y[tmp] = 1\n\n    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n\n# function [bboxA] = rerec(bboxA)\ndef rerec(bboxA):\n    """"""Convert bboxA to square.""""""\n    h = bboxA[:,3]-bboxA[:,1]\n    w = bboxA[:,2]-bboxA[:,0]\n    l = np.maximum(w, h)\n    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n    return bboxA\n\ndef imresample(img, sz):\n    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n    return im_data\n'"
4-Object_Detection/RPN/demo.py,2,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : demo.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-20 15:06:46\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom utils import compute_iou, plot_boxes_on_image, wandhG, load_gt_boxes, compute_regression, decode_output\n\npos_thresh = 0.5\nneg_thresh = 0.1\niou_thresh = 0.5\ngrid_width = 16\ngrid_height = 16\nimage_height = 720\nimage_width = 960\n\nimage_path = ""./synthetic_dataset/image/1.jpg""\nlabel_path = ""./synthetic_dataset/imageAno/1.txt""\ngt_boxes = load_gt_boxes(label_path)\nraw_image = cv2.imread(image_path)\nimage_with_gt_boxes = np.copy(raw_image)\nplot_boxes_on_image(image_with_gt_boxes, gt_boxes)\nImage.fromarray(image_with_gt_boxes).show()\nencoded_image = np.copy(raw_image)\n\ntarget_scores = np.zeros(shape=[45, 60, 9, 2]) # 0: background, 1: foreground, ,\ntarget_bboxes = np.zeros(shape=[45, 60, 9, 4]) # t_x, t_y, t_w, t_h\ntarget_masks  = np.zeros(shape=[45, 60, 9]) # negative_samples: -1, positive_samples: 1\n\n################################### ENCODE INPUT #################################\n\nfor i in range(45):\n    for j in range(60):\n        for k in range(9):\n            center_x = j * grid_width + grid_width * 0.5\n            center_y = i * grid_height + grid_height * 0.5\n            xmin = center_x - wandhG[k][0] * 0.5\n            ymin = center_y - wandhG[k][1] * 0.5\n            xmax = center_x + wandhG[k][0] * 0.5\n            ymax = center_y + wandhG[k][1] * 0.5\n            # ignore cross-boundary anchors\n            if (xmin > -5) & (ymin > -5) & (xmax < (image_width+5)) & (ymax < (image_height+5)):\n                anchor_boxes = np.array([xmin, ymin, xmax, ymax])\n                anchor_boxes = np.expand_dims(anchor_boxes, axis=0)\n                # compute iou between this anchor and all ground-truth boxes in image.\n                ious = compute_iou(anchor_boxes, gt_boxes)\n                positive_masks = ious > pos_thresh\n                negative_masks = ious < neg_thresh\n\n                if np.any(positive_masks):\n                    plot_boxes_on_image(encoded_image, anchor_boxes, thickness=1)\n                    print(""=> Encoding positive sample: %d, %d, %d"" %(i, j, k))\n                    cv2.circle(encoded_image, center=(int(0.5*(xmin+xmax)), int(0.5*(ymin+ymax))),\n                                    radius=1, color=[255,0,0], thickness=4)\n\n                    target_scores[i, j, k, 1] = 1.\n                    target_masks[i, j, k] = 1 # labeled as a positive sample\n                    # find out which ground-truth box matches this anchor\n                    max_iou_idx = np.argmax(ious)\n                    selected_gt_boxes = gt_boxes[max_iou_idx]\n                    target_bboxes[i, j, k] = compute_regression(selected_gt_boxes, anchor_boxes[0])\n\n                if np.all(negative_masks):\n                    target_scores[i, j, k, 0] = 1.\n                    target_masks[i, j, k] = -1 # labeled as a negative sample\n                    cv2.circle(encoded_image, center=(int(0.5*(xmin+xmax)), int(0.5*(ymin+ymax))),\n                                    radius=1, color=[0,0,0], thickness=4)\n\nImage.fromarray(encoded_image).show()\n\n################################### DECODE OUTPUT #################################\n\ndecode_image = np.copy(raw_image)\npred_boxes = []\npred_score = []\n\nfor i in range(45):\n    for j in range(60):\n        for k in range(9):\n            # pred boxes coordinate\n            center_x = j * grid_width + 0.5 * grid_width\n            center_y = i * grid_height + 0.5 * grid_height\n            anchor_xmin = center_x - 0.5 * wandhG[k, 0]\n            anchor_ymin = center_y - 0.5 * wandhG[k, 1]\n\n            xmin = target_bboxes[i, j, k, 0] * wandhG[k, 0] + anchor_xmin\n            ymin = target_bboxes[i, j, k, 1] * wandhG[k, 1] + anchor_ymin\n            xmax = tf.exp(target_bboxes[i, j, k, 2]) * wandhG[k, 0] + xmin\n            ymax = tf.exp(target_bboxes[i, j, k, 3]) * wandhG[k, 1] + ymin\n\n            if target_scores[i, j, k, 1] > 0: # it is a positive sample\n                print(""=> Decoding positive sample: %d, %d, %d"" %(i, j, k))\n                cv2.circle(decode_image, center=(int(0.5*(xmin+xmax)), int(0.5*(ymin+ymax))),\n                                radius=1, color=[255,0,0], thickness=4)\n                pred_boxes.append(np.array([xmin, ymin, xmax, ymax]))\n                pred_score.append(target_scores[i, j, k, 1])\n\npred_boxes = np.array(pred_boxes)\nplot_boxes_on_image(decode_image, pred_boxes, color=[0, 255, 0])\nImage.fromarray(np.uint8(decode_image)).show()\n\n\n############################## FASTER DECODE OUTPUT ###############################\n\nfaster_decode_image = np.copy(raw_image)\npred_bboxes = np.expand_dims(target_bboxes, 0).astype(np.float32)\npred_scores = np.expand_dims(target_scores, 0).astype(np.float32)\n\npred_scores, pred_bboxes = decode_output(pred_bboxes, pred_scores)\nplot_boxes_on_image(faster_decode_image, pred_bboxes, color=[255, 0, 0]) # red boundig box\nImage.fromarray(np.uint8(faster_decode_image)).show()\n\n'"
4-Object_Detection/RPN/kmeans.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : kmeans.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-19 11:29:04\n#   Description :\n#\n#================================================================\n\nimport glob\nimport numpy as np\nfrom utils import load_gt_boxes\n\ndef iou(box, clusters):\n    \'\'\'\n    :param box:      np.array of shape (2,) containing w and h\n    :param clusters: np.array of shape (N cluster, 2)\n    \'\'\'\n    x = np.minimum(clusters[:, 0], box[0])\n    y = np.minimum(clusters[:, 1], box[1])\n\n    intersection = x * y\n    box_area = box[0] * box[1]\n    cluster_area = clusters[:, 0] * clusters[:, 1]\n\n    iou_ = intersection / (box_area + cluster_area - intersection)\n\n    return iou_\n\ndef kmeans(boxes, k, dist=np.median,seed=1):\n    """"""\n    Calculates k-means clustering with the Intersection over Union (IoU) metric.\n    :param boxes: numpy array of shape (r, 2), where r is the number of rows\n    :param k: number of clusters\n    :param dist: distance function\n    :return: numpy array of shape (k, 2)\n    """"""\n    rows = boxes.shape[0]\n\n    distances     = np.empty((rows, k)) ## N row x N cluster\n    last_clusters = np.zeros((rows,))\n\n    np.random.seed(seed)\n\n    # initialize the cluster centers to be k items\n    clusters = boxes[np.random.choice(rows, k, replace=False)]\n\n    while True:\n        for icluster in range(k): # I made change to lars76\'s code here to make the code faster\n            distances[:,icluster] = 1 - iou(clusters[icluster], boxes)\n\n        nearest_clusters = np.argmin(distances, axis=1)\n        if (last_clusters == nearest_clusters).all():\n            break\n\n        for cluster in range(k):\n            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=0)\n\n        last_clusters = nearest_clusters\n\n    return clusters\n\ndef get_wh_from_boxes(boxes):\n    """"""\n    box shape: [-1, 4], return the width and height of boxes\n    """"""\n    return boxes[..., 2:4] - boxes[..., 0:2]\n\ntext_paths = glob.glob(""./synthetic_dataset/imageAno/*.txt"")\nall_boxes = [load_gt_boxes(path) for path in text_paths]\nall_boxes = np.vstack(all_boxes)\nall_boxes_wh = get_wh_from_boxes(all_boxes)\nanchors = kmeans(all_boxes_wh, k=9)\nprint(anchors)\n\n'"
4-Object_Detection/RPN/rpn.py,31,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : rpn.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-17 13:46:28\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\nclass RPNplus(tf.keras.Model):\n    # VGG_MEAN = [103.939, 116.779, 123.68]\n    def __init__(self):\n        super(RPNplus, self).__init__()\n        # conv1\n        self.conv1_1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n        self.conv1_2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n        self.pool1   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv2\n        self.conv2_1 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.conv2_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.pool2   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv3\n        self.conv3_1 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_2 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.pool3   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv4\n        self.conv4_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool4   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv5\n        self.conv5_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool5   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        ## region_proposal_conv\n        self.region_proposal_conv1 = tf.keras.layers.Conv2D(256, kernel_size=[5,2],\n                                                            activation=tf.nn.relu,\n                                                            padding='same', use_bias=False)\n        self.region_proposal_conv2 = tf.keras.layers.Conv2D(512, kernel_size=[5,2],\n                                                            activation=tf.nn.relu,\n                                                            padding='same', use_bias=False)\n        self.region_proposal_conv3 = tf.keras.layers.Conv2D(512, kernel_size=[5,2],\n                                                            activation=tf.nn.relu,\n                                                            padding='same', use_bias=False)\n        ## Bounding Boxes Regression layer\n        self.bboxes_conv = tf.keras.layers.Conv2D(36, kernel_size=[1,1],\n                                                padding='same', use_bias=False)\n        ## Output Scores layer\n        self.scores_conv = tf.keras.layers.Conv2D(18, kernel_size=[1,1],\n                                                padding='same', use_bias=False)\n\n\n    def call(self, x, training=False):\n        h = self.conv1_1(x)\n        h = self.conv1_2(h)\n        h = self.pool1(h)\n\n        h = self.conv2_1(h)\n        h = self.conv2_2(h)\n        h = self.pool2(h)\n\n        h = self.conv3_1(h)\n        h = self.conv3_2(h)\n        h = self.conv3_3(h)\n        h = self.pool3(h)\n        # Pooling to same size\n        pool3_p = tf.nn.max_pool2d(h, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                                   padding='SAME', name='pool3_proposal')\n        pool3_p = self.region_proposal_conv1(pool3_p) # [1, 45, 60, 256]\n\n        h = self.conv4_1(h)\n        h = self.conv4_2(h)\n        h = self.conv4_3(h)\n        h = self.pool4(h)\n        pool4_p = self.region_proposal_conv2(h) # [1, 45, 60, 512]\n\n        h = self.conv5_1(h)\n        h = self.conv5_2(h)\n        h = self.conv5_3(h)\n        pool5_p = self.region_proposal_conv2(h) # [1, 45, 60, 512]\n\n        region_proposal = tf.concat([pool3_p, pool4_p, pool5_p], axis=-1) # [1, 45, 60, 1280]\n\n        conv_cls_scores = self.scores_conv(region_proposal) # [1, 45, 60, 18]\n        conv_cls_bboxes = self.bboxes_conv(region_proposal) # [1, 45, 60, 36]\n\n        cls_scores = tf.reshape(conv_cls_scores, [-1, 45, 60, 9, 2])\n        cls_bboxes = tf.reshape(conv_cls_bboxes, [-1, 45, 60, 9, 4])\n\n        return cls_scores, cls_bboxes\n\n"""
4-Object_Detection/RPN/test.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : test.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-19 17:21:54\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom rpn import RPNplus\nfrom utils import decode_output, plot_boxes_on_image, nms\n\nsynthetic_dataset_path =""./synthetic_dataset""\nprediction_result_path = ""./prediction""\nif not os.path.exists(prediction_result_path): os.mkdir(prediction_result_path)\n\nmodel = RPNplus()\nfake_data = np.ones(shape=[1, 720, 960, 3]).astype(np.float32)\nmodel(fake_data) # initialize model to load weights\nmodel.load_weights(""./RPN.h5"")\n\nfor idx in range(8000, 8200):\n    image_path = os.path.join(synthetic_dataset_path, ""image/%d.jpg"" %(idx+1))\n    raw_image = cv2.imread(image_path)\n    image_data = np.expand_dims(raw_image / 255., 0)\n    pred_scores, pred_bboxes = model(image_data)\n    pred_scores = tf.nn.softmax(pred_scores, axis=-1)\n    pred_scores, pred_bboxes = decode_output(pred_bboxes, pred_scores, 0.9)\n    pred_bboxes = nms(pred_bboxes, pred_scores, 0.5)\n    plot_boxes_on_image(raw_image, pred_bboxes)\n    save_path = os.path.join(prediction_result_path, str(idx)+"".jpg"")\n    print(""=> saving prediction results into %s"" %save_path)\n    Image.fromarray(raw_image).save(save_path)\n\n'"
4-Object_Detection/RPN/train.py,15,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-17 15:00:25\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport random\nimport tensorflow as tf\nimport numpy as np\nfrom utils import compute_iou, load_gt_boxes, wandhG, compute_regression\nfrom rpn import RPNplus\n\npos_thresh = 0.5\nneg_thresh = 0.1\ngrid_width = grid_height = 16\nimage_height, image_width = 720, 960\n\ndef encode_label(gt_boxes):\n    target_scores = np.zeros(shape=[45, 60, 9, 2]) # 0: background, 1: foreground, ,\n    target_bboxes = np.zeros(shape=[45, 60, 9, 4]) # t_x, t_y, t_w, t_h\n    target_masks  = np.zeros(shape=[45, 60, 9]) # negative_samples: -1, positive_samples: 1\n    for i in range(45): # y: height\n        for j in range(60): # x: width\n            for k in range(9):\n                center_x = j * grid_width + grid_width * 0.5\n                center_y = i * grid_height + grid_height * 0.5\n                xmin = center_x - wandhG[k][0] * 0.5\n                ymin = center_y - wandhG[k][1] * 0.5\n                xmax = center_x + wandhG[k][0] * 0.5\n                ymax = center_y + wandhG[k][1] * 0.5\n                # print(xmin, ymin, xmax, ymax)\n                # ignore cross-boundary anchors\n                if (xmin > -5) & (ymin > -5) & (xmax < (image_width+5)) & (ymax < (image_height+5)):\n                    anchor_boxes = np.array([xmin, ymin, xmax, ymax])\n                    anchor_boxes = np.expand_dims(anchor_boxes, axis=0)\n                    # compute iou between this anchor and all ground-truth boxes in image.\n                    ious = compute_iou(anchor_boxes, gt_boxes)\n                    positive_masks = ious >= pos_thresh\n                    negative_masks = ious <= neg_thresh\n\n                    if np.any(positive_masks):\n                        target_scores[i, j, k, 1] = 1.\n                        target_masks[i, j, k] = 1 # labeled as a positive sample\n                        # find out which ground-truth box matches this anchor\n                        max_iou_idx = np.argmax(ious)\n                        selected_gt_boxes = gt_boxes[max_iou_idx]\n                        target_bboxes[i, j, k] = compute_regression(selected_gt_boxes, anchor_boxes[0])\n\n                    if np.all(negative_masks):\n                        target_scores[i, j, k, 0] = 1.\n                        target_masks[i, j, k] = -1 # labeled as a negative sample\n    return target_scores, target_bboxes, target_masks\n\ndef process_image_label(image_path, label_path):\n    raw_image = cv2.imread(image_path)\n    gt_boxes = load_gt_boxes(label_path)\n    target = encode_label(gt_boxes)\n    return raw_image/255., target\n\ndef create_image_label_path_generator(synthetic_dataset_path):\n    image_num = 8000\n    image_label_paths = [(os.path.join(synthetic_dataset_path, ""image/%d.jpg"" %(idx+1)),\n                          os.path.join(synthetic_dataset_path, ""imageAno/%d.txt""%(idx+1))) for idx in range(image_num)]\n    while True:\n        random.shuffle(image_label_paths)\n        for i in range(image_num):\n            yield image_label_paths[i]\n\ndef DataGenerator(synthetic_dataset_path, batch_size):\n    """"""\n    generate image and mask at the same time\n    """"""\n    image_label_path_generator = create_image_label_path_generator(synthetic_dataset_path)\n    while True:\n        images = np.zeros(shape=[batch_size, image_height, image_width, 3], dtype=np.float)\n        target_scores = np.zeros(shape=[batch_size, 45, 60, 9, 2], dtype=np.float)\n        target_bboxes = np.zeros(shape=[batch_size, 45, 60, 9, 4], dtype=np.float)\n        target_masks  = np.zeros(shape=[batch_size, 45, 60, 9], dtype=np.int)\n\n        for i in range(batch_size):\n            image_path, label_path = next(image_label_path_generator)\n            image, target = process_image_label(image_path, label_path)\n            images[i] = image\n            target_scores[i] = target[0]\n            target_bboxes[i] = target[1]\n            target_masks[i]  = target[2]\n        yield images, target_scores, target_bboxes, target_masks\n\ndef compute_loss(target_scores, target_bboxes, target_masks, pred_scores, pred_bboxes):\n    """"""\n    target_scores shape: [1, 45, 60, 9, 2],  pred_scores shape: [1, 45, 60, 9, 2]\n    target_bboxes shape: [1, 45, 60, 9, 4],  pred_bboxes shape: [1, 45, 60, 9, 4]\n    target_masks  shape: [1, 45, 60, 9]\n    """"""\n    score_loss = tf.nn.softmax_cross_entropy_with_logits(labels=target_scores, logits=pred_scores)\n    foreground_background_mask = (np.abs(target_masks) == 1).astype(np.int)\n    score_loss = tf.reduce_sum(score_loss * foreground_background_mask, axis=[1,2,3]) / np.sum(foreground_background_mask)\n    score_loss = tf.reduce_mean(score_loss)\n\n    boxes_loss = tf.abs(target_bboxes - pred_bboxes)\n    boxes_loss = 0.5 * tf.pow(boxes_loss, 2) * tf.cast(boxes_loss<1, tf.float32) + (boxes_loss - 0.5) * tf.cast(boxes_loss >=1, tf.float32)\n    boxes_loss = tf.reduce_sum(boxes_loss, axis=-1)\n    foreground_mask = (target_masks > 0).astype(np.float32)\n    boxes_loss = tf.reduce_sum(boxes_loss * foreground_mask, axis=[1,2,3]) / np.sum(foreground_mask)\n    boxes_loss = tf.reduce_mean(boxes_loss)\n\n    return score_loss, boxes_loss\n\nEPOCHS = 10\nSTEPS = 4000\nbatch_size = 2\nlambda_scale = 1.\nsynthetic_dataset_path=""./synthetic_dataset""\nTrainSet = DataGenerator(synthetic_dataset_path, batch_size)\n\nmodel = RPNplus()\noptimizer = tf.keras.optimizers.Adam(lr=1e-4)\nwriter = tf.summary.create_file_writer(""./log"")\nglobal_steps = tf.Variable(0, trainable=False, dtype=tf.int64)\n\nfor epoch in range(EPOCHS):\n    for step in range(STEPS):\n        global_steps.assign_add(1)\n        image_data, target_scores, target_bboxes, target_masks = next(TrainSet)\n        with tf.GradientTape() as tape:\n            pred_scores, pred_bboxes = model(image_data)\n            score_loss, boxes_loss = compute_loss(target_scores, target_bboxes, target_masks, pred_scores, pred_bboxes)\n            total_loss = score_loss + lambda_scale * boxes_loss\n            gradients = tape.gradient(total_loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            print(""=> epoch %d  step %d  total_loss: %.6f  score_loss: %.6f  boxes_loss: %.6f"" %(epoch+1, step+1,\n                                                        total_loss.numpy(), score_loss.numpy(), boxes_loss.numpy()))\n        # writing summary data\n        with writer.as_default():\n            tf.summary.scalar(""total_loss"", total_loss, step=global_steps)\n            tf.summary.scalar(""score_loss"", score_loss, step=global_steps)\n            tf.summary.scalar(""boxes_loss"", boxes_loss, step=global_steps)\n        writer.flush()\n    model.save_weights(""RPN.h5"")\n\n\n\n'"
4-Object_Detection/RPN/utils.py,9,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : utils.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-19 23:04:24\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\ngrid_h = 45\ngrid_w = 60\n\n# # original paper anchors\n# wandhG = np.array([[100.0, 100.0], [300.0, 300.0], [500.0, 500.0],\n                   # [200.0, 100.0], [370.0, 185.0], [440.0, 220.0],\n                   # [100.0, 200.0], [185.0, 370.0], [220.0, 440.0]])\n\n# k-means anchors (recommend)\nwandhG = np.array([[ 74., 149.],\n                   [ 34., 149.],\n                   [ 86.,  74.],\n                   [109., 132.],\n                   [172., 183.],\n                   [103., 229.],\n                   [149.,  91.],\n                   [ 51., 132.],\n                   [ 57., 200.]], dtype=np.float32)\n\ndef compute_iou(boxes1, boxes2):\n    """"""(xmin, ymin, xmax, ymax)\n    boxes1 shape:  [-1, 4], boxes2 shape: [-1, 4]\n    """"""\n    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2], )\n    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n    inter_wh = np.maximum(right_down - left_up, 0.0)\n    inter_area = inter_wh[..., 0] * inter_wh[..., 1]\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    union_area = boxes1_area + boxes2_area - inter_area\n    ious = inter_area / union_area\n    return ious\n\ndef plot_boxes_on_image(show_image_with_boxes, boxes, color=[0, 0, 255], thickness=2):\n    for box in boxes:\n        cv2.rectangle(show_image_with_boxes,\n                pt1=(int(box[0]), int(box[1])),\n                pt2=(int(box[2]), int(box[3])), color=color, thickness=thickness)\n    show_image_with_boxes = cv2.cvtColor(show_image_with_boxes, cv2.COLOR_BGR2RGB)\n    return show_image_with_boxes\n\ndef load_gt_boxes(path):\n    """"""\n    Don\'t care about what shit it is. whatever, this function\n    returns many ground truth boxes with the shape of [-1, 4].\n\n    xmin, ymin, xmax, ymax\n    """"""\n    bbs = open(path).readlines()[1:]\n    roi = np.zeros([len(bbs), 4])\n    for iter_, bb in zip(range(len(bbs)), bbs):\n        bb = bb.replace(\'\\n\', \'\').split(\' \')\n        bbtype = bb[0]\n        bba = np.array([float(bb[i]) for i in range(1, 5)])\n        # occ = float(bb[5])\n        # bbv = np.array([float(bb[i]) for i in range(6, 10)])\n        ignore = int(bb[10])\n\n        ignore = ignore or (bbtype != \'person\')\n        ignore = ignore or (bba[3] < 40)\n        bba[2] += bba[0]\n        bba[3] += bba[1]\n\n        roi[iter_, :4] = bba\n    return roi\n\ndef compute_regression(box1, box2):\n    """"""\n    box1: ground-truth boxes\n    box2: anchor boxes\n    """"""\n    target_reg = np.zeros(shape=[4,])\n    w1 = box1[2] - box1[0]\n    h1 = box1[3] - box1[1]\n    w2 = box2[2] - box2[0]\n    h2 = box2[3] - box2[1]\n\n    target_reg[0] = (box1[0] - box2[0]) / w2\n    target_reg[1] = (box1[1] - box2[1]) / h2\n    target_reg[2] = np.log(w1 / w2)\n    target_reg[3] = np.log(h1 / h2)\n\n    return target_reg\n\ndef decode_output(pred_bboxes, pred_scores, score_thresh=0.5):\n    """"""\n    pred_bboxes shape: [1, 45, 60, 9, 4]\n    pred_scores shape: [1, 45, 60, 9, 2]\n    """"""\n    grid_x, grid_y = tf.range(60, dtype=tf.int32), tf.range(45, dtype=tf.int32)\n    grid_x, grid_y = tf.meshgrid(grid_x, grid_y)\n    grid_x, grid_y = tf.expand_dims(grid_x, -1), tf.expand_dims(grid_y, -1)\n    grid_xy = tf.stack([grid_x, grid_y], axis=-1)\n    center_xy = grid_xy * 16 + 8\n    center_xy = tf.cast(center_xy, tf.float32)\n    anchor_xymin = center_xy - 0.5 * wandhG\n\n    xy_min = pred_bboxes[..., 0:2] * wandhG[:, 0:2] + anchor_xymin\n    xy_max = tf.exp(pred_bboxes[..., 2:4]) * wandhG[:, 0:2] + xy_min\n\n    pred_bboxes = tf.concat([xy_min, xy_max], axis=-1)\n    pred_scores = pred_scores[..., 1]\n    score_mask = pred_scores > score_thresh\n    pred_bboxes = tf.reshape(pred_bboxes[score_mask], shape=[-1,4]).numpy()\n    pred_scores = tf.reshape(pred_scores[score_mask], shape=[-1,]).numpy()\n    return  pred_scores, pred_bboxes\n\n\ndef nms(pred_boxes, pred_score, iou_thresh):\n    """"""\n    pred_boxes shape: [-1, 4]\n    pred_score shape: [-1,]\n    """"""\n    selected_boxes = []\n    while len(pred_boxes) > 0:\n        max_idx = np.argmax(pred_score)\n        selected_box = pred_boxes[max_idx]\n        selected_boxes.append(selected_box)\n        pred_boxes = np.concatenate([pred_boxes[:max_idx], pred_boxes[max_idx+1:]])\n        pred_score = np.concatenate([pred_score[:max_idx], pred_score[max_idx+1:]])\n        ious = compute_iou(selected_box, pred_boxes)\n        iou_mask = ious <= 0.1\n        pred_boxes = pred_boxes[iou_mask]\n        pred_score = pred_score[iou_mask]\n\n    selected_boxes = np.array(selected_boxes)\n    return selected_boxes\n\n'"
4-Object_Detection/SSD/ssd.py,30,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : ssd.py\n#   Author      : YunYang1994\n#   Created date: 2019-11-06 16:56:46\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n\nclass SSD(tf.keras.Model):\n    def __init__(self, num_class=21):\n        super(SSD, self).__init__()\n        # conv1\n        self.conv1_1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n        self.conv1_2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n        self.pool1   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv2\n        self.conv2_1 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.conv2_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.pool2   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv3\n        self.conv3_1 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_2 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.pool3   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv4\n        self.conv4_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool4   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same')\n\n        # conv5\n        self.conv5_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool5   = tf.keras.layers.MaxPooling2D(3, strides=1, padding='same')\n\n        # fc6, => vgg backbone is finished. now they are all SSD blocks\n        self.fc6 = tf.keras.layers.Conv2D(1024, 3, dilation_rate=6, activation='relu', padding='same')\n        # fc7\n        self.fc7 = tf.keras.layers.Conv2D(1024, 1, activation='relu', padding='same')\n        # Block 8/9/10/11: 1x1 and 3x3 convolutions strides 2 (except lasts)\n        # conv8\n        self.conv8_1 = tf.keras.layers.Conv2D(256, 1, activation='relu', padding='same')\n        self.conv8_2 = tf.keras.layers.Conv2D(512, 3, strides=2, activation='relu', padding='same')\n        # conv9\n        self.conv9_1 = tf.keras.layers.Conv2D(128, 1, activation='relu', padding='same')\n        self.conv9_2 = tf.keras.layers.Conv2D(256, 3, strides=2, activation='relu', padding='same')\n        # conv10\n        self.conv10_1 = tf.keras.layers.Conv2D(128, 1, activation='relu', padding='same')\n        self.conv10_2 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='valid')\n        # conv11\n        self.conv11_1 = tf.keras.layers.Conv2D(128, 1, activation='relu', padding='same')\n        self.conv11_2 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='valid')\n\n\n\n    def call(self, x, training=False):\n        h = self.conv1_1(x)\n        h = self.conv1_2(h)\n        h = self.pool1(h)\n\n        h = self.conv2_1(h)\n        h = self.conv2_2(h)\n        h = self.pool2(h)\n\n        h = self.conv3_1(h)\n        h = self.conv3_2(h)\n        h = self.conv3_3(h)\n        h = self.pool3(h)\n\n        h = self.conv4_1(h)\n        h = self.conv4_2(h)\n        h = self.conv4_3(h)\n        print(h.shape)\n        h = self.pool4(h)\n\n        h = self.conv5_1(h)\n        h = self.conv5_2(h)\n        h = self.conv5_3(h)\n        h = self.pool5(h)\n\n        h = self.fc6(h)     # [1,19,19,1024]\n        h = self.fc7(h)     # [1,19,19,1024]\n        print(h.shape)\n\n        h = self.conv8_1(h)\n        h = self.conv8_2(h) # [1,10,10, 512]\n        print(h.shape)\n\n        h = self.conv9_1(h)\n        h = self.conv9_2(h) # [1, 5, 5, 256]\n        print(h.shape)\n\n        h = self.conv10_1(h)\n        h = self.conv10_2(h) # [1, 3, 3, 256]\n        print(h.shape)\n\n        h = self.conv11_1(h)\n        h = self.conv11_2(h) # [1, 1, 1, 256]\n        print(h.shape)\n        return h\n\nmodel = SSD(21)\nx = model(tf.ones(shape=[1,300,300,3]))\n\n\n\n\n\n"""
4-Object_Detection/YOLOV3/image_demo.py,4,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : image_demo.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-12 13:07:27\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom core.yolov3 import YOLOv3, decode\nfrom PIL import Image\n\ninput_size   = 416\nimage_path   = ""./docs/kite.jpg""\n\ninput_layer  = tf.keras.layers.Input([input_size, input_size, 3])\nfeature_maps = YOLOv3(input_layer)\n\noriginal_image      = cv2.imread(image_path)\noriginal_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\noriginal_image_size = original_image.shape[:2]\n\nimage_data = utils.image_preporcess(np.copy(original_image), [input_size, input_size])\nimage_data = image_data[np.newaxis, ...].astype(np.float32)\n\nbbox_tensors = []\nfor i, fm in enumerate(feature_maps):\n    bbox_tensor = decode(fm, i)\n    bbox_tensors.append(bbox_tensor)\n\nmodel = tf.keras.Model(input_layer, bbox_tensors)\nutils.load_weights(model, ""./yolov3.weights"")\nmodel.summary()\n\npred_bbox = model.predict(image_data)\npred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\npred_bbox = tf.concat(pred_bbox, axis=0)\nbboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.3)\nbboxes = utils.nms(bboxes, 0.45, method=\'nms\')\n\nimage = utils.draw_bbox(original_image, bboxes)\nimage = Image.fromarray(image)\nimage.show()\n\n\n'"
4-Object_Detection/YOLOV3/test.py,4,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : test.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-19 10:29:34\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\nfrom core.yolov3 import YOLOv3, decode\n\n\nINPUT_SIZE   = 416\nNUM_CLASS    = len(utils.read_class_names(cfg.YOLO.CLASSES))\nCLASSES      = utils.read_class_names(cfg.YOLO.CLASSES)\n\npredicted_dir_path = \'../mAP/predicted\'\nground_truth_dir_path = \'../mAP/ground-truth\'\nif os.path.exists(predicted_dir_path): shutil.rmtree(predicted_dir_path)\nif os.path.exists(ground_truth_dir_path): shutil.rmtree(ground_truth_dir_path)\nif os.path.exists(cfg.TEST.DECTECTED_IMAGE_PATH): shutil.rmtree(cfg.TEST.DECTECTED_IMAGE_PATH)\n\nos.mkdir(predicted_dir_path)\nos.mkdir(ground_truth_dir_path)\nos.mkdir(cfg.TEST.DECTECTED_IMAGE_PATH)\n\n# Build Model\ninput_layer  = tf.keras.layers.Input([INPUT_SIZE, INPUT_SIZE, 3])\nfeature_maps = YOLOv3(input_layer)\n\nbbox_tensors = []\nfor i, fm in enumerate(feature_maps):\n    bbox_tensor = decode(fm, i)\n    bbox_tensors.append(bbox_tensor)\n\nmodel = tf.keras.Model(input_layer, bbox_tensors)\nmodel.load_weights(""./yolov3"")\n\nwith open(cfg.TEST.ANNOT_PATH, \'r\') as annotation_file:\n    for num, line in enumerate(annotation_file):\n        annotation = line.strip().split()\n        image_path = annotation[0]\n        image_name = image_path.split(\'/\')[-1]\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        bbox_data_gt = np.array([list(map(int, box.split(\',\'))) for box in annotation[1:]])\n\n        if len(bbox_data_gt) == 0:\n            bboxes_gt=[]\n            classes_gt=[]\n        else:\n            bboxes_gt, classes_gt = bbox_data_gt[:, :4], bbox_data_gt[:, 4]\n        ground_truth_path = os.path.join(ground_truth_dir_path, str(num) + \'.txt\')\n\n        print(\'=> ground truth of %s:\' % image_name)\n        num_bbox_gt = len(bboxes_gt)\n        with open(ground_truth_path, \'w\') as f:\n            for i in range(num_bbox_gt):\n                class_name = CLASSES[classes_gt[i]]\n                xmin, ymin, xmax, ymax = list(map(str, bboxes_gt[i]))\n                bbox_mess = \' \'.join([class_name, xmin, ymin, xmax, ymax]) + \'\\n\'\n                f.write(bbox_mess)\n                print(\'\\t\' + str(bbox_mess).strip())\n        print(\'=> predict result of %s:\' % image_name)\n        predict_result_path = os.path.join(predicted_dir_path, str(num) + \'.txt\')\n        # Predict Process\n        image_size = image.shape[:2]\n        image_data = utils.image_preporcess(np.copy(image), [INPUT_SIZE, INPUT_SIZE])\n        image_data = image_data[np.newaxis, ...].astype(np.float32)\n\n        pred_bbox = model.predict(image_data)\n        pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n        pred_bbox = tf.concat(pred_bbox, axis=0)\n        bboxes = utils.postprocess_boxes(pred_bbox, image_size, INPUT_SIZE, cfg.TEST.SCORE_THRESHOLD)\n        bboxes = utils.nms(bboxes, cfg.TEST.IOU_THRESHOLD, method=\'nms\')\n\n\n        if cfg.TEST.DECTECTED_IMAGE_PATH is not None:\n            image = utils.draw_bbox(image, bboxes)\n            cv2.imwrite(cfg.TEST.DECTECTED_IMAGE_PATH+image_name, image)\n\n        with open(predict_result_path, \'w\') as f:\n            for bbox in bboxes:\n                coor = np.array(bbox[:4], dtype=np.int32)\n                score = bbox[4]\n                class_ind = int(bbox[5])\n                class_name = CLASSES[class_ind]\n                score = \'%.4f\' % score\n                xmin, ymin, xmax, ymax = list(map(str, coor))\n                bbox_mess = \' \'.join([class_name, score, xmin, ymin, xmax, ymax]) + \'\\n\'\n                f.write(bbox_mess)\n                print(\'\\t\' + str(bbox_mess).strip())\n\n'"
4-Object_Detection/YOLOV3/train.py,13,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-18 09:18:54\n#   Description :\n#\n#================================================================\n\nimport os\nimport time\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom tqdm import tqdm\nfrom core.dataset import Dataset\nfrom core.yolov3 import YOLOv3, decode, compute_loss\nfrom core.config import cfg\n\ntrainset = Dataset(\'train\')\nlogdir = ""./data/log""\nsteps_per_epoch = len(trainset)\nglobal_steps = tf.Variable(1, trainable=False, dtype=tf.int64)\nwarmup_steps = cfg.TRAIN.WARMUP_EPOCHS * steps_per_epoch\ntotal_steps = cfg.TRAIN.EPOCHS * steps_per_epoch\n\ninput_tensor = tf.keras.layers.Input([416, 416, 3])\nconv_tensors = YOLOv3(input_tensor)\n\noutput_tensors = []\nfor i, conv_tensor in enumerate(conv_tensors):\n    pred_tensor = decode(conv_tensor, i)\n    output_tensors.append(conv_tensor)\n    output_tensors.append(pred_tensor)\n\nmodel = tf.keras.Model(input_tensor, output_tensors)\noptimizer = tf.keras.optimizers.Adam()\nif os.path.exists(logdir): shutil.rmtree(logdir)\nwriter = tf.summary.create_file_writer(logdir)\n\ndef train_step(image_data, target):\n    with tf.GradientTape() as tape:\n        pred_result = model(image_data, training=True)\n        giou_loss=conf_loss=prob_loss=0\n\n        # optimizing process\n        for i in range(3):\n            conv, pred = pred_result[i*2], pred_result[i*2+1]\n            loss_items = compute_loss(pred, conv, *target[i], i)\n            giou_loss += loss_items[0]\n            conf_loss += loss_items[1]\n            prob_loss += loss_items[2]\n\n        total_loss = giou_loss + conf_loss + prob_loss\n\n        gradients = tape.gradient(total_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        tf.print(""=> STEP %4d   lr: %.6f   giou_loss: %4.2f   conf_loss: %4.2f   ""\n                 ""prob_loss: %4.2f   total_loss: %4.2f"" %(global_steps, optimizer.lr.numpy(),\n                                                          giou_loss, conf_loss,\n                                                          prob_loss, total_loss))\n        # update learning rate\n        global_steps.assign_add(1)\n        if global_steps < warmup_steps:\n            lr = global_steps / warmup_steps *cfg.TRAIN.LR_INIT\n        else:\n            lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (\n                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))\n            )\n        optimizer.lr.assign(lr.numpy())\n\n        # writing summary data\n        with writer.as_default():\n            tf.summary.scalar(""lr"", optimizer.lr, step=global_steps)\n            tf.summary.scalar(""loss/total_loss"", total_loss, step=global_steps)\n            tf.summary.scalar(""loss/giou_loss"", giou_loss, step=global_steps)\n            tf.summary.scalar(""loss/conf_loss"", conf_loss, step=global_steps)\n            tf.summary.scalar(""loss/prob_loss"", prob_loss, step=global_steps)\n        writer.flush()\n\n\nfor epoch in range(cfg.TRAIN.EPOCHS):\n    for image_data, target in trainset:\n        train_step(image_data, target)\n    model.save_weights(""./yolov3"")\n\n'"
4-Object_Detection/YOLOV3/video_demo.py,4,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : video_demo.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-12 19:36:53\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport time\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom core.yolov3 import YOLOv3, decode\n\n\nvideo_path      = ""./docs/road.mp4""\n# video_path      = 0\nnum_classes     = 80\ninput_size      = 416\n\ninput_layer  = tf.keras.layers.Input([input_size, input_size, 3])\nfeature_maps = YOLOv3(input_layer)\n\nbbox_tensors = []\nfor i, fm in enumerate(feature_maps):\n    bbox_tensor = decode(fm, i)\n    bbox_tensors.append(bbox_tensor)\n\nmodel = tf.keras.Model(input_layer, bbox_tensors)\nutils.load_weights(model, ""./yolov3.weights"")\nmodel.summary()\nvid = cv2.VideoCapture(video_path)\nwhile True:\n    return_value, frame = vid.read()\n    if return_value:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    else:\n        raise ValueError(""No image!"")\n    frame_size = frame.shape[:2]\n    image_data = utils.image_preporcess(np.copy(frame), [input_size, input_size])\n    image_data = image_data[np.newaxis, ...].astype(np.float32)\n\n    prev_time = time.time()\n    pred_bbox = model.predict_on_batch(image_data)\n    curr_time = time.time()\n    exec_time = curr_time - prev_time\n\n    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n    pred_bbox = tf.concat(pred_bbox, axis=0)\n    bboxes = utils.postprocess_boxes(pred_bbox, frame_size, input_size, 0.3)\n    bboxes = utils.nms(bboxes, 0.45, method=\'nms\')\n    image = utils.draw_bbox(frame, bboxes)\n\n    result = np.asarray(image)\n    info = ""time: %.2f ms"" %(1000*exec_time)\n    cv2.putText(result, text=info, org=(50, 70), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                fontScale=1, color=(255, 0, 0), thickness=2)\n    cv2.namedWindow(""result"", cv2.WINDOW_AUTOSIZE)\n    result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    cv2.imshow(""result"", result)\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'): break\n\n\n\n'"
4-Object_Detection/mAP/__init__.py,0,b''
4-Object_Detection/mAP/main.py,0,"b'import glob\nimport json\nimport os\nimport shutil\nimport operator\nimport sys\nimport argparse\n\nMINOVERLAP = 0.5 # default value (defined in the PASCAL VOC2012 challenge)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-na\', \'--no-animation\', help=""no animation is shown."", action=""store_true"")\nparser.add_argument(\'-np\', \'--no-plot\', help=""no plot is shown."", action=""store_true"")\nparser.add_argument(\'-q\', \'--quiet\', help=""minimalistic console output."", action=""store_true"")\n# argparse receiving list of classes to be ignored\nparser.add_argument(\'-i\', \'--ignore\', nargs=\'+\', type=str, help=""ignore a list of classes."")\n# argparse receiving list of classes with specific IoU\nparser.add_argument(\'--set-class-iou\', nargs=\'+\', type=str, help=""set IoU for a specific class."")\nargs = parser.parse_args()\n\n# if there are no classes to ignore then replace None by empty list\nif args.ignore is None:\n  args.ignore = []\n\nspecific_iou_flagged = False\nif args.set_class_iou is not None:\n  specific_iou_flagged = True\n\n# if there are no images then no animation can be shown\nimg_path = \'images\'\nif os.path.exists(img_path): \n  for dirpath, dirnames, files in os.walk(img_path):\n    if not files:\n      # no image files found\n      args.no_animation = True\nelse:\n  args.no_animation = True\n\n# try to import OpenCV if the user didn\'t choose the option --no-animation\nshow_animation = False\nif not args.no_animation:\n  try:\n    import cv2\n    show_animation = True\n  except ImportError:\n    print(""\\""opencv-python\\"" not found, please install to visualize the results."")\n    args.no_animation = True\n\n# try to import Matplotlib if the user didn\'t choose the option --no-plot\ndraw_plot = False\nif not args.no_plot:\n  try:\n    import matplotlib.pyplot as plt\n    draw_plot = True\n  except ImportError:\n    print(""\\""matplotlib\\"" not found, please install it to get the resulting plots."")\n    args.no_plot = True\n\n""""""\n throw error and exit\n""""""\ndef error(msg):\n  print(msg)\n  sys.exit(0)\n\n""""""\n check if the number is a float between 0.0 and 1.0\n""""""\ndef is_float_between_0_and_1(value):\n  try:\n    val = float(value)\n    if val > 0.0 and val < 1.0:\n      return True\n    else:\n      return False\n  except ValueError:\n    return False\n\n""""""\n Calculate the AP given the recall and precision array\n  1st) We compute a version of the measured precision/recall curve with\n       precision monotonically decreasing\n  2nd) We compute the AP as the area under this curve by numerical integration.\n""""""\ndef voc_ap(rec, prec):\n  """"""\n  --- Official matlab code VOC2012---\n  mrec=[0 ; rec ; 1];\n  mpre=[0 ; prec ; 0];\n  for i=numel(mpre)-1:-1:1\n      mpre(i)=max(mpre(i),mpre(i+1));\n  end\n  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  rec.insert(0, 0.0) # insert 0.0 at begining of list\n  rec.append(1.0) # insert 1.0 at end of list\n  mrec = rec[:]\n  prec.insert(0, 0.0) # insert 0.0 at begining of list\n  prec.append(0.0) # insert 0.0 at end of list\n  mpre = prec[:]\n  """"""\n   This part makes the precision monotonically decreasing\n    (goes from the end to the beginning)\n    matlab:  for i=numel(mpre)-1:-1:1\n                mpre(i)=max(mpre(i),mpre(i+1));\n  """"""\n  # matlab indexes start in 1 but python in 0, so I have to do:\n  #   range(start=(len(mpre) - 2), end=0, step=-1)\n  # also the python function range excludes the end, resulting in:\n  #   range(start=(len(mpre) - 2), end=-1, step=-1)\n  for i in range(len(mpre)-2, -1, -1):\n    mpre[i] = max(mpre[i], mpre[i+1])\n  """"""\n   This part creates a list of indexes where the recall changes\n    matlab:  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  """"""\n  i_list = []\n  for i in range(1, len(mrec)):\n    if mrec[i] != mrec[i-1]:\n      i_list.append(i) # if it was matlab would be i + 1\n  """"""\n   The Average Precision (AP) is the area under the curve\n    (numerical integration)\n    matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  ap = 0.0\n  for i in i_list:\n    ap += ((mrec[i]-mrec[i-1])*mpre[i])\n  return ap, mrec, mpre\n\n\n""""""\n Convert the lines of a file to a list\n""""""\ndef file_lines_to_list(path):\n  # open txt file lines to a list\n  with open(path) as f:\n    content = f.readlines()\n  # remove whitespace characters like `\\n` at the end of each line\n  content = [x.strip() for x in content]\n  return content\n\n""""""\n Draws text in image\n""""""\ndef draw_text_in_image(img, text, pos, color, line_width):\n  font = cv2.FONT_HERSHEY_PLAIN\n  fontScale = 1\n  lineType = 1\n  bottomLeftCornerOfText = pos\n  cv2.putText(img, text,\n      bottomLeftCornerOfText,\n      font,\n      fontScale,\n      color,\n      lineType)\n  text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n  return img, (line_width + text_width)\n\n""""""\n Plot - adjust axes\n""""""\ndef adjust_axes(r, t, fig, axes):\n  # get text width for re-scaling\n  bb = t.get_window_extent(renderer=r)\n  text_width_inches = bb.width / fig.dpi\n  # get axis width in inches\n  current_fig_width = fig.get_figwidth()\n  new_fig_width = current_fig_width + text_width_inches\n  propotion = new_fig_width / current_fig_width\n  # get axis limit\n  x_lim = axes.get_xlim()\n  axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n\n""""""\n Draw plot using Matplotlib\n""""""\ndef draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n  # sort the dictionary by decreasing value, into a list of tuples\n  sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n  # unpacking the list of tuples into two lists\n  sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n  # \n  if true_p_bar != """":\n    """"""\n     Special case to draw in (green=true predictions) & (red=false predictions)\n    """"""\n    fp_sorted = []\n    tp_sorted = []\n    for key in sorted_keys:\n      fp_sorted.append(dictionary[key] - true_p_bar[key])\n      tp_sorted.append(true_p_bar[key])\n    plt.barh(range(n_classes), fp_sorted, align=\'center\', color=\'crimson\', label=\'False Predictions\')\n    plt.barh(range(n_classes), tp_sorted, align=\'center\', color=\'forestgreen\', label=\'True Predictions\', left=fp_sorted)\n    # add legend\n    plt.legend(loc=\'lower right\')\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      fp_val = fp_sorted[i]\n      tp_val = tp_sorted[i]\n      fp_str_val = "" "" + str(fp_val)\n      tp_str_val = fp_str_val + "" "" + str(tp_val)\n      # trick to paint multicolor with offset:\n      #   first paint everything and then repaint the first number\n      t = plt.text(val, i, tp_str_val, color=\'forestgreen\', va=\'center\', fontweight=\'bold\')\n      plt.text(val, i, fp_str_val, color=\'crimson\', va=\'center\', fontweight=\'bold\')\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  else:\n    plt.barh(range(n_classes), sorted_values, color=plot_color)\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      str_val = "" "" + str(val) # add a space before\n      if val < 1.0:\n        str_val = "" {0:.2f}"".format(val)\n      t = plt.text(val, i, str_val, color=plot_color, va=\'center\', fontweight=\'bold\')\n      # re-set axes to show number inside the figure\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  # set window title\n  fig.canvas.set_window_title(window_title)\n  # write classes in y axis\n  tick_font_size = 12\n  plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n  """"""\n   Re-scale height accordingly\n  """"""\n  init_height = fig.get_figheight()\n  # comput the matrix height in points and inches\n  dpi = fig.dpi\n  height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n  height_in = height_pt / dpi\n  # compute the required figure height \n  top_margin = 0.15    # in percentage of the figure height\n  bottom_margin = 0.05 # in percentage of the figure height\n  figure_height = height_in / (1 - top_margin - bottom_margin)\n  # set new height\n  if figure_height > init_height:\n    fig.set_figheight(figure_height)\n\n  # set plot title\n  plt.title(plot_title, fontsize=14)\n  # set axis titles\n  # plt.xlabel(\'classes\')\n  plt.xlabel(x_label, fontsize=\'large\')\n  # adjust size of window\n  fig.tight_layout()\n  # save the plot\n  fig.savefig(output_path)\n  # show image\n  if to_show:\n    plt.show()\n  # close the plot\n  plt.close()\n\n""""""\n Create a ""tmp_files/"" and ""results/"" directory\n""""""\ntmp_files_path = ""tmp_files""\nif not os.path.exists(tmp_files_path): # if it doesn\'t exist already\n  os.makedirs(tmp_files_path)\nresults_files_path = ""results""\nif os.path.exists(results_files_path): # if it exist already\n  # reset the results directory\n  shutil.rmtree(results_files_path)\n\nos.makedirs(results_files_path)\nif draw_plot:\n  os.makedirs(results_files_path + ""/classes"")\nif show_animation:\n  os.makedirs(results_files_path + ""/images"")\n  os.makedirs(results_files_path + ""/images/single_predictions"")\n\n""""""\n Ground-Truth\n   Load each of the ground-truth files into a temporary "".json"" file.\n   Create a list of all the class names present in the ground-truth (gt_classes).\n""""""\n# get a list with the ground-truth files\nground_truth_files_list = glob.glob(\'ground-truth/*.txt\')\nif len(ground_truth_files_list) == 0:\n  error(""Error: No ground-truth files found!"")\nground_truth_files_list.sort()\n# dictionary with counter per class\ngt_counter_per_class = {}\n\nfor txt_file in ground_truth_files_list:\n  #print(txt_file)\n  file_id = txt_file.split("".txt"",1)[0]\n  file_id = os.path.basename(os.path.normpath(file_id))\n  # check if there is a correspondent predicted objects file\n  if not os.path.exists(\'predicted/\' + file_id + "".txt""):\n    error_msg = ""Error. File not found: predicted/"" +  file_id + "".txt\\n""\n    error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n    error(error_msg)\n  lines_list = file_lines_to_list(txt_file)\n  # create ground-truth dictionary\n  bounding_boxes = []\n  is_difficult = False\n  for line in lines_list:\n    try:\n      if ""difficult"" in line:\n          class_name, left, top, right, bottom, _difficult = line.split()\n          is_difficult = True\n      else:\n          class_name, left, top, right, bottom = line.split()\n    except ValueError:\n      error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n      error_msg += "" Expected: <class_name> <left> <top> <right> <bottom> [\'difficult\']\\n""\n      error_msg += "" Received: "" + line\n      error_msg += ""\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n""\n      error_msg += ""by running the script \\""remove_space.py\\"" or \\""rename_class.py\\"" in the \\""extra/\\"" folder.""\n      error(error_msg)\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n    if is_difficult:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False, ""difficult"":True})\n        is_difficult = False\n    else:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False})\n        # count that object\n        if class_name in gt_counter_per_class:\n          gt_counter_per_class[class_name] += 1\n        else:\n          # if class didn\'t exist yet\n          gt_counter_per_class[class_name] = 1\n  # dump bounding_boxes into a "".json"" file\n  with open(tmp_files_path + ""/"" + file_id + ""_ground_truth.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\ngt_classes = list(gt_counter_per_class.keys())\n# let\'s sort the classes alphabetically\ngt_classes = sorted(gt_classes)\nn_classes = len(gt_classes)\n#print(gt_classes)\n#print(gt_counter_per_class)\n\n""""""\n Check format of the flag --set-class-iou (if used)\n  e.g. check if class exists\n""""""\nif specific_iou_flagged:\n  n_args = len(args.set_class_iou)\n  error_msg = \\\n    \'\\n --set-class-iou [class_1] [IoU_1] [class_2] [IoU_2] [...]\'\n  if n_args % 2 != 0:\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  # [class_1] [IoU_1] [class_2] [IoU_2]\n  # specific_iou_classes = [\'class_1\', \'class_2\']\n  specific_iou_classes = args.set_class_iou[::2] # even\n  # iou_list = [\'IoU_1\', \'IoU_2\']\n  iou_list = args.set_class_iou[1::2] # odd\n  if len(specific_iou_classes) != len(iou_list):\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  for tmp_class in specific_iou_classes:\n    if tmp_class not in gt_classes:\n          error(\'Error, unknown class \\""\' + tmp_class + \'\\"". Flag usage:\' + error_msg)\n  for num in iou_list:\n    if not is_float_between_0_and_1(num):\n      error(\'Error, IoU must be between 0.0 and 1.0. Flag usage:\' + error_msg)\n\n""""""\n Predicted\n   Load each of the predicted files into a temporary "".json"" file.\n""""""\n# get a list with the predicted files\npredicted_files_list = glob.glob(\'predicted/*.txt\')\npredicted_files_list.sort()\n\nfor class_index, class_name in enumerate(gt_classes):\n  bounding_boxes = []\n  for txt_file in predicted_files_list:\n    #print(txt_file)\n    # the first time it checks if all the corresponding ground-truth files exist\n    file_id = txt_file.split("".txt"",1)[0]\n    file_id = os.path.basename(os.path.normpath(file_id))\n    if class_index == 0:\n      if not os.path.exists(\'ground-truth/\' + file_id + "".txt""):\n        error_msg = ""Error. File not found: ground-truth/"" +  file_id + "".txt\\n""\n        error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n        error(error_msg)\n    lines = file_lines_to_list(txt_file)\n    for line in lines:\n      try:\n        tmp_class_name, confidence, left, top, right, bottom = line.split()\n      except ValueError:\n        error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n        error_msg += "" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n""\n        error_msg += "" Received: "" + line\n        error(error_msg)\n      if tmp_class_name == class_name:\n        #print(""match"")\n        bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n        bounding_boxes.append({""confidence"":confidence, ""file_id"":file_id, ""bbox"":bbox})\n        #print(bounding_boxes)\n  # sort predictions by decreasing confidence\n  bounding_boxes.sort(key=lambda x:float(x[\'confidence\']), reverse=True)\n  with open(tmp_files_path + ""/"" + class_name + ""_predictions.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\n""""""\n Calculate the AP for each class\n""""""\nsum_AP = 0.0\nap_dictionary = {}\n# open file to store the results\nwith open(results_files_path + ""/results.txt"", \'w\') as results_file:\n  results_file.write(""# AP and precision/recall per class\\n"")\n  count_true_positives = {}\n  for class_index, class_name in enumerate(gt_classes):\n    count_true_positives[class_name] = 0\n    """"""\n     Load predictions of that class\n    """"""\n    predictions_file = tmp_files_path + ""/"" + class_name + ""_predictions.json""\n    predictions_data = json.load(open(predictions_file))\n\n    """"""\n     Assign predictions to ground truth objects\n    """"""\n    nd = len(predictions_data)\n    tp = [0] * nd # creates an array of zeros of size nd\n    fp = [0] * nd\n    for idx, prediction in enumerate(predictions_data):\n      file_id = prediction[""file_id""]\n      if show_animation:\n        # find ground truth image\n        ground_truth_img = glob.glob1(img_path, file_id + "".*"")\n        #tifCounter = len(glob.glob1(myPath,""*.tif""))\n        if len(ground_truth_img) == 0:\n          error(""Error. Image not found with id: "" + file_id)\n        elif len(ground_truth_img) > 1:\n          error(""Error. Multiple image with id: "" + file_id)\n        else: # found image\n          #print(img_path + ""/"" + ground_truth_img[0])\n          # Load image\n          img = cv2.imread(img_path + ""/"" + ground_truth_img[0])\n          # load image with draws of multiple detections\n          img_cumulative_path = results_files_path + ""/images/"" + ground_truth_img[0]\n          if os.path.isfile(img_cumulative_path):\n            img_cumulative = cv2.imread(img_cumulative_path)\n          else:\n            img_cumulative = img.copy()\n          # Add bottom border to image\n          bottom_border = 60\n          BLACK = [0, 0, 0]\n          img = cv2.copyMakeBorder(img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n      # assign prediction to ground truth object if any\n      #   open ground-truth with that file_id\n      gt_file = tmp_files_path + ""/"" + file_id + ""_ground_truth.json""\n      ground_truth_data = json.load(open(gt_file))\n      ovmax = -1\n      gt_match = -1\n      # load prediction bounding-box\n      bb = [ float(x) for x in prediction[""bbox""].split() ]\n      for obj in ground_truth_data:\n        # look for a class_name match\n        if obj[""class_name""] == class_name:\n          bbgt = [ float(x) for x in obj[""bbox""].split() ]\n          bi = [max(bb[0],bbgt[0]), max(bb[1],bbgt[1]), min(bb[2],bbgt[2]), min(bb[3],bbgt[3])]\n          iw = bi[2] - bi[0] + 1\n          ih = bi[3] - bi[1] + 1\n          if iw > 0 and ih > 0:\n            # compute overlap (IoU) = area of intersection / area of union\n            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n                    + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n            ov = iw * ih / ua\n            if ov > ovmax:\n              ovmax = ov\n              gt_match = obj\n\n      # assign prediction as true positive/don\'t care/false positive\n      if show_animation:\n        status = ""NO MATCH FOUND!"" # status is only used in the animation\n      # set minimum overlap\n      min_overlap = MINOVERLAP\n      if specific_iou_flagged:\n        if class_name in specific_iou_classes:\n          index = specific_iou_classes.index(class_name)\n          min_overlap = float(iou_list[index])\n      if ovmax >= min_overlap:\n        if ""difficult"" not in gt_match:\n            if not bool(gt_match[""used""]):\n              # true positive\n              tp[idx] = 1\n              gt_match[""used""] = True\n              count_true_positives[class_name] += 1\n              # update the "".json"" file\n              with open(gt_file, \'w\') as f:\n                  f.write(json.dumps(ground_truth_data))\n              if show_animation:\n                status = ""MATCH!""\n            else:\n              # false positive (multiple detection)\n              fp[idx] = 1\n              if show_animation:\n                status = ""REPEATED MATCH!""\n      else:\n        # false positive\n        fp[idx] = 1\n        if ovmax > 0:\n          status = ""INSUFFICIENT OVERLAP""\n\n      """"""\n       Draw image to show animation\n      """"""\n      if show_animation:\n        height, widht = img.shape[:2]\n        # colors (OpenCV works with BGR)\n        white = (255,255,255)\n        light_blue = (255,200,100)\n        green = (0,255,0)\n        light_red = (30,30,255)\n        # 1st line\n        margin = 10\n        v_pos = int(height - margin - (bottom_border / 2))\n        text = ""Image: "" + ground_truth_img[0] + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        text = ""Class ["" + str(class_index) + ""/"" + str(n_classes) + ""]: "" + class_name + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), light_blue, line_width)\n        if ovmax != -1:\n          color = light_red\n          if status == ""INSUFFICIENT OVERLAP"":\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + ""< {0:.2f}% "".format(min_overlap*100)\n          else:\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + "">= {0:.2f}% "".format(min_overlap*100)\n            color = green\n          img, _ = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n        # 2nd line\n        v_pos += int(bottom_border / 2)\n        rank_pos = str(idx+1) # rank position (idx starts at 0)\n        text = ""Prediction #rank: "" + rank_pos + "" confidence: {0:.2f}% "".format(float(prediction[""confidence""])*100)\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        color = light_red\n        if status == ""MATCH!"":\n          color = green\n        text = ""Result: "" + status + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        if ovmax > 0: # if there is intersections between the bounding-boxes\n          bbgt = [ int(x) for x in gt_match[""bbox""].split() ]\n          cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.rectangle(img_cumulative,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.putText(img_cumulative, class_name, (bbgt[0],bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n        bb = [int(i) for i in bb]\n        cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.rectangle(img_cumulative,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.putText(img_cumulative, class_name, (bb[0],bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n        # show image\n        cv2.imshow(""Animation"", img)\n        cv2.waitKey(20) # show for 20 ms\n        # save image to results\n        output_img_path = results_files_path + ""/images/single_predictions/"" + class_name + ""_prediction"" + str(idx) + "".jpg""\n        cv2.imwrite(output_img_path, img)\n        # save the image with all the objects drawn to it\n        cv2.imwrite(img_cumulative_path, img_cumulative)\n\n    #print(tp)\n    # compute precision/recall\n    cumsum = 0\n    for idx, val in enumerate(fp):\n      fp[idx] += cumsum\n      cumsum += val\n    cumsum = 0\n    for idx, val in enumerate(tp):\n      tp[idx] += cumsum\n      cumsum += val\n    #print(tp)\n    rec = tp[:]\n    for idx, val in enumerate(tp):\n      rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n    #print(rec)\n    prec = tp[:]\n    for idx, val in enumerate(tp):\n      prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n    #print(prec)\n\n    ap, mrec, mprec = voc_ap(rec, prec)\n    sum_AP += ap\n    text = ""{0:.2f}%"".format(ap*100) + "" = "" + class_name + "" AP  "" #class_name + "" AP = {0:.2f}%"".format(ap*100)\n    """"""\n     Write to results.txt\n    """"""\n    rounded_prec = [ \'%.2f\' % elem for elem in prec ]\n    rounded_rec = [ \'%.2f\' % elem for elem in rec ]\n    results_file.write(text + ""\\n Precision: "" + str(rounded_prec) + ""\\n Recall   :"" + str(rounded_rec) + ""\\n\\n"")\n    if not args.quiet:\n      print(text)\n    ap_dictionary[class_name] = ap\n\n    """"""\n     Draw plot\n    """"""\n    if draw_plot:\n      plt.plot(rec, prec, \'-o\')\n      # add a new penultimate point to the list (mrec[-2], 0.0)\n      # since the last line segment (and respective area) do not affect the AP value\n      area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n      area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n      plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor=\'r\')\n      # set window title\n      fig = plt.gcf() # gcf - get current figure\n      fig.canvas.set_window_title(\'AP \' + class_name)\n      # set plot title\n      plt.title(\'class: \' + text)\n      #plt.suptitle(\'This is a somewhat long figure title\', fontsize=16)\n      # set axis titles\n      plt.xlabel(\'Recall\')\n      plt.ylabel(\'Precision\')\n      # optional - set axes\n      axes = plt.gca() # gca - get current axes\n      axes.set_xlim([0.0,1.0])\n      axes.set_ylim([0.0,1.05]) # .05 to give some extra space\n      # Alternative option -> wait for button to be pressed\n      #while not plt.waitforbuttonpress(): pass # wait for key display\n      # Alternative option -> normal display\n      #plt.show()\n      # save the plot\n      fig.savefig(results_files_path + ""/classes/"" + class_name + "".png"")\n      plt.cla() # clear axes for next plot\n\n  if show_animation:\n    cv2.destroyAllWindows()\n\n  results_file.write(""\\n# mAP of all classes\\n"")\n  mAP = sum_AP / n_classes\n  text = ""mAP = {0:.2f}%"".format(mAP*100)\n  results_file.write(text + ""\\n"")\n  print(text)\n\n# remove the tmp_files directory\nshutil.rmtree(tmp_files_path)\n\n""""""\n Count total of Predictions\n""""""\n# iterate through all the files\npred_counter_per_class = {}\n#all_classes_predicted_files = set([])\nfor txt_file in predicted_files_list:\n  # get lines to list\n  lines_list = file_lines_to_list(txt_file)\n  for line in lines_list:\n    class_name = line.split()[0]\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    # count that object\n    if class_name in pred_counter_per_class:\n      pred_counter_per_class[class_name] += 1\n    else:\n      # if class didn\'t exist yet\n      pred_counter_per_class[class_name] = 1\n#print(pred_counter_per_class)\npred_classes = list(pred_counter_per_class.keys())\n\n\n""""""\n Plot the total number of occurences of each class in the ground-truth\n""""""\nif draw_plot:\n  window_title = ""Ground-Truth Info""\n  plot_title = ""Ground-Truth\\n""\n  plot_title += ""("" + str(len(ground_truth_files_list)) + "" files and "" + str(n_classes) + "" classes)""\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Ground-Truth Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  draw_plot_func(\n    gt_counter_per_class,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    \'\',\n    )\n\n""""""\n Write number of ground-truth objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results.txt"", \'a\') as results_file:\n  results_file.write(""\\n# Number of ground-truth objects per class\\n"")\n  for class_name in sorted(gt_counter_per_class):\n    results_file.write(class_name + "": "" + str(gt_counter_per_class[class_name]) + ""\\n"")\n\n""""""\n Finish counting true positives\n""""""\nfor class_name in pred_classes:\n  # if class exists in predictions but not in ground-truth then there are no true positives in that class\n  if class_name not in gt_classes:\n    count_true_positives[class_name] = 0\n#print(count_true_positives)\n\n""""""\n Plot the total number of occurences of each class in the ""predicted"" folder\n""""""\nif draw_plot:\n  window_title = ""Predicted Objects Info""\n  # Plot title\n  plot_title = ""Predicted Objects\\n""\n  plot_title += ""("" + str(len(predicted_files_list)) + "" files and ""\n  count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(pred_counter_per_class.values()))\n  plot_title += str(count_non_zero_values_in_dictionary) + "" detected classes)""\n  # end Plot title\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Predicted Objects Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  true_p_bar = count_true_positives\n  draw_plot_func(\n    pred_counter_per_class,\n    len(pred_counter_per_class),\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    true_p_bar\n    )\n\n""""""\n Write number of predicted objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results.txt"", \'a\') as results_file:\n  results_file.write(""\\n# Number of predicted objects per class\\n"")\n  for class_name in sorted(pred_classes):\n    n_pred = pred_counter_per_class[class_name]\n    text = class_name + "": "" + str(n_pred)\n    text += "" (tp:"" + str(count_true_positives[class_name]) + """"\n    text += "", fp:"" + str(n_pred - count_true_positives[class_name]) + "")\\n""\n    results_file.write(text)\n\n""""""\n Draw mAP plot (Show AP\'s of all classes in decreasing order)\n""""""\nif draw_plot:\n  window_title = ""mAP""\n  plot_title = ""mAP = {0:.2f}%"".format(mAP*100)\n  x_label = ""Average Precision""\n  output_path = results_files_path + ""/mAP.png""\n  to_show = True\n  plot_color = \'royalblue\'\n  draw_plot_func(\n    ap_dictionary,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    """"\n    )\n'"
5-Image_Segmentation/FCN/fcn8s.py,31,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : fcn8s.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-12 15:42:20\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\nclass FCN8s(tf.keras.Model):\n    def __init__(self, n_class=21):\n        super(FCN8s, self).__init__()\n        # conv1\n        self.conv1_1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='valid')\n        self.conv1_2 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')\n        self.pool1   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same') # 1/2\n\n        # conv2\n        self.conv2_1 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.conv2_2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')\n        self.pool2   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same') # 1/4\n\n        # conv3\n        self.conv3_1 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_2 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.conv3_3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')\n        self.pool3   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same') # 1/8\n\n        # conv4\n        self.conv4_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv4_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool4   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same') # 1/16\n\n        # conv5\n        self.conv5_1 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_2 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.conv5_3 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')\n        self.pool5   = tf.keras.layers.MaxPooling2D(2, strides=2, padding='same') # 1/32\n\n        # fc6\n        self.fc6 = tf.keras.layers.Conv2D(4096, 7, activation='relu', padding='valid')\n        self.drop6 = tf.keras.layers.Dropout(0.5)\n\n        # fc7\n        self.fc7 = tf.keras.layers.Conv2D(4096, 1, activation='relu', padding='valid')\n        self.drop7 = tf.keras.layers.Dropout(0.5)\n\n        self.socre_fr = tf.keras.layers.Conv2D(n_class, 1)\n        self.score_pool3 = tf.keras.layers.Conv2D(n_class, 1)\n        self.score_pool4 = tf.keras.layers.Conv2D(n_class, 1)\n\n        self.upscore2 = tf.keras.layers.Conv2DTranspose(\n            n_class,  4, strides=2, padding='valid', use_bias=False)\n        self.upscore8 = tf.keras.layers.Conv2DTranspose(\n            n_class, 16, strides=8, padding='valid', use_bias=False)\n        self.upscore_pool4 = tf.keras.layers.Conv2DTranspose(\n            n_class,  4, strides=2, padding='valid', use_bias=False)\n\n    def call(self, x, training=False):\n        h = x\n        h = self.conv1_1(tf.keras.layers.ZeroPadding2D(padding=(100, 100))(h))\n        h = self.conv1_2(h)\n        h = self.pool1(h)\n\n        h = self.conv2_1(h)\n        h = self.conv2_2(h)\n        h = self.pool2(h)\n\n        h = self.conv3_1(h)\n        h = self.conv3_2(h)\n        h = self.conv3_3(h)\n        h = self.pool3(h)\n        pool3 = h # 1/8\n\n        h = self.conv4_1(h)\n        h = self.conv4_2(h)\n        h = self.conv4_3(h)\n        h = self.pool4(h)\n        pool4 = h # 1/16\n\n        h = self.conv5_1(h)\n        h = self.conv5_2(h)\n        h = self.conv5_3(h)\n        h = self.pool5(h)\n\n        h = self.fc6(h)\n        h = self.drop6(h, training)\n\n        h = self.fc7(h)\n        h = self.drop7(h, training)\n\n        h = self.socre_fr(h)\n        h = self.upscore2(h)\n        upscore2 = h # 1/16\n        # print(upscore2.shape)\n\n        h = self.score_pool4(pool4 * 0.01) # XXX: scaling to train at onece\n        h = h[:, 5:5+upscore2.shape[1], 5:5+upscore2.shape[2], :] # channel last\n        score_pool4c = h # 1/16\n\n        h = upscore2 + score_pool4c # 1/16\n        h = self.upscore_pool4(h)\n        upscore_pool4 = h # 1/8\n\n        h = self.score_pool3(pool3 * 0.0001) # XXX: scaling to train at onece\n        h = h[:,\n              9:9+upscore_pool4.shape[1],\n              9:9+upscore_pool4.shape[2], :] # channel last\n        score_pool3c = h # 1/8\n\n        h = upscore_pool4 + score_pool3c # 1/8\n\n        h = self.upscore8(h)\n        h = h[:, 31:31+x.shape[1], 31:31+x.shape[2], :] # channel last\n\n        return tf.nn.softmax(h, axis=-1)\n\n"""
5-Image_Segmentation/FCN/parser_voc.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : parser_voc.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-12 17:50:18\n#   Description :\n#\n#================================================================\n\nimport os\nimport argparse\nimport numpy as np\nfrom utils import colormap\nfrom scipy import misc\n\nif not os.path.exists(""./data""): os.mkdir(""./data"")\nif not os.path.exists(""./data/train_labels""): os.mkdir(""./data/train_labels"")\nif not os.path.exists(""./data/test_labels""): os.mkdir(""./data/test_labels"")\nif not os.path.exists(""./data/prediction""): os.mkdir(""./data/prediction"")\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--voc_path"", type=str, default=""/home/yang/dataset/VOC"")\nflags = parser.parse_args()\nif not os.path.exists(flags.voc_path): # ""/home/yang/dataset/VOC""\n    raise ValueError(""Path: %s does not exist"" %flags.voc_path)\n\nfor mode in [""train"", ""test""]:\n    image_write = open(os.path.join(os.getcwd(), ""data/%s_image.txt"" %mode), ""w"")\n    for year in [2007, 2012]:\n        if mode == ""test"" and year == 2012: continue\n        train_label_folder = os.path.join(flags.voc_path, ""%s/VOCdevkit/VOC%d/SegmentationClass"" %(mode, year))\n        train_image_folder = os.path.join(flags.voc_path, ""%s/VOCdevkit/VOC%d/JPEGImages"" %(mode, year))\n        train_label_images = os.listdir(train_label_folder)\n\n        for train_label_image in train_label_images:\n            label_name = train_label_image[:-4]\n            image_path = os.path.join(train_image_folder, label_name + "".jpg"")\n            if not os.path.exists(image_path): continue\n            image_write.writelines(image_path+""\\n"")\n            label_path = os.path.join(train_label_folder, train_label_image)\n            label_image = np.array(misc.imread(label_path))\n            write_label = open((""./data/%s_labels/"" % mode)+label_name+"".txt"", \'w\')\n            print(""=> processing %s"" %label_path)\n            H, W, C = label_image.shape\n            for i in range(H):\n                write_line = []\n                for j in range(W):\n                    pixel_color = label_image[i, j].tolist()\n                    if pixel_color in colormap:\n                        cls_idx = colormap.index(pixel_color)\n                    else:\n                        cls_idx = 0\n                    write_line.append(str(cls_idx))\n                write_label.writelines("" "".join(write_line) + ""\\n"")\n\n'"
5-Image_Segmentation/FCN/test.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : test.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-23 23:14:38\n#   Description :\n#\n#================================================================\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom fcn8s import FCN8s\nfrom utils import visual_result, DataGenerator\n\nmodel = FCN8s(n_class=21)\nTestSet  = DataGenerator(""./data/test_image.txt"", ""./data/test_labels"", 1)\n\n## load weights and test your model after training\n## if you want to test model, first you need to initialize your model\n## with ""model(data)"", and then load model weights\ndata = np.ones(shape=[1,224,224,3], dtype=np.float)\nmodel(data)\nmodel.load_weights(""FCN8s.h5"")\n\nfor idx, (x, y) in enumerate(TestSet):\n    result = model(x)\n    pred_label = tf.argmax(result, axis=-1)\n    result = visual_result(x[0], pred_label[0].numpy())\n    save_file = ""./data/prediction/%d.jpg"" %idx\n    print(""=> saving prediction result into "", save_file)\n    result.save(save_file)\n    if idx == 209:\n        result.show()\n        break\n\n'"
5-Image_Segmentation/FCN/train.py,2,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-14 19:12:36\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\nfrom fcn8s import FCN8s\nfrom utils import DataGenerator\n\nTrainSet = DataGenerator(""./data/train_image.txt"", ""./data/train_labels"", 2)\nmodel = FCN8s(n_class=21)\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n              loss=\'sparse_categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n## train your FCN8s model\ncallback = tf.keras.callbacks.ModelCheckpoint(""FCN8s.h5"", verbose=1, save_weights_only=True)\nmodel.fit_generator(TrainSet, steps_per_epoch=6000, epochs=30, callbacks=[callback])\n\n\n'"
5-Image_Segmentation/FCN/utils.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : utils.py\n#   Author      : YunYang1994\n#   Created date: 2019-10-12 17:47:24\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport random\nimport numpy as np\n\nfrom PIL import Image\n\nclasses = [\'background\',\'aeroplane\',\'bicycle\',\'bird\',\'boat\',\n           \'bottle\',\'bus\',\'car\',\'cat\',\'chair\',\'cow\',\'diningtable\',\n           \'dog\',\'horse\',\'motorbike\',\'person\',\'potted plant\',\n           \'sheep\',\'sofa\',\'train\',\'tv/monitor\']\n# RGB color for each class\ncolormap = [[0,0,0],[128,0,0],[0,128,0], [128,128,0], [0,0,128],\n            [128,0,128],[0,128,128],[128,128,128],[64,0,0],[192,0,0],\n            [64,128,0],[192,128,0],[64,0,128],[192,0,128],\n            [64,128,128],[192,128,128],[0,64,0],[128,64,0],\n            [0,192,0],[128,192,0],[0,64,128]]\n\nrgb_mean = np.array([0.485, 0.456, 0.406])\nrgb_std = np.array([0.229, 0.224, 0.225])\n\ndef visual_result(image, label, alpha=0.7):\n    """"""\n    image shape -> [H, W, C]\n    label shape -> [H, W]\n    """"""\n    image = (image * rgb_std + rgb_mean) * 255\n    image, label = image.astype(np.int), label.astype(np.int)\n    H, W, C = image.shape\n    masks_color = np.zeros(shape=[H, W, C])\n    inv_masks_color = np.zeros(shape=[H, W, C])\n    cls = []\n    for i in range(H):\n        for j in range(W):\n            cls_idx = label[i, j]\n            masks_color[i, j] = np.array(colormap[cls_idx])\n            cls.append(cls_idx)\n            if classes[cls_idx] == ""background"":\n                inv_masks_color[i, j] = alpha * image[i, j]\n\n    show_image = np.zeros(shape=[224, 672, 3])\n    cls = set(cls)\n    for x in cls:\n        print(""=> "", classes[x])\n    show_image[:, :224, :] = image\n    show_image[:, 224:448, :] = masks_color\n    show_image[:, 448:, :] = (1-alpha)*image + alpha*masks_color + inv_masks_color\n    show_image = Image.fromarray(np.uint8(show_image))\n    return show_image\n\ndef create_image_label_path_generator(images_filepath, labels_filepath):\n    image_paths = open(images_filepath).readlines()\n    all_label_txts = os.listdir(labels_filepath)\n    image_label_paths = []\n    for label_txt in all_label_txts:\n        label_name = label_txt[:-4]\n        label_path = labels_filepath + ""/"" + label_txt\n        for image_path in image_paths:\n            image_path = image_path.rstrip()\n            image_name = image_path.split(""/"")[-1][:-4]\n            if label_name == image_name:\n                image_label_paths.append((image_path, label_path))\n    while True:\n        random.shuffle(image_label_paths)\n        for i in range(len(image_label_paths)):\n            yield image_label_paths[i]\n\ndef process_image_label(image_path, label_path):\n    # image = misc.imread(image_path)\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_NEAREST)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # data augmentation here\n    # randomly shift gamma\n    gamma = random.uniform(0.8, 1.2)\n    image = image.copy() ** gamma\n    image = np.clip(image, 0, 255)\n    # randomly shift brightness\n    brightness = random.uniform(0.5, 2.0)\n    image = image.copy() * brightness\n    image = np.clip(image, 0, 255)\n    # image transformation here\n    image = (image / 255. - rgb_mean) / rgb_std\n\n    label = open(label_path).readlines()\n    label = [np.array(line.rstrip().split("" "")) for line in label]\n    label = np.array(label, dtype=np.int)\n    label = cv2.resize(label, (224, 224), interpolation=cv2.INTER_NEAREST)\n    label = label.astype(np.int)\n\n    return image, label\n\n\ndef DataGenerator(train_image_txt, train_labels_dir, batch_size):\n    """"""\n    generate image and mask at the same time\n    """"""\n    image_label_path_generator = create_image_label_path_generator(\n        train_image_txt, train_labels_dir\n    )\n    while True:\n        images = np.zeros(shape=[batch_size, 224, 224, 3])\n        labels = np.zeros(shape=[batch_size, 224, 224], dtype=np.float)\n        for i in range(batch_size):\n            image_path, label_path = next(image_label_path_generator)\n            image, label = process_image_label(image_path, label_path)\n            images[i], labels[i] = image, label\n        yield images, labels\n'"
5-Image_Segmentation/Unet/Unet.py,0,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : Unet.py\n#   Author      : YunYang1994\n#   Created date: 2019-09-20 16:49:08\n#   Description :\n#\n#================================================================\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D\n\n\ndef Unet(num_class, image_size):\n\n    inputs = Input(shape=[image_size, image_size, 1])\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same')(conv9)\n    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same')(conv9)\n    conv10 = Conv2D(num_class, 1, activation = 'sigmoid')(conv9)\n    model = Model(inputs = inputs, outputs = conv10)\n    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n    return model\n"""
5-Image_Segmentation/Unet/train.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-09-19 15:25:10\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport numpy as np\nfrom Unet import Unet\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef DataGenerator(file_path, batch_size):\n    """"""\n    generate image and mask at the same time\n    use the same seed for image_datagen and mask_datagen\n    to ensure the transformation for image and mask is the same\n    """"""\n    aug_dict = dict(rotation_range=0.2,\n                        width_shift_range=0.05,\n                        height_shift_range=0.05,\n                        shear_range=0.05,\n                        zoom_range=0.05,\n                        horizontal_flip=True,\n                        fill_mode=\'nearest\')\n    aug_dict = dict(horizontal_flip=True,\n                        fill_mode=\'nearest\')\n\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    image_generator = image_datagen.flow_from_directory(\n        file_path,\n        classes=[""images""],\n        color_mode = ""grayscale"",\n        target_size = (256, 256),\n        class_mode = None,\n        batch_size = batch_size, seed=1)\n\n    mask_generator = mask_datagen.flow_from_directory(\n        file_path,\n        classes=[""labels""],\n        color_mode = ""grayscale"",\n        target_size = (256, 256),\n        class_mode = None,\n        batch_size = batch_size, seed=1)\n\n    train_generator = zip(image_generator, mask_generator)\n    for (img,mask) in train_generator:\n        img = img / 255.\n        mask = mask / 255.\n        mask[mask > 0.5] = 1\n        mask[mask <= 0.5] = 0\n        yield (img,mask)\n\nmodel = Unet(1, image_size=256)\ntrainset = DataGenerator(""membrane/train"", batch_size=2)\nmodel.fit_generator(trainset,steps_per_epoch=5000,epochs=5)\nmodel.save_weights(""model.h5"")\n\ntestSet = DataGenerator(""membrane/test"", batch_size=1)\nalpha   = 0.3\nmodel.load_weights(""model.h5"")\nif not os.path.exists(""./results""): os.mkdir(""./results"")\n\nfor idx, (img, mask) in enumerate(testSet):\n    oring_img = img[0]\n    pred_mask = model.predict(img)[0]\n    pred_mask[pred_mask > 0.5] = 1\n    pred_mask[pred_mask <= 0.5] = 0\n    img = cv2.cvtColor(img[0], cv2.COLOR_GRAY2RGB)\n    H, W, C = img.shape\n    for i in range(H):\n        for j in range(W):\n            if pred_mask[i][j][0] <= 0.5:\n                img[i][j] = (1-alpha)*img[i][j]*255 + alpha*np.array([0, 0, 255])\n            else:\n                img[i][j] = img[i][j]*255\n    image_accuracy = np.mean(mask == pred_mask)\n    image_path = ""./results/pred_""+str(idx)+"".png""\n    print(""=> accuracy: %.4f, saving %s"" %(image_accuracy, image_path))\n    cv2.imwrite(image_path, img)\n    cv2.imwrite(""./results/origin_%d.png"" %idx, oring_img*255)\n    if idx == 29: break\n\n\n'"
4-Object_Detection/YOLOV3/core/__init__.py,0,b''
4-Object_Detection/YOLOV3/core/backbone.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : backbone.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-11 23:37:51\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\nimport core.common as common\n\n\ndef darknet53(input_data):\n\n    input_data = common.convolutional(input_data, (3, 3,  3,  32))\n    input_data = common.convolutional(input_data, (3, 3, 32,  64), downsample=True)\n\n    for i in range(1):\n        input_data = common.residual_block(input_data,  64,  32, 64)\n\n    input_data = common.convolutional(input_data, (3, 3,  64, 128), downsample=True)\n\n    for i in range(2):\n        input_data = common.residual_block(input_data, 128,  64, 128)\n\n    input_data = common.convolutional(input_data, (3, 3, 128, 256), downsample=True)\n\n    for i in range(8):\n        input_data = common.residual_block(input_data, 256, 128, 256)\n\n    route_1 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 256, 512), downsample=True)\n\n    for i in range(8):\n        input_data = common.residual_block(input_data, 512, 256, 512)\n\n    route_2 = input_data\n    input_data = common.convolutional(input_data, (3, 3, 512, 1024), downsample=True)\n\n    for i in range(4):\n        input_data = common.residual_block(input_data, 1024, 512, 1024)\n\n    return route_1, route_2, input_data\n\n\n'"
4-Object_Detection/YOLOV3/core/common.py,10,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : common.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-11 23:12:53\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\nclass BatchNormalization(tf.keras.layers.BatchNormalization):\n    """"""\n    ""Frozen state"" and ""inference mode"" are two separate concepts.\n    `layer.trainable = False` is to freeze the layer, so the layer will use\n    stored moving `var` and `mean` in the ""inference mode"", and both `gama`\n    and `beta` will not be updated !\n    """"""\n    def call(self, x, training=False):\n        if not training:\n            training = tf.constant(False)\n        training = tf.logical_and(training, self.trainable)\n        return super().call(x, training)\n\ndef convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):\n    if downsample:\n        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n        padding = \'valid\'\n        strides = 2\n    else:\n        strides = 1\n        padding = \'same\'\n\n    conv = tf.keras.layers.Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides, padding=padding,\n                                  use_bias=not bn, kernel_regularizer=tf.keras.regularizers.l2(0.0005),\n                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n                                  bias_initializer=tf.constant_initializer(0.))(input_layer)\n\n    if bn: conv = BatchNormalization()(conv)\n    if activate == True: conv = tf.nn.leaky_relu(conv, alpha=0.1)\n\n    return conv\n\ndef residual_block(input_layer, input_channel, filter_num1, filter_num2):\n    short_cut = input_layer\n    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))\n    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))\n\n    residual_output = short_cut + conv\n    return residual_output\n\ndef upsample(input_layer):\n    return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method=\'nearest\')\n\n'"
4-Object_Detection/YOLOV3/core/config.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : config.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 13:06:54\n#   Description :\n#\n#================================================================\n\nfrom easydict import EasyDict as edict\n\n\n__C                           = edict()\n# Consumers can get config by: from config import cfg\n\ncfg                           = __C\n\n# YOLO options\n__C.YOLO                      = edict()\n\n# Set the class name\n__C.YOLO.CLASSES              = ""./data/classes/coco.names""\n__C.YOLO.ANCHORS              = ""./data/anchors/basline_anchors.txt""\n__C.YOLO.STRIDES              = [8, 16, 32]\n__C.YOLO.ANCHOR_PER_SCALE     = 3\n__C.YOLO.IOU_LOSS_THRESH      = 0.5\n\n# Train options\n__C.TRAIN                     = edict()\n\n__C.TRAIN.ANNOT_PATH          = ""./data/dataset/yymnist_train.txt""\n__C.TRAIN.BATCH_SIZE          = 4\n# __C.TRAIN.INPUT_SIZE            = [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]\n__C.TRAIN.INPUT_SIZE          = [416]\n__C.TRAIN.DATA_AUG            = True\n__C.TRAIN.LR_INIT             = 1e-3\n__C.TRAIN.LR_END              = 1e-6\n__C.TRAIN.WARMUP_EPOCHS       = 2\n__C.TRAIN.EPOCHS              = 30\n\n\n\n# TEST options\n__C.TEST                      = edict()\n\n__C.TEST.ANNOT_PATH           = ""./data/dataset/yymnist_test.txt""\n__C.TEST.BATCH_SIZE           = 2\n__C.TEST.INPUT_SIZE           = 544\n__C.TEST.DATA_AUG             = False\n__C.TEST.DECTECTED_IMAGE_PATH = ""./data/detection/""\n__C.TEST.SCORE_THRESHOLD      = 0.3\n__C.TEST.IOU_THRESHOLD        = 0.45\n\n\n'"
4-Object_Detection/YOLOV3/core/dataset.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : dataset.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-15 18:05:03\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\n\n\n\nclass Dataset(object):\n    """"""implement Dataset here""""""\n    def __init__(self, dataset_type):\n        self.annot_path  = cfg.TRAIN.ANNOT_PATH if dataset_type == \'train\' else cfg.TEST.ANNOT_PATH\n        self.input_sizes = cfg.TRAIN.INPUT_SIZE if dataset_type == \'train\' else cfg.TEST.INPUT_SIZE\n        self.batch_size  = cfg.TRAIN.BATCH_SIZE if dataset_type == \'train\' else cfg.TEST.BATCH_SIZE\n        self.data_aug    = cfg.TRAIN.DATA_AUG   if dataset_type == \'train\' else cfg.TEST.DATA_AUG\n\n        self.train_input_sizes = cfg.TRAIN.INPUT_SIZE\n        self.strides = np.array(cfg.YOLO.STRIDES)\n        self.classes = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes = len(self.classes)\n        self.anchors = np.array(utils.get_anchors(cfg.YOLO.ANCHORS))\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.max_bbox_per_scale = 150\n\n        self.annotations = self.load_annotations(dataset_type)\n        self.num_samples = len(self.annotations)\n        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n        self.batch_count = 0\n\n\n    def load_annotations(self, dataset_type):\n        with open(self.annot_path, \'r\') as f:\n            txt = f.readlines()\n            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n        np.random.shuffle(annotations)\n        return annotations\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n\n        with tf.device(\'/cpu:0\'):\n            self.train_input_size = random.choice(self.train_input_sizes)\n            self.train_output_sizes = self.train_input_size // self.strides\n\n            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3), dtype=np.float32)\n\n            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n                                          self.anchor_per_scale, 5 + self.num_classes), dtype=np.float32)\n\n            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4), dtype=np.float32)\n\n            num = 0\n            if self.batch_count < self.num_batchs:\n                while num < self.batch_size:\n                    index = self.batch_count * self.batch_size + num\n                    if index >= self.num_samples: index -= self.num_samples\n                    annotation = self.annotations[index]\n                    image, bboxes = self.parse_annotation(annotation)\n                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n\n                    batch_image[num, :, :, :] = image\n                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n                    batch_sbboxes[num, :, :] = sbboxes\n                    batch_mbboxes[num, :, :] = mbboxes\n                    batch_lbboxes[num, :, :] = lbboxes\n                    num += 1\n                self.batch_count += 1\n                batch_smaller_target = batch_label_sbbox, batch_sbboxes\n                batch_medium_target  = batch_label_mbbox, batch_mbboxes\n                batch_larger_target  = batch_label_lbbox, batch_lbboxes\n\n                return batch_image, (batch_smaller_target, batch_medium_target, batch_larger_target)\n            else:\n                self.batch_count = 0\n                np.random.shuffle(self.annotations)\n                raise StopIteration\n\n    def random_horizontal_flip(self, image, bboxes):\n\n        if random.random() < 0.5:\n            _, w, _ = image.shape\n            image = image[:, ::-1, :]\n            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n\n        return image, bboxes\n\n    def random_crop(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n\n            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n\n        return image, bboxes\n\n    def random_translate(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n\n            M = np.array([[1, 0, tx], [0, 1, ty]])\n            image = cv2.warpAffine(image, M, (w, h))\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n\n        return image, bboxes\n\n    def parse_annotation(self, annotation):\n\n        line = annotation.split()\n        image_path = line[0]\n        if not os.path.exists(image_path):\n            raise KeyError(""%s does not exist ... "" %image_path)\n        image = cv2.imread(image_path)\n        bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]])\n\n        if self.data_aug:\n            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n        return image, bboxes\n\n    def bbox_iou(self, boxes1, boxes2):\n\n        boxes1 = np.array(boxes1)\n        boxes2 = np.array(boxes2)\n\n        boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n        boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n        boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n        boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n        left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n        right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n        inter_section = np.maximum(right_down - left_up, 0.0)\n        inter_area = inter_section[..., 0] * inter_section[..., 1]\n        union_area = boxes1_area + boxes2_area - inter_area\n\n        return inter_area / union_area\n\n    def preprocess_true_boxes(self, bboxes):\n\n        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n                           5 + self.num_classes)) for i in range(3)]\n        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n        bbox_count = np.zeros((3,))\n\n        for bbox in bboxes:\n            bbox_coor = bbox[:4]\n            bbox_class_ind = bbox[4]\n\n            onehot = np.zeros(self.num_classes, dtype=np.float)\n            onehot[bbox_class_ind] = 1.0\n            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n            deta = 0.01\n            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n\n            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n\n            iou = []\n            exist_positive = False\n            for i in range(3):\n                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n                anchors_xywh[:, 2:4] = self.anchors[i]\n\n                iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n                iou.append(iou_scale)\n                iou_mask = iou_scale > 0.3\n\n                if np.any(iou_mask):\n                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n\n                    label[i][yind, xind, iou_mask, :] = 0\n                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n\n                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n                    bbox_count[i] += 1\n\n                    exist_positive = True\n\n            if not exist_positive:\n                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n\n                label[best_detect][yind, xind, best_anchor, :] = 0\n                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n\n                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n                bbox_count[best_detect] += 1\n        label_sbbox, label_mbbox, label_lbbox = label\n        sbboxes, mbboxes, lbboxes = bboxes_xywh\n        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n\n    def __len__(self):\n        return self.num_batchs\n\n\n\n\n'"
4-Object_Detection/YOLOV3/core/utils.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : utils.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-12 01:33:38\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport random\nimport colorsys\nimport numpy as np\nfrom core.config import cfg\n\ndef load_weights(model, weights_file):\n    """"""\n    I agree that this code is very ugly, but I don\xe2\x80\x99t know any better way of doing it.\n    """"""\n    wf = open(weights_file, \'rb\')\n    major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n\n    j = 0\n    for i in range(75):\n        conv_layer_name = \'conv2d_%d\' %i if i > 0 else \'conv2d\'\n        bn_layer_name = \'batch_normalization_%d\' %j if j > 0 else \'batch_normalization\'\n\n        conv_layer = model.get_layer(conv_layer_name)\n        filters = conv_layer.filters\n        k_size = conv_layer.kernel_size[0]\n        in_dim = conv_layer.input_shape[-1]\n\n        if i not in [58, 66, 74]:\n            # darknet weights: [beta, gamma, mean, variance]\n            bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n            # tf weights: [gamma, beta, mean, variance]\n            bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n            bn_layer = model.get_layer(bn_layer_name)\n            j += 1\n        else:\n            conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n\n        # darknet shape (out_dim, in_dim, height, width)\n        conv_shape = (filters, in_dim, k_size, k_size)\n        conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n        # tf shape (height, width, in_dim, out_dim)\n        conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n\n        if i not in [58, 66, 74]:\n            conv_layer.set_weights([conv_weights])\n            bn_layer.set_weights(bn_weights)\n        else:\n            conv_layer.set_weights([conv_weights, conv_bias])\n\n    assert len(wf.read()) == 0, \'failed to read all data\'\n    wf.close()\n\n\ndef read_class_names(class_file_name):\n    \'\'\'loads class name from a file\'\'\'\n    names = {}\n    with open(class_file_name, \'r\') as data:\n        for ID, name in enumerate(data):\n            names[ID] = name.strip(\'\\n\')\n    return names\n\n\ndef get_anchors(anchors_path):\n    \'\'\'loads the anchors from a file\'\'\'\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = np.array(anchors.split(\',\'), dtype=np.float32)\n    return anchors.reshape(3, 3, 2)\n\n\ndef image_preporcess(image, target_size, gt_boxes=None):\n\n    ih, iw    = target_size\n    h,  w, _  = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh  = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_paded = image_paded / 255.\n\n    if gt_boxes is None:\n        return image_paded\n\n    else:\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n        return image_paded, gt_boxes\n\n\ndef draw_bbox(image, bboxes, classes=read_class_names(cfg.YOLO.CLASSES), show_label=True):\n    """"""\n    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n    """"""\n\n    num_classes = len(classes)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        fontScale = 0.5\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        bbox_color = colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n\n        if show_label:\n            bbox_mess = \'%s: %.2f\' % (classes[class_ind], score)\n            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n            cv2.rectangle(image, c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3), bbox_color, -1)  # filled\n\n            cv2.putText(image, bbox_mess, (c1[0], c1[1]-2), cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale, (0, 0, 0), bbox_thick//2, lineType=cv2.LINE_AA)\n\n    return image\n\n\n\ndef bboxes_iou(boxes1, boxes2):\n\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = np.maximum(right_down - left_up, 0.0)\n    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n    union_area    = boxes1_area + boxes2_area - inter_area\n    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n\n    return ious\n\n\ndef nms(bboxes, iou_threshold, sigma=0.3, method=\'nms\'):\n    """"""\n    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n\n    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n          https://github.com/bharatsingh430/soft-nms\n    """"""\n    classes_in_img = list(set(bboxes[:, 5]))\n    best_bboxes = []\n\n    for cls in classes_in_img:\n        cls_mask = (bboxes[:, 5] == cls)\n        cls_bboxes = bboxes[cls_mask]\n\n        while len(cls_bboxes) > 0:\n            max_ind = np.argmax(cls_bboxes[:, 4])\n            best_bbox = cls_bboxes[max_ind]\n            best_bboxes.append(best_bbox)\n            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n            weight = np.ones((len(iou),), dtype=np.float32)\n\n            assert method in [\'nms\', \'soft-nms\']\n\n            if method == \'nms\':\n                iou_mask = iou > iou_threshold\n                weight[iou_mask] = 0.0\n\n            if method == \'soft-nms\':\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n            score_mask = cls_bboxes[:, 4] > 0.\n            cls_bboxes = cls_bboxes[score_mask]\n\n    return best_bboxes\n\n\ndef postprocess_boxes(pred_bbox, org_img_shape, input_size, score_threshold):\n\n    valid_scale=[0, np.inf]\n    pred_bbox = np.array(pred_bbox)\n\n    pred_xywh = pred_bbox[:, 0:4]\n    pred_conf = pred_bbox[:, 4]\n    pred_prob = pred_bbox[:, 5:]\n\n    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n    org_h, org_w = org_img_shape\n    resize_ratio = min(input_size / org_w, input_size / org_h)\n\n    dw = (input_size - resize_ratio * org_w) / 2\n    dh = (input_size - resize_ratio * org_h) / 2\n\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n\n    # # (3) clip some boxes those are out of range\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n    pred_coor[invalid_mask] = 0\n\n    # # (4) discard some invalid boxes\n    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n\n    # # (5) discard some boxes with low scores\n    classes = np.argmax(pred_prob, axis=-1)\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n    score_mask = scores > score_threshold\n    mask = np.logical_and(scale_mask, score_mask)\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n\n    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n\n\n\n\n'"
4-Object_Detection/YOLOV3/core/yolov3.py,45,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : yolov3.py\n#   Author      : YunYang1994\n#   Created date: 2019-07-12 13:47:10\n#   Description :\n#\n#================================================================\n\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nimport core.common as common\nimport core.backbone as backbone\nfrom core.config import cfg\n\n\nNUM_CLASS       = len(utils.read_class_names(cfg.YOLO.CLASSES))\nANCHORS         = utils.get_anchors(cfg.YOLO.ANCHORS)\nSTRIDES         = np.array(cfg.YOLO.STRIDES)\nIOU_LOSS_THRESH = cfg.YOLO.IOU_LOSS_THRESH\n\ndef YOLOv3(input_layer):\n    route_1, route_2, conv = backbone.darknet53(input_layer)\n\n    conv = common.convolutional(conv, (1, 1, 1024,  512))\n    conv = common.convolutional(conv, (3, 3,  512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024,  512))\n    conv = common.convolutional(conv, (3, 3,  512, 1024))\n    conv = common.convolutional(conv, (1, 1, 1024,  512))\n\n    conv_lobj_branch = common.convolutional(conv, (3, 3, 512, 1024))\n    conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(conv, (1, 1,  512,  256))\n    conv = common.upsample(conv)\n\n    conv = tf.concat([conv, route_2], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 768, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n    conv = common.convolutional(conv, (3, 3, 256, 512))\n    conv = common.convolutional(conv, (1, 1, 512, 256))\n\n    conv_mobj_branch = common.convolutional(conv, (3, 3, 256, 512))\n    conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\n\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.upsample(conv)\n\n    conv = tf.concat([conv, route_1], axis=-1)\n\n    conv = common.convolutional(conv, (1, 1, 384, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n    conv = common.convolutional(conv, (3, 3, 128, 256))\n    conv = common.convolutional(conv, (1, 1, 256, 128))\n\n    conv_sobj_branch = common.convolutional(conv, (3, 3, 128, 256))\n    conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\n\n    return [conv_sbbox, conv_mbbox, conv_lbbox]\n\ndef decode(conv_output, i=0):\n    """"""\n    return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes]\n            contains (x, y, w, h, score, probability)\n    """"""\n\n    conv_shape       = tf.shape(conv_output)\n    batch_size       = conv_shape[0]\n    output_size      = conv_shape[1]\n\n    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n\n    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\n    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\n    conv_raw_conf = conv_output[:, :, :, :, 4:5]\n    conv_raw_prob = conv_output[:, :, :, :, 5: ]\n\n    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])\n    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])\n\n    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n    xy_grid = tf.cast(xy_grid, tf.float32)\n\n    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\n    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\n    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n\n    pred_conf = tf.sigmoid(conv_raw_conf)\n    pred_prob = tf.sigmoid(conv_raw_prob)\n\n    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n\ndef bbox_iou(boxes1, boxes2):\n\n    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = tf.maximum(right_down - left_up, 0.0)\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\n    union_area = boxes1_area + boxes2_area - inter_area\n\n    return 1.0 * inter_area / union_area\n\ndef bbox_giou(boxes1, boxes2):\n\n    boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n    boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n                        tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n    boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n                        tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = tf.maximum(right_down - left_up, 0.0)\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\n    union_area = boxes1_area + boxes2_area - inter_area\n    iou = inter_area / union_area\n\n    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n    enclose_area = enclose[..., 0] * enclose[..., 1]\n    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n\n    return giou\n\n\ndef compute_loss(pred, conv, label, bboxes, i=0):\n\n    conv_shape  = tf.shape(conv)\n    batch_size  = conv_shape[0]\n    output_size = conv_shape[1]\n    input_size  = STRIDES[i] * output_size\n    conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\n\n    conv_raw_conf = conv[:, :, :, :, 4:5]\n    conv_raw_prob = conv[:, :, :, :, 5:]\n\n    pred_xywh     = pred[:, :, :, :, 0:4]\n    pred_conf     = pred[:, :, :, :, 4:5]\n\n    label_xywh    = label[:, :, :, :, 0:4]\n    respond_bbox  = label[:, :, :, :, 4:5]\n    label_prob    = label[:, :, :, :, 5:]\n\n    giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\n    input_size = tf.cast(input_size, tf.float32)\n\n    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n    giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n\n    iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n\n    respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 )\n\n    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n\n    conf_loss = conf_focal * (\n            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n            +\n            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n    )\n\n    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n\n    giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n\n    return giou_loss, conf_loss, prob_loss\n\n\n\n\n\n'"
