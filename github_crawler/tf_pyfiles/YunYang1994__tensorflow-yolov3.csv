file_path,api_count,code
convert_weight.py,18,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : convert_weight.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 13:51:31\n#   Description :\n#\n#================================================================\n\nimport argparse\nimport tensorflow as tf\nfrom core.yolov3 import YOLOV3\nfrom core.config import cfg\nparser = argparse.ArgumentParser()\nparser.add_argument(""--train_from_coco"", action=\'store_true\')\nflag = parser.parse_args()\n\norg_weights_path = cfg.YOLO.ORIGINAL_WEIGHT\ncur_weights_path = cfg.YOLO.DEMO_WEIGHT\npreserve_cur_names = [\'conv_sbbox\', \'conv_mbbox\', \'conv_lbbox\']\npreserve_org_names = [\'Conv_6\', \'Conv_14\', \'Conv_22\']\n\n\norg_weights_mess = []\ntf.Graph().as_default()\nload = tf.train.import_meta_graph(org_weights_path + \'.meta\')\nwith tf.Session() as sess:\n    load.restore(sess, org_weights_path)\n    for var in tf.global_variables():\n        var_name = var.op.name\n        var_name_mess = str(var_name).split(\'/\')\n        var_shape = var.shape\n        if flag.train_from_coco:\n            if (var_name_mess[-1] not in [\'weights\', \'gamma\', \'beta\', \'moving_mean\', \'moving_variance\']) or \\\n                    (var_name_mess[1] == \'yolo-v3\' and (var_name_mess[-2] in preserve_org_names)): continue\n        org_weights_mess.append([var_name, var_shape])\n        print(""=> "" + str(var_name).ljust(50), var_shape)\nprint()\ntf.reset_default_graph()\n\ncur_weights_mess = []\ntf.Graph().as_default()\nwith tf.name_scope(\'input\'):\n    input_data = tf.placeholder(dtype=tf.float32, shape=(1, 416, 416, 3), name=\'input_data\')\n    training = tf.placeholder(dtype=tf.bool, name=\'trainable\')\nmodel = YOLOV3(input_data, training)\nfor var in tf.global_variables():\n    var_name = var.op.name\n    var_name_mess = str(var_name).split(\'/\')\n    var_shape = var.shape\n    print(var_name_mess[0])\n    if flag.train_from_coco:\n        if var_name_mess[0] in preserve_cur_names: continue\n    cur_weights_mess.append([var_name, var_shape])\n    print(""=> "" + str(var_name).ljust(50), var_shape)\n\norg_weights_num = len(org_weights_mess)\ncur_weights_num = len(cur_weights_mess)\nif cur_weights_num != org_weights_num:\n    raise RuntimeError\n\nprint(\'=> Number of weights that will rename:\\t%d\' % cur_weights_num)\ncur_to_org_dict = {}\nfor index in range(org_weights_num):\n    org_name, org_shape = org_weights_mess[index]\n    cur_name, cur_shape = cur_weights_mess[index]\n    if cur_shape != org_shape:\n        print(org_weights_mess[index])\n        print(cur_weights_mess[index])\n        raise RuntimeError\n    cur_to_org_dict[cur_name] = org_name\n    print(""=> "" + str(cur_name).ljust(50) + \' : \' + org_name)\n\nwith tf.name_scope(\'load_save\'):\n    name_to_var_dict = {var.op.name: var for var in tf.global_variables()}\n    restore_dict = {cur_to_org_dict[cur_name]: name_to_var_dict[cur_name] for cur_name in cur_to_org_dict}\n    load = tf.train.Saver(restore_dict)\n    save = tf.train.Saver(tf.global_variables())\n    for var in tf.global_variables():\n        print(""=> "" + var.op.name)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(\'=> Restoring weights from:\\t %s\' % org_weights_path)\n    load.restore(sess, org_weights_path)\n    save.save(sess, cur_weights_path)\ntf.reset_default_graph()\n\n\n'"
evaluate.py,7,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : evaluate.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-21 15:30:26\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\nfrom core.yolov3 import YOLOV3\n\nclass YoloTest(object):\n    def __init__(self):\n        self.input_size       = cfg.TEST.INPUT_SIZE\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.classes          = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes      = len(self.classes)\n        self.anchors          = np.array(utils.get_anchors(cfg.YOLO.ANCHORS))\n        self.score_threshold  = cfg.TEST.SCORE_THRESHOLD\n        self.iou_threshold    = cfg.TEST.IOU_THRESHOLD\n        self.moving_ave_decay = cfg.YOLO.MOVING_AVE_DECAY\n        self.annotation_path  = cfg.TEST.ANNOT_PATH\n        self.weight_file      = cfg.TEST.WEIGHT_FILE\n        self.write_image      = cfg.TEST.WRITE_IMAGE\n        self.write_image_path = cfg.TEST.WRITE_IMAGE_PATH\n        self.show_label       = cfg.TEST.SHOW_LABEL\n\n        with tf.name_scope('input'):\n            self.input_data = tf.placeholder(dtype=tf.float32, name='input_data')\n            self.trainable  = tf.placeholder(dtype=tf.bool,    name='trainable')\n\n        model = YOLOV3(self.input_data, self.trainable)\n        self.pred_sbbox, self.pred_mbbox, self.pred_lbbox = model.pred_sbbox, model.pred_mbbox, model.pred_lbbox\n\n        with tf.name_scope('ema'):\n            ema_obj = tf.train.ExponentialMovingAverage(self.moving_ave_decay)\n\n        self.sess  = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n        self.saver = tf.train.Saver(ema_obj.variables_to_restore())\n        self.saver.restore(self.sess, self.weight_file)\n\n    def predict(self, image):\n\n        org_image = np.copy(image)\n        org_h, org_w, _ = org_image.shape\n\n        image_data = utils.image_preporcess(image, [self.input_size, self.input_size])\n        image_data = image_data[np.newaxis, ...]\n\n        pred_sbbox, pred_mbbox, pred_lbbox = self.sess.run(\n            [self.pred_sbbox, self.pred_mbbox, self.pred_lbbox],\n            feed_dict={\n                self.input_data: image_data,\n                self.trainable: False\n            }\n        )\n\n        pred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + self.num_classes)),\n                                    np.reshape(pred_mbbox, (-1, 5 + self.num_classes)),\n                                    np.reshape(pred_lbbox, (-1, 5 + self.num_classes))], axis=0)\n        bboxes = utils.postprocess_boxes(pred_bbox, (org_h, org_w), self.input_size, self.score_threshold)\n        bboxes = utils.nms(bboxes, self.iou_threshold)\n\n        return bboxes\n\n    def evaluate(self):\n        predicted_dir_path = './mAP/predicted'\n        ground_truth_dir_path = './mAP/ground-truth'\n        if os.path.exists(predicted_dir_path): shutil.rmtree(predicted_dir_path)\n        if os.path.exists(ground_truth_dir_path): shutil.rmtree(ground_truth_dir_path)\n        if os.path.exists(self.write_image_path): shutil.rmtree(self.write_image_path)\n        os.mkdir(predicted_dir_path)\n        os.mkdir(ground_truth_dir_path)\n        os.mkdir(self.write_image_path)\n\n        with open(self.annotation_path, 'r') as annotation_file:\n            for num, line in enumerate(annotation_file):\n                annotation = line.strip().split()\n                image_path = annotation[0]\n                image_name = image_path.split('/')[-1]\n                image = cv2.imread(image_path)\n                bbox_data_gt = np.array([list(map(int, box.split(','))) for box in annotation[1:]])\n\n                if len(bbox_data_gt) == 0:\n                    bboxes_gt=[]\n                    classes_gt=[]\n                else:\n                    bboxes_gt, classes_gt = bbox_data_gt[:, :4], bbox_data_gt[:, 4]\n                ground_truth_path = os.path.join(ground_truth_dir_path, str(num) + '.txt')\n\n                print('=> ground truth of %s:' % image_name)\n                num_bbox_gt = len(bboxes_gt)\n                with open(ground_truth_path, 'w') as f:\n                    for i in range(num_bbox_gt):\n                        class_name = self.classes[classes_gt[i]]\n                        xmin, ymin, xmax, ymax = list(map(str, bboxes_gt[i]))\n                        bbox_mess = ' '.join([class_name, xmin, ymin, xmax, ymax]) + '\\n'\n                        f.write(bbox_mess)\n                        print('\\t' + str(bbox_mess).strip())\n                print('=> predict result of %s:' % image_name)\n                predict_result_path = os.path.join(predicted_dir_path, str(num) + '.txt')\n                bboxes_pr = self.predict(image)\n\n                if self.write_image:\n                    image = utils.draw_bbox(image, bboxes_pr, show_label=self.show_label)\n                    cv2.imwrite(self.write_image_path+image_name, image)\n\n                with open(predict_result_path, 'w') as f:\n                    for bbox in bboxes_pr:\n                        coor = np.array(bbox[:4], dtype=np.int32)\n                        score = bbox[4]\n                        class_ind = int(bbox[5])\n                        class_name = self.classes[class_ind]\n                        score = '%.4f' % score\n                        xmin, ymin, xmax, ymax = list(map(str, coor))\n                        bbox_mess = ' '.join([class_name, score, xmin, ymin, xmax, ymax]) + '\\n'\n                        f.write(bbox_mess)\n                        print('\\t' + str(bbox_mess).strip())\n\n    def voc_2012_test(self, voc2012_test_path):\n\n        img_inds_file = os.path.join(voc2012_test_path, 'ImageSets', 'Main', 'test.txt')\n        with open(img_inds_file, 'r') as f:\n            txt = f.readlines()\n            image_inds = [line.strip() for line in txt]\n\n        results_path = 'results/VOC2012/Main'\n        if os.path.exists(results_path):\n            shutil.rmtree(results_path)\n        os.makedirs(results_path)\n\n        for image_ind in image_inds:\n            image_path = os.path.join(voc2012_test_path, 'JPEGImages', image_ind + '.jpg')\n            image = cv2.imread(image_path)\n\n            print('predict result of %s:' % image_ind)\n            bboxes_pr = self.predict(image)\n            for bbox in bboxes_pr:\n                coor = np.array(bbox[:4], dtype=np.int32)\n                score = bbox[4]\n                class_ind = int(bbox[5])\n                class_name = self.classes[class_ind]\n                score = '%.4f' % score\n                xmin, ymin, xmax, ymax = list(map(str, coor))\n                bbox_mess = ' '.join([image_ind, score, xmin, ymin, xmax, ymax]) + '\\n'\n                with open(os.path.join(results_path, 'comp4_det_test_' + class_name + '.txt'), 'a') as f:\n                    f.write(bbox_mess)\n                print('\\t' + str(bbox_mess).strip())\n\n\nif __name__ == '__main__': YoloTest().evaluate()\n\n\n\n"""
freeze_graph.py,6,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : freeze_graph.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-20 15:57:33\n#   Description :\n#\n#================================================================\n\n\nimport tensorflow as tf\nfrom core.yolov3 import YOLOV3\n\npb_file = ""./yolov3_coco.pb""\nckpt_file = ""./checkpoint/yolov3_coco_demo.ckpt""\noutput_node_names = [""input/input_data"", ""pred_sbbox/concat_2"", ""pred_mbbox/concat_2"", ""pred_lbbox/concat_2""]\n\nwith tf.name_scope(\'input\'):\n    input_data = tf.placeholder(dtype=tf.float32, name=\'input_data\')\n\nmodel = YOLOV3(input_data, trainable=False)\nprint(model.conv_sbbox, model.conv_mbbox, model.conv_lbbox)\n\nsess  = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\nsaver = tf.train.Saver()\nsaver.restore(sess, ckpt_file)\n\nconverted_graph_def = tf.graph_util.convert_variables_to_constants(sess,\n                            input_graph_def  = sess.graph.as_graph_def(),\n                            output_node_names = output_node_names)\n\nwith tf.gfile.GFile(pb_file, ""wb"") as f:\n    f.write(converted_graph_def.SerializeToString())\n\n\n\n\n'"
from_darknet_weights_to_ckpt.py,8,"b'import tensorflow as tf\nfrom core.yolov3 import YOLOV3\n\niput_size = 416\ndarknet_weights = \'<your yolov3.weights\' path>\'\nckpt_file = \'./checkpoint/yolov3_coco.ckpt\'\n\ndef load_weights(var_list, weights_file):\n    """"""\n    Loads and converts pre-trained weights.\n    :param var_list: list of network variables.\n    :param weights_file: name of the binary file.\n    :return: list of assign ops\n    """"""\n    with open(weights_file, ""rb"") as fp:\n        _ = np.fromfile(fp, dtype=np.int32, count=5)\n        weights = np.fromfile(fp, dtype=np.float32)  # np.ndarray\n    print(\'weights_num:\', weights.shape[0])\n    ptr = 0\n    i = 0\n    assign_ops = []\n    while i < len(var_list) - 1:\n        var1 = var_list[i]\n        var2 = var_list[i + 1]\n        # do something only if we process conv layer\n        if \'conv\' in var1.name.split(\'/\')[-2]:\n            # check type of next layer\n            if \'batch_normalization\' in var2.name.split(\'/\')[-2]:\n                # load batch norm params\n                gamma, beta, mean, var = var_list[i + 1:i + 5]\n                batch_norm_vars = [beta, gamma, mean, var]\n                for vari in batch_norm_vars:\n                    shape = vari.shape.as_list()\n                    num_params = np.prod(shape)\n                    vari_weights = weights[ptr:ptr + num_params].reshape(shape)\n                    ptr += num_params\n                    assign_ops.append(\n                        tf.assign(vari, vari_weights, validate_shape=True))\n                i += 4\n            elif \'conv\' in var2.name.split(\'/\')[-2]:\n                # load biases\n                bias = var2\n                bias_shape = bias.shape.as_list()\n                bias_params = np.prod(bias_shape)\n                bias_weights = weights[ptr:ptr +\n                                           bias_params].reshape(bias_shape)\n                ptr += bias_params\n                assign_ops.append(\n                    tf.assign(bias, bias_weights, validate_shape=True))\n                i += 1\n            shape = var1.shape.as_list()\n            num_params = np.prod(shape)\n\n            var_weights = weights[ptr:ptr + num_params].reshape(\n                (shape[3], shape[2], shape[0], shape[1]))\n            # remember to transpose to column-major\n            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n            ptr += num_params\n            assign_ops.append(\n                tf.assign(var1, var_weights, validate_shape=True))\n            i += 1\n    print(\'ptr:\', ptr)\n    return assign_ops\n    \nwith tf.name_scope(\'input\'):\n    input_data = tf.placeholder(dtype=tf.float32,shape=(None, iput_size, iput_size, 3), name=\'input_data\')\nmodel = YOLOV3(input_data, trainable=False)\nload_ops = load_weights(tf.global_variables(), darknet_weights)\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    sess.run(load_ops)\n    save_path = saver.save(sess, save_path=ckpt_file)\n    print(\'Model saved in path: {}\'.format(save_path))\n'"
from_darknet_weights_to_pb.py,7,"b'import tensorflow as tf\nfrom core.yolov3 import YOLOV3\nfrom from_darknet_weights_to_ckpt import load_weights\n\ninput_size = 416\ndarknet_weights = \'<your darknet weights file path>\'\npb_file = \'./yolov3.pb\'\noutput_node_names = [""input/input_data"", ""pred_sbbox/concat_2"", ""pred_mbbox/concat_2"", ""pred_lbbox/concat_2""]\n\nwith tf.name_scope(\'input\'):\n    input_data = tf.placeholder(dtype=tf.float32, shape=(None, input_size, input_size, 3), name=\'input_data\')\nmodel = YOLOV3(input_data, trainable=False)\nload_ops = load_weights(tf.global_variables(), darknet_weights)\n\nwith tf.Session() as sess:\n    sess.run(load_ops)\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,\n        tf.get_default_graph().as_graph_def(),\n        output_node_names=output_node_names\n    )\n\n    with tf.gfile.GFile(output_graph, ""wb"") as f:\n        f.write(output_graph_def.SerializeToString())\n\n    print(""{} ops written to {}."".format(len(output_graph_def.node), output_graph))\n'"
image_demo.py,2,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : image_demo.py\n#   Author      : YunYang1994\n#   Created date: 2019-01-20 16:06:06\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom PIL import Image\n\nreturn_elements = [""input/input_data:0"", ""pred_sbbox/concat_2:0"", ""pred_mbbox/concat_2:0"", ""pred_lbbox/concat_2:0""]\npb_file         = ""./yolov3_coco.pb""\nimage_path      = ""./docs/images/road.jpeg""\nnum_classes     = 80\ninput_size      = 416\ngraph           = tf.Graph()\n\noriginal_image = cv2.imread(image_path)\noriginal_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\noriginal_image_size = original_image.shape[:2]\nimage_data = utils.image_preporcess(np.copy(original_image), [input_size, input_size])\nimage_data = image_data[np.newaxis, ...]\n\nreturn_tensors = utils.read_pb_return_tensors(graph, pb_file, return_elements)\n\n\nwith tf.Session(graph=graph) as sess:\n    pred_sbbox, pred_mbbox, pred_lbbox = sess.run(\n        [return_tensors[1], return_tensors[2], return_tensors[3]],\n                feed_dict={ return_tensors[0]: image_data})\n\npred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + num_classes)),\n                            np.reshape(pred_mbbox, (-1, 5 + num_classes)),\n                            np.reshape(pred_lbbox, (-1, 5 + num_classes))], axis=0)\n\nbboxes = utils.postprocess_boxes(pred_bbox, original_image_size, input_size, 0.3)\nbboxes = utils.nms(bboxes, 0.45, method=\'nms\')\nimage = utils.draw_bbox(original_image, bboxes)\nimage = Image.fromarray(image)\nimage.show()\n\n\n\n\n'"
train.py,49,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : train.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 17:50:26\n#   Description :\n#\n#================================================================\n\nimport os\nimport time\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom tqdm import tqdm\nfrom core.dataset import Dataset\nfrom core.yolov3 import YOLOV3\nfrom core.config import cfg\n\n\nclass YoloTrain(object):\n    def __init__(self):\n        self.anchor_per_scale    = cfg.YOLO.ANCHOR_PER_SCALE\n        self.classes             = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes         = len(self.classes)\n        self.learn_rate_init     = cfg.TRAIN.LEARN_RATE_INIT\n        self.learn_rate_end      = cfg.TRAIN.LEARN_RATE_END\n        self.first_stage_epochs  = cfg.TRAIN.FISRT_STAGE_EPOCHS\n        self.second_stage_epochs = cfg.TRAIN.SECOND_STAGE_EPOCHS\n        self.warmup_periods      = cfg.TRAIN.WARMUP_EPOCHS\n        self.initial_weight      = cfg.TRAIN.INITIAL_WEIGHT\n        self.time                = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n        self.moving_ave_decay    = cfg.YOLO.MOVING_AVE_DECAY\n        self.max_bbox_per_scale  = 150\n        self.train_logdir        = ""./data/log/train""\n        self.trainset            = Dataset(\'train\')\n        self.testset             = Dataset(\'test\')\n        self.steps_per_period    = len(self.trainset)\n        self.sess                = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n\n        with tf.name_scope(\'define_input\'):\n            self.input_data   = tf.placeholder(dtype=tf.float32, name=\'input_data\')\n            self.label_sbbox  = tf.placeholder(dtype=tf.float32, name=\'label_sbbox\')\n            self.label_mbbox  = tf.placeholder(dtype=tf.float32, name=\'label_mbbox\')\n            self.label_lbbox  = tf.placeholder(dtype=tf.float32, name=\'label_lbbox\')\n            self.true_sbboxes = tf.placeholder(dtype=tf.float32, name=\'sbboxes\')\n            self.true_mbboxes = tf.placeholder(dtype=tf.float32, name=\'mbboxes\')\n            self.true_lbboxes = tf.placeholder(dtype=tf.float32, name=\'lbboxes\')\n            self.trainable     = tf.placeholder(dtype=tf.bool, name=\'training\')\n\n        with tf.name_scope(""define_loss""):\n            self.model = YOLOV3(self.input_data, self.trainable)\n            self.net_var = tf.global_variables()\n            self.giou_loss, self.conf_loss, self.prob_loss = self.model.compute_loss(\n                                                    self.label_sbbox,  self.label_mbbox,  self.label_lbbox,\n                                                    self.true_sbboxes, self.true_mbboxes, self.true_lbboxes)\n            self.loss = self.giou_loss + self.conf_loss + self.prob_loss\n\n        with tf.name_scope(\'learn_rate\'):\n            self.global_step = tf.Variable(1.0, dtype=tf.float64, trainable=False, name=\'global_step\')\n            warmup_steps = tf.constant(self.warmup_periods * self.steps_per_period,\n                                        dtype=tf.float64, name=\'warmup_steps\')\n            train_steps = tf.constant( (self.first_stage_epochs + self.second_stage_epochs)* self.steps_per_period,\n                                        dtype=tf.float64, name=\'train_steps\')\n            self.learn_rate = tf.cond(\n                pred=self.global_step < warmup_steps,\n                true_fn=lambda: self.global_step / warmup_steps * self.learn_rate_init,\n                false_fn=lambda: self.learn_rate_end + 0.5 * (self.learn_rate_init - self.learn_rate_end) *\n                                    (1 + tf.cos(\n                                        (self.global_step - warmup_steps) / (train_steps - warmup_steps) * np.pi))\n            )\n            global_step_update = tf.assign_add(self.global_step, 1.0)\n\n        with tf.name_scope(""define_weight_decay""):\n            moving_ave = tf.train.ExponentialMovingAverage(self.moving_ave_decay).apply(tf.trainable_variables())\n\n        with tf.name_scope(""define_first_stage_train""):\n            self.first_stage_trainable_var_list = []\n            for var in tf.trainable_variables():\n                var_name = var.op.name\n                var_name_mess = str(var_name).split(\'/\')\n                if var_name_mess[0] in [\'conv_sbbox\', \'conv_mbbox\', \'conv_lbbox\']:\n                    self.first_stage_trainable_var_list.append(var)\n\n            first_stage_optimizer = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss,\n                                                      var_list=self.first_stage_trainable_var_list)\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                with tf.control_dependencies([first_stage_optimizer, global_step_update]):\n                    with tf.control_dependencies([moving_ave]):\n                        self.train_op_with_frozen_variables = tf.no_op()\n\n        with tf.name_scope(""define_second_stage_train""):\n            second_stage_trainable_var_list = tf.trainable_variables()\n            second_stage_optimizer = tf.train.AdamOptimizer(self.learn_rate).minimize(self.loss,\n                                                      var_list=second_stage_trainable_var_list)\n\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                with tf.control_dependencies([second_stage_optimizer, global_step_update]):\n                    with tf.control_dependencies([moving_ave]):\n                        self.train_op_with_all_variables = tf.no_op()\n\n        with tf.name_scope(\'loader_and_saver\'):\n            self.loader = tf.train.Saver(self.net_var)\n            self.saver  = tf.train.Saver(tf.global_variables(), max_to_keep=10)\n\n        with tf.name_scope(\'summary\'):\n            tf.summary.scalar(""learn_rate"",      self.learn_rate)\n            tf.summary.scalar(""giou_loss"",  self.giou_loss)\n            tf.summary.scalar(""conf_loss"",  self.conf_loss)\n            tf.summary.scalar(""prob_loss"",  self.prob_loss)\n            tf.summary.scalar(""total_loss"", self.loss)\n\n            logdir = ""./data/log/""\n            if os.path.exists(logdir): shutil.rmtree(logdir)\n            os.mkdir(logdir)\n            self.write_op = tf.summary.merge_all()\n            self.summary_writer  = tf.summary.FileWriter(logdir, graph=self.sess.graph)\n\n\n    def train(self):\n        self.sess.run(tf.global_variables_initializer())\n        try:\n            print(\'=> Restoring weights from: %s ... \' % self.initial_weight)\n            self.loader.restore(self.sess, self.initial_weight)\n        except:\n            print(\'=> %s does not exist !!!\' % self.initial_weight)\n            print(\'=> Now it starts to train YOLOV3 from scratch ...\')\n            self.first_stage_epochs = 0\n\n        for epoch in range(1, 1+self.first_stage_epochs+self.second_stage_epochs):\n            if epoch <= self.first_stage_epochs:\n                train_op = self.train_op_with_frozen_variables\n            else:\n                train_op = self.train_op_with_all_variables\n\n            pbar = tqdm(self.trainset)\n            train_epoch_loss, test_epoch_loss = [], []\n\n            for train_data in pbar:\n                _, summary, train_step_loss, global_step_val = self.sess.run(\n                    [train_op, self.write_op, self.loss, self.global_step],feed_dict={\n                                                self.input_data:   train_data[0],\n                                                self.label_sbbox:  train_data[1],\n                                                self.label_mbbox:  train_data[2],\n                                                self.label_lbbox:  train_data[3],\n                                                self.true_sbboxes: train_data[4],\n                                                self.true_mbboxes: train_data[5],\n                                                self.true_lbboxes: train_data[6],\n                                                self.trainable:    True,\n                })\n\n                train_epoch_loss.append(train_step_loss)\n                self.summary_writer.add_summary(summary, global_step_val)\n                pbar.set_description(""train loss: %.2f"" %train_step_loss)\n\n            for test_data in self.testset:\n                test_step_loss = self.sess.run( self.loss, feed_dict={\n                                                self.input_data:   test_data[0],\n                                                self.label_sbbox:  test_data[1],\n                                                self.label_mbbox:  test_data[2],\n                                                self.label_lbbox:  test_data[3],\n                                                self.true_sbboxes: test_data[4],\n                                                self.true_mbboxes: test_data[5],\n                                                self.true_lbboxes: test_data[6],\n                                                self.trainable:    False,\n                })\n\n                test_epoch_loss.append(test_step_loss)\n\n            train_epoch_loss, test_epoch_loss = np.mean(train_epoch_loss), np.mean(test_epoch_loss)\n            ckpt_file = ""./checkpoint/yolov3_test_loss=%.4f.ckpt"" % test_epoch_loss\n            log_time = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n            print(""=> Epoch: %2d Time: %s Train loss: %.2f Test loss: %.2f Saving %s ...""\n                            %(epoch, log_time, train_epoch_loss, test_epoch_loss, ckpt_file))\n            self.saver.save(self.sess, ckpt_file, global_step=epoch)\n\n\n\nif __name__ == \'__main__\': YoloTrain().train()\n\n\n\n\n'"
video_demo.py,2,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2018 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : video_demo.py\n#   Author      : YunYang1994\n#   Created date: 2018-11-30 15:56:37\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport time\nimport numpy as np\nimport core.utils as utils\nimport tensorflow as tf\nfrom PIL import Image\n\n\nreturn_elements = [""input/input_data:0"", ""pred_sbbox/concat_2:0"", ""pred_mbbox/concat_2:0"", ""pred_lbbox/concat_2:0""]\npb_file         = ""./yolov3_coco.pb""\nvideo_path      = ""./docs/images/road.mp4""\n# video_path      = 0\nnum_classes     = 80\ninput_size      = 416\ngraph           = tf.Graph()\nreturn_tensors  = utils.read_pb_return_tensors(graph, pb_file, return_elements)\n\nwith tf.Session(graph=graph) as sess:\n    vid = cv2.VideoCapture(video_path)\n    while True:\n        return_value, frame = vid.read()\n        if return_value:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            image = Image.fromarray(frame)\n        else:\n            raise ValueError(""No image!"")\n        frame_size = frame.shape[:2]\n        image_data = utils.image_preporcess(np.copy(frame), [input_size, input_size])\n        image_data = image_data[np.newaxis, ...]\n        prev_time = time.time()\n\n        pred_sbbox, pred_mbbox, pred_lbbox = sess.run(\n            [return_tensors[1], return_tensors[2], return_tensors[3]],\n                    feed_dict={ return_tensors[0]: image_data})\n\n        pred_bbox = np.concatenate([np.reshape(pred_sbbox, (-1, 5 + num_classes)),\n                                    np.reshape(pred_mbbox, (-1, 5 + num_classes)),\n                                    np.reshape(pred_lbbox, (-1, 5 + num_classes))], axis=0)\n\n        bboxes = utils.postprocess_boxes(pred_bbox, frame_size, input_size, 0.3)\n        bboxes = utils.nms(bboxes, 0.45, method=\'nms\')\n        image = utils.draw_bbox(frame, bboxes)\n\n        curr_time = time.time()\n        exec_time = curr_time - prev_time\n        result = np.asarray(image)\n        info = ""time: %.2f ms"" %(1000*exec_time)\n        cv2.namedWindow(""result"", cv2.WINDOW_AUTOSIZE)\n        result = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        cv2.imshow(""result"", result)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'): break\n\n\n\n\n'"
core/__init__.py,0,b''
core/backbone.py,1,"b""#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : backbone.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-17 11:03:35\n#   Description :\n#\n#================================================================\n\nimport core.common as common\nimport tensorflow as tf\n\n\ndef darknet53(input_data, trainable):\n\n    with tf.variable_scope('darknet'):\n\n        input_data = common.convolutional(input_data, filters_shape=(3, 3,  3,  32), trainable=trainable, name='conv0')\n        input_data = common.convolutional(input_data, filters_shape=(3, 3, 32,  64),\n                                          trainable=trainable, name='conv1', downsample=True)\n\n        for i in range(1):\n            input_data = common.residual_block(input_data,  64,  32, 64, trainable=trainable, name='residual%d' %(i+0))\n\n        input_data = common.convolutional(input_data, filters_shape=(3, 3,  64, 128),\n                                          trainable=trainable, name='conv4', downsample=True)\n\n        for i in range(2):\n            input_data = common.residual_block(input_data, 128,  64, 128, trainable=trainable, name='residual%d' %(i+1))\n\n        input_data = common.convolutional(input_data, filters_shape=(3, 3, 128, 256),\n                                          trainable=trainable, name='conv9', downsample=True)\n\n        for i in range(8):\n            input_data = common.residual_block(input_data, 256, 128, 256, trainable=trainable, name='residual%d' %(i+3))\n\n        route_1 = input_data\n        input_data = common.convolutional(input_data, filters_shape=(3, 3, 256, 512),\n                                          trainable=trainable, name='conv26', downsample=True)\n\n        for i in range(8):\n            input_data = common.residual_block(input_data, 512, 256, 512, trainable=trainable, name='residual%d' %(i+11))\n\n        route_2 = input_data\n        input_data = common.convolutional(input_data, filters_shape=(3, 3, 512, 1024),\n                                          trainable=trainable, name='conv43', downsample=True)\n\n        for i in range(4):\n            input_data = common.residual_block(input_data, 1024, 512, 1024, trainable=trainable, name='residual%d' %(i+19))\n\n        return route_1, route_2, input_data\n\n\n\n\n"""
core/common.py,22,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : common.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 09:56:29\n#   Description :\n#\n#================================================================\n\nimport tensorflow as tf\n\n\ndef convolutional(input_data, filters_shape, trainable, name, downsample=False, activate=True, bn=True):\n\n    with tf.variable_scope(name):\n        if downsample:\n            pad_h, pad_w = (filters_shape[0] - 2) // 2 + 1, (filters_shape[1] - 2) // 2 + 1\n            paddings = tf.constant([[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n            input_data = tf.pad(input_data, paddings, \'CONSTANT\')\n            strides = (1, 2, 2, 1)\n            padding = \'VALID\'\n        else:\n            strides = (1, 1, 1, 1)\n            padding = ""SAME""\n\n        weight = tf.get_variable(name=\'weight\', dtype=tf.float32, trainable=True,\n                                 shape=filters_shape, initializer=tf.random_normal_initializer(stddev=0.01))\n        conv = tf.nn.conv2d(input=input_data, filter=weight, strides=strides, padding=padding)\n\n        if bn:\n            conv = tf.layers.batch_normalization(conv, beta_initializer=tf.zeros_initializer(),\n                                                 gamma_initializer=tf.ones_initializer(),\n                                                 moving_mean_initializer=tf.zeros_initializer(),\n                                                 moving_variance_initializer=tf.ones_initializer(), training=trainable)\n        else:\n            bias = tf.get_variable(name=\'bias\', shape=filters_shape[-1], trainable=True,\n                                   dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n            conv = tf.nn.bias_add(conv, bias)\n\n        if activate == True: conv = tf.nn.leaky_relu(conv, alpha=0.1)\n\n    return conv\n\n\ndef residual_block(input_data, input_channel, filter_num1, filter_num2, trainable, name):\n\n    short_cut = input_data\n\n    with tf.variable_scope(name):\n        input_data = convolutional(input_data, filters_shape=(1, 1, input_channel, filter_num1),\n                                   trainable=trainable, name=\'conv1\')\n        input_data = convolutional(input_data, filters_shape=(3, 3, filter_num1,   filter_num2),\n                                   trainable=trainable, name=\'conv2\')\n\n        residual_output = input_data + short_cut\n\n    return residual_output\n\n\n\ndef route(name, previous_output, current_output):\n\n    with tf.variable_scope(name):\n        output = tf.concat([current_output, previous_output], axis=-1)\n\n    return output\n\n\ndef upsample(input_data, name, method=""deconv""):\n    assert method in [""resize"", ""deconv""]\n\n    if method == ""resize"":\n        with tf.variable_scope(name):\n            input_shape = tf.shape(input_data)\n            output = tf.image.resize_nearest_neighbor(input_data, (input_shape[1] * 2, input_shape[2] * 2))\n\n    if method == ""deconv"":\n        # replace resize_nearest_neighbor with conv2d_transpose To support TensorRT optimization\n        numm_filter = input_data.shape.as_list()[-1]\n        output = tf.layers.conv2d_transpose(input_data, numm_filter, kernel_size=2, padding=\'same\',\n                                            strides=(2,2), kernel_initializer=tf.random_normal_initializer())\n\n    return output\n\n\n\n'"
core/config.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : config.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 13:06:54\n#   Description :\n#\n#================================================================\n\nfrom easydict import EasyDict as edict\n\n\n__C                             = edict()\n# Consumers can get config by: from config import cfg\n\ncfg                             = __C\n\n# YOLO options\n__C.YOLO                        = edict()\n\n# Set the class name\n__C.YOLO.CLASSES                = ""./data/classes/coco.names""\n__C.YOLO.ANCHORS                = ""./data/anchors/basline_anchors.txt""\n__C.YOLO.MOVING_AVE_DECAY       = 0.9995\n__C.YOLO.STRIDES                = [8, 16, 32]\n__C.YOLO.ANCHOR_PER_SCALE       = 3\n__C.YOLO.IOU_LOSS_THRESH        = 0.5\n__C.YOLO.UPSAMPLE_METHOD        = ""resize""\n__C.YOLO.ORIGINAL_WEIGHT        = ""./checkpoint/yolov3_coco.ckpt""\n__C.YOLO.DEMO_WEIGHT            = ""./checkpoint/yolov3_coco_demo.ckpt""\n\n# Train options\n__C.TRAIN                       = edict()\n\n__C.TRAIN.ANNOT_PATH            = ""./data/dataset/voc_train.txt""\n__C.TRAIN.BATCH_SIZE            = 6\n__C.TRAIN.INPUT_SIZE            = [320, 352, 384, 416, 448, 480, 512, 544, 576, 608]\n__C.TRAIN.DATA_AUG              = True\n__C.TRAIN.LEARN_RATE_INIT       = 1e-4\n__C.TRAIN.LEARN_RATE_END        = 1e-6\n__C.TRAIN.WARMUP_EPOCHS         = 2\n__C.TRAIN.FISRT_STAGE_EPOCHS    = 20\n__C.TRAIN.SECOND_STAGE_EPOCHS   = 30\n__C.TRAIN.INITIAL_WEIGHT        = ""./checkpoint/yolov3_coco_demo.ckpt""\n\n\n\n# TEST options\n__C.TEST                        = edict()\n\n__C.TEST.ANNOT_PATH             = ""./data/dataset/voc_test.txt""\n__C.TEST.BATCH_SIZE             = 2\n__C.TEST.INPUT_SIZE             = 544\n__C.TEST.DATA_AUG               = False\n__C.TEST.WRITE_IMAGE            = True\n__C.TEST.WRITE_IMAGE_PATH       = ""./data/detection/""\n__C.TEST.WRITE_IMAGE_SHOW_LABEL = True\n__C.TEST.WEIGHT_FILE            = ""./checkpoint/yolov3_test_loss=9.2099.ckpt-5""\n__C.TEST.SHOW_LABEL             = True\n__C.TEST.SCORE_THRESHOLD        = 0.3\n__C.TEST.IOU_THRESHOLD          = 0.45\n\n\n\n\n\n\n'"
core/dataset.py,1,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : dataset.py\n#   Author      : YunYang1994\n#   Created date: 2019-03-15 18:05:03\n#   Description :\n#\n#================================================================\n\nimport os\nimport cv2\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nfrom core.config import cfg\n\n\n\nclass Dataset(object):\n    """"""implement Dataset here""""""\n    def __init__(self, dataset_type):\n        self.annot_path  = cfg.TRAIN.ANNOT_PATH if dataset_type == \'train\' else cfg.TEST.ANNOT_PATH\n        self.input_sizes = cfg.TRAIN.INPUT_SIZE if dataset_type == \'train\' else cfg.TEST.INPUT_SIZE\n        self.batch_size  = cfg.TRAIN.BATCH_SIZE if dataset_type == \'train\' else cfg.TEST.BATCH_SIZE\n        self.data_aug    = cfg.TRAIN.DATA_AUG   if dataset_type == \'train\' else cfg.TEST.DATA_AUG\n\n        self.train_input_sizes = cfg.TRAIN.INPUT_SIZE\n        self.strides = np.array(cfg.YOLO.STRIDES)\n        self.classes = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_classes = len(self.classes)\n        self.anchors = np.array(utils.get_anchors(cfg.YOLO.ANCHORS))\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.max_bbox_per_scale = 150\n\n        self.annotations = self.load_annotations(dataset_type)\n        self.num_samples = len(self.annotations)\n        self.num_batchs = int(np.ceil(self.num_samples / self.batch_size))\n        self.batch_count = 0\n\n\n    def load_annotations(self, dataset_type):\n        with open(self.annot_path, \'r\') as f:\n            txt = f.readlines()\n            annotations = [line.strip() for line in txt if len(line.strip().split()[1:]) != 0]\n        np.random.shuffle(annotations)\n        return annotations\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n\n        with tf.device(\'/cpu:0\'):\n            self.train_input_size = random.choice(self.train_input_sizes)\n            self.train_output_sizes = self.train_input_size // self.strides\n\n            batch_image = np.zeros((self.batch_size, self.train_input_size, self.train_input_size, 3))\n\n            batch_label_sbbox = np.zeros((self.batch_size, self.train_output_sizes[0], self.train_output_sizes[0],\n                                          self.anchor_per_scale, 5 + self.num_classes))\n            batch_label_mbbox = np.zeros((self.batch_size, self.train_output_sizes[1], self.train_output_sizes[1],\n                                          self.anchor_per_scale, 5 + self.num_classes))\n            batch_label_lbbox = np.zeros((self.batch_size, self.train_output_sizes[2], self.train_output_sizes[2],\n                                          self.anchor_per_scale, 5 + self.num_classes))\n\n            batch_sbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4))\n            batch_mbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4))\n            batch_lbboxes = np.zeros((self.batch_size, self.max_bbox_per_scale, 4))\n\n            num = 0\n            if self.batch_count < self.num_batchs:\n                while num < self.batch_size:\n                    index = self.batch_count * self.batch_size + num\n                    if index >= self.num_samples: index -= self.num_samples\n                    annotation = self.annotations[index]\n                    image, bboxes = self.parse_annotation(annotation)\n                    label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes = self.preprocess_true_boxes(bboxes)\n\n                    batch_image[num, :, :, :] = image\n                    batch_label_sbbox[num, :, :, :, :] = label_sbbox\n                    batch_label_mbbox[num, :, :, :, :] = label_mbbox\n                    batch_label_lbbox[num, :, :, :, :] = label_lbbox\n                    batch_sbboxes[num, :, :] = sbboxes\n                    batch_mbboxes[num, :, :] = mbboxes\n                    batch_lbboxes[num, :, :] = lbboxes\n                    num += 1\n                self.batch_count += 1\n                return batch_image, batch_label_sbbox, batch_label_mbbox, batch_label_lbbox, \\\n                       batch_sbboxes, batch_mbboxes, batch_lbboxes\n            else:\n                self.batch_count = 0\n                np.random.shuffle(self.annotations)\n                raise StopIteration\n\n    def random_horizontal_flip(self, image, bboxes):\n\n        if random.random() < 0.5:\n            _, w, _ = image.shape\n            image = image[:, ::-1, :]\n            bboxes[:, [0,2]] = w - bboxes[:, [2,0]]\n\n        return image, bboxes\n\n    def random_crop(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            crop_xmin = max(0, int(max_bbox[0] - random.uniform(0, max_l_trans)))\n            crop_ymin = max(0, int(max_bbox[1] - random.uniform(0, max_u_trans)))\n            crop_xmax = max(w, int(max_bbox[2] + random.uniform(0, max_r_trans)))\n            crop_ymax = max(h, int(max_bbox[3] + random.uniform(0, max_d_trans)))\n\n            image = image[crop_ymin : crop_ymax, crop_xmin : crop_xmax]\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] - crop_xmin\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] - crop_ymin\n\n        return image, bboxes\n\n    def random_translate(self, image, bboxes):\n\n        if random.random() < 0.5:\n            h, w, _ = image.shape\n            max_bbox = np.concatenate([np.min(bboxes[:, 0:2], axis=0), np.max(bboxes[:, 2:4], axis=0)], axis=-1)\n\n            max_l_trans = max_bbox[0]\n            max_u_trans = max_bbox[1]\n            max_r_trans = w - max_bbox[2]\n            max_d_trans = h - max_bbox[3]\n\n            tx = random.uniform(-(max_l_trans - 1), (max_r_trans - 1))\n            ty = random.uniform(-(max_u_trans - 1), (max_d_trans - 1))\n\n            M = np.array([[1, 0, tx], [0, 1, ty]])\n            image = cv2.warpAffine(image, M, (w, h))\n\n            bboxes[:, [0, 2]] = bboxes[:, [0, 2]] + tx\n            bboxes[:, [1, 3]] = bboxes[:, [1, 3]] + ty\n\n        return image, bboxes\n\n    def parse_annotation(self, annotation):\n\n        line = annotation.split()\n        image_path = line[0]\n        if not os.path.exists(image_path):\n            raise KeyError(""%s does not exist ... "" %image_path)\n        image = np.array(cv2.imread(image_path))\n        bboxes = np.array([list(map(lambda x: int(float(x)), box.split(\',\'))) for box in line[1:]])\n\n        if self.data_aug:\n            image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\n            image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\n\n        image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\n        return image, bboxes\n\n    def bbox_iou(self, boxes1, boxes2):\n\n        boxes1 = np.array(boxes1)\n        boxes2 = np.array(boxes2)\n\n        boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n        boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n        boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n        boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n        left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n        right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n        inter_section = np.maximum(right_down - left_up, 0.0)\n        inter_area = inter_section[..., 0] * inter_section[..., 1]\n        union_area = boxes1_area + boxes2_area - inter_area\n\n        return inter_area / union_area\n\n    def preprocess_true_boxes(self, bboxes):\n\n        label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale,\n                           5 + self.num_classes)) for i in range(3)]\n        bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\n        bbox_count = np.zeros((3,))\n\n        for bbox in bboxes:\n            bbox_coor = bbox[:4]\n            bbox_class_ind = bbox[4]\n\n            onehot = np.zeros(self.num_classes, dtype=np.float)\n            onehot[bbox_class_ind] = 1.0\n            uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\n            deta = 0.01\n            smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\n\n            bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\n            bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\n\n            iou = []\n            exist_positive = False\n            for i in range(3):\n                anchors_xywh = np.zeros((self.anchor_per_scale, 4))\n                anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n                anchors_xywh[:, 2:4] = self.anchors[i]\n\n                iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\n                iou.append(iou_scale)\n                iou_mask = iou_scale > 0.3\n\n                if np.any(iou_mask):\n                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\n\n                    label[i][yind, xind, iou_mask, :] = 0\n                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\n                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n                    label[i][yind, xind, iou_mask, 5:] = smooth_onehot\n\n                    bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\n                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\n                    bbox_count[i] += 1\n\n                    exist_positive = True\n\n            if not exist_positive:\n                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n                best_detect = int(best_anchor_ind / self.anchor_per_scale)\n                best_anchor = int(best_anchor_ind % self.anchor_per_scale)\n                xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\n\n                label[best_detect][yind, xind, best_anchor, :] = 0\n                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\n                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n                label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\n\n                bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\n                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\n                bbox_count[best_detect] += 1\n        label_sbbox, label_mbbox, label_lbbox = label\n        sbboxes, mbboxes, lbboxes = bboxes_xywh\n        return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\n\n    def __len__(self):\n        return self.num_batchs\n\n\n\n\n'"
core/utils.py,3,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : utils.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 13:14:19\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport random\nimport colorsys\nimport numpy as np\nimport tensorflow as tf\nfrom core.config import cfg\n\ndef read_class_names(class_file_name):\n    \'\'\'loads class name from a file\'\'\'\n    names = {}\n    with open(class_file_name, \'r\') as data:\n        for ID, name in enumerate(data):\n            names[ID] = name.strip(\'\\n\')\n    return names\n\n\ndef get_anchors(anchors_path):\n    \'\'\'loads the anchors from a file\'\'\'\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = np.array(anchors.split(\',\'), dtype=np.float32)\n    return anchors.reshape(3, 3, 2)\n\n\ndef image_preporcess(image, target_size, gt_boxes=None):\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n\n    ih, iw    = target_size\n    h,  w, _  = image.shape\n\n    scale = min(iw/w, ih/h)\n    nw, nh  = int(scale * w), int(scale * h)\n    image_resized = cv2.resize(image, (nw, nh))\n\n    image_paded = np.full(shape=[ih, iw, 3], fill_value=128.0)\n    dw, dh = (iw - nw) // 2, (ih-nh) // 2\n    image_paded[dh:nh+dh, dw:nw+dw, :] = image_resized\n    image_paded = image_paded / 255.\n\n    if gt_boxes is None:\n        return image_paded\n\n    else:\n        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + dw\n        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + dh\n        return image_paded, gt_boxes\n\n\ndef draw_bbox(image, bboxes, classes=read_class_names(cfg.YOLO.CLASSES), show_label=True):\n    """"""\n    bboxes: [x_min, y_min, x_max, y_max, probability, cls_id] format coordinates.\n    """"""\n\n    num_classes = len(classes)\n    image_h, image_w, _ = image.shape\n    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n\n    random.seed(0)\n    random.shuffle(colors)\n    random.seed(None)\n\n    for i, bbox in enumerate(bboxes):\n        coor = np.array(bbox[:4], dtype=np.int32)\n        fontScale = 0.5\n        score = bbox[4]\n        class_ind = int(bbox[5])\n        bbox_color = colors[class_ind]\n        bbox_thick = int(0.6 * (image_h + image_w) / 600)\n        c1, c2 = (coor[0], coor[1]), (coor[2], coor[3])\n        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)\n\n        if show_label:\n            bbox_mess = \'%s: %.2f\' % (classes[class_ind], score)\n            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, thickness=bbox_thick//2)[0]\n            cv2.rectangle(image, c1, (c1[0] + t_size[0], c1[1] - t_size[1] - 3), bbox_color, -1)  # filled\n\n            cv2.putText(image, bbox_mess, (c1[0], c1[1]-2), cv2.FONT_HERSHEY_SIMPLEX,\n                        fontScale, (0, 0, 0), bbox_thick//2, lineType=cv2.LINE_AA)\n\n    return image\n\n\n\ndef bboxes_iou(boxes1, boxes2):\n\n    boxes1 = np.array(boxes1)\n    boxes2 = np.array(boxes2)\n\n    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    inter_section = np.maximum(right_down - left_up, 0.0)\n    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n    union_area    = boxes1_area + boxes2_area - inter_area\n    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n\n    return ious\n\n\n\ndef read_pb_return_tensors(graph, pb_file, return_elements):\n\n    with tf.gfile.FastGFile(pb_file, \'rb\') as f:\n        frozen_graph_def = tf.GraphDef()\n        frozen_graph_def.ParseFromString(f.read())\n\n    with graph.as_default():\n        return_elements = tf.import_graph_def(frozen_graph_def,\n                                              return_elements=return_elements)\n    return return_elements\n\n\ndef nms(bboxes, iou_threshold, sigma=0.3, method=\'nms\'):\n    """"""\n    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n\n    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n          https://github.com/bharatsingh430/soft-nms\n    """"""\n    classes_in_img = list(set(bboxes[:, 5]))\n    best_bboxes = []\n\n    for cls in classes_in_img:\n        cls_mask = (bboxes[:, 5] == cls)\n        cls_bboxes = bboxes[cls_mask]\n\n        while len(cls_bboxes) > 0:\n            max_ind = np.argmax(cls_bboxes[:, 4])\n            best_bbox = cls_bboxes[max_ind]\n            best_bboxes.append(best_bbox)\n            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n            weight = np.ones((len(iou),), dtype=np.float32)\n\n            assert method in [\'nms\', \'soft-nms\']\n\n            if method == \'nms\':\n                iou_mask = iou > iou_threshold\n                weight[iou_mask] = 0.0\n\n            if method == \'soft-nms\':\n                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n\n            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n            score_mask = cls_bboxes[:, 4] > 0.\n            cls_bboxes = cls_bboxes[score_mask]\n\n    return best_bboxes\n\n\ndef postprocess_boxes(pred_bbox, org_img_shape, input_size, score_threshold):\n\n    valid_scale=[0, np.inf]\n    pred_bbox = np.array(pred_bbox)\n\n    pred_xywh = pred_bbox[:, 0:4]\n    pred_conf = pred_bbox[:, 4]\n    pred_prob = pred_bbox[:, 5:]\n\n    # # (1) (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n    # # (2) (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n    org_h, org_w = org_img_shape\n    resize_ratio = min(input_size / org_w, input_size / org_h)\n\n    dw = (input_size - resize_ratio * org_w) / 2\n    dh = (input_size - resize_ratio * org_h) / 2\n\n    pred_coor[:, 0::2] = 1.0 * (pred_coor[:, 0::2] - dw) / resize_ratio\n    pred_coor[:, 1::2] = 1.0 * (pred_coor[:, 1::2] - dh) / resize_ratio\n\n    # # (3) clip some boxes those are out of range\n    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))\n    pred_coor[invalid_mask] = 0\n\n    # # (4) discard some invalid boxes\n    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n\n    # # (5) discard some boxes with low scores\n    classes = np.argmax(pred_prob, axis=-1)\n    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n    score_mask = scores > score_threshold\n    mask = np.logical_and(scale_mask, score_mask)\n    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n\n    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)\n\n\n\n'"
core/yolov3.py,56,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : yolov3.py\n#   Author      : YunYang1994\n#   Created date: 2019-02-28 10:47:03\n#   Description :\n#\n#================================================================\n\nimport numpy as np\nimport tensorflow as tf\nimport core.utils as utils\nimport core.common as common\nimport core.backbone as backbone\nfrom core.config import cfg\n\n\nclass YOLOV3(object):\n    """"""Implement tensoflow yolov3 here""""""\n    def __init__(self, input_data, trainable):\n\n        self.trainable        = trainable\n        self.classes          = utils.read_class_names(cfg.YOLO.CLASSES)\n        self.num_class        = len(self.classes)\n        self.strides          = np.array(cfg.YOLO.STRIDES)\n        self.anchors          = utils.get_anchors(cfg.YOLO.ANCHORS)\n        self.anchor_per_scale = cfg.YOLO.ANCHOR_PER_SCALE\n        self.iou_loss_thresh  = cfg.YOLO.IOU_LOSS_THRESH\n        self.upsample_method  = cfg.YOLO.UPSAMPLE_METHOD\n\n        try:\n            self.conv_lbbox, self.conv_mbbox, self.conv_sbbox = self.__build_nework(input_data)\n        except:\n            raise NotImplementedError(""Can not build up yolov3 network!"")\n\n        with tf.variable_scope(\'pred_sbbox\'):\n            self.pred_sbbox = self.decode(self.conv_sbbox, self.anchors[0], self.strides[0])\n\n        with tf.variable_scope(\'pred_mbbox\'):\n            self.pred_mbbox = self.decode(self.conv_mbbox, self.anchors[1], self.strides[1])\n\n        with tf.variable_scope(\'pred_lbbox\'):\n            self.pred_lbbox = self.decode(self.conv_lbbox, self.anchors[2], self.strides[2])\n\n    def __build_nework(self, input_data):\n\n        route_1, route_2, input_data = backbone.darknet53(input_data, self.trainable)\n\n        input_data = common.convolutional(input_data, (1, 1, 1024,  512), self.trainable, \'conv52\')\n        input_data = common.convolutional(input_data, (3, 3,  512, 1024), self.trainable, \'conv53\')\n        input_data = common.convolutional(input_data, (1, 1, 1024,  512), self.trainable, \'conv54\')\n        input_data = common.convolutional(input_data, (3, 3,  512, 1024), self.trainable, \'conv55\')\n        input_data = common.convolutional(input_data, (1, 1, 1024,  512), self.trainable, \'conv56\')\n\n        conv_lobj_branch = common.convolutional(input_data, (3, 3, 512, 1024), self.trainable, name=\'conv_lobj_branch\')\n        conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3*(self.num_class + 5)),\n                                          trainable=self.trainable, name=\'conv_lbbox\', activate=False, bn=False)\n\n        input_data = common.convolutional(input_data, (1, 1,  512,  256), self.trainable, \'conv57\')\n        input_data = common.upsample(input_data, name=\'upsample0\', method=self.upsample_method)\n\n        with tf.variable_scope(\'route_1\'):\n            input_data = tf.concat([input_data, route_2], axis=-1)\n\n        input_data = common.convolutional(input_data, (1, 1, 768, 256), self.trainable, \'conv58\')\n        input_data = common.convolutional(input_data, (3, 3, 256, 512), self.trainable, \'conv59\')\n        input_data = common.convolutional(input_data, (1, 1, 512, 256), self.trainable, \'conv60\')\n        input_data = common.convolutional(input_data, (3, 3, 256, 512), self.trainable, \'conv61\')\n        input_data = common.convolutional(input_data, (1, 1, 512, 256), self.trainable, \'conv62\')\n\n        conv_mobj_branch = common.convolutional(input_data, (3, 3, 256, 512),  self.trainable, name=\'conv_mobj_branch\' )\n        conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3*(self.num_class + 5)),\n                                          trainable=self.trainable, name=\'conv_mbbox\', activate=False, bn=False)\n\n        input_data = common.convolutional(input_data, (1, 1, 256, 128), self.trainable, \'conv63\')\n        input_data = common.upsample(input_data, name=\'upsample1\', method=self.upsample_method)\n\n        with tf.variable_scope(\'route_2\'):\n            input_data = tf.concat([input_data, route_1], axis=-1)\n\n        input_data = common.convolutional(input_data, (1, 1, 384, 128), self.trainable, \'conv64\')\n        input_data = common.convolutional(input_data, (3, 3, 128, 256), self.trainable, \'conv65\')\n        input_data = common.convolutional(input_data, (1, 1, 256, 128), self.trainable, \'conv66\')\n        input_data = common.convolutional(input_data, (3, 3, 128, 256), self.trainable, \'conv67\')\n        input_data = common.convolutional(input_data, (1, 1, 256, 128), self.trainable, \'conv68\')\n\n        conv_sobj_branch = common.convolutional(input_data, (3, 3, 128, 256), self.trainable, name=\'conv_sobj_branch\')\n        conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3*(self.num_class + 5)),\n                                          trainable=self.trainable, name=\'conv_sbbox\', activate=False, bn=False)\n\n        return conv_lbbox, conv_mbbox, conv_sbbox\n\n    def decode(self, conv_output, anchors, stride):\n        """"""\n        return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes]\n               contains (x, y, w, h, score, probability)\n        """"""\n\n        conv_shape       = tf.shape(conv_output)\n        batch_size       = conv_shape[0]\n        output_size      = conv_shape[1]\n        anchor_per_scale = len(anchors)\n\n        conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, anchor_per_scale, 5 + self.num_class))\n\n        conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\n        conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\n        conv_raw_conf = conv_output[:, :, :, :, 4:5]\n        conv_raw_prob = conv_output[:, :, :, :, 5: ]\n\n        y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])\n        x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])\n\n        xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n        xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, anchor_per_scale, 1])\n        xy_grid = tf.cast(xy_grid, tf.float32)\n\n        pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * stride\n        pred_wh = (tf.exp(conv_raw_dwdh) * anchors) * stride\n        pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n\n        pred_conf = tf.sigmoid(conv_raw_conf)\n        pred_prob = tf.sigmoid(conv_raw_prob)\n\n        return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n\n    def focal(self, target, actual, alpha=1, gamma=2):\n        focal_loss = alpha * tf.pow(tf.abs(target - actual), gamma)\n        return focal_loss\n\n    def bbox_giou(self, boxes1, boxes2):\n\n        boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                            boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n        boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                            boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n        boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\n                            tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\n        boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\n                            tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\n\n        boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n        boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n\n        left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n        right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n        inter_section = tf.maximum(right_down - left_up, 0.0)\n        inter_area = inter_section[..., 0] * inter_section[..., 1]\n        union_area = boxes1_area + boxes2_area - inter_area\n        iou = inter_area / union_area\n\n        enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n        enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n        enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\n        enclose_area = enclose[..., 0] * enclose[..., 1]\n        giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\n\n        return giou\n\n    def bbox_iou(self, boxes1, boxes2):\n\n        boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n        boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n        boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                            boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n        boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                            boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n\n        left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n        right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n        inter_section = tf.maximum(right_down - left_up, 0.0)\n        inter_area = inter_section[..., 0] * inter_section[..., 1]\n        union_area = boxes1_area + boxes2_area - inter_area\n        iou = 1.0 * inter_area / union_area\n\n        return iou\n\n    def loss_layer(self, conv, pred, label, bboxes, anchors, stride):\n\n        conv_shape  = tf.shape(conv)\n        batch_size  = conv_shape[0]\n        output_size = conv_shape[1]\n        input_size  = stride * output_size\n        conv = tf.reshape(conv, (batch_size, output_size, output_size,\n                                 self.anchor_per_scale, 5 + self.num_class))\n        conv_raw_conf = conv[:, :, :, :, 4:5]\n        conv_raw_prob = conv[:, :, :, :, 5:]\n\n        pred_xywh     = pred[:, :, :, :, 0:4]\n        pred_conf     = pred[:, :, :, :, 4:5]\n\n        label_xywh    = label[:, :, :, :, 0:4]\n        respond_bbox  = label[:, :, :, :, 4:5]\n        label_prob    = label[:, :, :, :, 5:]\n\n        giou = tf.expand_dims(self.bbox_giou(pred_xywh, label_xywh), axis=-1)\n        input_size = tf.cast(input_size, tf.float32)\n\n        bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n        giou_loss = respond_bbox * bbox_loss_scale * (1- giou)\n\n        iou = self.bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\n        max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\n\n        respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < self.iou_loss_thresh, tf.float32 )\n\n        conf_focal = self.focal(respond_bbox, pred_conf)\n\n        conf_loss = conf_focal * (\n                respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n                +\n                respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n        )\n\n        prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n\n        giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\n        conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\n        prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\n\n        return giou_loss, conf_loss, prob_loss\n\n\n\n    def compute_loss(self, label_sbbox, label_mbbox, label_lbbox, true_sbbox, true_mbbox, true_lbbox):\n\n        with tf.name_scope(\'smaller_box_loss\'):\n            loss_sbbox = self.loss_layer(self.conv_sbbox, self.pred_sbbox, label_sbbox, true_sbbox,\n                                         anchors = self.anchors[0], stride = self.strides[0])\n\n        with tf.name_scope(\'medium_box_loss\'):\n            loss_mbbox = self.loss_layer(self.conv_mbbox, self.pred_mbbox, label_mbbox, true_mbbox,\n                                         anchors = self.anchors[1], stride = self.strides[1])\n\n        with tf.name_scope(\'bigger_box_loss\'):\n            loss_lbbox = self.loss_layer(self.conv_lbbox, self.pred_lbbox, label_lbbox, true_lbbox,\n                                         anchors = self.anchors[2], stride = self.strides[2])\n\n        with tf.name_scope(\'giou_loss\'):\n            giou_loss = loss_sbbox[0] + loss_mbbox[0] + loss_lbbox[0]\n\n        with tf.name_scope(\'conf_loss\'):\n            conf_loss = loss_sbbox[1] + loss_mbbox[1] + loss_lbbox[1]\n\n        with tf.name_scope(\'prob_loss\'):\n            prob_loss = loss_sbbox[2] + loss_mbbox[2] + loss_lbbox[2]\n\n        return giou_loss, conf_loss, prob_loss\n\n\n'"
mAP/__init__.py,0,b''
mAP/main.py,0,"b'import glob\nimport json\nimport os\nimport shutil\nimport operator\nimport sys\nimport argparse\n\nMINOVERLAP = 0.5 # default value (defined in the PASCAL VOC2012 challenge)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-na\', \'--no-animation\', help=""no animation is shown."", action=""store_true"")\nparser.add_argument(\'-np\', \'--no-plot\', help=""no plot is shown."", action=""store_true"")\nparser.add_argument(\'-q\', \'--quiet\', help=""minimalistic console output."", action=""store_true"")\n# argparse receiving list of classes to be ignored\nparser.add_argument(\'-i\', \'--ignore\', nargs=\'+\', type=str, help=""ignore a list of classes."")\n# argparse receiving list of classes with specific IoU\nparser.add_argument(\'--set-class-iou\', nargs=\'+\', type=str, help=""set IoU for a specific class."")\nargs = parser.parse_args()\n\n# if there are no classes to ignore then replace None by empty list\nif args.ignore is None:\n  args.ignore = []\n\nspecific_iou_flagged = False\nif args.set_class_iou is not None:\n  specific_iou_flagged = True\n\n# if there are no images then no animation can be shown\nimg_path = \'images\'\nif os.path.exists(img_path): \n  for dirpath, dirnames, files in os.walk(img_path):\n    if not files:\n      # no image files found\n      args.no_animation = True\nelse:\n  args.no_animation = True\n\n# try to import OpenCV if the user didn\'t choose the option --no-animation\nshow_animation = False\nif not args.no_animation:\n  try:\n    import cv2\n    show_animation = True\n  except ImportError:\n    print(""\\""opencv-python\\"" not found, please install to visualize the results."")\n    args.no_animation = True\n\n# try to import Matplotlib if the user didn\'t choose the option --no-plot\ndraw_plot = False\nif not args.no_plot:\n  try:\n    import matplotlib.pyplot as plt\n    draw_plot = True\n  except ImportError:\n    print(""\\""matplotlib\\"" not found, please install it to get the resulting plots."")\n    args.no_plot = True\n\n""""""\n throw error and exit\n""""""\ndef error(msg):\n  print(msg)\n  sys.exit(0)\n\n""""""\n check if the number is a float between 0.0 and 1.0\n""""""\ndef is_float_between_0_and_1(value):\n  try:\n    val = float(value)\n    if val > 0.0 and val < 1.0:\n      return True\n    else:\n      return False\n  except ValueError:\n    return False\n\n""""""\n Calculate the AP given the recall and precision array\n  1st) We compute a version of the measured precision/recall curve with\n       precision monotonically decreasing\n  2nd) We compute the AP as the area under this curve by numerical integration.\n""""""\ndef voc_ap(rec, prec):\n  """"""\n  --- Official matlab code VOC2012---\n  mrec=[0 ; rec ; 1];\n  mpre=[0 ; prec ; 0];\n  for i=numel(mpre)-1:-1:1\n      mpre(i)=max(mpre(i),mpre(i+1));\n  end\n  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  rec.insert(0, 0.0) # insert 0.0 at begining of list\n  rec.append(1.0) # insert 1.0 at end of list\n  mrec = rec[:]\n  prec.insert(0, 0.0) # insert 0.0 at begining of list\n  prec.append(0.0) # insert 0.0 at end of list\n  mpre = prec[:]\n  """"""\n   This part makes the precision monotonically decreasing\n    (goes from the end to the beginning)\n    matlab:  for i=numel(mpre)-1:-1:1\n                mpre(i)=max(mpre(i),mpre(i+1));\n  """"""\n  # matlab indexes start in 1 but python in 0, so I have to do:\n  #   range(start=(len(mpre) - 2), end=0, step=-1)\n  # also the python function range excludes the end, resulting in:\n  #   range(start=(len(mpre) - 2), end=-1, step=-1)\n  for i in range(len(mpre)-2, -1, -1):\n    mpre[i] = max(mpre[i], mpre[i+1])\n  """"""\n   This part creates a list of indexes where the recall changes\n    matlab:  i=find(mrec(2:end)~=mrec(1:end-1))+1;\n  """"""\n  i_list = []\n  for i in range(1, len(mrec)):\n    if mrec[i] != mrec[i-1]:\n      i_list.append(i) # if it was matlab would be i + 1\n  """"""\n   The Average Precision (AP) is the area under the curve\n    (numerical integration)\n    matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n  """"""\n  ap = 0.0\n  for i in i_list:\n    ap += ((mrec[i]-mrec[i-1])*mpre[i])\n  return ap, mrec, mpre\n\n\n""""""\n Convert the lines of a file to a list\n""""""\ndef file_lines_to_list(path):\n  # open txt file lines to a list\n  with open(path) as f:\n    content = f.readlines()\n  # remove whitespace characters like `\\n` at the end of each line\n  content = [x.strip() for x in content]\n  return content\n\n""""""\n Draws text in image\n""""""\ndef draw_text_in_image(img, text, pos, color, line_width):\n  font = cv2.FONT_HERSHEY_PLAIN\n  fontScale = 1\n  lineType = 1\n  bottomLeftCornerOfText = pos\n  cv2.putText(img, text,\n      bottomLeftCornerOfText,\n      font,\n      fontScale,\n      color,\n      lineType)\n  text_width, _ = cv2.getTextSize(text, font, fontScale, lineType)[0]\n  return img, (line_width + text_width)\n\n""""""\n Plot - adjust axes\n""""""\ndef adjust_axes(r, t, fig, axes):\n  # get text width for re-scaling\n  bb = t.get_window_extent(renderer=r)\n  text_width_inches = bb.width / fig.dpi\n  # get axis width in inches\n  current_fig_width = fig.get_figwidth()\n  new_fig_width = current_fig_width + text_width_inches\n  propotion = new_fig_width / current_fig_width\n  # get axis limit\n  x_lim = axes.get_xlim()\n  axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n\n""""""\n Draw plot using Matplotlib\n""""""\ndef draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n  # sort the dictionary by decreasing value, into a list of tuples\n  sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n  # unpacking the list of tuples into two lists\n  sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n  # \n  if true_p_bar != """":\n    """"""\n     Special case to draw in (green=true predictions) & (red=false predictions)\n    """"""\n    fp_sorted = []\n    tp_sorted = []\n    for key in sorted_keys:\n      fp_sorted.append(dictionary[key] - true_p_bar[key])\n      tp_sorted.append(true_p_bar[key])\n    plt.barh(range(n_classes), fp_sorted, align=\'center\', color=\'crimson\', label=\'False Predictions\')\n    plt.barh(range(n_classes), tp_sorted, align=\'center\', color=\'forestgreen\', label=\'True Predictions\', left=fp_sorted)\n    # add legend\n    plt.legend(loc=\'lower right\')\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      fp_val = fp_sorted[i]\n      tp_val = tp_sorted[i]\n      fp_str_val = "" "" + str(fp_val)\n      tp_str_val = fp_str_val + "" "" + str(tp_val)\n      # trick to paint multicolor with offset:\n      #   first paint everything and then repaint the first number\n      t = plt.text(val, i, tp_str_val, color=\'forestgreen\', va=\'center\', fontweight=\'bold\')\n      plt.text(val, i, fp_str_val, color=\'crimson\', va=\'center\', fontweight=\'bold\')\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  else:\n    plt.barh(range(n_classes), sorted_values, color=plot_color)\n    """"""\n     Write number on side of bar\n    """"""\n    fig = plt.gcf() # gcf - get current figure\n    axes = plt.gca()\n    r = fig.canvas.get_renderer()\n    for i, val in enumerate(sorted_values):\n      str_val = "" "" + str(val) # add a space before\n      if val < 1.0:\n        str_val = "" {0:.2f}"".format(val)\n      t = plt.text(val, i, str_val, color=plot_color, va=\'center\', fontweight=\'bold\')\n      # re-set axes to show number inside the figure\n      if i == (len(sorted_values)-1): # largest bar\n        adjust_axes(r, t, fig, axes)\n  # set window title\n  fig.canvas.set_window_title(window_title)\n  # write classes in y axis\n  tick_font_size = 12\n  plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n  """"""\n   Re-scale height accordingly\n  """"""\n  init_height = fig.get_figheight()\n  # comput the matrix height in points and inches\n  dpi = fig.dpi\n  height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n  height_in = height_pt / dpi\n  # compute the required figure height \n  top_margin = 0.15    # in percentage of the figure height\n  bottom_margin = 0.05 # in percentage of the figure height\n  figure_height = height_in / (1 - top_margin - bottom_margin)\n  # set new height\n  if figure_height > init_height:\n    fig.set_figheight(figure_height)\n\n  # set plot title\n  plt.title(plot_title, fontsize=14)\n  # set axis titles\n  # plt.xlabel(\'classes\')\n  plt.xlabel(x_label, fontsize=\'large\')\n  # adjust size of window\n  fig.tight_layout()\n  # save the plot\n  fig.savefig(output_path)\n  # show image\n  if to_show:\n    plt.show()\n  # close the plot\n  plt.close()\n\n""""""\n Create a ""tmp_files/"" and ""results/"" directory\n""""""\ntmp_files_path = ""tmp_files""\nif not os.path.exists(tmp_files_path): # if it doesn\'t exist already\n  os.makedirs(tmp_files_path)\nresults_files_path = ""results""\nif os.path.exists(results_files_path): # if it exist already\n  # reset the results directory\n  shutil.rmtree(results_files_path)\n\nos.makedirs(results_files_path)\nif draw_plot:\n  os.makedirs(results_files_path + ""/classes"")\nif show_animation:\n  os.makedirs(results_files_path + ""/images"")\n  os.makedirs(results_files_path + ""/images/single_predictions"")\n\n""""""\n Ground-Truth\n   Load each of the ground-truth files into a temporary "".json"" file.\n   Create a list of all the class names present in the ground-truth (gt_classes).\n""""""\n# get a list with the ground-truth files\nground_truth_files_list = glob.glob(\'ground-truth/*.txt\')\nif len(ground_truth_files_list) == 0:\n  error(""Error: No ground-truth files found!"")\nground_truth_files_list.sort()\n# dictionary with counter per class\ngt_counter_per_class = {}\n\nfor txt_file in ground_truth_files_list:\n  #print(txt_file)\n  file_id = txt_file.split("".txt"",1)[0]\n  file_id = os.path.basename(os.path.normpath(file_id))\n  # check if there is a correspondent predicted objects file\n  if not os.path.exists(\'predicted/\' + file_id + "".txt""):\n    error_msg = ""Error. File not found: predicted/"" +  file_id + "".txt\\n""\n    error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n    error(error_msg)\n  lines_list = file_lines_to_list(txt_file)\n  # create ground-truth dictionary\n  bounding_boxes = []\n  is_difficult = False\n  for line in lines_list:\n    try:\n      if ""difficult"" in line:\n          class_name, left, top, right, bottom, _difficult = line.split()\n          is_difficult = True\n      else:\n          class_name, left, top, right, bottom = line.split()\n    except ValueError:\n      error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n      error_msg += "" Expected: <class_name> <left> <top> <right> <bottom> [\'difficult\']\\n""\n      error_msg += "" Received: "" + line\n      error_msg += ""\\n\\nIf you have a <class_name> with spaces between words you should remove them\\n""\n      error_msg += ""by running the script \\""remove_space.py\\"" or \\""rename_class.py\\"" in the \\""extra/\\"" folder.""\n      error(error_msg)\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n    if is_difficult:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False, ""difficult"":True})\n        is_difficult = False\n    else:\n        bounding_boxes.append({""class_name"":class_name, ""bbox"":bbox, ""used"":False})\n        # count that object\n        if class_name in gt_counter_per_class:\n          gt_counter_per_class[class_name] += 1\n        else:\n          # if class didn\'t exist yet\n          gt_counter_per_class[class_name] = 1\n  # dump bounding_boxes into a "".json"" file\n  with open(tmp_files_path + ""/"" + file_id + ""_ground_truth.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\ngt_classes = list(gt_counter_per_class.keys())\n# let\'s sort the classes alphabetically\ngt_classes = sorted(gt_classes)\nn_classes = len(gt_classes)\n#print(gt_classes)\n#print(gt_counter_per_class)\n\n""""""\n Check format of the flag --set-class-iou (if used)\n  e.g. check if class exists\n""""""\nif specific_iou_flagged:\n  n_args = len(args.set_class_iou)\n  error_msg = \\\n    \'\\n --set-class-iou [class_1] [IoU_1] [class_2] [IoU_2] [...]\'\n  if n_args % 2 != 0:\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  # [class_1] [IoU_1] [class_2] [IoU_2]\n  # specific_iou_classes = [\'class_1\', \'class_2\']\n  specific_iou_classes = args.set_class_iou[::2] # even\n  # iou_list = [\'IoU_1\', \'IoU_2\']\n  iou_list = args.set_class_iou[1::2] # odd\n  if len(specific_iou_classes) != len(iou_list):\n    error(\'Error, missing arguments. Flag usage:\' + error_msg)\n  for tmp_class in specific_iou_classes:\n    if tmp_class not in gt_classes:\n          error(\'Error, unknown class \\""\' + tmp_class + \'\\"". Flag usage:\' + error_msg)\n  for num in iou_list:\n    if not is_float_between_0_and_1(num):\n      error(\'Error, IoU must be between 0.0 and 1.0. Flag usage:\' + error_msg)\n\n""""""\n Predicted\n   Load each of the predicted files into a temporary "".json"" file.\n""""""\n# get a list with the predicted files\npredicted_files_list = glob.glob(\'predicted/*.txt\')\npredicted_files_list.sort()\n\nfor class_index, class_name in enumerate(gt_classes):\n  bounding_boxes = []\n  for txt_file in predicted_files_list:\n    #print(txt_file)\n    # the first time it checks if all the corresponding ground-truth files exist\n    file_id = txt_file.split("".txt"",1)[0]\n    file_id = os.path.basename(os.path.normpath(file_id))\n    if class_index == 0:\n      if not os.path.exists(\'ground-truth/\' + file_id + "".txt""):\n        error_msg = ""Error. File not found: ground-truth/"" +  file_id + "".txt\\n""\n        error_msg += ""(You can avoid this error message by running extra/intersect-gt-and-pred.py)""\n        error(error_msg)\n    lines = file_lines_to_list(txt_file)\n    for line in lines:\n      try:\n        tmp_class_name, confidence, left, top, right, bottom = line.split()\n      except ValueError:\n        error_msg = ""Error: File "" + txt_file + "" in the wrong format.\\n""\n        error_msg += "" Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n""\n        error_msg += "" Received: "" + line\n        error(error_msg)\n      if tmp_class_name == class_name:\n        #print(""match"")\n        bbox = left + "" "" + top + "" "" + right + "" "" +bottom\n        bounding_boxes.append({""confidence"":confidence, ""file_id"":file_id, ""bbox"":bbox})\n        #print(bounding_boxes)\n  # sort predictions by decreasing confidence\n  bounding_boxes.sort(key=lambda x:float(x[\'confidence\']), reverse=True)\n  with open(tmp_files_path + ""/"" + class_name + ""_predictions.json"", \'w\') as outfile:\n    json.dump(bounding_boxes, outfile)\n\n""""""\n Calculate the AP for each class\n""""""\nsum_AP = 0.0\nap_dictionary = {}\n# open file to store the results\nwith open(results_files_path + ""/results.txt"", \'w\') as results_file:\n  results_file.write(""# AP and precision/recall per class\\n"")\n  count_true_positives = {}\n  for class_index, class_name in enumerate(gt_classes):\n    count_true_positives[class_name] = 0\n    """"""\n     Load predictions of that class\n    """"""\n    predictions_file = tmp_files_path + ""/"" + class_name + ""_predictions.json""\n    predictions_data = json.load(open(predictions_file))\n\n    """"""\n     Assign predictions to ground truth objects\n    """"""\n    nd = len(predictions_data)\n    tp = [0] * nd # creates an array of zeros of size nd\n    fp = [0] * nd\n    for idx, prediction in enumerate(predictions_data):\n      file_id = prediction[""file_id""]\n      if show_animation:\n        # find ground truth image\n        ground_truth_img = glob.glob1(img_path, file_id + "".*"")\n        #tifCounter = len(glob.glob1(myPath,""*.tif""))\n        if len(ground_truth_img) == 0:\n          error(""Error. Image not found with id: "" + file_id)\n        elif len(ground_truth_img) > 1:\n          error(""Error. Multiple image with id: "" + file_id)\n        else: # found image\n          #print(img_path + ""/"" + ground_truth_img[0])\n          # Load image\n          img = cv2.imread(img_path + ""/"" + ground_truth_img[0])\n          # load image with draws of multiple detections\n          img_cumulative_path = results_files_path + ""/images/"" + ground_truth_img[0]\n          if os.path.isfile(img_cumulative_path):\n            img_cumulative = cv2.imread(img_cumulative_path)\n          else:\n            img_cumulative = img.copy()\n          # Add bottom border to image\n          bottom_border = 60\n          BLACK = [0, 0, 0]\n          img = cv2.copyMakeBorder(img, 0, bottom_border, 0, 0, cv2.BORDER_CONSTANT, value=BLACK)\n      # assign prediction to ground truth object if any\n      #   open ground-truth with that file_id\n      gt_file = tmp_files_path + ""/"" + file_id + ""_ground_truth.json""\n      ground_truth_data = json.load(open(gt_file))\n      ovmax = -1\n      gt_match = -1\n      # load prediction bounding-box\n      bb = [ float(x) for x in prediction[""bbox""].split() ]\n      for obj in ground_truth_data:\n        # look for a class_name match\n        if obj[""class_name""] == class_name:\n          bbgt = [ float(x) for x in obj[""bbox""].split() ]\n          bi = [max(bb[0],bbgt[0]), max(bb[1],bbgt[1]), min(bb[2],bbgt[2]), min(bb[3],bbgt[3])]\n          iw = bi[2] - bi[0] + 1\n          ih = bi[3] - bi[1] + 1\n          if iw > 0 and ih > 0:\n            # compute overlap (IoU) = area of intersection / area of union\n            ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + (bbgt[2] - bbgt[0]\n                    + 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n            ov = iw * ih / ua\n            if ov > ovmax:\n              ovmax = ov\n              gt_match = obj\n\n      # assign prediction as true positive/don\'t care/false positive\n      if show_animation:\n        status = ""NO MATCH FOUND!"" # status is only used in the animation\n      # set minimum overlap\n      min_overlap = MINOVERLAP\n      if specific_iou_flagged:\n        if class_name in specific_iou_classes:\n          index = specific_iou_classes.index(class_name)\n          min_overlap = float(iou_list[index])\n      if ovmax >= min_overlap:\n        if ""difficult"" not in gt_match:\n            if not bool(gt_match[""used""]):\n              # true positive\n              tp[idx] = 1\n              gt_match[""used""] = True\n              count_true_positives[class_name] += 1\n              # update the "".json"" file\n              with open(gt_file, \'w\') as f:\n                  f.write(json.dumps(ground_truth_data))\n              if show_animation:\n                status = ""MATCH!""\n            else:\n              # false positive (multiple detection)\n              fp[idx] = 1\n              if show_animation:\n                status = ""REPEATED MATCH!""\n      else:\n        # false positive\n        fp[idx] = 1\n        if ovmax > 0:\n          status = ""INSUFFICIENT OVERLAP""\n\n      """"""\n       Draw image to show animation\n      """"""\n      if show_animation:\n        height, widht = img.shape[:2]\n        # colors (OpenCV works with BGR)\n        white = (255,255,255)\n        light_blue = (255,200,100)\n        green = (0,255,0)\n        light_red = (30,30,255)\n        # 1st line\n        margin = 10\n        v_pos = int(height - margin - (bottom_border / 2))\n        text = ""Image: "" + ground_truth_img[0] + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        text = ""Class ["" + str(class_index) + ""/"" + str(n_classes) + ""]: "" + class_name + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), light_blue, line_width)\n        if ovmax != -1:\n          color = light_red\n          if status == ""INSUFFICIENT OVERLAP"":\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + ""< {0:.2f}% "".format(min_overlap*100)\n          else:\n            text = ""IoU: {0:.2f}% "".format(ovmax*100) + "">= {0:.2f}% "".format(min_overlap*100)\n            color = green\n          img, _ = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n        # 2nd line\n        v_pos += int(bottom_border / 2)\n        rank_pos = str(idx+1) # rank position (idx starts at 0)\n        text = ""Prediction #rank: "" + rank_pos + "" confidence: {0:.2f}% "".format(float(prediction[""confidence""])*100)\n        img, line_width = draw_text_in_image(img, text, (margin, v_pos), white, 0)\n        color = light_red\n        if status == ""MATCH!"":\n          color = green\n        text = ""Result: "" + status + "" ""\n        img, line_width = draw_text_in_image(img, text, (margin + line_width, v_pos), color, line_width)\n\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        if ovmax > 0: # if there is intersections between the bounding-boxes\n          bbgt = [ int(x) for x in gt_match[""bbox""].split() ]\n          cv2.rectangle(img,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.rectangle(img_cumulative,(bbgt[0],bbgt[1]),(bbgt[2],bbgt[3]),light_blue,2)\n          cv2.putText(img_cumulative, class_name, (bbgt[0],bbgt[1] - 5), font, 0.6, light_blue, 1, cv2.LINE_AA)\n        bb = [int(i) for i in bb]\n        cv2.rectangle(img,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.rectangle(img_cumulative,(bb[0],bb[1]),(bb[2],bb[3]),color,2)\n        cv2.putText(img_cumulative, class_name, (bb[0],bb[1] - 5), font, 0.6, color, 1, cv2.LINE_AA)\n        # show image\n        cv2.imshow(""Animation"", img)\n        cv2.waitKey(20) # show for 20 ms\n        # save image to results\n        output_img_path = results_files_path + ""/images/single_predictions/"" + class_name + ""_prediction"" + str(idx) + "".jpg""\n        cv2.imwrite(output_img_path, img)\n        # save the image with all the objects drawn to it\n        cv2.imwrite(img_cumulative_path, img_cumulative)\n\n    #print(tp)\n    # compute precision/recall\n    cumsum = 0\n    for idx, val in enumerate(fp):\n      fp[idx] += cumsum\n      cumsum += val\n    cumsum = 0\n    for idx, val in enumerate(tp):\n      tp[idx] += cumsum\n      cumsum += val\n    #print(tp)\n    rec = tp[:]\n    for idx, val in enumerate(tp):\n      rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n    #print(rec)\n    prec = tp[:]\n    for idx, val in enumerate(tp):\n      prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n    #print(prec)\n\n    ap, mrec, mprec = voc_ap(rec, prec)\n    sum_AP += ap\n    text = ""{0:.2f}%"".format(ap*100) + "" = "" + class_name + "" AP  "" #class_name + "" AP = {0:.2f}%"".format(ap*100)\n    """"""\n     Write to results.txt\n    """"""\n    rounded_prec = [ \'%.2f\' % elem for elem in prec ]\n    rounded_rec = [ \'%.2f\' % elem for elem in rec ]\n    results_file.write(text + ""\\n Precision: "" + str(rounded_prec) + ""\\n Recall   :"" + str(rounded_rec) + ""\\n\\n"")\n    if not args.quiet:\n      print(text)\n    ap_dictionary[class_name] = ap\n\n    """"""\n     Draw plot\n    """"""\n    if draw_plot:\n      plt.plot(rec, prec, \'-o\')\n      # add a new penultimate point to the list (mrec[-2], 0.0)\n      # since the last line segment (and respective area) do not affect the AP value\n      area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n      area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n      plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor=\'r\')\n      # set window title\n      fig = plt.gcf() # gcf - get current figure\n      fig.canvas.set_window_title(\'AP \' + class_name)\n      # set plot title\n      plt.title(\'class: \' + text)\n      #plt.suptitle(\'This is a somewhat long figure title\', fontsize=16)\n      # set axis titles\n      plt.xlabel(\'Recall\')\n      plt.ylabel(\'Precision\')\n      # optional - set axes\n      axes = plt.gca() # gca - get current axes\n      axes.set_xlim([0.0,1.0])\n      axes.set_ylim([0.0,1.05]) # .05 to give some extra space\n      # Alternative option -> wait for button to be pressed\n      #while not plt.waitforbuttonpress(): pass # wait for key display\n      # Alternative option -> normal display\n      #plt.show()\n      # save the plot\n      fig.savefig(results_files_path + ""/classes/"" + class_name + "".png"")\n      plt.cla() # clear axes for next plot\n\n  if show_animation:\n    cv2.destroyAllWindows()\n\n  results_file.write(""\\n# mAP of all classes\\n"")\n  mAP = sum_AP / n_classes\n  text = ""mAP = {0:.2f}%"".format(mAP*100)\n  results_file.write(text + ""\\n"")\n  print(text)\n\n# remove the tmp_files directory\nshutil.rmtree(tmp_files_path)\n\n""""""\n Count total of Predictions\n""""""\n# iterate through all the files\npred_counter_per_class = {}\n#all_classes_predicted_files = set([])\nfor txt_file in predicted_files_list:\n  # get lines to list\n  lines_list = file_lines_to_list(txt_file)\n  for line in lines_list:\n    class_name = line.split()[0]\n    # check if class is in the ignore list, if yes skip\n    if class_name in args.ignore:\n      continue\n    # count that object\n    if class_name in pred_counter_per_class:\n      pred_counter_per_class[class_name] += 1\n    else:\n      # if class didn\'t exist yet\n      pred_counter_per_class[class_name] = 1\n#print(pred_counter_per_class)\npred_classes = list(pred_counter_per_class.keys())\n\n\n""""""\n Plot the total number of occurences of each class in the ground-truth\n""""""\nif draw_plot:\n  window_title = ""Ground-Truth Info""\n  plot_title = ""Ground-Truth\\n""\n  plot_title += ""("" + str(len(ground_truth_files_list)) + "" files and "" + str(n_classes) + "" classes)""\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Ground-Truth Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  draw_plot_func(\n    gt_counter_per_class,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    \'\',\n    )\n\n""""""\n Write number of ground-truth objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results.txt"", \'a\') as results_file:\n  results_file.write(""\\n# Number of ground-truth objects per class\\n"")\n  for class_name in sorted(gt_counter_per_class):\n    results_file.write(class_name + "": "" + str(gt_counter_per_class[class_name]) + ""\\n"")\n\n""""""\n Finish counting true positives\n""""""\nfor class_name in pred_classes:\n  # if class exists in predictions but not in ground-truth then there are no true positives in that class\n  if class_name not in gt_classes:\n    count_true_positives[class_name] = 0\n#print(count_true_positives)\n\n""""""\n Plot the total number of occurences of each class in the ""predicted"" folder\n""""""\nif draw_plot:\n  window_title = ""Predicted Objects Info""\n  # Plot title\n  plot_title = ""Predicted Objects\\n""\n  plot_title += ""("" + str(len(predicted_files_list)) + "" files and ""\n  count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(pred_counter_per_class.values()))\n  plot_title += str(count_non_zero_values_in_dictionary) + "" detected classes)""\n  # end Plot title\n  x_label = ""Number of objects per class""\n  output_path = results_files_path + ""/Predicted Objects Info.png""\n  to_show = False\n  plot_color = \'forestgreen\'\n  true_p_bar = count_true_positives\n  draw_plot_func(\n    pred_counter_per_class,\n    len(pred_counter_per_class),\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    true_p_bar\n    )\n\n""""""\n Write number of predicted objects per class to results.txt\n""""""\nwith open(results_files_path + ""/results.txt"", \'a\') as results_file:\n  results_file.write(""\\n# Number of predicted objects per class\\n"")\n  for class_name in sorted(pred_classes):\n    n_pred = pred_counter_per_class[class_name]\n    text = class_name + "": "" + str(n_pred)\n    text += "" (tp:"" + str(count_true_positives[class_name]) + """"\n    text += "", fp:"" + str(n_pred - count_true_positives[class_name]) + "")\\n""\n    results_file.write(text)\n\n""""""\n Draw mAP plot (Show AP\'s of all classes in decreasing order)\n""""""\nif draw_plot:\n  window_title = ""mAP""\n  plot_title = ""mAP = {0:.2f}%"".format(mAP*100)\n  x_label = ""Average Precision""\n  output_path = results_files_path + ""/mAP.png""\n  to_show = True\n  plot_color = \'royalblue\'\n  draw_plot_func(\n    ap_dictionary,\n    n_classes,\n    window_title,\n    plot_title,\n    x_label,\n    output_path,\n    to_show,\n    plot_color,\n    """"\n    )\n'"
scripts/show_bboxes.py,0,"b'#! /usr/bin/env python\n# coding=utf-8\n#================================================================\n#   Copyright (C) 2019 * Ltd. All rights reserved.\n#\n#   Editor      : VIM\n#   File name   : show_bboxes.py\n#   Author      : YunYang1994\n#   Created date: 2019-05-29 01:18:24\n#   Description :\n#\n#================================================================\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nID = 0\nlabel_txt = ""../data/dataset/traffic_test.txt""\nimage_info = open(label_txt).readlines()[ID].split()\n\nimage_path = image_info[0]\nimage = cv2.imread(image_path)\nfor bbox in image_info[1:]:\n    bbox = bbox.split("","")\n    image = cv2.rectangle(image,(int(float(bbox[0])),\n                                 int(float(bbox[1]))),\n                                (int(float(bbox[2])),\n                                 int(float(bbox[3]))), (255,0,0), 2)\n\nimage = Image.fromarray(np.uint8(image))\nimage.show()\n'"
scripts/voc_annotation.py,0,"b'import os\nimport argparse\nimport xml.etree.ElementTree as ET\n\ndef convert_voc_annotation(data_path, data_type, anno_path, use_difficult_bbox=True):\n\n    classes = [\'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\',\n               \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n               \'motorbike\', \'person\', \'pottedplant\', \'sheep\', \'sofa\',\n               \'train\', \'tvmonitor\']\n    img_inds_file = os.path.join(data_path, \'ImageSets\', \'Main\', data_type + \'.txt\')\n    with open(img_inds_file, \'r\') as f:\n        txt = f.readlines()\n        image_inds = [line.strip() for line in txt]\n\n    with open(anno_path, \'a\') as f:\n        for image_ind in image_inds:\n            image_path = os.path.join(data_path, \'JPEGImages\', image_ind + \'.jpg\')\n            annotation = image_path\n            label_path = os.path.join(data_path, \'Annotations\', image_ind + \'.xml\')\n            root = ET.parse(label_path).getroot()\n            objects = root.findall(\'object\')\n            for obj in objects:\n                difficult = obj.find(\'difficult\').text.strip()\n                if (not use_difficult_bbox) and(int(difficult) == 1):\n                    continue\n                bbox = obj.find(\'bndbox\')\n                class_ind = classes.index(obj.find(\'name\').text.lower().strip())\n                xmin = bbox.find(\'xmin\').text.strip()\n                xmax = bbox.find(\'xmax\').text.strip()\n                ymin = bbox.find(\'ymin\').text.strip()\n                ymax = bbox.find(\'ymax\').text.strip()\n                annotation += \' \' + \',\'.join([xmin, ymin, xmax, ymax, str(class_ind)])\n            print(annotation)\n            f.write(annotation + ""\\n"")\n    return len(image_inds)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_path"", default=""/home/yang/test/VOC/"")\n    parser.add_argument(""--train_annotation"", default=""./data/dataset/voc_train.txt"")\n    parser.add_argument(""--test_annotation"",  default=""./data/dataset/voc_test.txt"")\n    flags = parser.parse_args()\n\n    if os.path.exists(flags.train_annotation):os.remove(flags.train_annotation)\n    if os.path.exists(flags.test_annotation):os.remove(flags.test_annotation)\n\n    num1 = convert_voc_annotation(os.path.join(flags.data_path, \'train/VOCdevkit/VOC2007\'), \'trainval\', flags.train_annotation, False)\n    num2 = convert_voc_annotation(os.path.join(flags.data_path, \'train/VOCdevkit/VOC2012\'), \'trainval\', flags.train_annotation, False)\n    num3 = convert_voc_annotation(os.path.join(flags.data_path, \'test/VOCdevkit/VOC2007\'),  \'test\', flags.test_annotation, False)\n    print(\'=> The number of image for train is: %d\\tThe number of image for test is:%d\' %(num1 + num2, num3))\n\n\n'"
mAP/extra/convert_gt_xml.py,0,"b'import sys\nimport os\nimport glob\nimport xml.etree.ElementTree as ET\n\n\n# change directory to the one with the files to be changed\npath_to_folder = \'../ground-truth\'\n#print(path_to_folder)\nos.chdir(path_to_folder)\n\n# old files (xml format) will be moved to a ""backup"" folder\n## create the backup dir if it doesn\'t exist already\nif not os.path.exists(""backup""):\n  os.makedirs(""backup"")\n\n# create VOC format files\nxml_list = glob.glob(\'*.xml\')\nif len(xml_list) == 0:\n  print(""Error: no .xml files found in ground-truth"")\n  sys.exit()\nfor tmp_file in xml_list:\n  #print(tmp_file)\n  # 1. create new file (VOC format)\n  with open(tmp_file.replace("".xml"", "".txt""), ""a"") as new_f:\n    root = ET.parse(tmp_file).getroot()\n    for obj in root.findall(\'object\'):\n      obj_name = obj.find(\'name\').text\n      bndbox = obj.find(\'bndbox\')\n      left = bndbox.find(\'xmin\').text\n      top = bndbox.find(\'ymin\').text\n      right = bndbox.find(\'xmax\').text\n      bottom = bndbox.find(\'ymax\').text\n      new_f.write(obj_name + "" "" + left + "" "" + top + "" "" + right + "" "" + bottom + \'\\n\')\n  # 2. move old file (xml format) to backup\n  os.rename(tmp_file, ""backup/"" + tmp_file)\nprint(""Conversion completed!"")\n'"
mAP/extra/convert_gt_yolo.py,0,"b'import sys\nimport os\nimport glob\nimport cv2\n\n\ndef convert_yolo_coordinates_to_voc(x_c_n, y_c_n, width_n, height_n, img_width, img_height):\n  ## remove normalization given the size of the image\n  x_c = float(x_c_n) * img_width\n  y_c = float(y_c_n) * img_height\n  width = float(width_n) * img_width\n  height = float(height_n) * img_height\n  ## compute half width and half height\n  half_width = width / 2\n  half_height = height / 2\n  ## compute left, top, right, bottom\n  ## in the official VOC challenge the top-left pixel in the image has coordinates (1;1)\n  left = int(x_c - half_width) + 1\n  top = int(y_c - half_height) + 1\n  right = int(x_c + half_width) + 1\n  bottom = int(y_c + half_height) + 1\n  return left, top, right, bottom\n\n# read the class_list.txt to a list\nwith open(""class_list.txt"") as f:\n  obj_list = f.readlines()\n## remove whitespace characters like `\\n` at the end of each line\n  obj_list = [x.strip() for x in obj_list]\n## e.g. first object in the list\n#print(obj_list[0])\n\n# change directory to the one with the files to be changed\npath_to_folder = \'../ground-truth\'\n#print(path_to_folder)\nos.chdir(path_to_folder)\n\n# old files (YOLO format) will be moved to a new folder (backup/)\n## create the backup dir if it doesn\'t exist already\nif not os.path.exists(""backup""):\n  os.makedirs(""backup"")\n\n# create VOC format files\ntxt_list = glob.glob(\'*.txt\')\nif len(txt_list) == 0:\n  print(""Error: no .txt files found in ground-truth"")\n  sys.exit()\nfor tmp_file in txt_list:\n  #print(tmp_file)\n  # 1. check that there is an image with that name\n  ## get name before "".txt""\n  image_name = tmp_file.split("".txt"",1)[0]\n  #print(image_name)\n  ## check if image exists\n  for fname in os.listdir(\'../images\'):\n    if fname.startswith(image_name):\n      ## image found\n      #print(fname)\n      img = cv2.imread(\'../images/\' + fname)\n      ## get image width and height\n      img_height, img_width = img.shape[:2]\n      break\n  else:\n    ## image not found\n    print(""Error: image not found, corresponding to "" + tmp_file)\n    sys.exit()\n  # 2. open txt file lines to a list\n  with open(tmp_file) as f:\n    content = f.readlines()\n  ## remove whitespace characters like `\\n` at the end of each line\n  content = [x.strip() for x in content]\n  # 3. move old file (YOLO format) to backup\n  os.rename(tmp_file, ""backup/"" + tmp_file)\n  # 4. create new file (VOC format)\n  with open(tmp_file, ""a"") as new_f:\n    for line in content:\n      ## split a line by spaces.\n      ## ""c"" stands for center and ""n"" stands for normalized\n      obj_id, x_c_n, y_c_n, width_n, height_n = line.split()\n      obj_name = obj_list[int(obj_id)]\n      left, top, right, bottom = convert_yolo_coordinates_to_voc(x_c_n, y_c_n, width_n, height_n, img_width, img_height)\n      ## add new line to file\n      #print(obj_name + "" "" + str(left) + "" "" + str(top) + "" "" + str(right) + "" "" + str(bottom))\n      new_f.write(obj_name + "" "" + str(left) + "" "" + str(top) + "" "" + str(right) + "" "" + str(bottom) + \'\\n\')\nprint(""Conversion completed!"")\n'"
mAP/extra/convert_keras-yolo3.py,0,"b'\'\'\'\nABOUT THIS SCRIPT:\nConverts ground-truth from the annotation files\naccording to the https://github.com/qqwweee/keras-yolo3\nor https://github.com/gustavovaliati/keras-yolo3 format.\n\nAnd converts the predicitons from the annotation files\naccording to the https://github.com/gustavovaliati/keras-yolo3 format.\n\'\'\'\n\nimport argparse\nimport datetime\nimport os\n\n\'\'\'\nEach time this script runs, it saves the output in a different path\ncontrolled by the following folder suffix: annotation_version.\n\'\'\'\nannotation_version = datetime.datetime.now().strftime(\'%Y%m%d%H%M%S\')\n\nap = argparse.ArgumentParser()\n\nap.add_argument(""-o"", ""--output_path"",\n                required=False,\n                default=\'from_kerasyolo3/version_{}\'.format(annotation_version),\n                type=str,\n                help=""The dataset root path location."")\nap.add_argument(""-r"", ""--gen_recursive"",\n                required=False,\n                default=False,\n                action=""store_true"",\n                help=""Define if the output txt files will be placed in a \\\n                recursive folder tree or to direct txt files."")\ngroup = ap.add_mutually_exclusive_group(required=True)\ngroup.add_argument(\'--gt\',\n    type=str,\n    default=None,\n    help=""The annotation file that refers to ground-truth in (keras-yolo3 format)"")\ngroup.add_argument(\'--pred\',\n    type=str,\n    default=None,\n    help=""The annotation file that refers to predictions in (keras-yolo3 format)"")\n\nARGS = ap.parse_args()\n\nwith open(\'class_list.txt\', \'r\') as class_file:\n    class_map = class_file.readlines()\nprint(class_map)\nannotation_file = ARGS.gt if ARGS.gt else ARGS.pred\n\nos.makedirs(ARGS.output_path, exist_ok=True)\n\nwith open(annotation_file, \'r\') as annot_f:\n    for annot in annot_f:\n        annot = annot.split(\' \')\n        img_path = annot[0].strip()\n        if ARGS.gen_recursive:\n            annotation_dir_name = os.path.dirname(img_path)\n            # remove the root path to enable to path.join.\n            if annotation_dir_name.startswith(\'/\'):\n                annotation_dir_name = annotation_dir_name.replace(\'/\', \'\', 1)\n            destination_dir = os.path.join(ARGS.output_path, annotation_dir_name)\n            os.makedirs(destination_dir, exist_ok=True)\n            # replace .jpg with your image format.\n            file_name = os.path.basename(img_path).replace(\'.jpg\', \'.txt\')\n            output_file_path = os.path.join(destination_dir, file_name)\n        else:\n            file_name = img_path.replace(\'.jpg\', \'.txt\').replace(\'/\', \'__\')\n            output_file_path = os.path.join(ARGS.output_path, file_name)\n            os.path.dirname(output_file_path)\n\n        with open(output_file_path, \'w\') as out_f:\n            for bbox in annot[1:]:\n                if ARGS.gt:\n                    # Here we are dealing with ground-truth annotations\n                    # <class_name> <left> <top> <right> <bottom> [<difficult>]\n                    # todo: handle difficulty\n                    x_min, y_min, x_max, y_max, class_id = list(map(float, bbox.split(\',\')))\n                    out_box = \'{} {} {} {} {}\'.format(\n                        class_map[int(class_id)].strip(), x_min, y_min, x_max, y_max)\n                else:\n                    # Here we are dealing with predictions annotations\n                    # <class_name> <confidence> <left> <top> <right> <bottom>\n                    x_min, y_min, x_max, y_max, class_id, score = list(map(float, bbox.split(\',\')))\n                    out_box = \'{} {} {} {} {} {}\'.format(\n                        class_map[int(class_id)].strip(), score,  x_min, y_min, x_max, y_max)\n\n                out_f.write(out_box + ""\\n"")\n'"
mAP/extra/convert_pred_darkflow_json.py,0,"b'import sys\nimport os\nimport glob\nimport json\n\n\n# change directory to the one with the files to be changed\npath_to_folder = \'../predicted\'\n#print(path_to_folder)\nos.chdir(path_to_folder)\n\n# old files (darkflow json format) will be moved to a ""backup"" folder\n## create the backup dir if it doesn\'t exist already\nif not os.path.exists(""backup""):\n  os.makedirs(""backup"")\n\n# create VOC format files\njson_list = glob.glob(\'*.json\')\nif len(json_list) == 0:\n  print(""Error: no .json files found in predicted"")\n  sys.exit()\nfor tmp_file in json_list:\n  #print(tmp_file)\n  # 1. create new file (VOC format)\n  with open(tmp_file.replace("".json"", "".txt""), ""a"") as new_f:\n    data = json.load(open(tmp_file))\n    for obj in data:\n      obj_name = obj[\'label\']\n      conf = obj[\'confidence\']\n      left = obj[\'topleft\'][\'x\']\n      top = obj[\'topleft\'][\'y\']\n      right = obj[\'bottomright\'][\'x\']\n      bottom = obj[\'bottomright\'][\'y\']\n      new_f.write(obj_name + "" "" + str(conf) + "" "" + str(left) + "" "" + str(top) + "" "" + str(right) + "" "" + str(bottom) + \'\\n\')\n  # 2. move old file (darkflow format) to backup\n  os.rename(tmp_file, ""backup/"" + tmp_file)\nprint(""Conversion completed!"")\n'"
mAP/extra/convert_pred_yolo.py,0,"b'import os\nimport re\n\nIN_FILE = \'result.txt\'\nOUTPUT_DIR = os.path.join(\'..\', \'predicted\')\n\nSEPARATOR_KEY = \'Enter Image Path:\'\nIMG_FORMAT = \'.jpg\'\n\noutfile = None\nwith open(IN_FILE) as infile:\n    for line in infile:\n        if SEPARATOR_KEY in line:\n            if IMG_FORMAT not in line:\n                break\n            # get text between two substrings (SEPARATOR_KEY and IMG_FORMAT)\n            image_path = re.search(SEPARATOR_KEY + \'(.*)\' + IMG_FORMAT, line)\n            # get the image name (the final component of a image_path)\n            # e.g., from \'data/horses_1\' to \'horses_1\'\n            image_name = os.path.basename(image_path.group(1))\n            # close the previous file\n            if outfile is not None:\n                outfile.close()\n            # open a new file\n            outfile = open(os.path.join(OUTPUT_DIR, image_name + \'.txt\'), \'w\')\n        elif outfile is not None:\n            # split line on first occurrence of the character \':\' and \'%\'\n            class_name, info = line.split(\':\', 1)\n            confidence, bbox = info.split(\'%\', 1)\n            # get all the coordinates of the bounding box\n            bbox = bbox.replace(\')\',\'\') # remove the character \')\'\n            # go through each of the parts of the string and check if it is a digit\n            left, top, width, height = [int(s) for s in bbox.split() if s.lstrip(\'-\').isdigit()]\n            right = left + width\n            bottom = top + height\n            outfile.write(""{} {} {} {} {} {}\\n"".format(class_name, float(confidence)/100, left, top, right, bottom))\n'"
mAP/extra/find_class.py,0,"b'import sys\nimport os\nimport glob\n\nif len(sys.argv) != 2:\n  print(""Error: wrong format.\\nUsage: python find_class.py [class_name]"")\n  sys.exit(0)\n\nsearching_class_name = sys.argv[1]\n\ndef find_class(class_name):\n  file_list = glob.glob(\'*.txt\')\n  file_list.sort()\n  # iterate through the text files\n  file_found = False\n  for txt_file in file_list:\n    # open txt file lines to a list\n    with open(txt_file) as f:\n      content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    # go through each line of eache file\n    for line in content:\n      class_name = line.split()[0]\n      if class_name == searching_class_name:\n        print("" "" + txt_file)\n        file_found = True\n        break\n  if not file_found:\n    print("" No file found with that class"")\n\nprint(""Ground-Truth folder:"")\nos.chdir(""../ground-truth"")\nfind_class(searching_class_name)\nprint(""\\nPredicted folder:"")\nos.chdir(""../predicted"")\nfind_class(searching_class_name)\n'"
mAP/extra/intersect-gt-and-pred.py,0,"b'import sys\nimport os\nimport glob\n\n## This script ensures same number of files in ground-truth and predicted folder.\n## When you encounter file not found error, it\'s usually because you have\n## mismatched numbers of ground-truth and predicted files.\n## You can use this script to move ground-truth and predicted files that are\n## not in the intersection into a backup folder (backup_no_matches_found).\n## This will retain only files that have the same name in both folders.\n\n# change directory to the one with the files to be changed\npath_to_gt = \'../ground-truth\'\npath_to_pred = \'../predicted\'\nbackup_folder = \'backup_no_matches_found\' # must end without slash\n\nos.chdir(path_to_gt)\ngt_files = glob.glob(\'*.txt\')\nif len(gt_files) == 0:\n    print(""Error: no .txt files found in"", path_to_gt)\n    sys.exit()\nos.chdir(path_to_pred)\npred_files = glob.glob(\'*.txt\')\nif len(pred_files) == 0:\n    print(""Error: no .txt files found in"", path_to_pred)\n    sys.exit()\n\ngt_files = set(gt_files)\npred_files = set(pred_files)\nprint(\'total ground-truth files:\', len(gt_files))\nprint(\'total predicted files:\', len(pred_files))\nprint()\n\ngt_backup = gt_files - pred_files\npred_backup = pred_files - gt_files\n\ndef backup(src_folder, backup_files, backup_folder):\n    # non-intersection files (txt format) will be moved to a backup folder\n    if not backup_files:\n        print(\'No backup required for\', src_folder)\n        return\n    os.chdir(src_folder)\n    ## create the backup dir if it doesn\'t exist already\n    if not os.path.exists(backup_folder):\n        os.makedirs(backup_folder)\n    for file in backup_files:\n        os.rename(file, backup_folder + \'/\' + file)\n    \nbackup(path_to_gt, gt_backup, backup_folder)\nbackup(path_to_pred, pred_backup, backup_folder)\nif gt_backup:\n    print(\'total ground-truth backup files:\', len(gt_backup))\nif pred_backup:\n    print(\'total predicted backup files:\', len(pred_backup))\n\nintersection = gt_files & pred_files\nprint(\'total intersected files:\', len(intersection))\nprint(""Intersection completed!"")\n'"
mAP/extra/remove_class.py,0,"b'import sys\nimport os\nimport glob\n\nif len(sys.argv) != 2:\n  print(""Error: wrong format.\\nUsage: python remove_class.py [class_name]"")\n  sys.exit(0)\n\nsearching_class_name = sys.argv[1]\n\n\ndef query_yes_no(question, default=""yes""):\n  """"""Ask a yes/no question via raw_input() and return their answer.\n\n  ""question"" is a string that is presented to the user.\n  ""default"" is the presumed answer if the user just hits <Enter>.\n      It must be ""yes"" (the default), ""no"" or None (meaning\n      an answer is required of the user).\n\n  The ""answer"" return value is True for ""yes"" or False for ""no"".\n  """"""\n  valid = {""yes"": True, ""y"": True, ""ye"": True,\n           ""no"": False, ""n"": False}\n  if default is None:\n    prompt = "" [y/n] ""\n  elif default == ""yes"":\n    prompt = "" [Y/n] ""\n  elif default == ""no"":\n    prompt = "" [y/N] ""\n  else:\n    raise ValueError(""invalid default answer: \'%s\'"" % default)\n\n  while True:\n    sys.stdout.write(question + prompt)\n    if sys.version_info[0] == 3:\n      choice = input().lower() # if version 3 of Python\n    else:\n      choice = raw_input().lower()\n    if default is not None and choice == \'\':\n      return valid[default]\n    elif choice in valid:\n      return valid[choice]\n    else:\n      sys.stdout.write(""Please respond with \'yes\' or \'no\' ""\n                         ""(or \'y\' or \'n\').\\n"")\n\n\ndef remove_class(class_name):\n  # get list of txt files\n  file_list = glob.glob(\'*.txt\')\n  file_list.sort()\n  # iterate through the txt files\n  for txt_file in file_list:\n    class_found = False\n    # open txt file lines to a list\n    with open(txt_file) as f:\n      content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    new_content = []\n    # go through each line of eache file\n    for line in content:\n      class_name = line.split()[0]\n      if class_name == searching_class_name:\n        class_found = True\n      else:\n        new_content.append(line)\n    if class_found:\n      # rewrite file\n      with open(txt_file, \'w\') as new_f:\n        for line in new_content:\n          new_f.write(""%s\\n"" % line)\n\nif query_yes_no(""Are you sure you want to remove the class \\"""" + searching_class_name + ""\\""?""):\n  print("" Ground-Truth folder:"")\n  os.chdir(""../ground-truth"")\n  remove_class(searching_class_name)\n  print(""  Done!"")\n  print("" Predicted folder:"")\n  os.chdir(""../predicted"")\n  remove_class(searching_class_name)\n  print(""  Done!"")\n'"
mAP/extra/remove_delimiter_char.py,0,"b'import glob\nimport os\nimport sys\nimport argparse\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-c\', \'--char\', required=True, type=str, help=\'specific character to be removed (e.g. "";"").\')\n# mutually exclusive arguments (can\'t select both)\ngroup = parser.add_mutually_exclusive_group(required=True)\ngroup.add_argument(\'-g\', \'--ground-truth\', help=""if to remove that char from the ground-truth files."", action=""store_true"")\ngroup.add_argument(\'-p\', \'--predicted\', help=""if to remove that char from the predicted objects files."", action=""store_true"")\nargs = parser.parse_args()\n\ndef file_lines_to_list(path):\n  # open txt file lines to a list\n  with open(path) as f:\n    content = f.readlines()\n  # remove whitespace characters like `\\n` at the end of each line\n  content = [x.strip() for x in content]\n  return content\n\nif len(args.char) != 1:\n  print(""Error: Please select a single char to be removed."")\n  sys.exit(0)\n\nif args.predicted:\n  os.chdir(""../predicted/"")\nelse:\n  os.chdir(""../ground-truth/"")\n\n## create the backup dir if it doesn\'t exist already\nbackup_path = ""backup""\nif not os.path.exists(backup_path):\n  os.makedirs(backup_path)\n\n# get a list with the predicted files\nfiles_list = glob.glob(\'*.txt\')\nfiles_list.sort()\n\nfor txt_file in files_list:\n  lines = file_lines_to_list(txt_file)\n  is_char_present = any(args.char in line for line in lines)\n  if is_char_present:\n    # move old file to backup\n    os.rename(txt_file, backup_path + ""/"" + txt_file)\n    # create new file\n    with open(txt_file, ""a"") as new_f:\n      for line in lines:\n        #print(line)\n        if args.predicted:\n          class_name, confidence, left, top, right, bottom = line.split(args.char)\n          # remove any white space if existent in the class name\n          class_name = class_name.replace("" "", """")\n          new_f.write(class_name + "" "" + confidence + "" "" + left + "" "" + top + "" "" + right + "" "" + bottom + \'\\n\')\n        else:\n          # ground-truth has no ""confidence""\n          class_name, left, top, right, bottom = line.split(args.char)\n          # remove any white space if existent in the class name\n          class_name = class_name.replace("" "", """")\n          new_f.write(class_name  + "" "" + left + "" "" + top + "" "" + right + "" "" + bottom + \'\\n\')\nprint(""Conversion completed!"")\n'"
mAP/extra/remove_space.py,0,"b'import sys\nimport os\nimport glob\nimport argparse\n\n# this script will load class_list.txt and find class names with spaces\n# then replace spaces with delimiters inside ground-truth/ and predicted/\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-d\', \'--delimiter\', type=str, help=""delimiter to replace space (default: \'-\')"", default=\'-\')\nparser.add_argument(\'-y\', \'--yes\', action=\'store_true\', help=""force yes confirmation on yes/no query (default: False)"", default=False)\nargs = parser.parse_args()\n\ndef query_yes_no(question, default=""yes"", bypass=False):\n  """"""Ask a yes/no question via raw_input() and return their answer.\n\n  ""question"" is a string that is presented to the user.\n  ""default"" is the presumed answer if the user just hits <Enter>.\n      It must be ""yes"" (the default), ""no"" or None (meaning\n      an answer is required of the user).\n\n  The ""answer"" return value is True for ""yes"" or False for ""no"".\n  """"""\n  valid = {""yes"": True, ""y"": True, ""ye"": True,\n           ""no"": False, ""n"": False}\n  if default is None:\n    prompt = "" [y/n] ""\n  elif default == ""yes"":\n    prompt = "" [Y/n] ""\n  elif default == ""no"":\n    prompt = "" [y/N] ""\n  else:\n    raise ValueError(""invalid default answer: \'%s\'"" % default)\n\n  while True:\n    sys.stdout.write(question + prompt)\n    if bypass:\n        break\n    if sys.version_info[0] == 3:\n      choice = input().lower() # if version 3 of Python\n    else:\n      choice = raw_input().lower()\n    if default is not None and choice == \'\':\n      return valid[default]\n    elif choice in valid:\n      return valid[choice]\n    else:\n      sys.stdout.write(""Please respond with \'yes\' or \'no\' ""\n                         ""(or \'y\' or \'n\').\\n"")\n\n\ndef rename_class(current_class_name, new_class_name):\n  # get list of txt files\n  file_list = glob.glob(\'*.txt\')\n  file_list.sort()\n  # iterate through the txt files\n  for txt_file in file_list:\n    class_found = False\n    # open txt file lines to a list\n    with open(txt_file) as f:\n      content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    new_content = []\n    # go through each line of eache file\n    for line in content:\n      #class_name = line.split()[0]\n      if current_class_name in line:\n        class_found = True\n        line = line.replace(current_class_name, new_class_name)\n      new_content.append(line)\n    if class_found:\n      # rewrite file\n      with open(txt_file, \'w\') as new_f:\n        for line in new_content:\n          new_f.write(""%s\\n"" % line)\n\nwith open(\'class_list.txt\') as f:\n    for line in f:\n        current_class_name = line.rstrip(""\\n"")\n        new_class_name = line.replace(\' \', args.delimiter).rstrip(""\\n"")\n        if current_class_name == new_class_name:\n            continue\n        y_n_message = (""Are you sure you want ""\n                       ""to rename the class ""\n                       ""\\"""" + current_class_name + ""\\"" ""\n                       ""into \\"""" + new_class_name + ""\\""?""\n                      )\n\n        if query_yes_no(y_n_message, bypass=args.yes):\n          os.chdir(""../ground-truth"")\n          rename_class(current_class_name, new_class_name)\n          os.chdir(""../predicted"")\n          rename_class(current_class_name, new_class_name)\n\nprint(\'Done!\')\n'"
mAP/extra/rename_class.py,0,"b'import sys\nimport os\nimport glob\nimport argparse\n\nparser = argparse.ArgumentParser()\n# argparse current class name to a list (since it can contain more than one word, e.g.""dining table"")\nparser.add_argument(\'-c\', \'--current-class-name\', nargs=\'+\', type=str, help=""current class name e.g.:\\""dining table\\""."", required=True)\n# new class name (should be a single string without any spaces, e.g. ""diningtable"")\nparser.add_argument(\'-n\', \'--new-class-name\', type=str, help=""new class name."", required=True)\nargs = parser.parse_args()\n\ncurrent_class_name = "" "".join(args.current_class_name) # join current name to single string\nnew_class_name = args.new_class_name\n\n\ndef query_yes_no(question, default=""yes""):\n  """"""Ask a yes/no question via raw_input() and return their answer.\n\n  ""question"" is a string that is presented to the user.\n  ""default"" is the presumed answer if the user just hits <Enter>.\n      It must be ""yes"" (the default), ""no"" or None (meaning\n      an answer is required of the user).\n\n  The ""answer"" return value is True for ""yes"" or False for ""no"".\n  """"""\n  valid = {""yes"": True, ""y"": True, ""ye"": True,\n           ""no"": False, ""n"": False}\n  if default is None:\n    prompt = "" [y/n] ""\n  elif default == ""yes"":\n    prompt = "" [Y/n] ""\n  elif default == ""no"":\n    prompt = "" [y/N] ""\n  else:\n    raise ValueError(""invalid default answer: \'%s\'"" % default)\n\n  while True:\n    sys.stdout.write(question + prompt)\n    if sys.version_info[0] == 3:\n      choice = input().lower() # if version 3 of Python\n    else:\n      choice = raw_input().lower()\n    if default is not None and choice == \'\':\n      return valid[default]\n    elif choice in valid:\n      return valid[choice]\n    else:\n      sys.stdout.write(""Please respond with \'yes\' or \'no\' ""\n                         ""(or \'y\' or \'n\').\\n"")\n\n\ndef rename_class(current_class_name, new_class_name):\n  # get list of txt files\n  file_list = glob.glob(\'*.txt\')\n  file_list.sort()\n  # iterate through the txt files\n  for txt_file in file_list:\n    class_found = False\n    # open txt file lines to a list\n    with open(txt_file) as f:\n      content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    new_content = []\n    # go through each line of eache file\n    for line in content:\n      #class_name = line.split()[0]\n      if current_class_name in line:\n        class_found = True\n        line = line.replace(current_class_name, new_class_name)\n      new_content.append(line)\n    if class_found:\n      # rewrite file\n      with open(txt_file, \'w\') as new_f:\n        for line in new_content:\n          new_f.write(""%s\\n"" % line)\n\ny_n_message = (""Are you sure you want ""\n               ""to rename the class ""\n               ""\\"""" + current_class_name + ""\\"" ""\n               ""into \\"""" + new_class_name + ""\\""?""\n              )\n\nif query_yes_no(y_n_message):\n  print("" Ground-Truth folder:"")\n  os.chdir(""../ground-truth"")\n  rename_class(current_class_name, new_class_name)\n  print(""  Done!"")\n  print("" Predicted folder:"")\n  os.chdir(""../predicted"")\n  rename_class(current_class_name, new_class_name)\n  print(""  Done!"")\n'"
