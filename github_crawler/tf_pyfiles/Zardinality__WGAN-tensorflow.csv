file_path,api_count,code
get_svhn.py,0,"b'import sys\nimport os\nfrom six.moves import urllib\nfrom scipy.io import loadmat\nimport numpy as np\n\ndef dense_to_one_hot(labels_dense, num_classes):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = np.arange(num_labels) * num_classes\n  labels_one_hot = np.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\n\ndef maybe_download(data_dir):\n    new_data_dir = os.path.join(data_dir, \'svhn\')\n    if not os.path.exists(new_data_dir):\n        os.makedirs(new_data_dir)\n        def _progress(count, block_size, total_size):\n            sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (float(count * block_size) / float(total_size) * 100.0))\n            sys.stdout.flush()\n        filepath, _ = urllib.request.urlretrieve(\'http://ufldl.stanford.edu/housenumbers/train_32x32.mat\', new_data_dir+\'/train_32x32.mat\', _progress)\n        filepath, _ = urllib.request.urlretrieve(\'http://ufldl.stanford.edu/housenumbers/test_32x32.mat\', new_data_dir+\'/test_32x32.mat\', _progress)\n\ndef load(data_dir, subset=\'train\'):\n    maybe_download(data_dir)\n    if subset==\'train\':\n        train_data = loadmat(os.path.join(data_dir, \'svhn\') + \'/train_32x32.mat\')\n        trainx = train_data[\'X\']\n        trainy = train_data[\'y\'].flatten()\n        trainy[trainy==10] = 0\n        trainx = trainx.transpose((3, 0, 1, 2))\n        trainy = dense_to_one_hot(trainy, 10)\n        return trainx, trainy\n    elif subset==\'test\':\n        test_data = loadmat(os.path.join(data_dir, \'svhn\') + \'/test_32x32.mat\')\n        testx = test_data[\'X\']\n        testy = test_data[\'y\'].flatten()\n        testy[testy==10] = 0\n        testx = testx.transpose((3, 0, 1, 2))\n        testy = dense_to_one_hot(testy, 10)\n        return testx, testy\n    else:\n        raise NotImplementedError(\'subset should be either train or test\')\n\ndef main():\n    # maybe_download(\'./\')\n    tx, ty = load(\'./\')\n    print(tx.shape)\n\n\nif __name__ == \'__main__\':\n    main()'"
load_svhn.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\n\nfrom tensorflow.contrib.learn.python.learn.datasets import base\nfrom tensorflow.python.framework import dtypes\nfrom get_svhn import load\n\ndef dense_to_one_hot(labels_dense, num_classes):\n  """"""Convert class labels from scalars to one-hot vectors.""""""\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\n\n\n\n\nclass DataSet(object):\n\n  def __init__(self,\n               images,\n               labels,\n               fake_data=False,\n               one_hot=False,\n               dtype=dtypes.float32,\n               reshape=False):\n    """"""Construct a DataSet.\n    one_hot arg is used only if fake_data is true.  `dtype` can be either\n    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n    `[0, 1]`.\n    """"""\n    dtype = dtypes.as_dtype(dtype).base_dtype\n    if dtype not in (dtypes.uint8, dtypes.float32):\n      raise TypeError(\'Invalid image dtype %r, expected uint8 or float32\' %\n                      dtype)\n    if fake_data:\n      self._num_examples = 10000\n      self.one_hot = one_hot\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \'images.shape: %s labels.shape: %s\' % (images.shape, labels.shape))\n      self._num_examples = images.shape[0]\n\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      if reshape:\n        assert images.shape[3] == 1\n        images = images.reshape(images.shape[0],\n                                images.shape[1] * images.shape[2])\n      if dtype == dtypes.float32:\n        # Convert from [0, 255] -> [0.0, 1.0].\n        images = images.astype(numpy.float32)\n        images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n\n  @property\n  def images(self):\n    return self._images\n\n  @property\n  def labels(self):\n    return self._labels\n\n  @property\n  def num_examples(self):\n    return self._num_examples\n\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n\n  def next_batch(self, batch_size, fake_data=False):\n    """"""Return the next `batch_size` examples from this data set.""""""\n    if fake_data:\n      fake_image = [1] * 1024\n      if self.one_hot:\n        fake_label = [1] + [0] * 9\n      else:\n        fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)\n      ]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_dir,\n                   fake_data=False,\n                   one_hot=False,\n                   dtype=dtypes.float32,\n                   reshape=False,\n                   validation_size=5000):\n  if fake_data:\n\n    def fake():\n      return DataSet([], [], fake_data=True, one_hot=one_hot, dtype=dtype)\n\n    train = fake()\n    validation = fake()\n    test = fake()\n    return base.Datasets(train=train, validation=validation, test=test)\n\n#   TRAIN_IMAGES = \'train-images-idx3-ubyte.gz\'\n#   TRAIN_LABELS = \'train-labels-idx1-ubyte.gz\'\n#   TEST_IMAGES = \'t10k-images-idx3-ubyte.gz\'\n#   TEST_LABELS = \'t10k-labels-idx1-ubyte.gz\'\n\n#   local_file = base.maybe_download(TRAIN_IMAGES, train_dir,\n#                                    SOURCE_URL + TRAIN_IMAGES)\n#   with open(local_file, \'rb\') as f:\n#     train_images = extract_images(f)\n\n#   local_file = base.maybe_download(TRAIN_LABELS, train_dir,\n#                                    SOURCE_URL + TRAIN_LABELS)\n#   with open(local_file, \'rb\') as f:\n#     train_labels = extract_labels(f, one_hot=one_hot)\n\n#   local_file = base.maybe_download(TEST_IMAGES, train_dir,\n#                                    SOURCE_URL + TEST_IMAGES)\n#   with open(local_file, \'rb\') as f:\n#     test_images = extract_images(f)\n\n#   local_file = base.maybe_download(TEST_LABELS, train_dir,\n#                                    SOURCE_URL + TEST_LABELS)\n#   with open(local_file, \'rb\') as f:\n#     test_labels = extract_labels(f, one_hot=one_hot)\n\n  train_images, train_labels = load(train_dir, subset=\'train\')\n  test_images, test_labels = load(train_dir, subset=\'test\')  \n  if not 0 <= validation_size <= len(train_images):\n    raise ValueError(\n        \'Validation size should be between 0 and {}. Received: {}.\'\n        .format(len(train_images), validation_size))\n\n  validation_images = train_images[:validation_size]\n  validation_labels = train_labels[:validation_size]\n  train_images = train_images[validation_size:]\n  train_labels = train_labels[validation_size:]\n\n  train = DataSet(train_images, train_labels, dtype=dtype, reshape=reshape)\n  validation = DataSet(validation_images,\n                       validation_labels,\n                       dtype=dtype,\n                       reshape=reshape)\n  test = DataSet(test_images, test_labels, dtype=dtype, reshape=reshape)\n\n  return base.Datasets(train=train, validation=validation, test=test)\n\n\ndef load_svhn(train_dir=\'./\'):\n  return read_data_sets(train_dir)\n'"
