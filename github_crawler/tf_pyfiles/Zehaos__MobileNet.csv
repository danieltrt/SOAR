file_path,api_count,code
eval_image_classifier.py,30,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 100, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\n    \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size\')\n\ntf.app.flags.DEFINE_float(\'width_multiplier\', 1.0,\n                            \'Width Multiplier, for MobileNet only.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = slim.get_or_create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ####################\n    # Select the model #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        is_training=False,\n        width_multiplier=FLAGS.width_multiplier)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=False,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size)\n    [image, label] = provider.get([\'image\', \'label\'])\n    label -= FLAGS.labels_offset\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=False)\n\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n    images, labels = tf.train.batch(\n        [image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    ####################\n    # Define the model #\n    ####################\n    logits, _ = network_fn(images)\n\n    if FLAGS.moving_average_decay:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    predictions = tf.argmax(logits, 1)\n    labels = tf.squeeze(labels)\n\n    # Define the metrics:\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n        \'Recall_5\': slim.metrics.streaming_recall_at_k(\n            logits, labels, 5),\n    })\n\n    # Print the summaries to screen.\n    for name, value in names_to_values.iteritems():\n      summary_name = \'eval/%s\' % name\n      op = tf.summary.scalar(summary_name, value, collections=[])\n      op = tf.Print(op, [value], summary_name)\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n    # TODO(sguada) use num_epochs=1\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n    slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()),\n        variables_to_restore=variables_to_restore)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
train_image_classifier.py,95,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\nfrom optimizer.yellowfin import YFOptimizer\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\ntf.app.flags.DEFINE_float(\'width_multiplier\', 1.0,\n                            \'Width Multiplier, for MobileNet only.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  elif FLAGS.optimizer == \'yellowfin\':\n    optimizer = YFOptimizer(lr=1.0, mu=0.0)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n  summaries = []\n  for variable in slim.get_model_variables():\n    summaries.append(tf.summary.histogram(variable.op.name, variable))\n  summaries.append(tf.summary.scalar(\'training/Learning Rate\', learning_rate))\n  return summaries\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    #######################\n    # Config model_deploy #\n    #######################\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.worker_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks)\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ######################\n    # Select the network #\n    ######################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        weight_decay=FLAGS.weight_decay,\n        is_training=True,\n        width_multiplier=FLAGS.width_multiplier)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n          dataset,\n          num_readers=FLAGS.num_readers,\n          common_queue_capacity=20 * FLAGS.batch_size,\n          common_queue_min=10 * FLAGS.batch_size)\n      [image, label] = provider.get([\'image\', \'label\'])\n      label -= FLAGS.labels_offset\n\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n\n    ####################\n    # Define the model #\n    ####################\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      images, labels = batch_queue.dequeue()\n      logits, end_points = network_fn(images)\n\n      #############################\n      # Specify the loss function #\n      #############################\n      if \'AuxLogits\' in end_points:\n        tf.losses.softmax_cross_entropy(\n            logits=end_points[\'AuxLogits\'], onehot_labels=labels,\n            label_smoothing=FLAGS.label_smoothing, weights=0.4, scope=\'aux_loss\')\n      tf.losses.softmax_cross_entropy(\n          logits=logits, onehot_labels=labels,\n          label_smoothing=FLAGS.label_smoothing, weights=1.0)\n      return end_points\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n      summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables,\n          replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n          total_num_replicas=FLAGS.worker_replicas)\n    elif FLAGS.moving_average_decay:\n      # Update ops executed locally by trainer.\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    variables_to_train = _get_variables_to_train()\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n    # Add total_loss to summary.\n    summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    # Create gradient updates.\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    update_op = tf.group(*update_ops)\n    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    slim.learning.train(\n        train_tensor,\n        logdir=FLAGS.train_dir,\n        master=FLAGS.master,\n        is_chief=(FLAGS.task == 0),\n        init_fn=_get_init_fn(),\n        summary_op=summary_op,\n        number_of_steps=FLAGS.max_number_of_steps,\n        log_every_n_steps=FLAGS.log_every_n_steps,\n        save_summaries_secs=FLAGS.save_summaries_secs,\n        save_interval_secs=FLAGS.save_interval_secs,\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
train_object_detector.py,98,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nfrom utils.det_utils import encode_annos, losses, interpre_prediction\n\nfrom configs.kitti_config import config\n\nimport tensorflow.contrib.slim as slim\n\n# slim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n  \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n  \'train_dir\', \'/tmp/tfmodel/\',\n  \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n  \'num_ps_tasks\', 0,\n  \'The number of parameter servers. If the value is 0, then the parameters \'\n  \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n  \'num_readers\', 4,\n  \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n  \'num_preprocessing_threads\', 4,\n  \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n  \'log_every_n_steps\', 10,\n  \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n  \'save_summaries_secs\', 600,\n  \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n  \'save_interval_secs\', 600,\n  \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n  \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n  \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n  \'optimizer\', \'rmsprop\',\n  \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n  \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n  \'adadelta_rho\', 0.95,\n  \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n  \'adagrad_initial_accumulator_value\', 0.1,\n  \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n  \'adam_beta1\', 0.9,\n  \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n  \'adam_beta2\', 0.999,\n  \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n  \'ftrl_initial_accumulator_value\', 0.1,\n  \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n  \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n  \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n  \'momentum\', 0.9,\n  \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n  \'learning_rate_decay_type\',\n  \'exponential\',\n  \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n  \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n  \'end_learning_rate\', 0.0001,\n  \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n  \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n  \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n  \'num_epochs_per_decay\', 2.0,\n  \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n  \'sync_replicas\', False,\n  \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n  \'replicas_to_aggregate\', 1,\n  \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n  \'moving_average_decay\', None,\n  \'The decay to use for the moving average.\'\n  \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n  \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n  \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n  \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n  \'labels_offset\', 0,\n  \'An offset for the labels in the dataset. This flag is primarily used to \'\n  \'evaluate the VGG and ResNet architectures which do not use a background \'\n  \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n  \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n  \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n                              \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n  \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n  \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\ntf.app.flags.DEFINE_float(\'width_multiplier\', 1.0,\n                          \'Width Multiplier, for MobileNet only.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n  \'checkpoint_path\', None,\n  \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n  \'checkpoint_exclude_scopes\', None,\n  \'Comma-separated list of scopes of variables to exclude when restoring \'\n  \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n  \'trainable_scopes\', None,\n  \'Comma-separated list of scopes to filter the set of variables to train.\'\n  \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n  \'ignore_missing_vars\', False,\n  \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n      learning_rate,\n      rho=FLAGS.adadelta_rho,\n      epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n      learning_rate,\n      initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n      learning_rate,\n      beta1=FLAGS.adam_beta1,\n      beta2=FLAGS.adam_beta2,\n      epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n      learning_rate,\n      learning_rate_power=FLAGS.ftrl_learning_rate_power,\n      initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n      l1_regularization_strength=FLAGS.ftrl_l1,\n      l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n      learning_rate,\n      momentum=FLAGS.momentum,\n      name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n      learning_rate,\n      decay=FLAGS.rmsprop_decay,\n      momentum=FLAGS.rmsprop_momentum,\n      epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n  summaries = []\n  for variable in slim.get_model_variables():\n    summaries.append(tf.summary.histogram(variable.op.name, variable))\n  summaries.append(tf.summary.scalar(\'training/Learning Rate\', learning_rate))\n  return summaries\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n      \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n      % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n    checkpoint_path,\n    variables_to_restore,\n    ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    #######################\n    # Config model_deploy #\n    #######################\n    deploy_config = model_deploy.DeploymentConfig(\n      num_clones=FLAGS.num_clones,\n      clone_on_cpu=FLAGS.clone_on_cpu,\n      replica_id=FLAGS.task,\n      num_replicas=FLAGS.worker_replicas,\n      num_ps_tasks=FLAGS.num_ps_tasks)\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n      FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ######################\n    # Select the network #\n    ######################\n    network_fn = nets_factory.get_network_fn(\n      FLAGS.model_name,\n      num_classes=(dataset.num_classes - FLAGS.labels_offset),\n      weight_decay=FLAGS.weight_decay,\n      is_training=True,\n      width_multiplier=FLAGS.width_multiplier)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n      preprocessing_name,\n      is_training=True)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        num_readers=FLAGS.num_readers,\n        common_queue_capacity=20 * FLAGS.batch_size,\n        common_queue_min=10 * FLAGS.batch_size)\n\n      # gt_bboxes format [ymin, xmin, ymax, xmax]\n      [image, img_shape, gt_labels, gt_bboxes] = provider.get([\'image\', \'shape\',\n                                                               \'object/label\',\n                                                               \'object/bbox\'])\n\n      # Preprocesing\n      # gt_bboxes = scale_bboxes(gt_bboxes, img_shape)  # bboxes format [0,1) for tf draw\n\n      image, gt_labels, gt_bboxes = image_preprocessing_fn(image,\n                                                           config.IMG_HEIGHT,\n                                                           config.IMG_WIDTH,\n                                                           labels=gt_labels,\n                                                           bboxes=gt_bboxes,\n                                                           )\n\n      #############################################\n      # Encode annotations for losses computation #\n      #############################################\n\n      # anchors format [cx, cy, w, h]\n      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n\n      # encode annos, box_input format [cx, cy, w, h]\n      input_mask, labels_input, box_delta_input, box_input = encode_annos(gt_labels,\n                                                                          gt_bboxes,\n                                                                          anchors,\n                                                                          config.NUM_CLASSES)\n\n      images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input = tf.train.batch(\n        [image, input_mask, labels_input, box_delta_input, box_input],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n        [images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input], capacity=2 * deploy_config.num_clones)\n\n    ####################\n    # Define the model #\n    ####################\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      images, b_input_mask, b_labels_input, b_box_delta_input, b_box_input = batch_queue.dequeue()\n      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n      end_points = network_fn(images)\n      end_points[""viz_images""] = images\n      conv_ds_14 = end_points[\'MobileNet/conv_ds_14/depthwise_conv\']\n      dropout = slim.dropout(conv_ds_14, keep_prob=0.5, is_training=True)\n      num_output = config.NUM_ANCHORS * (config.NUM_CLASSES + 1 + 4)\n      predict = slim.conv2d(dropout, num_output, kernel_size=(3, 3), stride=1, padding=\'SAME\',\n                            activation_fn=None,\n                            weights_initializer=tf.truncated_normal_initializer(stddev=0.0001),\n                            scope=""MobileNet/conv_predict"")\n\n      with tf.name_scope(""Interpre_prediction"") as scope:\n        pred_box_delta, pred_class_probs, pred_conf, ious, det_probs, det_boxes, det_class = \\\n          interpre_prediction(predict, b_input_mask, anchors, b_box_input)\n        end_points[""viz_det_probs""] = det_probs\n        end_points[""viz_det_boxes""] = det_boxes\n        end_points[""viz_det_class""] = det_class\n\n      with tf.name_scope(""Losses"") as scope:\n        losses(b_input_mask, b_labels_input, ious, b_box_delta_input, pred_class_probs, pred_conf, pred_box_delta)\n\n      return end_points\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      if end_point not in [""viz_images"", ""viz_det_probs"", ""viz_det_boxes"", ""viz_det_class""]:\n        x = end_points[end_point]\n        summaries.add(tf.summary.histogram(\'activations/\' + end_point, x))\n        summaries.add(tf.summary.scalar(\'sparsity/\' + end_point,\n                                        tf.nn.zero_fraction(x)))\n\n    # Add summaries for det result TODO(shizehao): vizulize prediction\n\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.summary.scalar(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.summary.histogram(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.summary.scalar(\'learning_rate\', learning_rate))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      optimizer = tf.train.SyncReplicasOptimizer(\n        opt=optimizer,\n        replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n        variable_averages=variable_averages,\n        variables_to_average=moving_average_variables,\n        replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n        total_num_replicas=FLAGS.worker_replicas)\n    elif FLAGS.moving_average_decay:\n      # Update ops executed locally by trainer.\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    variables_to_train = _get_variables_to_train()\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n      clones,\n      optimizer,\n      var_list=variables_to_train)\n    # Add total_loss to summary.\n    summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    # Create gradient updates.\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    update_op = tf.group(*update_ops)\n    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    slim.learning.train(\n      train_tensor,\n      logdir=FLAGS.train_dir,\n      master=FLAGS.master,\n      is_chief=(FLAGS.task == 0),\n      init_fn=_get_init_fn(),\n      summary_op=summary_op,\n      number_of_steps=FLAGS.max_number_of_steps,\n      log_every_n_steps=FLAGS.log_every_n_steps,\n      save_summaries_secs=FLAGS.save_summaries_secs,\n      save_interval_secs=FLAGS.save_interval_secs,\n      sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
configs/__init__.py,0,b'\n'
configs/kitti_config.py,0,"b'from easydict import EasyDict as edict\nimport numpy as np\n\nconfig = edict()\n\nconfig.IMG_HEIGHT = 375\nconfig.IMG_WIDTH = 1242\n\n# TODO(shizehao): infer fea shape in run time\nconfig.FEA_HEIGHT = 12\nconfig.FEA_WIDTH = 39\n\nconfig.EPSILON = 1e-16\n\nconfig.LOSS_COEF_BBOX = 5.0\nconfig.LOSS_COEF_CONF_POS = 75.0\nconfig.LOSS_COEF_CONF_NEG = 100.0\nconfig.LOSS_COEF_CLASS = 1.0\n\nconfig.EXP_THRESH = 1.0\n\nconfig.RBG_MEANS = np.array([[[ 123.68, 116.779, 103.939]]])\n\n\ndef set_anchors(H, W):\n  B = 9\n  shape = np.array(\n          [[  36.,  37.], [ 366., 174.], [ 115.,  59.],\n           [ 162.,  87.], [  38.,  90.], [ 258., 173.],\n           [ 224., 108.], [  78., 170.], [  72.,  43.]])\n\n  # # scale\n  # shape[:, 0] = shape[:, 0] / config.IMG_HEIGHT\n  # shape[:, 1] = shape[:, 1] / config.IMG_WIDTH\n\n  anchor_shapes = np.reshape(\n      [shape] * H * W,\n      (H, W, B, 2)\n  )\n  center_x = np.reshape(\n      np.transpose(\n          np.reshape(\n              np.array([np.arange(1, W+1)*float(config.IMG_WIDTH)/(W+1)]*H*B),\n              (B, H, W)\n          ),\n          (1, 2, 0)\n      ),\n      (H, W, B, 1)\n  )\n  center_y = np.reshape(\n      np.transpose(\n          np.reshape(\n              np.array([np.arange(1, H+1)*float(config.IMG_HEIGHT)/(H+1)]*W*B),\n              (B, W, H)\n          ),\n          (2, 1, 0)\n      ),\n      (H, W, B, 1)\n  )\n  anchors = np.reshape(\n      np.concatenate((center_x, center_y, anchor_shapes), axis=3),\n      (-1, 4)\n  )\n\n  return anchors\n\nconfig.ANCHOR_SHAPE = set_anchors(config.FEA_HEIGHT, config.FEA_WIDTH)\n\nconfig.NUM_ANCHORS = 9\nconfig.NUM_CLASSES = 3\nconfig.ANCHORS = config.NUM_ANCHORS * config.FEA_HEIGHT * config.FEA_WIDTH\n\nconfig.PLOT_PROB_THRESH = 0.4\nconfig.NMS_THRESH = 0.4\nconfig.PROB_THRESH = 0.005\nconfig.TOP_N_DETECTION = 64\n\n\n\n\n\n\n'"
datasets/__init__.py,0,b'\n'
datasets/cifar10.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/datasets/download_and_convert_cifar10.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/dataset_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\nfrom datasets import kitti\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n    \'kitti\': kitti,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n'"
datasets/dataset_utils.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef float_feature(value):\n    """"""Wrapper for inserting float features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef bytes_feature(value):\n    """"""Wrapper for inserting bytes features into Example proto.\n    """"""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'r\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n'"
datasets/download_and_convert_cifar10.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cPickle\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'r\') as f:\n    data = cPickle.load(f)\n\n  images = data[\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n'"
datasets/download_and_convert_flowers.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts Flowers data to TFRecords of TF-Example protos.\n\nThis module downloads the Flowers data, uncompresses it, reads the files\nthat make up the Flowers data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\n\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the Flowers data can be downloaded.\n_DATA_URL = \'http://download.tensorflow.org/example_images/flower_photos.tgz\'\n\n# The number of images in the validation set.\n_NUM_VALIDATION = 350\n\n# Seed for repeatability.\n_RANDOM_SEED = 0\n\n# The number of shards per dataset split.\n_NUM_SHARDS = 5\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _get_filenames_and_classes(dataset_dir):\n  """"""Returns a list of filenames and inferred class names.\n\n  Args:\n    dataset_dir: A directory containing a set of subdirectories representing\n      class names. Each subdirectory should contain PNG or JPG encoded images.\n\n  Returns:\n    A list of image file paths, relative to `dataset_dir` and the list of\n    subdirectories, representing class names.\n  """"""\n  flower_root = os.path.join(dataset_dir, \'flower_photos\')\n  directories = []\n  class_names = []\n  for filename in os.listdir(flower_root):\n    path = os.path.join(flower_root, filename)\n    if os.path.isdir(path):\n      directories.append(path)\n      class_names.append(filename)\n\n  photo_filenames = []\n  for directory in directories:\n    for filename in os.listdir(directory):\n      path = os.path.join(directory, filename)\n      photo_filenames.append(path)\n\n  return photo_filenames, sorted(class_names)\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n  output_filename = \'flowers_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, _NUM_SHARDS)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n  """"""Converts the given filenames to a TFRecord dataset.\n\n  Args:\n    split_name: The name of the dataset, either \'train\' or \'validation\'.\n    filenames: A list of absolute paths to png or jpg images.\n    class_names_to_ids: A dictionary from class names (strings) to ids\n      (integers).\n    dataset_dir: The directory where the converted datasets are stored.\n  """"""\n  assert split_name in [\'train\', \'validation\']\n\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n\n  with tf.Graph().as_default():\n    image_reader = ImageReader()\n\n    with tf.Session(\'\') as sess:\n\n      for shard_id in range(_NUM_SHARDS):\n        output_filename = _get_dataset_filename(\n            dataset_dir, split_name, shard_id)\n\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n          for i in range(start_ndx, end_ndx):\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (\n                i+1, len(filenames), shard_id))\n            sys.stdout.flush()\n\n            # Read the filename:\n            image_data = tf.gfile.FastGFile(filenames[i], \'r\').read()\n            height, width = image_reader.read_image_dims(sess, image_data)\n\n            class_name = os.path.basename(os.path.dirname(filenames[i]))\n            class_id = class_names_to_ids[class_name]\n\n            example = dataset_utils.image_to_tfexample(\n                image_data, \'jpg\', height, width, class_id)\n            tfrecord_writer.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'flower_photos\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef _dataset_exists(dataset_dir):\n  for split_name in [\'train\', \'validation\']:\n    for shard_id in range(_NUM_SHARDS):\n      output_filename = _get_dataset_filename(\n          dataset_dir, split_name, shard_id)\n      if not tf.gfile.Exists(output_filename):\n        return False\n  return True\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  if _dataset_exists(dataset_dir):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n  # Divide into train and test:\n  random.seed(_RANDOM_SEED)\n  random.shuffle(photo_filenames)\n  training_filenames = photo_filenames[_NUM_VALIDATION:]\n  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n\n  # First, convert the training and validation sets.\n  _convert_dataset(\'train\', training_filenames, class_names_to_ids,\n                   dataset_dir)\n  _convert_dataset(\'validation\', validation_filenames, class_names_to_ids,\n                   dataset_dir)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Flowers dataset!\')\n\n'"
datasets/download_and_convert_mnist.py,11,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\n\nThis module downloads the MNIST data, uncompresses it, reads the files\nthat make up the MNIST data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URLs where the MNIST data can be downloaded.\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\n\n_IMAGE_SIZE = 28\n_NUM_CHANNELS = 1\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'zero\',\n    \'one\',\n    \'two\',\n    \'three\',\n    \'four\',\n    \'five\',\n    \'size\',\n    \'seven\',\n    \'eight\',\n    \'nine\',\n]\n\n\ndef _extract_images(filename, num_images):\n  """"""Extract the images into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].\n  """"""\n  print(\'Extracting images from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  return data\n\n\ndef _extract_labels(filename, num_labels):\n  """"""Extract the labels into a vector of int64 label IDs.\n\n  Args:\n    filename: The path to an MNIST labels file.\n    num_labels: The number of labels in the file.\n\n  Returns:\n    A numpy array of shape [number_of_labels]\n  """"""\n  print(\'Extracting labels from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_labels)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n  return labels\n\n\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\n                     tfrecord_writer):\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\n\n  Args:\n    data_filename: The filename of the MNIST images.\n    labels_filename: The filename of the MNIST labels.\n    num_images: The number of images in the dataset.\n    tfrecord_writer: The TFRecord writer to use for writing.\n  """"""\n  images = _extract_images(data_filename, num_images)\n  labels = _extract_labels(labels_filename, num_images)\n\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  with tf.Graph().as_default():\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n    encoded_png = tf.image.encode_png(image)\n\n    with tf.Session(\'\') as sess:\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\n        sys.stdout.flush()\n\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n        tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_dataset(dataset_dir):\n  """"""Downloads MNIST locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n\n    if not os.path.exists(filepath):\n      print(\'Downloading file %s...\' % filename)\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n                                               filepath,\n                                               _progress)\n      print()\n      with tf.gfile.GFile(filepath) as f:\n        size = f.size()\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n    tf.gfile.Remove(filepath)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  _download_dataset(dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the MNIST dataset!\')\n'"
datasets/flowers.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the flowers dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/datasets/download_and_convert_flowers.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'flowers_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading flowers.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/imagenet.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n  else:\n    labels_to_names = create_readable_names_for_imagenet_labels()\n    dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
datasets/kitti.py,1,"b'# Copyright 2017 Zehao Shi. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the KITTI Object Dataset (images + annotations).\n""""""\nimport tensorflow as tf\nfrom datasets import kitti_common\n\nslim = tf.contrib.slim\n\nFILE_PATTERN = \'%s_*.tfrecord\'\n\nITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'shape\': \'Shape of the image\',\n    \'object/bbox\': \'A list of bounding boxes, one per each object.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\nSPLITS_TO_SIZES = {\n    \'train\': 3740,\n    \'val\': 3741,\n}\n\nNUM_CLASSES = 3\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if not file_pattern:\n        file_pattern = FILE_PATTERN\n    return kitti_common.get_split(split_name, dataset_dir,\n                                      file_pattern, reader,\n                                      SPLITS_TO_SIZES,\n                                      ITEMS_TO_DESCRIPTIONS,\n                                      NUM_CLASSES)\n'"
datasets/kitti_common.py,13,"b'# Copyright 2017 Zehao Shi. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the KITTI Object Dataset (images + annotations).\n""""""\nimport os\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef get_split(split_name, dataset_dir, file_pattern, reader, split_to_sizes,\n              items_to_descriptions, num_classes):\n  """"""Gets a dataset tuple with instructions for reading KITTI dataset.\n\n  Args:\n    split_name: A train/val split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n      ValueError: if `split_name` is not a valid train/val split.\n  """"""\n  if split_name not in [\'train\', \'val\']:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n    \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n    \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n    \'image/height\': tf.FixedLenFeature([1], tf.int64),\n    \'image/width\': tf.FixedLenFeature([1], tf.int64),\n    \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n    \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n    \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n    \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n    \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n    \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n    \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n  }\n  items_to_handlers = {\n    \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n    \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n    \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n      [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n    \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n  }\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n    keys_to_features, items_to_handlers)\n\n  labels_to_names = {0: ""Pedestrian"",\n                     1: ""Cyclist"",\n                     2: ""Car""}\n\n  return slim.dataset.Dataset(\n    data_sources=file_pattern,\n    reader=reader,\n    decoder=decoder,\n    num_samples=split_to_sizes[split_name],\n    items_to_descriptions=items_to_descriptions,\n    num_classes=num_classes,\n    labels_to_names=labels_to_names)\n'"
datasets/kitti_object_to_tfrecords.py,5,"b'# Copyright 2017 Zehao Shi. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts KITTI Object data to TFRecords file format with Example protos.\n\nThe raw KITTI Object data set is expected to reside in PNG files located in the\ndirectory \'image_2\'. Similarly, annotations are supposed to be stored in the\n\'label_2\'.\n\n""""""\nimport os\nimport sys\nimport random\nimport re\n\nimport tensorflow as tf\nimport cv2\nimport numpy as np\nfrom datasets.dataset_utils import int64_feature, float_feature, bytes_feature\n\n# TFRecords convertion parameters.\nRANDOM_SEED = 4242\nSAMPLES_PER_FILES = 200\n\nCLASSES = {\n    \'Pedestrian\': 0,\n    \'Cyclist\': 1,\n    \'Car\': 2,\n}\n\ndef _process_image(directory, split, name):\n    # Read the image file.\n    filename = os.path.join(directory, \'image_2\', name + \'.png\')\n    image_data = tf.gfile.FastGFile(filename, \'r\').read()\n\n    # Get shape\n    img = cv2.imread(filename)\n    shape = np.shape(img)\n\n    label_list = []\n    type_list = []\n\n    bbox_x1_list = []\n    bbox_y1_list = []\n    bbox_x2_list = []\n    bbox_y2_list = []\n\n\n    # If \'test\' split, skip annotations\n    if re.findall(r\'train\', split):\n      # Read the txt annotation file.\n      filename = os.path.join(directory, \'label_2\', name + \'.txt\')\n      with open(filename) as anno_file:\n        objects = anno_file.readlines()\n\n      for object in objects:\n          obj_anno = object.split(\' \')\n          type_txt = obj_anno[0].encode(\'ascii\')\n          if type_txt in CLASSES:\n            label_list.append(CLASSES[type_txt])\n            type_list.append(type_txt)\n\n            # Bounding Box\n            bbox_x1 = float(obj_anno[4])\n            bbox_y1 = float(obj_anno[5])\n            bbox_x2 = float(obj_anno[6])\n            bbox_y2 = float(obj_anno[7])\n            bbox_x1_list.append(bbox_x1)\n            bbox_y1_list.append(bbox_y1)\n            bbox_x2_list.append(bbox_x2)\n            bbox_y2_list.append(bbox_y2)\n\n    image_format = b\'PNG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/encoded\': bytes_feature(image_data),\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/shape\': int64_feature(shape),\n            \'image/object/bbox/xmin\': float_feature(bbox_x1_list),\n            \'image/object/bbox/xmax\': float_feature(bbox_x2_list),\n            \'image/object/bbox/ymin\': float_feature(bbox_y1_list),\n            \'image/object/bbox/ymax\': float_feature(bbox_y2_list),\n            \'image/object/bbox/label\': int64_feature(label_list),\n            \'image/object/bbox/label_text\': bytes_feature(type_list),\n    }))\n    return example\n\n\ndef _add_to_tfrecord(dataset_dir, split, name, tfrecord_writer):\n    """"""Loads data from image and annotations files and add them to a TFRecord.\n\n    Args:\n      dataset_dir: Dataset directory;\n      split: train/val/test\n      name: Image name;\n      tfrecord_writer: The TFRecord writer to use for writing.\n    """"""\n    example = _process_image(dataset_dir, split, name)\n    tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(output_dir, name, idx):\n    return \'%s/%s_%03d.tfrecord\' % (output_dir, name, idx)\n\n\ndef run(kitti_root, split, output_dir, shuffling=False):\n    """"""Runs the conversion operation.\n\n    Args:\n      kitti_root: KITTI dataset root dir.\n      split: trainval/train/val\n      output_dir: Output directory.\n    """"""\n    if not tf.gfile.Exists(output_dir):\n        tf.gfile.MakeDirs(output_dir)\n\n    # Dataset filenames, and shuffling.\n    split_file_path = os.path.join(kitti_root,\n                                 \'ImageSets\',\n                                 \'%s.txt\'%split)\n    with open(split_file_path) as f:\n        filenames = f.readlines()\n\n    if shuffling:\n        random.seed(RANDOM_SEED)\n        random.shuffle(filenames)\n\n    # Process dataset files.\n    i = 0\n    fidx = 0\n    image_dir = os.path.join(kitti_root, \'%sing\'%split)\n    if split == \'val\':\n      image_dir = os.path.join(kitti_root, \'%sing\' % \'train\')\n    while i < len(filenames):\n        # Open new TFRecord file.\n        tf_filename = _get_output_filename(output_dir, split, fidx)\n        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n            j = 0\n            while i < len(filenames) and j < SAMPLES_PER_FILES:\n                sys.stdout.write(\'\\r>> Converting image %d/%d\' % (i+1, len(filenames)))\n                sys.stdout.flush()\n\n                filename = filenames[i].strip()\n                _add_to_tfrecord(image_dir, split, filename, tfrecord_writer)\n                i += 1\n                j += 1\n            fidx += 1\n\n    print(\'\\nFinished converting the KITTI dataset!\')\n'"
datasets/mnist.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the MNIST dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/datasets/download_and_convert_mnist.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'mnist_%s.tfrecord\'\n\n_SPLITS_TO_SIZES = {\'train\': 60000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [28 x 28 x 1] grayscale image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n'"
datasets/pascalvoc_common.py,15,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Pascal VOC Dataset (images + annotations).\n""""""\nimport os\n\nimport tensorflow as tf\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\nVOC_LABELS = {\n    \'none\': (0, \'Background\'),\n    \'aeroplane\': (1, \'Vehicle\'),\n    \'bicycle\': (2, \'Vehicle\'),\n    \'bird\': (3, \'Animal\'),\n    \'boat\': (4, \'Vehicle\'),\n    \'bottle\': (5, \'Indoor\'),\n    \'bus\': (6, \'Vehicle\'),\n    \'car\': (7, \'Vehicle\'),\n    \'cat\': (8, \'Animal\'),\n    \'chair\': (9, \'Indoor\'),\n    \'cow\': (10, \'Animal\'),\n    \'diningtable\': (11, \'Indoor\'),\n    \'dog\': (12, \'Animal\'),\n    \'horse\': (13, \'Animal\'),\n    \'motorbike\': (14, \'Vehicle\'),\n    \'person\': (15, \'Person\'),\n    \'pottedplant\': (16, \'Indoor\'),\n    \'sheep\': (17, \'Animal\'),\n    \'sofa\': (18, \'Indoor\'),\n    \'train\': (19, \'Vehicle\'),\n    \'tvmonitor\': (20, \'Indoor\'),\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern, reader,\n              items_to_descriptions, num_classes):\n    """"""Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n\n    Args:\n      split_name: A trainval/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a \'%s\' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    """"""\n    if split_name not in [\'trainval\', \'test\']:\n        raise ValueError(\'split name %s was not recognized.\' % split_name)\n    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.TFRecordReader\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n        \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'jpeg\'),\n        \'image/height\': tf.FixedLenFeature([1], tf.int64),\n        \'image/width\': tf.FixedLenFeature([1], tf.int64),\n        \'image/channels\': tf.FixedLenFeature([1], tf.int64),\n        \'image/shape\': tf.FixedLenFeature([3], tf.int64),\n        \'image/object/bbox/xmin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymin\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/xmax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/ymax\': tf.VarLenFeature(dtype=tf.float32),\n        \'image/object/bbox/label\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/difficult\': tf.VarLenFeature(dtype=tf.int64),\n        \'image/object/bbox/truncated\': tf.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n        \'shape\': slim.tfexample_decoder.Tensor(\'image/shape\'),\n        \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n                [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n        \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/bbox/label\'),\n        \'object/difficult\': slim.tfexample_decoder.Tensor(\'image/object/bbox/difficult\'),\n        \'object/truncated\': slim.tfexample_decoder.Tensor(\'image/object/bbox/truncated\'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n\n    labels_to_names = None\n    if dataset_utils.has_labels(dataset_dir):\n        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n    # else:\n    #     labels_to_names = create_readable_names_for_imagenet_labels()\n    #     dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n    return slim.dataset.Dataset(\n            data_sources=file_pattern,\n            reader=reader,\n            decoder=decoder,\n            num_samples=split_to_sizes[split_name],\n            items_to_descriptions=items_to_descriptions,\n            num_classes=num_classes,\n            labels_to_names=labels_to_names)\n'"
datasets/pascalvoc_to_tfrecords.py,5,"b'# Copyright 2015 Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts Pascal VOC data to TFRecords file format with Example protos.\n\nThe raw Pascal VOC data set is expected to reside in JPEG files located in the\ndirectory \'JPEGImages\'. Similarly, bounding box annotations are supposed to be\nstored in the \'Annotation directory\'\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\nEach validation TFRecord file contains ~500 records. Each training TFREcord\nfile contains ~1000 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n    image/encoded: string containing JPEG encoded image in RGB colorspace\n    image/height: integer, image height in pixels\n    image/width: integer, image width in pixels\n    image/channels: integer, specifying the number of channels, always 3\n    image/format: string, specifying the format, always\'JPEG\'\n\n\n    image/object/bbox/xmin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/xmax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymin: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/ymax: list of float specifying the 0+ human annotated\n        bounding boxes\n    image/object/bbox/label: list of integer specifying the classification index.\n    image/object/bbox/label_text: list of string descriptions.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n""""""\nimport os\nimport sys\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\nimport xml.etree.ElementTree as ET\n\nfrom datasets.dataset_utils import int64_feature, float_feature, bytes_feature\nfrom datasets.pascalvoc_common import VOC_LABELS\n\n# Original dataset organisation.\nDIRECTORY_ANNOTATIONS = \'Annotations/\'\nDIRECTORY_IMAGES = \'JPEGImages/\'\n\n# TFRecords convertion parameters.\nRANDOM_SEED = 4242\nSAMPLES_PER_FILES = 200\n\n\ndef _process_image(directory, name):\n    """"""Process a image and annotation file.\n\n    Args:\n      filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n      coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n      image_buffer: string, JPEG encoding of RGB image.\n      height: integer, image height in pixels.\n      width: integer, image width in pixels.\n    """"""\n    # Read the image file.\n    filename = os.path.join(directory, DIRECTORY_IMAGES, name + \'.jpg\')\n    image_data = tf.gfile.FastGFile(filename, \'r\').read()\n\n    # Read the XML annotation file.\n    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + \'.xml\')\n    tree = ET.parse(filename)\n    root = tree.getroot()\n\n    # Image shape.\n    size = root.find(\'size\')\n    shape = [int(size.find(\'height\').text),\n             int(size.find(\'width\').text),\n             int(size.find(\'depth\').text)]\n    # Find annotations.\n    bboxes = []\n    labels = []\n    labels_text = []\n    difficult = []\n    truncated = []\n    for obj in root.findall(\'object\'):\n        label = obj.find(\'name\').text\n        labels.append(int(VOC_LABELS[label][0]))\n        labels_text.append(label.encode(\'ascii\'))\n\n        if obj.find(\'difficult\'):\n            difficult.append(int(obj.find(\'difficult\').text))\n        else:\n            difficult.append(0)\n        if obj.find(\'truncated\'):\n            truncated.append(int(obj.find(\'truncated\').text))\n        else:\n            truncated.append(0)\n\n        bbox = obj.find(\'bndbox\')\n        bboxes.append((float(bbox.find(\'ymin\').text) / shape[0],\n                       float(bbox.find(\'xmin\').text) / shape[1],\n                       float(bbox.find(\'ymax\').text) / shape[0],\n                       float(bbox.find(\'xmax\').text) / shape[1]\n                       ))\n    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n\n\ndef _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n                        difficult, truncated):\n    """"""Build an Example proto for an image example.\n\n    Args:\n      image_data: string, JPEG encoding of RGB image;\n      labels: list of integers, identifier for the ground truth;\n      labels_text: list of strings, human-readable labels;\n      bboxes: list of bounding boxes; each box is a list of integers;\n          specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong\n          to the same label as the image label.\n      shape: 3 integers, image shapes in pixels.\n    Returns:\n      Example proto\n    """"""\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    for b in bboxes:\n        assert len(b) == 4\n        # pylint: disable=expression-not-assigned\n        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n        # pylint: enable=expression-not-assigned\n\n    image_format = b\'JPEG\'\n    example = tf.train.Example(features=tf.train.Features(feature={\n            \'image/height\': int64_feature(shape[0]),\n            \'image/width\': int64_feature(shape[1]),\n            \'image/channels\': int64_feature(shape[2]),\n            \'image/shape\': int64_feature(shape),\n            \'image/object/bbox/xmin\': float_feature(xmin),\n            \'image/object/bbox/xmax\': float_feature(xmax),\n            \'image/object/bbox/ymin\': float_feature(ymin),\n            \'image/object/bbox/ymax\': float_feature(ymax),\n            \'image/object/bbox/label\': int64_feature(labels),\n            \'image/object/bbox/label_text\': bytes_feature(labels_text),\n            \'image/object/bbox/difficult\': int64_feature(difficult),\n            \'image/object/bbox/truncated\': int64_feature(truncated),\n            \'image/format\': bytes_feature(image_format),\n            \'image/encoded\': bytes_feature(image_data)}))\n    return example\n\n\ndef _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n    """"""Loads data from image and annotations files and add them to a TFRecord.\n\n    Args:\n      dataset_dir: Dataset directory;\n      name: Image name to add to the TFRecord;\n      tfrecord_writer: The TFRecord writer to use for writing.\n    """"""\n    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n        _process_image(dataset_dir, name)\n    example = _convert_to_example(image_data, labels, labels_text,\n                                  bboxes, shape, difficult, truncated)\n    tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(output_dir, name, idx):\n    return \'%s/%s_%03d.tfrecord\' % (output_dir, name, idx)\n\n\ndef run(voc_root, year, split, output_dir, shuffling=False):\n    """"""Runs the conversion operation.\n\n    Args:\n      year: VOC year, 2007/2012/0712\n      voc_root: VOC root dir.\n      output_dir: Output directory.\n    """"""\n    if not tf.gfile.Exists(output_dir):\n        tf.gfile.MakeDirs(output_dir)\n\n    # Dataset filenames, and shuffling.\n    split_file_path = os.path.join(voc_root,\n                                 \'VOC%s\'%year,\n                                 \'ImageSets\',\n                                 \'Main\',\n                                 \'%s.txt\'%split)\n    with open(split_file_path) as f:\n        filenames = f.readlines()\n\n    if shuffling:\n        random.seed(RANDOM_SEED)\n        random.shuffle(filenames)\n\n    # Process dataset files.\n    i = 0\n    fidx = 0\n    dataset_dir = os.path.join(voc_root, \'VOC%s\'%year)\n    while i < len(filenames):\n        # Open new TFRecord file.\n        tf_filename = _get_output_filename(output_dir, split, fidx)\n        with tf.python_io.TFRecordWriter(tf_filename) as tfrecord_writer:\n            j = 0\n            while i < len(filenames) and j < SAMPLES_PER_FILES:\n                sys.stdout.write(\'\\r>> Converting image %d/%d\' % (i+1, len(filenames)))\n                sys.stdout.flush()\n\n                filename = filenames[i].strip()\n                _add_to_tfrecord(dataset_dir, filename, tfrecord_writer)\n                i += 1\n                j += 1\n            fidx += 1\n\n    print(\'\\nFinished converting the Pascal VOC dataset!\')\n'"
deployment/__init__.py,0,b'\n'
deployment/model_deploy.py,52,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.summary.scalar(clone.scope + \'/clone_loss\', clone_loss)\n  if regularization_loss is not None:\n    tf.summary.scalar(\'regularization_loss\', regularization_loss)\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones, regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients across clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        global_step = slim.get_or_create_global_step()\n\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        train_op = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          regularization_losses = None\n      if clones_losses:\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.summary.scalar(\'total_loss\', total_loss))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.summary.merge(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.summary.histogram(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      if self._num_clones > 1:\n        device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op == \'Variable\':\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
deployment/model_deploy_test.py,96,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for model_deploy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\nclass DeploymentConfigTest(tf.test.TestCase):\n\n  def testDefaults(self):\n    deploy_config = model_deploy.DeploymentConfig()\n\n    self.assertEqual(slim.get_variables(), [])\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testCPUonly(self):\n    deploy_config = model_deploy.DeploymentConfig(clone_on_cpu=True)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'CPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testMultiGPU(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1), \'GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=1, num_ps_tasks=1)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n  def testMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2, num_ps_tasks=1)\n\n    self.assertEqual(deploy_config.caching_device()(tf.no_op()), \'\')\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_ps_tasks=2)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_clones=2,\n                                                  num_ps_tasks=2)\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testVariablesPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_ps_tasks=2)\n\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:1/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n\ndef LogisticClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'LogisticClassifier\', [inputs, labels],\n                         reuse=reuse):\n    predictions = slim.fully_connected(inputs, 1, activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\ndef BatchNormClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'BatchNormClassifier\', [inputs, labels],\n                         reuse=reuse):\n    inputs = slim.batch_norm(inputs, decay=0.1)\n    predictions = slim.fully_connected(inputs, 1,\n                                       activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\nclass CreatecloneTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 2)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'LogisticClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(len(clones), num_clones)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n        self.assertEqual(len(update_ops), 2)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'GPU:%d\' % i)\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(clones), 1)\n      clone = clones[0]\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertDeviceEqual(clone.device, \'/job:worker\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n        self.assertDeviceEqual(v.device, v.value().device)\n\n  def testCreateMulticloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    num_ps_tasks=2)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for i, v in enumerate(slim.get_variables()):\n        t = i % 2\n        self.assertDeviceEqual(v.device, \'/job:ps/task:%d/device:CPU:0\' % t)\n        self.assertDeviceEqual(v.device, v.value().device)\n      self.assertEqual(len(clones), 2)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:%d\' % i)\n\n\nclass OptimizeclonesTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 2)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticloneCPU(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones,\n                                                    clone_on_cpu=True)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'/job:worker\')\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n\n\nclass DeployTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testLocalTrainOp(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    clone_on_cpu=True)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n\n      self.assertEqual(slim.get_variables(), [])\n      model = model_deploy.deploy(deploy_config, model_fn, model_args,\n                                  optimizer=optimizer)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 4)\n      self.assertEqual(len(model.clones), 2)\n      self.assertEqual(model.total_loss.op.name, \'total_loss\')\n      self.assertEqual(model.summary_op.op.name, \'summary_op/summary_op\')\n      self.assertEqual(model.train_op.op.name, \'train_op\')\n\n      with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        moving_mean = tf.contrib.framework.get_variables_by_name(\n            \'moving_mean\')[0]\n        moving_variance = tf.contrib.framework.get_variables_by_name(\n            \'moving_variance\')[0]\n        initial_loss = sess.run(model.total_loss)\n        initial_mean, initial_variance = sess.run([moving_mean,\n                                                   moving_variance])\n        self.assertAllClose(initial_mean, [0.0, 0.0, 0.0, 0.0])\n        self.assertAllClose(initial_variance, [1.0, 1.0, 1.0, 1.0])\n        for _ in range(10):\n          sess.run(model.train_op)\n        final_loss = sess.run(model.total_loss)\n        self.assertLess(final_loss, initial_loss / 10.0)\n\n        final_mean, final_variance = sess.run([moving_mean,\n                                               moving_variance])\n        self.assertAllClose(final_mean, [0.125, 0.25, 0.375, 0.25])\n        self.assertAllClose(final_variance, [0.109375, 0.1875,\n                                             0.234375, 0.1875])\n\n  def testNoSummariesOnGPU(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      model = model_deploy.deploy(\n          deploy_config, ModelFn,\n          optimizer=tf.train.GradientDescentOptimizer(1.0))\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n  def testNoSummariesOnGPUForEvals(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      # No optimizer here, it\'s an eval.\n      model = model_deploy.deploy(deploy_config, ModelFn)\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/__init__.py,0,b'\n'
nets/alexnet.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
nets/alexnet_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/cifarnet.py,12,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer(),\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
nets/inception_resnet_v2.py,39,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1088\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # Auxiliary tower\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
nets/inception_resnet_v2_test.py,20,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
nets/inception_v1.py,60,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'AvgPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
nets/inception_v1_test.py,25,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/inception_v2.py,68,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
nets/inception_v2_test.py,28,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/inception_v3.py,79,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
nets/inception_v3_test.py,29,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.global_variables_initializer().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.global_variables_initializer().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/inception_v4.py,48,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in xrange(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in xrange(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in xrange(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
nets/inception_v4_test.py,24,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/lenet.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
nets/mobilenet.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\ndef mobilenet(inputs,\n          num_classes=1000,\n          is_training=True,\n          width_multiplier=1,\n          scope=\'MobileNet\'):\n  """""" MobileNet\n  More detail, please refer to Google\'s paper(https://arxiv.org/abs/1704.04861).\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    scope: Optional scope for the variables.\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n\n  def _depthwise_separable_conv(inputs,\n                                num_pwc_filters,\n                                width_multiplier,\n                                sc,\n                                downsample=False):\n    """""" Helper function to build the depth-wise separable convolution layer.\n    """"""\n    num_pwc_filters = round(num_pwc_filters * width_multiplier)\n    _stride = 2 if downsample else 1\n\n    # skip pointwise by setting num_outputs=None\n    depthwise_conv = slim.separable_convolution2d(inputs,\n                                                  num_outputs=None,\n                                                  stride=_stride,\n                                                  depth_multiplier=1,\n                                                  kernel_size=[3, 3],\n                                                  scope=sc+\'/depthwise_conv\')\n\n    bn = slim.batch_norm(depthwise_conv, scope=sc+\'/dw_batch_norm\')\n    pointwise_conv = slim.convolution2d(bn,\n                                        num_pwc_filters,\n                                        kernel_size=[1, 1],\n                                        scope=sc+\'/pointwise_conv\')\n    bn = slim.batch_norm(pointwise_conv, scope=sc+\'/pw_batch_norm\')\n    return bn\n\n  with tf.variable_scope(scope) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.convolution2d, slim.separable_convolution2d],\n                        activation_fn=None,\n                        outputs_collections=[end_points_collection]):\n      with slim.arg_scope([slim.batch_norm],\n                          is_training=is_training,\n                          activation_fn=tf.nn.relu,\n                          fused=True):\n        net = slim.convolution2d(inputs, round(32 * width_multiplier), [3, 3], stride=2, padding=\'SAME\', scope=\'conv_1\')\n        net = slim.batch_norm(net, scope=\'conv_1/batch_norm\')\n        net = _depthwise_separable_conv(net, 64, width_multiplier, sc=\'conv_ds_2\')\n        net = _depthwise_separable_conv(net, 128, width_multiplier, downsample=True, sc=\'conv_ds_3\')\n        net = _depthwise_separable_conv(net, 128, width_multiplier, sc=\'conv_ds_4\')\n        net = _depthwise_separable_conv(net, 256, width_multiplier, downsample=True, sc=\'conv_ds_5\')\n        net = _depthwise_separable_conv(net, 256, width_multiplier, sc=\'conv_ds_6\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, downsample=True, sc=\'conv_ds_7\')\n\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_8\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_9\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_10\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_11\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_12\')\n\n        net = _depthwise_separable_conv(net, 1024, width_multiplier, downsample=True, sc=\'conv_ds_13\')\n        net = _depthwise_separable_conv(net, 1024, width_multiplier, sc=\'conv_ds_14\')\n        net = slim.avg_pool2d(net, [7, 7], scope=\'avg_pool_15\')\n\n    end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n    net = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n    end_points[\'squeeze\'] = net\n    logits = slim.fully_connected(net, num_classes, activation_fn=None, scope=\'fc_16\')\n    predictions = slim.softmax(logits, scope=\'Predictions\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = predictions\n\n  return logits, end_points\n\nmobilenet.default_image_size = 224\n\n\ndef mobilenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default mobilenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the MobileNet model.\n  """"""\n  with slim.arg_scope(\n      [slim.convolution2d, slim.separable_convolution2d],\n      weights_initializer=slim.initializers.xavier_initializer(),\n      biases_initializer=slim.init_ops.zeros_initializer(),\n      weights_regularizer=slim.l2_regularizer(weight_decay)) as sc:\n    return sc\n'"
nets/mobilenet_test.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for MobileNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import mobilenet\n\nslim = tf.contrib.slim\n\n\nclass MobileNetTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = mobilenet.mobilenet(inputs, num_classes)\n      self.assertEquals(end_points[\'MobileNet/conv_ds_2/depthwise_conv\'].get_shape().as_list(), [5, 112, 112, 32])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_3/depthwise_conv\'].get_shape().as_list(), [5, 56, 56, 64])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_4/depthwise_conv\'].get_shape().as_list(), [5, 56, 56, 128])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_5/depthwise_conv\'].get_shape().as_list(), [5, 28, 28, 128])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_6/depthwise_conv\'].get_shape().as_list(), [5, 28, 28, 256])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_7/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 256])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_8/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_9/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_10/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_11/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_12/depthwise_conv\'].get_shape().as_list(), [5, 14, 14, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_13/depthwise_conv\'].get_shape().as_list(), [5, 7, 7, 512])\n      self.assertEquals(end_points[\'MobileNet/conv_ds_14/depthwise_conv\'].get_shape().as_list(), [5, 7, 7, 1024])\n      self.assertEquals(end_points[\'squeeze\'].get_shape().as_list(), [5, 1024])\n      self.assertEquals(logits.op.name, \'MobileNet/fc_16/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = mobilenet.mobilenet(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = mobilenet.mobilenet(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/mobilenetdet.py,163,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nfrom configs.kitti_config import config\n\n\ndef xywh_to_yxyx(bbox):\n  shape = bbox.get_shape().as_list()\n  _axis = 1 if len(shape) > 1 else 0\n  [x, y, w, h] = tf.unstack(bbox, axis=_axis)\n  y_min = y - 0.5 * h\n  x_min = x - 0.5 * w\n  y_max = y + 0.5 * h\n  x_max = x + 0.5 * w\n  return tf.stack([y_min, x_min, y_max, x_max], axis=_axis)\n\n\ndef yxyx_to_xywh_(bbox):\n  y_min = bbox[:, 0]\n  x_min = bbox[:, 1]\n  y_max = bbox[:, 2]\n  x_max = bbox[:, 3]\n  x = (x_min + x_max) * 0.5\n  y = (y_min + y_max) * 0.5\n  w = x_max - x_min\n  h = y_max - y_min\n  return tf.stack([x, y, w, h], axis=1)\n\n\ndef scale_bboxes(bbox, img_shape):\n  """"""Scale bboxes to [0, 1). bbox format [ymin, xmin, ymax, xmax]\n  Args:\n    bbox: 2-D with shape \'[num_bbox, 4]\'\n    img_shape: 1-D with shape \'[4]\'\n  Return:\n    sclaed_bboxes: scaled bboxes\n  """"""\n  img_h = tf.cast(img_shape[0], dtype=tf.float32)\n  img_w = tf.cast(img_shape[1], dtype=tf.float32)\n  shape = bbox.get_shape().as_list()\n  _axis = 1 if len(shape) > 1 else 0\n  [y_min, x_min, y_max, x_max] = tf.unstack(bbox, axis=_axis)\n  y_1 = tf.truediv(y_min, img_h)\n  x_1 = tf.truediv(x_min, img_w)\n  y_2 = tf.truediv(y_max, img_h)\n  x_2 = tf.truediv(x_max, img_w)\n  return tf.stack([y_1, x_1, y_2, x_2], axis=_axis)\n\n\ndef iou(bbox_1, bbox_2):\n  """"""Compute iou of a box with another box. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    bbox_1: 1-D with shape `[4]`.\n    bbox_2: 1-D with shape `[4]`.\n\n  Returns:\n    IOU\n  """"""\n  lr = tf.minimum(bbox_1[3], bbox_2[3]) - tf.maximum(bbox_1[1], bbox_2[1])\n  tb = tf.minimum(bbox_1[2], bbox_2[2]) - tf.maximum(bbox_1[0], bbox_2[0])\n  lr = tf.maximum(lr, lr * 0)\n  tb = tf.maximum(tb, tb * 0)\n  intersection = tf.multiply(tb, lr)\n  union = tf.subtract(\n    tf.multiply((bbox_1[3] - bbox_1[1]), (bbox_1[2] - bbox_1[0])) +\n    tf.multiply((bbox_2[3] - bbox_2[1]), (bbox_2[2] - bbox_2[0])),\n    intersection\n  )\n  iou = tf.div(intersection, union)\n  return iou\n\n\ndef batch_iou(bboxes, bbox):\n  """"""Compute iou of a batch of boxes with another box. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    bboxes: A batch of boxes. 2-D with shape `[B, 4]`.\n    bbox: A single box. 1-D with shape `[4]`.\n\n  Returns:\n    Batch of IOUs\n  """"""\n  lr = tf.maximum(\n    tf.minimum(bboxes[:, 3], bbox[3]) -\n    tf.maximum(bboxes[:, 1], bbox[1]),\n    0\n  )\n  tb = tf.maximum(\n    tf.minimum(bboxes[:, 2], bbox[2]) -\n    tf.maximum(bboxes[:, 0], bbox[0]),\n    0\n  )\n  intersection = tf.multiply(tb, lr)\n  union = tf.subtract(\n    tf.multiply((bboxes[:, 3] - bboxes[:, 1]), (bboxes[:, 2] - bboxes[:, 0])) +\n    tf.multiply((bbox[3] - bbox[1]), (bbox[2] - bbox[0])),\n    intersection\n  )\n  iou = tf.div(intersection, union)\n  return iou\n\n\ndef batch_iou_(anchors, bboxes):\n  """""" Compute iou of two batch of boxes. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    anchors: know shape\n    bboxes: dynamic shape\n  Return:\n    ious: 2-D with shape \'[num_bboxes, num_anchors]\'\n    indices: [num_bboxes, 1]\n  """"""\n  num_anchors = anchors.get_shape().as_list()[0]\n  ious_list = []\n  for i in range(num_anchors):\n    anchor = anchors[i]\n    _ious = batch_iou(bboxes, anchor)\n    ious_list.append(_ious)\n  ious = tf.stack(ious_list, axis=0)\n  ious = tf.transpose(ious)\n\n  indices = tf.arg_max(ious, dimension=1)\n\n  return ious, indices\n\n\ndef batch_iou_fast(anchors, bboxes):\n  """""" Compute iou of two batch of boxes. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    anchors: know shape\n    bboxes: dynamic shape\n  Return:\n    ious: 2-D with shape \'[num_bboxes, num_anchors]\'\n    indices: [num_bboxes, 1]\n  """"""\n  num_anchors = anchors.get_shape().as_list()[0]\n  tensor_num_bboxes = tf.shape(bboxes)[0]\n  indices = tf.reshape(tf.range(tensor_num_bboxes), shape=[-1, 1])\n  indices = tf.reshape(tf.stack([indices]*num_anchors, axis=1), shape=[-1, 1])\n  bboxes_m = tf.gather_nd(bboxes, indices)\n\n  anchors_m = tf.tile(anchors, [tensor_num_bboxes, 1])\n\n  lr = tf.maximum(\n    tf.minimum(bboxes_m[:, 3], anchors_m[:, 3]) -\n    tf.maximum(bboxes_m[:, 1], anchors_m[:, 1]),\n    0\n  )\n  tb = tf.maximum(\n    tf.minimum(bboxes_m[:, 2], anchors_m[:, 2]) -\n    tf.maximum(bboxes_m[:, 0], anchors_m[:, 0]),\n    0\n  )\n  intersection = tf.multiply(tb, lr)\n  union = tf.subtract(\n    tf.multiply((bboxes_m[:, 3] - bboxes_m[:, 1]), (bboxes_m[:, 2] - bboxes_m[:, 0])) +\n    tf.multiply((anchors_m[:, 3] - anchors_m[:, 1]), (anchors_m[:, 2] - anchors_m[:, 0])),\n    intersection\n  )\n  ious = tf.div(intersection, union)\n\n  ious = tf.reshape(ious, shape=[tensor_num_bboxes, num_anchors])\n\n  indices = tf.arg_max(ious, dimension=1)\n\n  return ious, indices\n\n\ndef compute_delta(gt_box, anchor):\n  """"""Compute delta, anchor+delta = gt_box. Box format \'[cx, cy, w, h]\'.\n  Args:\n    gt_box: A batch of boxes. 2-D with shape `[B, 4]`.\n    anchor: A single box. 1-D with shape `[4]`.\n\n  Returns:\n    delta: 1-D tensor with shape \'[4]\', [dx, dy, dw, dh]\n  """"""\n  delta_x = (gt_box[0] - anchor[0]) / gt_box[2]\n  delta_y = (gt_box[1] - anchor[1]) / gt_box[3]\n  delta_w = tf.log(gt_box[2] / anchor[2])\n  delta_h = tf.log(gt_box[3] / anchor[3])\n  return tf.stack([delta_x, delta_y, delta_w, delta_h], axis=0)\n\n\ndef batch_delta(bboxes, anchors):\n  """"""\n  Args:\n     bboxes: [num_bboxes, 4]\n     anchors: [num_bboxes, 4]\n  Return:\n    deltas: [num_bboxes, 4]\n  """"""\n  bbox_x, bbox_y, bbox_w, bbox_h = tf.unstack(bboxes, axis=1)\n  anchor_x, anchor_y, anchor_w, anchor_h = tf.unstack(anchors, axis=1)\n  delta_x = (bbox_x - anchor_x) / bbox_w\n  delta_y = (bbox_y - anchor_y) / bbox_h\n  delta_w = tf.log(bbox_w / anchor_w)\n  delta_h = tf.log(bbox_h / anchor_h)\n  return tf.stack([delta_x, delta_y, delta_w, delta_h], axis=1)\n\n\n# TODO(shizehao): improve speed, use sparse tensor instead\ndef encode_annos(image, labels, bboxes, anchors, num_classes):\n  """"""Encode annotations for losses computations.\n  All the output tensors have a fix shape(none dynamic dimention).\n\n  Args:\n    image: 4-D with shape `[H, W, C]`.\n    b_labels: 2-D with shape `[num_bounding_boxes]`.\n    b_bboxes: 3-D with shape `[num_bounding_boxes, 4]`. Scaled.\n    anchors: 4-D tensor with shape `[fea_h, fea_w, num_anchors, 4]`\n\n  Returns:\n    input_mask: 2-D with shape `[num_anchors, 1]`, indicate which anchor to be used to cal loss.\n    labels_input: 2-D with shape `[num_anchors, num_classes]`, one hot encode for every anchor.\n    box_delta_input: 2-D with shape `[num_anchors, 4]`.\n    box_input: 2-D with shape \'[num_anchors, 4]\'.\n  """"""\n  anchors_shape = anchors.get_shape().as_list()\n  fea_h = anchors_shape[0]\n  fea_w = anchors_shape[1]\n  num_anchors = anchors_shape[2] * fea_h * fea_w\n  anchors = tf.reshape(anchors, [num_anchors, 4])  # reshape anchors\n\n  # Cal iou, find the target anchor\n  _anchors = xywh_to_yxyx(anchors)\n  ious, indices = batch_iou_fast(_anchors, bboxes)\n  indices = tf.reshape(indices, shape=[-1, 1])\n\n  target_anchors = tf.gather(anchors, indices)\n  target_anchors = tf.squeeze(target_anchors, axis=1)\n  delta = batch_delta(yxyx_to_xywh_(bboxes), target_anchors)\n\n  # bbox\n  box_input = tf.scatter_nd(\n    indices,\n    bboxes,\n    shape=[num_anchors, 4]\n  )\n  # label\n  labels_input = tf.scatter_nd(\n    indices,\n    tf.one_hot(labels, num_classes),\n    shape=[num_anchors, num_classes]\n  )\n  # anchor mask\n  onehot_anchor = tf.one_hot(indices, num_anchors)\n  onehot_anchor = tf.squeeze(onehot_anchor, axis=1)\n  print(""indices shape:"", indices.get_shape().as_list())\n  print(""one hot anchors shape:"", onehot_anchor.get_shape().as_list())\n  input_mask = tf.reduce_sum(onehot_anchor, axis=0)\n  input_mask = tf.reshape(input_mask, shape=[-1, 1])\n  # delta\n  box_delta_input = tf.scatter_nd(\n    indices,\n    delta,\n    shape=[num_anchors, 4]\n  )\n\n  return input_mask, labels_input, box_delta_input, box_input\n\n\n# TODO(shizehao): align anchor center to the grid\ndef set_anchors(img_shape, fea_shape):\n  """"""Set anchors.\n\n  Args:\n    img_shape: 1-D list with shape `[2]`.\n    fea_shape: 1-D list with shape `[2]`.\n\n  Returns:\n    anchors: 4-D tensor with shape `[fea_h, fea_w, num_anchors, 4]`\n\n  """"""\n  H = fea_shape[0]\n  W = fea_shape[1]\n  B = config.NUM_ANCHORS\n\n  anchor_shape = tf.constant(config.ANCHOR_SHAPE, dtype=tf.float32)\n  anchor_shapes = tf.reshape(\n    tf.concat(\n      [anchor_shape for i in range(W * H)],\n      0\n    ),\n    [H, W, B, 2]\n  )\n\n  center_x = tf.truediv(\n    tf.range(1, W + 1, 1, dtype=tf.float32),  # * img_w,\n    float(W + 1)\n  )\n  center_x = tf.concat(\n    [center_x for i in range(H * B)], 0\n  )\n  center_x = tf.reshape(center_x, [B, H, W])\n  center_x = tf.transpose(center_x, (1, 2, 0))\n  center_x = tf.reshape(center_x, [H, W, B, 1])\n\n  center_y = tf.truediv(\n    tf.range(1, H + 1, 1, dtype=tf.float32),  # * img_h,\n    float(H + 1)\n  )\n  center_y = tf.concat(\n    [center_y for i in range(W * B)], 0\n  )\n  center_y = tf.reshape(center_y, [B, W, H])\n  center_y = tf.transpose(center_y, (2, 1, 0))\n  center_y = tf.reshape(center_y, [H, W, B, 1])\n\n  anchors = tf.concat([center_x, center_y, anchor_shapes], 3)\n\n  return anchors\n\n\ndef interpre_prediction(prediction, input_mask, anchors, box_input, fea_h, fea_w):\n  # probability\n  batch_size = tf.shape(input_mask)[0]\n  num_class_probs = config.NUM_ANCHORS * config.NUM_CLASSES\n  pred_class_probs = tf.reshape(\n    tf.nn.softmax(\n      tf.reshape(\n        prediction[:, :, :, :num_class_probs],\n        [-1, config.NUM_CLASSES]\n      )\n    ),\n    [batch_size, config.NUM_ANCHORS * fea_h * fea_w, config.NUM_CLASSES],\n    name=\'pred_class_probs\'\n  )\n\n  # confidence\n  num_confidence_scores = config.NUM_ANCHORS + num_class_probs\n  pred_conf = tf.sigmoid(\n    tf.reshape(\n      prediction[:, :, :, num_class_probs:num_confidence_scores],\n      [batch_size, config.NUM_ANCHORS * fea_h * fea_w]\n    ),\n    name=\'pred_confidence_score\'\n  )\n\n  # bbox_delta\n  pred_box_delta = tf.reshape(\n    prediction[:, :, :, num_confidence_scores:],\n    [batch_size, config.NUM_ANCHORS * fea_h * fea_w, 4],\n    name=\'bbox_delta\'\n  )\n\n  # number of object. Used to normalize bbox and classification loss\n  num_objects = tf.reduce_sum(input_mask, name=\'num_objects\')\n\n  with tf.variable_scope(\'bbox\') as scope:\n    with tf.variable_scope(\'stretching\'):\n      delta_x, delta_y, delta_w, delta_h = tf.unstack(\n        pred_box_delta, axis=2)\n\n      anchor_x = anchors[:, 0]\n      anchor_y = anchors[:, 1]\n      anchor_w = anchors[:, 2]\n      anchor_h = anchors[:, 3]\n\n      box_center_x = tf.identity(\n        anchor_x + delta_x * anchor_w, name=\'bbox_cx\')\n      box_center_y = tf.identity(\n        anchor_y + delta_y * anchor_h, name=\'bbox_cy\')\n      box_width = tf.identity(\n        anchor_w * safe_exp(delta_w, config.EXP_THRESH),\n        name=\'bbox_width\')\n      box_height = tf.identity(\n        anchor_h * safe_exp(delta_h, config.EXP_THRESH),\n        name=\'bbox_height\')\n\n    with tf.variable_scope(\'trimming\'):\n      xmins, ymins, xmaxs, ymaxs = bbox_transform(\n        [box_center_x, box_center_y, box_width, box_height])\n\n      # The max x position is mc.IMAGE_WIDTH - 1 since we use zero-based\n      # pixels. Same for y.\n      xmins = tf.minimum(\n        tf.maximum(0.0, xmins), config.IMG_WIDTH - 1.0, name=\'bbox_xmin\')\n\n      ymins = tf.minimum(\n        tf.maximum(0.0, ymins), config.IMG_HEIGHT - 1.0, name=\'bbox_ymin\')\n\n      xmaxs = tf.maximum(\n        tf.minimum(config.IMG_WIDTH - 1.0, xmaxs), 0.0, name=\'bbox_xmax\')\n\n      ymaxs = tf.maximum(\n        tf.minimum(config.IMG_HEIGHT - 1.0, ymaxs), 0.0, name=\'bbox_ymax\')\n\n      det_boxes = tf.transpose(\n        tf.stack(bbox_transform_inv([xmins, ymins, xmaxs, ymaxs])),\n        (1, 2, 0), name=\'bbox\'\n      )\n\n      with tf.variable_scope(\'IOU\'):\n        def _tensor_iou(box1, box2):\n          with tf.variable_scope(\'intersection\'):\n            xmin = tf.maximum(box1[0], box2[0], name=\'xmin\')\n            ymin = tf.maximum(box1[1], box2[1], name=\'ymin\')\n            xmax = tf.minimum(box1[2], box2[2], name=\'xmax\')\n            ymax = tf.minimum(box1[3], box2[3], name=\'ymax\')\n\n            w = tf.maximum(0.0, xmax - xmin, name=\'inter_w\')\n            h = tf.maximum(0.0, ymax - ymin, name=\'inter_h\')\n            intersection = tf.multiply(w, h, name=\'intersection\')\n\n          with tf.variable_scope(\'union\'):\n            w1 = tf.subtract(box1[2], box1[0], name=\'w1\')\n            h1 = tf.subtract(box1[3], box1[1], name=\'h1\')\n            w2 = tf.subtract(box2[2], box2[0], name=\'w2\')\n            h2 = tf.subtract(box2[3], box2[1], name=\'h2\')\n\n            union = w1 * h1 + w2 * h2 - intersection\n\n          return intersection / (union + config.EPSILON) \\\n                 * tf.reshape(input_mask, [batch_size, config.NUM_ANCHORS * fea_h * fea_w])\n\n        # TODO(shizehao): need test\n        ious = _tensor_iou(\n          bbox_transform(tf.unstack(det_boxes, axis=2)),\n          bbox_transform(tf.unstack(box_input, axis=2))\n        )\n\n      with tf.variable_scope(\'probability\') as scope:\n        probs = tf.multiply(\n          pred_class_probs,\n          tf.reshape(pred_conf, [batch_size, config.NUM_ANCHORS * fea_h * fea_w, 1]),\n          name=\'final_class_prob\'\n        )\n\n        det_probs = tf.reduce_max(probs, 2, name=\'score\')\n        det_class = tf.argmax(probs, 2, name=\'class_idx\')\n\n  return pred_box_delta, pred_class_probs, pred_conf, ious, det_probs, det_boxes, det_class\n\n\ndef losses(input_mask, labels, ious, box_delta_input, pred_class_probs, pred_conf, pred_box_delta):\n  batch_size = tf.shape(input_mask)[0]\n  num_objects = tf.reduce_sum(input_mask, name=\'num_objects\')\n  with tf.variable_scope(\'class_regression\') as scope:\n    # cross-entropy: q * -log(p) + (1-q) * -log(1-p)\n    # add a small value into log to prevent blowing up\n    class_loss = tf.truediv(\n      tf.reduce_sum(\n        (labels * (-tf.log(pred_class_probs + config.EPSILON))\n         + (1 - labels) * (-tf.log(1 - pred_class_probs + config.EPSILON)))\n        * input_mask * config.LOSS_COEF_CLASS),\n      num_objects,\n      name=\'class_loss\'\n    )\n    tf.losses.add_loss(class_loss)\n\n  with tf.variable_scope(\'confidence_score_regression\') as scope:\n    input_mask_ = tf.reshape(input_mask, [batch_size, config.ANCHORS])\n    conf_loss = tf.reduce_mean(\n      tf.reduce_sum(\n        tf.square((ious - pred_conf))\n        * (input_mask_ * config.LOSS_COEF_CONF_POS / num_objects\n           + (1 - input_mask_) * config.LOSS_COEF_CONF_NEG / (config.ANCHORS - num_objects)),\n        reduction_indices=[1]\n      ),\n      name=\'confidence_loss\'\n    )\n    tf.losses.add_loss(conf_loss)\n\n  with tf.variable_scope(\'bounding_box_regression\') as scope:\n    bbox_loss = tf.truediv(\n      tf.reduce_sum(\n        config.LOSS_COEF_BBOX * tf.square(\n          input_mask * (pred_box_delta - box_delta_input))),\n      num_objects,\n      name=\'bbox_loss\'\n    )\n    tf.losses.add_loss(bbox_loss)\n\n  # add above losses as well as weight decay losses to form the total loss\n  loss = tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n  return loss\n\n\ndef safe_exp(w, thresh):\n  """"""Safe exponential function for tensors.""""""\n\n  slope = np.exp(thresh)\n  with tf.variable_scope(\'safe_exponential\'):\n    lin_region = tf.to_float(w > thresh)\n\n    lin_out = slope * (w - thresh + 1.)\n    exp_out = tf.exp(w)\n\n    out = lin_region * lin_out + (1. - lin_region) * exp_out\n  return out\n\n\ndef bbox_transform_inv(bbox):\n  """"""convert a bbox of form [xmin, ymin, xmax, ymax] to [cx, cy, w, h]. Works\n  for numpy array or list of tensors.\n  """"""\n  with tf.variable_scope(\'bbox_transform_inv\') as scope:\n    xmin, ymin, xmax, ymax = bbox\n    out_box = [[]] * 4\n\n    width = xmax - xmin + 1.0\n    height = ymax - ymin + 1.0\n    out_box[0] = xmin + 0.5 * width\n    out_box[1] = ymin + 0.5 * height\n    out_box[2] = width\n    out_box[3] = height\n\n  return out_box\n\n\ndef bbox_transform(bbox):\n  """"""convert a bbox of form [cx, cy, w, h] to [xmin, ymin, xmax, ymax]. Works\n  for numpy array or list of tensors.\n  """"""\n  with tf.variable_scope(\'bbox_transform\') as scope:\n    cx, cy, w, h = bbox\n    out_box = [[]] * 4\n    out_box[0] = cx - w / 2\n    out_box[1] = cy - h / 2\n    out_box[2] = cx + w / 2\n    out_box[3] = cy + h / 2\n\n  return out_box\n\n\ndef mobilenet(inputs,\n          is_training=True,\n          width_multiplier=1,\n          scope=\'MobileNet\'):\n  def _depthwise_separable_conv(inputs,\n                                num_pwc_filters,\n                                width_multiplier,\n                                sc,\n                                downsample=False):\n    """""" Helper function to build the depth-wise separable convolution layer.\n    """"""\n    num_pwc_filters = round(num_pwc_filters * width_multiplier)\n    _stride = 2 if downsample else 1\n\n    # skip pointwise by setting num_outputs=None\n    depthwise_conv = slim.separable_convolution2d(inputs,\n                                                  num_outputs=None,\n                                                  stride=_stride,\n                                                  depth_multiplier=1,\n                                                  kernel_size=[3, 3],\n                                                  scope=sc+\'/depthwise_conv\')\n\n    bn = slim.batch_norm(depthwise_conv, scope=sc+\'/dw_batch_norm\')\n    pointwise_conv = slim.convolution2d(bn,\n                                        num_pwc_filters,\n                                        kernel_size=[1, 1],\n                                        scope=sc+\'/pointwise_conv\')\n    bn = slim.batch_norm(pointwise_conv, scope=sc+\'/pw_batch_norm\')\n    return bn\n\n  with tf.variable_scope(scope) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.convolution2d, slim.separable_convolution2d],\n                        activation_fn=None,\n                        outputs_collections=[end_points_collection]):\n      with slim.arg_scope([slim.batch_norm],\n                          is_training=is_training,\n                          activation_fn=tf.nn.relu):\n        net = slim.convolution2d(inputs, round(32 * width_multiplier), [3, 3], stride=2, padding=\'SAME\', scope=\'conv_1\')\n        net = slim.batch_norm(net, scope=\'conv_1/batch_norm\')\n        net = _depthwise_separable_conv(net, 64, width_multiplier, sc=\'conv_ds_2\')\n        net = _depthwise_separable_conv(net, 128, width_multiplier, downsample=True, sc=\'conv_ds_3\')\n        net = _depthwise_separable_conv(net, 128, width_multiplier, sc=\'conv_ds_4\')\n        net = _depthwise_separable_conv(net, 256, width_multiplier, downsample=True, sc=\'conv_ds_5\')\n        net = _depthwise_separable_conv(net, 256, width_multiplier, sc=\'conv_ds_6\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, downsample=True, sc=\'conv_ds_7\')\n\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_8\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_9\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_10\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_11\')\n        net = _depthwise_separable_conv(net, 512, width_multiplier, sc=\'conv_ds_12\')\n\n        net = _depthwise_separable_conv(net, 1024, width_multiplier, downsample=True, sc=\'conv_ds_13\')\n        net = _depthwise_separable_conv(net, 1024, width_multiplier, sc=\'conv_ds_14\')\n\n    end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n\n  return end_points\n\n\ndef mobilenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default mobilenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the MobileNet model.\n  """"""\n  with slim.arg_scope(\n      [slim.convolution2d, slim.separable_convolution2d],\n      weights_initializer=slim.initializers.xavier_initializer(),\n      biases_initializer=slim.init_ops.zeros_initializer(),\n      weights_regularizer=slim.l2_regularizer(weight_decay)) as sc:\n    return sc'"
nets/mobilenetdet_test.py,19,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets.mobilenetdet import *\nfrom configs.kitti_config import config\nimport numpy as np\n\n\nclass MobileNetDetTest(tf.test.TestCase):\n  def test_xywh_to_yxyx(self):\n    with self.test_session() as sess:\n      bbox = tf.constant([1, 2, 3, 4], dtype=tf.float32)\n      bbox_yxyx = xywh_to_yxyx(bbox)\n      output = sess.run(bbox_yxyx)\n      self.assertAllEqual(output, [0, -0.5, 4, 2.5])\n      bbox = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4]], dtype=tf.float32)\n      bbox_yxyx = xywh_to_yxyx(bbox)\n      output = sess.run(bbox_yxyx)\n      self.assertAllEqual(output, [[0, -0.5, 4, 2.5], [0, -0.5, 4, 2.5]])\n\n  def test_scale_bbox(self):\n    with self.test_session() as sess:\n      bbox = tf.constant([[1, 2, 3, 4], [1, 2, 3, 4]], dtype=tf.float32)\n      scaled_bbox = scale_bboxes(bbox, [10., 10.])\n      output = sess.run(scaled_bbox)\n      print(output)\n\n  def test_iou(self):\n    with self.test_session() as sess:\n      bbox_1 = tf.constant([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n      bbox_2 = tf.constant([0.15, 0.15, 0.25, 0.25], dtype=tf.float32)\n      iou_ = iou(bbox_1, bbox_2)\n      output = sess.run(iou_)\n      self.assertLess(np.abs(output - 1 / 7.), 1e-4)\n\n  def test_compute_delta(self):\n    with self.test_session() as sess:\n      image_shape = [config.IMG_HEIGHT, config.IMG_WIDTH]\n      fea_shape = [3, 3]\n      anchors = set_anchors(image_shape, fea_shape)\n      gt_box = tf.convert_to_tensor([0.25, 0.25, 0.0805153, 0.26666668])\n      delta = compute_delta(gt_box, anchors[0][0][0])\n      print(sess.run(delta))\n\n  def test_batch_iou(self):\n    with self.test_session() as sess:\n      anchors = set_anchors(img_shape=[config.IMG_HEIGHT, config.IMG_WIDTH],\n                            fea_shape=[config.FEA_HEIGHT, config.FEA_WIDTH])\n      anchors_shape = anchors.get_shape().as_list()\n      fea_h = anchors_shape[0]\n      fea_w = anchors_shape[1]\n      num_anchors = anchors_shape[2] * fea_h * fea_w\n      anchors = tf.reshape(anchors, [num_anchors, 4])  # reshape anchors\n      anchors = xywh_to_yxyx(anchors)\n      bbox = tf.constant([0.75, 0.75, 0.2, 0.2], dtype=tf.float32)\n      bbox = xywh_to_yxyx(bbox)\n      iou = batch_iou(anchors, bbox)\n      anchor_idx = tf.arg_max(iou, dimension=0)\n      anchors, output, anchor_idx = sess.run([anchors, iou, anchor_idx])\n      print(anchors)\n      print(output)\n      print(anchor_idx)\n\n  def test_batch_iou_(self):\n    anchors = set_anchors(img_shape=[config.IMG_HEIGHT, config.IMG_WIDTH],\n                          fea_shape=[config.FEA_HEIGHT, config.FEA_WIDTH])\n    anchors_shape = anchors.get_shape().as_list()\n    fea_h = anchors_shape[0]\n    fea_w = anchors_shape[1]\n    num_anchors = anchors_shape[2] * fea_h * fea_w\n    anchors = tf.reshape(anchors, [num_anchors, 4])  # reshape anchors\n    anchors = xywh_to_yxyx(anchors)\n    bboxes = tf.placeholder(dtype=tf.float32, shape=[None, 4])\n    bboxes_ = xywh_to_yxyx(bboxes)\n    ious, indices = batch_iou_(anchors, bboxes_)\n    with self.test_session() as sess:\n      ious, indices, bboxes_ = sess.run([ious, indices, bboxes], feed_dict={bboxes: [[0.25, 0.25, 0.5, 0.5],\n                                                                                     [0.75, 0.75, 0.2, 0.2]]}\n                                        )\n      print(ious)\n      print(indices)\n      print(bboxes_)\n\n  def test_batch_iou_fast(self):\n    anchors = set_anchors(img_shape=[config.IMG_HEIGHT, config.IMG_WIDTH],\n                          fea_shape=[config.FEA_HEIGHT, config.FEA_WIDTH])\n    anchors_shape = anchors.get_shape().as_list()\n    fea_h = anchors_shape[0]\n    fea_w = anchors_shape[1]\n    num_anchors = anchors_shape[2] * fea_h * fea_w\n    anchors = tf.reshape(anchors, [num_anchors, 4])  # reshape anchors\n    anchors = xywh_to_yxyx(anchors)\n    bboxes = tf.placeholder(dtype=tf.float32, shape=[None, 4])\n    bboxes_ = xywh_to_yxyx(bboxes)\n    ious, indices = batch_iou_fast(anchors, bboxes_)\n    with self.test_session() as sess:\n      ious, indices, bboxes_ = sess.run([ious, indices, bboxes],\n                                        feed_dict={bboxes: [[0.07692308, 0.025, 0.13333334, 0.04025765],\n                                                            [0.75, 0.75, 0.2, 0.2]]}\n                                        )\n      print(ious)\n      print(indices)\n      print(bboxes_)\n\n  def test_encode_annos(self):\n    with self.test_session() as sess:\n      num_obj = 2\n      image_shape = [config.IMG_HEIGHT, config.IMG_WIDTH]\n      fea_shape = [config.FEA_HEIGHT, config.FEA_WIDTH]\n      num_classes = config.NUM_CLASSES\n\n      images = tf.constant(0, shape=[image_shape[0], image_shape[1], 3])\n      labels = tf.constant(1, shape=[num_obj])\n      anchors = set_anchors(image_shape, fea_shape)\n\n      # Construct test bbox\n      bbox_1 = tf.convert_to_tensor(xywh_to_yxyx(anchors[0][0][0]), dtype=tf.float32)\n      bbox_2 = tf.convert_to_tensor(xywh_to_yxyx(anchors[2][2][1]), dtype=tf.float32)\n      bboxes = tf.stack([bbox_1, bbox_2], axis=0)\n      input_mask, labels_input, box_delta_input, box_input = \\\n        encode_annos(images, labels, bboxes, anchors, num_classes)\n      out_input_mask, out_labels_input, out_box_delta_input, out_box_input = \\\n        sess.run([input_mask, labels_input, box_delta_input, box_input])\n      print(""input_mask:"", out_input_mask)\n      print(""box_input:"", out_box_input)\n      print(""label_input:"", out_labels_input)\n      print(""box_delta_input:"", out_box_delta_input)\n      print(""shape:"",\n            ""input_mask:"", np.shape(out_input_mask),\n            ""labels_input:"", np.shape(out_labels_input),\n            ""box_delta_input:"", np.shape(out_box_delta_input),\n            ""box_input:"", np.shape(out_box_input)\n            )\n\n  def test_set_anchors(self):\n    with self.test_session() as sess:\n      anchors = set_anchors(img_shape=[config.IMG_HEIGHT, config.IMG_WIDTH],\n                            fea_shape=[config.FEA_HEIGHT, config.FEA_WIDTH])\n      output = sess.run(anchors)\n      self.assertAllEqual(np.shape(output), [config.FEA_HEIGHT, config.FEA_WIDTH, config.NUM_ANCHORS, 4])\n      print(""Anchors:"", output)\n      print(""Anchors shape:"", np.shape(output))\n      print(""Num of anchors:"", config.NUM_ANCHORS)'"
nets/nets_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\nfrom nets import mobilenet\nfrom nets import mobilenetdet\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n                \'mobilenet\': mobilenet.mobilenet,\n                \'mobilenetdet\': mobilenetdet.mobilenet\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                  \'mobilenet\': mobilenet.mobilenet_arg_scope,\n                  \'mobilenetdet\': mobilenetdet.mobilenet_arg_scope\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False, width_multiplier=1):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n    with slim.arg_scope(arg_scope):\n      if name==\'mobilenet\':\n        return func(images, num_classes, is_training=is_training, width_multiplier=width_multiplier)\n      elif name==\'mobilenetdet\':\n        return func(images, is_training=is_training, width_multiplier=width_multiplier)\n      else:\n        return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
nets/nets_factory_test.py,7,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\nslim = tf.contrib.slim\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\n  def testGetNetworkFnArgScope(self):\n    batch_size = 5\n    num_classes = 10\n    net = \'cifarnet\'\n    with self.test_session(use_gpu=True):\n      net_fn = nets_factory.get_network_fn(net, num_classes)\n      image_size = getattr(net_fn, \'default_image_size\', 224)\n      with slim.arg_scope([slim.model_variable, slim.variable],\n                          device=\'/CPU:0\'):\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        net_fn(inputs)\n      weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \'CifarNet/conv1\')[0]\n      self.assertDeviceEqual(\'/CPU:0\', weights.device)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/overfeat.py,8,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer(),\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
nets/overfeat_test.py,16,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/resnet_utils.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=1,\n                                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=unit_stride,\n                                rate=1)\n            current_stride *= unit_stride\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
nets/resnet_v1.py,6,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_50.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_101.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_152.default_image_size = resnet_v1.default_image_size\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v1_200.default_image_size = resnet_v1.default_image_size\n'"
nets/resnet_v1_test.py,45,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v1.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/resnet_v2.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\nAnother difference is that \'v2\' ResNets do not include an activation function in\nthe main pathway. Also see [2; Fig. 4e].\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              spatial_squeeze=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        if spatial_squeeze:\n          logits = tf.squeeze(net, [1, 2], name=\'SpatialSqueeze\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(logits, scope=\'predictions\')\n        return logits, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_50.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_101.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_152.default_image_size = resnet_v2.default_image_size\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\nresnet_v2_200.default_image_size = resnet_v2.default_image_size\n'"
nets/resnet_v2_test.py,45,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.global_variables_initializer())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v2.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.global_variables_initializer())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
nets/vgg.py,9,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer()):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\',\n          fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\',\n           fc_conv_padding=\'VALID\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n    fc_conv_padding: the type of padding to use for the fully connected layer\n      that is implemented as a convolutional layer. Use \'SAME\' padding if you\n      are applying the network in a fully convolutional manner and want to\n      get a prediction map downsampled by a factor of 32 as an output. Otherwise,\n      the output prediction map will be (input / 32) - 6 in case of \'VALID\' padding.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
nets/vgg_test.py,44,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.global_variables_initializer())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
optimizer/__init__.py,0,b''
optimizer/yellowfin.py,59,"b'import numpy as np\nfrom math import ceil, floor\nimport tensorflow as tf\nfrom tensorflow.python.training import momentum\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.framework import ops\n\n# Values for gate_gradients.\nGATE_NONE = 0\nGATE_OP = 1\nGATE_GRAPH = 2\n\nclass YFOptimizer(object):\n  def __init__(self, lr=0.1, mu=0.0, clip_thresh=None, beta=0.999, curv_win_width=20,\n    mu_update_interval=1, zero_debias=True, delta_mu=0.0):\n    \'\'\'\n    clip thresh is the threshold value on ||lr * gradient||\n    delta_mu can be place holder/variable/python scalar. They are used for additional\n    momentum in situations such as asynchronous-parallel training. The default is 0.0\n    for basic usage of the optimizer.\n    Args:\n      lr: python scalar. The initial value of learning rate, we use 1.0 in our paper.\n      mu: python scalar. The initial value of momentum, we use 0.0 in our paper.\n      clip_thresh: python scalar. The cliping threshold for tf.clip_by_global_norm.\n        if None, no clipping will be carried out. \n      beta: python scalar. The smoothing parameter for estimations.\n      delta_mu: for extensions. Not necessary in the basic use.\n    Other features:\n      If you want to manually control the learning rates, self.lr_factor is\n      an interface to the outside, it is an multiplier for the internal learning rate\n      in YellowFin. It is helpful when you want to do additional hand tuning\n      or some decaying scheme to the tuned learning rate in YellowFin. \n      Example on using lr_factor can be found here:\n      https://github.com/JianGoForIt/YellowFin/blob/master/char-rnn-tensorflow/train_YF.py#L140\n    \'\'\'\n    self._lr = lr\n    self._mu = mu\n\n    self._lr_var = tf.Variable(lr, dtype=tf.float32, name=""YF_lr"", trainable=False)\n    self._mu_var = tf.Variable(mu, dtype=tf.float32, name=""YF_mu"", trainable=False)\n    # for step scheme or decaying scheme for the learning rates\n    self.lr_factor = tf.Variable(1.0, dtype=tf.float32, name=""YF_lr_factor"", trainable=False)\n    if clip_thresh is not None:\n      self._clip_thresh_var = tf.Variable(clip_thresh, dtype=tf.float32, name=""YF_clip_thresh"", trainable=False)\n    else:\n      self._clip_thresh_var = None\n\n    # the underlying momentum optimizer\n    self._optimizer = \\\n      tf.train.MomentumOptimizer(self._lr_var * self.lr_factor, self._mu_var + delta_mu)\n\n    # moving average for statistics\n    self._beta = beta\n    self._moving_averager = None\n    \n    # for global step counting    \n    self._global_step = tf.Variable(0, trainable=False)\n\n    # for conditional tuning\n    self._do_tune = tf.greater(self._global_step, tf.constant(0) )\n\n    self._zero_debias = zero_debias\n\n    self._tvars = None\n\n    # for curvature range\n    self._curv_win_width = curv_win_width\n    self._curv_win = None\n\n\n  def curvature_range(self):\n    # set up the curvature window\n    self._curv_win = \\\n      tf.Variable(np.zeros( [self._curv_win_width, ] ), dtype=tf.float32, name=""curv_win"", trainable=False)\n    self._curv_win = tf.scatter_update(self._curv_win, \n      self._global_step % self._curv_win_width, self._grad_norm_squared)\n    # note here the iterations start from iteration 0\n    valid_window = tf.slice(self._curv_win, tf.constant( [0, ] ), \n      tf.expand_dims(tf.minimum(tf.constant(self._curv_win_width), self._global_step + 1), dim=0) )\n    self._h_min_t = tf.reduce_min(valid_window)\n    self._h_max_t = tf.reduce_max(valid_window)\n\n    curv_range_ops = []\n    with tf.control_dependencies([self._h_min_t, self._h_max_t] ):\n      avg_op = self._moving_averager.apply([self._h_min_t, self._h_max_t] )\n      with tf.control_dependencies([avg_op] ):\n        self._h_min = tf.identity(self._moving_averager.average(self._h_min_t) )\n        self._h_max = tf.identity(self._moving_averager.average(self._h_max_t) )\n    curv_range_ops.append(avg_op)\n    return curv_range_ops\n\n\n  def grad_variance(self):\n    grad_var_ops = []\n    tensor_to_avg = []\n    for t, g in zip(self._tvars, self._grads):\n      if isinstance(g, ops.IndexedSlices):\n        tensor_to_avg.append(tf.reshape(tf.unsorted_segment_sum(g.values, g.indices, g.dense_shape[0] ), shape=t.get_shape() ) )\n      else:\n        tensor_to_avg.append(g)\n    avg_op = self._moving_averager.apply(tensor_to_avg)\n    grad_var_ops.append(avg_op)\n    with tf.control_dependencies([avg_op] ):\n      self._grad_avg = [self._moving_averager.average(val) for val in tensor_to_avg]\n      self._grad_avg_squared = [tf.square(val) for val in self._grad_avg]\n    self._grad_var = self._grad_norm_squared_avg - tf.add_n( [tf.reduce_sum(val) for val in self._grad_avg_squared] )\n    return grad_var_ops\n\n\n  def dist_to_opt(self):\n    dist_to_opt_ops = []\n    # running average of the norm of gradeint\n    self._grad_norm = tf.sqrt(self._grad_norm_squared)\n    avg_op = self._moving_averager.apply([self._grad_norm,] )\n    dist_to_opt_ops.append(avg_op)\n    with tf.control_dependencies([avg_op] ):\n      self._grad_norm_avg = self._moving_averager.average(self._grad_norm)\n      # single iteration distance estimation, note here self._grad_norm_avg is per variable\n      self._dist_to_opt = self._grad_norm_avg / self._grad_norm_squared_avg\n    # running average of distance\n    avg_op = self._moving_averager.apply([self._dist_to_opt] )\n    dist_to_opt_ops.append(avg_op)\n    with tf.control_dependencies([avg_op]):\n      self._dist_to_opt_avg = tf.identity(self._moving_averager.average(self._dist_to_opt) )\n    return dist_to_opt_ops\n\n\n  def after_apply(self):\n    self._moving_averager = tf.train.ExponentialMovingAverage(decay=self._beta, zero_debias=self._zero_debias)\n    assert self._grads != None and len(self._grads) > 0\n    after_apply_ops = []\n\n    # get per var g**2 and norm**2\n    self._grad_squared = []\n    self._grad_norm_squared = []\n    for v, g in zip(self._tvars, self._grads):\n      with ops.colocate_with(v):\n        self._grad_squared.append(tf.square(g) )\n    self._grad_norm_squared = [tf.reduce_sum(grad_squared) for grad_squared in self._grad_squared]\n\n    # the following running average on squared norm of gradient is shared by grad_var and dist_to_opt\n    avg_op = self._moving_averager.apply(self._grad_norm_squared)\n    with tf.control_dependencies([avg_op] ):\n      self._grad_norm_squared_avg = [self._moving_averager.average(val) for val in self._grad_norm_squared]\n      self._grad_norm_squared = tf.add_n(self._grad_norm_squared)\n      self._grad_norm_squared_avg = tf.add_n(self._grad_norm_squared_avg)\n    after_apply_ops.append(avg_op)\n\n    with tf.control_dependencies([avg_op] ):\n      curv_range_ops = self.curvature_range()\n      after_apply_ops += curv_range_ops\n      grad_var_ops = self.grad_variance()\n      after_apply_ops += grad_var_ops\n      dist_to_opt_ops = self.dist_to_opt() \n      after_apply_ops += dist_to_opt_ops\n\n    return tf.group(*after_apply_ops)\n\n\n  def get_lr_tensor(self):\n    lr = (1.0 - tf.sqrt(self._mu) )**2 / self._h_min\n    return lr\n\n\n  def get_mu_tensor(self):\n    const_fact = self._dist_to_opt_avg**2 * self._h_min**2 / 2 / self._grad_var\n    coef = tf.Variable([-1.0, 3.0, 0.0, 1.0], dtype=tf.float32, name=""cubic_solver_coef"")\n    coef = tf.scatter_update(coef, tf.constant(2), -(3 + const_fact) )        \n    roots = tf.py_func(np.roots, [coef], Tout=tf.complex64, stateful=False)\n    \n    # filter out the correct root\n    root_idx = tf.logical_and(tf.logical_and(tf.greater(tf.real(roots), tf.constant(0.0) ),\n      tf.less(tf.real(roots), tf.constant(1.0) ) ), tf.less(tf.abs(tf.imag(roots) ), 1e-5) )\n    # in case there are two duplicated roots satisfying the above condition\n    root = tf.reshape(tf.gather(tf.gather(roots, tf.where(root_idx) ), tf.constant(0) ), shape=[] )\n    tf.assert_equal(tf.size(root), tf.constant(1) )\n\n    dr = self._h_max / self._h_min\n    mu = tf.maximum(tf.real(root)**2, ( (tf.sqrt(dr) - 1)/(tf.sqrt(dr) + 1) )**2)    \n    return mu\n\n\n  def update_hyper_param(self):\n    assign_hyper_ops = []\n    self._mu = tf.identity(tf.cond(self._do_tune, lambda: self.get_mu_tensor(),\n      lambda: self._mu_var) )\n    with tf.control_dependencies([self._mu] ):\n      self._lr = tf.identity(tf.cond(self._do_tune, lambda: self.get_lr_tensor(),\n        lambda: self._lr_var) )\n    \n    with tf.control_dependencies([self._mu, self._lr] ):\n      self._mu = self._beta * self._mu_var + (1 - self._beta) * self._mu\n      self._lr = self._beta * self._lr_var + (1 - self._beta) * self._lr       \n      assign_hyper_ops.append(tf.assign(self._mu_var, self._mu) )\n      assign_hyper_ops.append(tf.assign(self._lr_var, self._lr) )\n    assign_hyper_op = tf.group(*assign_hyper_ops)\n    return assign_hyper_op\n\n\n  def apply_gradients(self, grads_tvars, global_step=None, name=None):\n    self._grads, self._tvars = zip(*grads_tvars)\n\n    with tf.variable_scope(""apply_updates""):\n      if self._clip_thresh_var is not None:\n        self._grads_clip, self._grads_norm = tf.clip_by_global_norm(self._grads, self._clip_thresh_var)\n        apply_grad_op = \\\n          self._optimizer.apply_gradients(zip(self._grads_clip, self._tvars) )\n      else:\n        apply_grad_op = \\\n          self._optimizer.apply_gradients(zip(self._grads, self._tvars) )\n\n\n    with tf.variable_scope(""after_apply""):\n      after_apply_op = self.after_apply()\n\n    with tf.variable_scope(""update_hyper""):\n      with tf.control_dependencies( [after_apply_op] ):\n        update_hyper_op = self.update_hyper_param()\n\n    with tf.control_dependencies([update_hyper_op] ):\n      self._increment_global_step_op = tf.assign(self._global_step, self._global_step + 1)\n\n    return tf.group(apply_grad_op, after_apply_op, update_hyper_op, self._increment_global_step_op)\n\n\n  def minimize(self, loss, global_step=None, var_list=None,\n               gate_gradients=GATE_OP, aggregation_method=None,\n               colocate_gradients_with_ops=False, name=None,\n               grad_loss=None):\n    """"""Adapted from Tensorflow Optimizer base class member function:\n    Add operations to minimize `loss` by updating `var_list`.\n    This method simply combines calls `compute_gradients()` and\n    `apply_gradients()`. If you want to process the gradient before applying\n    them call `tf.gradients()` and `self.apply_gradients()` explicitly instead\n    of using this function.\n    """"""\n    grads_and_vars = self._optimizer.compute_gradients(\n        loss, var_list=var_list, gate_gradients=gate_gradients,\n        aggregation_method=aggregation_method,\n        colocate_gradients_with_ops=colocate_gradients_with_ops,\n        grad_loss=grad_loss)\n\n    vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n    if not vars_with_grad:\n      raise ValueError(\n          ""No gradients provided for any variable, check your graph for ops""\n          "" that do not support gradients, between variables %s and loss %s."" %\n          ([str(v) for _, v in grads_and_vars], loss))\n    for g, v in grads_and_vars:\n      print(""g "", g)\n      print(""v "", v)\n\n    return self.apply_gradients(grads_and_vars)\n\n\n  def compute_gradients(self, loss, var_list, gate_gradients=GATE_OP,\n                        aggregation_method=None, colocate_gradients_with_ops=False,\n                        name=None,grad_loss=None):\n\n    return self._optimizer.compute_gradients(\n      loss, var_list=var_list, gate_gradients=gate_gradients,\n      aggregation_method=aggregation_method,\n      colocate_gradients_with_ops=colocate_gradients_with_ops,\n      grad_loss=grad_loss)\n        '"
optimizer/yellowfin_test.py,24,"b'import os\n# os.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\nimport tensorflow as tf\nimport numpy as np\nfrom yellowfin import YFOptimizer\nfrom tensorflow.python.ops import variables\nimport time\n\n\nn_dim = 1000000\nn_iter = 50\n\ndef tune_everything(x0squared, C, T, gmin, gmax):\n  # First tune based on dynamic range    \n  if C==0:\n    dr=gmax/gmin\n    mustar=((np.sqrt(dr)-1)/(np.sqrt(dr)+1))**2\n    alpha_star = (1+np.sqrt(mustar))**2/gmax\n    \n    return alpha_star,mustar\n\n  dist_to_opt = x0squared\n  grad_var = C\n  max_curv = gmax\n  min_curv = gmin\n  const_fact = dist_to_opt * min_curv**2 / 2 / grad_var\n  coef = [-1, 3, -(3 + const_fact), 1]\n  roots = np.roots(coef)\n  roots = roots[np.real(roots) > 0]\n  roots = roots[np.real(roots) < 1]\n  root = roots[np.argmin(np.imag(roots) ) ]\n\n  assert root > 0 and root < 1 and np.absolute(root.imag) < 1e-6\n\n  dr = max_curv / min_curv\n  assert max_curv >= min_curv\n  mu = max( ( (np.sqrt(dr) - 1) / (np.sqrt(dr) + 1) )**2, root**2)\n\n  lr_min = (1 - np.sqrt(mu) )**2 / min_curv\n  lr_max = (1 + np.sqrt(mu) )**2 / max_curv\n\n  alpha_star = lr_min\n  mustar = mu\n\n  return alpha_star, mustar\n\n\ndef test_measurement():\n  opt = YFOptimizer(zero_debias=False)\n  w = tf.Variable(np.ones([n_dim, ] ), dtype=tf.float32, name=""w"", trainable=True)\n  b = tf.Variable(np.ones([1, ], dtype=np.float32), dtype=tf.float32, name=""b"", trainable=True)\n  x = tf.constant(np.ones([n_dim, ], dtype=np.float32), dtype=tf.float32)\n  loss = tf.multiply(w, x) + b\n  tvars = tf.trainable_variables()\n\n  w_grad_val = tf.placeholder(tf.float32, shape=(n_dim, ) )\n  b_grad_val = tf.placeholder(tf.float32, shape=(1, ) )\n  apply_op = opt.apply_gradients(zip([w_grad_val, b_grad_val], tvars) )\n\n  init_op = tf.global_variables_initializer()\n  with tf.Session() as sess:\n    sess.run(init_op)\n    target_h_max = 0.0\n    target_h_min = 0.0\n    g_norm_squared_avg = 0.0\n    g_norm_avg = 0.0\n    g_avg = 0.0\n    target_dist = 0.0\n    for i in range(n_iter):\n      feed_dict = {w_grad_val: (i + 1) * np.ones( [n_dim, ], dtype=np.float32),\n             b_grad_val: (i + 1) * np.ones( [1, ], dtype=np.float32) }\n      res = sess.run( [opt._curv_win, opt._h_max, opt._h_min, opt._grad_var, opt._dist_to_opt_avg, apply_op], feed_dict=feed_dict)\n\n      g_norm_squared_avg = 0.999 * g_norm_squared_avg  \\\n        + 0.001 * np.sum(( (i + 1)*np.ones( [n_dim + 1, ] ) )**2)\n      g_norm_avg = 0.999 * g_norm_avg  \\\n        + 0.001 * np.linalg.norm( (i + 1)*np.ones( [n_dim + 1, ] ) )\n      g_avg = 0.999 * g_avg + 0.001 * (i + 1)\n \n      target_h_max = 0.999 * target_h_max + 0.001 * (i + 1)**2*(n_dim + 1)\n      target_h_min = 0.999 * target_h_min + 0.001 * max(1, i + 2 - 20)**2*(n_dim + 1)\n      target_var = g_norm_squared_avg - g_avg**2 * (n_dim + 1)\n      target_dist = 0.999 * target_dist + 0.001 * g_norm_avg / g_norm_squared_avg\n\n      # print ""iter "", i, "" h max "", res[1], target_h_max, "" h min "", res[2], target_h_min, \\\n      #   "" var "", res[3], target_var, "" dist "", res[4], target_dist\n      assert np.abs(target_h_max - res[1] ) < np.abs(target_h_max) * 1e-3\n      assert np.abs(target_h_min - res[2] ) < np.abs(target_h_min) * 1e-3\n      assert np.abs(target_var - res[3] ) < np.abs(res[3] ) * 1e-3\n      assert np.abs(target_dist - res[4] ) < np.abs(res[4] ) * 1e-3\n  print(""sync measurement test passed!"")\n\n\ndef test_lr_mu():\n  opt = YFOptimizer(zero_debias=False)\n  w = tf.Variable(np.ones([n_dim, ] ), dtype=tf.float32, name=""w"", trainable=True)\n  b = tf.Variable(np.ones([1, ], dtype=np.float32), dtype=tf.float32, name=""b"", trainable=True)\n  x = tf.constant(np.ones([n_dim, ], dtype=np.float32), dtype=tf.float32)\n  loss = tf.multiply(w, x) + b\n  tvars = tf.trainable_variables()\n\n  w_grad_val = tf.Variable(np.zeros( [n_dim, ] ), dtype=tf.float32, trainable=False)\n  b_grad_val = tf.Variable(np.zeros([1, ] ), dtype=tf.float32, trainable=False)\n  apply_op = opt.apply_gradients(zip([w_grad_val, b_grad_val], tvars) )\n\n  init_op = tf.global_variables_initializer()\n  with tf.Session() as sess:\n    sess.run(init_op)\n    target_h_max = 0.0\n    target_h_min = 0.0\n    g_norm_squared_avg = 0.0\n    g_norm_avg = 0.0\n    g_avg = 0.0\n    target_dist = 0.0\n    target_lr = 0.1\n    target_mu = 0.0\n    for i in range(n_iter):\n    \n      sess.run(tf.assign(w_grad_val, (i + 1) * np.ones( [n_dim, ], dtype=np.float32) ) )\n      sess.run(tf.assign(b_grad_val, (i + 1) * np.ones( [1, ], dtype=np.float32) ) )\n  \n      res = sess.run( [opt._curv_win, opt._h_max, opt._h_min, opt._grad_var, opt._dist_to_opt_avg, \n        opt._lr_var, opt._mu_var, apply_op] )\n    \n      res[5] = opt._lr_var.eval()\n      res[6] = opt._mu_var.eval()\n  \n      g_norm_squared_avg = 0.999 * g_norm_squared_avg  \\\n        + 0.001 * np.sum(( (i + 1)*np.ones( [n_dim + 1, ] ) )**2)\n      g_norm_avg = 0.999 * g_norm_avg  \\\n        + 0.001 * np.linalg.norm( (i + 1)*np.ones( [n_dim + 1, ] ) )\n      g_avg = 0.999 * g_avg + 0.001 * (i + 1)\n \n      target_h_max = 0.999 * target_h_max + 0.001 * (i + 1)**2*(n_dim + 1)\n      target_h_min = 0.999 * target_h_min + 0.001 * max(1, i + 2 - 20)**2*(n_dim + 1)\n      target_var = g_norm_squared_avg - g_avg**2 * (n_dim + 1)\n      target_dist = 0.999 * target_dist + 0.001 * g_norm_avg / g_norm_squared_avg\n\n      if i > 0:\n        lr, mu = tune_everything(target_dist**2, target_var, 1, target_h_min, target_h_max)\n        target_lr = 0.999 * target_lr + 0.001 * lr\n        target_mu = 0.999 * target_mu + 0.001 * mu\n\n      # print ""iter "", i, "" h max "", res[1], target_h_max, "" h min "", res[2], target_h_min, \\\n   #                              "" var "", res[3], target_var, "" dist "", res[4], target_dist\n      # print ""iter "", i, "" lr "", res[5], target_lr, "" mu "", res[6], target_mu\n\n      assert np.abs(target_h_max - res[1] ) < np.abs(target_h_max) * 1e-3\n      assert np.abs(target_h_min - res[2] ) < np.abs(target_h_min) * 1e-3\n      assert np.abs(target_var - res[3] ) < np.abs(res[3] ) * 1e-3\n      assert np.abs(target_dist - res[4] ) < np.abs(res[4] ) * 1e-3\n      assert target_lr == 0.0 or np.abs(target_lr - res[5] ) < np.abs(res[5] ) * 1e-3\n      assert target_mu == 0.0 or np.abs(target_mu - res[6] ) < np.abs(res[6] ) * 5e-3 \n  print(""lr and mu computing test passed!"")\n\n\nif __name__ == ""__main__"":\n  # test gpu mode\n  with tf.variable_scope(""test_sync_measurement""):\n    start = time.time()\n    test_measurement()\n    end = time.time()\n    print(""GPU measurement test done in "", (end - start)/float(n_iter), "" s/iter!"")\n  with tf.variable_scope(""test_sync_lr_mu""):\n    start = time.time()\n    test_lr_mu()\n    end = time.time()\n    print(""GPU lr and mu test done in "", (end - start)/float(n_iter), "" s/iter!"")\n\n  # test cpu mode\n  with tf.variable_scope(""test_sync_measurement_cpu""), tf.device(""cpu:0""):\n    start = time.time()\n    test_measurement()\n    end = time.time()\n    print(""CPU measurement test done in "", (end - start)/float(n_iter), "" s/iter!"")\n  with tf.variable_scope(""test_sync_lr_mu_cpu""), tf.device(""cpu:0""):\n    start = time.time()\n    test_lr_mu()\n    end = time.time()\n    print(""CPU lr and mu test done in "", (end - start)/float(n_iter), "" s/iter!"")\n\n\n'"
preprocessing/__init__.py,0,b'\n'
preprocessing/cifarnet_preprocessing.py,15,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images in CIFAR-10.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_PADDING = 4\n\nslim = tf.contrib.slim\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         padding=_PADDING):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    padding: The amound of padding before and after each dimension of the image.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  tf.summary.image(\'image\', tf.expand_dims(image, 0))\n\n  # Transform the image to floats.\n  image = tf.to_float(image)\n  if padding > 0:\n    image = tf.pad(image, [[padding, padding], [padding, padding], [0, 0]])\n  # Randomly crop a [height, width] section of the image.\n  distorted_image = tf.random_crop(image,\n                                   [output_height, output_width, 3])\n\n  # Randomly flip the image horizontally.\n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  tf.summary.image(\'distorted_image\', tf.expand_dims(distorted_image, 0))\n\n  # Because these operations are not commutative, consider randomizing\n  # the order their operation.\n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n  # Subtract off the mean and divide by the variance of the pixels.\n  return tf.image.per_image_standardization(distorted_image)\n\n\ndef preprocess_for_eval(image, output_height, output_width):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  tf.summary.image(\'image\', tf.expand_dims(image, 0))\n  # Transform the image to floats.\n  image = tf.to_float(image)\n\n  # Resize and crop if needed.\n  resized_image = tf.image.resize_image_with_crop_or_pad(image,\n                                                         output_width,\n                                                         output_height)\n  tf.summary.image(\'resized_image\', tf.expand_dims(resized_image, 0))\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  return tf.image.per_image_standardization(resized_image)\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width)\n  else:\n    return preprocess_for_eval(image, output_height, output_width)\n'"
preprocessing/inception_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
preprocessing/lenet_preprocessing.py,5,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities for preprocessing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef preprocess_image(image, output_height, output_width, is_training):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = tf.to_float(image)\n  image = tf.image.resize_image_with_crop_or_pad(\n      image, output_width, output_height)\n  image = tf.subtract(image, 128.0)\n  image = tf.div(image, 128.0)\n  return image\n'"
preprocessing/mobilenet_preprocessing.py,62,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the MobileNet networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.summary.image(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.summary.image(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.summary.image(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.subtract(distorted_image, 0.5)\n    distorted_image = tf.multiply(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
preprocessing/mobilenetdet_preprocessing.py,63,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the MobileNet networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import control_flow_ops\nfrom preprocessing import tf_image\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef check_3d_image(image, require_static=True):\n  """"""Assert that we are working with properly shaped image.\n\n  Args:\n    image: 3-D Tensor of shape [height, width, channels]\n    require_static: If `True`, requires that all dimensions of `image` are\n      known and non-zero.\n\n  Raises:\n    ValueError: if `image.shape` is not a 3-vector.\n\n  Returns:\n    An empty list, if `image` has fully defined dimensions. Otherwise, a list\n    containing an assert op is returned.\n  """"""\n  try:\n    image_shape = image.get_shape().with_rank(3)\n  except ValueError:\n    raise ValueError(""\'image\' must be three-dimensional."")\n  if require_static and not image_shape.is_fully_defined():\n    raise ValueError(""\'image\' must be fully defined."")\n  if any(x == 0 for x in image_shape):\n    raise ValueError(""all dims of \'image.shape\' must be > 0: %s"" %\n                     image_shape)\n  if not image_shape.is_fully_defined():\n    return [tf.assert_positive(tf.shape(image),\n                                      [""all dims of \'image.shape\' ""\n                                       ""must be > 0.""])]\n  else:\n    return []\n\n\ndef flip_with_bboxes(image, bboxes):\n  uniform_random = tf.random_uniform([], 0, 1.0)\n  mirror_cond = tf.less(uniform_random, .5)\n  stride = tf.where(mirror_cond, -1, 1)\n\n  def flip(image, bboxes, stride):\n    image = image[:, ::stride, :]\n    img_w = tf.cast(tf.shape(image)[1], dtype=tf.float32)\n    bbox_coords = tf.unstack(bboxes, num=4, axis=1)\n    y_min = bbox_coords[0]\n    x_min = bbox_coords[1]\n    y_max = bbox_coords[2]\n    x_max = bbox_coords[3]\n    x_min_flip = img_w - x_max\n    x_max_flip = img_w - x_min\n    bboxes = tf.stack([y_min, x_min_flip, y_max, x_max_flip], 1, name=\'flip_bboxes\')\n    return image, bboxes\n\n  def not_flip(image, bboxes):\n    return image, bboxes\n\n  image_fliped, bboxes = tf.cond(mirror_cond, lambda: flip(image, bboxes, stride), lambda: not_flip(image, bboxes))\n\n  return tf_image.fix_image_flip_shape(image, image_fliped), bboxes\n\n\n""""""\ndef shift_with_bboxes(image, bboxes):\n  shift_ratio = 0.2  # TODO(shizehao): remove hard code\n  image = tf.convert_to_tensor(image)\n  check_3d_image(image)\n  img_h, img_w, _ = image.get_shape.aslist()\n  shift_x = tf.random_uniform([], -shift_ratio, shift_ratio) * img_w\n  shift_y = tf.random_uniform([], -shift_ratio, shift_ratio) * img_h\n  tf.image.crop_to_bounding_box()\n  tf.image.resize_image_with_crop_or_pad()\n  tf.image.pad_to_bounding_box()\n  return image, bboxes\n""""""\n\ndef preprocess_for_train(image, height, width, labels, bboxes,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bboxes]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n    tf.summary.image(\'ori_image\',\n                     tf.expand_dims(image, 0))\n    # Randomly distort the colors. There are 4 ways to do it.\n    image = apply_with_random_selector(\n      image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    # Randomly flip the image horizontally.\n    image, bboxes = flip_with_bboxes(image, bboxes)\n\n    print(image.get_shape())\n\n    # TODO(shizehao): bistort bbox\n    image = tf.squeeze(tf.image.resize_nearest_neighbor(tf.expand_dims(image,axis=0),size=[height, width]))\n\n    tf.summary.image(\'final_distorted_image\',\n                     tf.expand_dims(image, 0))\n\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n\n    return image, labels, bboxes\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     labels, bboxes,\n                     is_training=False,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, labels, bboxes, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
preprocessing/preprocessing_factory.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import cifarnet_preprocessing\nfrom preprocessing import inception_preprocessing\nfrom preprocessing import lenet_preprocessing\nfrom preprocessing import vgg_preprocessing\nfrom preprocessing import mobilenet_preprocessing\nfrom preprocessing import mobilenetdet_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n  """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n  Args:\n    name: The name of the preprocessing function.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\n      It has the following signature:\n        image = preprocessing_fn(image, output_height, output_width, ...).\n\n  Raises:\n    ValueError: If Preprocessing `name` is not recognized.\n  """"""\n  preprocessing_fn_map = {\n      \'cifarnet\': cifarnet_preprocessing,\n      \'inception\': inception_preprocessing,\n      \'inception_v1\': inception_preprocessing,\n      \'inception_v2\': inception_preprocessing,\n      \'inception_v3\': inception_preprocessing,\n      \'inception_v4\': inception_preprocessing,\n      \'inception_resnet_v2\': inception_preprocessing,\n      \'lenet\': lenet_preprocessing,\n      \'resnet_v1_50\': vgg_preprocessing,\n      \'resnet_v1_101\': vgg_preprocessing,\n      \'resnet_v1_152\': vgg_preprocessing,\n      \'resnet_v2_50\': vgg_preprocessing,\n      \'resnet_v2_101\': vgg_preprocessing,\n      \'resnet_v2_152\': vgg_preprocessing,\n      \'vgg\': vgg_preprocessing,\n      \'vgg_a\': vgg_preprocessing,\n      \'vgg_16\': vgg_preprocessing,\n      \'vgg_19\': vgg_preprocessing,\n      \'mobilenet\': mobilenet_preprocessing,\n      \'mobilenetdet\': mobilenetdet_preprocessing\n  }\n\n  if name not in preprocessing_fn_map:\n    raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n  def preprocessing_fn(image, output_height, output_width, **kwargs):\n    return preprocessing_fn_map[name].preprocess_image(\n        image, output_height, output_width, is_training=is_training, **kwargs)\n\n  return preprocessing_fn\n'"
preprocessing/tf_image.py,15,"b'# Copyright 2015 The TensorFlow Authors and Paul Balanca. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Custom image operations.\nMost of the following methods extend TensorFlow image library, and part of\nthe code is shameless copy-paste of the former!\n""""""\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import check_ops\nfrom tensorflow.python.ops import clip_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import gen_image_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import string_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import random_ops\nfrom tensorflow.python.ops import variables\n\n\n# =========================================================================== #\n# Modification of TensorFlow image routines.\n# =========================================================================== #\ndef _assert(cond, ex_type, msg):\n    """"""A polymorphic assert, works with tensors and boolean expressions.\n    If `cond` is not a tensor, behave like an ordinary assert statement, except\n    that a empty list is returned. If `cond` is a tensor, return a list\n    containing a single TensorFlow assert op.\n    Args:\n      cond: Something evaluates to a boolean value. May be a tensor.\n      ex_type: The exception class to use.\n      msg: The error message.\n    Returns:\n      A list, containing at most one assert op.\n    """"""\n    if _is_tensor(cond):\n        return [control_flow_ops.Assert(cond, [msg])]\n    else:\n        if not cond:\n            raise ex_type(msg)\n        else:\n            return []\n\n\ndef _is_tensor(x):\n    """"""Returns `True` if `x` is a symbolic tensor-like object.\n    Args:\n      x: A python object to check.\n    Returns:\n      `True` if `x` is a `tf.Tensor` or `tf.Variable`, otherwise `False`.\n    """"""\n    return isinstance(x, (ops.Tensor, variables.Variable))\n\n\ndef _ImageDimensions(image):\n    """"""Returns the dimensions of an image tensor.\n    Args:\n      image: A 3-D Tensor of shape `[height, width, channels]`.\n    Returns:\n      A list of `[height, width, channels]` corresponding to the dimensions of the\n        input image.  Dimensions that are statically known are python integers,\n        otherwise they are integer scalar tensors.\n    """"""\n    if image.get_shape().is_fully_defined():\n        return image.get_shape().as_list()\n    else:\n        static_shape = image.get_shape().with_rank(3).as_list()\n        dynamic_shape = array_ops.unstack(array_ops.shape(image), 3)\n        return [s if s is not None else d\n                for s, d in zip(static_shape, dynamic_shape)]\n\n\ndef _Check3DImage(image, require_static=True):\n    """"""Assert that we are working with properly shaped image.\n    Args:\n      image: 3-D Tensor of shape [height, width, channels]\n        require_static: If `True`, requires that all dimensions of `image` are\n        known and non-zero.\n    Raises:\n      ValueError: if `image.shape` is not a 3-vector.\n    Returns:\n      An empty list, if `image` has fully defined dimensions. Otherwise, a list\n        containing an assert op is returned.\n    """"""\n    try:\n        image_shape = image.get_shape().with_rank(3)\n    except ValueError:\n        raise ValueError(""\'image\' must be three-dimensional."")\n    if require_static and not image_shape.is_fully_defined():\n        raise ValueError(""\'image\' must be fully defined."")\n    if any(x == 0 for x in image_shape):\n        raise ValueError(""all dims of \'image.shape\' must be > 0: %s"" %\n                         image_shape)\n    if not image_shape.is_fully_defined():\n        return [check_ops.assert_positive(array_ops.shape(image),\n                                          [""all dims of \'image.shape\' ""\n                                           ""must be > 0.""])]\n    else:\n        return []\n\n\ndef fix_image_flip_shape(image, result):\n    """"""Set the shape to 3 dimensional if we don\'t know anything else.\n    Args:\n      image: original image size\n      result: flipped or transformed image\n    Returns:\n      An image whose shape is at least None,None,None.\n    """"""\n    image_shape = image.get_shape()\n    if image_shape == tensor_shape.unknown_shape():\n        result.set_shape([None, None, None])\n    else:\n        result.set_shape(image_shape)\n    return result\n\n\n# =========================================================================== #\n# Image + BBoxes methods: cropping, resizing, flipping, ...\n# =========================================================================== #\ndef bboxes_crop_or_pad(bboxes,\n                       height, width,\n                       offset_y, offset_x,\n                       target_height, target_width):\n    """"""Adapt bounding boxes to crop or pad operations.\n    Coordinates are always supposed to be relative to the image.\n\n    Arguments:\n      bboxes: Tensor Nx4 with bboxes coordinates [y_min, x_min, y_max, x_max];\n      height, width: Original image dimension;\n      offset_y, offset_x: Offset to apply,\n        negative if cropping, positive if padding;\n      target_height, target_width: Target dimension after cropping / padding.\n    """"""\n    with tf.name_scope(\'bboxes_crop_or_pad\'):\n        # Rescale bounding boxes in pixels.\n        scale = tf.cast(tf.stack([height, width, height, width]), bboxes.dtype)\n        bboxes = bboxes * scale\n        # Add offset.\n        offset = tf.cast(tf.stack([offset_y, offset_x, offset_y, offset_x]), bboxes.dtype)\n        bboxes = bboxes + offset\n        # Rescale to target dimension.\n        scale = tf.cast(tf.stack([target_height, target_width,\n                                  target_height, target_width]), bboxes.dtype)\n        bboxes = bboxes / scale\n        return bboxes\n\n\ndef resize_image_bboxes_with_crop_or_pad(image, bboxes,\n                                         target_height, target_width):\n    """"""Crops and/or pads an image to a target width and height.\n    Resizes an image to a target width and height by either centrally\n    cropping the image or padding it evenly with zeros.\n\n    If `width` or `height` is greater than the specified `target_width` or\n    `target_height` respectively, this op centrally crops along that dimension.\n    If `width` or `height` is smaller than the specified `target_width` or\n    `target_height` respectively, this op centrally pads with 0 along that\n    dimension.\n    Args:\n      image: 3-D tensor of shape `[height, width, channels]`\n      target_height: Target height.\n      target_width: Target width.\n    Raises:\n      ValueError: if `target_height` or `target_width` are zero or negative.\n    Returns:\n      Cropped and/or padded image of shape\n        `[target_height, target_width, channels]`\n    """"""\n    with tf.name_scope(\'resize_with_crop_or_pad\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n\n        assert_ops = []\n        assert_ops += _Check3DImage(image, require_static=False)\n        assert_ops += _assert(target_width > 0, ValueError,\n                              \'target_width must be > 0.\')\n        assert_ops += _assert(target_height > 0, ValueError,\n                              \'target_height must be > 0.\')\n\n        image = control_flow_ops.with_dependencies(assert_ops, image)\n        # `crop_to_bounding_box` and `pad_to_bounding_box` have their own checks.\n        # Make sure our checks come first, so that error messages are clearer.\n        if _is_tensor(target_height):\n            target_height = control_flow_ops.with_dependencies(\n                assert_ops, target_height)\n        if _is_tensor(target_width):\n            target_width = control_flow_ops.with_dependencies(assert_ops, target_width)\n\n        def max_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.maximum(x, y)\n            else:\n                return max(x, y)\n\n        def min_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.minimum(x, y)\n            else:\n                return min(x, y)\n\n        def equal_(x, y):\n            if _is_tensor(x) or _is_tensor(y):\n                return math_ops.equal(x, y)\n            else:\n                return x == y\n\n        height, width, _ = _ImageDimensions(image)\n        width_diff = target_width - width\n        offset_crop_width = max_(-width_diff // 2, 0)\n        offset_pad_width = max_(width_diff // 2, 0)\n\n        height_diff = target_height - height\n        offset_crop_height = max_(-height_diff // 2, 0)\n        offset_pad_height = max_(height_diff // 2, 0)\n\n        # Maybe crop if needed.\n        height_crop = min_(target_height, height)\n        width_crop = min_(target_width, width)\n        cropped = tf.image.crop_to_bounding_box(image, offset_crop_height, offset_crop_width,\n                                                height_crop, width_crop)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height, width,\n                                    -offset_crop_height, -offset_crop_width,\n                                    height_crop, width_crop)\n        # Maybe pad if needed.\n        resized = tf.image.pad_to_bounding_box(cropped, offset_pad_height, offset_pad_width,\n                                               target_height, target_width)\n        bboxes = bboxes_crop_or_pad(bboxes,\n                                    height_crop, width_crop,\n                                    offset_pad_height, offset_pad_width,\n                                    target_height, target_width)\n\n        # In theory all the checks below are redundant.\n        if resized.get_shape().ndims is None:\n            raise ValueError(\'resized contains no shape.\')\n\n        resized_height, resized_width, _ = _ImageDimensions(resized)\n\n        assert_ops = []\n        assert_ops += _assert(equal_(resized_height, target_height), ValueError,\n                              \'resized height is not correct.\')\n        assert_ops += _assert(equal_(resized_width, target_width), ValueError,\n                              \'resized width is not correct.\')\n\n        resized = control_flow_ops.with_dependencies(assert_ops, resized)\n        return resized, bboxes\n\n\ndef resize_image(image, size,\n                 method=tf.image.ResizeMethod.BILINEAR,\n                 align_corners=False):\n    """"""Resize an image and bounding boxes.\n    """"""\n    # Resize image.\n    with tf.name_scope(\'resize_image\'):\n        height, width, channels = _ImageDimensions(image)\n        image = tf.expand_dims(image, 0)\n        image = tf.image.resize_images(image, size,\n                                       method, align_corners)\n        image = tf.reshape(image, tf.stack([size[0], size[1], channels]))\n        return image\n\n\ndef random_flip_left_right(image, bboxes, seed=None):\n    """"""Random flip left-right of an image and its bounding boxes.\n    """"""\n    def flip_bboxes(bboxes):\n        """"""Flip bounding boxes coordinates.\n        """"""\n        bboxes = tf.stack([bboxes[:, 0], 1 - bboxes[:, 3],\n                           bboxes[:, 2], 1 - bboxes[:, 1]], axis=-1)\n        return bboxes\n\n    # Random flip. Tensorflow implementation.\n    with tf.name_scope(\'random_flip_left_right\'):\n        image = ops.convert_to_tensor(image, name=\'image\')\n        _Check3DImage(image, require_static=False)\n        uniform_random = random_ops.random_uniform([], 0, 1.0, seed=seed)\n        mirror_cond = math_ops.less(uniform_random, .5)\n        # Flip image.\n        result = control_flow_ops.cond(mirror_cond,\n                                       lambda: array_ops.reverse_v2(image, [1]),\n                                       lambda: image)\n        # Flip bboxes.\n        bboxes = control_flow_ops.cond(mirror_cond,\n                                       lambda: flip_bboxes(bboxes),\n                                       lambda: bboxes)\n        return fix_image_flip_shape(image, result), bboxes\n\n'"
preprocessing/vgg_preprocessing.py,54,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  cropped_shape = control_flow_ops.with_dependencies(\n      [rank_assertion],\n      tf.stack([crop_height, crop_width, original_shape[2]]))\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  image = control_flow_ops.with_dependencies(\n      [size_assertion],\n      tf.slice(image, offsets, cropped_shape))\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n      [rank_assertions[0]],\n      tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(axis=2, values=channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
test/test_tfrecord.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom preprocessing import preprocessing_factory\nfrom configs.kitti_config import config\nfrom nets.mobilenetdet import scale_bboxes\n\nfrom datasets import dataset_factory\nfrom tensorflow.contrib import slim\n\ndataset = dataset_factory.get_dataset(\n  \'kitti\', \'train\', \'/home/zehao/Dataset/KITII/tfrecord\')\n\n# def conver_box(bboxes, img_h, img_w):\n#   [ymin, xmin, ymax, xmax] = tf.unstack(bboxes, axis=1)\n#   img_h = tf.cast(img_h, tf.float32)\n#   img_w = tf.cast(img_w, tf.float32)\n#   ymin = tf.truediv(ymin, img_h)\n#   xmin = tf.truediv(xmin, img_w)\n#   ymax = tf.truediv(ymax, img_h)\n#   xmax = tf.truediv(xmax, img_w)\n#   return tf.expand_dims(tf.stack([ymin,xmin,ymax,xmax], axis=1), axis=0)\n\nwith tf.Graph().as_default() as graph:\n  with tf.device(\'/cpu:0\'):\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n      dataset,\n      num_readers=1,\n      common_queue_capacity=20 * 1,\n      common_queue_min=10 * 1)\n    [image, shape, bbox, label] = provider.get([\'image\', \'shape\', \'object/bbox\', \'object/label\'])\n\n    bbox = scale_bboxes(bbox, shape)\n    bbox = tf.expand_dims(bbox, axis=0)\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n          \'mobilenetdet\',\n          is_training=True)\n\n    image, gt_labels, gt_bboxes = image_preprocessing_fn(image,\n                                                         config.IMG_HEIGHT,\n                                                         config.IMG_WIDTH,\n                                                         labels=label,\n                                                         bboxes=bbox,\n                                                         )\n\n\n\n  with tf.Session() as sess:\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    # [image, bbox, label] = sess.run([image, gt_bboxes, gt_labels])\n\n    summary_writer = tf.summary.FileWriter(""/home/zehao/PycharmProjects/MobileNet/summary"", sess.graph)\n    merge = tf.summary.merge_all()\n    merge = sess.run(merge)\n    summary_writer.add_summary(merge)\n    summary_writer.close()\n'"
tools/__init__.py,0,b''
tools/freeze_graph.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts checkpoint variables into Const ops in a standalone GraphDef file.\n\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\nvariable values stored in a checkpoint file, and output a GraphDef with all of\nthe variable ops converted into const ops containing the values of the\nvariables.\n\nIt\'s useful to do this when we need to load a single file in C++, especially in\nenvironments like mobile or embedded where we may not have access to the\nRestoreTensor ops and file loading calls that they rely on.\n\nAn example of command-line usage is:\nbazel build tensorflow/python/tools:freeze_graph && \\\nbazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=some_graph_def.pb \\\n--input_checkpoint=model.ckpt-8361242 \\\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\n\nYou can also look at freeze_graph_test.py for an example of how to use it.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nfrom google.protobuf import text_format\n\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.protobuf import saver_pb2\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.training import saver as saver_lib\n\nFLAGS = None\n\n\ndef freeze_graph(input_graph,\n                 input_saver,\n                 input_binary,\n                 input_checkpoint,\n                 output_node_names,\n                 restore_op_name,\n                 filename_tensor_name,\n                 output_graph,\n                 clear_devices,\n                 initializer_nodes,\n                 variable_names_blacklist=""""):\n  """"""Converts all variables in a graph and checkpoint into constants.""""""\n\n  del restore_op_name, filename_tensor_name  # Unused by updated loading code.\n\n  if not gfile.Exists(input_graph):\n    print(""Input graph file \'"" + input_graph + ""\' does not exist!"")\n    return -1\n\n  if input_saver and not gfile.Exists(input_saver):\n    print(""Input saver file \'"" + input_saver + ""\' does not exist!"")\n    return -1\n\n  # \'input_checkpoint\' may be a prefix if we\'re using Saver V2 format\n  if not saver_lib.checkpoint_exists(input_checkpoint):\n    print(""Input checkpoint \'"" + input_checkpoint + ""\' doesn\'t exist!"")\n    return -1\n\n  if not output_node_names:\n    print(""You need to supply the name of a node to --output_node_names."")\n    return -1\n\n  input_graph_def = graph_pb2.GraphDef()\n  mode = ""rb"" if input_binary else ""r""\n  with gfile.FastGFile(input_graph, mode) as f:\n    if input_binary:\n      input_graph_def.ParseFromString(f.read())\n    else:\n      text_format.Merge(f.read(), input_graph_def)\n  # Remove all the explicit device specifications for this node. This helps to\n  # make the graph more portable.\n  if clear_devices:\n    for node in input_graph_def.node:\n      node.device = """"\n\n  _ = importer.import_graph_def(input_graph_def, name="""")\n\n  with session.Session() as sess:\n    if input_saver:\n      with gfile.FastGFile(input_saver, mode) as f:\n        saver_def = saver_pb2.SaverDef()\n        if input_binary:\n          saver_def.ParseFromString(f.read())\n        else:\n          text_format.Merge(f.read(), saver_def)\n        saver = saver_lib.Saver(saver_def=saver_def)\n        saver.restore(sess, input_checkpoint)\n    else:\n      var_list = {}\n      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      for key in var_to_shape_map:\n        try:\n          tensor = sess.graph.get_tensor_by_name(key + "":0"")\n        except KeyError:\n          # This tensor doesn\'t exist in the graph (for example it\'s\n          # \'global_step\' or a similar housekeeping element) so skip it.\n          continue\n        var_list[key] = tensor\n      saver = saver_lib.Saver(var_list=var_list)\n      saver.restore(sess, input_checkpoint)\n      if initializer_nodes:\n        sess.run(initializer_nodes)\n\n    variable_names_blacklist = (variable_names_blacklist.split("","") if\n                                variable_names_blacklist else None)\n    output_graph_def = graph_util.convert_variables_to_constants(\n        sess,\n        input_graph_def,\n        output_node_names.split("",""),\n        variable_names_blacklist=variable_names_blacklist)\n\n  with gfile.GFile(output_graph, ""wb"") as f:\n    f.write(output_graph_def.SerializeToString())\n  print(""%d ops in the final graph."" % len(output_graph_def.node))\n\n\ndef main(unused_args):\n  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,\n               FLAGS.input_checkpoint, FLAGS.output_node_names,\n               FLAGS.restore_op_name, FLAGS.filename_tensor_name,\n               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes,\n               FLAGS.variable_names_blacklist)\n\n\nif __name__ == ""__main__"":\n  parser = argparse.ArgumentParser()\n  parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n  parser.add_argument(\n      ""--input_graph"",\n      type=str,\n      default="""",\n      help=""TensorFlow \\\'GraphDef\\\' file to load."")\n  parser.add_argument(\n      ""--input_saver"",\n      type=str,\n      default="""",\n      help=""TensorFlow saver file to load."")\n  parser.add_argument(\n      ""--input_checkpoint"",\n      type=str,\n      default="""",\n      help=""TensorFlow variables file to load."")\n  parser.add_argument(\n      ""--output_graph"",\n      type=str,\n      default="""",\n      help=""Output \\\'GraphDef\\\' file name."")\n  parser.add_argument(\n      ""--input_binary"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=False,\n      help=""Whether the input files are in binary format."")\n  parser.add_argument(\n      ""--output_node_names"",\n      type=str,\n      default="""",\n      help=""The name of the output nodes, comma separated."")\n  parser.add_argument(\n      ""--restore_op_name"",\n      type=str,\n      default=""save/restore_all"",\n      help=""The name of the master restore operator."")\n  parser.add_argument(\n      ""--filename_tensor_name"",\n      type=str,\n      default=""save/Const:0"",\n      help=""The name of the tensor holding the save path."")\n  parser.add_argument(\n      ""--clear_devices"",\n      nargs=""?"",\n      const=True,\n      type=""bool"",\n      default=True,\n      help=""Whether to remove device specifications."")\n  parser.add_argument(\n      ""--initializer_nodes"",\n      type=str,\n      default="""",\n      help=""comma separated list of initializer nodes to run before freezing."")\n  parser.add_argument(\n      ""--variable_names_blacklist"",\n      type=str,\n      default="""",\n      help=""""""\\\n      comma separated list of variables to skip converting to constants\\\n      """""")\n  FLAGS, unparsed = parser.parse_known_args()\n  app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tools/insert_placeholder.py,5,"b'import tensorflow as tf\nfrom nets.mobilenet import mobilenet\nfrom tensorflow.python.training import saver as saver_lib\nfrom tensorflow.python import pywrap_tensorflow\n\ninput_checkpoint = \'/home/zehao/PycharmProjects/MobileNet/mobilenet-model/model.ckpt-439074\'\n\n# Where to save the modified graph\nsave_path = \'/home/zehao/PycharmProjects/MobileNet/mobilenet-model/with_placeholder\'\n\n# TODO(shizehao): use graph editor library insead\nwith tf.Graph().as_default() as graph:\n  input_images = tf.placeholder(tf.float32, [None, 224, 224, 3], \'MobileNet/input_images\')\n  logits, predictions = mobilenet(inputs=input_images, num_classes=1001, is_training=False)\n  saver = tf.train.Saver()\n  with tf.Session() as sess:\n    var_list = {}\n    reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    for key in var_to_shape_map:\n      try:\n        tensor = sess.graph.get_tensor_by_name(key + "":0"")\n      except KeyError:\n        # This tensor doesn\'t exist in the graph (for example it\'s\n        # \'global_step\' or a similar housekeeping element) so skip it.\n        continue\n      var_list[key] = tensor\n    saver = saver_lib.Saver(var_list=var_list)\n\n    # Restore variables\n    saver.restore(sess, input_checkpoint)\n\n    # Save new checkpoint and the graph\n    saver.save(sess, save_path+\'/with_placeholder\')\n    tf.train.write_graph(graph, save_path, \'graph.pbtxt\')\n\n\n'"
tools/inspect_checkpoint.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple script for inspect checkpoint files.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python import pywrap_tensorflow\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nFLAGS = None\n\n\ndef print_tensors_in_checkpoint_file(file_name, tensor_name, all_tensors):\n    """"""Prints tensors in a checkpoint file.\n\n    If no `tensor_name` is provided, prints the tensor names and shapes\n    in the checkpoint file.\n\n    If `tensor_name` is provided, prints the content of the tensor.\n\n    Args:\n        file_name: Name of the checkpoint file.\n        tensor_name: Name of the tensor in the checkpoint file to print.\n        all_tensors: Boolean indicating whether to print all tensors.\n    """"""\n    try:\n        reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n        if all_tensors:\n            var_to_shape_map = reader.get_variable_to_shape_map()\n            for key in var_to_shape_map:\n                print(""tensor_name: "", key)\n                print(reader.get_tensor(key))\n        elif not tensor_name:\n            print(reader.debug_string().decode(""utf-8""))\n        else:\n            print(""tensor_name: "", tensor_name)\n            print(reader.get_tensor(tensor_name))\n    except Exception as e:  # pylint: disable=broad-except\n        print(str(e))\n        if ""corrupted compressed block contents"" in str(e):\n            print(""It\'s likely that your checkpoint file has been compressed ""\n                  ""with SNAPPY."")\n\n\ndef parse_numpy_printoption(kv_str):\n    """"""Sets a single numpy printoption from a string of the form \'x=y\'.\n\n    See documentation on numpy.set_printoptions() for details about what values\n    x and y can take. x can be any option listed there other than \'formatter\'.\n\n    Args:\n        kv_str: A string of the form \'x=y\', such as \'threshold=100000\'\n\n    Raises:\n        argparse.ArgumentTypeError: If the string couldn\'t be used to set any\n                nump printoption.\n    """"""\n    k_v_str = kv_str.split(""="", 1)\n    if len(k_v_str) != 2 or not k_v_str[0]:\n        raise argparse.ArgumentTypeError(""\'%s\' is not in the form k=v."" % kv_str)\n    k, v_str = k_v_str\n    printoptions = np.get_printoptions()\n    if k not in printoptions:\n        raise argparse.ArgumentTypeError(""\'%s\' is not a valid printoption."" % k)\n    v_type = type(printoptions[k])\n    if v_type is type(None):\n        raise argparse.ArgumentTypeError(\n                ""Setting \'%s\' from the command line is not supported."" % k)\n    try:\n        v = (v_type(v_str) if v_type is not bool\n             else flags.BooleanParser().Parse(v_str))\n    except ValueError as e:\n        raise argparse.ArgumentTypeError(e.message)\n    np.set_printoptions(**{k: v})\n\n\ndef main(unused_argv):\n    if not FLAGS.file_name:\n        print(""Usage: inspect_checkpoint --file_name=checkpoint_file_name ""\n              ""[--tensor_name=tensor_to_print]"")\n        sys.exit(1)\n    else:\n        print_tensors_in_checkpoint_file(FLAGS.file_name, FLAGS.tensor_name,\n                                         FLAGS.all_tensors)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.register(""type"", ""bool"", lambda v: v.lower() == ""true"")\n    parser.add_argument(\n            ""--file_name"", type=str, default="""", help=""Checkpoint filename. ""\n                                        ""Note, if using Checkpoint V2 format, file_name is the ""\n                                        ""shared prefix between all files in the checkpoint."")\n    parser.add_argument(\n            ""--tensor_name"",\n            type=str,\n            default="""",\n            help=""Name of the tensor to inspect"")\n    parser.add_argument(\n            ""--all_tensors"",\n            nargs=""?"",\n            const=True,\n            type=""bool"",\n            default=False,\n            help=""If True, print the values of all the tensors."")\n    parser.add_argument(\n            ""--printoptions"",\n            nargs=""*"",\n            type=parse_numpy_printoption,\n            help=""Argument for numpy.set_printoptions(), in the form \'k=v\'."")\n    FLAGS, unparsed = parser.parse_known_args()\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
tools/kitti_random_split_train_val.py,0,"b""import numpy as np\n\nimage_set_dir = './KITTI/ImageSets'\ntrainval_file = image_set_dir+'/trainval.txt'\ntrain_file = image_set_dir+'/train.txt'\nval_file = image_set_dir+'/val.txt'\n\nidx = []\nwith open(trainval_file) as f:\n  for line in f:\n    idx.append(line.strip())\nf.close()\n\nidx = np.random.permutation(idx)\n\ntrain_idx = sorted(idx[:len(idx)/2])\nval_idx = sorted(idx[len(idx)/2:])\n\nwith open(train_file, 'w') as f:\n  for i in train_idx:\n    f.write('{}\\n'.format(i))\nf.close()\n\nwith open(val_file, 'w') as f:\n  for i in val_idx:\n    f.write('{}\\n'.format(i))\nf.close()\n\nprint('Trainining set is saved to ' + train_file)\nprint('Validation set is saved to ' + val_file)"""
tools/quantize_graph.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Transforms a float-trained graph into an equivalent quantized version.\n\nAn example of command-line usage is:\nbazel build tensorflow/tools/quantization:quantize_graph \\\n&& bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n--input=tensorflow_inception_graph.pb\n--output_node_names=""softmax2"" --print_nodes --output=/tmp/quantized_graph.pb \\\n--mode=eightbit --logtostderr\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport numpy as np\n\nfrom tensorflow.core.framework import attr_value_pb2\nfrom tensorflow.core.framework import graph_pb2\nfrom tensorflow.core.framework import node_def_pb2\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags as flags_lib\nfrom tensorflow.python.platform import gfile\n\nflags = flags_lib\nFLAGS = flags.FLAGS\n\nflags.DEFINE_boolean(""print_nodes"", False, """"""Lists all nodes in the model."""""")\nflags.DEFINE_string(""input"", """", """"""TensorFlow \'GraphDef\' file to load."""""")\nflags.DEFINE_string(""output_node_names"", """",\n                    """"""Output node names, comma separated."""""")\nflags.DEFINE_string(""output"", """", """"""File to save the output graph to."""""")\nflags.DEFINE_integer(""bitdepth"", 8,\n                     """"""How many bits to quantize the graph to."""""")\nflags.DEFINE_string(""mode"", ""round"",\n                    """"""What transformation to apply (round, quantize,""""""\n                    """""" eightbit, weights, or weights_rounded)."""""")\nflags.DEFINE_string(""test_input_dims"", ""1,224,224,3"",\n                    """"""The size of the input tensor to use when testing a""""""\n                    """""" graph loaded from a file."""""")\nflags.DEFINE_boolean(""strip_redundant_quantization"", True,\n                     """"""Removes redundant dequantize/quantize pairs."""""")\nflags.DEFINE_boolean(""quantized_input"", False,\n                     ""If true, assume Placeholders are quantized with values ""\n                     ""covering [--quantized_input_min,--quantized_input_max]. ""\n                     ""Only supported when --mode=eightbit"")\nflags.DEFINE_float(""quantized_input_min"", 0,\n                   ""The minimum of the actual input range when ""\n                   ""--quantized_input"")\nflags.DEFINE_float(""quantized_input_max"", 1,\n                   ""The maximum of the actual input range when ""\n                   ""--quantized_input"")\nflags.DEFINE_float(\n    ""quantized_fallback_min"", None,\n    ""The fallback \'min\' value to use for layers which lack min-max ""\n    ""information. Note: this should be considered a coarse tool just good ""\n    ""enough for experimentation purposes, since graphs quantized in this way ""\n    ""would be very inaccurate."")\nflags.DEFINE_float(\n    ""quantized_fallback_max"", None,\n    ""The fallback \'max\' value to use for layers which lack min-max ""\n    ""information. Note: this should be considered a coarse tool just good ""\n    ""enough for experimentation purposes, since graphs quantized in this way ""\n    ""would be very inaccurate."")\n\n\ndef print_input_nodes(current_node, nodes_map, indent, already_visited):\n  print("" "" * indent + current_node.op + "":"" + current_node.name)\n  already_visited[current_node.name] = True\n  for input_node_name in current_node.input:\n    if input_node_name in already_visited:\n      continue\n    input_node = nodes_map[input_node_name]\n    print_input_nodes(input_node, nodes_map, indent + 1, already_visited)\n\n\ndef create_node(op, name, inputs):\n  new_node = node_def_pb2.NodeDef()\n  new_node.op = op\n  new_node.name = name\n  for input_name in inputs:\n    new_node.input.extend([input_name])\n  return new_node\n\n\ndef create_constant_node(name, value, dtype, shape=None):\n  node = create_node(""Const"", name, [])\n  set_attr_dtype(node, ""dtype"", dtype)\n  set_attr_tensor(node, ""value"", value, dtype, shape)\n  return node\n\n\ndef copy_attr(node, key, attr_value):\n  try:\n    node.attr[key].CopyFrom(attr_value)\n  except KeyError:\n    pass\n\n\ndef set_attr_dtype(node, key, value):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(type=value.as_datatype_enum))\n  except KeyError:\n    pass\n\n\ndef set_attr_shape(node, key, value):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(shape=tensor_shape.as_shape(value).as_proto()))\n  except KeyError:\n    pass\n\n\ndef set_attr_tensor(node, key, value, dtype, shape=None):\n  try:\n    node.attr[key].CopyFrom(\n        attr_value_pb2.AttrValue(tensor=tensor_util.make_tensor_proto(\n            value, dtype=dtype, shape=shape)))\n  except KeyError:\n    pass\n\n\ndef set_attr_string(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(s=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_int_list(node, key, value):\n  list_value = attr_value_pb2.AttrValue.ListValue(i=value)\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(list=list_value))\n  except KeyError:\n    pass\n\n\ndef set_attr_bool(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(b=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_int(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(i=value))\n  except KeyError:\n    pass\n\n\ndef set_attr_float(node, key, value):\n  try:\n    node.attr[key].CopyFrom(attr_value_pb2.AttrValue(f=value))\n  except KeyError:\n    pass\n\n\ndef node_name_from_input(node_name):\n  """"""Strips off ports and other decorations to get the underlying node name.""""""\n  if node_name.startswith(""^""):\n    node_name = node_name[1:]\n  m = re.search(r""(.*):\\d+$"", node_name)\n  if m:\n    node_name = m.group(1)\n  return node_name\n\n\ndef ensure_tensor_name_has_port(node_name):\n  """"""Makes sure that a tensor name has :0 if no explicit port exists.""""""\n  m = re.search(r""(.*):\\d+$"", node_name)\n  if m:\n    name_with_port = node_name\n  else:\n    name_with_port = node_name + "":0""\n  return name_with_port\n\n\ndef unique_node_name_from_input(node_name):\n  """"""Replaces invalid characters in input names to get a unique node name.""""""\n  return node_name.replace("":"", ""__port__"").replace(""^"", ""__hat__"")\n\n\ndef quantize_array(arr, num_buckets):\n  """"""Quantizes a numpy array.\n\n  This function maps each scalar in arr to the center of one of num_buckets\n  buckets. For instance,\n  quantize_array([0, 0.3, 0.6, 1], 2) => [0.25, 0.25, 0.75, 0.75]\n\n  Args:\n    arr: The numpy array to quantize.\n    num_buckets: The number of buckets to map ""var"" to.\n  Returns:\n    The quantized numpy array.\n  Raises:\n    ValueError: when num_buckets < 1.\n  """"""\n  if num_buckets < 1:\n    raise ValueError(""num_buckets must be >= 1"")\n  arr_max = arr.max()\n  arr_min = arr.min()\n  if arr_max == arr_min:\n    return arr\n  bucket_width = (arr_max - arr_min) / num_buckets\n  # Map scalars to bucket indices. Take special care of max(arr).\n  bucket_indices = np.floor((arr - arr_min) / bucket_width)\n  bucket_indices[bucket_indices == num_buckets] = num_buckets - 1\n  # Map each scalar to the center of a bucket.\n  arr = arr_min + bucket_width * (bucket_indices + 0.5)\n  return arr\n\n\ndef quantize_weight_rounded(input_node):\n  """"""Returns a replacement node for input_node containing bucketed floats.""""""\n  input_tensor = input_node.attr[""value""].tensor\n  tensor_value = tensor_util.MakeNdarray(input_tensor)\n  shape = input_tensor.tensor_shape\n  # Currently, the parameter FLAGS.bitdepth is used to compute the\n  # number of buckets as 1 << FLAGS.bitdepth, meaning the number of\n  # buckets can only be a power of 2.\n  # This could be fixed by introducing a new parameter, num_buckets,\n  # which would allow for more flexibility in chosing the right model\n  # size/accuracy tradeoff. But I didn\'t want to add more parameters\n  # to this script than absolutely necessary.\n  num_buckets = 1 << FLAGS.bitdepth\n  tensor_value_rounded = quantize_array(tensor_value, num_buckets)\n  tensor_shape_list = tensor_util.TensorShapeProtoToList(shape)\n  return [\n      create_constant_node(\n          input_node.name,\n          tensor_value_rounded,\n          dtypes.float32,\n          shape=tensor_shape_list)\n  ]\n\n\ndef quantize_weight_eightbit(input_node, quantization_mode):\n  """"""Returns replacement nodes for input_node using the Dequantize op.""""""\n  base_name = input_node.name + ""_""\n  quint8_const_name = base_name + ""quint8_const""\n  min_name = base_name + ""min""\n  max_name = base_name + ""max""\n  float_tensor = tensor_util.MakeNdarray(input_node.attr[""value""].tensor)\n  min_value = np.min(float_tensor.flatten())\n  max_value = np.max(float_tensor.flatten())\n  # Make sure that the range includes zero.\n  if min_value > 0.0:\n    min_value = 0.0\n  # min_value == max_value is a tricky case. It can occur for general\n  # tensors, and of course for scalars. The quantized ops cannot deal\n  # with this case, so we set max_value to something else.\n  # It\'s a tricky question what is the numerically best solution to\n  # deal with this degeneracy.\n  # TODO(petewarden): Better use a tolerance than a hard comparison?\n  if min_value == max_value:\n    if abs(min_value) < 0.000001:\n      max_value = min_value + 1.0\n    elif min_value > 0:\n      max_value = 2 * min_value\n    else:\n      max_value = min_value / 2.0\n\n  sess = session.Session()\n  with sess.as_default():\n    quantize_op = array_ops.quantize_v2(\n        float_tensor,\n        min_value,\n        max_value,\n        dtypes.quint8,\n        mode=quantization_mode)\n    quint8_tensor = quantize_op[0].eval()\n  shape = tensor_util.TensorShapeProtoToList(input_node.attr[""value""]\n                                             .tensor.tensor_shape)\n  quint8_const_node = create_constant_node(\n      quint8_const_name, quint8_tensor, dtypes.quint8, shape=shape)\n  min_node = create_constant_node(min_name, min_value, dtypes.float32)\n  max_node = create_constant_node(max_name, max_value, dtypes.float32)\n  dequantize_node = create_node(""Dequantize"", input_node.name,\n                                [quint8_const_name, min_name, max_name])\n  set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n  set_attr_string(dequantize_node, ""mode"", quantization_mode)\n  return [quint8_const_node, min_node, max_node, dequantize_node]\n\n\nEightbitizeRecursionState = collections.namedtuple(\n    ""EightbitizeRecursionState"",\n    [""already_visited"", ""output_node_stack"", ""merged_with_fake_quant""])\n\n\nclass GraphRewriter(object):\n  """"""Takes a float graph, and rewrites it in quantized form.""""""\n\n  def __init__(self,\n               input_graph,\n               mode,\n               quantized_input_range,\n               fallback_quantization_range=None):\n    """"""Sets up the class to rewrite a float graph.\n\n    Args:\n      input_graph: A float graph to transform.\n      mode: A string controlling how quantization is performed -\n        round, quantize, eightbit, or weights.\n      quantized_input_range: if set, assume the input is\n        quantized and represents the range\n        [quantized_input_range[0], quantized_input_range[1]]\n      fallback_quantization_range: if set, then for nodes where the quantization\n        range can\'t be inferred from the graph, use the range\n        [fallback_quantization_range[0], fallback_quantization_range[1]) instead\n        of using a RequantizationRange node in the graph.\n\n    Raises:\n      ValueError: Two nodes with the same name were found in the graph.\n    """"""\n    self.input_graph = input_graph\n    self.nodes_map = self.create_nodes_map(input_graph)\n    self.output_graph = None\n    self.mode = mode\n    self.final_node_renames = {}\n    if quantized_input_range:\n      self.input_range = (quantized_input_range[0], quantized_input_range[1])\n      if self.input_range[0] >= self.input_range[1]:\n        raise ValueError(""Invalid quantized_input_range: [%s,%s]"" %\n                         self.input_range)\n      if self.mode != ""eightbit"":\n        raise ValueError(\n            ""quantized_input_range can only be specified in eightbit mode"")\n    else:\n      self.input_range = None\n\n    if fallback_quantization_range:\n      self.fallback_quantization_range = [\n          fallback_quantization_range[0], fallback_quantization_range[1]\n      ]\n      if (self.fallback_quantization_range[0] >=\n          self.fallback_quantization_range[1]):\n        raise ValueError(""Invalid fallback_quantization_range: [%s,%s]"" %\n                         self.fallback_quantization_range)\n      if self.mode != ""eightbit"":\n        raise ValueError(""fallback_quantization_range can only be ""\n                         ""specified in eightbit mode"")\n    else:\n      self.fallback_quantization_range = None\n\n    # Data that is valid only during the recursive call to rewrite the graph.\n    self.state = None\n\n  def create_nodes_map(self, graph):\n    """"""Builds a mapping of node names to their defs from the graph.""""""\n    nodes_map = {}\n    for node in graph.node:\n      if node.name not in nodes_map.keys():\n        nodes_map[node.name] = node\n      else:\n        raise ValueError(""Duplicate node names detected."")\n    return nodes_map\n\n  def rewrite(self, output_node_names):\n    """"""Triggers rewriting of the float graph.\n\n    Args:\n      output_node_names: A list of names of the nodes that produce the final\n        results.\n\n    Returns:\n      A quantized version of the float graph.\n    """"""\n    self.output_graph = graph_pb2.GraphDef()\n    output_nodes = [\n        self.nodes_map[output_node_name]\n        for output_node_name in output_node_names\n    ]\n    if self.mode == ""round"":\n      self.already_visited = {}\n      for output_node in output_nodes:\n        self.round_nodes_recursively(output_node)\n    elif self.mode == ""quantize"":\n      self.already_visited = {}\n      self.already_quantized = {}\n      for output_node in output_nodes:\n        self.quantize_nodes_recursively(output_node)\n    elif self.mode == ""eightbit"":\n      self.set_input_graph(graph_util.remove_training_nodes(self.input_graph))\n      output_nodes = [\n          self.nodes_map[output_node_name]\n          for output_node_name in output_node_names\n      ]\n\n      self.state = EightbitizeRecursionState(\n          already_visited={}, output_node_stack=[], merged_with_fake_quant={})\n      for output_node in output_nodes:\n        self.eightbitize_nodes_recursively(output_node)\n      self.state = None\n      if self.input_range:\n        self.add_output_graph_node(\n            create_constant_node(""quantized_input_min_value"", self.input_range[\n                0], dtypes.float32, []))\n        self.add_output_graph_node(\n            create_constant_node(""quantized_input_max_value"", self.input_range[\n                1], dtypes.float32, []))\n      if self.fallback_quantization_range:\n        self.add_output_graph_node(\n            create_constant_node(""fallback_quantization_min_value"",\n                                 self.fallback_quantization_range[0],\n                                 dtypes.float32, []))\n        self.add_output_graph_node(\n            create_constant_node(""fallback_quantization_max_value"",\n                                 self.fallback_quantization_range[1],\n                                 dtypes.float32, []))\n      if FLAGS.strip_redundant_quantization:\n        self.output_graph = self.remove_redundant_quantization(\n            self.output_graph)\n        self.remove_dead_nodes(output_node_names)\n      self.apply_final_node_renames()\n    elif self.mode == ""weights"":\n      self.output_graph = self.quantize_weights(self.input_graph,\n                                                b""MIN_COMBINED"")\n      self.remove_dead_nodes(output_node_names)\n    elif self.mode == ""weights_rounded"":\n      self.output_graph = self.quantize_weights(self.input_graph, self.mode)\n      self.remove_dead_nodes(output_node_names)\n    else:\n      print(""Bad mode - "" + self.mode + ""."")\n    return self.output_graph\n\n  def round_nodes_recursively(self, current_node):\n    """"""The entry point for simple rounding quantization.""""""\n    if self.already_visited[current_node.name]:\n      return\n    self.already_visited[current_node.name] = True\n    for input_node_name in current_node.input:\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.round_nodes_recursively(input_node)\n    nodes_to_quantize = [""Conv2D"", ""BiasAdd"", ""MatMul""]\n    if any(current_node.op in s for s in nodes_to_quantize):\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      new_node.name = current_node.name + ""_original""\n      self.add_output_graph_node(new_node)\n      levels = 1 << FLAGS.bitdepth\n      constant_name = current_node.name + ""_round_depth""\n      constant_tensor = constant_op.constant(\n          levels, dtype=dtypes.int32, name=constant_name)\n      constant_node = constant_tensor.op.node_def\n      self.add_output_graph_node(constant_node)\n      quantize_node = node_def_pb2.NodeDef()\n      quantize_node.op = ""RoundToSteps""\n      quantize_node.name = current_node.name\n      quantize_node.input.extend([current_node.name + ""_original""])\n      quantize_node.input.extend([constant_node.name])\n      self.add_output_graph_node(quantize_node)\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n  def quantize_nodes_recursively(self, current_node):\n    """"""The entry point for quantizing nodes to eight bit and back.""""""\n    if self.already_visited[current_node.name]:\n      return\n    self.already_visited[current_node.name] = True\n    for input_node_name in current_node.input:\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.quantize_nodes_recursively(input_node)\n    nodes_to_quantize = [""Conv2D"", ""BiasAdd"", ""MatMul""]\n    if any(current_node.op in s for s in nodes_to_quantize):\n      for input_name in current_node.input:\n        input_name = node_name_from_input(input_name)\n        input_node = self.nodes_map[input_name]\n        self.quantize_node(input_node)\n      self.quantize_node(current_node)\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n  def quantize_node(self, input_node):\n    """"""Handles quantizing a single node.""""""\n    input_name = input_node.name\n    if input_name in self.already_quantized:\n      return\n    self.already_quantized[input_name] = True\n    original_input_name = input_name + ""_original""\n    reshape_name = input_name + ""_reshape""\n    reshape_dims_name = input_name + ""_reshape_dims""\n    max_name = input_name + ""_max""\n    min_name = input_name + ""_min""\n    dims_name = input_name + ""_dims""\n    quantize_name = input_name + ""_quantize""\n    dequantize_name = input_name\n    original_input_node = node_def_pb2.NodeDef()\n    original_input_node.CopyFrom(input_node)\n    original_input_node.name = original_input_name\n    self.add_output_graph_node(original_input_node)\n    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                             dtypes.int32, [1])\n    self.add_output_graph_node(reshape_dims_node)\n    reshape_node = create_node(""Reshape"", reshape_name,\n                               [original_input_name, reshape_dims_name])\n    set_attr_dtype(reshape_node, ""T"", dtypes.float32)\n    self.add_output_graph_node(reshape_node)\n    dims_node = create_constant_node(dims_name, 0, dtypes.int32, [1])\n    self.add_output_graph_node(dims_node)\n    max_node = create_node(""Max"", max_name, [reshape_name, dims_name])\n    set_attr_dtype(max_node, ""T"", dtypes.float32)\n    set_attr_bool(max_node, ""keep_dims"", False)\n    self.add_output_graph_node(max_node)\n    min_node = create_node(""Min"", min_name, [reshape_name, dims_name])\n    set_attr_dtype(min_node, ""T"", dtypes.float32)\n    set_attr_bool(min_node, ""keep_dims"", False)\n    self.add_output_graph_node(min_node)\n    quantize_node = create_node(""Quantize"", quantize_name,\n                                [original_input_name, min_name, max_name])\n    set_attr_dtype(quantize_node, ""T"", dtypes.quint8)\n    set_attr_string(quantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(quantize_node)\n    dequantize_node = create_node(""Dequantize"", dequantize_name,\n                                  [quantize_name, min_name, max_name])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n  def should_merge_with_fake_quant_node(self):\n    """"""Should the current node merge with self.state.output_node_stack[-1]?""""""\n    if not self.state.output_node_stack:\n      return False\n    top = self.state.output_node_stack[-1]\n    return top[1] == 0 and top[0].op in [""FakeQuantWithMinMaxVars""]\n\n  def should_quantize_const(self, node):\n    if not self.state.output_node_stack:\n      return False\n    top = self.state.output_node_stack[-1]\n    if not top[2]:\n      return False\n    dtype = dtypes.as_dtype(node.attr[""dtype""].type)\n    assert dtype == dtypes.float32, (\n        ""Failed to quantized constant %s of type %s"" % (node.name, dtype))\n    return True\n\n  def eightbitize_nodes_recursively(self, current_node):\n    """"""The entry point for transforming a graph into full eight bit.""""""\n    if current_node.name in self.state.already_visited:\n      if (self.should_merge_with_fake_quant_node() or\n          current_node.name in self.state.merged_with_fake_quant):\n        raise ValueError(""Unsupported graph structure: output of node %s ""\n                         ""is processed by a FakeQuant* node and should have ""\n                         ""no other outputs."", current_node.name)\n      return\n    self.state.already_visited[current_node.name] = True\n\n    for i, input_node_name in enumerate(current_node.input):\n      quantize_input = False\n      if current_node.op in (""MatMul"", ""Conv2D"", ""BiasAdd"", ""MaxPool"",\n                             ""AvgPool"", ""Relu"", ""Relu6"",\n                             ""BatchNormWithGlobalNormalization""):\n        quantize_input = True\n      elif current_node.op == ""Concat"" and i > 0:\n        quantize_input = (\n            dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32)\n      elif current_node.op == ""Reshape"" and i == 0:\n        quantize_input = (\n            dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32)\n\n      self.state.output_node_stack.append((current_node, i, quantize_input))\n\n      input_node_name = node_name_from_input(input_node_name)\n      input_node = self.nodes_map[input_node_name]\n      self.eightbitize_nodes_recursively(input_node)\n\n      self.state.output_node_stack.pop()\n\n    if current_node.op == ""MatMul"":\n      self.eightbitize_mat_mul_node(current_node)\n    elif current_node.op == ""Conv2D"":\n      self.eightbitize_conv_node(current_node)\n    elif current_node.op == ""BiasAdd"":\n      self.eightbitize_bias_add_node(current_node)\n    elif current_node.op == ""MaxPool"" or current_node.op == ""AvgPool"":\n      self.eightbitize_single_input_tensor_node(current_node,\n                                                self.add_pool_function)\n    elif current_node.op == ""Relu"" or current_node.op == ""Relu6"":\n      self.eightbitize_single_input_tensor_node(current_node,\n                                                self.add_relu_function)\n    elif (current_node.op == ""Concat"" and\n          dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32):\n      self.eightbitize_concat_node(current_node)\n    elif current_node.op == ""BatchNormWithGlobalNormalization"":\n      self.eightbitize_batch_norm_node(current_node)\n    elif (current_node.op == ""Reshape"" and\n          dtypes.as_dtype(current_node.attr[""T""].type) == dtypes.float32):\n      self.eightbitize_reshape_node(current_node)\n    elif (self.input_range and\n          current_node.op in (""Placeholder"", ""PlaceholderV2"")):\n      self.eightbitize_placeholder_node(current_node)\n    elif current_node.op == ""FakeQuantWithMinMaxVars"":\n      # It will have been merged into the underlying node.\n      pass\n    elif current_node.op == ""Const"":\n      if self.should_quantize_const(current_node):\n        for n in quantize_weight_eightbit(current_node, b""MIN_FIRST""):\n          self.add_output_graph_node(n)\n      else:\n        new_node = node_def_pb2.NodeDef()\n        new_node.CopyFrom(current_node)\n        self.add_output_graph_node(new_node)\n\n    ###################################################################\n    # Note: if more cases are added here, you may need to update the op\n    # name lists in the loop over children at the start of the function.\n    ###################################################################\n    else:\n      new_node = node_def_pb2.NodeDef()\n      new_node.CopyFrom(current_node)\n      self.add_output_graph_node(new_node)\n\n    if (self.should_merge_with_fake_quant_node() and\n        current_node.name not in self.state.merged_with_fake_quant):\n      raise ValueError(\n          ""FakeQuant* node %s failed to merge with node %s of type %s"" %\n          (self.state.output_node_stack[-1][0], current_node.name,\n           current_node.op))\n\n  def add_eightbit_prologue_nodes(self, original_node):\n    """"""Adds input conversion nodes to handle quantizing the underlying node.""""""\n    namespace_prefix = original_node.name + ""_eightbit""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    input_names = []\n    min_max_names = []\n    for original_input_name in original_node.input:\n      quantize_input_name, min_input_name, max_input_name = (\n          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                         reshape_dims_name,\n                                         reduction_dims_name))\n      input_names.append(quantize_input_name)\n      min_max_names.append(min_input_name)\n      min_max_names.append(max_input_name)\n    all_input_names = []\n    all_input_names.extend(input_names)\n    all_input_names.extend(min_max_names)\n    return all_input_names\n\n  def add_common_quantization_nodes(self, namespace_prefix):\n    """"""Builds constant nodes needed for quantization of inputs.""""""\n    reshape_dims_name = namespace_prefix + ""_reshape_dims""\n    reduction_dims_name = namespace_prefix + ""_reduction_dims""\n\n    reshape_dims_node = create_constant_node(reshape_dims_name, -1,\n                                             dtypes.int32, [1])\n    self.add_output_graph_node(reshape_dims_node)\n    reduction_dims_node = create_constant_node(reduction_dims_name, 0,\n                                               dtypes.int32, [1])\n    self.add_output_graph_node(reduction_dims_node)\n    return reshape_dims_name, reduction_dims_name\n\n  def eightbitize_input_to_node(self, namespace_prefix, original_input_name,\n                                reshape_dims_name, reduction_dims_name):\n    """"""Takes one float input to an op, and converts it to quantized form.""""""\n    unique_input_name = unique_node_name_from_input(original_input_name)\n    reshape_input_name = namespace_prefix + ""_reshape_"" + unique_input_name\n    min_input_name = namespace_prefix + ""_min_"" + unique_input_name\n    max_input_name = namespace_prefix + ""_max_"" + unique_input_name\n    quantize_input_name = namespace_prefix + ""_quantize_"" + unique_input_name\n    reshape_input_node = create_node(""Reshape"", reshape_input_name,\n                                     [original_input_name, reshape_dims_name])\n    set_attr_dtype(reshape_input_node, ""T"", dtypes.float32)\n    self.add_output_graph_node(reshape_input_node)\n    min_input_node = create_node(""Min"", min_input_name,\n                                 [reshape_input_name, reduction_dims_name])\n    set_attr_dtype(min_input_node, ""T"", dtypes.float32)\n    set_attr_bool(min_input_node, ""keep_dims"", False)\n    self.add_output_graph_node(min_input_node)\n    max_input_node = create_node(""Max"", max_input_name,\n                                 [reshape_input_name, reduction_dims_name])\n    set_attr_dtype(max_input_node, ""T"", dtypes.float32)\n    set_attr_bool(max_input_node, ""keep_dims"", False)\n    self.add_output_graph_node(max_input_node)\n    quantize_input_node = create_node(\n        ""QuantizeV2"", quantize_input_name,\n        [original_input_name, min_input_name, max_input_name])\n    set_attr_dtype(quantize_input_node, ""T"", dtypes.quint8)\n    set_attr_string(quantize_input_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(quantize_input_node)\n    min_output_name = quantize_input_name + "":1""\n    max_output_name = quantize_input_name + "":2""\n    return quantize_input_name, min_output_name, max_output_name\n\n  def add_quantize_down_nodes(self, original_node, quantized_output_name):\n    quantized_outputs = [\n        quantized_output_name, quantized_output_name + "":1"",\n        quantized_output_name + "":2""\n    ]\n    min_max_inputs = None\n    if self.should_merge_with_fake_quant_node():\n      # Use the inputs to the FakeQuantWithMinMaxVars node as the inputs to\n      # Requantize.\n      fake_quant_node = self.state.output_node_stack[-1][0]\n      min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n      assert original_node.name not in self.state.merged_with_fake_quant\n      self.state.merged_with_fake_quant[original_node.name] = True\n    elif self.fallback_quantization_range:\n      min_max_inputs = [\n          ""fallback_quantization_min_value:0"",\n          ""fallback_quantization_max_value:0""\n      ]\n    else:\n      # Add a RequantizationRange node for finding the min and max values.\n      requant_range_node = create_node(\n          ""RequantizationRange"", original_node.name + ""_eightbit_requant_range"",\n          quantized_outputs)\n      set_attr_dtype(requant_range_node, ""Tinput"", dtypes.qint32)\n      self.add_output_graph_node(requant_range_node)\n      min_max_inputs = [\n          requant_range_node.name + "":0"", requant_range_node.name + "":1""\n      ]\n    requantize_node = create_node(""Requantize"",\n                                  original_node.name + ""_eightbit_requantize"",\n                                  quantized_outputs + min_max_inputs)\n    set_attr_dtype(requantize_node, ""Tinput"", dtypes.qint32)\n    set_attr_dtype(requantize_node, ""out_type"", dtypes.quint8)\n    self.add_output_graph_node(requantize_node)\n    return requantize_node.name\n\n  def add_dequantize_result_node(self,\n                                 quantized_output_name,\n                                 original_node_name,\n                                 min_tensor_index=1):\n    min_max_inputs = [\n        ""%s:%s"" % (quantized_output_name, min_tensor_index),\n        ""%s:%s"" % (quantized_output_name, (min_tensor_index + 1))\n    ]\n    dequantize_name = original_node_name\n    if self.should_merge_with_fake_quant_node():\n      fake_quant_node = self.state.output_node_stack[-1][0]\n      if original_node_name not in self.state.merged_with_fake_quant:\n        min_max_inputs = [fake_quant_node.input[1], fake_quant_node.input[2]]\n        self.state.merged_with_fake_quant[original_node_name] = True\n      dequantize_name = fake_quant_node.name\n\n    dequantize_node = create_node(\n        ""Dequantize"", dequantize_name,\n        [quantized_output_name, min_max_inputs[0], min_max_inputs[1]])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n  def eightbitize_mat_mul_node(self, original_node):\n    """"""Replaces a MatMul node with the eight bit equivalent sub-graph.""""""\n    quantized_mat_mul_name = original_node.name + ""_eightbit_quantized_mat_mul""\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_mat_mul_node = create_node(""QuantizedMatMul"",\n                                         quantized_mat_mul_name,\n                                         all_input_names)\n    set_attr_dtype(quantized_mat_mul_node, ""T1"", dtypes.quint8)\n    set_attr_dtype(quantized_mat_mul_node, ""T2"", dtypes.quint8)\n    set_attr_dtype(quantized_mat_mul_node, ""Toutput"", dtypes.qint32)\n    copy_attr(quantized_mat_mul_node, ""transpose_a"",\n              original_node.attr[""transpose_a""])\n    copy_attr(quantized_mat_mul_node, ""transpose_b"",\n              original_node.attr[""transpose_b""])\n    self.add_output_graph_node(quantized_mat_mul_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_mat_mul_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_conv_node(self, original_node):\n    """"""Replaces a Conv2D node with the eight bit equivalent sub-graph.""""""\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_conv_name = original_node.name + ""_eightbit_quantized_conv""\n    quantized_conv_node = create_node(""QuantizedConv2D"", quantized_conv_name,\n                                      all_input_names)\n    copy_attr(quantized_conv_node, ""strides"", original_node.attr[""strides""])\n    copy_attr(quantized_conv_node, ""padding"", original_node.attr[""padding""])\n    set_attr_dtype(quantized_conv_node, ""Tinput"", dtypes.quint8)\n    set_attr_dtype(quantized_conv_node, ""Tfilter"", dtypes.quint8)\n    set_attr_dtype(quantized_conv_node, ""out_type"", dtypes.qint32)\n    self.add_output_graph_node(quantized_conv_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_conv_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_bias_add_node(self, original_node):\n    """"""Replaces a BiasAdd node with the eight bit equivalent sub-graph.""""""\n    quantized_bias_add_name = (\n        original_node.name + ""_eightbit_quantized_bias_add"")\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_bias_add_node = create_node(""QuantizedBiasAdd"",\n                                          quantized_bias_add_name,\n                                          all_input_names)\n    set_attr_dtype(quantized_bias_add_node, ""T1"", dtypes.quint8)\n    set_attr_dtype(quantized_bias_add_node, ""T2"", dtypes.quint8)\n    set_attr_dtype(quantized_bias_add_node, ""out_type"", dtypes.qint32)\n    self.add_output_graph_node(quantized_bias_add_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_bias_add_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def eightbitize_single_input_tensor_node(self, original_node,\n                                           add_op_function):\n    """"""Replaces a single-tensor node with the eight bit equivalent sub-graph.\n\n    Converts a node like this:\n\n       Shape(f)   Input(f)\n         |          |\n         +--------v v\n                Operation\n                    |\n                    v\n                   (f)\n\n     Into a quantized equivalent:\n\n                    Input(f)              ReshapeDims\n                       +------v v-------------+\n                       |    Reshape\n                       |      |\n                       |      |          ReductionDims\n                       |      +-----+         |\n                       |      | +---c---------+\n                       |      v v   v v-------+\n                       |      Min   Max\n                       |  +----+      |\n                       v  v  v--------+\n                      Quantize\n                          |\n                          v\n                   QuantizedOperation\n                      |   |   |\n                      v   v   v\n                      Dequantize\n                          |\n                          v\n                         (f)\n\n\n    Args:\n      original_node: Float node to be converted.\n      add_op_function: Function to create the actual node.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    quantized_op_name = original_node.name + ""_eightbit_quantized""\n    quantized_op_type = ""Quantized"" + original_node.op\n    all_input_names = self.add_eightbit_prologue_nodes(original_node)\n    quantized_op_node = create_node(quantized_op_type, quantized_op_name,\n                                    all_input_names)\n    add_op_function(original_node, quantized_op_node)\n    self.add_output_graph_node(quantized_op_node)\n    self.add_dequantize_result_node(quantized_op_name, original_node.name)\n\n  def add_pool_function(self, original_node, quantized_op_node):\n    set_attr_dtype(quantized_op_node, ""T"", dtypes.quint8)\n    copy_attr(quantized_op_node, ""ksize"", original_node.attr[""ksize""])\n    copy_attr(quantized_op_node, ""strides"", original_node.attr[""strides""])\n    copy_attr(quantized_op_node, ""padding"", original_node.attr[""padding""])\n\n  def add_relu_function(self, unused_arg_node, quantized_op_node):\n    set_attr_dtype(quantized_op_node, ""Tinput"", dtypes.quint8)\n\n  def eightbitize_concat_node(self, original_node):\n    """"""Replaces a Concat node with the eight bit equivalent sub-graph.\n\n    Converts a node like this:\n\n       Shape(f)   Input0(f)   Input1(f)\n         |          |            |\n         +--------v v v----------+\n                  Concat\n                    |\n                    v\n                   (f)\n\n     Into a quantized equivalent:\n\n       Shape(f)     Input0(f)             ReshapeDims                  Input1(f)\n         |             +------v v--------------+------------------v v------+\n         |             |    Reshape                             Reshape    |\n         |             |      |                                     |      |\n         |             |      |           ReductionDims             |      |\n         |             |      +------+         |           +--------+      |\n         |             |      |  +---c---------+-----------c-----+  |      |\n         |             |      +v v   v v-------+---------v v     v v+      |\n         |             |       Min   Max                 Min     Max       |\n         |             |  +----+      |                   |       +-----+  |\n         |             v  v  v--------+                   +----------v  v  v\n         |            Quantize                                       Quantize\n         |                +------------------+   +----------------------+\n         +-------------------------------+   |   |\n                                         v   v   v\n                                      QuantizedConcat\n                                         |   |   |\n                                         v   v   v\n                                        Dequantize\n                                             |\n                                             v\n                                            (f)\n    Args:\n      original_node: Float node to be converted.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    namespace_prefix = original_node.name + ""_eightbit""\n    quantized_concat_name = namespace_prefix + ""_quantized_concat""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    shape_input_name = original_node.input[0]\n    original_inputs = original_node.input[1:]\n    input_names = []\n    min_names = []\n    max_names = []\n    for original_input_name in original_inputs:\n      quantize_input_name, min_input_name, max_input_name = (\n          self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                         reshape_dims_name,\n                                         reduction_dims_name))\n      input_names.append(quantize_input_name)\n      min_names.append(min_input_name)\n      max_names.append(max_input_name)\n    all_input_names = [shape_input_name]\n    all_input_names.extend(input_names)\n    all_input_names.extend(min_names)\n    all_input_names.extend(max_names)\n    quantized_concat_node = create_node(""QuantizedConcat"",\n                                        quantized_concat_name, all_input_names)\n    set_attr_int(quantized_concat_node, ""N"", len(original_inputs))\n    set_attr_dtype(quantized_concat_node, ""T"", dtypes.quint8)\n    self.add_output_graph_node(quantized_concat_node)\n    self.add_dequantize_result_node(quantized_concat_name, original_node.name)\n\n  def eightbitize_placeholder_node(self, current_node):\n    """"""Replaces a placeholder node with a quint8 placeholder node+dequantize.""""""\n    name = current_node.name\n\n    # Convert the placeholder into a quantized type.\n    output_node = node_def_pb2.NodeDef()\n    output_node.CopyFrom(current_node)\n    set_attr_dtype(output_node, ""dtype"", dtypes.quint8)\n    output_node.name += ""_original_input""\n    self.add_output_graph_node(output_node)\n\n    # Add a dequantize to convert back to float.\n    dequantize_node = create_node(""Dequantize"", name, [\n        output_node.name, ""quantized_input_min_value"",\n        ""quantized_input_max_value""\n    ])\n    set_attr_dtype(dequantize_node, ""T"", dtypes.quint8)\n    set_attr_string(dequantize_node, ""mode"", b""MIN_FIRST"")\n    self.add_output_graph_node(dequantize_node)\n\n    # For the descent over the graph to work, the dequantize node must be named\n    # current_node.name.  However, for the feeding of the graph to work, the\n    # placeholder must have the name current_node.name; so record a final set\n    # of renames to apply after all processing has been done.\n    self.final_node_renames[output_node.name] = name\n    self.final_node_renames[dequantize_node.name] = name + ""_dequantize""\n\n  def eightbitize_reshape_node(self, original_node):\n    """"""Replaces a Reshape node with the eight bit equivalent sub-graph.\n\n    Args:\n      original_node: Float node to be converted.\n\n    Returns:\n      Subgraph representing the quantized version of the original node.\n\n    """"""\n    namespace_prefix = original_node.name + ""_eightbit""\n    quantized_reshape_name = namespace_prefix + ""_quantized_reshape""\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    shape_input_name = original_node.input[1]\n    quantize_input_name, min_input_name, max_input_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_node.input[0],\n                                       reshape_dims_name, reduction_dims_name))\n    quantized_reshape_node = create_node(\n        ""QuantizedReshape"", quantized_reshape_name,\n        [quantize_input_name, shape_input_name, min_input_name, max_input_name])\n    set_attr_dtype(quantized_reshape_node, ""T"", dtypes.quint8)\n    self.add_output_graph_node(quantized_reshape_node)\n    self.add_dequantize_result_node(quantized_reshape_name, original_node.name)\n\n  def eightbitize_batch_norm_node(self, original_node):\n    """"""Replaces a MatMul node with the eight bit equivalent sub-graph.""""""\n    namespace_prefix = original_node.name + ""_eightbit""\n    original_input_name = original_node.input[0]\n    original_mean_name = original_node.input[1]\n    original_variance_name = original_node.input[2]\n    original_beta_name = original_node.input[3]\n    original_gamma_name = original_node.input[4]\n    quantized_batch_norm_name = namespace_prefix + ""_quantized_batch_norm""\n\n    reshape_dims_name, reduction_dims_name = self.add_common_quantization_nodes(\n        namespace_prefix)\n    quantize_input_name, min_input_name, max_input_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_input_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_mean_name, min_mean_name, max_mean_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_mean_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_variance_name, min_variance_name, max_variance_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_variance_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_beta_name, min_beta_name, max_beta_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_beta_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantize_gamma_name, min_gamma_name, max_gamma_name = (\n        self.eightbitize_input_to_node(namespace_prefix, original_gamma_name,\n                                       reshape_dims_name, reduction_dims_name))\n    quantized_batch_norm_node = create_node(\n        ""QuantizedBatchNormWithGlobalNormalization"", quantized_batch_norm_name,\n        [\n            quantize_input_name, min_input_name, max_input_name,\n            quantize_mean_name, min_mean_name, max_mean_name,\n            quantize_variance_name, min_variance_name, max_variance_name,\n            quantize_beta_name, min_beta_name, max_beta_name,\n            quantize_gamma_name, min_gamma_name, max_gamma_name\n        ])\n    set_attr_dtype(quantized_batch_norm_node, ""Tinput"", dtypes.quint8)\n    set_attr_dtype(quantized_batch_norm_node, ""out_type"", dtypes.qint32)\n    copy_attr(quantized_batch_norm_node, ""scale_after_normalization"",\n              original_node.attr[""scale_after_normalization""])\n    copy_attr(quantized_batch_norm_node, ""variance_epsilon"",\n              original_node.attr[""variance_epsilon""])\n    self.add_output_graph_node(quantized_batch_norm_node)\n    quantize_down_name = self.add_quantize_down_nodes(original_node,\n                                                      quantized_batch_norm_name)\n    self.add_dequantize_result_node(quantize_down_name, original_node.name)\n\n  def add_output_graph_node(self, output_node):\n    """"""Inserts one node into the new graph.""""""\n    self.output_graph.node.extend([output_node])\n\n  def remove_redundant_quantization(self, old_graph):\n    """"""Removes unneeded pairs of quantize/dequantize ops from the graph.\n\n    This is a bit of a tricky function, because it\'s attempting to spot the\n    pattern of dequantizing from eight-bit up to float, and then immediately\n    quantizing back down to eight bits again, that\'s introduced by previous\n    passes that do \'key-hole\' conversions of individual nodes but have to\n    convert back to float to match the previous output interface, since they\n    don\'t know that the next op can handle quantized tensors.\n    It works by:\n     - Looking for Quantize nodes.\n     - Checking to see if their first input is a Dequantize node.\n     - Seeing if their min/max inputs come from Min/Max nodes.\n     - Making sure those Min/Max nodes are being fed from the same Dequantize.\n     - Or that the Min is indirectly being fed from the same Dequantize as Max.\n     - Making sure the Dequantize is going through a Reshape (which we add\n       during the previous pass when we create the quantize sub-graph).\n     - Looking for the dims Const op for the Min/Max dims.\n    If all of these conditions are met, then it\'s a sub-graph pattern that\n    we know how to optimize out (and is likely the common one we\'ve introduced).\n    We then rewire the graph to skip it entirely, and then rely on the dead node\n    removal pass to get rid of any nodes that are no longer needed.\n\n    Args:\n      old_graph: The model we\'ll be stripping redundant nodes from.\n\n    Returns:\n      A graph with the unnecessary nodes removed.\n\n    Raises:\n      ValueError: Two nodes with the same name were found in the graph.\n    """"""\n    old_nodes_map = self.create_nodes_map(old_graph)\n    self.output_graph = graph_pb2.GraphDef()\n    inputs_to_rename = {}\n    # We go through all the nodes, looking for any that match the patterns we\n    # know how to optimize away.\n    for node in old_graph.node:\n      # We always start with a Quantize node, and examine its inputs to see if\n      # they are in a form that can be removed.\n      if node.op not in [""Quantize"", ""QuantizeV2""]:\n        continue\n      dequantize_node_name = node_name_from_input(node.input[0])\n      if dequantize_node_name not in old_nodes_map:\n        raise ValueError(""Input node name \'"" + dequantize_node_name +\n                         ""\' not found in node \'"" + node.name + ""\'"")\n      dequantize_node = old_nodes_map[dequantize_node_name]\n      # Do we have a Dequantize feeding in, with the same type as the Quantize?\n      if dequantize_node.op != ""Dequantize"":\n        continue\n      if node.attr[""T""] != dequantize_node.attr[""T""]:\n        continue\n      # Now look at the other inputs, and ensure they\'re Min/Max nodes.\n      min_node_name = node_name_from_input(node.input[1])\n      max_node_name = node_name_from_input(node.input[2])\n      min_node = old_nodes_map[min_node_name]\n      max_node = old_nodes_map[max_node_name]\n      is_min_right_type = (min_node.op in [""Min"", ""Dequantize""])\n      is_max_right_type = (max_node.op in [""Max"", ""Dequantize""])\n      if not is_min_right_type or not is_max_right_type:\n        print(""Didn\'t find expected types on inputs : %s, %s."" % (min_node.op,\n                                                                  max_node.op))\n        continue\n      min_node_input_name = node_name_from_input(min_node.input[0])\n      max_node_input_name = node_name_from_input(max_node.input[0])\n      # There are two different patterns for Min nodes we can recognize, one\n      # where the input comes directly from the same one as the Max, and\n      # another where we run it through another Min first, so check for both.\n      is_same_input = False\n      if min_node_input_name == max_node_input_name:\n        is_same_input = True\n      else:\n        first_min_node_input = old_nodes_map[min_node_input_name]\n        if first_min_node_input.op == ""Concat"":\n          second_min_node_name = node_name_from_input(\n              first_min_node_input.input[1])\n          second_min_node = old_nodes_map[second_min_node_name]\n          if second_min_node.op == ""Min"":\n            second_min_node_input_name = node_name_from_input(\n                second_min_node.input[0])\n            is_same_input = (second_min_node_input_name == max_node_input_name)\n      if not is_same_input:\n        print(""Different min/max inputs: "" + min_node_input_name)\n        continue\n      # We recognize this pattern, so mark the graph edges to be rewired to\n      # route around it entirely, since we know it\'s a no-op.\n      dequantize_source_name = node_name_from_input(dequantize_node.input[0])\n      node_tensor_name = ensure_tensor_name_has_port(node.name)\n      min_tensor_name = node.name + "":1""\n      max_tensor_name = node.name + "":2""\n      inputs_to_rename[node_tensor_name] = dequantize_source_name\n      inputs_to_rename[min_tensor_name] = dequantize_node.input[1]\n      inputs_to_rename[max_tensor_name] = dequantize_node.input[2]\n    # Finally we apply all the rewiring we\'ve marked to the graph.\n    for node in old_graph.node:\n      for index, input_full_name in enumerate(node.input):\n        input_name = ensure_tensor_name_has_port(input_full_name)\n        if input_name in inputs_to_rename:\n          node.input[index] = inputs_to_rename[input_name]\n      self.add_output_graph_node(node)\n    return self.output_graph\n\n  def apply_final_node_renames(self):\n    """"""Applies node renames in self.final_node_renames to self.output_graph.""""""\n    old_graph = self.output_graph\n    self.output_graph = graph_pb2.GraphDef()\n    for node in old_graph.node:\n      node.name = self.final_node_renames.get(node.name, node.name)\n      for index, input_name in enumerate(node.input):\n        node_name = node_name_from_input(input_name)\n        input_full_name = ensure_tensor_name_has_port(input_name)\n        if node_name in self.final_node_renames:\n          node.input[index] = ""%s%s"" % (self.final_node_renames[node_name],\n                                        input_full_name[len(node_name):])\n      self.add_output_graph_node(node)\n    return self.output_graph\n\n  def remove_dead_nodes(self, output_names):\n    """"""Removes nodes that are no longer needed for inference from the graph.""""""\n    old_output_graph = self.output_graph\n    self.output_graph = graph_util.extract_sub_graph(old_output_graph,\n                                                     output_names)\n\n  def quantize_weights(self, input_graph, quantization_mode):\n    """"""Quantize float Const ops.\n\n    There are two modes of operations, both replace float Const ops with\n    quantized values.\n    1. If quantization_mode is ""weights_rounded"", this function replaces float\n    Const ops with quantized float Const ops - same as the original op, but\n    float values being mapped to the center of one of 1<<FLAGS.bitdepth buckets.\n    This does not change the raw model size, but compression algorithms such as\n    zip (as used for compressing apks) or bzip2 will achieve a very good\n    compression ratio.\n    2. For other quantization modes (""MIN_COMBINED"" or ""MIN_FIRST""), float\n    Const ops are quantized and replaced by a tuple of four ops to perform\n    the dequantization at runtime:\n    * eight-bit Const (bucket indices, same shape as original float Const op\n    * two float Const ops (min and max value of original float Const op)\n    * Dequantize op to convert the eight-bit consts to float tensors.\n    The quantization mode is important because we see accuracy problems when\n    quantizing weights for different situations depending on the algorithm\n    used. We haven\'t figured out exactly what the underlying cause is yet,\n    unfortunately.\n\n    Args:\n      input_graph: A GraphDef of the model containing float Const ops.\n      quantization_mode: How to quantize and dequantize the values.\n\n    Returns:\n      A GraphDef of the converted graph.\n\n    Raises:\n      ValueError: If quantization_mode is unsupported.\n    """"""\n    output_graph = graph_pb2.GraphDef()\n    for input_node in input_graph.node:\n      should_quantize = False\n      if input_node.op == ""Const"":\n        dtype = dtypes.as_dtype(input_node.attr[""dtype""].type)\n        if dtype == dtypes.float32:\n          should_quantize = True\n      if should_quantize:\n        if quantization_mode == ""weights_rounded"":\n          output_graph.node.extend(quantize_weight_rounded(input_node))\n        elif quantization_mode in (b""MIN_COMBINED"", b""MIN_FIRST""):\n          output_graph.node.extend(\n              quantize_weight_eightbit(input_node, quantization_mode))\n        else:\n          raise ValueError(""Unsupported quantization mode %s."" %\n                           quantization_mode)\n      else:\n        output_node = node_def_pb2.NodeDef()\n        output_node.CopyFrom(input_node)\n        output_graph.node.extend([output_node])\n    return output_graph\n\n  def set_input_graph(self, new_input_graph):\n    self.input_graph = new_input_graph\n    self.nodes_map = self.create_nodes_map(self.input_graph)\n\n\ndef main(unused_args):\n  if not gfile.Exists(FLAGS.input):\n    print(""Input graph file \'"" + FLAGS.input + ""\' does not exist!"")\n    return -1\n\n  known_modes = [\n      ""round"", ""quantize"", ""eightbit"", ""weights"", ""test"", ""weights_rounded""\n  ]\n  if not any(FLAGS.mode in s for s in known_modes):\n    print(""mode is \'"" + FLAGS.mode + ""\', not in "" + "", "".join(known_modes) +\n          ""."")\n    return -1\n\n  tf_graph = graph_pb2.GraphDef()\n  with gfile.Open(FLAGS.input, ""rb"") as f:\n    data = f.read()\n    tf_graph.ParseFromString(data)\n\n  graph = ops.Graph()\n  with graph.as_default():\n    importer.import_graph_def(tf_graph, input_map={}, name="""")\n\n  quantized_input_range = None\n  if FLAGS.quantized_input:\n    quantized_input_range = [\n        FLAGS.quantized_input_min, FLAGS.quantized_input_max\n    ]\n\n  fallback_quantization_range = None\n  if (FLAGS.quantized_fallback_min is not None or\n      FLAGS.quantized_fallback_max is not None):\n    assert FLAGS.quantized_fallback_min is not None\n    assert FLAGS.quantized_fallback_max is not None\n    fallback_quantization_range = [\n        FLAGS.quantized_fallback_min, FLAGS.quantized_fallback_max\n    ]\n\n  rewriter = GraphRewriter(tf_graph, FLAGS.mode, quantized_input_range,\n                           fallback_quantization_range)\n\n  output_graph = rewriter.rewrite(FLAGS.output_node_names.split("",""))\n\n  f = gfile.FastGFile(FLAGS.output, ""wb"")\n  f.write(output_graph.SerializeToString())\n\n  return 0\n\n\nif __name__ == ""__main__"":\n  app.run()\n'"
tools/test_forzen_graph.py,5,"b'import tensorflow as tf\nfrom scipy.misc import imread, imresize\nimport numpy as np\n\n# Quantize\nuse_quantized_graph = True\n\n# Read image\nimg = imread(""/home/zehao/Desktop/dog.png"")\nimg = imresize(img, (224, 224, 3))\nimg = img.astype(np.float32)\nimg = np.expand_dims(img, 0)\n\n# Preprocess\nimg = img / 255.\nimg = img - 0.5\nimg = img * 2.\n\n# Graph\nif use_quantized_graph:\n  graph_filename = ""../mobilenet-model/with_placeholder/quantized_graph.pb""\nelse:\n  graph_filename = ""../mobilenet-model/with_placeholder/frozen_graph.pb""\n\n# Create labels dict from labels.txt\nlabels_file = ""/home/zehao/Dataset/imagenet-data/labels.txt""\nlabels_dict = {}\nwith open(labels_file, \'r\') as f:\n  for kv in [d.strip().split(\':\') for d in f]:\n    labels_dict[int(kv[0])] = kv[1]\n\n# Create a graph def object to read the graph\nwith tf.gfile.GFile(graph_filename, ""rb"") as f:\n  graph_def = tf.GraphDef()\n  graph_def.ParseFromString(f.read())\n\n# Construct the graph and import the graph from graphdef\nwith tf.Graph().as_default() as graph:\n  tf.import_graph_def(graph_def)\n\n  # We define the input and output node we will feed in\n  input_node = graph.get_tensor_by_name(\'import/MobileNet/input_images:0\')\n  output_node = graph.get_tensor_by_name(\'import/MobileNet/Predictions/Softmax:0\')\n\n  with tf.Session() as sess:\n    predictions = sess.run(output_node, feed_dict={input_node: img})[0]\n    top_5_predictions = predictions.argsort()[-5:][::-1]\n    top_5_probabilities = predictions[top_5_predictions]\n    prediction_names = [labels_dict[i] for i in top_5_predictions]\n\n    for i in xrange(len(prediction_names)):\n      print \'Prediction: %s, Probability: %s \\n\' % (prediction_names[i], top_5_probabilities[i])\n'"
tools/tf_convert_data.py,7,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convert a dataset to TFRecords format, which can be easily integrated into\na TensorFlow pipeline.\n\nUsage:\n```shell\npython tf_convert_data.py \\\n    --datset_root=VOCdevkit \\\n    --year=0712 \\\n    --split=trainval \\\n    --output_dir=/tmp/pascalvoc0712_tfrecord\n```\n""""""\nimport tensorflow as tf\nimport os, sys\ncurr_path = os.path.abspath(os.path.dirname(__file__))\nsys.path.insert(0, os.path.join(curr_path, \'../\'))\n\nfrom datasets import pascalvoc_to_tfrecords, kitti_object_to_tfrecords\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'pascalvoc\',\n    \'The name of the dataset to convert.\')\ntf.app.flags.DEFINE_string(\n    \'dataset_root\', None,\n    \'Directory where the original dataset is stored.\')\ntf.app.flags.DEFINE_string(\n    \'year\', \'0712\',\n    \'Year of VOC dataset.\')\ntf.app.flags.DEFINE_string(\n    \'split\', \'trainval\',\n    \'Split of dataset, trainval/train/val/test.\')\ntf.app.flags.DEFINE_string(\n    \'output_dir\', \'./\',\n    \'Output directory where to store TFRecords files.\')\n\n\ndef main(_):\n    print(\'Dataset root dir:\', FLAGS.dataset_root)\n    print(\'Output directory:\', FLAGS.output_dir)\n\n    if FLAGS.dataset_name == \'pascalvoc\':\n        pascalvoc_to_tfrecords.run(FLAGS.dataset_root,\n                                   FLAGS.year,\n                                   FLAGS.split,\n                                   FLAGS.output_dir,\n                                   shuffling=True)\n    elif FLAGS.dataset_name == \'kitti\':\n        kitti_object_to_tfrecords.run(FLAGS.dataset_root,\n                                   FLAGS.split,\n                                   FLAGS.output_dir,\n                                   shuffling=True)\n    else:\n        raise ValueError(\'Dataset [%s] was not recognized.\' % FLAGS.dataset_name)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n'"
tools/time_benchmark.py,16,"b'import tensorflow as tf\nimport time\nfrom datetime import datetime\nimport math\nimport argparse\nimport sys\nfrom nets.mobilenet import mobilenet, mobilenet_arg_scope\nimport numpy as np\n\nslim = tf.contrib.slim\n\n\ndef time_tensorflow_run(session, target, info_string):\n  num_steps_burn_in = 10\n  total_duration = 0.0\n  total_duration_squared = 0.0\n\n  for i in range(FLAGS.num_batches + num_steps_burn_in):\n    start_time = time.time()\n    _ = session.run(target)\n    duration = time.time() - start_time\n    if i >= num_steps_burn_in:\n      if not i % 10:\n        print(\'%s: step %d, duration = %.3f\' % (datetime.now(), i - num_steps_burn_in, duration))\n      total_duration += duration\n      total_duration_squared += duration * duration\n\n  mn = total_duration / FLAGS.num_batches\n  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n  sd = math.sqrt(vr)\n  print(\'%s: %s across %d steps, %.3f +/- %.3f sec / batch\' % (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n\ndef time_tensorflow_run_placeholder(session, target, feed_dict, info_string):\n  num_steps_burn_in = 10\n  total_duration = 0.0\n  total_duration_squared = 0.0\n\n  for i in range(FLAGS.num_batches + num_steps_burn_in):\n    start_time = time.time()\n    _ = session.run(target,feed_dict=feed_dict)\n    duration = time.time() - start_time\n    if i >= num_steps_burn_in:\n      if not i % 10:\n        print(\'%s: step %d, duration = %.3f\' % (datetime.now(), i - num_steps_burn_in, duration))\n      total_duration += duration\n      total_duration_squared += duration * duration\n\n  mn = total_duration / FLAGS.num_batches\n  vr = total_duration_squared / FLAGS.num_batches - mn * mn\n  sd = math.sqrt(vr)\n  print(\'%s: %s across %d steps, %.3f +/- %.3f sec / batch\' % (datetime.now(), info_string, FLAGS.num_batches, mn, sd))\n\ndef run_benchmark():\n  if FLAGS.quantized:\n    graph_filename = FLAGS.quantized_graph\n    # Create a graph def object to read the graph\n    with tf.gfile.GFile(graph_filename, ""rb"") as f:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(f.read())\n\n  with tf.Graph().as_default() as graph:\n    with tf.device(\'/\'+FLAGS.mode+\':0\'):\n      image_size = 224\n      if FLAGS.quantized:\n        inputs = np.random.random((FLAGS.batch_size, image_size, image_size, 3))\n        tf.import_graph_def(graph_def)\n        config = tf.ConfigProto()\n        config.gpu_options.allocator_type = \'BFC\'\n        sess = tf.Session(config=config)\n        # We define the input and output node we will feed in\n        input_node = graph.get_tensor_by_name(\'import/MobileNet/input_images:0\')\n        output_node = graph.get_tensor_by_name(\'import/MobileNet/Predictions/Softmax:0\')\n        time_tensorflow_run_placeholder(sess, output_node, {input_node: inputs}, ""Forward"")\n      else:\n        image_size = 224\n        inputs = tf.Variable(tf.random_normal([FLAGS.batch_size,\n                                               image_size,\n                                               image_size, 3],\n                                              dtype=tf.float32,\n                                              stddev=1e-1))\n        with slim.arg_scope(mobilenet_arg_scope()):\n          logits, end_points = mobilenet(inputs, is_training=False)\n\n        init = tf.global_variables_initializer()\n        config = tf.ConfigProto()\n        config.gpu_options.allocator_type = \'BFC\'\n        sess = tf.Session(config=config)\n        sess.run(init)\n\n        time_tensorflow_run(sess, logits, ""Forward"")\n\n        # Add a simple objective so we can calculate the backward pass.\n        objective = tf.nn.l2_loss(logits)\n\n        # Compute the gradient with respect to all the parameters.\n        grad = tf.gradients(objective, tf.trainable_variables())\n\n        # Run the backward benchmark.\n        time_tensorflow_run(sess, grad, ""Forward-backward"")\n\ndef main(_):\n  run_benchmark()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n    \'--batch_size\',\n    type=int,\n    default=1,\n    help=\'Batch size.\'\n  )\n  parser.add_argument(\n    \'--num_batches\',\n    type=int,\n    default=100,\n    help=\'Number of batches to run.\'\n  )\n  parser.add_argument(\n    \'--mode\',\n    type=str,\n    default=\'cpu\',\n    help=\'gpu/cpu mode.\'\n  )\n  parser.add_argument(\n    \'--quantized\',\n    type=bool,\n    default=False,\n    help=\'Benchmark quantized graph.\'\n  )\n  parser.add_argument(\n    \'--quantized_graph\',\n    type=str,\n    default=\'\',\n    help=\'Path to quantized graph file.\'\n  )\n  FLAGS, unparsed = parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
utils/__init__.py,0,b''
utils/det_utils.py,209,"b'import tensorflow as tf\nimport numpy as np\nimport cv2\nfrom configs.kitti_config import config\n\n\n# ################## From SqueezeDet #########################\n\ndef safe_exp(w, thresh):\n  """"""Safe exponential function for tensors.""""""\n\n  slope = np.exp(thresh)\n  with tf.name_scope(\'safe_exponential\'):\n    lin_region = tf.to_float(w > thresh)\n\n    lin_out = slope * (w - thresh + 1.)\n    exp_out = tf.exp(w)\n\n    out = lin_region * lin_out + (1. - lin_region) * exp_out\n  return out\n\n\ndef bbox_transform_inv(bbox):\n  """"""convert a bbox of form [xmin, ymin, xmax, ymax] to [cx, cy, w, h]. Works\n  for numpy array or list of tensors.\n  """"""\n  with tf.name_scope(\'bbox_transform_inv\') as scope:\n    xmin, ymin, xmax, ymax = bbox\n    out_box = [[]] * 4\n\n    width = xmax - xmin + 1.0\n    height = ymax - ymin + 1.0\n    out_box[0] = xmin + 0.5 * width\n    out_box[1] = ymin + 0.5 * height\n    out_box[2] = width\n    out_box[3] = height\n\n  return out_box\n\n\ndef bbox_transform(bbox):\n  """"""convert a bbox of form [cx, cy, w, h] to [xmin, ymin, xmax, ymax]. Works\n  for numpy array or list of tensors.\n  """"""\n  with tf.name_scope(\'bbox_transform\') as scope:\n    cx, cy, w, h = bbox\n    out_box = [[]] * 4\n    out_box[0] = cx - w / 2\n    out_box[1] = cy - h / 2\n    out_box[2] = cx + w / 2\n    out_box[3] = cy + h / 2\n\n  return out_box\n\n\ndef nms(boxes, probs, threshold):\n  """"""Non-Maximum supression.\n  Args:\n    boxes: array of [cx, cy, w, h] (center format)\n    probs: array of probabilities\n    threshold: two boxes are considered overlapping if their IOU is largher than\n        this threshold\n    form: \'center\' or \'diagonal\'\n  Returns:\n    keep: array of True or False.\n  """"""\n\n  order = probs.argsort()[::-1]\n  keep = [True]*len(order)\n\n  for i in range(len(order)-1):\n    ovps = batch_iou(boxes[order[i+1:]], boxes[order[i]])\n    for j, ov in enumerate(ovps):\n      if ov > threshold:\n        keep[order[j+i+1]] = False\n  return keep\n\n\ndef interpre_prediction(prediction, input_mask, anchors, box_input):\n  # probability\n  batch_size = tf.shape(input_mask)[0]\n  num_class_probs = config.NUM_ANCHORS * config.NUM_CLASSES\n  pred_class_probs = tf.reshape(\n    tf.nn.softmax(\n      tf.reshape(\n        prediction[:, :, :, :num_class_probs],\n        [-1, config.NUM_CLASSES]\n      )\n    ),\n    [batch_size, config.ANCHORS, config.NUM_CLASSES],\n    name=\'pred_class_probs\'\n  )\n\n  # confidence\n  num_confidence_scores = config.NUM_ANCHORS + num_class_probs\n  pred_conf = tf.sigmoid(\n    tf.reshape(\n      prediction[:, :, :, num_class_probs:num_confidence_scores],\n      [batch_size, config.ANCHORS]\n    ),\n    name=\'pred_confidence_score\'\n  )\n\n  # bbox_delta\n  pred_box_delta = tf.reshape(\n    prediction[:, :, :, num_confidence_scores:],\n    [batch_size, config.ANCHORS, 4],\n    name=\'bbox_delta\'\n  )\n\n  # number of object. Used to normalize bbox and classification loss\n  num_objects = tf.reduce_sum(input_mask, name=\'num_objects\')\n\n  with tf.name_scope(\'bbox\') as scope:\n    with tf.name_scope(\'stretching\'):\n      delta_x, delta_y, delta_w, delta_h = tf.unstack(\n        pred_box_delta, axis=2)\n\n      anchor_x = anchors[:, 0]\n      anchor_y = anchors[:, 1]\n      anchor_w = anchors[:, 2]\n      anchor_h = anchors[:, 3]\n\n      box_center_x = tf.identity(\n        anchor_x + delta_x * anchor_w, name=\'bbox_cx\')\n      box_center_y = tf.identity(\n        anchor_y + delta_y * anchor_h, name=\'bbox_cy\')\n      box_width = tf.identity(\n        anchor_w * safe_exp(delta_w, config.EXP_THRESH),\n        name=\'bbox_width\')\n      box_height = tf.identity(\n        anchor_h * safe_exp(delta_h, config.EXP_THRESH),\n        name=\'bbox_height\')\n\n    with tf.name_scope(\'trimming\'):\n      xmins, ymins, xmaxs, ymaxs = bbox_transform(\n        [box_center_x, box_center_y, box_width, box_height])\n\n      # The max x position is mc.IMAGE_WIDTH - 1 since we use zero-based\n      # pixels. Same for y.\n      xmins = tf.minimum(\n        tf.maximum(0.0, xmins), config.IMG_WIDTH - 1.0, name=\'bbox_xmin\')\n\n      ymins = tf.minimum(\n        tf.maximum(0.0, ymins), config.IMG_HEIGHT - 1.0, name=\'bbox_ymin\')\n\n      xmaxs = tf.maximum(\n        tf.minimum(config.IMG_WIDTH - 1.0, xmaxs), 0.0, name=\'bbox_xmax\')\n\n      ymaxs = tf.maximum(\n        tf.minimum(config.IMG_HEIGHT - 1.0, ymaxs), 0.0, name=\'bbox_ymax\')\n\n      det_boxes = tf.transpose(\n        tf.stack(bbox_transform_inv([xmins, ymins, xmaxs, ymaxs])),\n        (1, 2, 0), name=\'bbox\'\n      )\n\n      with tf.name_scope(\'IOU\'):\n        def _tensor_iou(box1, box2):\n          with tf.name_scope(\'intersection\'):\n            xmin = tf.maximum(box1[0], box2[0], name=\'xmin\')\n            ymin = tf.maximum(box1[1], box2[1], name=\'ymin\')\n            xmax = tf.minimum(box1[2], box2[2], name=\'xmax\')\n            ymax = tf.minimum(box1[3], box2[3], name=\'ymax\')\n\n            w = tf.maximum(0.0, xmax - xmin, name=\'inter_w\')\n            h = tf.maximum(0.0, ymax - ymin, name=\'inter_h\')\n            intersection = tf.multiply(w, h, name=\'intersection\')\n\n          with tf.name_scope(\'union\'):\n            w1 = tf.subtract(box1[2], box1[0], name=\'w1\')\n            h1 = tf.subtract(box1[3], box1[1], name=\'h1\')\n            w2 = tf.subtract(box2[2], box2[0], name=\'w2\')\n            h2 = tf.subtract(box2[3], box2[1], name=\'h2\')\n\n            union = w1 * h1 + w2 * h2 - intersection\n\n          return intersection / (union + config.EPSILON) \\\n                 * tf.reshape(input_mask, [batch_size, config.ANCHORS])\n\n        ious = _tensor_iou(\n          bbox_transform(tf.unstack(det_boxes, axis=2)),\n          bbox_transform(tf.unstack(box_input, axis=2))\n        )\n\n      with tf.name_scope(\'probability\') as scope:\n        probs = tf.multiply(\n          pred_class_probs,\n          tf.reshape(pred_conf, [batch_size, config.ANCHORS, 1]),\n          name=\'final_class_prob\'\n        )\n\n        det_probs = tf.reduce_max(probs, 2, name=\'score\')\n        det_class = tf.argmax(probs, 2, name=\'class_idx\')\n\n  return pred_box_delta, pred_class_probs, pred_conf, ious, det_probs, det_boxes, det_class\n\n\ndef losses(input_mask, labels, ious, box_delta_input, pred_class_probs, pred_conf, pred_box_delta):\n  batch_size = tf.shape(input_mask)[0]\n  num_objects = tf.reduce_sum(input_mask, name=\'num_objects\')\n\n  with tf.name_scope(\'class_regression\') as scope:\n    # cross-entropy: q * -log(p) + (1-q) * -log(1-p)\n    # add a small value into log to prevent blowing up\n    class_loss = tf.truediv(\n      tf.reduce_sum(\n        (labels * (-tf.log(pred_class_probs + config.EPSILON))\n         + (1 - labels) * (-tf.log(1 - pred_class_probs + config.EPSILON)))\n        * input_mask * config.LOSS_COEF_CLASS),\n      num_objects,\n      name=\'class_loss\'\n    )\n    tf.losses.add_loss(class_loss)\n\n  with tf.name_scope(\'confidence_score_regression\') as scope:\n    input_mask_ = tf.reshape(input_mask, [batch_size, config.ANCHORS])\n    conf_loss = tf.reduce_mean(\n      tf.reduce_sum(\n        tf.square((ious - pred_conf))\n        * (input_mask_ * config.LOSS_COEF_CONF_POS / num_objects\n           + (1 - input_mask_) * config.LOSS_COEF_CONF_NEG / (config.ANCHORS - num_objects)),\n        reduction_indices=[1]\n      ),\n      name=\'confidence_loss\'\n    )\n    tf.losses.add_loss(conf_loss)\n\n  with tf.name_scope(\'bounding_box_regression\') as scope:\n    bbox_loss = tf.truediv(\n      tf.reduce_sum(\n        config.LOSS_COEF_BBOX * tf.square(\n          input_mask * (pred_box_delta - box_delta_input))),\n      num_objects,\n      name=\'bbox_loss\'\n    )\n    tf.losses.add_loss(bbox_loss)\n\n    # # add above losses as well as weight decay losses to form the total loss\n    # loss = tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n    # return loss\n\n\n# ################# MobileNet Det ########################\n\ndef xywh_to_yxyx(bbox):\n  shape = bbox.get_shape().as_list()\n  _axis = 1 if len(shape) > 1 else 0\n  [x, y, w, h] = tf.unstack(bbox, axis=_axis)\n  y_min = y - 0.5 * h\n  x_min = x - 0.5 * w\n  y_max = y + 0.5 * h\n  x_max = x + 0.5 * w\n  return tf.stack([y_min, x_min, y_max, x_max], axis=_axis)\n\n\ndef yxyx_to_xywh(bbox):\n  y_min = bbox[:, 0]\n  x_min = bbox[:, 1]\n  y_max = bbox[:, 2]\n  x_max = bbox[:, 3]\n  x = (x_min + x_max) * 0.5\n  y = (y_min + y_max) * 0.5\n  w = x_max - x_min\n  h = y_max - y_min\n  return tf.stack([x, y, w, h], axis=1)\n\n\ndef rearrange_coords(bbox):\n  y_min = bbox[:, 0]\n  x_min = bbox[:, 1]\n  y_max = bbox[:, 2]\n  x_max = bbox[:, 3]\n  return tf.stack([x_min, y_min, x_max, y_max])\n\n\ndef scale_bboxes(bbox, img_shape):\n  """"""Scale bboxes to [0, 1). bbox format [ymin, xmin, ymax, xmax]\n  Args:\n    bbox: 2-D with shape \'[num_bbox, 4]\'\n    img_shape: 1-D with shape \'[4]\'\n  Return:\n    sclaed_bboxes: scaled bboxes\n  """"""\n  img_h = tf.cast(img_shape[0], dtype=tf.float32)\n  img_w = tf.cast(img_shape[1], dtype=tf.float32)\n  shape = bbox.get_shape().as_list()\n  _axis = 1 if len(shape) > 1 else 0\n  [y_min, x_min, y_max, x_max] = tf.unstack(bbox, axis=_axis)\n  y_1 = tf.truediv(y_min, img_h)\n  x_1 = tf.truediv(x_min, img_w)\n  y_2 = tf.truediv(y_max, img_h)\n  x_2 = tf.truediv(x_max, img_w)\n  return tf.stack([y_1, x_1, y_2, x_2], axis=_axis)\n\n\ndef iou(bbox_1, bbox_2):\n  """"""Compute iou of a box with another box. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    bbox_1: 1-D with shape `[4]`.\n    bbox_2: 1-D with shape `[4]`.\n\n  Returns:\n    IOU\n  """"""\n  lr = tf.minimum(bbox_1[3], bbox_2[3]) - tf.maximum(bbox_1[1], bbox_2[1])\n  tb = tf.minimum(bbox_1[2], bbox_2[2]) - tf.maximum(bbox_1[0], bbox_2[0])\n  lr = tf.maximum(lr, lr * 0)\n  tb = tf.maximum(tb, tb * 0)\n  intersection = tf.multiply(tb, lr)\n  union = tf.subtract(\n    tf.multiply((bbox_1[3] - bbox_1[1]), (bbox_1[2] - bbox_1[0])) +\n    tf.multiply((bbox_2[3] - bbox_2[1]), (bbox_2[2] - bbox_2[0])),\n    intersection\n  )\n  iou = tf.div(intersection, union)\n  return iou\n\n\ndef batch_iou(bboxes, bbox):\n  """"""Compute iou of a batch of boxes with another box. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    bboxes: A batch of boxes. 2-D with shape `[B, 4]`.\n    bbox: A single box. 1-D with shape `[4]`.\n\n  Returns:\n    Batch of IOUs\n  """"""\n  lr = tf.maximum(\n    tf.minimum(bboxes[:, 3], bbox[3]) -\n    tf.maximum(bboxes[:, 1], bbox[1]),\n    0\n  )\n  tb = tf.maximum(\n    tf.minimum(bboxes[:, 2], bbox[2]) -\n    tf.maximum(bboxes[:, 0], bbox[0]),\n    0\n  )\n  intersection = tf.multiply(tb, lr)\n  union = tf.subtract(\n    tf.multiply((bboxes[:, 3] - bboxes[:, 1]), (bboxes[:, 2] - bboxes[:, 0])) +\n    tf.multiply((bbox[3] - bbox[1]), (bbox[2] - bbox[0])),\n    intersection\n  )\n  iou = tf.div(intersection, union)\n  return iou\n\n\ndef batch_iou_fast(anchors, bboxes):\n  """""" Compute iou of two batch of boxes. Box format \'[y_min, x_min, y_max, x_max]\'.\n  Args:\n    anchors: know shape\n    bboxes: dynamic shape\n  Return:\n    ious: 2-D with shape \'[num_bboxes, num_anchors]\'\n  """"""\n  num_anchors = anchors.get_shape().as_list()[0]\n  num_bboxes = tf.shape(bboxes)[0]\n\n  box_indices = tf.reshape(tf.range(num_bboxes), shape=[-1, 1])\n  box_indices = tf.reshape(tf.stack([box_indices] * num_anchors, axis=1), shape=[-1, 1]) # use tf.tile instead\n  # box_indices = tf.tile(box_indices, [num_anchors, 1])\n\n  # box_indices = tf.Print(box_indices, [box_indices], ""box_indices"", summarize=100)\n\n\n  bboxes_m = tf.gather_nd(bboxes, box_indices)\n  # bboxes_m = tf.Print(bboxes_m, [bboxes_m], ""bboxes_m"", summarize=100)\n\n  anchors_m = tf.tile(anchors, [num_bboxes, 1])\n  # anchors_m = tf.Print(anchors_m, [anchors_m], ""anchors_m"", summarize=100)\n\n  lr = tf.maximum(\n    tf.minimum(bboxes_m[:, 3], anchors_m[:, 3]) -\n    tf.maximum(bboxes_m[:, 1], anchors_m[:, 1]),\n    0\n  )\n  tb = tf.maximum(\n    tf.minimum(bboxes_m[:, 2], anchors_m[:, 2]) -\n    tf.maximum(bboxes_m[:, 0], anchors_m[:, 0]),\n    0\n  )\n\n  intersection = tf.multiply(tb, lr)\n\n  union = tf.subtract(\n    tf.multiply((bboxes_m[:, 3] - bboxes_m[:, 1]), (bboxes_m[:, 2] - bboxes_m[:, 0])) +\n    tf.multiply((anchors_m[:, 3] - anchors_m[:, 1]), (anchors_m[:, 2] - anchors_m[:, 0])),\n    intersection\n  )\n\n  ious = tf.truediv(intersection, union)\n\n  ious = tf.reshape(ious, shape=[num_bboxes, num_anchors])\n\n  return ious\n\n\ndef compute_delta(gt_box, anchor):\n  """"""Compute delta, anchor+delta = gt_box. Box format \'[cx, cy, w, h]\'.\n  Args:\n    gt_box: A batch of boxes. 2-D with shape `[B, 4]`.\n    anchor: A single box. 1-D with shape `[4]`.\n\n  Returns:\n    delta: 1-D tensor with shape \'[4]\', [dx, dy, dw, dh]\n  """"""\n  delta_x = (gt_box[0] - anchor[0]) / gt_box[2]\n  delta_y = (gt_box[1] - anchor[1]) / gt_box[3]\n  delta_w = tf.log(gt_box[2] / anchor[2])\n  delta_h = tf.log(gt_box[3] / anchor[3])\n  return tf.stack([delta_x, delta_y, delta_w, delta_h], axis=0)\n\n\ndef batch_delta(bboxes, anchors):\n  """"""\n  Args:\n     bboxes: [num_bboxes, 4]\n     anchors: [num_bboxes, 4]\n  Return:\n    deltas: [num_bboxes, 4]\n  """"""\n  bbox_x, bbox_y, bbox_w, bbox_h = tf.unstack(bboxes, axis=1)\n  anchor_x, anchor_y, anchor_w, anchor_h = tf.unstack(anchors, axis=1)\n  delta_x = (bbox_x - anchor_x) / bbox_w\n  delta_y = (bbox_y - anchor_y) / bbox_h\n  delta_w = tf.log(bbox_w / anchor_w)\n  delta_h = tf.log(bbox_h / anchor_h)\n  return tf.stack([delta_x, delta_y, delta_w, delta_h], axis=1)\n\n\ndef arg_closest_anchor(bboxes, anchors):\n  """"""Find the closest anchor. Box Format [ymin, xmin, ymax, xmax]\n  """"""\n  num_anchors = anchors.get_shape().as_list()[0]\n  num_bboxes = tf.shape(bboxes)[0]\n\n  _indices = tf.reshape(tf.range(num_bboxes), shape=[-1, 1])\n  _indices = tf.reshape(tf.stack([_indices] * num_anchors, axis=1), shape=[-1, 1])\n  bboxes_m = tf.gather_nd(bboxes, _indices)\n  # bboxes_m = tf.Print(bboxes_m, [bboxes_m], ""bboxes_m"", summarize=100)\n\n  anchors_m = tf.tile(anchors, [num_bboxes, 1])\n  # anchors_m = tf.Print(anchors_m, [anchors_m], ""anchors_m"", summarize=100)\n\n  square_dist = tf.squared_difference(bboxes_m[:, 0], anchors_m[:, 0]) + \\\n                tf.squared_difference(bboxes_m[:, 1], anchors_m[:, 1]) + \\\n                tf.squared_difference(bboxes_m[:, 2], anchors_m[:, 2]) + \\\n                tf.squared_difference(bboxes_m[:, 3], anchors_m[:, 3])\n\n  square_dist = tf.reshape(square_dist, shape=[num_bboxes, num_anchors])\n\n  # square_dist = tf.Print(square_dist, [square_dist], ""square_dist"", summarize=100)\n\n  indices = tf.arg_min(square_dist, dimension=1)\n\n  return indices\n\n\ndef update_tensor(ref, indices, update):\n  zero = tf.cast(tf.sparse_to_dense(indices,\n                                    tf.shape(ref, out_type=tf.int64),\n                                    0,\n                                    default_value=1\n                                    ),\n                 dtype=tf.int64\n                 )\n\n  update_value = tf.cast(tf.sparse_to_dense(indices,\n                                            tf.shape(ref, out_type=tf.int64),\n                                            update,\n                                            default_value=0\n                                            ),\n                         dtype=tf.int64\n                         )\n  return ref * zero + update_value\n\n\ndef find_dup(a):\n  """""" Find the duplicated elements in 1-D a tensor.\n  Args:\n    a: 1-D tensor.\n    \n  Return:\n    more_than_one_vals: duplicated value in a.\n    indexes_in_a: duplicated value\'s index in a.\n    dups_in_a: duplicated value with duplicate in a.\n  """"""\n  unique_a_vals, unique_idx = tf.unique(a)\n  count_a_unique = tf.unsorted_segment_sum(tf.ones_like(a),\n                                           unique_idx,\n                                           tf.shape(a)[0])\n\n  more_than_one = tf.greater(count_a_unique, 1)\n  more_than_one_idx = tf.squeeze(tf.where(more_than_one))\n  more_than_one_vals = tf.squeeze(tf.gather(unique_a_vals, more_than_one_idx))\n\n  not_duplicated, _ = tf.setdiff1d(a, more_than_one_vals)\n  dups_in_a, indexes_in_a = tf.setdiff1d(a, not_duplicated)\n\n  return more_than_one_vals, indexes_in_a, dups_in_a\n\n\ndef encode_annos(labels, bboxes, anchors, num_classes):\n  """"""Encode annotations for losses computations.\n  All the output tensors have a fix shape(none dynamic dimention).\n\n  Args:\n    labels: 1-D with shape `[num_bounding_boxes]`.\n    bboxes: 2-D with shape `[num_bounding_boxes, 4]`. Format [ymin, xmin, ymax, xmax]\n    anchors: 4-D tensor with shape `[num_anchors, 4]`. Format [cx, cy, w, h]\n\n  Returns:\n    input_mask: 2-D with shape `[num_anchors, 1]`, indicate which anchor to be used to cal loss.\n    labels_input: 2-D with shape `[num_anchors, num_classes]`, one hot encode for every anchor.\n    box_delta_input: 2-D with shape `[num_anchors, 4]`. Format [dcx, dcy, dw, dh]\n    box_input: 2-D with shape \'[num_anchors, 4]\'. Format [ymin, xmin, ymax, xmax]\n  """"""\n  with tf.name_scope(""Encode_annotations"") as scope:\n    num_anchors = config.ANCHORS\n    # num_bboxes = tf.shape(bboxes)[0]\n\n    # Cal iou, find the target anchor\n    with tf.name_scope(""Matching"") as subscope:\n      ious = batch_iou_fast(xywh_to_yxyx(anchors), bboxes)\n      anchor_indices = tf.reshape(tf.arg_max(ious, dimension=1), shape=[-1, 1])  # target anchor indices\n      # anchor_indices = tf.Print(anchor_indices, [anchor_indices], ""anchor_indices"", summarize=100)\n\n      # discard duplicate # unique_idx wrong\n      anchor_indices, idx, count = tf.unique_with_counts(tf.reshape(anchor_indices, shape=[-1]))\n      ori_idx = tf.cumsum(tf.pad(count, [[1, 0]]))[:-1]\n      anchor_indices = tf.reshape(anchor_indices, shape=[-1, 1])\n      bboxes = tf.gather(bboxes, tf.unique(ori_idx)[0])\n      labels = tf.gather(labels, tf.unique(ori_idx)[0])\n      ious = tf.gather(ious, tf.unique(ori_idx)[0])\n      num_bboxes = tf.shape(anchor_indices)[0]\n\n      # TODO(shizehao):deal with duplicate\n      # with tf.name_scope(""Deal_with_duplicate""):\n      #   dup_anchor_indices, indices_in_a, dup_anchor_indices_with_dup = find_dup(tf.reshape(anchor_indices, shape=[-1]))\n      #\n      #   # reset duplicated corresponding anchor\n      #   conflicted_ious = tf.gather(ious, indices_in_a)\n      #   top_k_anchor_indices = tf.nn.top_k(conflicted_ious, k=20).indices  # shape = [num_conflicted_bboxes, 20]\n      #   dup_group_idx = tf.where(tf.equal(dup_anchor_indices_with_dup, tf.reshape(dup_anchor_indices, shape=[-1, 1])))\n      #   seg_group = tf.unstack(dup_group_idx, axis=1)[0]\n\n\n      with tf.name_scope(""Deal_with_noneoverlap""):\n        # find the none-overlap bbox\n        bbox_indices = tf.reshape(tf.range(num_bboxes), shape=[-1, 1])\n        # bbox_indices = tf.Print(bbox_indices, [bbox_indices], ""bbox_indices"", summarize=100)\n\n        # anchor_indices = tf.Print(anchor_indices, [anchor_indices], ""anchor_indices"", summarize=100)\n        iou_indices = tf.concat([bbox_indices, tf.cast(anchor_indices, dtype=tf.int32)], axis=1)\n        # iou_indices = tf.Print(iou_indices, [iou_indices], ""iou_indices"", summarize=100)\n\n        target_iou = tf.gather_nd(ious, iou_indices)\n        # target_iou = tf.Print(target_iou,[target_iou],""target_iou"",summarize=100)\n\n        none_overlap_bbox_indices = tf.where(target_iou <= 0)  # 1-D\n        # none_overlap_bbox_indices = tf.Print(none_overlap_bbox_indices, [none_overlap_bbox_indices], ""none_overlap_bbox_indices"", summarize=100)\n\n        # find it\'s corresponding anchor\n        target_bbox = tf.gather_nd(bboxes, none_overlap_bbox_indices)\n        # target_bbox = tf.Print(target_bbox, [target_bbox], ""target_bbox"", summarize=100)\n\n        closest_anchor_indices = arg_closest_anchor(target_bbox, xywh_to_yxyx(anchors))  # 1-D\n        # closest_anchor_indices = tf.Print(closest_anchor_indices, [closest_anchor_indices, tf.gather(anchors, closest_anchor_indices)], ""closest_anchor_indices"", summarize=100)\n\n      with tf.name_scope(""Update_anchor_indices""):\n        anchor_indices = tf.reshape(anchor_indices, shape=[-1])\n        anchor_indices = update_tensor(anchor_indices, none_overlap_bbox_indices, closest_anchor_indices)\n        anchor_indices = tf.reshape(anchor_indices, shape=[-1, 1])\n\n\n    with tf.name_scope(""Delta"") as subscope:\n      target_anchors = tf.gather_nd(anchors, anchor_indices)\n      bboxes = yxyx_to_xywh(bboxes)\n      delta = batch_delta(bboxes, target_anchors)\n\n\n\n    with tf.name_scope(""Scattering"") as subscope:\n      # bbox\n      box_input = tf.scatter_nd(anchor_indices,\n                                bboxes,\n                                shape=[num_anchors, 4]\n                                )\n\n      # label\n      labels_input = tf.scatter_nd(anchor_indices,\n                                   tf.one_hot(labels, num_classes),\n                                   shape=[num_anchors, num_classes]\n                                   )\n\n      # delta\n      box_delta_input = tf.scatter_nd(anchor_indices,\n                                      delta,\n                                      shape=[num_anchors, 4]\n                                      )\n\n\n\n\n\n      # anchor mask\n      # unique_indices, _ = tf.unique(tf.reshape(anchor_indices, shape=[-1]))\n      # unique_indices = tf.Print(unique_indices, [unique_indices], summarize=100)\n      # num_bboxes = tf.Print(num_bboxes, [num_bboxes])\n      input_mask = tf.scatter_nd(anchor_indices,\n                                 tf.ones([num_bboxes]),\n                                 shape=[num_anchors])\n      input_mask = tf.reshape(input_mask, shape=[-1, 1])\n\n  return input_mask, labels_input, box_delta_input, box_input\n\n\ndef filter_prediction(boxes, probs, cls_idx):\n  """"""Filter bounding box predictions with probability threshold and\n  non-maximum supression.\n\n  Args:\n    boxes: array of [cx, cy, w, h].\n    probs: array of probabilities\n    cls_idx: array of class indices\n  Returns:\n    final_boxes: array of filtered bounding boxes.\n    final_probs: array of filtered probabilities\n    final_cls_idx: array of filtered class indices\n  """"""\n  pass\n  # if config.TOP_N_DETECTION < len(probs) and config.TOP_N_DETECTION > 0:\n  #   order = probs.argsort()[:-config.TOP_N_DETECTION - 1:-1]\n  #   probs = probs[order]\n  #   boxes = boxes[order]\n  #   cls_idx = cls_idx[order]\n  # else:\n  #   filtered_idx = np.nonzero(probs > config.PROB_THRESH)[0]\n  #   probs = probs[filtered_idx]\n  #   boxes = boxes[filtered_idx]\n  #   cls_idx = cls_idx[filtered_idx]\n  #\n  # final_boxes = []\n  # final_probs = []\n  # final_cls_idx = []\n  #\n  # for c in range(config.CLASSES):\n  #   idx_per_class = [i for i in range(len(probs)) if cls_idx[i] == c]\n  #   keep = nms(boxes[idx_per_class], probs[idx_per_class], config.NMS_THRESH)\n  #   for i in range(len(keep)):\n  #     if keep[i]:\n  #       final_boxes.append(boxes[idx_per_class[i]])\n  #       final_probs.append(probs[idx_per_class[i]])\n  #       final_cls_idx.append(c)\n  # return final_boxes, final_probs, final_cls_idx\n\n\ndef viz_prediction_result(images, batch_det_bbox, batch_det_class, batch_det_prob):\n  pass\n  # for i in range(len(images)):\n  #   # draw prediction\n  #   det_bbox, det_prob, det_class = filter_prediction(\n  #     batch_det_bbox[i], batch_det_prob[i], batch_det_class[i])\n  #\n  #   keep_idx = [idx for idx in range(len(det_prob)) \\\n  #               if det_prob[idx] > config.PLOT_PROB_THRESH]\n  #   det_bbox = [det_bbox[idx] for idx in keep_idx]\n  #   det_prob = [det_prob[idx] for idx in keep_idx]\n  #   det_class = [det_class[idx] for idx in keep_idx]\n  #\n  #   _draw_box(\n  #       images[i], det_bbox,\n  #       [mc.CLASS_NAMES[idx]+\': (%.2f)\'% prob \\\n  #           for idx, prob in zip(det_class, det_prob)],\n  #       (0, 0, 255))'"
utils/det_utils_test.py,16,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom utils.det_utils import *\nfrom configs.kitti_config import config\n\nimport pickle\n\nclass MobileNetDetTest(tf.test.TestCase):\n  def test_batch_iou_fast(self):\n    anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n    anchors = xywh_to_yxyx(anchors)\n    bboxes = tf.placeholder(dtype=tf.float32, shape=[None, 4])\n    bboxes_ = xywh_to_yxyx(bboxes)\n    ious = batch_iou_fast(anchors, bboxes_)\n    with self.test_session() as sess:\n      ious, bboxes_ = sess.run([ious, bboxes],\n                                        feed_dict={bboxes: [[599.37, 212.45, 27.62, 25.34],\n                                                            config.ANCHOR_SHAPE[2]]}\n                                        )\n      print(""ious:"", ious)\n      print(""max iou idx:"", np.argmax(ious, axis=-1))\n      print(""bboxes:"", bboxes_)\n\n  def test_set_anchors(self):\n    with self.test_session() as sess:\n      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n      output = sess.run(anchors)\n      print(np.shape(output))\n      self.assertAllEqual(np.shape(output), [config.FEA_HEIGHT * config.FEA_WIDTH * config.NUM_ANCHORS, 4])\n      print(""Anchors:"", output)\n      print(""Anchors shape:"", np.shape(output))\n      print(""Num of anchors:"", config.NUM_ANCHORS)\n\n  def test_arg_closest_anchor(self):\n    with self.test_session() as sess:\n      bbox_1 = tf.convert_to_tensor([10, 10, 20, 20], dtype=tf.float32)\n      bbox_2 = tf.convert_to_tensor([110, 110, 30, 30], dtype=tf.float32)\n      bboxes = tf.stack([bbox_1, bbox_2], axis=0)\n\n      anchor_1 = tf.convert_to_tensor([0,0,10,10], dtype=tf.float32)\n      anchor_2 = tf.convert_to_tensor([100,100,110,110], dtype=tf.float32)\n      anchors = tf.stack([anchor_1, anchor_2], axis=0)\n\n      indices = arg_closest_anchor(bboxes, anchors)\n      output = sess.run(indices)\n      print(\'test_arg_closest_anchor\')\n      print(output)\n\n\n  def test_update_tensor(self):\n    with self.test_session() as sess:\n      ref = tf.placeholder(dtype=tf.int64, shape=[None])#tf.convert_to_tensor([1, 2, 3], dtype=tf.int64)\n      indices = tf.convert_to_tensor([2], dtype=tf.int64)\n      update = tf.convert_to_tensor([9], dtype=tf.int64)\n      tensor_updated = update_tensor(ref, indices, update)\n      output = sess.run(tensor_updated, feed_dict={ref: [1, 2, 3]})\n      print(""test update tensor:"")\n      print(""tensor updated"", output)\n\n  def test_encode_annos(self):\n    with open(""/home/zehao/PycharmProjects/MobileNet/utils/test_data.pkl"", ""rb"") as fin:\n      test_data = pickle.load(fin)\n\n    with self.test_session() as sess:\n      anchors = tf.convert_to_tensor(config.ANCHOR_SHAPE, dtype=tf.float32)\n\n      num_image = len(test_data[""test_bbox""])\n      for i in range(50):\n        bboxes = tf.convert_to_tensor(test_data[""test_bbox""][i][0], dtype=tf.float32)\n        bboxes = xywh_to_yxyx(bboxes)\n        labels = tf.convert_to_tensor(test_data[""test_label""][i][0])\n\n        input_mask, labels_input, box_delta_input, box_input = \\\n          encode_annos(labels, bboxes, anchors, config.NUM_CLASSES)\n\n        out_input_mask, out_labels_input, out_box_delta_input, out_box_input, out_anchors = \\\n          sess.run([input_mask, labels_input, box_delta_input, box_input, anchors])\n\n\n\n        print(""num_bbox:"", np.shape(test_data[""test_bbox""][i][0])[0])\n\n        sd_indices = np.where(test_data[""test_input_mask""][i][0] > 0)[1]\n        print(""SDet:"")\n        print(""indices:"", sd_indices)\n        print(""mask:"", np.where(test_data[""test_input_mask""][i][0] > 0)[1])\n        print(""bbox:"", test_data[""test_bbox""][i][0])\n        print(""label:"", test_data[""test_label""][i][0])\n        print(""delta:"", test_data[""test_input_delta""][i][0][0][sd_indices])\n        print(""first:"", sd_indices[0], test_data[""test_input_bbox""][i][0][0][sd_indices[0]], test_data[""test_input_delta""][i][0][0][sd_indices[0]])\n\n        indices = np.where(out_input_mask > 0)[0]\n        print(""Mine:"")\n        print(""indices:"", indices)\n        print(""mask:"", np.where(out_input_mask > 0)[0])\n        print(""bbox:"", out_box_input[indices])\n        print(""label:"", out_labels_input[indices])\n        print(""delta:"", out_box_delta_input[indices])\n        print(""first:"", indices[0], out_box_input[indices[0]], out_box_delta_input[indices[0]])\n\n        print(""\\n"")\n        # print(""bbox:"", out_box_input[indices])\n        # aidx = np.where(test_data[""test_input_mask""][i][0] > 0)[1]\n        # encode_idx = np.where(out_input_mask > 0)[0]\n        # flag = False\n        # if np.shape(aidx)[0] != np.shape(encode_idx)[0]:\n        #   flag = True\n        # elif not np.alltrue(np.equal(aidx, encode_idx)):\n        #   flag = True\n        #   error_bidx = np.where(aidx != encode_idx)\n        #   true_aidx = aidx[error_bidx]\n        #   error_aidx = encode_idx[error_bidx]\n        # if flag:\n        #   image = test_data[""test_image""][i][0]\n        #   for b in range(np.shape(test_data[""test_bbox""][i][0])[0]):\n        #     bboxes = test_data[""test_bbox""][i][0]\n        #     bbox = bboxes[b]\n        #     x = bbox[0]\n        #     y = bbox[1]\n        #     w = bbox[2]\n        #     h = bbox[3]\n        #     x1 = x-0.5*w\n        #     y1 = y-0.5*h\n        #     x2 = x+0.5*w\n        #     y2 = y+0.5*h\n        #     color = (255,0,0)\n        #     cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness=2)\n        #     if np.any(error_bidx[0] == b):#b in error_bidx:\n        #       for a in config.ANCHOR_SHAPE[true_aidx]:\n        #         true_a = a\n        #         x = true_a[0]\n        #         y = true_a[1]\n        #         w = true_a[2]\n        #         h = true_a[3]\n        #         x1 = x - 0.5 * w\n        #         y1 = y - 0.5 * h\n        #         x2 = x + 0.5 * w\n        #         y2 = y + 0.5 * h\n        #         color = (0, 255, 255)\n        #         cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness=2)\n        #       for ea in config.ANCHOR_SHAPE[error_aidx]:\n        #         error_a = ea\n        #         x = error_a[0]\n        #         y = error_a[1]\n        #         w = error_a[2]\n        #         h = error_a[3]\n        #         x1 = x - 0.5 * w\n        #         y1 = y - 0.5 * h\n        #         x2 = x + 0.5 * w\n        #         y2 = y + 0.5 * h\n        #         color = (255, 255, 0)\n        #         cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, thickness=2)\n        #   # cv2.imwrite(""img"" + str(b) + "".jpg"", image)\n        #   cv2.imshow(""img"", image)\n        #   cv2.waitKey(0)\n\n  def test_set_anchors(self):\n    anchors = config.ANCHOR_SHAPE\n    image = np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH, 3))\n    num_anchors = np.shape(anchors)[0]\n    for i in range(num_anchors):\n      anchor = anchors[i]\n      x = anchor[0]\n      y = anchor[1]\n      w = anchor[2]\n      h = anchor[3]\n      x1 = x - 0.5*w\n      y1 = y - 0.5*h\n      x2 = x + 0.5*w\n      y2 = y + 0.5*h\n      cv2.rectangle(image,\n                    (int(x1), int(y1)),\n                    (int(x2), int(y2)),\n                    (255,255,255)\n                    )\n      cv2.rectangle(image,\n                    (int(739.72003), int(181.11)),\n                    (int(770.04), int(204.92)),\n                    (255, 0, 0),\n                    2)\n      if i == 2313:\n        cv2.rectangle(image,\n                      (int(x1), int(y1)),\n                      (int(x2), int(y2)),\n                      (0, 255, 255),\n                      2\n                      )\n    cv2.imshow(""anchors"", image)\n    cv2.waitKey(0)'"
