file_path,api_count,code
src/__init__.py,0,b''
src/chess_zero/__init__.py,0,b''
src/chess_zero/config.py,0,"b'""""""\nEverything related to configuration of running this application\n""""""\n\nimport os\nimport numpy as np\n\n\nclass PlayWithHumanConfig:\n    """"""\n    Config for allowing human to play against an agent using uci\n\n    """"""\n    def __init__(self):\n        self.simulation_num_per_move = 1200\n        self.threads_multiplier = 2\n        self.c_puct = 1 # lower  = prefer mean action value\n        self.noise_eps = 0\n        self.tau_decay_rate = 0  # start deterministic mode\n        self.resign_threshold = None\n\n    def update_play_config(self, pc):\n        """"""\n        :param PlayConfig pc:\n        :return:\n        """"""\n        pc.simulation_num_per_move = self.simulation_num_per_move\n        pc.search_threads *= self.threads_multiplier\n        pc.c_puct = self.c_puct\n        pc.noise_eps = self.noise_eps\n        pc.tau_decay_rate = self.tau_decay_rate\n        pc.resign_threshold = self.resign_threshold\n        pc.max_game_length = 999999\n\n\nclass Options:\n    new = False\n\n\nclass ResourceConfig:\n    """"""\n    Config describing all of the directories and resources needed during running this project\n    """"""\n    def __init__(self):\n        self.project_dir = os.environ.get(""PROJECT_DIR"", _project_dir())\n        self.data_dir = os.environ.get(""DATA_DIR"", _data_dir())\n\n        self.model_dir = os.environ.get(""MODEL_DIR"", os.path.join(self.data_dir, ""model""))\n        self.model_best_config_path = os.path.join(self.model_dir, ""model_best_config.json"")\n        self.model_best_weight_path = os.path.join(self.model_dir, ""model_best_weight.h5"")\n\n        self.model_best_distributed_ftp_server = ""alpha-chess-zero.mygamesonline.org""\n        self.model_best_distributed_ftp_user = ""2537576_chess""\n        self.model_best_distributed_ftp_password = ""alpha-chess-zero-2""\n        self.model_best_distributed_ftp_remote_path = ""/alpha-chess-zero.mygamesonline.org/""\n\n        self.next_generation_model_dir = os.path.join(self.model_dir, ""next_generation"")\n        self.next_generation_model_dirname_tmpl = ""model_%s""\n        self.next_generation_model_config_filename = ""model_config.json""\n        self.next_generation_model_weight_filename = ""model_weight.h5""\n\n        self.play_data_dir = os.path.join(self.data_dir, ""play_data"")\n        self.play_data_filename_tmpl = ""play_%s.json""\n\n        self.log_dir = os.path.join(self.project_dir, ""logs"")\n        self.main_log_path = os.path.join(self.log_dir, ""main.log"")\n\n    def create_directories(self):\n        dirs = [self.project_dir, self.data_dir, self.model_dir, self.play_data_dir, self.log_dir,\n                self.next_generation_model_dir]\n        for d in dirs:\n            if not os.path.exists(d):\n                os.makedirs(d)\n\n\ndef flipped_uci_labels():\n    """"""\n    Seems to somehow transform the labels used for describing the universal chess interface format, putting\n    them into a returned list.\n    :return:\n    """"""\n    def repl(x):\n        return """".join([(str(9 - int(a)) if a.isdigit() else a) for a in x])\n\n    return [repl(x) for x in create_uci_labels()]\n\n\ndef create_uci_labels():\n    """"""\n    Creates the labels for the universal chess interface into an array and returns them\n    :return:\n    """"""\n    labels_array = []\n    letters = [\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\', \'h\']\n    numbers = [\'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\']\n    promoted_to = [\'q\', \'r\', \'b\', \'n\']\n\n    for l1 in range(8):\n        for n1 in range(8):\n            destinations = [(t, n1) for t in range(8)] + \\\n                           [(l1, t) for t in range(8)] + \\\n                           [(l1 + t, n1 + t) for t in range(-7, 8)] + \\\n                           [(l1 + t, n1 - t) for t in range(-7, 8)] + \\\n                           [(l1 + a, n1 + b) for (a, b) in\n                            [(-2, -1), (-1, -2), (-2, 1), (1, -2), (2, -1), (-1, 2), (2, 1), (1, 2)]]\n            for (l2, n2) in destinations:\n                if (l1, n1) != (l2, n2) and l2 in range(8) and n2 in range(8):\n                    move = letters[l1] + numbers[n1] + letters[l2] + numbers[n2]\n                    labels_array.append(move)\n    for l1 in range(8):\n        l = letters[l1]\n        for p in promoted_to:\n            labels_array.append(l + \'2\' + l + \'1\' + p)\n            labels_array.append(l + \'7\' + l + \'8\' + p)\n            if l1 > 0:\n                l_l = letters[l1 - 1]\n                labels_array.append(l + \'2\' + l_l + \'1\' + p)\n                labels_array.append(l + \'7\' + l_l + \'8\' + p)\n            if l1 < 7:\n                l_r = letters[l1 + 1]\n                labels_array.append(l + \'2\' + l_r + \'1\' + p)\n                labels_array.append(l + \'7\' + l_r + \'8\' + p)\n    return labels_array\n\n\nclass Config:\n    """"""\n    Config describing how to run the application\n\n    Attributes (best guess so far):\n        :ivar list(str) labels: labels to use for representing the game using UCI\n        :ivar int n_lables: number of labels\n        :ivar list(str) flipped_labels: some transformation of the labels\n        :ivar int unflipped_index: idk\n        :ivar Options opts: options to use to configure this config\n        :ivar ResourceConfig resources: resources used by this config.\n        :ivar ModelConfig mode: config for the model to use\n        :ivar PlayConfig play: configuration for the playing of the game\n        :ivar PlayDataConfig play_date: configuration for the saved data from playing\n        :ivar TrainerConfig trainer: config for how training should go\n        :ivar EvaluateConfig eval: config for how evaluation should be done\n    """"""\n    labels = create_uci_labels()\n    n_labels = int(len(labels))\n    flipped_labels = flipped_uci_labels()\n    unflipped_index = None\n\n    def __init__(self, config_type=""mini""):\n        """"""\n\n        :param str config_type: one of ""mini"", ""normal"", or ""distributed"", representing the set of\n            configs to use for all of the config attributes. Mini is a small version, normal is the\n            larger version, and distributed is a version which runs across multiple GPUs it seems\n        """"""\n        self.opts = Options()\n        self.resource = ResourceConfig()\n\n        if config_type == ""mini"":\n            import chess_zero.configs.mini as c\n        elif config_type == ""normal"":\n            import chess_zero.configs.normal as c\n        elif config_type == ""distributed"":\n            import chess_zero.configs.distributed as c\n        else:\n            raise RuntimeError(f""unknown config_type: {config_type}"")\n        self.model = c.ModelConfig()\n        self.play = c.PlayConfig()\n        self.play_data = c.PlayDataConfig()\n        self.trainer = c.TrainerConfig()\n        self.eval = c.EvaluateConfig()\n        self.labels = Config.labels\n        self.n_labels = Config.n_labels\n        self.flipped_labels = Config.flipped_labels\n\n    @staticmethod\n    def flip_policy(pol):\n        """"""\n\n        :param pol policy to flip:\n        :return: the policy, flipped (for switching between black and white it seems)\n        """"""\n        return np.asarray([pol[ind] for ind in Config.unflipped_index])\n\n\nConfig.unflipped_index = [Config.labels.index(x) for x in Config.flipped_labels]\n\n\n# print(Config.labels)\n# print(Config.flipped_labels)\n\n\ndef _project_dir():\n    d = os.path.dirname\n    return d(d(d(os.path.abspath(__file__))))\n\n\ndef _data_dir():\n    return os.path.join(_project_dir(), ""data"")'"
src/chess_zero/manager.py,0,"b'""""""\nManages starting off each of the separate processes involved in ChessZero -\nself play, training, and evaluation.\n""""""\nimport argparse\n\nfrom logging import getLogger,disable\n\nfrom .lib.logger import setup_logger\nfrom .config import Config\n\nlogger = getLogger(__name__)\n\nCMD_LIST = [\'self\', \'opt\', \'eval\', \'sl\', \'uci\']\n\n\ndef create_parser():\n    """"""\n    Parses each of the arguments from the command line\n    :return ArgumentParser representing the command line arguments that were supplied to the command line:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--cmd"", help=""what to do"", choices=CMD_LIST)\n    parser.add_argument(""--new"", help=""run from new best model"", action=""store_true"")\n    parser.add_argument(""--type"", help=""use normal setting"", default=""mini"")\n    parser.add_argument(""--total-step"", help=""set TrainerConfig.start_total_steps"", type=int)\n    return parser\n\n\ndef setup(config: Config, args):\n    """"""\n    Sets up a new config by creating the required directories and setting up logging.\n\n    :param Config config: config to create directories for and to set config from based on the args\n    :param ArgumentParser args: args to use to control config.\n    """"""\n    config.opts.new = args.new\n    if args.total_step is not None:\n        config.trainer.start_total_steps = args.total_step\n    config.resource.create_directories()\n    setup_logger(config.resource.main_log_path)\n\n\ndef start():\n    """"""\n    Starts one of the processes based on command line arguments.\n\n    :return : the worker class that was started\n    """"""\n    parser = create_parser()\n    args = parser.parse_args()\n    config_type = args.type\n\n    if args.cmd == \'uci\':\n        disable(999999) # plz don\'t interfere with uci\n\n    config = Config(config_type=config_type)\n    setup(config, args)\n\n    logger.info(f""config type: {config_type}"")\n\n    if args.cmd == \'self\':\n        from .worker import self_play\n        return self_play.start(config)\n    elif args.cmd == \'opt\':\n        from .worker import optimize\n        return optimize.start(config)\n    elif args.cmd == \'eval\':\n        from .worker import evaluate\n        return evaluate.start(config)\n    elif args.cmd == \'sl\':\n        from .worker import sl\n        return sl.start(config)\n    elif args.cmd == \'uci\':\n        from .play_game import uci\n        return uci.start(config)\n'"
src/chess_zero/run.py,0,"b'""""""\nMain entry point for running from command line.\n""""""\n\nimport os\nimport sys\nimport multiprocessing as mp\n\n_PATH_ = os.path.dirname(os.path.dirname(__file__))\n\n\nif _PATH_ not in sys.path:\n    sys.path.append(_PATH_)\n\n\nif __name__ == ""__main__"":\n    mp.set_start_method(\'spawn\')\n    sys.setrecursionlimit(10000)\n    from chess_zero import manager\n    manager.start()\n'"
src/chess_zero/agent/__init__.py,0,b''
src/chess_zero/agent/api_chess.py,0,"b'""""""\nDefines the process which will listen on the pipe for\nan observation of the game state and return a prediction from the policy and\nvalue network.\n""""""\nfrom multiprocessing import connection, Pipe\nfrom threading import Thread\n\nimport numpy as np\n\nfrom chess_zero.config import Config\n\n\nclass ChessModelAPI:\n    """"""\n    Defines the process which will listen on the pipe for\n    an observation of the game state and return the predictions from the policy and\n    value networks.\n    Attributes:\n        :ivar ChessModel agent_model: ChessModel to use to make predictions.\n        :ivar list(Connection): list of pipe connections to listen for states on and return predictions on.\n    """"""\n    # noinspection PyUnusedLocal\n    def __init__(self, agent_model):  # ChessModel\n        """"""\n\n        :param ChessModel agent_model: trained model to use to make predictions\n        """"""\n        self.agent_model = agent_model\n        self.pipes = []\n\n    def start(self):\n        """"""\n        Starts a thread to listen on the pipe and make predictions\n        :return:\n        """"""\n        prediction_worker = Thread(target=self._predict_batch_worker, name=""prediction_worker"")\n        prediction_worker.daemon = True\n        prediction_worker.start()\n\n    def create_pipe(self):\n        """"""\n        Creates a new two-way pipe and returns the connection to one end of it (the other will be used\n        by this class)\n        :return Connection: the other end of this pipe.\n        """"""\n        me, you = Pipe()\n        self.pipes.append(me)\n        return you\n\n    def _predict_batch_worker(self):\n        """"""\n        Thread worker which listens on each pipe in self.pipes for an observation, and then outputs\n        the predictions for the policy and value networks when the observations come in. Repeats.\n        """"""\n        while True:\n            ready = connection.wait(self.pipes,timeout=0.001)\n            if not ready:\n                continue\n            data, result_pipes = [], []\n            for pipe in ready:\n                while pipe.poll():\n                    data.append(pipe.recv())\n                    result_pipes.append(pipe)\n\n            data = np.asarray(data, dtype=np.float32)\n            policy_ary, value_ary = self.agent_model.model.predict_on_batch(data)\n            for pipe, p, v in zip(result_pipes, policy_ary, value_ary):\n                pipe.send((p, float(v)))\n'"
src/chess_zero/agent/model_chess.py,0,"b'""""""\nDefines the actual model for making policy and value predictions given an observation.\n""""""\n\nimport ftplib\nimport hashlib\nimport json\nimport os\nfrom logging import getLogger\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.core import Activation, Dense, Flatten\nfrom keras.layers.merge import Add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\n\nfrom chess_zero.agent.api_chess import ChessModelAPI\nfrom chess_zero.config import Config\n\n# noinspection PyPep8Naming\n\nlogger = getLogger(__name__)\n\n\nclass ChessModel:\n    """"""\n    The model which can be trained to take observations of a game of chess and return value and policy\n    predictions.\n\n    Attributes:\n        :ivar Config config: configuration to use\n        :ivar Model model: the Keras model to use for predictions\n        :ivar digest: basically just a hash of the file containing the weights being used by this model\n        :ivar ChessModelAPI api: the api to use to listen for and then return this models predictions (on a pipe).\n    """"""\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None  # type: Model\n        self.digest = None\n        self.api = None\n\n    def get_pipes(self, num = 1):\n        """"""\n        Creates a list of pipes on which observations of the game state will be listened for. Whenever\n        an observation comes in, returns policy and value network predictions on that pipe.\n\n        :param int num: number of pipes to create\n        :return str(Connection): a list of all connections to the pipes that were created\n        """"""\n        if self.api is None:\n            self.api = ChessModelAPI(self)\n            self.api.start()\n        return [self.api.create_pipe() for _ in range(num)]\n\n    def build(self):\n        """"""\n        Builds the full Keras model and stores it in self.model.\n        """"""\n        mc = self.config.model\n        in_x = x = Input((18, 8, 8))\n\n        # (batch, channels, height, width)\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_first_filter_size, padding=""same"",\n                   data_format=""channels_first"", use_bias=False, kernel_regularizer=l2(mc.l2_reg),\n                   name=""input_conv-""+str(mc.cnn_first_filter_size)+""-""+str(mc.cnn_filter_num))(x)\n        x = BatchNormalization(axis=1, name=""input_batchnorm"")(x)\n        x = Activation(""relu"", name=""input_relu"")(x)\n\n        for i in range(mc.res_layer_num):\n            x = self._build_residual_block(x, i + 1)\n\n        res_out = x\n        \n        # for policy output\n        x = Conv2D(filters=2, kernel_size=1, data_format=""channels_first"", use_bias=False, kernel_regularizer=l2(mc.l2_reg),\n                    name=""policy_conv-1-2"")(res_out)\n        x = BatchNormalization(axis=1, name=""policy_batchnorm"")(x)\n        x = Activation(""relu"", name=""policy_relu"")(x)\n        x = Flatten(name=""policy_flatten"")(x)\n        # no output for \'pass\'\n        policy_out = Dense(self.config.n_labels, kernel_regularizer=l2(mc.l2_reg), activation=""softmax"", name=""policy_out"")(x)\n\n        # for value output\n        x = Conv2D(filters=4, kernel_size=1, data_format=""channels_first"", use_bias=False, kernel_regularizer=l2(mc.l2_reg),\n                    name=""value_conv-1-4"")(res_out)\n        x = BatchNormalization(axis=1, name=""value_batchnorm"")(x)\n        x = Activation(""relu"",name=""value_relu"")(x)\n        x = Flatten(name=""value_flatten"")(x)\n        x = Dense(mc.value_fc_size, kernel_regularizer=l2(mc.l2_reg), activation=""relu"", name=""value_dense"")(x)\n        value_out = Dense(1, kernel_regularizer=l2(mc.l2_reg), activation=""tanh"", name=""value_out"")(x)\n\n        self.model = Model(in_x, [policy_out, value_out], name=""chess_model"")\n\n    def _build_residual_block(self, x, index):\n        mc = self.config.model\n        in_x = x\n        res_name = ""res""+str(index)\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_filter_size, padding=""same"",\n                   data_format=""channels_first"", use_bias=False, kernel_regularizer=l2(mc.l2_reg), \n                   name=res_name+""_conv1-""+str(mc.cnn_filter_size)+""-""+str(mc.cnn_filter_num))(x)\n        x = BatchNormalization(axis=1, name=res_name+""_batchnorm1"")(x)\n        x = Activation(""relu"",name=res_name+""_relu1"")(x)\n        x = Conv2D(filters=mc.cnn_filter_num, kernel_size=mc.cnn_filter_size, padding=""same"",\n                   data_format=""channels_first"", use_bias=False, kernel_regularizer=l2(mc.l2_reg), \n                   name=res_name+""_conv2-""+str(mc.cnn_filter_size)+""-""+str(mc.cnn_filter_num))(x)\n        x = BatchNormalization(axis=1, name=""res""+str(index)+""_batchnorm2"")(x)\n        x = Add(name=res_name+""_add"")([in_x, x])\n        x = Activation(""relu"", name=res_name+""_relu2"")(x)\n        return x\n\n    @staticmethod\n    def fetch_digest(weight_path):\n        if os.path.exists(weight_path):\n            m = hashlib.sha256()\n            with open(weight_path, ""rb"") as f:\n                m.update(f.read())\n            return m.hexdigest()\n\n    def load(self, config_path, weight_path):\n        """"""\n\n        :param str config_path: path to the file containing the entire configuration\n        :param str weight_path: path to the file containing the model weights\n        :return: true iff successful in loading\n        """"""\n        mc = self.config.model\n        resources = self.config.resource\n        if mc.distributed and config_path == resources.model_best_config_path:\n            try:\n                logger.debug(""loading model from server"")\n                ftp_connection = ftplib.FTP(resources.model_best_distributed_ftp_server,\n                                            resources.model_best_distributed_ftp_user,\n                                            resources.model_best_distributed_ftp_password)\n                ftp_connection.cwd(resources.model_best_distributed_ftp_remote_path)\n                ftp_connection.retrbinary(""RETR model_best_config.json"", open(config_path, \'wb\').write)\n                ftp_connection.retrbinary(""RETR model_best_weight.h5"", open(weight_path, \'wb\').write)\n                ftp_connection.quit()\n            except:\n                pass\n        if os.path.exists(config_path) and os.path.exists(weight_path):\n            logger.debug(f""loading model from {config_path}"")\n            with open(config_path, ""rt"") as f:\n                self.model = Model.from_config(json.load(f))\n            self.model.load_weights(weight_path)\n            self.model._make_predict_function()\n            self.digest = self.fetch_digest(weight_path)\n            logger.debug(f""loaded model digest = {self.digest}"")\n            return True\n        else:\n            logger.debug(f""model files does not exist at {config_path} and {weight_path}"")\n            return False\n\n    def save(self, config_path, weight_path):\n        """"""\n\n        :param str config_path: path to save the entire configuration to\n        :param str weight_path: path to save the model weights to\n        """"""\n        logger.debug(f""save model to {config_path}"")\n        with open(config_path, ""wt"") as f:\n            json.dump(self.model.get_config(), f)\n            self.model.save_weights(weight_path)\n        self.digest = self.fetch_digest(weight_path)\n        logger.debug(f""saved model digest {self.digest}"")\n\n        mc = self.config.model\n        resources = self.config.resource\n        if mc.distributed and config_path == resources.model_best_config_path:\n            try:\n                logger.debug(""saving model to server"")\n                ftp_connection = ftplib.FTP(resources.model_best_distributed_ftp_server,\n                                            resources.model_best_distributed_ftp_user,\n                                            resources.model_best_distributed_ftp_password)\n                ftp_connection.cwd(resources.model_best_distributed_ftp_remote_path)\n                fh = open(config_path, \'rb\')\n                ftp_connection.storbinary(\'STOR model_best_config.json\', fh)\n                fh.close()\n\n                fh = open(weight_path, \'rb\')\n                ftp_connection.storbinary(\'STOR model_best_weight.h5\', fh)\n                fh.close()\n                ftp_connection.quit()\n            except:\n                pass\n'"
src/chess_zero/agent/player_chess.py,0,"b'""""""\nThis encapsulates all of the functionality related to actually playing the game itself, not just\nmaking / training predictions.\n""""""\nfrom collections import defaultdict\nfrom concurrent.futures import ThreadPoolExecutor\nfrom logging import getLogger\nfrom threading import Lock\n\nimport chess\nimport numpy as np\n\nfrom chess_zero.config import Config\nfrom chess_zero.env.chess_env import ChessEnv, Winner\n\nlogger = getLogger(__name__)\n\n\n# these are from AGZ nature paper\nclass VisitStats:\n    """"""\n    Holds information for use by the AGZ MCTS algorithm on all moves from a given game state (this is generally used inside\n    of a defaultdict where a game state in FEN format maps to a VisitStats object).\n    Attributes:\n        :ivar defaultdict(ActionStats) a: known stats for all actions to take from the the state represented by\n            this visitstats.\n        :ivar int sum_n: sum of the n value for each of the actions in self.a, representing total\n            visits over all actions in self.a.\n    """"""\n    def __init__(self):\n        self.a = defaultdict(ActionStats)\n        self.sum_n = 0\n\n\nclass ActionStats:\n    """"""\n    Holds the stats needed for the AGZ MCTS algorithm for a specific action taken from a specific state.\n\n    Attributes:\n        :ivar int n: number of visits to this action by the algorithm\n        :ivar float w: every time a child of this action is visited by the algorithm,\n            this accumulates the value (calculated from the value network) of that child. This is modified\n            by a virtual loss which encourages threads to explore different nodes.\n        :ivar float q: mean action value (total value from all visits to actions\n            AFTER this action, divided by the total number of visits to this action)\n            i.e. it\'s just w / n.\n        :ivar float p: prior probability of taking this action, given\n            by the policy network.\n\n    """"""\n    def __init__(self):\n        self.n = 0\n        self.w = 0\n        self.q = 0\n        self.p = 0\n\n\nclass ChessPlayer:\n    """"""\n    Plays the actual game of chess, choosing moves based on policy and value network predictions coming\n    from a learned model on the other side of a pipe.\n\n    Attributes:\n        :ivar list: stores info on the moves that have been performed during the game\n        :ivar Config config: stores the whole config for how to run\n        :ivar PlayConfig play_config: just stores the PlayConfig to use to play the game. Taken from the config\n            if not specifically specified.\n        :ivar int labels_n: length of self.labels.\n        :ivar list(str) labels: all of the possible move labels (like a1b1, a1c1, etc...)\n        :ivar dict(str,int) move_lookup: dict from move label to its index in self.labels\n        :ivar list(Connection) pipe_pool: the pipes to send the observations of the game to to get back\n            value and policy predictions from\n        :ivar dict(str,Lock) node_lock: dict from FEN game state to a Lock, indicating\n            whether that state is currently being explored by another thread.\n        :ivar VisitStats tree: holds all of the visited game states and actions\n            during the running of the AGZ algorithm\n    """"""\n    # dot = False\n    def __init__(self, config: Config, pipes=None, play_config=None, dummy=False):\n        self.moves = []\n\n        self.tree = defaultdict(VisitStats)\n        self.config = config\n        self.play_config = play_config or self.config.play\n        self.labels_n = config.n_labels\n        self.labels = config.labels\n        self.move_lookup = {chess.Move.from_uci(move): i for move, i in zip(self.labels, range(self.labels_n))}\n        if dummy:\n            return\n\n        self.pipe_pool = pipes\n        self.node_lock = defaultdict(Lock)\n\n    def reset(self):\n        """"""\n        reset the tree to begin a new exploration of states\n        """"""\n        self.tree = defaultdict(VisitStats)\n\n    def deboog(self, env):\n        print(env.testeval())\n\n        state = state_key(env)\n        my_visit_stats = self.tree[state]\n        stats = []\n        for action, a_s in my_visit_stats.a.items():\n            moi = self.move_lookup[action]\n            stats.append(np.asarray([a_s.n, a_s.w, a_s.q, a_s.p, moi]))\n        stats = np.asarray(stats)\n        a = stats[stats[:,0].argsort()[::-1]]\n\n        for s in a:\n            print(f\'{self.labels[int(s[4])]:5}: \'\n                  f\'n: {s[0]:3.0f} \'\n                  f\'w: {s[1]:7.3f} \'\n                  f\'q: {s[2]:7.3f} \'\n                  f\'p: {s[3]:7.5f}\')\n\n    def action(self, env, can_stop = True) -> str:\n        """"""\n        Figures out the next best move\n        within the specified environment and returns a string describing the action to take.\n\n        :param ChessEnv env: environment in which to figure out the action\n        :param boolean can_stop: whether we are allowed to take no action (return None)\n        :return: None if no action should be taken (indicating a resign). Otherwise, returns a string\n            indicating the action to take in uci format\n        """"""\n        self.reset()\n\n        # for tl in range(self.play_config.thinking_loop):\n        root_value, naked_value = self.search_moves(env)\n        policy = self.calc_policy(env)\n        my_action = int(np.random.choice(range(self.labels_n), p = self.apply_temperature(policy, env.num_halfmoves)))\n\n        if can_stop and self.play_config.resign_threshold is not None and \\\n                        root_value <= self.play_config.resign_threshold \\\n                        and env.num_halfmoves > self.play_config.min_resign_turn:\n            # noinspection PyTypeChecker\n            return None\n        else:\n            self.moves.append([env.observation, list(policy)])\n            return self.config.labels[my_action]\n\n    def search_moves(self, env) -> (float, float):\n        """"""\n        Looks at all the possible moves using the AGZ MCTS algorithm\n         and finds the highest value possible move. Does so using multiple threads to get multiple\n         estimates from the AGZ MCTS algorithm so we can pick the best.\n\n        :param ChessEnv env: env to search for moves within\n        :return (float,float): the maximum value of all values predicted by each thread,\n            and the first value that was predicted.\n        """"""\n        futures = []\n        with ThreadPoolExecutor(max_workers=self.play_config.search_threads) as executor:\n            for _ in range(self.play_config.simulation_num_per_move):\n                futures.append(executor.submit(self.search_my_move,env=env.copy(),is_root_node=True))\n\n        vals = [f.result() for f in futures]\n\n        return np.max(vals), vals[0] # vals[0] is kind of racy\n\n    def search_my_move(self, env: ChessEnv, is_root_node=False) -> float:\n        """"""\n        Q, V is value for this Player(always white).\n        P is value for the player of next_player (black or white)\n\n        This method searches for possible moves, adds them to a search tree, and eventually returns the\n        best move that was found during the search.\n\n        :param ChessEnv env: environment in which to search for the move\n        :param boolean is_root_node: whether this is the root node of the search.\n        :return float: value of the move. This is calculated by getting a prediction\n            from the value network.\n        """"""\n        if env.done:\n            if env.winner == Winner.draw:\n                return 0\n            # assert env.whitewon != env.white_to_move # side to move can\'t be winner!\n            return -1\n\n        state = state_key(env)\n\n        with self.node_lock[state]:\n            if state not in self.tree:\n                leaf_p, leaf_v = self.expand_and_evaluate(env)\n                self.tree[state].p = leaf_p\n                return leaf_v # I\'m returning everything from the POV of side to move\n\n            # SELECT STEP\n            action_t = self.select_action_q_and_u(env, is_root_node)\n\n            virtual_loss = self.play_config.virtual_loss\n\n            my_visit_stats = self.tree[state]\n            my_stats = my_visit_stats.a[action_t]\n\n            my_visit_stats.sum_n += virtual_loss\n            my_stats.n += virtual_loss\n            my_stats.w += -virtual_loss\n            my_stats.q = my_stats.w / my_stats.n\n\n        env.step(action_t.uci())\n        leaf_v = self.search_my_move(env)  # next move from enemy POV\n        leaf_v = -leaf_v\n\n        # BACKUP STEP\n        # on returning search path\n        # update: N, W, Q\n        with self.node_lock[state]:\n            my_visit_stats.sum_n += -virtual_loss + 1\n            my_stats.n += -virtual_loss + 1\n            my_stats.w += virtual_loss + leaf_v\n            my_stats.q = my_stats.w / my_stats.n\n\n        return leaf_v\n\n    def expand_and_evaluate(self, env) -> (np.ndarray, float):\n        """""" expand new leaf, this is called only once per state\n        this is called with state locked\n        insert P(a|s), return leaf_v\n\n        This gets a prediction for the policy and value of the state within the given env\n        :return (float, float): the policy and value predictions for this state\n        """"""\n        state_planes = env.canonical_input_planes()\n\n        leaf_p, leaf_v = self.predict(state_planes)\n        # these are canonical policy and value (i.e. side to move is ""white"")\n\n        if not env.white_to_move:\n            leaf_p = Config.flip_policy(leaf_p) # get it back to python-chess form\n\n        return leaf_p, leaf_v\n\n    def predict(self, state_planes):\n        """"""\n        Gets a prediction from the policy and value network\n        :param state_planes: the observation state represented as planes\n        :return (float,float): policy (prior probability of taking the action leading to this state)\n            and value network (value of the state) prediction for this state.\n        """"""\n        pipe = self.pipe_pool.pop()\n        pipe.send(state_planes)\n        ret = pipe.recv()\n        self.pipe_pool.append(pipe)\n        return ret\n\n    #@profile\n    def select_action_q_and_u(self, env, is_root_node) -> chess.Move:\n        """"""\n        Picks the next action to explore using the AGZ MCTS algorithm.\n\n        Picks based on the action which maximizes the maximum action value\n        (ActionStats.q) + an upper confidence bound on that action.\n\n        :param Environment env: env to look for the next moves within\n        :param is_root_node: whether this is for the root node of the MCTS search.\n        :return chess.Move: the move to explore\n        """"""\n        # this method is called with state locked\n        state = state_key(env)\n\n        my_visitstats = self.tree[state]\n\n        if my_visitstats.p is not None: #push p to edges\n            tot_p = 1e-8\n            for mov in env.board.legal_moves:\n                mov_p = my_visitstats.p[self.move_lookup[mov]]\n                my_visitstats.a[mov].p = mov_p\n                tot_p += mov_p\n            for a_s in my_visitstats.a.values():\n                a_s.p /= tot_p\n            my_visitstats.p = None\n\n        xx_ = np.sqrt(my_visitstats.sum_n + 1)  # sqrt of sum(N(s, b); for all b)\n\n        e = self.play_config.noise_eps\n        c_puct = self.play_config.c_puct\n        dir_alpha = self.play_config.dirichlet_alpha\n\n        best_s = -999\n        best_a = None\n        if is_root_node:\n            noise = np.random.dirichlet([dir_alpha] * len(my_visitstats.a))\n        \n        i = 0\n        for action, a_s in my_visitstats.a.items():\n            p_ = a_s.p\n            if is_root_node:\n                p_ = (1-e) * p_ + e * noise[i]\n                i += 1\n            b = a_s.q + c_puct * p_ * xx_ / (1 + a_s.n)\n            if b > best_s:\n                best_s = b\n                best_a = action\n\n        return best_a\n\n    def apply_temperature(self, policy, turn):\n        """"""\n        Applies a random fluctuation to probability of choosing various actions\n        :param policy: list of probabilities of taking each action\n        :param turn: number of turns that have occurred in the game so far\n        :return: policy, randomly perturbed based on the temperature. High temp = more perturbation. Low temp\n            = less.\n        """"""\n        tau = np.power(self.play_config.tau_decay_rate, turn + 1)\n        if tau < 0.1:\n            tau = 0\n        if tau == 0:\n            action = np.argmax(policy)\n            ret = np.zeros(self.labels_n)\n            ret[action] = 1.0\n            return ret\n        else:\n            ret = np.power(policy, 1/tau)\n            ret /= np.sum(ret)\n            return ret\n\n    def calc_policy(self, env):\n        """"""calc \xcf\x80(a|s0)\n        :return list(float): a list of probabilities of taking each action, calculated based on visit counts.\n        """"""\n        state = state_key(env)\n        my_visitstats = self.tree[state]\n        policy = np.zeros(self.labels_n)\n        for action, a_s in my_visitstats.a.items():\n            policy[self.move_lookup[action]] = a_s.n\n\n        policy /= np.sum(policy)\n        return policy\n\n    def sl_action(self, observation, my_action, weight=1):\n        """"""\n        Logs the action in self.moves. Useful for generating a game using game data.\n\n        :param str observation: FEN format observation indicating the game state\n        :param str my_action: uci format action to take\n        :param float weight: weight to assign to the taken action when logging it in self.moves\n        :return str: the action, unmodified.\n        """"""\n        policy = np.zeros(self.labels_n)\n\n        k = self.move_lookup[chess.Move.from_uci(my_action)]\n        policy[k] = weight\n\n        self.moves.append([observation, list(policy)])\n        return my_action\n\n    def finish_game(self, z):\n        """"""\n        When game is done, updates the value of all past moves based on the result.\n\n        :param self:\n        :param z: win=1, lose=-1, draw=0\n        :return:\n        """"""\n        for move in self.moves:  # add this game winner result to all past moves.\n            move += [z]\n\n\ndef state_key(env: ChessEnv) -> str:\n    """"""\n    :param ChessEnv env: env to encode\n    :return str: a str representation of the game state\n    """"""\n    fen = env.board.fen().rsplit(\' \', 1) # drop the move clock\n    return fen[0]\n'"
src/chess_zero/configs/__init__.py,0,b''
src/chess_zero/configs/distributed.py,0,"b'""""""\nContains the set of configs to use for the ""distributed"" version of the app, to use when running\nit in a distributed environment (with multiple GPUs)\n""""""\nclass EvaluateConfig:\n    def __init__(self):\n        self.vram_frac = 1.0\n        self.game_num = 50\n        self.replace_rate = 0.55\n        self.play_config = PlayConfig()\n        self.play_config.simulation_num_per_move = 200\n        self.play_config.thinking_loop = 1\n        self.play_config.c_puct = 1 # lower  = prefer mean action value\n        self.play_config.tau_decay_rate = 0.6 # I need a better distribution...\n        self.play_config.noise_eps = 0\n        self.evaluate_latest_first = True\n        self.max_game_length = 1000\n\n\nclass PlayDataConfig:\n    def __init__(self):\n        self.min_elo_policy = 500 # 0 weight\n        self.max_elo_policy = 1800 # 1 weight\n        self.sl_nb_game_in_file = 250\n        self.nb_game_in_file = 50\n        self.max_file_num = 150\n\n\nclass PlayConfig:\n    def __init__(self):\n        self.max_processes = 3\n        self.search_threads = 16\n        self.vram_frac = 1.0\n        self.simulation_num_per_move = 800\n        self.thinking_loop = 1\n        self.logging_thinking = False\n        self.c_puct = 1.5\n        self.noise_eps = 0.25\n        self.dirichlet_alpha = 0.3\n        self.tau_decay_rate = 0.99\n        self.virtual_loss = 3\n        self.resign_threshold = -0.8\n        self.min_resign_turn = 5\n        self.max_game_length = 1000\n\n\nclass TrainerConfig:\n    def __init__(self):\n        self.min_data_size_to_learn = 0\n        self.cleaning_processes = 5 # RAM explosion...\n        self.vram_frac = 1.0\n        self.batch_size = 384 # tune this to your gpu memory\n        self.epoch_to_checkpoint = 1\n        self.dataset_size = 100000\n        self.start_total_steps = 0\n        self.save_model_steps = 25\n        self.load_data_steps = 100\n        self.loss_weights = [1.25, 1.0] # [policy, value] prevent value overfit in SL\n\n\nclass ModelConfig:\n    cnn_filter_num = 256\n    cnn_first_filter_size = 5\n    cnn_filter_size = 3\n    res_layer_num = 7\n    l2_reg = 1e-4\n    value_fc_size = 256\n    distributed = True\n    input_depth = 18\n'"
src/chess_zero/configs/mini.py,0,"b'""""""\nContains the set of configs to use for the ""mini"" version of the app, which seems to be a smaller\nversion of the agent that is easier and quicker to run locally\n""""""\nclass EvaluateConfig:\n    def __init__(self):\n        self.vram_frac = 1.0\n        self.game_num = 50\n        self.replace_rate = 0.55\n        self.play_config = PlayConfig()\n        self.play_config.simulation_num_per_move = 200\n        self.play_config.thinking_loop = 1\n        self.play_config.c_puct = 1 # lower  = prefer mean action value\n        self.play_config.tau_decay_rate = 0.6 # I need a better distribution...\n        self.play_config.noise_eps = 0\n        self.evaluate_latest_first = True\n        self.max_game_length = 1000\n\n\nclass PlayDataConfig:\n    def __init__(self):\n        self.min_elo_policy = 500 # 0 weight\n        self.max_elo_policy = 1800 # 1 weight\n        self.sl_nb_game_in_file = 250\n        self.nb_game_in_file = 50\n        self.max_file_num = 150\n\n\nclass PlayConfig:\n    def __init__(self):\n        self.max_processes = 3\n        self.search_threads = 16\n        self.vram_frac = 1.0\n        self.simulation_num_per_move = 100\n        self.thinking_loop = 1\n        self.logging_thinking = False\n        self.c_puct = 1.5\n        self.noise_eps = 0.25\n        self.dirichlet_alpha = 0.3\n        self.tau_decay_rate = 0.99\n        self.virtual_loss = 3\n        self.resign_threshold = -0.8\n        self.min_resign_turn = 5\n        self.max_game_length = 1000\n\n\nclass TrainerConfig:\n    def __init__(self):\n        self.min_data_size_to_learn = 0\n        self.cleaning_processes = 5 # RAM explosion...\n        self.vram_frac = 1.0\n        self.batch_size = 384 # tune this to your gpu memory\n        self.epoch_to_checkpoint = 1\n        self.dataset_size = 100000\n        self.start_total_steps = 0\n        self.save_model_steps = 25\n        self.load_data_steps = 100\n        self.loss_weights = [1.25, 1.0] # [policy, value] prevent value overfit in SL\n\n\nclass ModelConfig:\n    cnn_filter_num = 256\n    cnn_first_filter_size = 5\n    cnn_filter_size = 3\n    res_layer_num = 7\n    l2_reg = 1e-4\n    value_fc_size = 256\n    distributed = False\n    input_depth = 18\n'"
src/chess_zero/configs/normal.py,0,"b'""""""\nContains the set of configs to use for the ""normal"" version of the app.\n""""""\nclass EvaluateConfig:\n    def __init__(self):\n        self.vram_frac = 1.0\n        self.game_num = 50\n        self.replace_rate = 0.55\n        self.play_config = PlayConfig()\n        self.play_config.simulation_num_per_move = 200\n        self.play_config.thinking_loop = 1\n        self.play_config.c_puct = 1 # lower  = prefer mean action value\n        self.play_config.tau_decay_rate = 0.6 # I need a better distribution...\n        self.play_config.noise_eps = 0\n        self.evaluate_latest_first = True\n        self.max_game_length = 1000\n\n\nclass PlayDataConfig:\n    def __init__(self):\n        self.min_elo_policy = 500 # 0 weight\n        self.max_elo_policy = 1800 # 1 weight\n        self.sl_nb_game_in_file = 250\n        self.nb_game_in_file = 50\n        self.max_file_num = 150\n\n\nclass PlayConfig:\n    def __init__(self):\n        self.max_processes = 3\n        self.search_threads = 16\n        self.vram_frac = 1.0\n        self.simulation_num_per_move = 800\n        self.thinking_loop = 1\n        self.logging_thinking = False\n        self.c_puct = 1.5\n        self.noise_eps = 0.25\n        self.dirichlet_alpha = 0.3\n        self.tau_decay_rate = 0.99\n        self.virtual_loss = 3\n        self.resign_threshold = -0.8\n        self.min_resign_turn = 5\n        self.max_game_length = 1000\n\n\nclass TrainerConfig:\n    def __init__(self):\n        self.min_data_size_to_learn = 0\n        self.cleaning_processes = 5 # RAM explosion...\n        self.vram_frac = 1.0\n        self.batch_size = 384 # tune this to your gpu memory\n        self.epoch_to_checkpoint = 1\n        self.dataset_size = 100000\n        self.start_total_steps = 0\n        self.save_model_steps = 25\n        self.load_data_steps = 100\n        self.loss_weights = [1.25, 1.0] # [policy, value] prevent value overfit in SL\n\n\nclass ModelConfig:\n    cnn_filter_num = 256\n    cnn_first_filter_size = 5\n    cnn_filter_size = 3\n    res_layer_num = 7\n    l2_reg = 1e-4\n    value_fc_size = 256\n    distributed = False\n    input_depth = 18\n'"
src/chess_zero/env/__init__.py,0,b''
src/chess_zero/env/chess_env.py,0,"b'""""""\nEncapsulates the functionality for representing\nand operating on the chess environment.\n""""""\nimport enum\nimport chess.pgn\nimport numpy as np\nimport copy\n\nfrom logging import getLogger\n\nlogger = getLogger(__name__)\n\n# noinspection PyArgumentList\nWinner = enum.Enum(""Winner"", ""black white draw"")\n\n# input planes\n# noinspection SpellCheckingInspection\npieces_order = \'KQRBNPkqrbnp\' # 12x8x8\ncastling_order = \'KQkq\'       # 4x8x8\n# fifty-move-rule             # 1x8x8\n# en en_passant               # 1x8x8\n\nind = {pieces_order[i]: i for i in range(12)}\n\n\nclass ChessEnv:\n    """"""\n    Represents a chess environment where a chess game is played/\n\n    Attributes:\n        :ivar chess.Board board: current board state\n        :ivar int num_halfmoves: number of half moves performed in total by each player\n        :ivar Winner winner: winner of the game\n        :ivar boolean resigned: whether non-winner resigned\n        :ivar str result: str encoding of the result, 1-0, 0-1, or 1/2-1/2\n    """"""\n    def __init__(self):\n        self.board = None\n        self.num_halfmoves = 0\n        self.winner = None  # type: Winner\n        self.resigned = False\n        self.result = None\n\n    def reset(self):\n        """"""\n        Resets to begin a new game\n        :return ChessEnv: self\n        """"""\n        self.board = chess.Board()\n        self.num_halfmoves = 0\n        self.winner = None\n        self.resigned = False\n        return self\n\n    def update(self, board):\n        """"""\n        Like reset, but resets the position to whatever was supplied for board\n        :param chess.Board board: position to reset to\n        :return ChessEnv: self\n        """"""\n        self.board = chess.Board(board)\n        self.winner = None\n        self.resigned = False\n        return self\n\n    @property\n    def done(self):\n        return self.winner is not None\n\n    @property\n    def white_won(self):\n        return self.winner == Winner.white\n\n    @property\n    def white_to_move(self):\n        return self.board.turn == chess.WHITE\n\n    def step(self, action: str, check_over = True):\n        """"""\n\n        Takes an action and updates the game state\n\n        :param str action: action to take in uci notation\n        :param boolean check_over: whether to check if game is over\n        """"""\n        if check_over and action is None:\n            self._resign()\n            return\n\n        self.board.push_uci(action)\n\n        self.num_halfmoves += 1\n\n        if check_over and self.board.result(claim_draw=True) != ""*"":\n            self._game_over()\n\n    def _game_over(self):\n        if self.winner is None:\n            self.result = self.board.result(claim_draw = True)\n            if self.result == \'1-0\':\n                self.winner = Winner.white\n            elif self.result == \'0-1\':\n                self.winner = Winner.black\n            else:\n                self.winner = Winner.draw\n\n    def _resign(self):\n        self.resigned = True\n        if self.white_to_move: # WHITE RESIGNED!\n            self.winner = Winner.black\n            self.result = ""0-1""\n        else:\n            self.winner = Winner.white\n            self.result = ""1-0""\n\n    def adjudicate(self):\n        score = self.testeval(absolute = True)\n        if abs(score) < 0.01:\n            self.winner = Winner.draw\n            self.result = ""1/2-1/2""\n        elif score > 0:\n            self.winner = Winner.white\n            self.result = ""1-0""\n        else:\n            self.winner = Winner.black\n            self.result = ""0-1""\n\n    def ending_average_game(self):\n        self.winner = Winner.draw\n        self.result = ""1/2-1/2""\n\n    def copy(self):\n        env = copy.copy(self)\n        env.board = copy.copy(self.board)\n        return env\n\n    def render(self):\n        print(""\\n"")\n        print(self.board)\n        print(""\\n"")\n\n    @property\n    def observation(self):\n        return self.board.fen()\n\n    def deltamove(self, fen_next):\n        moves = list(self.board.legal_moves)\n        for mov in moves:\n            self.board.push(mov)\n            fee = self.board.fen()\n            self.board.pop()\n            if fee == fen_next:\n                return mov.uci()\n        return None\n\n    def replace_tags(self):\n        return replace_tags_board(self.board.fen())\n\n    def canonical_input_planes(self):\n        """"""\n\n        :return: a representation of the board using an (18, 8, 8) shape, good as input to a policy / value network\n        """"""\n        return canon_input_planes(self.board.fen())\n\n    def testeval(self, absolute=False) -> float:\n        return testeval(self.board.fen(), absolute)\n\n\ndef testeval(fen, absolute = False) -> float:\n    piece_vals = {\'K\': 3, \'Q\': 14, \'R\': 5, \'B\': 3.25, \'N\': 3, \'P\': 1} # somehow it doesn\'t know how to keep its queen\n    ans = 0.0\n    tot = 0\n    for c in fen.split(\' \')[0]:\n        if not c.isalpha():\n            continue\n\n        if c.isupper():\n            ans += piece_vals[c]\n            tot += piece_vals[c]\n        else:\n            ans -= piece_vals[c.upper()]\n            tot += piece_vals[c.upper()]\n    v = ans/tot\n    if not absolute and is_black_turn(fen):\n        v = -v\n    assert abs(v) < 1\n    return np.tanh(v * 3) # arbitrary\n\n\ndef check_current_planes(realfen, planes):\n    cur = planes[0:12]\n    assert cur.shape == (12, 8, 8)\n    fakefen = [""1""] * 64\n    for i in range(12):\n        for rank in range(8):\n            for file in range(8):\n                if cur[i][rank][file] == 1:\n                    assert fakefen[rank * 8 + file] == \'1\'\n                    fakefen[rank * 8 + file] = pieces_order[i]\n\n    castling = planes[12:16]\n    fiftymove = planes[16][0][0]\n    ep = planes[17]\n\n    castlingstring = """"\n    for i in range(4):\n        if castling[i][0][0] == 1:\n            castlingstring += castling_order[i]\n\n    if len(castlingstring) == 0:\n        castlingstring = \'-\'\n\n    epstr = ""-""\n    for rank in range(8):\n        for file in range(8):\n            if ep[rank][file] == 1:\n                epstr = coord_to_alg((rank, file))\n\n    realfen = maybe_flip_fen(realfen, flip=is_black_turn(realfen))\n    realparts = realfen.split(\' \')\n    assert realparts[1] == \'w\'\n    assert realparts[2] == castlingstring\n    assert realparts[3] == epstr\n    assert int(realparts[4]) == fiftymove\n    # realparts[5] is the fifty-move clock, discard that\n    return """".join(fakefen) == replace_tags_board(realfen)\n\n\ndef canon_input_planes(fen):\n    """"""\n\n    :param fen:\n    :return : (18, 8, 8) representation of the game state\n    """"""\n    fen = maybe_flip_fen(fen, is_black_turn(fen))\n    return all_input_planes(fen)\n\n\ndef all_input_planes(fen):\n    current_aux_planes = aux_planes(fen)\n\n    history_both = to_planes(fen)\n\n    ret = np.vstack((history_both, current_aux_planes))\n    assert ret.shape == (18, 8, 8)\n    return ret\n\n\ndef maybe_flip_fen(fen, flip = False):\n    if not flip:\n        return fen\n    foo = fen.split(\' \')\n    rows = foo[0].split(\'/\')\n    def swapcase(a):\n        if a.isalpha():\n            return a.lower() if a.isupper() else a.upper()\n        return a\n    def swapall(aa):\n        return """".join([swapcase(a) for a in aa])\n    return ""/"".join([swapall(row) for row in reversed(rows)]) \\\n        + "" "" + (\'w\' if foo[1] == \'b\' else \'b\') \\\n        + "" "" + """".join(sorted(swapall(foo[2]))) \\\n        + "" "" + foo[3] + "" "" + foo[4] + "" "" + foo[5]\n\n\ndef aux_planes(fen):\n    foo = fen.split(\' \')\n\n    en_passant = np.zeros((8, 8), dtype=np.float32)\n    if foo[3] != \'-\':\n        eps = alg_to_coord(foo[3])\n        en_passant[eps[0]][eps[1]] = 1\n\n    fifty_move_count = int(foo[4])\n    fifty_move = np.full((8, 8), fifty_move_count, dtype=np.float32)\n\n    castling = foo[2]\n    auxiliary_planes = [np.full((8, 8), int(\'K\' in castling), dtype=np.float32),\n                        np.full((8, 8), int(\'Q\' in castling), dtype=np.float32),\n                        np.full((8, 8), int(\'k\' in castling), dtype=np.float32),\n                        np.full((8, 8), int(\'q\' in castling), dtype=np.float32),\n                        fifty_move,\n                        en_passant]\n\n    ret = np.asarray(auxiliary_planes, dtype=np.float32)\n    assert ret.shape == (6, 8, 8)\n    return ret\n\n# FEN board is like this:\n# a8 b8 .. h8\n# a7 b7 .. h7\n# .. .. .. ..\n# a1 b1 .. h1\n# \n# FEN string is like this:\n#  0  1 ..  7\n#  8  9 .. 15\n# .. .. .. ..\n# 56 57 .. 63\n\n# my planes are like this:\n# 00 01 .. 07\n# 10 11 .. 17\n# .. .. .. ..\n# 70 71 .. 77\n#\n\n\ndef alg_to_coord(alg):\n    rank = 8 - int(alg[1])        # 0-7\n    file = ord(alg[0]) - ord(\'a\') # 0-7\n    return rank, file\n\n\ndef coord_to_alg(coord):\n    letter = chr(ord(\'a\') + coord[1])\n    number = str(8 - coord[0])\n    return letter + number\n\n\ndef to_planes(fen):\n    board_state = replace_tags_board(fen)\n    pieces_both = np.zeros(shape=(12, 8, 8), dtype=np.float32)\n    for rank in range(8):\n        for file in range(8):\n            v = board_state[rank * 8 + file]\n            if v.isalpha():\n                pieces_both[ind[v]][rank][file] = 1\n    assert pieces_both.shape == (12, 8, 8)\n    return pieces_both\n\n\ndef replace_tags_board(board_san):\n    board_san = board_san.split("" "")[0]\n    board_san = board_san.replace(""2"", ""11"")\n    board_san = board_san.replace(""3"", ""111"")\n    board_san = board_san.replace(""4"", ""1111"")\n    board_san = board_san.replace(""5"", ""11111"")\n    board_san = board_san.replace(""6"", ""111111"")\n    board_san = board_san.replace(""7"", ""1111111"")\n    board_san = board_san.replace(""8"", ""11111111"")\n    return board_san.replace(""/"", """")\n\n\ndef is_black_turn(fen):\n    return fen.split("" "")[1] == \'b\'\n'"
src/chess_zero/lib/__init__.py,0,b''
src/chess_zero/lib/data_helper.py,0,"b'""""""\nVarious helper functions for working with the data used in this app\n""""""\n\nimport os\nimport json\nfrom datetime import datetime\nfrom glob import glob\nfrom logging import getLogger\n\nimport chess\nimport pyperclip\nfrom chess_zero.config import ResourceConfig\n\nlogger = getLogger(__name__)\n\n\ndef pretty_print(env, colors):\n    new_pgn = open(""test3.pgn"", ""at"")\n    game = chess.pgn.Game.from_board(env.board)\n    game.headers[""Result""] = env.result\n    game.headers[""White""], game.headers[""Black""] = colors\n    game.headers[""Date""] = datetime.now().strftime(""%Y.%m.%d"")\n    new_pgn.write(str(game) + ""\\n\\n"")\n    new_pgn.close()\n    pyperclip.copy(env.board.fen())\n\n\ndef find_pgn_files(directory, pattern=\'*.pgn\'):\n    dir_pattern = os.path.join(directory, pattern)\n    files = list(sorted(glob(dir_pattern)))\n    return files\n\n\ndef get_game_data_filenames(rc: ResourceConfig):\n    pattern = os.path.join(rc.play_data_dir, rc.play_data_filename_tmpl % ""*"")\n    files = list(sorted(glob(pattern)))\n    return files\n\n\ndef get_next_generation_model_dirs(rc: ResourceConfig):\n    dir_pattern = os.path.join(rc.next_generation_model_dir, rc.next_generation_model_dirname_tmpl % ""*"")\n    dirs = list(sorted(glob(dir_pattern)))\n    return dirs\n\n\ndef write_game_data_to_file(path, data):\n    try:\n        with open(path, ""wt"") as f:\n            json.dump(data, f)\n    except Exception as e:\n        print(e)\n\n\ndef read_game_data_from_file(path):\n    try:\n        with open(path, ""rt"") as f:\n            return json.load(f)\n    except Exception as e:\n        print(e)\n\n'"
src/chess_zero/lib/logger.py,0,"b'""""""\nLogging helper methods\n""""""\n\nfrom logging import StreamHandler, basicConfig, DEBUG, getLogger, Formatter\n\n\ndef setup_logger(log_filename):\n    format_str = \'%(asctime)s@%(name)s %(levelname)s # %(message)s\'\n    basicConfig(filename=log_filename, level=DEBUG, format=format_str)\n    stream_handler = StreamHandler()\n    stream_handler.setFormatter(Formatter(format_str))\n    getLogger().addHandler(stream_handler)\n\n\nif __name__ == \'__main__\':\n    setup_logger(""aa.log"")\n    logger = getLogger(""test"")\n    logger.info(""OK"")\n'"
src/chess_zero/lib/model_helper.py,0,"b'""""""\nHelper methods for working with trained models.\n""""""\n\nfrom logging import getLogger\n\nlogger = getLogger(__name__)\n\n\ndef load_best_model_weight(model):\n    """"""\n    :param chess_zero.agent.model.ChessModel model:\n    :return:\n    """"""\n    return model.load(model.config.resource.model_best_config_path, model.config.resource.model_best_weight_path)\n\n\ndef save_as_best_model(model):\n    """"""\n\n    :param chess_zero.agent.model.ChessModel model:\n    :return:\n    """"""\n    return model.save(model.config.resource.model_best_config_path, model.config.resource.model_best_weight_path)\n\n\ndef reload_best_model_weight_if_changed(model):\n    """"""\n\n    :param chess_zero.agent.model.ChessModel model:\n    :return:\n    """"""\n    if model.config.model.distributed:\n        return load_best_model_weight(model)\n    else:\n        logger.debug(""start reload the best model if changed"")\n        digest = model.fetch_digest(model.config.resource.model_best_weight_path)\n        if digest != model.digest:\n            return load_best_model_weight(model)\n\n        logger.debug(""the best model is not changed"")\n        return False\n'"
src/chess_zero/lib/tf_util.py,3,"b'""""""\nFor helping to configure tensorflow\n""""""\n\n\ndef set_session_config(per_process_gpu_memory_fraction=None, allow_growth=None):\n    """"""\n\n    :param allow_growth: When necessary, reserve memory\n    :param float per_process_gpu_memory_fraction: specify GPU memory usage as 0 to 1\n\n    :return:\n    """"""\n    import tensorflow as tf\n    import keras.backend as k\n\n    config = tf.ConfigProto(\n        gpu_options=tf.GPUOptions(\n            per_process_gpu_memory_fraction=per_process_gpu_memory_fraction,\n            allow_growth=allow_growth,\n        )\n    )\n    sess = tf.Session(config=config)\n    k.set_session(sess)\n'"
src/chess_zero/play_game/__init__.py,0,b''
src/chess_zero/play_game/uci.py,0,"b'""""""\nUtility methods for playing an actual game as a human against a model.\n""""""\n\nimport sys\nfrom logging import getLogger\n\nfrom chess_zero.agent.player_chess import ChessPlayer\nfrom chess_zero.config import Config, PlayWithHumanConfig\nfrom chess_zero.env.chess_env import ChessEnv\n\nlogger = getLogger(__name__)\n\n# noinspection SpellCheckingInspection,SpellCheckingInspection,SpellCheckingInspection,SpellCheckingInspection,SpellCheckingInspection,SpellCheckingInspection\ndef start(config: Config):\n\n    PlayWithHumanConfig().update_play_config(config.play)\n\n    me_player = None\n    env = ChessEnv().reset()\n\n    while True:\n        line = input()\n        words = line.rstrip().split("" "",1)\n        if words[0] == ""uci"":\n            print(""id name ChessZero"")\n            print(""id author ChessZero"")\n            print(""uciok"")\n        elif words[0] == ""isready"":\n            if not me_player:\n                me_player = get_player(config)\n            print(""readyok"")\n        elif words[0] == ""ucinewgame"":\n            env.reset()\n        elif words[0] == ""position"":\n            words = words[1].split("" "",1)\n            if words[0] == ""startpos"":\n                env.reset()\n            else:\n                if words[0] == ""fen"": # skip extraneous word\n                    words = words[1].split(\' \',1)\n                fen = words[0]\n                for _ in range(5):\n                    words = words[1].split(\' \',1)\n                    fen += "" "" + words[0]\n                env.update(fen)\n            if len(words) > 1:\n                words = words[1].split("" "",1)\n                if words[0] == ""moves"":\n                    for w in words[1].split("" ""):\n                        env.step(w, False)\n        elif words[0] == ""go"":\n            if not me_player:\n                me_player = get_player(config)\n            action = me_player.action(env, False)\n            print(f""bestmove {action}"")\n        elif words[0] == ""stop"":\n            pass\n        elif words[0] == ""quit"":\n            break\n\n\ndef get_player(config):\n    from chess_zero.agent.model_chess import ChessModel\n    from chess_zero.lib.model_helper import load_best_model_weight\n    model = ChessModel(config)\n    if not load_best_model_weight(model):\n        raise RuntimeError(""Best model not found!"")\n    return ChessPlayer(config, model.get_pipes(config.play.search_threads))\n\n\ndef info(depth, move, score):\n    print(f""info score cp {int(score*100)} depth {depth} pv {move}"")\n    sys.stdout.flush()\n'"
src/chess_zero/worker/__init__.py,0,b''
src/chess_zero/worker/evaluate.py,0,"b'""""""\nEncapsulates the worker which evaluates newly-trained models and picks the best one\n""""""\n\nimport os\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom logging import getLogger\nfrom multiprocessing import Manager\nfrom time import sleep\n\nfrom chess_zero.agent.model_chess import ChessModel\nfrom chess_zero.agent.player_chess import ChessPlayer\nfrom chess_zero.config import Config\nfrom chess_zero.env.chess_env import ChessEnv, Winner\nfrom chess_zero.lib.data_helper import get_next_generation_model_dirs, pretty_print\nfrom chess_zero.lib.model_helper import save_as_best_model, load_best_model_weight\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    return EvaluateWorker(config).start()\n\nclass EvaluateWorker:\n    """"""\n    Worker which evaluates trained models and keeps track of the best one\n\n    Attributes:\n        :ivar Config config: config to use for evaluation\n        :ivar PlayConfig config: PlayConfig to use to determine how to play, taken from config.eval.play_config\n        :ivar ChessModel current_model: currently chosen best model\n        :ivar Manager m: multiprocessing manager\n        :ivar list(Connection) cur_pipes: pipes on which the current best ChessModel is listening which will be used to\n            make predictions while playing a game.\n    """"""\n    def __init__(self, config: Config):\n        """"""\n        :param config: Config to use to control how evaluation should work\n        """"""\n        self.config = config\n        self.play_config = config.eval.play_config\n        self.current_model = self.load_current_model()\n        self.m = Manager()\n        self.cur_pipes = self.m.list([self.current_model.get_pipes(self.play_config.search_threads) for _ in range(self.play_config.max_processes)])\n\n    def start(self):\n        """"""\n        Start evaluation, endlessly loading the latest models from the directory which stores them and\n        checking if they do better than the current model, saving the result in self.current_model\n        """"""\n        while True:\n            ng_model, model_dir = self.load_next_generation_model()\n            logger.debug(f""start evaluate model {model_dir}"")\n            ng_is_great = self.evaluate_model(ng_model)\n            if ng_is_great:\n                logger.debug(f""New Model become best model: {model_dir}"")\n                save_as_best_model(ng_model)\n                self.current_model = ng_model\n            self.move_model(model_dir)\n\n    def evaluate_model(self, ng_model):\n        """"""\n        Given a model, evaluates it by playing a bunch of games against the current model.\n\n        :param ChessModel ng_model: model to evaluate\n        :return: true iff this model is better than the current_model\n        """"""\n        ng_pipes = self.m.list([ng_model.get_pipes(self.play_config.search_threads) for _ in range(self.play_config.max_processes)])\n\n        futures = []\n        with ProcessPoolExecutor(max_workers=self.play_config.max_processes) as executor:\n            for game_idx in range(self.config.eval.game_num):\n                fut = executor.submit(play_game, self.config, cur=self.cur_pipes, ng=ng_pipes, current_white=(game_idx % 2 == 0))\n                futures.append(fut)\n\n            results = []\n            for fut in as_completed(futures):\n                # ng_score := if ng_model win -> 1, lose -> 0, draw -> 0.5\n                ng_score, env, current_white = fut.result()\n                results.append(ng_score)\n                win_rate = sum(results) / len(results)\n                game_idx = len(results)\n                logger.debug(f""game {game_idx:3}: ng_score={ng_score:.1f} as {\'black\' if current_white else \'white\'} ""\n                             f""{\'by resign \' if env.resigned else \'          \'}""\n                             f""win_rate={win_rate*100:5.1f}% ""\n                             f""{env.board.fen().split(\' \')[0]}"")\n\n                colors = (""current_model"", ""ng_model"")\n                if not current_white:\n                    colors = reversed(colors)\n                pretty_print(env, colors)\n\n                if len(results)-sum(results) >= self.config.eval.game_num * (1-self.config.eval.replace_rate):\n                    logger.debug(f""lose count reach {results.count(0)} so give up challenge"")\n                    return False\n                if sum(results) >= self.config.eval.game_num * self.config.eval.replace_rate:\n                    logger.debug(f""win count reach {results.count(1)} so change best model"")\n                    return True\n\n        win_rate = sum(results) / len(results)\n        logger.debug(f""winning rate {win_rate*100:.1f}%"")\n        return win_rate >= self.config.eval.replace_rate\n\n    def move_model(self, model_dir):\n        """"""\n        Moves the newest model to the specified directory\n\n        :param file model_dir: directory where model should be moved\n        """"""\n        rc = self.config.resource\n        new_dir = os.path.join(rc.next_generation_model_dir, ""copies"", model_dir.name)\n        os.rename(model_dir, new_dir)\n\n    def load_current_model(self):\n        """"""\n        Loads the best model from the standard directory.\n        :return ChessModel: the model\n        """"""\n        model = ChessModel(self.config)\n        load_best_model_weight(model)\n        return model\n\n    def load_next_generation_model(self):\n        """"""\n        Loads the next generation model from the standard directory\n        :return (ChessModel, file): the model and the directory that it was in\n        """"""\n        rc = self.config.resource\n        while True:\n            dirs = get_next_generation_model_dirs(self.config.resource)\n            if dirs:\n                break\n            logger.info(""There is no next generation model to evaluate"")\n            sleep(60)\n        model_dir = dirs[-1] if self.config.eval.evaluate_latest_first else dirs[0]\n        config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n        weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n        model = ChessModel(self.config)\n        model.load(config_path, weight_path)\n        return model, model_dir\n\n\ndef play_game(config, cur, ng, current_white: bool) -> (float, ChessEnv, bool):\n    """"""\n    Plays a game against models cur and ng and reports the results.\n\n    :param Config config: config for how to play the game\n    :param ChessModel cur: should be the current model\n    :param ChessModel ng: should be the next generation model\n    :param bool current_white: whether cur should play white or black\n    :return (float, ChessEnv, bool): the score for the ng model\n        (0 for loss, .5 for draw, 1 for win), the env after the game is finished, and a bool\n        which is true iff cur played as white in that game.\n    """"""\n    cur_pipes = cur.pop()\n    ng_pipes = ng.pop()\n    env = ChessEnv().reset()\n\n    current_player = ChessPlayer(config, pipes=cur_pipes, play_config=config.eval.play_config)\n    ng_player = ChessPlayer(config, pipes=ng_pipes, play_config=config.eval.play_config)\n    if current_white:\n        white, black = current_player, ng_player\n    else:\n        white, black = ng_player, current_player\n\n    while not env.done:\n        if env.white_to_move:\n            action = white.action(env)\n        else:\n            action = black.action(env)\n        env.step(action)\n        if env.num_halfmoves >= config.eval.max_game_length:\n            env.adjudicate()\n\n    if env.winner == Winner.draw:\n        ng_score = 0.5\n    elif env.white_won == current_white:\n        ng_score = 0\n    else:\n        ng_score = 1\n    cur.append(cur_pipes)\n    ng.append(ng_pipes)\n    return ng_score, env, current_white\n'"
src/chess_zero/worker/optimize.py,0,"b'""""""\nEncapsulates the worker which trains ChessModels using game data from recorded games from a file.\n""""""\nimport os\nfrom collections import deque\nfrom concurrent.futures import ProcessPoolExecutor\nfrom datetime import datetime\nfrom logging import getLogger\nfrom time import sleep\nfrom random import shuffle\n\nimport numpy as np\n\nfrom chess_zero.agent.model_chess import ChessModel\nfrom chess_zero.config import Config\nfrom chess_zero.env.chess_env import canon_input_planes, is_black_turn, testeval\nfrom chess_zero.lib.data_helper import get_game_data_filenames, read_game_data_from_file, get_next_generation_model_dirs\nfrom chess_zero.lib.model_helper import load_best_model_weight\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    """"""\n    Helper method which just kicks off the optimization using the specified config\n    :param Config config: config to use\n    """"""\n    return OptimizeWorker(config).start()\n\n\nclass OptimizeWorker:\n    """"""\n    Worker which optimizes a ChessModel by training it on game data\n\n    Attributes:\n        :ivar Config config: config for this worker\n        :ivar ChessModel model: model to train\n        :ivar dequeue,dequeue,dequeue dataset: tuple of dequeues where each dequeue contains game states,\n            target policy network values (calculated based on visit stats\n                for each state during the game), and target value network values (calculated based on\n                    who actually won the game after that state)\n        :ivar ProcessPoolExecutor executor: executor for running all of the training processes\n    """"""\n    def __init__(self, config: Config):\n        self.config = config\n        self.model = None  # type: ChessModel\n        self.dataset = deque(),deque(),deque()\n        self.executor = ProcessPoolExecutor(max_workers=config.trainer.cleaning_processes)\n\n    def start(self):\n        """"""\n        Load the next generation model from disk and start doing the training endlessly.\n        """"""\n        self.model = self.load_model()\n        self.training()\n\n    def training(self):\n        """"""\n        Does the actual training of the model, running it on game data. Endless.\n        """"""\n        self.compile_model()\n        self.filenames = deque(get_game_data_filenames(self.config.resource))\n        shuffle(self.filenames)\n        total_steps = self.config.trainer.start_total_steps\n\n        while True:\n            self.fill_queue()\n            steps = self.train_epoch(self.config.trainer.epoch_to_checkpoint)\n            total_steps += steps\n            self.save_current_model()\n            a, b, c = self.dataset\n            while len(a) > self.config.trainer.dataset_size/2:\n                a.popleft()\n                b.popleft()\n                c.popleft()\n\n    def train_epoch(self, epochs):\n        """"""\n        Runs some number of epochs of training\n        :param int epochs: number of epochs\n        :return: number of datapoints that were trained on in total\n        """"""\n        tc = self.config.trainer\n        state_ary, policy_ary, value_ary = self.collect_all_loaded_data()\n        tensorboard_cb = TensorBoard(log_dir=""./logs"", batch_size=tc.batch_size, histogram_freq=1)\n        self.model.model.fit(state_ary, [policy_ary, value_ary],\n                             batch_size=tc.batch_size,\n                             epochs=epochs,\n                             shuffle=True,\n                             validation_split=0.02,\n                             callbacks=[tensorboard_cb])\n        steps = (state_ary.shape[0] // tc.batch_size) * epochs\n        return steps\n\n    def compile_model(self):\n        """"""\n        Compiles the model to use optimizer and loss function tuned for supervised learning\n        """"""\n        opt = Adam()\n        losses = [\'categorical_crossentropy\', \'mean_squared_error\'] # avoid overfit for supervised \n        self.model.model.compile(optimizer=opt, loss=losses, loss_weights=self.config.trainer.loss_weights)\n\n    def save_current_model(self):\n        """"""\n        Saves the current model as the next generation model to the appropriate directory\n        """"""\n        rc = self.config.resource\n        model_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        model_dir = os.path.join(rc.next_generation_model_dir, rc.next_generation_model_dirname_tmpl % model_id)\n        os.makedirs(model_dir, exist_ok=True)\n        config_path = os.path.join(model_dir, rc.next_generation_model_config_filename)\n        weight_path = os.path.join(model_dir, rc.next_generation_model_weight_filename)\n        self.model.save(config_path, weight_path)\n\n    def fill_queue(self):\n        """"""\n        Fills the self.dataset queues with data from the training dataset.\n        """"""\n        futures = deque()\n        with ProcessPoolExecutor(max_workers=self.config.trainer.cleaning_processes) as executor:\n            for _ in range(self.config.trainer.cleaning_processes):\n                if len(self.filenames) == 0:\n                    break\n                filename = self.filenames.popleft()\n                logger.debug(f""loading data from {filename}"")\n                futures.append(executor.submit(load_data_from_file,filename))\n            while futures and len(self.dataset[0]) < self.config.trainer.dataset_size:\n                for x,y in zip(self.dataset,futures.popleft().result()):\n                    x.extend(y)\n                if len(self.filenames) > 0:\n                    filename = self.filenames.popleft()\n                    logger.debug(f""loading data from {filename}"")\n                    futures.append(executor.submit(load_data_from_file,filename))\n\n    def collect_all_loaded_data(self):\n        """"""\n\n        :return: a tuple containing the data in self.dataset, split into\n        (state, policy, and value).\n        """"""\n        state_ary,policy_ary,value_ary=self.dataset\n\n        state_ary1 = np.asarray(state_ary, dtype=np.float32)\n        policy_ary1 = np.asarray(policy_ary, dtype=np.float32)\n        value_ary1 = np.asarray(value_ary, dtype=np.float32)\n        return state_ary1, policy_ary1, value_ary1\n\n    def load_model(self):\n        """"""\n        Loads the next generation model from the appropriate directory. If not found, loads\n        the best known model.\n        """"""\n        model = ChessModel(self.config)\n        rc = self.config.resource\n\n        dirs = get_next_generation_model_dirs(rc)\n        if not dirs:\n            logger.debug(""loading best model"")\n            if not load_best_model_weight(model):\n                raise RuntimeError(""Best model can not loaded!"")\n        else:\n            latest_dir = dirs[-1]\n            logger.debug(""loading latest model"")\n            config_path = os.path.join(latest_dir, rc.next_generation_model_config_filename)\n            weight_path = os.path.join(latest_dir, rc.next_generation_model_weight_filename)\n            model.load(config_path, weight_path)\n        return model\n\n\ndef load_data_from_file(filename):\n    data = read_game_data_from_file(filename)\n    return convert_to_cheating_data(data)\n\n\ndef convert_to_cheating_data(data):\n    """"""\n    :param data: format is SelfPlayWorker.buffer\n    :return:\n    """"""\n    state_list = []\n    policy_list = []\n    value_list = []\n    for state_fen, policy, value in data:\n\n        state_planes = canon_input_planes(state_fen)\n\n        if is_black_turn(state_fen):\n            policy = Config.flip_policy(policy)\n\n        move_number = int(state_fen.split(\' \')[5])\n        value_certainty = min(5, move_number)/5 # reduces the noise of the opening... plz train faster\n        sl_value = value*value_certainty + testeval(state_fen, False)*(1-value_certainty)\n\n        state_list.append(state_planes)\n        policy_list.append(policy)\n        value_list.append(sl_value)\n\n    return np.asarray(state_list, dtype=np.float32), np.asarray(policy_list, dtype=np.float32), np.asarray(value_list, dtype=np.float32)\n'"
src/chess_zero/worker/self_play.py,0,"b'""""""\nHolds the worker which trains the chess model using self play data.\n""""""\nimport os\nfrom collections import deque\nfrom concurrent.futures import ProcessPoolExecutor\nfrom datetime import datetime\nfrom logging import getLogger\nfrom multiprocessing import Manager\nfrom threading import Thread\nfrom time import time\n\nfrom chess_zero.agent.model_chess import ChessModel\nfrom chess_zero.agent.player_chess import ChessPlayer\nfrom chess_zero.config import Config\nfrom chess_zero.env.chess_env import ChessEnv, Winner\nfrom chess_zero.lib.data_helper import get_game_data_filenames, write_game_data_to_file, pretty_print\nfrom chess_zero.lib.model_helper import load_best_model_weight, save_as_best_model, \\\n    reload_best_model_weight_if_changed\n\nlogger = getLogger(__name__)\n\n\ndef start(config: Config):\n    return SelfPlayWorker(config).start()\n\n\n# noinspection PyAttributeOutsideInit\nclass SelfPlayWorker:\n    """"""\n    Worker which trains a chess model using self play data. ALl it does is do self play and then write the\n    game data to file, to be trained on by the optimize worker.\n\n    Attributes:\n        :ivar Config config: config to use to configure this worker\n        :ivar ChessModel current_model: model to use for self play\n        :ivar Manager m: the manager to use to coordinate between other workers\n        :ivar list(Connection) cur_pipes: pipes to send observations to and get back mode predictions.\n        :ivar list((str,list(float))): list of all the moves. Each tuple has the observation in FEN format and\n            then the list of prior probabilities for each action, given by the visit count of each of the states\n            reached by the action (actions indexed according to how they are ordered in the uci move list).\n    """"""\n    def __init__(self, config: Config):\n        self.config = config\n        self.current_model = self.load_model()\n        self.m = Manager()\n        self.cur_pipes = self.m.list([self.current_model.get_pipes(self.config.play.search_threads) for _ in range(self.config.play.max_processes)])\n        self.buffer = []\n\n    def start(self):\n        """"""\n        Do self play and write the data to the appropriate file.\n        """"""\n        self.buffer = []\n\n        futures = deque()\n        with ProcessPoolExecutor(max_workers=self.config.play.max_processes) as executor:\n            for game_idx in range(self.config.play.max_processes * 2):\n                futures.append(executor.submit(self_play_buffer, self.config, cur=self.cur_pipes))\n            game_idx = 0\n            while True:\n                game_idx += 1\n                start_time = time()\n                env, data = futures.popleft().result()\n                print(f""game {game_idx:3} time={time() - start_time:5.1f}s ""\n                    f""halfmoves={env.num_halfmoves:3} {env.winner:12} ""\n                    f""{\'by resign \' if env.resigned else \'          \'}"")\n\n                pretty_print(env, (""current_model"", ""current_model""))\n                self.buffer += data\n                if (game_idx % self.config.play_data.nb_game_in_file) == 0:\n                    self.flush_buffer()\n                    reload_best_model_weight_if_changed(self.current_model)\n                futures.append(executor.submit(self_play_buffer, self.config, cur=self.cur_pipes)) # Keep it going\n\n        if len(data) > 0:\n            self.flush_buffer()\n\n    def load_model(self):\n        """"""\n        Load the current best model\n        :return ChessModel: current best model\n        """"""\n        model = ChessModel(self.config)\n        if self.config.opts.new or not load_best_model_weight(model):\n            model.build()\n            save_as_best_model(model)\n        return model\n\n    def flush_buffer(self):\n        """"""\n        Flush the play data buffer and write the data to the appropriate location\n        """"""\n        rc = self.config.resource\n        game_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        path = os.path.join(rc.play_data_dir, rc.play_data_filename_tmpl % game_id)\n        logger.info(f""save play data to {path}"")\n        thread = Thread(target=write_game_data_to_file, args=(path, self.buffer))\n        thread.start()\n        self.buffer = []\n\n    def remove_play_data(self):\n        """"""\n        Delete the play data from disk\n        """"""\n        files = get_game_data_filenames(self.config.resource)\n        if len(files) < self.config.play_data.max_file_num:\n            return\n        for i in range(len(files) - self.config.play_data.max_file_num):\n            os.remove(files[i])\n\n\ndef self_play_buffer(config, cur) -> (ChessEnv, list):\n    """"""\n    Play one game and add the play data to the buffer\n    :param Config config: config for how to play\n    :param list(Connection) cur: list of pipes to use to get a pipe to send observations to for getting\n        predictions. One will be removed from this list during the game, then added back\n    :return (ChessEnv,list((str,list(float)): a tuple containing the final ChessEnv state and then a list\n        of data to be appended to the SelfPlayWorker.buffer\n    """"""\n    pipes = cur.pop() # borrow\n    env = ChessEnv().reset()\n\n    white = ChessPlayer(config, pipes=pipes)\n    black = ChessPlayer(config, pipes=pipes)\n\n    while not env.done:\n        if env.white_to_move:\n            action = white.action(env)\n        else:\n            action = black.action(env)\n        env.step(action)\n        if env.num_halfmoves >= config.play.max_game_length:\n            env.adjudicate()\n\n    if env.winner == Winner.white:\n        black_win = -1\n    elif env.winner == Winner.black:\n        black_win = 1\n    else:\n        black_win = 0\n\n    black.finish_game(black_win)\n    white.finish_game(-black_win)\n\n    data = []\n    for i in range(len(white.moves)):\n        data.append(white.moves[i])\n        if i < len(black.moves):\n            data.append(black.moves[i])\n\n    cur.append(pipes)\n    return env, data'"
src/chess_zero/worker/sl.py,0,"b'""""""\nContains the worker for training the model using recorded game data rather than self-play\n""""""\nimport os\nimport re\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom datetime import datetime\nfrom logging import getLogger\nfrom threading import Thread\nfrom time import time\n\nimport chess.pgn\n\nfrom chess_zero.agent.player_chess import ChessPlayer\nfrom chess_zero.config import Config\nfrom chess_zero.env.chess_env import ChessEnv, Winner\nfrom chess_zero.lib.data_helper import write_game_data_to_file, find_pgn_files\n\nlogger = getLogger(__name__)\n\nTAG_REGEX = re.compile(r""^\\[([A-Za-z0-9_]+)\\s+\\""(.*)\\""\\]\\s*$"")\n\n\ndef start(config: Config):\n    return SupervisedLearningWorker(config).start()\n\n\nclass SupervisedLearningWorker:\n    """"""\n    Worker which performs supervised learning on recorded games.\n\n    Attributes:\n        :ivar Config config: config for this worker\n        :ivar list((str,list(float)) buffer: buffer containing the data to use for training -\n            each entry contains a FEN encoded game state and a list where every index corresponds\n            to a chess move. The move that was taken in the actual game is given a value (based on\n            the player elo), all other moves are given a 0.\n    """"""\n    def __init__(self, config: Config):\n        """"""\n        :param config:\n        """"""\n        self.config = config\n        self.buffer = []\n\n    def start(self):\n        """"""\n        Start the actual training.\n        """"""\n        self.buffer = []\n        # noinspection PyAttributeOutsideInit\n        self.idx = 0\n        start_time = time()\n        with ProcessPoolExecutor(max_workers=7) as executor:\n            games = self.get_games_from_all_files()\n            for res in as_completed([executor.submit(get_buffer, self.config, game) for game in games]): #poisoned reference (memleak)\n                self.idx += 1\n                env, data = res.result()\n                self.save_data(data)\n                end_time = time()\n                logger.debug(f""game {self.idx:4} time={(end_time - start_time):.3f}s ""\n                             f""halfmoves={env.num_halfmoves:3} {env.winner:12}""\n                             f""{\' by resign \' if env.resigned else \'           \'}""\n                             f""{env.observation.split(\' \')[0]}"")\n                start_time = end_time\n\n        if len(self.buffer) > 0:\n            self.flush_buffer()\n\n    def get_games_from_all_files(self):\n        """"""\n        Loads game data from pgn files\n        :return list(chess.pgn.Game): the games\n        """"""\n        files = find_pgn_files(self.config.resource.play_data_dir)\n        print(files)\n        games = []\n        for filename in files:\n            games.extend(get_games_from_file(filename))\n        print(""done reading"")\n        return games\n\n    def save_data(self, data):\n        """"""\n\n        :param (str,list(float)) data: a FEN encoded game state and a list where every index corresponds\n            to a chess move. The move that was taken in the actual game is given a value (based on\n            the player elo), all other moves are given a 0.\n        """"""\n        self.buffer += data\n        if self.idx % self.config.play_data.sl_nb_game_in_file == 0:\n            self.flush_buffer()\n\n    def flush_buffer(self):\n        """"""\n        Clears out the moves loaded into the buffer and saves the to file.\n        """"""\n        rc = self.config.resource\n        game_id = datetime.now().strftime(""%Y%m%d-%H%M%S.%f"")\n        path = os.path.join(rc.play_data_dir, rc.play_data_filename_tmpl % game_id)\n        logger.info(f""save play data to {path}"")\n        thread = Thread(target = write_game_data_to_file, args=(path, self.buffer))\n        thread.start()\n        self.buffer = []\n\n\ndef get_games_from_file(filename):\n    """"""\n\n    :param str filename: file containing the pgn game data\n    :return list(pgn.Game): chess games in that file\n    """"""\n    pgn = open(filename, errors=\'ignore\')\n    offsets = []\n    while True:\n        offset = pgn.tell()\n        \n        headers = chess.pgn.read_headers(pgn)\n        if headers is None:\n           break\n\n        offsets.append(offset)\n    n = len(offsets)\n                             \n    print(f""found {n} games"")\n    games = []\n    for offset in offsets:\n        pgn.seek(offset)\n        games.append(chess.pgn.read_game(pgn))\n    return games\n\n\ndef clip_elo_policy(config, elo):\n\t# 0 until min_elo, 1 after max_elo, linear in between\n\treturn min(1, max(0, elo - config.play_data.min_elo_policy) / (\n\t\t\t\tconfig.play_data.max_elo_policy - config.play_data.min_elo_policy))\n\n\ndef get_buffer(config, game) -> (ChessEnv, list):\n    """"""\n    Gets data to load into the buffer by playing a game using PGN data.\n    :param Config config: config to use to play the game\n    :param pgn.Game game: game to play\n    :return list(str,list(float)): data from this game for the SupervisedLearningWorker.buffer\n    """"""\n    env = ChessEnv().reset()\n    white = ChessPlayer(config, dummy=True)\n    black = ChessPlayer(config, dummy=True)\n    result = game.headers[""Result""]\n    white_elo, black_elo = int(game.headers[""WhiteElo""]), int(game.headers[""BlackElo""])\n    white_weight = clip_elo_policy(config, white_elo)\n    black_weight = clip_elo_policy(config, black_elo)\n    \n    actions = []\n    while not game.is_end():\n        game = game.variation(0)\n        actions.append(game.move.uci())\n    k = 0\n    while not env.done and k < len(actions):\n        if env.white_to_move:\n            action = white.sl_action(env.observation, actions[k], weight=white_weight) #ignore=True\n        else:\n            action = black.sl_action(env.observation, actions[k], weight=black_weight) #ignore=True\n        env.step(action, False)\n        k += 1\n\n    if not env.board.is_game_over() and result != \'1/2-1/2\':\n        env.resigned = True\n    if result == \'1-0\':\n        env.winner = Winner.white\n        black_win = -1\n    elif result == \'0-1\':\n        env.winner = Winner.black\n        black_win = 1\n    else:\n        env.winner = Winner.draw\n        black_win = 0\n\n    black.finish_game(black_win)\n    white.finish_game(-black_win)\n\n    data = []\n    for i in range(len(white.moves)):\n        data.append(white.moves[i])\n        if i < len(black.moves):\n            data.append(black.moves[i])\n\n    return env, data\n'"
