file_path,api_count,code
assignments/assignment1/mp1/run_computation.py,3,"b""import tensorflow as tf\n\n\ndef run_computation(inp):\n    '''\n    This function takes all of the steps necessary to run the provided\n    tensor, inp. More specifically:\n        -First, create a session\n        -Next, initialize variables\n        -Then, evaluate the tensor\n        -Finally, return the evaluated value\n\n    Args:\n       inp(tf.Tensor): an arbitrary Tensor to be evaluated\n    Return:\n\t The result of the computation (exact type depends on input)\n    '''\n\n    # Input your code here\n    sess = tf.Session()\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n\n    return sess.run(inp)\n"""
assignments/assignment1/mp1/test.py,13,"b'""""""Simple unit tests for students.""""""\n\nimport unittest\nimport tensorflow as tf\nfrom run_computation import run_computation\nfrom toy_functions import *\n\n\nclass RunComputationTests(unittest.TestCase):\n    def test_run_simple(self):\n        val = tf.constant(True)\n        result = run_computation(val)\n        self.assertEqual(result, True)\n\n\nclass RunToyFnTests(unittest.TestCase):\n    def test_fn_1(self):\n        arg1 = tf.constant([1, 4])\n        arg2 = tf.constant([2, -1])\n        correct = tf.constant([7, 28])\n        attempt = toy_fn_1(arg1, arg2)\n        result = run_computation(tf.reduce_all(tf.equal(correct, attempt)))\n        self.assertEqual(result, True)\n\n    def test_fn_2(self):\n        arg1 = tf.constant([[1, 2], [3, 4]])\n        arg2 = tf.constant([4, 2])\n        correct = tf.constant([-1, 3])\n        attempt = toy_fn_2(arg1, arg2)\n        result = run_computation(tf.reduce_all(tf.equal(correct, attempt)))\n        self.assertEqual(result, True)\n\n    def test_fn_3(self):\n        arg1 = tf.constant([1, 2])\n        arg2 = tf.constant([10, 20])\n        correct = tf.constant([1, 10, 2, 20])\n        attempt = toy_fn_3(arg1, arg2)\n        result = run_computation(tf.reduce_all(tf.equal(correct, attempt)))\n        self.assertEqual(result, True)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment1/mp1/toy_functions.py,22,"b""import tensorflow as tf\n\n\ndef toy_fn_1(arg1, arg2):\n    '''Given two tensors of arbitrary (but same) rank and size, build a computation\n    graph for the following function, which should be computed element-wise:\n\n    arg1^3 + 4*arg2^2 - 10*arg1\n\n    Args:\n        arg1(tf.Tensor): A tensor of arbitrary rank\n        arg2(tf.Tensor): A tensor of the same rank as arg1\n    Returns:\n        (tf.Tensor): the result of the computation (same rank as inputs)\n    '''\n    # Input your code here\n    sess = tf.Session()\n    node1 = tf.pow(arg1, 3)\n    node2 = tf.multiply(tf.pow(arg2, 2), 4)\n    node3 = tf.multiply(arg1, 10)\n\n    addNode = tf.subtract(tf.add(node1, node2), node3)\n\n    return sess.run(addNode)\n\n\ndef toy_fn_2(arg1, arg2):\n    '''Given a rank-two tensor and a rank-one tensor, build a computation graph\n    that computes the following:\n\n    first, it sums over the first dimension of the rank-two tensor\n    (zero-indexed - i.e. sum over the rows). It then subtracts the maximum\n    value of the rank-1 tensor from each element of the result.\n\n    Args:\n        arg1(tf.Tensor): A rank-2 tensor with dimensions (m, n)\n        arg2(tf.Tensor): A rank-1 tensor with dimension p\n    Returns:\n        (tf.Tensor): the result of the computation, which is a rank-1 tensor\n          with dimension m\n    '''\n    # Input your code here\n    sess = tf.Session()\n    reduced_arg = tf.reduce_sum(arg1, 1)\n    output_tensor = tf.subtract(reduced_arg, tf.reduce_max(arg2))\n\n    return sess.run(output_tensor)\n   \n\n\ndef toy_fn_3(arg1, arg2):\n    '''\n    Given two rank-one tensors of the same size, build a computation graph that\n    builds a rank-one tensor by interleaving the two original tensors. For\n    example, given the following inputs:\n\n    arg1 = [1, 2]\n    arg2 = [10, 20]\n\n    The result should be [1, 10, 2, 20]\n\n    Hint: this can be accomplished by first creating a rank-two tensor whose\n    columns are the two original tensors and then reshaping it. Make sure the\n    final tensor is rank-1!\n\n    Args:\n        arg1(tf.Tensor): A rank-1 tensor with dimension m\n        arg2(tf.Tensor): A rank-1 tensor with dimension m\n    Returns:\n        (tf.Tensor): the result of the computation, which is a rank-1 tensor\n          with dimension 2*m\n    '''\n    # Input your code here\n    sess = tf.Session()\n    z = tf.concat([[arg1, arg2]], 0)\n    zT = tf.transpose(z)\n    \n    num = tf.size(zT)\n\n    return (sess.run(tf.reshape(zT, [1, num]))[0])\n"""
assignments/assignment10/mp10/main_vae.py,1,"b'""""""CS446 2018 Spring MP10.\n\nImplementation of a variational autoencoder for image generation.\n""""""\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom vae import VariationalAutoencoder\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n\n\ndef train(model, mnist_dataset, learning_rate=0.0005, batch_size=16,\n          num_steps=5000):\n    """"""Implement the training loop of mini-batch gradient descent.\n\n    Performs mini-batch gradient descent with the indicated batch_size and\n    learning_rate. (**Do not modify this function.**)\n\n    Args:\n        model(VariationalAutoencoder): Initialized VAE model.\n        mnist_dataset: Mnist dataset.\n        learning_rate(float): Learning rate.\n        batch_size(int): Batch size used for training.\n        num_steps(int): Number of steps to run the update ops.\n    """"""\n    for step in range(0, num_steps):\n        batch_x, _ = mnist_dataset.train.next_batch(batch_size)\n        model.session.run(\n            model.update_op_tensor,\n            feed_dict={model.x_placeholder: batch_x,\n                       model.learning_rate_placeholder: learning_rate}\n        )\n\n\ndef main(_):\n    """"""High level pipeline.\n\n    This script performs the training for VAEs.\n    """"""\n    # Get dataset.\n    mnist_dataset = read_data_sets(\'MNIST_data\', one_hot=True)\n\n    # Build model.\n    model = VariationalAutoencoder()\n\n    # Start training\n    train(model, mnist_dataset)\n\n    # Plot out latent space, for +/- 3 std.\n    std = 1\n    x_z = np.linspace(-3 * std, 3 * std, 20)\n    y_z = np.linspace(-3 * std, 3 * std, 20)\n\n    out = np.empty((28 * 20, 28 * 20))\n    for x_idx, x in enumerate(x_z):\n        for y_idx, y in enumerate(y_z):\n            z_mu = np.array([[y, x]])\n            img = model.generate_samples(z_mu)\n            out[x_idx * 28:(x_idx + 1) * 28,\n                y_idx * 28:(y_idx + 1) * 28] = img[0].reshape(28, 28)\n    plt.imsave(\'latent_space_vae.png\', out, cmap=""gray"")\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
assignments/assignment10/mp10/test.py,2,"b'""""""Simple unit tests.""""""\n\nimport unittest\nimport numpy as np\nimport tensorflow as tf\nfrom vae import VariationalAutoencoder\n\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        self.model = VariationalAutoencoder(ndims=20, nlatent=2)\n\n    def test_ouput_shape(self):\n        output_dim = tf.shape(self.model.outputs_tensor)[1]\n        fd = {self.model.x_placeholder: np.zeros([1, 20])}\n        dim = self.model.session.run(output_dim,\n                                     feed_dict=fd)\n        self.assertEqual(dim, 20)\n\n    def test_loss_shape(self):\n        tf.assert_scalar(self.model.loss_tensor)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment10/mp10/vae.py,42,"b'""""""Variation Autoencoder.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import contrib\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib.slim import fully_connected\n\n\nclass VariationalAutoencoder(object):\n    """"""Varational Autoencoder.""""""\n\n    def __init__(self, ndims=784, nlatent=2):\n        """"""Initialize a VAE.\n\n        (**Do not change this function**)\n        Args:\n            ndims(int): Number of dimensions in the feature.\n            nlatent(int): Number of dimensions in the latent space.\n        """"""\n        self._ndims = ndims\n        self._nlatent = nlatent\n\n        # Create session\n        self.session = tf.Session()\n        self.x_placeholder = tf.placeholder(tf.float32, [None, ndims])\n        self.learning_rate_placeholder = tf.placeholder(tf.float32, [])\n\n        # Build graph.\n        self.z_mean, self.z_log_var = self._encoder(self.x_placeholder)\n        self.z = self._sample_z(self.z_mean, self.z_log_var)\n        self.outputs_tensor = self._decoder(self.z)\n\n        # Setup loss tensor, predict_tensor, update_op_tensor\n        self.loss_tensor = self.loss(self.outputs_tensor, self.x_placeholder,\n                                     self.z_mean, self.z_log_var)\n\n        self.update_op_tensor = self.update_op(self.loss_tensor,\n                                               self.learning_rate_placeholder)\n\n        # Initialize all variables.\n        self.session.run(tf.global_variables_initializer())\n\n    def _sample_z(self, z_mean, z_log_var):\n        """"""Sample z using reparametrization trick.\n\n        Args:\n            z_mean (tf.Tensor): The latent mean,\n                tensor of dimension (None, _nlatent)\n            z_log_var (tf.Tensor): The latent log variance,\n                tensor of dimension (None, _nlatent)\n        Returns:\n            z (tf.Tensor): Random sampled z of dimension (None, _nlatent)\n        """"""\n        # z = tf.random_normal(tf.shape(z_mean), mean=z_mean, stddev=tf.sqrt(\n        #     tf.exp(z_log_var)), dtype=tf.float32)\n        epsilon = tf.random_normal(tf.shape(z_mean), 0, 1, dtype=tf.float32)\n        z = z_mean + tf.sqrt(tf.exp(z_log_var)) * epsilon\n        return z\n\n    def _encoder(self, x):\n        """"""Encoder block of the network.\n\n        Builds a two layer network of fully connected layers, with 100 nodes,\n        then 50 nodes, and outputs two branches each with _nlatent nodes\n        representing z_mean and z_log_var. Network illustrated below:\n\n                             |-> _nlatent (z_mean)\n        Input --> 100 --> 50 -\n                             |-> _nlatent (z_log_var)\n\n        Use activation of tf.nn.softplus for hidden layers.\n\n        Args:\n            x (tf.Tensor): The input tensor of dimension (None, _ndims).\n        Returns:\n            z_mean(tf.Tensor): The latent mean, tensor of dimension\n                (None, _nlatent).\n            z_log_var(tf.Tensor): The latent log variance, tensor of dimension\n                (None, _nlatent).\n        """"""\n        # Number of outputs for encoder\n        num_outputs = self._nlatent * 2\n        # Input Layer\n        input_ = fully_connected(\n            inputs=x, num_outputs=100, activation_fn=tf.nn.softplus)\n\n        # Hidden Layer\n        hidden = fully_connected(\n            inputs=input_, num_outputs=50, activation_fn=tf.nn.softplus)\n\n        # Output Layer\n        output = fully_connected(\n            inputs=hidden, num_outputs=num_outputs, activation_fn=None)\n\n        # Extract mean and log-variance from Output\n        z_mean = output[:, 0:self._nlatent]\n        z_log_var = output[:, self._nlatent:]\n        return z_mean, z_log_var\n\n    def _decoder(self, z):\n        """"""Decode back into image, from a sampled z.\n\n        Builds a three layer network of fully connected layers,\n        with 50, 100, _ndims nodes.\n\n        z (_nlatent) --> 50 --> 100 --> _ndims.\n\n        Use activation of tf.nn.softplus for hidden layers.\n\n        Args:\n            z(tf.Tensor): z from _sample_z of dimension (None, _nlatent).\n        Returns:\n            f(tf.Tensor): Decoded features, tensor of dimension (None, _ndims).\n        """"""\n        # Number of outputs for decoder\n        num_outputs = self._ndims\n\n        # Dense Layer\n        dense = fully_connected(inputs=z, num_outputs=50,\n                                activation_fn=tf.nn.softplus)\n\n        # Hidden Layer\n        hidden = fully_connected(\n            inputs=dense, num_outputs=100, activation_fn=tf.nn.softplus)\n\n        # Output Layer\n        output = fully_connected(\n            inputs=hidden, num_outputs=num_outputs, activation_fn=tf.nn.sigmoid)\n        return output\n\n    def _latent_loss(self, z_mean, z_log_var):\n        """"""Construct the latent loss.\n\n        Args:\n            z_mean(tf.Tensor): Tensor of dimension (None, _nlatent)\n            z_log_var(tf.Tensor): Tensor of dimension (None, _nlatent)\n\n        Returns:\n            latent_loss(tf.Tensor): A scalar Tensor of dimension ()\n                containing the latent loss.\n        """"""\n        latent_loss = 0.5 * tf.reduce_mean(tf.reduce_sum(\n            tf.exp(z_log_var) + tf.square(z_mean) - 1 - z_log_var, 1),\n            name=""latent_loss"")\n        return latent_loss\n\n    def _reconstruction_loss(self, f, x_gt):\n        """"""Construct the reconstruction loss, assuming Gaussian distribution.\n\n        Args:\n            f(tf.Tensor): Predicted score for each example, dimension (None,\n                _ndims).\n            x_gt(tf.Tensor): Ground truth for each example, dimension (None,\n                _ndims).\n        Returns:\n            recon_loss(tf.Tensor): A scalar Tensor for dimension ()\n                containing the reconstruction loss.\n        """"""\n        # recon_loss = tf.losses.mean_squared_error(labels=x_gt, predictions=f)\n        recon_loss = tf.nn.l2_loss(f - x_gt, name=""recon_loss"")\n        return recon_loss\n\n    def loss(self, f, x_gt, z_mean, z_log_var):\n        """"""Compute the total loss.\n\n        Computes the sum of latent and reconstruction loss.\n\n        Args:\n            f (tf.Tensor): Decoded image for each example, dimension (None,\n                _ndims).\n            x_gt (tf.Tensor): Ground truth for each example, dimension (None,\n                _ndims)\n            z_mean (tf.Tensor): The latent mean,\n                tensor of dimension (None, _nlatent)\n            z_log_var (tf.Tensor): The latent log variance,\n                tensor of dimension (None, _nlatent)\n\n        Returns:\n            total_loss: Tensor for dimension (). Sum of\n                latent_loss and reconstruction loss.\n        """"""\n        total_loss = self._latent_loss(\n            z_mean, z_log_var) + self._reconstruction_loss(f, x_gt)\n        return total_loss\n\n    def update_op(self, loss, learning_rate):\n        """"""Create the update optimizer.\n\n        Use tf.train.AdamOptimizer to obtain the update op.\n\n        Args:\n            loss(tf.Tensor): Tensor of shape () containing the loss function.\n            learning_rate(tf.Tensor): Tensor of shape (). Learning rate for\n                gradient descent.\n        Returns:\n            train_op(tf.Operation): Update opt tensorflow operation.\n        """"""\n        train_op = tf.train.AdamOptimizer(\n            learning_rate=learning_rate, name=\'Adam\').minimize(loss)\n        return train_op\n\n    def generate_samples(self, z_np):\n        """"""Generate random samples from the provided z_np.\n\n        Args:\n            z_np(numpy.ndarray): Numpy array of dimension\n                (batch_size, _nlatent).\n\n        Returns:\n            out(numpy.ndarray): The sampled images (numpy.ndarray) of\n                dimension (batch_size, _ndims).\n        """"""\n        out = None\n        out = self.outputs_tensor.eval(\n            session=self.session, feed_dict={self.z: z_np})\n        return out\n'"
assignments/assignment11/mp11/input_data.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport tempfile\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets'"
assignments/assignment11/mp11/main_tf.py,2,"b'""""""Generative Adversarial Networks.""""""\n\nimport input_data\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom models.gan import Gan\n\n\ndef train(model, mnist_dataset, learning_rate=0.0005, batch_size=16,\n          num_steps=5000):\n    """"""Implement the training loop of stochastic gradient descent.\n\n    Performs stochastic gradient descent with the indicated batch_size and\n    learning_rate.\n\n    Args:\n        model (GAN): Initialized generative network.\n        mnist_dataset: input_data.\n        learning_rate (float): Learning rate. 0.0005\n        batch_size (int): batch size used for training.\n        num_steps (int): Number of steps to run the update ops. 5000\n    """"""\n    # Iterations for discriminator\n    # According to original GAN paper, k=1\n    d_iters = 5\n\n    # Iterations for generator\n    g_iters = 1\n\n    print(\'batch size: %d, epoch num: %d, learning rate: %f\' %\n          (batch_size, num_steps, learning_rate))\n    print(\'Start training...\')\n\n    # Loss\n    loss_g = []\n    loss_d = []\n\n    # Training\n    for step in range(num_steps):\n        batch_x, _ = mnist_dataset.train.next_batch(batch_size)\n        batch_z = np.random.uniform(-1., 1.,\n                                    [batch_size, model._nlatent]).astype(np.float32)\n\n        # merge = tf.summary.merge_all()\n\n        # Update discriminator by ascending its stochastic gradient\n        for k in range(d_iters):\n\n            _, d_loss = model.session.run(\n                [model.d_optimizer, model.d_loss],\n                feed_dict={model.x_placeholder: batch_x,\n                           model.z_placeholder: batch_z,\n                           model.learning_rate_placeholder: learning_rate}\n            )\n\n            loss_d.append(d_loss)\n\n        # Update generator by descending its stochastic gradient\n        for j in range(g_iters):\n\n            _, g_loss = model.session.run(\n                [model.g_optimizer, model.g_loss],\n                feed_dict={model.z_placeholder: batch_z,\n                           model.learning_rate_placeholder: learning_rate}\n            )\n\n            loss_g.append(g_loss)\n\n        if step % 100 == 0:\n            print(\'Iter: {}\'.format(step))\n            print(\'D_loss: {:.4}\'.format(d_loss))\n            print(\'G_loss: {:.4}\'.format(g_loss))\n\n    #     if step % 50 == 0:\n    #         out = np.empty((28 * 20, 28 * 20))\n    #         for x_idx in range(20):\n    #             for y_idx in range(20):\n    #                 z_mu = np.random.uniform(-1., 1.,\n    #                                          [16, model._nlatent]).astype(np.float32)\n    #                 img = model.generate_samples(z_mu)\n    #                 out[x_idx * 28:(x_idx + 1) * 28,\n    #                     y_idx * 28:(y_idx + 1) * 28] = img[0].reshape(28, 28)\n    #         plt.imsave(\'./tmp/gan_\' + str(step) + \'.png\', out, cmap=""gray"")\n\n    # np.savetxt(""loss_g"", np.array(loss_g), delimiter=\',\')\n    # np.savetxt(""loss_d"", np.array(loss_d), delimiter=\',\')\n\n\ndef main(_):\n    """"""High level pipeline.\n\n    This scripts performs the training for GANs.\n    """"""\n    # Get dataset.\n    mnist_dataset = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\n\n    # Build model.\n    model = Gan(nlatent=10)\n\n    # Start training\n    train(model, mnist_dataset)\n\n    # Plot\n    out = np.empty((28 * 20, 28 * 20))\n    for x_idx in range(20):\n        for y_idx in range(20):\n            z_mu = np.random.uniform(-1., 1.,\n                                     [16, model._nlatent]).astype(np.float32)\n            img = model.generate_samples(z_mu)\n            out[x_idx * 28:(x_idx + 1) * 28,\n                y_idx * 28:(y_idx + 1) * 28] = img[0].reshape(28, 28)\n    plt.imsave(\'gan.png\', out, cmap=""gray"")\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
assignments/assignment11/mp11/test.py,3,"b'""""""Simple unit tests for students.""""""\n\nimport unittest\nimport numpy as np\nfrom models import gan\nimport tensorflow as tf\n\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        tf.reset_default_graph()\n        self.model = gan.Gan()\n\n    def test_ouput_shape(self):\n        test_z = np.random.uniform(-1, 1, [10, 2])\n        np.testing.assert_array_equal(self.model.session.run(tf.shape(\n            self.model.x_hat), feed_dict={self.model.z_placeholder: test_z}), (10, 784))\n\n    def test_generator_loss_shape(self):\n        tf.assert_scalar(self.model.g_loss)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment12/mp12/pong_game.py,0,"b'\nimport numpy as np\nimport pygame\nimport os\nfrom pygame.locals import *\nfrom sys import exit\nimport random\nimport pygame.surfarray as surfarray\n#import matplotlib.pyplot as plt\n\nposition = 5, 325\nos.environ[\'SDL_VIDEO_WINDOW_POS\'] = str(position[0]) + "","" + str(position[1])\npygame.init()\nscreen = pygame.display.set_mode((640, 480), 0, 32)\n\n# Creating 2 bars, a ball and background.\nback = pygame.Surface((640, 480))\nbackground = back.convert()\nbackground.fill((0, 0, 0))\nbar = pygame.Surface((10, 50))\nbar1 = bar.convert()\nbar1.fill((0, 255, 255))\nbar2 = bar.convert()\nbar2.fill((255, 255, 255))\ncirc_sur = pygame.Surface((15, 15))\ncirc = pygame.draw.circle(circ_sur, (255, 255, 255), (7, 7), (7))\ncircle = circ_sur.convert()\ncircle.set_colorkey((0, 0, 0))\nfont = pygame.font.SysFont(""calibri"", 40)\n\n# Variable initialization.\nai_speed = 15.\nHIT_REWARD = 0\nLOSE_REWARD = -1\nSCORE_REWARD = 1\n\n\nclass GameState:\n    """"""The pong game.""""""\n\n    def __init__(self):\n        """"""Model init.""""""\n        self.bar1_x, self.bar2_x = 10., 620.\n        self.bar1_y, self.bar2_y = 215., 215.\n        self.circle_x, self.circle_y = 307.5, 232.5\n        self.bar1_move, self.bar2_move = 0., 0.\n        self.bar1_score, self.bar2_score = 0, 0\n        self.speed_x, self.speed_y = 7., 7.\n\n    def frame_step(self, input_vect):\n        """"""Run one step of the game.\n\n        Args:\n            input_vect: an array with the actions taken\n\n        Returns:\n            image_data: the playground image.\n            reward: the reward obtained from the one move in input_vect.\n            terminal: the game terminated and the scores are reset.\n        """"""\n        # Internally process pygame event handlers.\n        pygame.event.pump()\n\n        # Initialize the reward.\n        reward = 0\n\n        # Check that only one input action is given.\n        if sum(input_vect) != 1:\n            raise ValueError(\'Multiple input actions!\')\n\n        # Key up.\n        if input_vect[1] == 1:\n            self.bar1_move = -ai_speed\n        # Key down.\n        elif input_vect[2] == 1:\n            self.bar1_move = ai_speed\n        # Don\'t move.\n        else:\n            self.bar1_move = 0\n\n        # Scores of the players.\n        self.score1 = font.render(str(self.bar1_score), True, (255, 255, 255))\n        self.score2 = font.render(str(self.bar2_score), True, (255, 255, 255))\n\n        # Draw the screen.\n        screen.blit(background, (0, 0))\n        frame = pygame.draw.rect(\n            screen, (255, 255, 255), Rect((5, 5), (630, 470)), 2)\n        middle_line = pygame.draw.aaline(\n            screen, (255, 255, 255), (330, 5), (330, 475))\n        screen.blit(bar1, (self.bar1_x, self.bar1_y))\n        screen.blit(bar2, (self.bar2_x, self.bar2_y))\n        screen.blit(circle, (self.circle_x, self.circle_y))\n        screen.blit(self.score1, (250., 210.))\n        screen.blit(self.score2, (380., 210.))\n\n        self.bar1_y += self.bar1_move\n\n        # AI of the computer.\n        if self.circle_x >= 305.:\n            if not self.bar2_y == self.circle_y + 7.5:\n                if self.bar2_y < self.circle_y + 7.5:\n                    self.bar2_y += ai_speed\n                if self.bar2_y > self.circle_y - 42.5:\n                    self.bar2_y -= ai_speed\n            else:\n                self.bar2_y == self.circle_y + 7.5\n\n        # Bounds of movement.\n        if self.bar1_y >= 420.:\n            self.bar1_y = 420.\n        elif self.bar1_y <= 10.:\n            self.bar1_y = 10.\n        if self.bar2_y >= 420.:\n            self.bar2_y = 420.\n        elif self.bar2_y <= 10.:\n            self.bar2_y = 10.\n\n        # The ball hitting bars case.\n        if self.circle_x <= self.bar1_x + 10.:\n            if self.circle_y >= self.bar1_y - 7.5 and self.circle_y <= self.bar1_y + 42.5:\n                self.circle_x = 20.\n                self.speed_x = -self.speed_x\n                reward = HIT_REWARD\n\n        if self.circle_x >= self.bar2_x - 15.:\n            if self.circle_y >= self.bar2_y - 7.5 and self.circle_y <= self.bar2_y + 42.5:\n                self.circle_x = 605.\n                self.speed_x = -self.speed_x\n\n        # Scoring.\n        if self.circle_x < 5.:\n            self.bar2_score += 1\n            reward = LOSE_REWARD\n            self.circle_x, self.circle_y = 320., 232.5\n            self.bar1_y, self.bar_2_y = 215., 215.\n        elif self.circle_x > 620.:\n            self.bar1_score += 1\n            reward = SCORE_REWARD\n            self.circle_x, self.circle_y = 307.5, 232.5\n            self.bar1_y, self.bar2_y = 215., 215.\n\n        # Collisions on sides.\n        if self.circle_y <= 10.:\n            self.speed_y = -self.speed_y\n            self.circle_y = 10.\n        elif self.circle_y >= 457.5:\n            self.speed_y = -self.speed_y\n            self.circle_y = 457.5\n\n        # Update the position of the circle.\n        self.circle_x += self.speed_x\n        self.circle_y += self.speed_y\n\n        # Get the playground.\n        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n\n        pygame.display.update()\n\n        # Reset the game.\n        terminal = False\n        if max(self.bar1_score, self.bar2_score) >= 20:\n            self.bar1_score = 0\n            self.bar2_score = 0\n            terminal = True\n\n        return image_data, reward, terminal\n'"
assignments/assignment12/mp12/q_learning.py,23,"b'\nimport tensorflow as tf\nimport cv2\nimport pong_game as game\nimport random\nimport numpy as np\nfrom collections import deque\n\n# Game name.\nGAME = \'Pong\'\n\n# Number of valid actions.\nACTIONS = 3\n\n# Decay rate of past observations.\nGAMMA = 0.99\n\n# Timesteps to observe before training.\nOBSERVE = 5000.\n\n# Frames over which to anneal epsilon.\nEXPLORE = 5000.\n\n# Final value of epsilon.\nFINAL_EPSILON = 0.05\n\n# Starting value of epsilon.\nINITIAL_EPSILON = 1.0\n\n# Number of previous transitions to remember in the replay memory.\nREPLAY_MEMORY = 590000\n\n# Size of minibatch.\nBATCH = 32\n\n# Only select an action every Kth frame, repeat the same action\n# for other frames.\nK = 2\n\n# Learning Rate.\nLr = 1e-6\n\n\ndef weight_variable(shape):\n    """"""Initialize the weight variable.""""""\n    initial = tf.truncated_normal(shape, stddev=0.01)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""Initializa the bias variable.""""""\n    initial = tf.constant(0.01, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W, stride):\n    """"""Define a convolutional layer.""""""\n    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding=""SAME"")\n\n\ndef max_pool_2x2(x):\n    """"""Define a maxpooling layer.""""""\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n\n\ndef createNetwork():\n    """"""Create a convolutional network for estimating the Q value.\n\n    Args:\n    Returns:\n        s: Input layer\n        readout: Output layer with the Q-values for every possible action\n    """"""\n    # Initialize the network weights and biases.\n    W_conv1 = weight_variable([8, 8, 4, 32])\n    b_conv1 = bias_variable([32])\n\n    W_conv2 = weight_variable([4, 4, 32, 64])\n    b_conv2 = bias_variable([64])\n\n    W_conv3 = weight_variable([3, 3, 64, 64])\n    b_conv3 = bias_variable([64])\n\n    W_fc1 = weight_variable([1600, 512])\n    b_fc1 = bias_variable([512])\n\n    W_fc2 = weight_variable([512, ACTIONS])\n    b_fc2 = bias_variable([ACTIONS])\n\n    # Input layer.\n    s = tf.placeholder(""float"", [None, 80, 80, 4])\n\n    # Hidden layers.\n    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n\n    # Output layer\n    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n\n    return s, readout\n\n\ndef get_action_index(readout_t, epsilon, t):\n    """"""Choose an action epsilon-greedily.\n\n    Details:\n        choose an action randomly:\n        (1) in the observation phase (t < OBSERVE).\n        (2) beyond the observation phase with probability ""epsilon"".\n        otherwise, choose the action with the highest Q-value.\n    Args:\n        readout_t: a vector with the Q-value associated with every action.\n        epsilon: temperature variable for exploration-exploitation.\n        t: current number of iterations.\n    Returns:\n        index: the index of the action to be taken next.\n    """"""\n    p = random.random()\n    if t < OBSERVE:\n        action_index = random.randint(0, len(readout_t) - 1)\n    else:\n        if p >= epsilon:\n            action_index = np.argmax(readout_t)\n        else:\n            action_index = random.randint(0, len(readout_t) - 1)\n\n    return action_index\n\n\ndef scale_down_epsilon(epsilon, t):\n    """"""Epsilon decrease.\n\n    Decrease epsilon after by ((INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE)\n    in case epsilon is larger than the desired final epsilon or beyond\n    the observation phase.\n    Args:\n        epsilon: the current value of epsilon.\n        t: current number of iterations.\n    Returns:\n        the updated epsilon\n    """"""\n    if (t > OBSERVE) and (epsilon > FINAL_EPSILON):\n        epsilon -= ((INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE)\n    return epsilon\n\n\ndef run_selected_action(a_t, s_t, game_state):\n    """"""Run the selected action and return the next state and reward.\n\n    Do not forget that state is composed of the 4 previous frames.\n    Hint: check the initialization for the interface to the game simulator.\n\n    Args:\n        a_t: current action.\n        s_t: current state.\n        game_state: game state to communicate with emulator.\n    Returns:\n        s_t1: next state.\n        r_t: reward.\n        terminal: indicating whether the episode terminated (output of the simulator).\n    """"""\n    x_t, r_t, terminal = game_state.frame_step(a_t)\n    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n    ret, x_t = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\n    s_t1 = np.stack((s_t[:, :, 1], s_t[:, :, 2], s_t[:, :, 3], x_t), axis=2)\n\n    return s_t1, r_t, terminal\n\n\ndef compute_cost(target_q, a_t, q_value):\n    """"""Compute the cost.\n\n    Args:\n        target_q: target Q-value.\n        a_t: current action.\n        q_value: current Q-value.\n    Returns:\n        cost\n    """"""\n    # Q-value for the action.\n    readout_action = tf.reduce_sum(\n        tf.multiply(q_value, a_t), reduction_indices=1)\n\n    # Q-Learning Cost.\n    cost = tf.reduce_mean(tf.square(target_q - readout_action))\n\n    return cost\n\n\ndef compute_target_q(r_batch, readout_j1_batch, terminal_batch):\n    """"""Compute the target Q-value for all samples in the batch.\n\n    Distinguish two cases:\n    1. The next state is a terminal state.\n    2. The next state is not a terminal state.\n    Args:\n        r_batch: batch of rewards.\n        readout_j1_batch: batch of Q-values associated with the next state.\n        terminal_batch: batch of boolean variables indicating the game termination.\n    Returns:\n        target_q_batch: batch of target Q values.\n\n    Hint: distinguish two cases: (1) terminal state and (2) non terminal states\n    """"""\n    target_q_batch = []\n\n    for i in range(0, len(terminal_batch)):\n        # If the terminal state is reached, the Q-value is only equal to the reward.\n        if terminal_batch[i]:\n            target_q_batch.append(r_batch[i])\n        else:\n            target_q_batch.append(\n                r_batch[i] + GAMMA * readout_j1_batch[i].max())\n\n    return target_q_batch\n\n\ndef trainNetwork(s, readout, sess):\n    """"""Train the artificial agent using Q-learning to play the pong game.\n\n    Args:\n        s: the current state formed by 4 frames of the playground.\n        readout: the Q value for each passible action in the current state.\n        sess: session\n    """"""\n    # Placeholder for the action.\n    a = tf.placeholder(""float"", [None, ACTIONS])\n\n    # Placeholder for the target Q value.\n    y = tf.placeholder(""float"", [None])\n\n    # Compute the loss.\n    cost = compute_cost(y, a, readout)\n\n    # Training operation.\n    train_step = tf.train.AdamOptimizer(Lr).minimize(cost)\n\n    # Open up a game state to communicate with emulator.\n    game_state = game.GameState()\n\n    # Initialize the replay memory.\n    D = deque()\n\n    # Initialize the action vector.\n    do_nothing = np.zeros(ACTIONS)\n    do_nothing[0] = 1\n\n    # Initialize the state of the game.\n    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n    ret, x_t = cv2.threshold(x_t, 1, 255, cv2.THRESH_BINARY)\n    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n\n    # Save and load model checkpoints.\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    checkpoint = tf.train.get_checkpoint_state(""./saved_networks_q_learning/"")\n    if checkpoint and checkpoint.model_checkpoint_path:\n        saver.restore(sess, checkpoint.model_checkpoint_path)\n        print(""Successfully loaded:"", checkpoint.model_checkpoint_path)\n    else:\n        print(""Could not find old network weights"")\n\n    # Initialize the epsilon value for the exploration phase.\n    epsilon = INITIAL_EPSILON\n\n    # Initialize the iteration counter.\n    t = 0\n\n    while True:\n        # Choose an action epsilon-greedily.\n        readout_t = readout.eval(feed_dict={s: [s_t]})[0]\n\n        action_index = get_action_index(readout_t, epsilon, t)\n\n        a_t = np.zeros([ACTIONS])\n\n        a_t[action_index] = 1\n\n        # Scale down epsilon during the exploitation phase.\n        epsilon = scale_down_epsilon(epsilon, t)\n\n        # Run the selected action and update the replay memeory\n        for i in range(0, K):\n            # Run the selected action and observe next state and reward.\n            s_t1, r_t, terminal = run_selected_action(a_t, s_t, game_state)\n\n            # Store the transition in the replay memory D.\n            D.append((s_t, a_t, r_t, s_t1, terminal))\n            if len(D) > REPLAY_MEMORY:\n                D.popleft()\n\n        # Start training once the observation phase is over.\n        if (t > OBSERVE):\n\n            # Sample a minibatch to train on.\n            minibatch = random.sample(list(D), BATCH)\n\n            # Get the batch variables.\n            s_j_batch = [d[0] for d in minibatch]\n            a_batch = [d[1] for d in minibatch]\n            r_batch = [d[2] for d in minibatch]\n            s_j1_batch = [d[3] for d in minibatch]\n            terminal_batch = [d[4] for d in minibatch]\n\n            # Compute the target Q-Value\n            readout_j1_batch = readout.eval(feed_dict={s: s_j1_batch})\n            target_q_batch = compute_target_q(\n                r_batch, readout_j1_batch, terminal_batch)\n\n            # Perform gradient step.\n            train_step.run(feed_dict={\n                y: target_q_batch,\n                a: a_batch,\n                s: s_j_batch})\n\n        # Update the state.\n        s_t = s_t1\n\n        # Update the number of iterations.\n        t += 1\n\n        # Save a checkpoint every 10000 iterations.\n        if t % 10000 == 0:\n            saver.save(sess, \'saved_networks_q_learning/\' +\n                       GAME + \'-dqn\', global_step=t)\n\n        # Print info.\n        state = """"\n        if t <= OBSERVE:\n            state = ""observe""\n        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n            state = ""explore""\n        else:\n            state = ""train""\n        print(""TIMESTEP"", t, ""/ STATE"", state, ""/ EPSILON"", epsilon, ""/ ACTION"",\n              action_index, ""/ REWARD"", r_t, ""/ Q_MAX %e"" % np.max(readout_t))\n\n\ndef playGame():\n    """"""Paly the pong game.""""""\n    # Start an active session.\n    sess = tf.InteractiveSession()\n\n    # Create the network.\n    s, readout = createNetwork()\n\n    # Q-Learning\n    s, readout = trainNetwork(s, readout, sess)\n\n\ndef main():\n    """"""Main function.""""""\n    playGame()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
assignments/assignment12/mp12/test.py,0,"b'""""""Unit Tests examples for mp12.""""""\n\nfrom q_learning import get_action_index, scale_down_epsilon, run_selected_action, compute_target_q\nimport unittest\nimport pong_game as game\nimport numpy as np\nimport cv2\n\n# Frames over which to anneal epsilon.\nEXPLORE = 5000.\n\n# Final value of epsilon.\nFINAL_EPSILON = 0.05\n\n# Starting value of epsilon.\nINITIAL_EPSILON = 1.0\n\n# Number of valid actions.\nACTIONS = 3\n\n# Decay rate of past observations.\nGAMMA = 0.99\n\n\nclass QlearningTest(unittest.TestCase):\n\n    def test_1(self):\n        epsilon = 0.01\n        t = 2000\n        out = scale_down_epsilon(epsilon, t)\n        self.assertEqual(out, epsilon)\n\n    def test_2(self):\n        action_index_list = []\n\n        for i in range(10):\n            readout_t = [5, 2, 7, 18, 3]\n            epsilon = 0.1\n            t = 7000\n            action_index = get_action_index(readout_t, epsilon, t)\n            action_index_list.append(action_index)\n\n        self.assertGreaterEqual(action_index_list.count(3), 6)\n\n    def test_3(self):\n        r_batch = [5]\n        readout_j1_batch = [[1, 2, 3, 4]]\n        terminal_batch = [True]\n        target_q_batch = compute_target_q(\n            r_batch, readout_j1_batch, terminal_batch)\n        self.assertEqual(target_q_batch[0], r_batch[0])\n'"
assignments/assignment2/mp2/main.py,2,"b'""""""Main function for train, eval, and test.""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.linear_regression import LinearRegression\nfrom train_eval_model import train_model, eval_model, train_model_analytic\nfrom utils.io_tools import read_dataset\nfrom utils.data_tools import preprocess_data\nfrom utils.plot_tools import plot_x_vs_y\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float(\'learning_rate\', 0.00001, \'Initial learning rate.\')\nflags.DEFINE_float(\'w_decay_factor\', 0.0001, \'Weight decay factor.\')\nflags.DEFINE_integer(\'num_steps\', 100000, \'Number of update steps to run.\')\nflags.DEFINE_string(\'opt_method\', \'iter\', \'Supports [""iter"", ""analytic""]\')\nflags.DEFINE_string(\n    \'feature_columns\',\n    \'BldgType,OverallQual\',\n    \'Comma separated feature names.\')\n\n# 100000  Id,BldgType,OverallQual,GrLivArea,GarageArea,SalePrice\n# flags.DEFINE_float(\'learning_rate\', 0.00001, \'Initial learning rate.\')\n# flags.DEFINE_float(\'w_decay_factor\', 0.0001, \'Weight decay factor.\')\n# flags.DEFINE_integer(\'num_steps\', 100000, \'Number of update steps to run.\')\n\n\ndef main(_):\n    """"""High level pipeline.\n    This script performs the trainsing, evaling and testing state of the model.\n    """"""\n    learning_rate = FLAGS.learning_rate\n    w_decay_factor = FLAGS.w_decay_factor\n    num_steps = FLAGS.num_steps\n    opt_method = FLAGS.opt_method\n    feature_columns = FLAGS.feature_columns.split(\',\')\n\n    # Load dataset.\n    dataset = read_dataset(""data/train.csv"")\n\n    # Data processing.\n    train_set = preprocess_data(dataset, feature_columns=feature_columns,\n                                squared_features=True)\n\n    # Initialize model.\n    ndim = train_set[0].shape[1]\n    model = LinearRegression(ndim, \'zeros\')\n\n    # Train model.\n    if opt_method == \'iter\':\n        # Perform gradient descent.\n        train_model(train_set, model, learning_rate, num_steps=num_steps)\n        print(\'Performed gradient descent.\')\n    else:\n        # Compute closed form solution.\n        train_model_analytic(train_set, model)\n        print(\'Closed form solution.\')\n\n    train_loss = eval_model(train_set, model)\n    print(""Train loss: %s"" % train_loss)\n\n    # Plot the x vs. y if one dimension.\n    if train_set[0].shape[1] == 1:\n        plot_x_vs_y(train_set, model)\n\n    # Eval model.\n    raw_eval = read_dataset(""data/val.csv"")\n    eval_set = preprocess_data(raw_eval, feature_columns=feature_columns,\n                               squared_features=True)\n    eval_loss = eval_model(eval_set, model)\n    print(""Eval loss: %s"" % eval_loss)\n\n    # Test model.\n    raw_test = read_dataset(""data/test.csv"")\n    test_set = preprocess_data(raw_test, feature_columns=feature_columns,\n                               squared_features=True)\n    test_loss = eval_model(test_set, model)\n    print(""Test loss: %s"" % test_loss)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
assignments/assignment2/mp2/test.py,0,"b'""""""Simple unit tests for students.""""""\n\nimport unittest\nimport numpy as np\nfrom utils import io_tools\nfrom models import linear_regression\n\n\nclass IoToolsTests(unittest.TestCase):\n    def setUp(self):\n        self.dataset = io_tools.read_dataset(""data/train.csv"")\n\n    def test_read_dataset_not_none(self):\n        self.assertIsNotNone(self.dataset)\n\n    def test_first_row(self):\n        keys = sorted(list(self.dataset.keys()))\n        val0 = (self.dataset[keys[0]])\n        val0_true = (\'1\', \'1Fam\', \'7\', \'1710\', \'548\', \'208500\')\n        self.assertEqual(val0, val0_true)\n\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        self.model = linear_regression.LinearRegression(5, \'zeros\')\n\n    def test_forward_shape(self):\n        x = np.zeros((10, 5))\n        y_hat = self.model.forward(x)\n        self.assertEqual(y_hat.shape, (10, 1))\n\n    def test_forward_zero(self):\n        x = np.zeros((10, 5))\n        y = np.zeros((10, 1))\n        y_hat = self.model.forward(x)\n        np.testing.assert_array_equal(y, y_hat)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment2/mp2/train_eval_model.py,0,"b'""""""Train model and eval model helpers.""""""\nfrom __future__ import print_function\n\nimport numpy as np\nfrom models.linear_regression import LinearRegression\n\n\ndef train_model(processed_dataset, model, learning_rate=0.001, batch_size=16,\n                num_steps=1000, shuffle=True):\n    """"""Implement the training loop of stochastic gradient descent.\n\n    Performs stochastic gradient descent with the indicated batch_size.\n    If shuffle is true:\n        Shuffle data at every epoch, including the 0th epoch.\n    If the number of example is not divisible by batch_size, the last batch\n    will simply be the remaining examples.\n\n    Args:\n        processed_dataset(list): Data loaded from data_tools\n        model(LinearModel): Initialized linear model.\n        learning_rate(float): Learning rate of your choice\n        batch_size(int): Batch size of your choise.\n        num_steps(int): Number of steps to run the updated.\n        shuffle(bool): Whether to shuffle data at every epoch.\n    Returns:\n        model(LinearModel): Returns a trained model.\n    """"""\n    # Seperate the processed_dataset to two parts\n    x = processed_dataset[0]\n    y = processed_dataset[1]\n\n    # Assign dimesion\n    M, N = x.shape\n\n    # If the number of example is not divisible by batch_size, the last batch\n    # will simply be the remaining examples.\n    if M % batch_size != 0:\n        last_batch_size = M % batch_size\n        num_batches = M // batch_size + 1\n\n        for step in range(num_steps):\n\n            idx_total = np.arange(M)\n            np.random.shuffle(idx_total)\n\n            for batch in range(num_batches):\n                if (batch == num_batches - 1):\n                    if shuffle:\n                        idx = idx_total[batch * batch_size:]\n                    else:\n                        idx = np.arange(batch * batch_size, x.shape[0])\n                else:\n                    if shuffle:\n                        idx = idx_total[batch * batch_size:(batch + 1) * batch_size]\n                    else:\n                        idx = np.arange(batch * batch_size, (batch + 1) * batch_size)\n\n                # Assignment batch for x and y\n                x_batch = x[idx]\n                y_batch = y[idx]\n\n                model.w = update_step(x_batch, y_batch, model, learning_rate)\n\n    return model\n\n\ndef update_step(x_batch, y_batch, model, learning_rate):\n    """"""Perform on single update step, (i.e. forward then backward).\n\n    Args:\n        x_batch(numpy.ndarray): input data of dimension (N, ndims).\n        y_batch(numpy.ndarray): label data of dimension (N, 1).\n        model(LinearModel): Initialized linear model.\n    """"""\n    y_batch = np.reshape(y_batch, (x_batch.shape[0], 1))\n    y_hat = model.forward(x_batch)\n    temp_grad = model.backward(y_hat, y_batch)\n    model.w = model.w - (learning_rate * temp_grad)\n\n    return model.w\n\n\ndef train_model_analytic(processed_dataset, model):\n    """"""Compute and sets the optimal model weights (model.w).\n\n    Args:\n        processed_dataset(list): List of [x,y] processed\n            from utils.data_tools.preprocess_data.\n        model(LinearRegression): LinearRegression model.\n    """"""\n    x = processed_dataset[0]\n    y = processed_dataset[1]\n\n    # weight is a (N,1) column vector\n    x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n\n    model.w = np.linalg.inv(x.T.dot(x) + model.w_decay_factor * np.ones((x.T.dot(x).shape))).dot(x.T).dot(y)\n    return model.w\n\n\ndef eval_model(processed_dataset, model):\n    """"""Perform evaluation on a dataset.\n\n    Args:\n        processed_dataset(list): Data loaded from data_tools.\n        model(LinearModel): Initialized linear model.\n    Returns:\n        loss(float): model loss on data.\n        acc(float): model accuracy on data.\n    """"""\n    # Seperate the processed_dataset to two parts\n    x = processed_dataset[0]\n    y = processed_dataset[1]\n\n    f = model.forward(x)\n\n    # You can disregard accuracy.\n    loss = model.total_loss(f, y)\n    return loss\n'"
assignments/assignment3/mp3/test_scratch.py,0,"b'""""""Simple unit tests on codefromscratch.""""""\n\nimport unittest\nimport numpy as np\nfrom codefromscratch import io_tools\nfrom codefromscratch import logistic_model\n\n\nclass ScratchTests(unittest.TestCase):\n    def setUp(self):\n        # load dataset\n        self.A = None\n        self.T = None\n        self.N = None\n        try:\n            self.A, self.T = io_tools.read_dataset(\n                path_to_dataset_folder=\'data/trainset\', index_filename=\'indexing.txt\')\n            self.N = len(self.T)\n        except BaseException:\n            pass\n        # Initialize model.\n        self.model = None\n        self.model_W = None\n        try:\n            self.model = logistic_model.LogisticModel(ndims=16, W_init=\'zeros\')\n            self.model_W = self.model.W\n        except BaseException:\n            pass\n\n    def test_read_dataset_not_none(self):\n        self.assertIsNotNone(self.A)\n        self.assertIsNotNone(self.T)\n\n    def test_init_model_not_none(self):\n        self.assertIsNotNone(self.model)\n        self.assertIsNotNone(self.model_W)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment3/mp3/test_tf.py,0,"b'""""""Simple unit tests on codefromtf""""""\n\nimport unittest\nimport numpy as np\nfrom codefromtf import io_tools\nfrom codefromtf import logistic_model\n\n\nclass TFTests(unittest.TestCase):\n    def setUp(self):\n        # load dataset\n        self.A = None\n        self.T = None\n        self.N = None\n        try:\n            self.A, self.T = io_tools.read_dataset_tf(\n                path_to_dataset_folder=\'data/trainset\', index_filename=\'indexing.txt\')\n            self.N = len(self.T)\n        except BaseException:\n            pass\n        # Initialize model.\n        self.model = None\n        self.model_W0 = None\n        try:\n            self.model = logistic_model.LogisticModel_TF(\n                ndims=16, W_init=\'zeros\')\n            self.model_W0 = self.model.W0\n        except BaseException:\n            pass\n\n    def test_read_dataset_not_none(self):\n        self.assertIsNotNone(self.A)\n        self.assertIsNotNone(self.T)\n\n    def test_init_model_not_none(self):\n        self.assertIsNotNone(self.model)\n        self.assertIsNotNone(self.model_W0)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment4/mp4/main.py,2,"b'""""""Main function for train, eval, and test. This file will not be graded.""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom models.support_vector_machine import SupportVectorMachine\nfrom train_eval_model import train_model, eval_model, train_model_qp\nfrom utils.io_tools import read_dataset\nfrom utils.data_tools import preprocess_data\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Initial learning rate.\')\nflags.DEFINE_float(\'w_decay_factor\', 0.001, \'Weight decay factor.\')\nflags.DEFINE_integer(\'num_steps\', 5000, \'Number of update steps to run.\')\nflags.DEFINE_string(\n    \'feature_type\',\n    \'default\',\n    \'Feature type, supports [raw, default, custom]\')\nflags.DEFINE_string(\'opt_method\', \'qp\', \'Supports [""iter"", ""qp""]\')\n\n\ndef main(_):\n    """"""High level pipeline.\n\n    This script performs the trainsing, evaling and testing state of the model.\n    """"""\n    learning_rate = FLAGS.learning_rate\n    w_decay_factor = FLAGS.w_decay_factor\n    num_steps = FLAGS.num_steps\n    opt_method = FLAGS.opt_method\n    feature_type = FLAGS.feature_type\n\n    # Load dataset and data processing.\n    train_set = read_dataset(""data/train.txt"", ""data/image_data/"")\n    train_set = preprocess_data(train_set, feature_type)\n\n    # Initialize model.\n    ndim = train_set[\'image\'][0].shape[0]\n    model = SupportVectorMachine(\n        ndim, \'ones\', w_decay_factor=FLAGS.w_decay_factor)\n\n    # Train model.\n    if opt_method == \'iter\':\n        # Perform gradient descent.\n        train_model(train_set, model, learning_rate, num_steps=num_steps)\n        print(\'Performed gradient descent.\')\n    else:\n        # Compute closed form solution.\n        train_model_qp(train_set, model)\n        print(\'Finished QP Solver.\')\n\n    train_loss, train_acc = eval_model(train_set, model)\n    print(""Train loss: %s"" % train_loss)\n    print(""Train acc: %s"" % train_acc)\n\n    # Eval model.\n    eval_set = read_dataset(""data/val.txt"", ""data/image_data/"")\n    eval_set = preprocess_data(eval_set, feature_type)\n    eval_loss, eval_acc = eval_model(eval_set, model)\n    print(""Eval loss: %s"" % eval_loss)\n    print(""Eval acc: %s"" % eval_acc)\n\n    # Test model.\n    test_set = read_dataset(""data/test.txt"", ""data/image_data/"")\n    test_set = preprocess_data(test_set, feature_type)\n    test_loss, test_acc = eval_model(test_set, model)\n    print(""Test loss: %s"" % test_loss)\n    print(""Test acc: %s"" % test_acc)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
assignments/assignment4/mp4/test.py,0,"b'""""""Simple unit tests for students. (This file will not be graded.)""""""\n\nimport unittest\nimport numpy as np\nfrom utils import io_tools\nfrom utils import data_tools\nfrom models import support_vector_machine\nfrom train_eval_model import qp_helper\n\n\nclass IoToolsTests(unittest.TestCase):\n    def setUp(self):\n        self.dataset = io_tools.read_dataset(\n            ""data/train.txt"", ""data/image_data/"")\n\n    def test_read_dataset_not_none(self):\n        self.assertIsNotNone(self.dataset)\n\n    def test_data_shape(self):\n        image = self.dataset[\'image\']\n        label = self.dataset[\'label\']\n        self.assertEqual(image.shape[0], label.shape[0])\n\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        self.model = support_vector_machine.SupportVectorMachine(5, \'zeros\')\n\n    def test_forward_shape(self):\n        x = np.zeros((10, 5))\n        y_hat = self.model.forward(x)\n        self.assertEqual(y_hat.shape, (10, 1))\n\n    def test_forward_zero(self):\n        x = np.zeros((10, 5))\n        y = np.zeros((10, 1))\n        y_hat = self.model.forward(x)\n        np.testing.assert_array_equal(y, y_hat)\n\n\nclass QpTests(unittest.TestCase):\n    def setUp(self):\n        self.dataset = io_tools.read_dataset(\n            ""data/train.txt"", ""data/image_data/"")\n        self.dataset = data_tools.preprocess_data(self.dataset, \'raw\')\n        self.model = support_vector_machine.SupportVectorMachine(\n            8 * 8 * 3, \'zeros\')\n\n    def test_qp(self):\n        P, q, G, h = qp_helper(self.dataset, self.model)\n        self.assertEqual(P.shape[0], q.shape[0])\n        self.assertEqual(G.shape[0], h.shape[0])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment4/mp4/train_eval_model.py,0,"b'""""""Train model and eval model helpers.""""""\nfrom __future__ import print_function\n\nimport numpy as np\nimport cvxopt\nimport cvxopt.solvers\n\n\ndef train_model(data, model, learning_rate=0.001, batch_size=16,\n                num_steps=1000, shuffle=True):\n    """"""Implement the training loop of stochastic gradient descent.\n\n    Perform stochastic gradient descent with the indicated batch_size.\n\n    If shuffle is true:\n        Shuffle data at every epoch, including the 0th epoch.\n\n    If the number of example is not divisible by batch_size, the last batch\n    will simply be the remaining examples.\n\n    Args:\n        data(dict): Data loaded from io_tools\n        model(LinearModel): Initialized linear model.\n        learning_rate(float): Learning rate of your choice\n        batch_size(int): Batch size of your choise.\n        num_steps(int): Number of steps to run the updated.\n        shuffle(bool): Whether to shuffle data at every epoch.\n\n    Returns:\n        model(LinearModel): Returns a trained model.\n    """"""\n    # Performs gradient descent. (This function will not be graded.)\n    # Seperate the data dict to two parts\n    x = data[\'image\']\n    y = data[\'label\']\n\n    # Assign dimesion\n    M, N = x.shape\n\n    # If the number of example is not divisible by batch_size, the last batch\n    # will simply be the remaining examples.\n    if M % batch_size != 0:\n        last_batch_size = M % batch_size\n        num_batches = M // batch_size + 1\n\n        for step in range(num_steps):\n\n            idx_total = np.arange(M)\n            np.random.shuffle(idx_total)\n            # np.random.permutation(len(M)) is another choice to shuffle the\n            # data. Here I reserved the previous method.\n\n            for batch in range(num_batches):\n                if (batch == num_batches - 1):\n                    if shuffle:\n                        idx = idx_total[batch * batch_size:]\n                    else:\n                        idx = np.arange(batch * batch_size, x.shape[0])\n                else:\n                    if shuffle:\n                        idx = idx_total[batch *\n                                        batch_size:(batch + 1) * batch_size]\n                    else:\n                        idx = np.arange(\n                            batch * batch_size, (batch + 1) * batch_size)\n\n                # Assignment batch for x and y\n                x_batch = x[idx]\n                y_batch = y[idx]\n\n                update_step(x_batch, y_batch, model, learning_rate)\n\n    return model\n\n\ndef update_step(x_batch, y_batch, model, learning_rate):\n    """"""Perform on single update step, (i.e. forward then backward).\n\n    Args:\n        x_batch(numpy.ndarray): input data of dimension (N, ndims).\n        y_batch(numpy.ndarray): label data of dimension (N, 1).\n        model(LinearModel): Initialized linear model.\n    """"""\n    # Implementation here. (This function will not be graded.)\n    y_batch = np.reshape(y_batch, (x_batch.shape[0], 1))\n    y_hat = model.forward(x_batch)\n    grad = np.reshape(model.backward(y_hat, y_batch), (model.ndims + 1, 1))\n    model.w = model.w - (learning_rate * grad)\n\n\ndef train_model_qp(data, model):\n    """"""Compute and set the optimal model wegiths (model.w) using a QP solver.\n\n    Args:\n        data(dict): Data from utils.data_tools.preprocess_data.\n        model(SupportVectorMachine): Support vector machine model.\n    """"""\n    P, q, G, h = qp_helper(data, model)\n    P = cvxopt.matrix(P, P.shape, \'d\')\n    q = cvxopt.matrix(q, q.shape, \'d\')\n    G = cvxopt.matrix(G, G.shape, \'d\')\n    h = cvxopt.matrix(h, h.shape, \'d\')\n    sol = cvxopt.solvers.qp(P, q, G, h)\n    z = np.array(sol[\'x\'])\n\n    # Implementation here (do not modify the code above)\n    x = data[\'image\']\n    # dim = x.shape[1]\n\n    # If we have only one example, it will be vector\n    try:\n        dim = x.shape[1]\n    except BaseException:\n        dim = x.shape[0]\n\n    # Set model.w\n    model.w = z[0:dim + 1]\n\n\ndef qp_helper(data, model):\n    """"""Prepare arguments for the qpsolver.\n\n    Args:\n        data(dict): Data from utils.data_tools.preprocess_data.\n        model(SupportVectorMachine): Support vector machine model.\n\n    Returns:\n        P(numpy.ndarray): P matrix in the qp program.\n        q(numpy.ndarray): q matrix in the qp program.\n        G(numpy.ndarray): G matrix in the qp program.\n        h(numpy.ndarray): h matrix in the qp program.\n    """"""\n    # Implementation here.\n    x = data[\'image\']\n    y = data[\'label\']\n    # num = x.shape[0]\n    # dim = x.shape[1]\n    # ndims = num + dim\n\n    # x = data[\'image\'][0]\n    # y = data[\'label\'][0]\n    # If we have only one example, it will be vector\n    try:\n        num = x.shape[0]\n        dim = x.shape[1]\n    except BaseException:\n        dim = x.shape[0]\n        num = 1\n        x = np.reshape(x, (1, x.shape[0]))\n\n    ndims = num + dim\n\n    # P matrix -> Qudratic parameters\n    P = np.zeros((ndims + 1, ndims + 1))\n    P[0:dim, 0:dim] = np.eye(dim) * model.w_decay_factor\n\n    # q vecor -> linear parameters\n    q1 = np.zeros((dim + 1, 1))\n    q2 = np.ones((num, 1))\n    q = np.vstack((q1, q2))\n\n    # First block of G matrix\n    x1 = np.hstack((x, np.ones((num, 1))))\n    x2 = np.multiply(-y, x1)\n    G1 = np.hstack((x2, -np.eye(num)))\n\n    # Second block of G matrix\n    g1 = np.zeros((num, dim + 1))\n    g2 = -np.eye(num)\n    G2 = np.hstack((g1, g2))\n\n    # G matrix -> Constraints parameters\n    G = np.vstack((G1, G2))\n\n    h1 = -np.ones((num, 1))\n    h2 = np.zeros((num, 1))\n    h = np.vstack((h1, h2))\n\n    return P, q, G, h\n\n\ndef eval_model(data, model):\n    """"""Perform evaluation on a dataset.\n\n    Args:\n        data(dict): Data loaded from io_tools.\n        model(LinearModel): Initialized linear model.\n\n    Returns:\n        loss(float): model loss on data.\n        acc(float): model accuracy on data.\n    """"""\n    # Implementation here.\n    x = data[\'image\']\n    y = data[\'label\']\n\n    f = model.forward(x)\n    loss = model.total_loss(f, y)\n    acc = np.asscalar(np.sum(model.predict(f) == y) / y.shape[0])\n\n    return loss, acc\n'"
assignments/assignment5/mp5/main.py,0,"b""import numpy as np\nfrom sklearn import metrics\n\nfrom model.sklearn_multiclass import sklearn_multiclass_prediction\nfrom model.self_multiclass import MulticlassSVM\n\nif __name__ == '__main__':\n    print('Loading data...')\n    mnist = np.loadtxt('data/mnist_test.csv', delimiter=',')\n\n    X_train = mnist[:len(mnist) // 2, 1:]\n    y_train = mnist[:len(mnist) // 2, 0].astype(np.int)\n\n    X_test = mnist[len(mnist) // 2:, 1:]\n    y_test = mnist[len(mnist) // 2:, 0].astype(np.int)\n\n    print('Training Sklearn OVR...')\n    y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n        'ovr', X_train, y_train, X_test)\n    print('Sklearn OVR Accuracy (train):',\n          metrics.accuracy_score(y_train, y_pred_train))\n    print('Sklearn OVR Accuracy (test) :',\n          metrics.accuracy_score(y_test, y_pred_test))\n\n    print('Training Sklearn OVO...')\n    y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n        'ovo', X_train, y_train, X_test)\n    print('Sklearn OVO Accuracy (train):',\n          metrics.accuracy_score(y_train, y_pred_train))\n    print('Sklearn OVO Accuracy (test) :',\n          metrics.accuracy_score(y_test, y_pred_test))\n\n    print('Training Sklearn Crammer-Singer...')\n    y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n        'crammer', X_train, y_train, X_test)\n    print('Sklearn Crammer-Singer Accuracy (train):',\n          metrics.accuracy_score(y_train, y_pred_train))\n    print('Sklearn Crammer-Singer Accuracy (test) :',\n          metrics.accuracy_score(y_test, y_pred_test))\n\n    print('Training self OVR...')\n    self_ovr = MulticlassSVM('ovr')\n    self_ovr.fit(X_train, y_train)\n    print('Self OVR Accuracy (train):',\n          metrics.accuracy_score(y_train, self_ovr.predict(X_train)))\n    print('Self OVR Accuracy (test) :',\n          metrics.accuracy_score(y_test, self_ovr.predict(X_test)))\n\n    print('Training self OVO...')\n    self_ovo = MulticlassSVM('ovo')\n    self_ovo.fit(X_train, y_train)\n    print('Self OVO Accuracy (train):',\n          metrics.accuracy_score(y_train, self_ovo.predict(X_train)))\n    print('Self OVO Accuracy (test) :',\n          metrics.accuracy_score(y_test, self_ovo.predict(X_test)))\n\n    print('Training self Crammer-Singer...')\n    self_cs = MulticlassSVM('crammer-singer')\n    self_cs.fit(X_train, y_train)\n    print('Self Crammer-Singer Accuracy (train):',\n          metrics.accuracy_score(y_train, self_cs.predict(X_train)))\n    print('Self Crammer-Singer Accuracy (test) :',\n          metrics.accuracy_score(y_test, self_cs.predict(X_test)))\n"""
assignments/assignment5/mp5/test.py,0,"b""'''Simple unit tests for students.'''\n\nimport unittest\nimport numpy as np\n\nfrom model.self_multiclass import MulticlassSVM\n\n\nclass TestSklearn(unittest.TestCase):\n\n    def setUp(self):\n        self.X_train = TestSklearn.mnist[:len(TestSklearn.mnist) // 2, 1:]\n        self.y_train = (TestSklearn.mnist[:len(TestSklearn.mnist) // 2, 0]\n                        .astype(np.int))\n\n        self.X_test = TestSklearn.mnist[len(TestSklearn.mnist) // 2:, 1:]\n        self.y_test = (TestSklearn.mnist[len(TestSklearn.mnist) // 2:, 0]\n                       .astype(np.int))\n\n    @classmethod\n    def setUpClass(cls):\n        super(TestSklearn, cls).setUpClass()\n        print('Loading data...')\n        cls.mnist = np.loadtxt('data/mnist_test.csv', delimiter=',')\n\n    def test_score_shape(self):\n        msvm = MulticlassSVM('ovr')\n        msvm.fit(self.X_train, self.y_train)\n        scores = msvm.scores_ovr_student(self.X_test)\n        self.assertTrue(scores.shape[0] == 5000 and scores.shape[1] == 10)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
assignments/assignment5/mp5/test_new.py,0,"b""import unittest\nimport numpy as np\nfrom sklearn import metrics\n\nfrom model.sklearn_multiclass import sklearn_multiclass_prediction\nfrom model.self_multiclass import MulticlassSVM\n\n\nclass TestSklearn(unittest.TestCase):\n\n    def setUp(self):\n        self.X_train = TestSklearn.mnist[:len(TestSklearn.mnist) // 5, 1:]\n        self.y_train = (TestSklearn.mnist[:len(TestSklearn.mnist) // 5, 0]\n                        .astype(np.int))\n\n        self.X_test = TestSklearn.mnist[len(TestSklearn.mnist) // 5:, 1:]\n        self.y_test = (TestSklearn.mnist[len(TestSklearn.mnist) // 5:, 0]\n                       .astype(np.int))\n\n        self.X = np.array([[0.74062279, 0.17422722],\n                           [0.66662075, 0.21494885],\n                           [0.94704997, 0.68120293],\n                           [0.86448145, 0.96164814],\n                           [0.71720711, 0.26782964]])\n        self.y = np.array([2, 0, 0, 1, 0])\n        self.W = np.array([[0.02640325, 0.21658084],\n                           [0.41876674, 0.73586187],\n                           [0.03738621, 0.93108151]])\n\n    @classmethod\n    def setUpClass(cls):\n        super(TestSklearn, cls).setUpClass()\n        cls.mnist = np.loadtxt('data/mnist_test.csv', delimiter=',')\n\n    def test_sklearn_ovr_accuracy(self):\n        y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n            'ovr', self.X_train, self.y_train, self.X_test)\n\n        train_acc = metrics.accuracy_score(self.y_train, y_pred_train)\n        test_acc = metrics.accuracy_score(self.y_test, y_pred_test)\n\n        self.assertTrue(abs(train_acc - 1.0) < 5e-3)\n        self.assertTrue(abs(test_acc - 0.814875) < 5e-3)\n\n    def test_sklearn_ovo_accuracy(self):\n        y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n            'ovo', self.X_train, self.y_train, self.X_test)\n\n        train_acc = metrics.accuracy_score(self.y_train, y_pred_train)\n        test_acc = metrics.accuracy_score(self.y_test, y_pred_test)\n\n        self.assertTrue(abs(train_acc - 1.0) < 5e-3)\n        self.assertTrue(abs(test_acc - 0.892625) < 5e-3)\n\n    def test_sklearn_crammer_accuracy(self):\n        y_pred_train, y_pred_test = sklearn_multiclass_prediction(\n            'crammer', self.X_train, self.y_train, self.X_test)\n\n        train_acc = metrics.accuracy_score(self.y_train, y_pred_train)\n        test_acc = metrics.accuracy_score(self.y_test, y_pred_test)\n\n        self.assertTrue(abs(train_acc - 1.0) < 5e-3)\n        self.assertTrue(abs(test_acc - 0.85825) < 5e-3)\n\n    def test_self_ovr_accuracy(self):\n        self_ovr = MulticlassSVM('ovr')\n        self_ovr.fit(self.X_train, self.y_train)\n\n        train_acc = metrics.accuracy_score(\n            self.y_train, self_ovr.predict(self.X_train))\n        test_acc = metrics.accuracy_score(\n            self.y_test, self_ovr.predict(self.X_test))\n\n        self.assertTrue(abs(train_acc - 1.0) < 5e-3)\n        self.assertTrue(abs(test_acc - 0.814875) < 5e-3)\n\n    def test_self_ovo_accuracy(self):\n        self_ovr = MulticlassSVM('ovo')\n        self_ovr.fit(self.X_train, self.y_train)\n\n        train_acc = metrics.accuracy_score(\n            self.y_train, self_ovr.predict(self.X_train))\n        test_acc = metrics.accuracy_score(\n            self.y_test, self_ovr.predict(self.X_test))\n\n        self.assertTrue(abs(train_acc - 1.0) < 5e-3)\n        self.assertTrue(abs(test_acc - 0.892625) < 5e-3)\n\n    def test_grad(self):\n        my_cs = MulticlassSVM('crammer-singer')\n        norm_diff = np.linalg.norm(my_cs.grad_student(self.W, self.X, self.y) -\n                                   np.array([[-2.30447458, -0.94740057],\n                                             [2.62578591, 1.11242235],\n                                             [0.16124486, 1.71850243]]))\n        self.assertTrue(norm_diff < 1e-6)\n\n    def test_loss(self):\n        my_cs = MulticlassSVM('crammer-singer')\n        loss_diff = abs(my_cs.loss_student(self.W, self.X, self.y)\n                        - 7.441854144192854)\n        self.assertTrue(loss_diff < 1e-6)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
assignments/assignment6/mp6/back_prop.py,0,"b'import numpy as np\n\n# define the number of iterations.\nnum_itr = 5000\n\n# define batch size.\nbatchSize = 3\n\n# define the input data dimension.\ninputSize = 2\n\n# define the output dimension.\noutputSize = 1\n\n# define the dimension of the hidden layer.\nhiddenSize = 3\n\n\nclass Neural_Network():\n    def __init__(self):\n        # weights\n        self.U = np.random.randn(inputSize, hiddenSize)\n        self.W = np.random.randn(hiddenSize, outputSize)\n        self.e = np.random.randn(hiddenSize)\n        self.f = np.random.randn(outputSize)\n\n    def fully_connected(self, X, U, e):\n        \'\'\'\n        fully connected layer.\n        inputs:\n            U: weight\n            e: bias\n        outputs:\n            X * U + e\n        \'\'\'\n        return np.dot(X, U) + e\n\n    def sigmoid(self, s):\n        \'\'\'\n        sigmoid activation function.\n        inputs: s\n        outputs: sigmoid(s)\n        \'\'\'\n        return 1 / (1 + np.exp(-s))\n\n    def sigmoidPrime(self, s):\n        \'\'\'\n        derivative of sigmoid (Written section, Part a).\n        inputs:\n            s = sigmoid(x)\n        outputs:\n            derivative sigmoid(x) as a function of s\n        \'\'\'\n        d_sigmoid = np.multiply(s, (1 - s))\n        return d_sigmoid\n\n    def forward(self, X):\n        \'\'\'\n        forward propagation through the network.\n        inputs:\n            X: input data (batchSize, inputSize)\n        outputs:\n            c: output (batchSize, outputSize)\n        \'\'\'\n        z2 = self.fully_connected(X, self.U, self.e)\n        b2 = self.sigmoid(z2)\n\n        h3 = self.fully_connected(b2, self.W, self.f)\n        c = self.sigmoid(h3)\n        return c\n\n    def d_loss_o(self, gt, o):\n        \'\'\'\n        computes the derivative of the L2 loss with respect to\n        the network\'s output.\n        inputs:\n            gt: ground-truth (batchSize, outputSize)\n            o: network output (batchSize, outputSize)\n        outputs:\n            d_o: derivative of the L2 loss with respect to the network\'s\n            output o. (batchSize, outputSize)\n        \'\'\'\n        d_o = o - gt\n        return (1 / batchSize) * d_o\n\n    def error_at_layer2(self, d_o, o):\n        \'\'\'\n        computes the derivative of the loss with respect to layer2\'s output\n        (Written section, Part b).\n        inputs:\n            d_o: derivative of the loss with respect to the network output (batchSize, outputSize)\n            o: the network output (batchSize, outputSize)\n        returns\n            delta_k: the derivative of the loss with respect to the output of the second\n            fully connected layer (batchSize, outputSize).\n        \'\'\'\n        delta_k = np.multiply(d_o, self.sigmoidPrime(o))\n        return delta_k\n\n    def error_at_layer1(self, delta_k, W, b):\n        \'\'\'\n        computes the derivative of the loss with respect to layer1\'s output (Written section, Part e).\n        inputs:\n            delta_k: derivative of the loss with respect to the output of the second\n            fully connected layer (batchSize, outputSize).\n            W: the weights of the second fully connected layer (hiddenSize, outputSize).\n            b: the input to the second fully connected layer (batchSize, hiddenSize).\n        returns:\n            delta_j: the derivative of the loss with respect to the output of the second\n            fully connected layer (batchSize, hiddenSize).\n        \'\'\'\n        delta_j = np.multiply(delta_k.dot(W.T), self.sigmoidPrime(b))\n        return delta_j\n\n    def derivative_of_w(self, b, delta_k):\n        \'\'\'\n        computes the derivative of the loss with respect to W (Written section, Part c).\n        inputs:\n            b: the input to the second fully connected layer (batchSize, hiddenSize).\n            delta_k: the derivative of the loss with respect to the output of the second\n            fully connected layer\'s output (batchSize, outputSize).\n        returns:\n            d_w: the derivative of loss with respect to W  (hiddenSize ,outputSize).\n        \'\'\'\n        d_w = b.T.dot(delta_k)\n        return d_w\n\n    def derivative_of_u(self, X, delta_j):\n        \'\'\'\n        computes the derivative of the loss with respect to U (Written section, Part f).\n        inputs:\n            X: the input to the network (batchSize, inputSize).\n            delta_j: the derivative of the loss with respect to the output of the first\n            fully connected layer\'s output (batchSize, hiddenSize).\n        returns:\n            d_u: the derivative of loss with respect to U (inputSize, hiddenSize).\n        \'\'\'\n        d_u = X.T.dot(delta_j)\n        return d_u\n\n    def derivative_of_e(self, delta_j):\n        \'\'\'\n        computes the derivative of the loss with respect to e (Written section, Part g).\n        inputs:\n            delta_j: the derivative of the loss with respect to the output of the first\n            fully connected layer\'s output (batchSize, hiddenSize).\n        returns:\n            d_e: the derivative of loss with respect to e (hiddenSize).\n        \'\'\'\n        d_e = np.sum(delta_j, 0)\n        return d_e\n\n    def derivative_of_f(self, delta_k):\n        \'\'\'\n        computes the derivative of the loss with respect to f (Written section, Part d).\n        inputs:\n            delta_k: the derivative of the loss with respect to the output of the second\n            fully connected layer\'s output (batchSize, outputSize).\n        returns:\n            d_f: the derivative of loss with respect to f (outputSize).\n        \'\'\'\n        d_f = np.sum(delta_k, 0)\n        return d_f\n\n    def backward(self, X, gt, o):\n        \'\'\'\n        backpropagation through the network.\n        Task: perform the 8 steps required below.\n        inputs:\n            X: input data (batchSize, inputSize)\n            y: ground truth (batchSize, outputSize)\n            o: network output (batchSize, outputSize)\n        \'\'\'\n        z2 = self.fully_connected(X, self.U, self.e)\n        b = self.sigmoid(z2)\n\n        # 1. Compute the derivative of the loss with respect to c.\n        # Call: d_loss_o\n        d_o = self.d_loss_o(gt, o)\n\n        # 2. Compute the error at the second layer (Written section, Part b).\n        # Call: error_at_layer2\n        delta_k = self.error_at_layer2(d_o, o)\n\n        # 3. Compute the derivative of W (Written section, Part c).\n        # Call: derivative_of_w\n        d_w = self.derivative_of_w(b, delta_k)\n\n        # 4. Compute the derivative of f (Written section, Part d).\n        # Call: derivative_of_f\n        d_f = self.derivative_of_f(delta_k)\n\n        # 5. Compute the error at the first layer (Written section, Part e).\n        # Call: error_at_layer1\n        delta_j = self.error_at_layer1(delta_k, self.W, b)\n\n        # 6. Compute the derivative of U (Written section, Part f).\n        # Call: derivative_of_u\n        d_u = self.derivative_of_u(X, delta_j)\n\n        # 7. Compute the derivative of e (Written section, Part g).\n        # Call: derivative_of_e\n        d_e = self.derivative_of_e(delta_j)\n\n        # 8. Update the parameters\n        self.W -= d_w\n        self.U -= d_u\n        self.e -= d_e\n        self.f -= d_f\n\n    def train(self, X, y):\n        o = self.forward(X)\n        self.backward(X, y, o)\n\n\ndef main():\n    """""" Main function """"""\n    # generate random input data of dimension (batchSize, inputSize).\n    # np.random.seed(123)\n    a = np.random.randint(0, high=10, size=[3, 2], dtype=\'l\')\n\n    # generate random ground truth.\n    t = np.random.randint(0, high=100, size=[3, 1], dtype=\'l\')\n\n    # scale the input and output data.\n    a = a / np.amax(a, axis=0)\n    t = t / 100\n\n    # create an instance of Neural_Network.\n    NN = Neural_Network()\n    for i in range(num_itr):\n        print(""Input: \\n"" + str(a))\n        print(""Actual Output: \\n"" + str(t))\n        print(""Predicted Output: \\n"" + str(NN.forward(a)))\n        print(""Loss: \\n"" + str(np.mean(np.square(t - NN.forward(a)))))\n        print(""\\n"")\n        NN.train(a, t)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
assignments/assignment6/mp6/test.py,0,"b'""""""Unit Tests examples for mp6.""""""\n\nfrom back_prop import Neural_Network\nimport unittest\nimport numpy as np\n\n\nclass DeepLearningTest(unittest.TestCase):\n\n    def test_sigmoidPrime(self):\n        NN = Neural_Network()\n        s = np.array([0.2, 3.4, 7.4])\n        d_sig_1 = NN.sigmoidPrime(s)\n        d_sig_2 = s * (1 - s)\n        self.assertEqual(d_sig_1[0], d_sig_2[0])\n        self.assertEqual(d_sig_1[1], d_sig_2[1])\n        self.assertEqual(d_sig_1[2], d_sig_2[2])\n\n    def test_forward(self):\n        NN = Neural_Network()\n        X = np.random.randint(0, high=10, size=[3, 2], dtype=\'l\')\n        X = X / np.amax(X, axis=0)\n        sol_1 = NN.forward(X)\n        z = np.dot(X, NN.U) + NN.e\n        b = NN.sigmoid(z)\n        h = np.dot(b, NN.W) + NN.f\n        sol_2 = NN.sigmoid(h)\n        self.assertEqual(sol_1[0][0], sol_2[0][0])\n        self.assertEqual(sol_1[1][0], sol_2[1][0])\n        self.assertEqual(sol_1[2][0], sol_2[2][0])\n\n    def test_d_loss_o(self):\n        NN = Neural_Network()\n        gt = np.array([0.2, 3.4, 7.4])\n        o = np.array([2.2, 5.4, 9.4])\n        d_o_1 = NN.d_loss_o(gt, o)\n        d_o_2 = np.array([2., 2., 2.])\n        self.assertAlmostEqual(d_o_1[0], d_o_2[0])\n        self.assertAlmostEqual(d_o_1[1], d_o_2[1])\n        self.assertAlmostEqual(d_o_1[2], d_o_2[2])\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment7/mp7/data_tools.py,0,"b'""""""Implements feature extraction and data processing helpers.\n""""""\n\n\nimport numpy as np\nimport pylab as pl\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\n\ndef load_dataset(input_file_path, num_samples):\n    """"""\n    Generates a dataset by loading an image and creating the specified number\n    of noisy samples of it.\n    Inputs:\n        input_file_path\n    Output:\n        dataset\n    """"""\n    original_img = load_image(input_file_path)\n    samples = []\n    for i in range(num_samples):\n        samples.append(inject_noise(original_img))\n\n    return original_img, samples\n\n\ndef load_image(input_file_path):\n    """"""\n    Loads the image and binarizes it by:\n    0. Read the image\n    1. Consider the first channel in the image\n    2. Binarize the pixel values to {-1, 1} by setting the values\n    below the binarization_threshold to 0 and above to 1.\n    Inputs:\n        input_file_path\n    Output:\n        binarized image\n    """"""\n    img = pl.imread(input_file_path)\n    img = img[:, :, 0]\n    img = np.where(img < 0.1, 0, 1)\n\n    return img\n\n\ndef inject_noise(image):\n    """"""\n    Inject noise by flipping the value of some randomly chosen pixels.\n    1. Generate a matrix of probabilities of pixels keeping their\n    original values.\n    2. Flip the pixels if their corresponding probability in the matrix\n    is below 0.1.\n\n    Input:\n        original image\n    Output:\n        noisy image\n    """"""\n    J = image.copy()\n\n    # Generate the matrix of probabilities of each pixel in the image\n    # to keep its value\n    N = np.shape(J)[0]\n    noise = np.random.rand(N, N)\n\n    # Extract the indices of the pixels to be flipped.\n    ind = np.where(noise < 0.1)\n\n    # Flip the pixels\n    J[ind] = 1 - J[ind]\n\n    return J\n\n\ndef plot_image(image, title, path):\n    pl.figure()\n    pl.imshow(image)\n    pl.title(title)\n    pl.savefig(path)\n'"
assignments/assignment7/mp7/linear_mrf.py,29,"b'import tensorflow as tf\nimport numpy as np\nfrom collections import defaultdict\n\n\nclass LinearMRF(object):\n    """"""Class implementing the MRF used for denoising""""""\n\n    def __init__(self, width, height):\n        """"""Initializes the MRF model.\n\n        Specifically, the model used is a grid graph, i.e. there is a\n        node for every pixel in an image and a pair connecting every pair\n        of adjacent pixels.\n\n        The nodes are numbered sequentially, starting from the upper right\n        corner of the image, first increasing to the right and then downward.\n        For example, here are the numberings of the nodes of a 3x4 graph:\n        0  1  2  3\n        4  5  6  7\n        8  9  10 11\n\n        Pairs are represented as tuples (m, n), where m and n are the\n        indices of the two nodes. For example, in the above graph,\n        (5, 9) and (1, 2) are both pairs.\n\n        After initialization, this class contains the following fields:\n            width(int): the width in pixels of the images represented\n            height(int): the height in pixels of the images represented\n            unary_weight(tf.Variable): the linear weight for unary potentials\n            pairwise_weight(tf.Variable): the linear weight for pairwise\n                potentials\n            pairs(list of tuples): represents the edges in the MRF. Each entry\n                is a tuple containing two integers, each representing a node\n            pair_inds(dict): provides the index within the list of pairs\n                for a given pair\n            neighbors(dict): the keys are nodes (integers), and the values\n                are a list of integers representing the nodes adjacent to\n                the specified node in the MRF\n            pairwise_features(np.array): The pairwise features for the model\n        """"""\n        self.width = width\n        self.height = height\n        self.unary_weight = tf.Variable(1.)\n        self.pairwise_weight = tf.Variable(1.)\n\n        self.pairs = []\n        self.pair_inds = {}\n        self.neighbors = defaultdict(list)\n        i = 0\n\n        # Initialize neighbors, pairs, and pair_inds\n        for row in range(height):\n            for col in range(width):\n                ind1 = row * width + col\n\n                if row > 0:\n                    ind2 = (row - 1) * width + col\n                    self.pairs.append((ind2, ind1))\n                    self.pair_inds[(ind2, ind1)] = i\n                    self.pair_inds[(ind1, ind2)] = i\n\n                    self.neighbors[ind1].append(ind2)\n                    self.neighbors[ind2].append(ind1)\n\n                    i += 1\n                if col > 0:\n                    ind2 = row * width + (col - 1)\n                    self.pairs.append((ind2, ind1))\n                    self.pair_inds[(ind2, ind1)] = i\n                    self.pair_inds[(ind1, ind2)] = i\n\n                    self.neighbors[ind1].append(ind2)\n                    self.neighbors[ind2].append(ind1)\n\n                    i += 1\n\n        # pre-compute pair features, since they do not depend on the image\n        self.pairwise_features = self.get_pairwise_features()\n\n    def get_unary_features(self, img):\n        """"""Calculate the full matrix of unary features for a provided.\n\n        set of values (which can be either the noisy observations or the\n        true image)\n\n        Recall that, for a given node observation x_i and possible assignment\n        y_i to that node, the feature function you should calculate is\n\n        f_unary(x_i, y_i) = 1[x_i == y_i]\n\n        where 1[] is the indicator function that equals 1 if the argument is\n        true and 0 if the argument is false.\n\n        As mentioned, this calculates the full matrix of unary features - you\n        should use the following index scheme:\n\n        result[i, 0] is the feature value for node x_i when y_i = 0\n        result[i, 1] is the feature value for node x_i when y_i = 1\n\n        Args:\n            img(np.array): An array of length (width x height) representing\n              the observations of an image\n        Returns:\n            (np.array): An array of size (width x height, 2) containing the\n              features for an image\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        # Initialize f_unary\n        f_unary = np.zeros((self.width * self.height, 2))\n\n        # Query for y_i\n        y_i = np.copy(img).flatten()\n\n        # f_unary(x_i, y_i) = 1[x_i == y_i]\n        f_unary[np.where(y_i == 0), 0] = 1\n        f_unary[np.where(y_i == 1), 1] = 1\n\n        return f_unary\n\n    def get_pairwise_features(self):\n        """"""Calculate the full matrix of pairwise features.\n\n        Recall that, for a given set of possible assignments y_i and y_j to\n        a pair of nodes (i, j) in the graph, the feature function you should\n        calculate is\n\n        f_pairwise(y_i, y_j) = 1[y_i == y_j]\n\n        where 1[] is the indicator function that equals 1 if the argument is\n        true and 0 if the argument is false.\n\n        As mentioned, this calculates the full matrix of pairwise features -\n        you should use the following index scheme, where i is the index for\n        pair (m, n) as found in self.pair_inds (and m < n):\n\n        result[i, 0] is the feature value for y_m = 0, y_n = 0\n        result[i, 1] is the feature value for y_m = 0, y_n = 1\n        result[i, 2] is the feature value for y_m = 1, y_n = 0\n        result[i, 3] is the feature value for y_m = 1, y_n = 1\n\n        Returns:\n            (np.array): An array of size (len(pairs), 4) containing the\n              pairwise features for an image\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        return np.array([[1, 0, 0, 1], ] * len(self.pairs))\n\n    def calculate_unary_potentials(self, unary_features):\n        """"""Calculates the full matrix of unary potentials for a provided\n        matrix of unary features.\n\n        Recall that, for a given node observation x_i and an assignment y_i to\n        that node, the potential function you should calculate is\n\n        phi_i(x_i, y_i) = w_unary * f_unary(x_i, y_i)\n\n        where f_unary(x_i, y_i) is the value of the feature function for a\n        given node/assignment\n\n        Args:\n            unary_features(np.array): a (height * width, 2)-sized matrix\n              containing the features for a noisy sample (see\n              get_unary_features for more details)\n        Returns:\n            (tf.Tensor): The unary potentials, which is a rank-2 tensor of the\n              same size as the unary_features.\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        # unary_potentials = tf.multiply(self.unary_weight, unary_features)\n        return tf.multiply(self.unary_weight, unary_features)\n\n    def calculate_pairwise_potentials(self, pairwise_features):\n        """"""Calculates the full matrix of pairwise potentials for a provided\n        matrix of pairwise features.\n\n        Recall that, for a given pair of assignments y_i and y_j to nodes i\n        and j, the potential function you should calculate is\n\n        phi_ij(y_i, y_j) = w_pairwise * f_pairwise(y_i, y_j)\n\n        where f_pairwise(y_i, y_j) is the value of the pairwise feature\n        function for a given node/assignment\n\n        Args:\n            pairwise_features(np.array): a (len(pairs), 4)-sized matrix\n              containing the pairwise features (see get_pairwise_features\n              for more details)\n        Returns:\n            (tf.Tensor): The pairwise potentials, which is a rank-2 tensor of\n              the same size as the pairwise_features\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        # pairwise_potentials = tf.multiply(self.pairwise_weight, pairwise_features)\n        return tf.multiply(self.pairwise_weight, pairwise_features)\n\n    def build_training_obj(self, img_features, unary_beliefs, pair_beliefs,\n                           unary_potentials, pairwise_potentials):\n        """"""Build the training objective, as specified in the handout.\n\n        Hint: the image features can be thought of as a ""correct"" set of\n        beliefs for that image\n\n        Args:\n            img_features(np.array): The unary feature representation of the\n                true image (as returned by get_unary_features)\n            unary_beliefs(list(tf.Tensor)): A list of the unary beliefs\n                for each of the noisy samples\n            pair_beliefs(list(tf.Tensor)): A list of the pairwise beliefs for each\n                of the noisy samples\n            unary_potentials(list(tf.Tensor)): A list of the unary potentials\n                for each of the noisy samples. Each entry is a rank-2 tensor\n                of size (height x width, 2)\n            pairwise_potentials(tf.Tensor): The pairwise potentials, which is\n                a rank-2 tensor of size (len(self.pairs), 4)\n        Returns:\n            (tf.Tensor): the training objective, which is a rank-0 tensor\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        unary_loss = tf.reduce_sum(\n            tf.multiply(\n                unary_beliefs,\n                unary_potentials))\n        pairwise_loss = tf.reduce_sum(\n            tf.multiply(\n                pair_beliefs,\n                pairwise_potentials))\n\n        F1 = tf.reduce_sum(tf.multiply(unary_potentials, img_features))\n\n        pairwise_features = np.zeros((len(self.pairs), 4))\n\n        for pair in self.pairs:\n            i, j = pair\n            y = int(img_features[i, 1] * 2 + img_features[j, 1])\n            x = self.pair_inds[pair]\n            pairwise_features[x, y] = 1\n\n        F2 = tf.reduce_sum(tf.multiply(pairwise_potentials, pairwise_features))\n\n        # Compute traning object\n        obj = unary_loss + pairwise_loss - (F1 + len(unary_beliefs) * F2)\n        return obj\n\n    def train(self, original_img, noisy_samples, lr, num_epochs,\n              convergence_margin):\n        """"""Train the model using the provided data and training parameters.\n\n        Args:\n            original_img(np.array): The true, denoised image\n            noisy_samples(list(np.array)): Noisy samples of the true image\n            lr(float): The learning rate for gradient descent\n            num_epochs(int): The number of training iterations\n            convergence_margin(float): The convergence margin for inference\n                (see run_greedy_inference for more details)\n        """"""\n\n        # Initialize placeholders for beliefs\n        unary_belief_placeholders = []\n        pairwise_belief_placeholders = []\n        for i in range(len(noisy_samples)):\n            unary_belief_placeholders.append(tf.placeholder(\n                tf.float32, [self.height * self.width, 2]))\n            pairwise_belief_placeholders.append(\n                tf.placeholder(tf.float32, [len(self.pairs), 4]))\n\n        # Compute features for original image and noisy samples\n        img_features = self.get_unary_features(original_img)\n        noisy_features = [self.get_unary_features(noisy)\n                          for noisy in noisy_samples]\n\n        # Compute initial beliefs. We initialize them to be identical to the\n        # noisy features (meaning the beliefs are set such that the model\n        # believes the noisy observations to be correct\n        unary_beliefs = []\n        pairwise_beliefs = []\n        unary_beliefs = [feat.copy() for feat in noisy_features]\n        pairwise_beliefs = [self.get_pairwise_beliefs(belief)\n                            for belief in unary_beliefs]\n\n        # Build the computation graph for training\n        unary_potentials = [self.calculate_unary_potentials(feat)\n                            for feat in noisy_features]\n        pairwise_potentials = self.calculate_pairwise_potentials(\n            self.pairwise_features)\n\n        train_obj = self.build_training_obj(img_features,\n                                            unary_belief_placeholders,\n                                            pairwise_belief_placeholders,\n                                            unary_potentials,\n                                            pairwise_potentials)\n        optimizer = tf.train.GradientDescentOptimizer(lr)\n        train = optimizer.minimize(train_obj)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            for epoch in range(num_epochs):\n                print(""EPOCH %d"" % (epoch + 1))\n\n                # First: Calculate Potentials\n                nodes = [pairwise_potentials] + unary_potentials\n                results = sess.run(nodes)\n                pairwise_pot_result = results[0]\n                unary_pot_result = results[1:]\n\n                # Second: Run Inference\n                unary_beliefs, pairwise_beliefs = self.run_greedy_inference(\n                    unary_beliefs, unary_pot_result, pairwise_pot_result,\n                    convergence_margin)\n\n                score = 0\n                for unary_belief in unary_beliefs:\n                    score += np.sum(unary_beliefs != img_features) /\\\n                        (len(unary_beliefs) * 2)\n                score /= len(unary_beliefs)\n                print(""CURRENT SCORE: "", score)\n                print(""WEIGHTS: "")\n                print(sess.run(self.unary_weight))\n                print(sess.run(self.pairwise_weight))\n\n                # Third: Update model parameters based on current beliefs\n                feed_dict = {}\n                for belief, placeholder in zip(unary_beliefs,\n                                               unary_belief_placeholders):\n                    feed_dict[placeholder] = belief\n                for belief, placeholder in zip(pairwise_beliefs,\n                                               pairwise_belief_placeholders):\n                    feed_dict[placeholder] = belief\n                nodes = [train, pairwise_potentials] + unary_potentials\n                results = sess.run(nodes, feed_dict)\n\n    def test(self, noisy_samples, convergence_margin):\n        """"""Given a list of noisy samples of an image, produce denoised\n        versions of that image.\n\n        Args:\n            noisy_samples(list(np.array)): A list of noisy samples of an image\n            convergence_margin(float): The convergence margin for inference\n                (see run_greedy_inference for more details)\n        Returns:\n            (list(np.array)): The denoised images\n        """"""\n\n        # Initialize Beliefs\n        unary_beliefs = []\n        noisy_features = [self.get_unary_features(noisy)\n                          for noisy in noisy_samples]\n        unary_beliefs = [feat.copy() for feat in noisy_features]\n\n        unary_potentials = [self.calculate_unary_potentials(feat)\n                            for feat in noisy_features]\n        pairwise_potentials = self.calculate_pairwise_potentials(\n            self.pairwise_features)\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n            # Calculate potentials\n            nodes = [pairwise_potentials] + unary_potentials\n            results = sess.run(nodes)\n            pairwise_pot_result = results[0]\n            unary_pot_result = results[1:]\n\n        unary_beliefs, pairwise_beliefs = self.run_greedy_inference(\n            unary_beliefs, unary_pot_result, pairwise_pot_result,\n            convergence_margin)\n\n        denoised_imgs = [self.beliefs2img(unary_belief)\n                         for unary_belief in unary_beliefs]\n        return denoised_imgs\n\n    def beliefs2img(self, unary_belief):\n        """"""Convert a provided set of beliefs into the correct format for\n        an image by setting each pixel to the value that has belief 1\n        """"""\n        result = np.empty(len(unary_belief))\n        for i in range(len(unary_belief)):\n            if unary_belief[i, 0] == 1:\n                result[i] = 0\n            else:\n                result[i] = 1\n        return result.reshape((self.height, self.width))\n\n    def run_greedy_inference(self, unary_beliefs, unary_pots, pairwise_pots,\n                             convergence_margin):\n        """"""Run our greedy inference procedure on the provided lists of\n        beliefs/potentials. Note that we run inference for a maximum of 10\n        iterations per image.\n        """"""\n        new_unary_beliefs = []\n        new_pairwise_beliefs = []\n\n        for unary_belief, unary_pot in zip(unary_beliefs, unary_pots):\n            itr = 0\n            converged = False\n\n            unary_belief = unary_belief.copy()\n\n            while not converged:\n                if itr > 10:\n                    break\n\n                itr += 1\n\n                new_unary_belief = self.inference_itr(unary_belief, unary_pot,\n                                                      pairwise_pots)\n                converged = self.check_convergence(new_unary_belief,\n                                                   unary_belief,\n                                                   convergence_margin)\n\n                unary_belief = new_unary_belief\n\n            pairwise_belief = self.get_pairwise_beliefs(unary_belief)\n            new_unary_beliefs.append(unary_belief)\n            new_pairwise_beliefs.append(pairwise_belief)\n        return new_unary_beliefs, new_pairwise_beliefs\n\n    def inference_itr(self, unary_beliefs, unary_pots, pairwise_pots):\n        """"""Run a single iteration of inference with the provided beliefs\n        and potentials.\n\n        Here, inference should be implemented as a simple (randomized) greedy\n        algorithm. The steps are as follows:\n        1. Determine a random order of nodes\n        2. For each node in the graph:\n            a. Calculate the scores (using calculate_local_score) for that\n               node having assignment 0 and 1\n            b. Set the belief for the assignment having the larger score to 1\n               and that for the assignment having the smaller score to 0\n\n        Args:\n            unary_beliefs(np.array): The input set of beliefs, having shape\n            (width x height, 2)\n            unary_pots(np.array): The unary potentials for the image, having\n              shape (width x height, 2)\n            pairwise_pots(np.array): The pairwise potentials for the image,\n              having shape (len(self.pairs), 4)\n        Returns:\n            (np.array): The new set of unary beliefs, having the same shape\n              as the input set of unary beliefs.\n        """"""\n        unary_beliefs = unary_beliefs.copy()\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n\n        # Initialize random order\n        random_idx = np.arange(unary_beliefs.shape[0])\n        np.random.shuffle(random_idx)\n\n        for node_idx in random_idx:\n            # Calculate the scores\n            score_0 = self.calculate_local_score(\n                node_idx, 0, unary_beliefs, unary_pots, pairwise_pots)\n            score_1 = self.calculate_local_score(\n                node_idx, 1, unary_beliefs, unary_pots, pairwise_pots)\n            if score_1 > score_0:\n                # Set the belief for the assignment\n                unary_beliefs[node_idx, 1] = 1\n                unary_beliefs[node_idx, 0] = 0\n            else:\n                # Set the belief for the assignment\n                unary_beliefs[node_idx, 1] = 0\n                unary_beliefs[node_idx, 0] = 1\n        return unary_beliefs\n\n    def calculate_local_score(self, node, assignment, unary_beliefs,\n                              unary_potentials, pairwise_potentials):\n        """"""Calculate the score of a ""patch"" surrounding the specified node,\n        assuming that node has the specified assignment, given the current\n        beliefs for the assignments of values to the pixels in the image\n\n        This score consists of the sum of the unary potential for this node\n        given this assignment, plus the pairwise potentials of all pairs\n        that include this node given the assignment specified for this node\n        and the assignment for the other nodes specified by the provided\n        unary beliefs.\n\n        Args:\n            node(int): The node whose patch is being scored\n            assignment(int): The assignment that should be considered for\n              the node (either 0 or 1)\n            unary_beliefs(np.array): The current set of unary beliefs for\n              the image, having shape (width x height, 2)\n            unary_potentials(np.array): The current set of unary potentials\n              for the image, having shape (width x height, 2)\n            pairwise_potentials(np.array): The current set of pairwise\n              potentials for the image, having shape (len(self.pairs), 4)\n        Returns:\n            (float): The score of the patch\n        """"""\n        score = 0.0\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        sum1 = unary_potentials[node, assignment]\n        sum2 = 0\n\n        for neighbor in self.neighbors[node]:\n            if unary_beliefs[neighbor][0] == 1:\n                y_j = 0\n            else:\n                y_j = 1\n\n            if assignment == 0 and y_j == 0:\n                idx = 0\n            if assignment == 0 and y_j == 1:\n                idx = 1\n            if assignment == 1 and y_j == 0:\n                idx = 2\n            if assignment == 1 and y_j == 1:\n                idx = 3\n\n            sum2 += pairwise_potentials[self.pair_inds[(node, neighbor)], idx]\n\n        score = sum1 + sum2\n        return score\n\n    def check_convergence(self, new_unary_beliefs, old_unary_beliefs,\n                          convergence_margin):\n        """"""Given two sets of unary beliefs, determine if inference has\n        converged.\n\n        Convergence occurs when the percentage of nodes in the graph whose\n        beliefs have changed is less than the provided margin.\n\n        Args:\n            new_unary_beliefs(np.array): One set of unary beliefs, having\n              the same shape as elsewhere in the code\n            old_unary_beliefs(np.array): Another set of unary beliefs, having\n              the same shape as elsewhere in the code\n            convergence_margin(float): the threshold determining convergence.\n              This should be a number between 0 and 1\n        Returns:\n            (bool): whether inference has converged\n        """"""\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        num = 0\n        # Amount of nodes in the graph whose beliefs have changed\n        for idx, belief in enumerate(new_unary_beliefs):\n            if (old_unary_beliefs[idx][0] != belief[0]) or (\n                    old_unary_beliefs[idx][1] != belief[1]):\n                num += 1\n\n        # Percentage of nodes in the graph whose beliefs have changed\n        ratio = num / new_unary_beliefs.shape[0]\n        if ratio <= convergence_margin:\n            return True\n        else:\n            return False\n\n    def get_pairwise_beliefs(self, unary_beliefs):\n        """"""Generate the appropriate pairwise beliefs for a specified set of\n        unary beliefs.\n\n        Due to the fact that all of the unary beliefs for this inference\n        implementation are either 0 or 1, the pairwise beliefs are a\n        simple deterministic function of the unary beliefs.\n\n        Specifically, given a pair of nodes (m, n), the pairwise belief\n        for assignment (y_m, y_n) = 1 iff the unary belief for node m with\n        assignment y_m is 1 and the unary belief for node n with assignment\n        y_n is 1.\n\n        Args:\n            unary_beliefs(np.array): The set of unary beliefs for a noisy\n              sample. This array has shape (width x height, 2)\n        Returns:\n            (np.array): The set of pairwise beliefs. This array should have\n              shape (len(self.pairs), 4) and is indexed the same way as\n              specified in get_pairwise_features.\n        """"""\n        result = np.zeros((len(self.pairs), 4))\n        #########################\n        # IMPLEMENT THIS METHOD #\n        #########################\n        for pair in self.pairs:\n            # if unary_beliefs[pair[0]][1] == 1 and unary_beliefs[pair[1]][1]\n            # == 1:\n            result[self.pair_inds[pair], 0] = unary_beliefs[pair[0]\n                                                            ][0] * unary_beliefs[pair[1]][0]\n            result[self.pair_inds[pair], 1] = unary_beliefs[pair[0]\n                                                            ][0] * unary_beliefs[pair[1]][1]\n            result[self.pair_inds[pair], 2] = unary_beliefs[pair[0]\n                                                            ][1] * unary_beliefs[pair[1]][0]\n            result[self.pair_inds[pair], 3] = unary_beliefs[pair[0]\n                                                            ][1] * unary_beliefs[pair[1]][1]\n        return result\n'"
assignments/assignment7/mp7/main.py,2,"b'""""""Main function for train, eval, and test.""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom linear_mrf import LinearMRF\nfrom data_tools import load_dataset, plot_image\n\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\nflags.DEFINE_integer(\'num_epochs\', 2, \'Number of update steps to run.\')\nflags.DEFINE_float(\'convergence_margin\', 0.01,\n                   \'Margin of convergence for inference\')\nflags.DEFINE_string(\'input_file_path\', \'data/circle.png\', \'Original Image.\')\n\n\ndef main(_):\n    """"""High level pipeline.\n    This script performs the trainsing, evaling and testing state of the model.\n    """"""\n    learning_rate = FLAGS.learning_rate\n    num_epochs = FLAGS.num_epochs\n    convergence_margin = FLAGS.convergence_margin\n    input_file_path = FLAGS.input_file_path\n\n    # Load dataset.\n    original_img, noisy_samples = load_dataset(input_file_path, 10)\n    height = original_img.shape[0]\n    width = original_img.shape[1]\n\n    original_img = original_img.flatten()\n    noisy_samples = [sample.flatten() for sample in noisy_samples]\n\n    # Initialize model.\n    model = LinearMRF(width, height)\n\n    model.train(original_img, noisy_samples, learning_rate, num_epochs,\n                convergence_margin)\n\n    # Evaluate model on training dataset\n    denoised_images = model.test(noisy_samples, convergence_margin)\n\n    # Plot inference result on image\n    plot_image(noisy_samples[0].reshape(height, width), \'Noisy Image\',\n               \'data/noisy_sample.png\')\n    plot_image(denoised_images[0], \'Denoised Version\', \'data/denoised_img.png\')\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
assignments/assignment7/mp7/test.py,6,"b'""""""Simple unit tests for students.""""""\n\nimport unittest\nimport numpy as np\nfrom linear_mrf import LinearMRF\nimport tensorflow as tf\n\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        self.model = LinearMRF(3, 2)\n\n    def test_unary_feature_shape(self):\n        img = np.array([1, 0, 0, 1, 0, 1])\n        result = self.model.get_unary_features(img)\n        self.assertEqual(result.shape, (6, 2))\n\n    def test_pairwise_feature_shape(self):\n        result = self.model.get_pairwise_features()\n        self.assertEqual(result.shape, (7, 4))\n\n\nclass BeliefTests(unittest.TestCase):\n    def setUp(self):\n        self.model = LinearMRF(1, 2)\n\n    def test_belief_convergence(self):\n        new_beliefs = np.array([[0, 1],\n                                [0, 1]])\n        old_beliefs = np.array([[1, 0],\n                                [1, 0]])\n        result = self.model.check_convergence(new_beliefs, old_beliefs, 0.2)\n        self.assertEqual(result, False)\n\n    def test_pairwise_beliefs_shape(self):\n        beliefs = np.array([[0, 1],\n                            [1, 0]])\n        result = self.model.get_pairwise_beliefs(beliefs)\n        self.assertEqual(result.shape, (1, 4))\n\n\nclass InferenceTests(unittest.TestCase):\n    def setUp(self):\n        self.model = LinearMRF(1, 2)\n\n    def test_inf(self):\n        unary_beliefs = np.array([[0, 1],\n                                  [0, 1]])\n        unary_potentials = np.array([[1, 0],\n                                     [1, 0]])\n        pairwise_potentials = np.array([[2, 1, 1, 0]])\n        correct = np.array([[1, 0],\n                            [1, 0]])\n        result = self.model.inference_itr(unary_beliefs, unary_potentials,\n                                          pairwise_potentials)\n        np.testing.assert_array_equal(correct, result)\n\n\nclass LearningTests(unittest.TestCase):\n    def setUp(self):\n        self.model = LinearMRF(1, 2)\n\n    def test_learning_obj(self):\n        img_features = np.array([[1, 0], [1, 0]])\n        unary_beliefs = [tf.constant([[1, 0], [1, 0]])]\n        pair_beliefs = [tf.constant([[1, 0, 0, 0]])]\n        unary_potentials = [tf.constant([[1, 1], [1, 1]])]\n        pairwise_potentials = tf.constant([[1, 1, 1, 1]])\n        correct = 0\n        result = self.model.build_training_obj(img_features, unary_beliefs,\n                                               pair_beliefs, unary_potentials,\n                                               pairwise_potentials)\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            result_val = sess.run(result)\n        self.assertEqual(correct, result_val)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment8/mp8/k_means.py,0,"b'from copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport sys\n\n\'\'\'\nIn this problem you write your own K-Means\nClustering code.\n\nYour code should return a 2d array containing\nthe centers.\n\n\'\'\'\n# Configure the path to data\ndata_dir = \'data/data/iris.data\'\n\n# Import the dataset\ndf = pd.read_table(data_dir, delimiter=\',\')\nX = df[[\'V1\', \'V2\', \'V3\', \'V4\']].as_matrix()\n\n# Make 3 clusters\nk = 3\n\n# Initial Centroids\nC = [[2., 0., 3., 4.],\n     [1., 2., 1., 3.],\n     [0., 2., 1., 0.]]\nC = np.array(C)\n\nprint(""Initial Centers: "")\nprint(C)\n\n\ndef distance(data_point, center):\n    """"""Compute distance between data point and corresponding center.\n\n    Args:\n        data_point(list): single data point.\n        center(list): corresponding center.\n    Returns:\n        distance between data point and corresponding center.\n    """"""\n    return np.linalg.norm(data_point - center)\n\n\ndef assign_cluster(data, centers):\n    """"""Assign cluster index to each of data point.\n\n    Args:\n        data(np.ndarray): data points\n        centers(np.ndarray): centers\n    Returns:\n        index(list): index of which cluster each of data points belongs to\n    """"""\n    index = []\n    num_center = centers.shape[0]\n\n    for point in data:\n        dist = []\n        for i in range(num_center):\n            dist.append(distance(point, centers[i]))\n        index.append(np.argmin(dist))\n    return index\n\n\ndef update_center(X, indx):\n    """"""Update centers for data points.\n\n    Args:\n        data(np.ndarray): data points.\n        old_index(list): The previous/current index.\n    Returns:\n        new_centers(np.ndarray): new centers after updating.\n    """"""\n    newCenters = []\n    for clusterNumber in range(0, k):\n        Cluster = X[[i for i, find in enumerate(indx) if find == clusterNumber]]\n        centroid = Cluster.mean(axis=0)\n        newCenters.append(centroid)\n    return np.array(newCenters)\n\n\ndef k_means(C):\n    """"""Perform K-Means Algorithm on dataset.\n\n    Args:\n        C(np.ndarray): initial centers.\n    """"""\n    # Write your code here!\n    # Get the initial cluster of data points\n    C = np.array(C)\n    indexofcluster = assign_cluster(X, C)\n    old_dist = 0\n    while (True):\n        new_centers = update_center(X, indexofcluster)\n        new_indexofcluster = assign_cluster(X, new_centers)\n\n        dist = 0\n        for i in range(X.shape[0]):\n            dist += distance(X[i], new_centers[new_indexofcluster[i]])\n        if abs(dist - old_dist) < 10e-3:\n            break\n        else:\n            old_dist = dist\n            indexofcluster = new_indexofcluster\n    return new_centers\n\nnew_centers = k_means(C)\nprint(""New centers: "")\nprint(new_centers)\n'"
assignments/assignment8/mp8/test.py,0,"b'"""""" simple unit test for students """"""\n\n\nimport unittest\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport sys\nfrom k_means import k_means\n\n\nC = [[2.,  0.,  3.,  4.], [1.,  2.,  1.,  3.], [0., 2.,  1.,  0.]]\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        self.centers = k_means(C)\n\n    def test_shape(self):\n        result = self.centers\n        self.assertEqual(result.shape, (3,4))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment9/mp9/main.py,2,"b'""""""Semi-supervised learning for EM for GMM.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom utils import io_tools\nfrom models.gaussian_mixture_model import GaussianMixtureModel\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_integer(\'max_iter\', 100, \'Number of EM steps to run.\')\nflags.DEFINE_integer(\'n_components\', 15, \'Number of components\')\n\n\ndef main(_):\n    """"""High level pipeline.\n\n    This scripts performs the training and evaling and testing stages for\n    semi-supervised learning using kMeans algorithm.\n    """"""\n    #########################################################################\n\n    # # Load dataset.\n    # _, unlabeled_data = io_tools.read_dataset(\'data/simple_test.csv\')\n    # n_dims = unlabeled_data.shape[1]\n\n    # # Initialize model.\n    # model = GaussianMixtureModel(n_dims, n_components=FLAGS.n_components,\n    #                              max_iter=FLAGS.max_iter)\n\n    # # Unsupervised training.\n    # model.fit(unlabeled_data)\n\n    # # Supervised training.\n    # train_label, train_data = io_tools.read_dataset(\'data/simple_test.csv\')\n    # model.supervised_fit(train_data, train_label)\n\n    # # Eval model.\n    # eval_label, eval_data = io_tools.read_dataset(\'data/simple_test.csv\')\n    # y_hat_eval = model.supervised_predict(eval_data)\n    # print(eval_label)\n    # print(np.unique(y_hat_eval))\n    # acc = np.sum(y_hat_eval == eval_label) / (1. * eval_data.shape[0])\n    # print(""Accuracy: %s"" % acc)\n\n    #########################################################################\n\n    # Load dataset.\n    train_label, train_data = io_tools.read_dataset(\'data/mnist_train.csv\')\n    n_dims = train_data.shape[1]\n\n    # Initialize model.\n    model = GaussianMixtureModel(n_dims, n_components=FLAGS.n_components,\n                                 max_iter=FLAGS.max_iter)\n\n    # Unsupervised training.\n    # model.fit(train_data)\n\n    # Supervised training.\n    train_label, train_data = io_tools.read_dataset(\'data/mnist_train.csv\')\n    model.supervised_fit(train_data, train_label)\n\n    # print(train_label.tolist().count(1))\n    # print(train_label.tolist().count(2))\n    # print(train_label.tolist().count(3))\n    # print(train_label.tolist().count(4))\n\n    # Eval model.\n    eval_label, eval_data = io_tools.read_dataset(\'data/mnist_test.csv\')\n    y_hat_eval = model.supervised_predict(eval_data)\n    # print(eval_label)\n    # print(eval_label.tolist().count(1))\n    # print(eval_label.tolist().count(2))\n    # print(eval_label.tolist().count(3))\n    # print(eval_label.tolist().count(4))\n    # print(np.unique(y_hat_eval))\n    acc = np.sum(y_hat_eval == eval_label) / (1. * eval_data.shape[0])\n    print(""Accuracy: %s"" % acc)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
assignments/assignment9/mp9/test.py,0,"b'""""""Simple unit tests on model for grading.""""""\n\nimport unittest\nimport numpy as np\nfrom models.gaussian_mixture_model import GaussianMixtureModel\nfrom utils import io_tools\n\nclass ModelTests(unittest.TestCase):\n    def setUp(self):\n        n_dims = 2\n        unlabeled_data = np.random.normal(-1,0.1,[100,n_dims])\n        unlabeled_data = np.concatenate((unlabeled_data,np.random.normal(1,0.1,[50,n_dims])))\n\n        self.n_dims = 2\n        self.n_components = 2\n        self.max_iter = 1\n        self.model = GaussianMixtureModel(self.n_dims, n_components=self.n_components,\n                                     max_iter=self.max_iter)\n\n    def test_io(self):\n        train_label, train_data = io_tools.read_dataset(\'data/simple_test.csv\')      \n        np.testing.assert_array_equal(train_data.shape,np.asarray([200,2]))\n\n\n    def test_model(self):\n        self.assertEquals(self.model._mu.shape[0], self.n_components)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
assignments/assignment11/mp11/models/__init__.py,0,b''
assignments/assignment11/mp11/models/gan.py,82,"b'""""""Generative adversarial network.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow import contrib\nfrom tensorflow.contrib import layers\n\n\nclass Gan(object):\n    """"""Adversary based generator network.""""""\n\n    def __init__(self, ndims=784, nlatent=2):\n        """"""Initialize a GAN.\n\n        Args:\n            ndims (int): Number of dimensions in the feature.\n            nlatent (int): Number of dimensions in the latent space.\n        """"""\n        self._ndims = ndims\n        self._nlatent = nlatent\n\n        # Input images\n        self.x_placeholder = tf.placeholder(tf.float32, [None, ndims])\n\n        # Input noise\n        self.z_placeholder = tf.placeholder(tf.float32, [None, nlatent])\n\n        # Build graph.\n        self.x_hat = self._generator(self.z_placeholder)\n        y_hat = self._discriminator(self.x_hat)\n        y = self._discriminator(self.x_placeholder, reuse=True)\n\n        # Discriminator loss\n        self.d_loss = self._discriminator_loss(y, y_hat)\n\n        # Generator loss\n        self.g_loss = self._generator_loss(y_hat)\n\n        # Learning rate\n        self.learning_rate_placeholder = tf.placeholder(tf.float32, [])\n\n        # Add optimizers for appropriate variables\n        d_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, ""discriminator"")\n        self.d_optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.learning_rate_placeholder,\n            name=\'d_optimizer\').minimize(self.d_loss, var_list=d_vars)\n\n        g_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, ""generator"")\n        self.g_optimizer = tf.train.AdamOptimizer(\n            learning_rate=self.learning_rate_placeholder,\n            name=\'g_optimizer\').minimize(self.g_loss, var_list=g_vars)\n\n        # Create session\n        self.session = tf.InteractiveSession()\n        self.session.run(tf.global_variables_initializer())\n\n    def _discriminator(self, x, reuse=False):\n        """"""Discriminator block of the network.\n\n        Args:\n            x (tf.Tensor): The input tensor of dimension (None, 784).\n            reuse (Boolean): reuse variables with same name in scope instead\n                of creating new ones, check Tensorflow documentation\n        Returns:\n            y (tf.Tensor): Scalar output prediction D(x) for true vs fake\n                image(None, 1).\n\n        DO NOT USE AN ACTIVATION FUNCTION AT THE OUTPUT LAYER HERE.\n\n        """"""\n        with tf.variable_scope(""discriminator"", reuse=reuse) as scope:\n\n            # ---------------------------------------------------------------#\n            # # Input Layer\n            # inputs = layers.fully_connected(\n            #     inputs=x, num_outputs=512, activation_fn=tf.nn.relu)\n\n            # # Drop Out\n            # drop_out = tf.layers.dropout(\n            #     inputs=inputs, rate=0.05)\n\n            # # Hidden Layer 1\n            # hidden1 = layers.fully_connected(\n            #     inputs=drop_out, num_outputs=256, activation_fn=tf.nn.relu)\n\n            # # Drop Out\n            # drop_out = tf.layers.dropout(\n            #     inputs=hidden1, rate=0.05)\n\n            # # Hidden Layer 2\n            # hidden2 = layers.fully_connected(\n            #     inputs=drop_out, num_outputs=128, activation_fn=tf.nn.relu)\n\n            # # Output Layer\n            # y = layers.fully_connected(\n            #     inputs=hidden2, num_outputs=1, activation_fn=None)\n\n            # if reuse:\n            #     scope.reuse_variables()\n            # tf.truncated_normal_initializer()\n\n            # ---------------------------------------------------------------#\n\n            keep_prob = 0.9\n            num_h1 = 392\n            num_h2 = 196\n\n            # Fully Connected Layer 1, dropout\n            w1 = tf.get_variable(name=""d_w1"",\n                                 shape=[self._ndims, num_h1],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b1 = tf.get_variable(name=""d_b1"",\n                                 shape=[num_h1],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(x, w1) + b1), keep_prob)\n\n            # Fully Connected Layer 2, dropout\n            w2 = tf.get_variable(name=""d_w2"",\n                                 shape=[num_h1, num_h2],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b2 = tf.get_variable(name=""d_b2"",\n                                 shape=[num_h2],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1, w2) + b2), keep_prob)\n\n            # Fully Connected Layer 3\n            w3 = tf.get_variable(name=""d_w3"",\n                                 shape=[num_h2, 1],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b3 = tf.get_variable(name=""d_b3"",\n                                 shape=[1],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            y = tf.matmul(h2, w3) + b3  # logits\n\n            # ---------------------------------------------------------------#\n\n            # n_units = 392\n            # alpha = 0.01\n\n            # # Hidden layer\n            # h1 = tf.layers.dense(x, n_units, activation=None)\n            # # Leaky ReLU\n            # h1 = tf.maximum(h1, alpha * h1)\n\n            # # logits\n            # y = tf.layers.dense(h1, 1, activation=None)\n\n        return y\n\n    def _discriminator_loss(self, y, y_hat):\n        """"""Loss for the discriminator.\n\n        Args:\n            y (tf.Tensor): The output tensor of the discriminator for true\n                images of dimension (None, 1).\n            y_hat (tf.Tensor): The output tensor of the discriminator for fake\n                images of dimension (None, 1).\n        Returns:\n            l (tf.Scalar): average batch loss for the discriminator.\n\n        """"""\n        # Label smoothing\n        # smooth = 0.1\n\n        # create labels for discriminator\n        d_labels_real = tf.ones_like(y)\n        d_labels_fake = tf.zeros_like(y_hat)\n\n        # compute loss for real/fake images\n        d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=y, labels=d_labels_real)\n        d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=y_hat, labels=d_labels_fake)\n\n        l = tf.reduce_mean(d_loss_fake + d_loss_real)\n\n        return l\n\n    def _generator(self, z, reuse=False):\n        """"""From a sampled z, generate an image.\n\n        Args:\n            z(tf.Tensor): z from _sample_z of dimension (None, nlatent).\n            reuse (Boolean): reuse variables with same name in scope instead\n                of creating new ones, check Tensorflow documentation\n        Returns:\n            x_hat(tf.Tensor): Fake image G(z) (None, 784).\n        """"""\n        with tf.variable_scope(""generator"", reuse=reuse) as scope:\n\n            # ---------------------------------------------------------------#\n            # # Input Layer\n            # inputs = layers.fully_connected(\n            #     inputs=z, num_outputs=128, activation_fn=tf.nn.relu)\n\n            # # Hidden Layer 1\n            # hidden1 = layers.fully_connected(\n            #     inputs=inputs, num_outputs=256, activation_fn=tf.nn.relu)\n\n            # # Hidden Layer 2\n            # hidden2 = layers.fully_connected(\n            #     inputs=hidden1, num_outputs=512, activation_fn=tf.nn.relu)\n\n            # # Output Layer\n            # x_hat = layers.fully_connected(\n            #     inputs=hidden2, num_outputs=self._ndims, activation_fn=tf.nn.softplus)\n\n            # if reuse:\n            #     scope.reuse_variables()\n\n            # tf.truncated_normal_initializer()\n\n            # ---------------------------------------------------------------#\n\n            h1_size = 196\n            h2_size = 392\n\n            # Fully Connected Layer 1\n            w1 = tf.get_variable(name=""g_w1"",\n                                 shape=[self._nlatent, h1_size],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b1 = tf.get_variable(name=""g_b1"",\n                                 shape=[h1_size],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            h1 = tf.nn.relu(tf.matmul(z, w1) + b1)\n\n            # Fully Connected Layer 2\n            w2 = tf.get_variable(name=""g_w2"",\n                                 shape=[h1_size, h2_size],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b2 = tf.get_variable(name=""g_b2"",\n                                 shape=[h2_size],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n\n            # Fully Connected Layer 3\n            w3 = tf.get_variable(name=""g_w3"",\n                                 shape=[h2_size, self._ndims],\n                                 dtype=tf.float32,\n                                 initializer=layers.xavier_initializer(uniform=False))\n\n            b3 = tf.get_variable(name=""g_b3"",\n                                 shape=[self._ndims],\n                                 dtype=tf.float32,\n                                 initializer=tf.zeros_initializer())\n\n            x_hat = tf.matmul(h2, w3) + b3\n\n            # ---------------------------------------------------------------#\n\n            # alpha = 0.01\n            # n_units = 392\n\n            # # Hidden layer\n            # h1 = tf.layers.dense(z, n_units, activation=None)\n\n            # # Leaky ReLU\n            # h1 = tf.maximum(h1, alpha * h1)\n\n            # # Logits\n            # x_hat = tf.layers.dense(h1, self._ndims, activation=None)\n\n            return x_hat\n\n    def _generator_loss(self, y_hat):\n        """"""Loss for the discriminator.\n\n        Args:\n            y_hat (tf.Tensor): The output tensor of the discriminator for fake\n                images of dimension (None, 1).\n        Returns:\n            l (tf.Scalar): average batch loss for the generator.\n\n        """"""\n        # create labels for generator\n        labels = tf.ones_like(y_hat)\n\n        l = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n            logits=y_hat, labels=labels))\n\n        return l\n\n    def generate_samples(self, z_np):\n        """"""Generate random samples from the provided z_np.\n\n        Args:\n            z_np(numpy.ndarray): Numpy array of dimension\n                (batch_size, _nlatent).\n\n        Returns:\n            out(numpy.ndarray): The sampled images (numpy.ndarray) of\n                dimension (batch_size, _ndims).\n        """"""\n        out = self.x_hat.eval(session=self.session, feed_dict={\n                              self.z_placeholder: z_np})\n        return out\n'"
assignments/assignment2/mp2/models/linear_model.py,0,"b'""""""Linear model base class.""""""\n\nimport abc\nimport numpy as np\nimport six\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass LinearModel(object):\n    """"""Abstract class for linear models.""""""\n\n    def __init__(self, ndims, w_init=\'zeros\', w_decay_factor=0.001):\n        """"""Initialize a linear model.\n\n        This function prepares an uninitialized linear model.\n        It will initialize the weight vector, self.w, based on the method\n        specified in w_init.\n\n        We assume that the last index of w is the bias term, self.w = [w,b]\n\n        self.w(numpy.ndarray): array of dimension (n_dims+1,1)\n\n        w_init needs to support:\n          \'zeros\': initialize self.w with all zeros.\n          \'ones\': initialze self.w with all ones.\n          \'uniform\': initialize self.w with uniform random number between [0,1)\n\n        Args:\n            ndims(int): feature dimension\n            w_init(str): types of initialization.\n            w_decay_factor(float): Weight decay factor for regularization.\n        """"""\n        self.ndims = ndims\n        self.w_init = w_init\n        self.w_decay_factor = w_decay_factor\n\n        if w_init == \'zeros\':\n            self.w = np.zeros((ndims + 1, 1))\n        elif w_init == \'ones\':\n            self.w = np.ones((ndims + 1, 1))\n        elif w_init == \'uniform\':\n            self.w = np.random.uniform(low=0.0, high=1.0, size=(ndims + 1, 1))\n\n        self.x = None\n\n        # pass\n\n    def forward(self, x):\n        """"""Perform forward operation for linear models.\n\n        Perform the forward operation. Appends 1 to x then compute\n        f=w^Tx, and return f.\n\n        Args:\n            x(numpy.ndarray): Dimension of (N, ndims), N is the number\n              of examples.\n\n        Returns:\n            (numpy.ndarray): Dimension of (N,1)\n        """"""\n        self.x = x[:, 0:self.ndims]\n        x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n        f = np.matmul(x, self.w)\n\n        return f\n\n    @abc.abstractmethod\n    def backward(self, f, y):\n        """"""Do not need to be implemented here.""""""\n        pass\n\n    @abc.abstractmethod\n    def total_loss(self, f, y):\n        """"""Do not need to be implemented here.""""""\n        pass\n\n    def predict(self, f):\n        """"""Do not need to be implemented here.""""""\n        pass\n'"
assignments/assignment2/mp2/models/linear_regression.py,0,"b'""""""Implements linear regression.""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport numpy as np\nfrom models.linear_model import LinearModel\n\n\nclass LinearRegression(LinearModel):\n    """"""Implement a linear regression mode model.""""""\n\n    def backward(self, f, y):\n        """"""Perform the backward operation.\n\n        By backward operation, it means to compute the gradient of the loss\n        with respect to w.\n\n        Hint: You may need to use self.x, and you made need to change the\n        forward operation.\n\n        Args:\n            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n            y(numpy.ndarray): Ground truth label, dimension (N,1).\n\n        Returns:\n            total_grad(numpy.ndarray): Gradient of L w.r.t to self.w,\n            dimension (ndims+1,1).\n        """"""\n        l = f - y\n\n        self.x = np.concatenate((self.x, np.ones((self.x.shape[0], 1))), axis=1)\n        xt = np.transpose(self.x)\n\n        square_grad = np.matmul(xt, l)\n        reg_grad = self.w_decay_factor * self.w\n        total_grad = square_grad + reg_grad\n\n        return total_grad\n\n    def total_loss(self, f, y):\n        """"""Compute the total loss, square loss + L2 regularization.\n\n        Overall loss is sum of squared_loss + w_decay_factor*l2_loss\n        Note: Don\'t forget the 0.5 in the squared_loss!\n\n        Args:\n            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n            y(numpy.ndarray): Ground truth label, dimension (N,1).\n        Returns:\n            total_loss (float): sum square loss + reguarlization.\n        """"""\n        # Same as \'Xw - y\'\n        diff = f - y\n        # print(diff.shape)\n        # squared_loss\n        squared_loss = 1 / 2 * np.matmul(np.transpose(diff), diff)\n\n        # L2 regularization\n        reg = self.w_decay_factor / 2 * np.matmul(np.transpose(self.w), self.w)\n\n        # total loss\n        total_loss = squared_loss + reg\n\n        # return total_loss[0][0]\n        return total_loss\n\n    def predict(self, f):\n        """"""Nothing to do here.""""""\n        return f\n'"
assignments/assignment2/mp2/utils/data_tools.py,0,"b'""""""Implement feature extraction and data processing helpers.""""""\n\n\nimport numpy as np\n\n\ndef preprocess_data(dataset,\n                    feature_columns=[\n                        \'Id\', \'BldgType\', \'OverallQual\',\n                        \'GrLivArea\', \'GarageArea\'\n                    ],\n                    squared_features=False\n                    ):\n    """"""Process the dataset into vector representation.\n\n    When converting the BldgType to a vector, use one-hot encoding, the order\n    has been provided in the one_hot_bldg_type helper function. Otherwise,\n    the values in the column can be directly used.\n\n    If squared_features is true, then the feature values should be\n    element-wise squared.\n\n    Args:\n        dataset(dict): Dataset extracted from io_tools.read_dataset\n        feature_columns(list): List of feature names.\n        squred_features(bool): Whether to square the features.\n\n    Returns:\n        processed_datas(list): List of numpy arrays x, y.\n            x is a numpy array, of dimension (N,K), N is the number of example\n            in the dataset, and K is the length of the feature vector.\n            Note: BldgType when converted to one hot vector is of length 5.\n            Each row of x contains an example.\n            y is a numpy array, of dimension (N,1) containing the SalePrice.\n    """"""\n    columns_to_id = {\'Id\': 0, \'BldgType\': 1, \'OverallQual\': 6,\n                     \'GrLivArea\': 7, \'GarageArea\': 8, \'SalePrice\': 9}\n\n    cat_col = []\n    for key in feature_columns:\n        cat_col.append(columns_to_id[key])\n    if \'BldgType\' in feature_columns:\n        cat_col.extend([2, 3, 4, 5])\n\n    x = []\n    y = []\n\n    for keys in dataset:\n            temp = list(dataset[keys])\n            vector = one_hot_bldg_type(temp[1])\n\n            temp = [temp[0]] + vector + temp[2:]\n            temp = list(map(int, temp))\n\n            x.append(temp[0:-1])\n            y.append(temp[-1])\n\n    # If squared_features is true, then the feature values should be\n    # element-wise squared.\n    if squared_features:\n        x = np.square(x)\n\n    # Select specific number of features\n    x = np.array(x)[:, sorted(cat_col)]\n    y = np.reshape(y, (x.shape[0], 1))\n\n    processed_dataset = [x, y]\n    return processed_dataset\n\n\ndef one_hot_bldg_type(bldg_type):\n    """"""Build the one-hot encoding vector.\n\n    Args:\n        bldg_type(str): String indicating the building type.\n\n    Returns:\n        ret(list): A list representing the one-hot encoding vector.\n            (e.g. for 1Fam building type, the returned list should be\n            [1,0,0,0,0].\n    """"""\n    type_to_id = {\'1Fam\': 0,\n                  \'2FmCon\': 1,\n                  \'Duplx\': 2,\n                  \'TwnhsE\': 3,\n                  \'TwnhsI\': 4,\n                  }\n\n    # bldg_type in train.csv first row is \'1Fam\'\n    idx = type_to_id[bldg_type]\n\n    vector = [0, 0, 0, 0, 0]\n\n    # change the vector to one-hot encoding\n    vector[idx] = 1\n\n    # vector assignment\n    ret = vector[:]\n    return ret\n'"
assignments/assignment2/mp2/utils/io_tools.py,0,"b'""""""Input and output helpers to load in data.""""""\n\nimport csv\n\n\ndef read_dataset(input_csv_file):\n    """"""Read data into a python list.\n\n    Args:\n        input_csv_file: Path to the data csv file.\n\n    Returns:\n        dataset(dict): A python dictionary with the key value pair of\n            (example_id, example_feature).\n\n            example_feature is represented with a tuple\n            (Id, BldgType, OverallQual, GrLivArea, GarageArea)\n\n            For example, the first row will be in the train.csv is\n            example_id = 1\n            example_feature = (1,1Fam,7,1710,548)\n    """"""\n    dataset = {}\n\n    # Imeplemntation here.\n    with open(input_csv_file) as csvfile:\n        reader = csv.DictReader(csvfile)\n        for row in reader:\n            dataset[row[\'Id\']] = (row[\'Id\'],\n                                  row[\'BldgType\'],\n                                  row[\'OverallQual\'],\n                                  row[\'GrLivArea\'],\n                                  row[\'GarageArea\'],\n                                  row[\'SalePrice\'])\n\n    return dataset\n'"
assignments/assignment2/mp2/utils/plot_tools.py,0,"b'""""""Helper functions for plotting.""""""\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\n\ndef plot_x_vs_y(dataset, model):\n    """"""Plot x vs y. Do not have to change this function.\n\n    Args:\n        dataset(list): processed dataset, from data_tools.\n        model(LinearRegression): Linear regression model.\n    """"""\n    plt.scatter(dataset[0], dataset[1])\n    # x_tick = np.arange(np.min(dataset[0]), np.max(dataset[0]), 0.001)\n    x_tick = np.linspace(np.min(dataset[0]), np.max(dataset[0]))\n    y_tick = model.w[0] * x_tick + model.w[1]\n    plt.plot(x_tick, y_tick, \'r\')\n    plt.savefig(\'xy_plot.png\')\n'"
assignments/assignment3/mp3/codefromscratch/io_tools.py,0,"b'""""""Input and output helpers to load in data.""""""\nimport numpy as np\n\n\ndef read_dataset(path_to_dataset_folder, index_filename):\n    """""" Read dataset into numpy arrays with preprocessing included.\n    Args:\n        path_to_dataset_folder(str): path to the folder containing samples and indexing.txt\n        index_filename(str): indexing.txt\n    Returns:\n        A(numpy.ndarray): sample feature matrix A = [[1, x1],\n                                                     [1, x2],\n                                                     [1, x3],\n                                                     .......]\n                                where xi is the 16-dimensional feature of each sample\n\n        T(numpy.ndarray): class label vector T = [y1, y2, y3, ...]\n                             where yi is +1/-1, the label of each sample\n    """"""\n    ###############################################################\n    # Fill your code in this function\n    ###############################################################\n    # Hint: open(path_to_dataset_folder+\'/\'+index_filename,\'r\')\n\n    T = []\n    A = []\n    with open(path_to_dataset_folder + \'/\' + index_filename, \'r\') as data_file:\n        for line in data_file:\n            T.append(line[:2])\n            T = [int(item) for item in T]\n            idx = line.find(\' \')\n            fdir = line[idx + 1:]\n            with open(path_to_dataset_folder + \'/\' + fdir[:-1], \'r\') as sample_file:\n                for line_sample in sample_file:\n                    temp = line_sample.split(\' \')\n                    A_prime = []\n                    for _ in temp:\n                        if _:\n                            A_prime.append(_)\n                    A_prime = [float(item) for item in A_prime]\n                    A.append(A_prime)\n    A = np.array(A)\n    A = np.concatenate((np.ones((A.shape[0], 1)), A), axis=1)\n    T = np.array(T)\n\n    return (A, T)\n'"
assignments/assignment3/mp3/codefromscratch/logistic_model.py,0,"b'""""""logistic model class for binary classification.""""""\n\nimport numpy as np\n\n\nclass LogisticModel(object):\n\n    def __init__(self, ndims, W_init=\'zeros\'):\n        """"""Initialize a logistic model.\n\n        This function prepares an initialized logistic model.\n        It will initialize the weight vector, self.W, based on the method\n        specified in W_init.\n\n        We assume that the FIRST index of W is the bias term,\n            self.W = [Bias, W1, W2, W3, ...]\n            where Wi correspnds to each feature dimension\n\n        W_init needs to support:\n          \'zeros\': initialize self.W with all zeros.\n          \'ones\': initialze self.W with all ones.\n          \'uniform\': initialize self.W with uniform random number between [0,1)\n          \'gaussian\': initialize self.W with gaussion distribution (0, 0.1)\n\n        Args:\n            ndims(int): feature dimension\n            W_init(str): types of initialization.\n        """"""\n        self.ndims = ndims\n        self.W_init = W_init\n        self.W = None\n        ###############################################################\n        # Fill your code below\n        ###############################################################\n        if W_init == \'zeros\':\n            self.W = np.zeros([ndims + 1, ])\n        elif W_init == \'ones\':\n            self.W = np.ones([ndims + 1, ])\n        elif W_init == \'uniform\':\n            self.W = np.random.uniform(low=0, high=1, size=(ndims + 1, ))\n        elif W_init == \'gaussian\':\n            self.W = np.random.normal(loc=0, scale=0.1, size=(ndims + 1, ))\n        else:\n            print(\'Unknown W_init \', W_init)\n\n    def save_model(self, weight_file):\n        """""" Save well-trained weight into a binary file.\n        Args:\n            weight_file(str): binary file to save into.\n        """"""\n        self.W.astype(\'float32\').tofile(weight_file)\n        print(\'model saved to\', weight_file)\n\n    def load_model(self, weight_file):\n        """""" Load pretrained weghit from a binary file.\n        Args:\n            weight_file(str): binary file to load from.\n        """"""\n        self.W = np.fromfile(weight_file, dtype=np.float32)\n        print(\'model loaded from\', weight_file)\n\n    def forward(self, X):\n        """""" Forward operation for logistic models.\n            Performs the forward operation, and return probability score (sigmoid).\n        Args:\n            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n        Returns:\n            (numpy.ndarray): probability score of (label == +1) for each sample\n                             with a dimension of (# of samples,)\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n        z = X.dot(self.W)\n        p = 1 / (1 + np.exp(-z))\n\n        return p\n\n    def backward(self, Y_true, X):\n        """""" Backward operation for logistic models.\n            Compute gradient according to the probability loss on lecture slides\n        Args:\n            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n            Y_true(numpy.ndarray): dataset labels with a dimension of (# of samples,)\n        Returns:\n            (numpy.ndarray): gradients of self.W\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n        self.W = np.reshape(self.W, (self.W.shape[0], 1))\n        z = X.dot(self.W)\n        z = np.reshape(z, (z.shape[0], 1))\n\n        Y_true = np.reshape(Y_true, (Y_true.shape[0], 1))\n        den = 1 + np.exp(-1 * np.multiply(Y_true, z))\n        num = np.exp(-1 * np.multiply(Y_true, z))\n        grad = np.multiply(np.divide(num, den), -Y_true).T.dot(X)\n\n        return grad.T\n\n    def classify(self, X):\n        """""" Performs binary classification on input dataset.\n        Args:\n            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n        Returns:\n            (numpy.ndarray): predicted label = +1/-1 for each sample\n                             with a dimension of (# of samples,)\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n        p = self.forward(X)\n        p[p >= 0.5] = 1\n        p[p < 0.5] = -1\n\n        return p.flatten()\n\n    def fit(self, Y_true, X, learn_rate, max_iters):\n        """""" train model with input dataset using gradient descent.\n        Args:\n            Y_true(numpy.ndarray): dataset labels with a dimension of (# of samples,)\n            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n            learn_rate: learning rate for gradient descent\n            max_iters: maximal number of iterations\n            ......: append as many arguments as you want\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n        for iter in range(max_iters):\n\n            grad = self.backward(Y_true, X)\n            self.W = self.W - learn_rate * grad\n\n            if iter == max_iters - 1:\n                l = self.classify(X)\n                s = np.sum(Y_true == l)\n                acc = s / len(Y_true)\n                print(\'Iter:\', iter + 1, \'Accuracy: \', acc)\n'"
assignments/assignment3/mp3/codefromscratch/main.py,0,"b'""""""Main function for binary classifier""""""\n\n\nimport numpy as np\nfrom io_tools import *\nfrom logistic_model import *\n\n"""""" Hyperparameter for Training """"""\nlearn_rate = 0.0007\nmax_iters = 300\n\nif __name__ == \'__main__\':\n    ###############################################################\n    # Fill your code in this function to learn the general flow\n    # (..., although this funciton will not be graded)\n    ###############################################################\n\n    # Load dataset.\n    A, T = read_dataset(\'../data/trainset\', \'indexing.txt\')\n\n    # Initialize model.\n    ndims = A.shape[1]\n    model = LogisticModel(ndims - 1, W_init=\'ones\')\n\n    # Train model via gradient descent.\n    # model.fit(T[0:411], A[0:411,:], learn_rate, max_iters)\n    model.fit(T, A, learn_rate, max_iters)\n\n    # Save trained model to \'trained_weights.np\'\n    model.save_model(\'trained_weights.np\')\n\n    # Load trained model from \'trained_weights.np\'\n    model.load_model(\'trained_weights.np\')\n\n    # Cross validation\n    model.fit(T[411:543], A[411:543, :], learn_rate, max_iters)\n\n    # Test model\n    model.fit(T[543:], A[543:, :], learn_rate, max_iters)\n    # Try all other methods: forward, backward, classify, compute accuracy\n'"
assignments/assignment3/mp3/codefromtf/io_tools.py,0,"b'""""""Input and output helpers to load in data.""""""\nimport numpy as np\n\n\ndef read_dataset_tf(path_to_dataset_folder, index_filename):\n    """""" Read dataset into numpy arrays with preprocessing included\n    Args:\n        path_to_dataset_folder(str): path to the folder containing samples and indexing.txt\n        index_filename(str): indexing.txt\n    Returns:\n        A(numpy.ndarray): sample feature matrix A = [[1, x1],\n                                                     [1, x2],\n                                                     [1, x3],\n                                                     .......]\n                                where xi is the 16-dimensional feature of each sample\n\n        T(numpy.ndarray): class label vector T = [[y1],\n                                                  [y2],\n                                                  [y3],\n                                                   ...]\n                             where yi is 1/0, the label of each sample\n    """"""\n    ###############################################################\n    # Fill your code in this function\n    ###############################################################\n    # Hint: open(path_to_dataset_folder+\'/\'+index_filename,\'r\')\n    T = []\n    A = []\n    with open(path_to_dataset_folder + \'/\' + index_filename, \'r\') as data_file:\n        for line in data_file:\n            T.append(line[:2])\n            T = [int(item) for item in T]\n            idx = line.find(\' \')\n            fdir = line[idx + 1:]\n            with open(path_to_dataset_folder + \'/\' + fdir[:-1], \'r\') as sample_file:\n                for line_sample in sample_file:\n                    temp = line_sample.split(\' \')\n                    A_prime = []\n                    for _ in temp:\n                        if _:\n                            A_prime.append(_)\n                    A_prime = [float(item) for item in A_prime]\n                    A.append(A_prime)\n    A = np.array(A)\n    A = np.concatenate((np.ones((A.shape[0], 1)), A), axis=1)\n    T = np.array(T)\n    T[T == -1] = 0\n    T = np.reshape(T, (T.shape[0], 1))\n\n    return (A, T)\n'"
assignments/assignment3/mp3/codefromtf/logistic_model.py,17,"b'""""""logistic model class for binary classification.""""""\nimport tensorflow as tf\nimport numpy as np\n\n\nclass LogisticModel_TF(object):\n\n    def __init__(self, ndims, W_init=\'zeros\'):\n        """"""Initialize a logistic model.\n\n        This function prepares an initialized logistic model.\n        It will initialize the weight vector, self.W, based on the method\n        specified in W_init.\n\n        We assume that the FIRST index of Weight is the bias term,\n            Weight = [Bias, W1, W2, W3, ...]\n            where Wi correspnds to each feature dimension\n\n        W_init needs to support:\n          \'zeros\': initialize self.W with all zeros.\n          \'ones\': initialze self.W with all ones.\n          \'uniform\': initialize self.W with uniform random number between [0,1)\n          \'gaussian\': initialize self.W with gaussion distribution (0, 0.1)\n\n        Args:\n            ndims(int): feature dimension\n            W_init(str): types of initialization.\n        """"""\n        self.ndims = ndims\n        self.W_init = W_init\n        self.W0 = None\n        ###############################################################\n        # Fill your code below\n        ###############################################################\n        if W_init == \'zeros\':\n            self.W0 = tf.zeros([self.ndims + 1, 1], dtype=tf.float64)\n        elif W_init == \'ones\':\n            self.W0 = tf.ones([self.ndims + 1, 1], dtype=tf.float64)\n        elif W_init == \'uniform\':\n            self.W0 = tf.random_uniform(\n                [self.ndims + 1, 1], minval=0, maxval=1, dtype=tf.float64)\n        elif W_init == \'gaussian\':\n            self.W0 = tf.random_normal(\n                [self.ndims + 1, 1], mean=0, stddev=0.1, dtype=tf.float64)\n        else:\n            print(\'Unknown W_init \', W_init)\n\n    def build_graph(self, learn_rate):\n        """""" build tensorflow training graph for logistic model.\n        Args:\n            learn_rate: learn rate for gradient descent\n            ......: append as many arguments as you want\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n        # Hint: self.W = tf.Variable(self.W0)\n        self.W = tf.Variable(self.W0, dtype=tf.float64)\n        self.X = tf.placeholder(tf.float64, (None, self.ndims + 1))\n        self.Y = tf.placeholder(tf.float64, (None, 1))\n\n        self.loss = tf.matmul(tf.transpose(self.Y - (1 / (1 + tf.exp(tf.matmul(-self.X, self.W))))),\n                              self.Y - (1 / (1 + tf.exp(tf.matmul(-self.X, self.W)))))\n        self.optimizer = tf.train.GradientDescentOptimizer(\n            learning_rate=learn_rate).minimize(self.loss)\n\n    def fit(self, Y_true, X, max_iters, learn_rate):\n        """""" train model with input dataset using gradient descent.\n        Args:\n            Y_true(numpy.ndarray): dataset labels with a dimension of (# of samples,1)\n            X(numpy.ndarray): input dataset with a dimension of (# of samples, ndims+1)\n            max_iters: maximal number of training iterations\n            ......: append as many arguments as you want\n        Returns:\n            (numpy.ndarray): sigmoid output from well trained logistic model, used for classification\n                             with a dimension of (# of samples, 1)\n        """"""\n        ###############################################################\n        # Fill your code in this function\n        ###############################################################\n\n        with tf.Session() as sess:\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            for i in range(max_iters):\n                sess.run([self.optimizer, self.loss],\n                         feed_dict={self.X: X, self.Y: Y_true})\n\n                # Print accuracy when it is last iteration\n                if i == max_iters - 1:\n                    prob = sess.run(1 / (1 + tf.exp(tf.matmul(-X, self.W))))\n                    prob = prob.flatten()\n                    prob[prob >= 0.5] = 1\n                    prob[prob < 0.5] = 0\n\n                    s = np.sum(Y_true.flatten() == prob)\n                    acc = s / len(Y_true)\n\n                    print(\'Iter: \', i + 1, \'Accuracy: \', acc)\n            out = sess.run(1 / (1 + tf.exp(tf.matmul(-X, self.W))))\n\n        # dimension (# of samples, 1)\n        output = np.reshape(out.flatten(), (out.flatten().shape[0], 1))\n\n        return output\n'"
assignments/assignment3/mp3/codefromtf/main.py,1,"b'""""""Main function for binary classifier""""""\nimport tensorflow as tf\nimport numpy as np\nfrom io_tools import *\nfrom logistic_model import *\n\n"""""" Hyperparameter for Training """"""\nlearn_rate = 0.01\nmax_iters = 300\n\n\ndef main(_):\n    ###############################################################\n    # Fill your code in this function to learn the general flow\n    # (..., although this funciton will not be graded)\n    ###############################################################\n\n    # Load dataset.\n    # Hint: A, T = read_dataset_tf(\'../data/trainset\',\'indexing.txt\')\n    A, T = read_dataset_tf(\'../data/trainset\', \'indexing.txt\')\n    # Initialize model.\n    N, ndims = A.shape\n    model = LogisticModel_TF(ndims - 1, W_init=\'zeros\')\n\n    # Build TensorFlow training graph\n    model.build_graph(learn_rate)\n\n    # Train model via gradient descent.\n    # Compute classification accuracy based on the return of the ""fit"" method\n    # prob = model.fit(T[:774], A[:774,:], max_iters, learn_rate)\n    prob = model.fit(T, A, max_iters, learn_rate)\n    # Print Sigmoid output\n    print(prob)\n\n    prob = model.fit(T[774:], A[774:, :], max_iters, learn_rate)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
assignments/assignment4/mp4/models/linear_model.py,0,"b'""""""Linear model base class.""""""\n\nimport abc\nimport numpy as np\nimport six\n\n# - Double check that the __init__ function works with zeros and ones.\n# - Double check that your forward function output is reasonable, (e.g. weights\n# of all zeros, then the output should be 0).\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass LinearModel(object):\n    """"""Abstract class for linear models.""""""\n\n    def __init__(self, ndims, w_init=\'zeros\', w_decay_factor=0.001):\n        """"""Initialize a linear model.\n\n        This function prepares an uninitialized linear model.\n        It will initialize the weight vector, self.w, based on the method\n        specified in w_init.\n\n        We assume that the last index of w is the bias term, self.w = [w,b]\n\n        self.w(numpy.ndarray): array of dimension (n_dims+1,1)\n\n        w_init needs to support:\n          \'zeros\': initialize self.w with all zeros.\n          \'ones\': initialze self.w with all ones.\n          \'uniform\': initialize self.w with uniform random number between [0,1)\n\n        Args:\n            ndims(int): feature dimension\n            w_init(str): types of initialization.\n            w_decay_factor(float): Weight decay factor.\n        """"""\n        self.ndims = ndims\n        self.w_init = w_init\n        self.w_decay_factor = w_decay_factor\n        self.w = None\n        self.x = None\n        # Implementation here.\n        if w_init == \'zeros\':\n            self.w = np.zeros([ndims + 1, 1])\n        elif w_init == \'ones\':\n            self.w = np.ones([ndims + 1, 1])\n        elif w_init == \'uniform\':\n            self.w = np.random.uniform(low=0, high=1, size=(ndims + 1, 1))\n        else:\n            print(\'Unknown w_init \', w_init)\n\n    def forward(self, x):\n        """"""Forward operation for linear models.\n\n        Performs the forward operation. Appends 1 to x then compute\n        f=w^Tx, and return f.\n\n        Args:\n            x(numpy.ndarray): Dimension of (N, ndims), N is the number\n              of examples.\n\n        Returns:\n            (numpy.ndarray): Dimension of (N,1)\n        """"""\n        # Implementation here.\n        self.x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n        f = self.x.dot(self.w)\n\n        return f\n\n    @abc.abstractmethod\n    def backward(self, f, y):\n        """"""Do not need to be implemented here.""""""\n        pass\n\n    @abc.abstractmethod\n    def total_loss(self, f, y):\n        """"""Do not need to be implemented here.""""""\n        pass\n\n    def predict(self, f):\n        """"""Do not need to be implemented here.""""""\n        pass\n'"
assignments/assignment4/mp4/models/support_vector_machine.py,0,"b'""""""Implements support vector machine.""""""\n\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport numpy as np\nfrom models.linear_model import LinearModel\n\n\nclass SupportVectorMachine(LinearModel):\n    """"""Implement a linear regression mode model.""""""\n\n    def backward(self, f, y):\n        """"""Perform the backward operation based on the loss in total_loss.\n\n        By backward operation, it means to compute the gradient of the loss\n        w.r.t w.\n\n        Hint: You may need to use self.x, and you made need to change the\n        forward operation.\n\n        Args:\n            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n            y(numpy.ndarray): Ground truth label, dimension (N,1).\n        Returns:\n            total_grad(numpy.ndarray): Gradient of L w.r.t to self.w,\n              dimension (ndims+1,).\n        """"""\n        # Implementation here.\n        reg_grad = self.w_decay_factor * self.w\n        indicator = np.multiply(y, f)\n\n        loss_grad = np.zeros((self.ndims + 1, 1))\n        for i, value in enumerate(indicator):\n            if value < 1:\n                loss_grad[:, 0] += self.x[i].T * -y[i]\n\n        total_grad = reg_grad + loss_grad\n\n        return total_grad.flatten()\n\n    def total_loss(self, f, y):\n        """"""The sum of the loss across batch examples + L2 regularization.\n\n        Total loss is hinge_loss + w_decay_factor / 2 * ||w||^2\n\n        Args:\n            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n            y(numpy.ndarray): Ground truth label, dimension (N,1).\n        Returns:\n            total_loss (float): sum hinge loss + reguarlization.\n        """"""\n        # Implementation here.\n        hinge_loss = np.sum(np.maximum(0, 1 - np.multiply(y, f)))\n        l2_loss = self.w_decay_factor / 2 * self.w.T.dot(self.w)\n\n        total_loss = hinge_loss + l2_loss\n        total_loss = np.asscalar(total_loss)\n\n        return total_loss\n\n    def predict(self, f):\n        """"""Convert score to prediction.\n\n        Args:\n            f(numpy.ndarray): Output of forward operation, dimension (N,1).\n        Returns:\n            (numpy.ndarray): Hard predictions from the score, f,\n              dimension (N,1). Tie break 0 to 1.0.\n        """"""\n        # Implementation here.\n        y_predict = np.zeros(f.shape)\n        y_predict[f >= 0] = 1\n        y_predict[f < 0] = -1\n\n        return y_predict\n'"
assignments/assignment4/mp4/utils/data_tools.py,0,"b'""""""\nImplements feature extraction and other data processing helpers.\n\n(This file will not be graded).\n""""""\n\n\nimport numpy as np\nimport skimage\nfrom skimage import color\n\n\ndef preprocess_data(data, process_method=\'default\'):\n    """"""Preprocesses dataset.\n\n    Args:\n        data(dict): Python dict loaded using io_tools.\n        process_method(str): processing methods needs to support\n          [\'raw\', \'default\'].\n\n        if process_method is \'raw\'\n          1. Convert the images to range of [0, 1] by dividing by 255.\n          2. Remove dataset mean. Average the images across the batch dimension.\n             This will result in a mean image of dimension (8,8,3).\n          3. Flatten images, data[\'image\'] is converted to dimension (N, 8*8*3)\n\n        if process_method is \'default\':\n          1. Convert images to range [0,1]\n          2. Convert from rgb to gray then back to rgb. Use skimage\n          3. Take the absolute value of the difference with the original image.\n          4. Remove dataset mean. Average the absolute value differences across\n             the batch dimension. This will result in a mean of dimension (8,8,3).\n          5. Flatten images, data[\'image\'] is converted to dimension (N, 8*8*3)\n\n    Returns:\n        data(dict): Apply the described processing based on the process_method\n        str to data[\'image\'], then return data.\n    """"""\n    if process_method == \'raw\':\n        # Convert images to range [0,1]\n        scaled_image = data[\'image\'] / 255\n        N = len(scaled_image)\n\n        # Remove dataset mean\n        image_mean = np.sum(scaled_image, axis=0) / N\n        for image in scaled_image:\n            image -= image_mean\n\n        # Flatten images\n        data[\'image\'] = scaled_image.flatten().reshape(N, 8 * 8 * 3)\n\n    elif process_method == \'default\':\n        # Convert images to range [0,1]\n        scaled_image = data[\'image\'] / 255\n        N = len(scaled_image)\n\n        # Convert from rgb to gray then back to rgb\n        grayscale = color.rgb2gray(scaled_image)\n        recRgb = color.gray2rgb(grayscale)\n\n        # Take the absolute value of the difference with the original image\n        absval = abs(recRgb - scaled_image)\n\n        # Remove dataset mean\n        image_mean = np.sum(absval, axis=0) / N\n\n        for image in absval:\n            image -= image_mean\n\n        # Flatten images\n        data[\'image\'] = absval.flatten().reshape(N, 8 * 8 * 3)\n\n    elif process_method == \'custom\':\n        # Design your own feature!\n        pass\n\n    return data\n\n\ndef compute_image_mean(data):\n    """""" Computes mean image.\n\n    Args:\n        data(dict): Python dict loaded using io_tools.\n\n    Returns:\n        image_mean(numpy.ndarray): Average across the example dimension.\n    """"""\n    pass\n    return data\n\n\ndef remove_data_mean(data):\n    """"""Removes data mean.\n\n    Args:\n        data(dict): Python dict loaded using io_tools.\n\n    Returns:\n        data(dict): Remove mean from data[\'image\'] and return data.\n    """"""\n    pass\n    return data\n'"
assignments/assignment4/mp4/utils/io_tools.py,0,"b'""""""\nInput and output helpers to load in data.\n\n(This file will not be graded.)\n""""""\n\nimport numpy as np\nimport skimage\nimport os\nfrom skimage import io\n\n\ndef read_dataset(data_txt_file, image_data_path):\n    """"""Read data into a Python dictionary.\n\n    Args:\n        data_txt_file(str): path to the data txt file.\n        image_data_path(str): path to the image directory.\n\n    Returns:\n        data(dict): A Python dictionary with keys \'image\' and \'label\'.\n            The value of dict[\'image\'] is a numpy array of dimension (N,8,8,3)\n            containing the loaded images.\n\n            The value of dict[\'label\'] is a numpy array of dimension (N,1)\n            containing the loaded label.\n\n            N is the number of examples in the data split, the examples should\n            be stored in the same order as in the txt file.\n    """"""\n    data = {}\n    data[\'image\'] = []\n    data[\'label\'] = []\n\n    # Read txt file\n    with open(data_txt_file, \'r\') as f:\n        for line in f:\n            # Split the string to imgdirs & labels\n            dirs = line.rstrip().split(\',\')\n\n            # output of dirs -> [\'0000\', \'-1\']\n            data[\'label\'].append([int(dirs[1])])\n\n            # Append \'.jpg\' to filename\n            imname = dirs[0] + \'.jpg\'\n            imgdir = os.path.join(image_data_path, imname)\n            data[\'image\'].append(io.imread(imgdir))\n\n    # Transform to Numpy array with specific dimension\n    data[\'label\'] = np.array(data[\'label\'])\n    data[\'image\'] = np.array(data[\'image\'])\n\n    return data\n'"
assignments/assignment5/mp5/model/self_multiclass.py,0,"b""import numpy as np\nfrom sklearn import svm\n\n\nclass MulticlassSVM:\n\n    def __init__(self, mode):\n        if mode != 'ovr' and mode != 'ovo' and mode != 'crammer-singer':\n            raise ValueError('mode must be ovr or ovo or crammer-singer')\n        self.mode = mode\n\n    def fit(self, X, y):\n        if self.mode == 'ovr':\n            self.fit_ovr(X, y)\n        elif self.mode == 'ovo':\n            self.fit_ovo(X, y)\n        elif self.mode == 'crammer-singer':\n            self.fit_cs(X, y)\n\n    def fit_ovr(self, X, y):\n        self.labels = np.unique(y)\n        self.binary_svm = self.bsvm_ovr_student(X, y)\n\n    def fit_ovo(self, X, y):\n        self.labels = np.unique(y)\n        self.binary_svm = self.bsvm_ovo_student(X, y)\n\n    def fit_cs(self, X, y):\n        self.labels = np.unique(y)\n        X_intercept = np.hstack([X, np.ones((len(X), 1))])\n\n        N, d = X_intercept.shape\n        K = len(self.labels)\n\n        W = np.zeros((K, d))\n\n        n_iter = 1500\n        learning_rate = 1e-8\n        for i in range(n_iter):\n            W -= learning_rate * self.grad_student(W, X_intercept, y)\n\n        self.W = W\n\n    def predict(self, X):\n        if self.mode == 'ovr':\n            return self.predict_ovr(X)\n        elif self.mode == 'ovo':\n            return self.predict_ovo(X)\n        else:\n            return self.predict_cs(X)\n\n    def predict_ovr(self, X):\n        scores = self.scores_ovr_student(X)\n        return self.labels[np.argmax(scores, axis=1)]\n\n    def predict_ovo(self, X):\n        scores = self.scores_ovo_student(X)\n        return self.labels[np.argmax(scores, axis=1)]\n\n    def predict_cs(self, X):\n        X_intercept = np.hstack([X, np.ones((len(X), 1))])\n        return np.argmax(self.W.dot(X_intercept.T), axis=0)\n\n    def bsvm_ovr_student(self, X, y):\n        '''\n        Train OVR binary classfiers.\n\n        Arguments:\n            X, y: training features and labels.\n\n        Returns:\n            binary_svm: a dictionary with labels as keys,\n                        and binary SVM models as values.\n        '''\n        binary_svm = {}\n        for i in range(self.labels.shape[0]):\n            temp = np.copy(y)\n            temp[y != self.labels[i]] = 0\n            temp[y == self.labels[i]] = 1\n            clf = svm.LinearSVC(random_state=12345)\n            binary_svm[self.labels[i]] = clf.fit(X, temp)\n        return binary_svm\n\n    def bsvm_ovo_student(self, X, y):\n        '''\n        Train OVO binary classfiers.\n\n        Arguments:\n            X, y: training features and labels.\n\n        Returns:\n            binary_svm: a dictionary with label pairs as keys,\n                        and binary SVM models as values.\n        '''\n        binary_svm = {}\n        summation = np.hstack((np.reshape(y, (X.shape[0], 1)), X))\n        for i in range(self.labels.shape[0]):\n            for j in range(i + 1, self.labels.shape[0]):\n                tempi = summation[y == i]\n                tempj = summation[y == j]\n                temp = np.vstack((tempi, tempj))\n                clf = svm.LinearSVC(random_state=12345)\n                binary_svm[tuple((i, j))] = clf.fit(temp[:, 1:], temp[:, 0])\n        return binary_svm\n\n    def scores_ovr_student(self, X):\n        '''\n        Compute class scores for OVR.\n\n        Arguments:\n            X: Features to predict.\n\n        Returns:\n            scores: a numpy ndarray with scores.\n        '''\n        scores = []\n        for idx in self.labels:\n            scores.append(self.binary_svm[idx].decision_function(X))\n        scores = np.array(scores)\n        return np.transpose(scores)\n\n    def scores_ovo_student(self, X):\n        '''\n        Compute class scores for OVO.\n\n        Arguments:\n            X: Features to predict.\n\n        Returns:\n            scores: a numpy ndarray with scores.\n        '''\n        scores = []\n        temp = []\n        for idx in self.binary_svm:\n            temp.append(self.binary_svm[idx].predict(X))\n\n        np_temp = np.transpose(np.array(temp))\n\n        for i in range(np_temp.shape[0]):\n            item = np_temp[i, :]\n            item = item.astype(np.int)\n            scores.append(np.bincount(item, minlength=self.labels.shape[0]))\n        return np.array(scores)\n\n    def loss_student(self, W, X, y, C=1.0):\n        '''\n        Compute loss function given W, X, y.\n\n        For exact definitions, please check the MP document.\n\n        Arugments:\n            W: Weights. Numpy array of shape (K, d)\n            X: Features. Numpy array of shape (N, d)\n            y: Labels. Numpy array of shape N\n            C: Penalty constant. Will always be 1 in the MP.\n\n        Returns:\n            The value of loss function given W, X and y.\n        '''\n        # Loss function = 1/2 * ||W||^2 + max(sum(1 - delta + w_j^Tx) - w_yi^Tx)\n        # Loss of regularization\n        reg_loss = 0.5 * np.trace(W.dot(W.T))\n\n        # Number of observations and number of classes\n        N = X.shape[0]\n        K = W.shape[0]\n\n        # Initialize Delta matrix\n        Delta = np.zeros((K, N))\n        for j in range(K):\n            for i in range(N):\n                if j == y[i]:\n                    Delta[j, i] = 1\n        I = np.ones((K, N))\n        sub = I - Delta + W.dot(X.T)\n        sub = np.reshape(np.amax(sub, axis=0), (N, 1))\n\n        # Initialize w_yi^Tx\n        foo = []\n        for i in range(N):\n            # foo[i] = W[y[i], :].dot((X[i, :]).T)\n            foo.append(W[y[i]].dot((X[i]).T))\n        foo = np.reshape(np.array(foo), (N, 1))\n\n        # Max loss\n        max_loss = C * np.sum(sub - foo)\n\n        # Combine together\n        total_loss = reg_loss + max_loss\n        return total_loss\n\n    def grad_student(self, W, X, y, C=1.0):\n        '''\n        Compute gradient function w.r.t. W given W, X, y.\n\n        For exact definitions, please check the MP document.\n\n        Arugments:\n            W: Weights. Numpy array of shape (K, d)\n            X: Features. Numpy array of shape (N, d)\n            y: Labels. Numpy array of shape N\n            C: Penalty constant. Will always be 1 in the MP.\n\n        Returns:\n            The graident of loss function w.r.t. W,\n            in a numpy array of shape (K, d).\n        '''\n        # Gradient of regularization\n        reg_grad = W\n\n        # Number of observations and number of classes\n        N = X.shape[0]\n        K = W.shape[0]\n\n        # Initialize Delta matrix\n        Delta = np.zeros((K, N))\n        for j in range(K):\n            for i in range(N):\n                if j == y[i]:\n                    Delta[j, i] = 1\n        I = np.ones((K, N))\n        sub = I - Delta + W.dot(X.T)\n        # # reshape\n        # rsub = np.reshape(np.amax(sub, axis=0), (N, 1))\n        # Which j is the max in max expression\n        idx = np.argmax(sub, axis=0)\n        max_grad = np.zeros_like(W)\n\n        # Compute gradient for max()\n        for num, val in enumerate(idx):\n            max_grad[val, :] += X[num, :]\n            max_grad[y[num], :] -= X[num, :]\n\n        # Combine together\n        total_grad = reg_grad + max_grad * C\n        return total_grad\n"""
assignments/assignment5/mp5/model/sklearn_multiclass.py,0,"b""from sklearn import multiclass, svm\n\n\ndef sklearn_multiclass_prediction(mode, X_train, y_train, X_test):\n    '''\n    Use Scikit Learn built-in functions multiclass.OneVsRestClassifier\n    and multiclass.OneVsOneClassifier to perform multiclass classification.\n\n    Arguments:\n        mode: one of 'ovr', 'ovo' or 'crammer'.\n        X_train, X_test: numpy ndarray of training and test features.\n        y_train: labels of training data, from 0 to 9.\n\n    Returns:\n        y_pred_train, y_pred_test: a tuple of 2 numpy ndarrays,\n                                   being your prediction of labels on\n                                   training and test data, from 0 to 9.\n    '''\n    if mode == 'ovr':\n        clf = multiclass.OneVsRestClassifier(\n            svm.LinearSVC(random_state=12345, dual=False))\n        clf.fit(X_train, y_train)\n        y_pred_train = clf.predict(X_train)\n        y_pred_test = clf.predict(X_test)\n        return y_pred_train, y_pred_test\n\n    elif mode == 'ovo':\n        clf = multiclass.OneVsOneClassifier(\n            svm.LinearSVC(random_state=12345, dual=False))\n        clf.fit(X_train, y_train)\n        y_pred_train = clf.predict(X_train)\n        y_pred_test = clf.predict(X_test)\n        return y_pred_train, y_pred_test\n\n    elif mode == 'crammer':\n        clf = svm.LinearSVC(\n            random_state=12345,\n            dual=False,\n            multi_class='crammer_singer')\n        clf.fit(X_train, y_train)\n        y_pred_train = clf.predict(X_train)\n        y_pred_test = clf.predict(X_test)\n        return y_pred_train, y_pred_test\n\n    else:\n        raise ValueError('mode must be ovr or ovo or crammer-singer')\n"""
assignments/assignment9/mp9/models/__init__.py,0,b''
assignments/assignment9/mp9/models/gaussian_mixture_model.py,0,"b'""""""Implements the Gaussian Mixture model, and trains using EM algorithm.""""""\nimport numpy as np\nimport scipy\nfrom scipy.stats import multivariate_normal\n# from scipy.cluster.vq import kmeans2\n\n\nclass GaussianMixtureModel(object):\n    """"""Gaussian Mixture Model.""""""\n\n    def __init__(self, n_dims, n_components=1,\n                 max_iter=25,\n                 reg_covar=1e-6):\n        """"""\n        Gaussian Mixture Model init funtion.\n\n        Args:\n            n_dims: The dimension of the feature.\n            n_components: Number of Gaussians in the GMM.\n            max_iter: Number of steps to run EM.\n            reg_covar: Amount to regularize the covariance matrix, (i.e. add\n                to the diagonal of covariance matrices).\n        """"""\n        self._n_dims = n_dims\n        self._n_components = n_components\n        self._max_iter = max_iter\n        self._reg_covar = reg_covar\n\n        # Randomly Initialize model parameters\n        # np.array of size (n_components, n_dims)\n        self._mu = np.random.rand(self._n_components, self._n_dims)\n\n        # Initialized with uniform distribution.\n        # np.array of size (n_components, 1)\n        tmp = np.random.dirichlet(np.ones(self._n_components), size=1)\n        self._pi = tmp.reshape(tmp.shape[1], 1)\n\n        # Initialized with identity.\n        # np.array of size (n_components, n_dims, n_dims)\n        i = np.eye(self._n_dims) * 100\n        self._sigma = np.repeat(i[np.newaxis, :, :],\n                                self._n_components, axis=0)\n\n    def fit(self, x):\n        """"""Run EM steps.\n\n        Runs EM steps for max_iter number of steps.\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n        """"""\n        # self._mu, _ = kmeans2(x, self._n_components)\n        self._mu = x[np.random.choice(\n            x.shape[0], size=self._n_components, replace=False), :]\n\n        for iters in range(self._max_iter):\n            z_ik = self._e_step(x)\n            # print(""e_step: "", iters)\n            self._m_step(x, z_ik)\n            # print(""m_step: "", iters)\n\n    def _e_step(self, x):\n        """"""E step.\n\n        Wraps around get_posterior.\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n        Returns:\n            z_ik(numpy.ndarray): Array containing the posterior probability\n                of each example, dimension (N, n_components).\n        """"""\n        return self.get_posterior(x)\n\n    def _m_step(self, x, z_ik):\n        """"""M step, update the parameters.\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n            z_ik(numpy.ndarray): Array containing the posterior probability\n                of each example, dimension (N, n_components).\n                (Alternate way of representing categorical distribution of z_i)\n        """"""\n        # Update the parameters.\n        # Update for pi (n_components, 1)\n\n        # avg = np.mean(z_ik, axis=0).tolist()\n        # norm = [i / sum(avg) for i in avg]\n        # self._pi = np.array(norm).reshape(-1, 1)\n\n        sum_ = np.sum(z_ik, axis=0)\n        self._pi = sum_ / x.shape[0]\n\n        # Update for mu (n_components, ndims)\n        # new_mu = np.zeros_like(self._mu)\n        # mu_down = np.sum(z_ik, axis=0)\n        # for k in range(self._n_components):\n        #     mu_up = np.zeros((1, self._n_dims))\n        #     for i in range(x.shape[0]):\n        #         mu_up += z_ik[i, k] * x[i, :]\n        #     new_mu[k, :] = mu_up / mu_down[k]\n\n        mu_up = z_ik.T.dot(x)\n        mu_down = np.sum(z_ik, axis=0).reshape(-1, 1)\n        self._mu = mu_up / mu_down\n\n        # Update for sigma (n_components, n_dims, n_dims)\n        new_sigma = np.zeros_like(self._sigma)\n        sigma_down = np.sum(z_ik, axis=0)\n        reg = np.zeros((self._n_dims, self._n_dims))\n        np.fill_diagonal(reg, self._reg_covar)\n\n        for k in range(self._n_components):\n            # mu_k = self._mu[k, :]\n            # sigma_k = np.zeros((self._n_dims, self._n_dims))\n            # for i in range(x.shape[0]):\n            #     sigma_k += z_ik[i, k] * np.diag(self._reg_covar +\n            #                                     np.diag(np.outer(x[i, :]\n            #                                    - mu_k, x[i, :] - mu_k)))\n            # new_sigma[k] = sigma_k / sigma_down[k]\n            x_demean = x - self._mu[k, :]\n            sigma_up = z_ik[:, k][:, np.newaxis] * x_demean\n            new_sigma[k, :, :] = x_demean.T.dot(sigma_up) / sigma_down[k] + reg\n        self._sigma = new_sigma\n\n    def get_conditional(self, x):\n        """"""Compute the conditional probability.\n\n        p(x^(i)|z_ik=1)\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n        Returns:\n            response(numpy.ndarray): The conditional probability for each example,\n                dimension (N, n_components).\n        """"""\n        # response = np.zeros((x.shape[0], self._n_components))\n        response = []\n\n        # compute conditional probability for each data example\n        for k in range(self._n_components):\n            # for i in range(x.shape[0]):\n                # response[i, k] = self._multivariate_gaussian(\n                #     x[i, :], self._mu[k, :], self._sigma[k])\n            response.append(self._multivariate_gaussian(\n                x, self._mu[k], self._sigma[k]))\n\n        response = np.transpose(np.array(response))\n        return response\n\n    def get_marginals(self, x):\n        """"""Compute the marginal probability.\n\n        p(x^(i)|pi, mu, sigma)\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n        Returns:\n            The marginal probability for each example, dimension (N,).\n        """"""\n        # get conditional probability\n        conditions = self.get_conditional(x)\n\n        # multiply conditional probability with pi_{k}\n        culmulate = conditions.dot(self._pi)\n        return culmulate.flatten()\n\n    def get_posterior(self, x):\n        """"""Compute the posterior probability.\n\n        p(z_{ik}=1|x^(i))\n\n        Args:\n            x(numpy.ndarray): Feature array of dimension (N, ndims).\n        Returns:\n            z_ik(numpy.ndarray): Array containing the posterior probability\n                of each example, dimension (N, n_components).\n        """"""\n        # get conditional probability\n        conditions = self.get_conditional(x)\n\n        # get marginal probability\n        marginals = self.get_marginals(x)\n\n        # for i in range(conditions.shape[0]):\n        #     down = marginals[i] + np.finfo(float).eps\n        #     for k in range(self._n_components):\n        #         up = conditions[i, k] * self._pi[k]\n        #         z_ik[i, k] = up / down\n\n        weighted_conditions = np.multiply(conditions, np.transpose(self._pi))\n        z_ik = np.transpose(np.transpose(\n            weighted_conditions) / (marginals + np.finfo(float).eps))\n        return z_ik\n\n    def _multivariate_gaussian(self, x, mu_k, sigma_k):\n        """"""Multivariate Gaussian, implemented for you.\n\n        Args:\n            x(numpy.ndarray): Array containing the features of dimension (N,\n                ndims)\n            mu_k(numpy.ndarray): Array containing one single mean (ndims,)\n            sigma_k(numpy.ndarray): Array containing one signle covariance\n                matrix (ndims, ndims)\n        """"""\n        return multivariate_normal.pdf(x, mu_k, sigma_k)\n\n    def supervised_fit(self, x, y):\n        """"""Assign each cluster with a label through counting.\n\n        For each cluster, find the most common digit using the provided (x, y)\n        and store it in self.cluster_label_map.\n        self.cluster_label_map should be a list of length n_components,\n        where each element maps to the most common digit in that cluster.\n        (e.g. If self.cluster_label_map[0] = 9. Then the most common digit\n        in cluster 0 is 9.\n        Args:\n            x(numpy.ndarray): Array containing the feature of dimension (N,\n                ndims).\n            y(numpy.ndarray): Array containing the label of dimension (N,)\n        """"""\n        self.cluster_label_map = np.random.rand(self._n_components).tolist()\n\n        # Perform EM in dataset\n        self.fit(x)\n\n        # Get z_{ik} after performing EM algorithm\n        z_ik = self.get_posterior(x)\n\n        # Get the highest probability of k^th GMM for each data points\n        # Assign labels for data points\n        em_label = np.argmax(z_ik, axis=1)\n\n        # Check with grountruth labels\n        for k in range(self._n_components):\n            data_idx = np.where(em_label == k)\n            if data_idx[0].size:\n                coor_data = y[data_idx]\n                vote_label = scipy.stats.mode(coor_data)[0][0]\n                self.cluster_label_map[k] = vote_label\n\n    def supervised_predict(self, x):\n        """"""Predict a label for each example in x.\n\n        Find the get the cluster assignment for each x, then use\n        self.cluster_label_map to map to the corresponding digit.\n        Args:\n            x(numpy.ndarray): Array containing the feature of dimension (N,\n                ndims).\n        Returns:\n            y_hat(numpy.ndarray): Array containing the predicted label for each\n            x, dimension (N,)\n        """"""\n        z_ik = self.get_posterior(x)\n        em_label = np.argmax(z_ik, axis=1)\n        y_hat = [self.cluster_label_map[idx] for idx in em_label]\n        return np.array(y_hat)\n'"
assignments/assignment9/mp9/utils/__init__.py,0,b''
assignments/assignment9/mp9/utils/io_tools.py,0,"b'""""""Input and output helpers to load in data.""""""\n\nimport pickle\nimport numpy as np\nfrom numpy import genfromtxt\n\n\ndef read_dataset(input_file_path):\n    """"""Read input file in csv format from file.\n\n    In this csv, each row is an example, stored in the following format.\n    label, pixel1, pixel2, pixel3...\n\n    Args:\n        input_file_path(str): Path to the csv file.\n    Returns:\n        (1) label (np.ndarray): Array of dimension (N,) containing the label.\n        (2) feature (np.ndarray): Array of dimension (N, ndims) containing the\n        images.\n    """"""\n    # Imeplemntation here.\n    data = genfromtxt(input_file_path, delimiter=\',\')\n    features = data[:, 1:]\n    labels = data[:, 0]\n    return labels, features\n'"
