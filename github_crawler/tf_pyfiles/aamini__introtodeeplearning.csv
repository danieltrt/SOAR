file_path,api_count,code
setup.py,0,"b'from pkg_resources import DistributionNotFound, get_distribution\nfrom distutils.core import setup\n\n\ndef get_dist(pkgname):\n    try:\n        return get_distribution(pkgname)\n    except DistributionNotFound:\n        return None\n\ninstall_deps = [\n    \'numpy\',\n    \'regex\',\n    \'tqdm\',\n    \'gym\'\n\n]\ntf_ver = \'2.0.0a\'\nif get_dist(\'tensorflow>=\'+tf_ver) is None and get_dist(\'tensorflow_gpu>=\'+tf_ver) is None:\n    install_deps.append(\'tensorflow>=\'+tf_ver)\n\nsetup(\n  name = \'mitdeeplearning\',         # How you named your package folder (MyLib)\n  packages = [\'mitdeeplearning\'],   # Chose the same as ""name""\n  version = \'0.1.2\',      # Start with a small number and increase it with every change you make\n  license=\'MIT\',        # Chose a license from here: https://help.github.com/articles/licensing-a-repository\n  description = \'Official software labs for MIT Introduction to Deep Learning (http://introtodeeplearning.com)\',   # Give a short description about your library\n  author = \'Alexander Amini\',                   # Type in your name\n  author_email = \'introtodeeplearning-staff@mit.edu\',      # Type in your E-Mail\n  url = \'http://introtodeeplearning.com\',   # Provide either the link to your github or to your website\n  download_url = \'https://github.com/aamini/introtodeeplearning/archive/v0.1.2.tar.gz\',    # I explain this later on\n  keywords = [\'deep learning\', \'neural networks\', \'tensorflow\', \'introduction\'],   # Keywords that define your package best\n  install_requires=install_deps,\n  classifiers=[\n    \'Development Status :: 3 - Alpha\',      # Chose either ""3 - Alpha"", ""4 - Beta"" or ""5 - Production/Stable"" as the current state of your package\n    \'License :: OSI Approved :: MIT License\',   # Again, pick a license\n    \'Programming Language :: Python :: 3\',      #Specify which pyhton versions that you want to support\',\n    \'Programming Language :: Python :: 3.6\',\n  ],\n  package_data={\n      \'mitdeeplearning\': [\'bin/*\', \'data/*\', \'data/faces/DF/*\', \'data/faces/DM/*\', \'data/faces/LF/*\', \'data/faces/LM/*\'],\n   },\n\n)\n'"
test.py,0,"b""import mitdeeplearning as mdl\n\nsongs = mdl.lab1.load_training_data()\n\nbasename = mdl.lab1.save_song_to_abc(songs[0])\nret = mdl.lab1.abc2wav(basename+'.abc')\n\nimport pdb; pdb.set_trace()\n"""
mitdeeplearning/__init__.py,0,b'import mitdeeplearning.util\n\nimport mitdeeplearning.lab1\nimport mitdeeplearning.lab2\nimport mitdeeplearning.lab3\n'
mitdeeplearning/lab1.py,1,"b'import os\nimport regex as re\nimport subprocess\nimport urllib\nimport numpy as np\nimport tensorflow as tf\n\nfrom IPython.display import Audio\n\n\ncwd = os.path.dirname(__file__)\n\ndef load_training_data():\n    with open(os.path.join(cwd, ""data"", ""irish.abc""), ""r"") as f:\n        text = f.read()\n    songs = extract_song_snippet(text)\n    return songs\n\ndef extract_song_snippet(text):\n    pattern = \'\\n\\n(.*?)\\n\\n\'\n    search_results = re.findall(pattern, text, overlapped=True, flags=re.DOTALL)\n    songs = [song for song in search_results]\n    print(""Found {} songs in text"".format(len(songs)))\n    return songs\n\ndef save_song_to_abc(song, filename=""tmp""):\n    save_name = ""{}.abc"".format(filename)\n    with open(save_name, ""w"") as f:\n        f.write(song)\n    return filename\n\ndef abc2wav(abc_file):\n    path_to_tool = os.path.join(cwd, \'bin\', \'abc2wav\')\n    cmd = ""{} {}"".format(path_to_tool, abc_file)\n    return os.system(cmd)\n\ndef play_wav(wav_file):\n    return Audio(wav_file)\n\ndef play_song(song):\n    basename = save_song_to_abc(song)\n    ret = abc2wav(basename+\'.abc\')\n    if ret == 0: #did not suceed\n        return play_wav(basename+\'.wav\')\n    return None\n\ndef play_generated_song(generated_text):\n    songs = extract_song_snippet(generated_text)\n    if len(songs) == 0:\n        print(""No valid songs found in generated text. Try training the \\\n            model longer or increasing the amount of generated music to \\\n            ensure complete songs are generated!"")\n\n    for song in songs:\n        play_song(song)\n    print(""None of the songs were valid, try training longer to improve \\\n        syntax."")\n\ndef test_batch_func_types(func, args):\n    ret = func(*args)\n    assert len(ret) == 2, ""[FAIL] get_batch must return two arguments (input and label)""\n    assert type(ret[0]) == np.ndarray, ""[FAIL] test_batch_func_types: x is not np.array""\n    assert type(ret[1]) == np.ndarray, ""[FAIL] test_batch_func_types: y is not np.array""\n    print(""[PASS] test_batch_func_types"")\n    return True\n\ndef test_batch_func_shapes(func, args):\n    dataset, seq_length, batch_size = args\n    x, y = func(*args)\n    correct = (batch_size, seq_length)\n    assert x.shape == correct, ""[FAIL] test_batch_func_shapes: x {} is not correct shape {}"".format(x.shape, correct)\n    assert y.shape == correct, ""[FAIL] test_batch_func_shapes: y {} is not correct shape {}"".format(y.shape, correct)\n    print(""[PASS] test_batch_func_shapes"")\n    return True\n\ndef test_batch_func_next_step(func, args):\n    x, y = func(*args)\n    assert (x[:,1:] == y[:,:-1]).all(), ""[FAIL] test_batch_func_next_step: x_{t} must equal y_{t-1} for all t""\n    print(""[PASS] test_batch_func_next_step"")\n    return True\n\ndef test_custom_dense_layer_output(y):\n    true_y = np.array([[0.2697859,  0.45750418, 0.66536945]],dtype=\'float32\')\n    assert tf.shape(y).numpy().tolist() == list(true_y.shape), ""[FAIL] output is of incorrect shape. expected {} but got {}"".format(true_y.shape, y.numpy().shape)\n    np.testing.assert_almost_equal(y.numpy(), true_y, decimal=7, err_msg=""[FAIL] output is of incorrect value. expected {} but got {}"".format(y.numpy(), true_y), verbose=True)\n    print(""[PASS] test_custom_dense_layer_output"")\n    return True\n'"
mitdeeplearning/lab2.py,0,"b'import cv2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport time\nimport h5py\nimport sys\nimport glob\n\nIM_SHAPE = (64, 64, 3)\n\ndef plot_image_prediction(i, predictions_array, true_label, img):\n  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(np.squeeze(img), cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = \'blue\'\n  else:\n    color = \'red\'\n\n  plt.xlabel(""{} {:2.0f}% ({})"".format(predicted_label,\n                                100*np.max(predictions_array),\n                                true_label),\n                                color=color)\n\ndef plot_value_prediction(i, predictions_array, true_label):\n  predictions_array, true_label = predictions_array[i], true_label[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=""#777777"")\n  plt.ylim([0, 1])\n  predicted_label = np.argmax(predictions_array)\n\n  thisplot[predicted_label].set_color(\'red\')\n  thisplot[true_label].set_color(\'blue\')\n\n\nclass TrainingDatasetLoader(object):\n    def __init__(self, data_path):\n\n        print (""Opening {}"".format(data_path))\n        sys.stdout.flush()\n\n        self.cache = h5py.File(data_path, \'r\')\n\n        print (""Loading data into memory..."")\n        sys.stdout.flush()\n        self.images = self.cache[\'images\'][:]\n        self.labels = self.cache[\'labels\'][:].astype(np.float32)\n        self.image_dims = self.images.shape\n        n_train_samples = self.image_dims[0]\n\n        self.train_inds = np.random.permutation(np.arange(n_train_samples))\n\n        self.pos_train_inds = self.train_inds[ self.labels[self.train_inds, 0] == 1.0 ]\n        self.neg_train_inds = self.train_inds[ self.labels[self.train_inds, 0] != 1.0 ]\n\n    def get_train_size(self):\n        return self.train_inds.shape[0]\n\n    def get_train_steps_per_epoch(self, batch_size, factor=10):\n        return self.get_train_size()//factor//batch_size\n\n    def get_batch(self, n, only_faces=False, p_pos=None, p_neg=None, return_inds=False):\n        if only_faces:\n            selected_inds = np.random.choice(self.pos_train_inds, size=n, replace=False, p=p_pos)\n        else:\n            selected_pos_inds = np.random.choice(self.pos_train_inds, size=n//2, replace=False, p=p_pos)\n            selected_neg_inds = np.random.choice(self.neg_train_inds, size=n//2, replace=False, p=p_neg)\n            selected_inds = np.concatenate((selected_pos_inds, selected_neg_inds))\n\n        sorted_inds = np.sort(selected_inds)\n        train_img = (self.images[sorted_inds,:,:,::-1]/255.).astype(np.float32)\n        train_label = self.labels[sorted_inds,...]\n        return (train_img, train_label, sorted_inds) if return_inds else (train_img, train_label)\n\n    def get_n_most_prob_faces(self, prob, n):\n        idx = np.argsort(prob)[::-1]\n        most_prob_inds = self.pos_train_inds[idx[:10*n:10]]\n        return (self.images[most_prob_inds,...]/255.).astype(np.float32)\n\n    def get_all_train_faces(self):\n        return self.images[ self.pos_train_inds ]\n\n\ndef get_test_faces():\n    cwd = os.path.dirname(__file__)\n    images = {\n        ""LF"": [],\n        ""LM"": [],\n        ""DF"": [],\n        ""DM"": []\n    }\n    for key in images.keys():\n        files = glob.glob(os.path.join(cwd, ""data"", ""faces"", key, ""*.png""))\n        for file in sorted(files):\n            image = cv2.resize(cv2.imread(file), (64,64))[:,:,::-1]/255.\n            images[key].append(image)\n\n    return images[""LF""], images[""LM""], images[""DF""], images[""DM""]\n'"
mitdeeplearning/lab3.py,0,"b'import io\nimport base64\nfrom IPython.display import HTML\nimport gym\nimport numpy as np\n\ndef play_video(filename):\n    encoded = base64.b64encode(io.open(filename, \'r+b\').read())\n    embedded = HTML(data=\'\'\'\n        <video controls>\n            <source src=""data:video/mp4;base64,{0}"" type=""video/mp4"" />\n        </video>\'\'\'.format(encoded.decode(\'ascii\')))\n\n    return embedded\n\ndef preprocess_pong(image):\n    I = image[35:195] # Crop\n    I = I[::2, ::2, 0] # Downsample width and height by a factor of 2\n    I[I == 144] = 0 # Remove background type 1\n    I[I == 109] = 0 # Remove background type 2\n    I[I != 0] = 1 # Set remaining elements (paddles, ball, etc.) to 1\n    return I.astype(np.float).reshape(80, 80, 1)\n\n\ndef save_video_of_model(model, env_name, obs_diff=False, pp_fn=None):\n    import skvideo.io\n    from pyvirtualdisplay import Display\n    display = Display(visible=0, size=(400, 300))\n    display.start()\n\n    if pp_fn is None:\n        pp_fn = lambda x: x\n\n    env = gym.make(env_name)\n    obs = env.reset()\n    obs = pp_fn(obs)\n    prev_obs = obs\n\n    filename = env_name + "".mp4""\n    output_video = skvideo.io.FFmpegWriter(filename)\n\n    counter = 0\n    done = False\n    while not done:\n        frame = env.render(mode=\'rgb_array\')\n        output_video.writeFrame(frame)\n\n        if obs_diff:\n            input_obs = obs - prev_obs\n        else:\n            input_obs = obs\n        action = model(np.expand_dims(input_obs, 0)).numpy().argmax()\n\n        prev_obs = obs\n        obs, reward, done, info = env.step(action)\n        obs = pp_fn(obs)\n        counter += 1\n\n    output_video.close()\n    print(""Successfully saved {} frames into {}!"".format(counter, filename))\n    return filename\n'"
mitdeeplearning/util.py,1,"b'import matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport time\r\nimport numpy as np\r\n\r\nfrom IPython import display as ipythondisplay\r\nfrom string import Formatter\r\n\r\n\r\n\r\n\r\n\r\ndef display_model(model):\r\n  tf.keras.utils.plot_model(model,\r\n             to_file=\'tmp.png\',\r\n             show_shapes=True)\r\n  return ipythondisplay.Image(\'tmp.png\')\r\n\r\n\r\ndef plot_sample(x,y,vae):\r\n    plt.figure(figsize=(2,1))\r\n    plt.subplot(1, 2, 1)\r\n\r\n    idx = np.where(y==1)[0][0]\r\n    plt.imshow(x[idx])\r\n    plt.grid(False)\r\n\r\n    plt.subplot(1, 2, 2)\r\n    _, _, _, recon = vae(x)\r\n    recon = np.clip(recon, 0, 1)\r\n    plt.imshow(recon[idx])\r\n    plt.grid(False)\r\n\r\n    plt.show()\r\n\r\n\r\n\r\nclass LossHistory:\r\n  def __init__(self, smoothing_factor=0.0):\r\n    self.alpha = smoothing_factor\r\n    self.loss = []\r\n  def append(self, value):\r\n    self.loss.append( self.alpha*self.loss[-1] + (1-self.alpha)*value if len(self.loss)>0 else value )\r\n  def get(self):\r\n    return self.loss\r\n\r\nclass PeriodicPlotter:\r\n  def __init__(self, sec, xlabel=\'\', ylabel=\'\', scale=None):\r\n\r\n    self.xlabel = xlabel\r\n    self.ylabel = ylabel\r\n    self.sec = sec\r\n    self.scale = scale\r\n\r\n    self.tic = time.time()\r\n\r\n  def plot(self, data):\r\n    if time.time() - self.tic > self.sec:\r\n      plt.cla()\r\n\r\n      if self.scale is None:\r\n        plt.plot(data)\r\n      elif self.scale == \'semilogx\':\r\n        plt.semilogx(data)\r\n      elif self.scale == \'semilogy\':\r\n        plt.semilogy(data)\r\n      elif self.scale == \'loglog\':\r\n        plt.loglog(data)\r\n      else:\r\n        raise ValueError(""unrecognized parameter scale {}"".format(self.scale))\r\n\r\n      plt.xlabel(self.xlabel); plt.ylabel(self.ylabel)\r\n      ipythondisplay.clear_output(wait=True)\r\n      ipythondisplay.display(plt.gcf())\r\n\r\n      self.tic = time.time()\r\n'"
