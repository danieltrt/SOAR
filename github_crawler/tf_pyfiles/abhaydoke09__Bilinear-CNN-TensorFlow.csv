file_path,api_count,code
core/bcnn_DD_woft.py,110,"b'\'\'\'\nThis file is used for the first step of the training procedure of Bilinear_CNN\nwhere only last layer of the Bilinear_CNN (DD) model is trained. \nTwo VGG16 networks are connected at the output of conv5_3 layer to form \na Bilinear_CNN (DD) network and bilinear merging is performed on connect \nthese two convolutional layers.\nNo finetuning is performed on the convolutional layers.\nOnly blinear layers are trained in this first step.\n\'\'\'\n\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport tflearn\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\nimport os\nfrom tflearn.data_utils import shuffle\n\nimport pickle \nfrom tflearn.data_utils import image_preloader\nimport h5py\nimport math\nimport logging\nimport random\nimport time\n\n\n\n\n\ndef random_flip_right_to_left(image_batch):\n    result = []\n    for n in range(image_batch.shape[0]):\n        if bool(random.getrandbits(1)):\n            result.append(image_batch[n][:,::-1,:])\n        else:\n            result.append(image_batch[n])\n    return result\n\nclass vgg16:\n    def __init__(self, imgs, weights=None, sess=None):\n        self.imgs = imgs\n        self.last_layer_parameters = []     ## Parameters in this list will be optimized when only last layer is being trained \n        self.parameters = []                ## Parameters in this list will be optimized when whole BCNN network is finetuned\n        self.convlayers()                   ## Create Convolutional layers\n        self.fc_layers()                    ## Create Fully connected layer\n        self.weight_file = weights          \n        #self.load_weights(weights, sess)\n\n\n    def convlayers(self):\n        \n        # zero-mean input\n        with tf.name_scope(\'preprocess\') as scope:\n            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n            images = self.imgs-mean\n            print(\'Adding Data Augmentation\')\n            \n\n        # conv1_1\n        with tf.name_scope(\'conv1_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv1_2\n        with tf.name_scope(\'conv1_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64],  dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool1\n        self.pool1 = tf.nn.max_pool(self.conv1_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool1\')\n\n        # conv2_1\n        with tf.name_scope(\'conv2_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv2_2\n        with tf.name_scope(\'conv2_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool2\n        self.pool2 = tf.nn.max_pool(self.conv2_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool2\')\n\n        # conv3_1\n        with tf.name_scope(\'conv3_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_2\n        with tf.name_scope(\'conv3_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_3\n        with tf.name_scope(\'conv3_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool3\n        self.pool3 = tf.nn.max_pool(self.conv3_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool3\')\n\n        # conv4_1\n        with tf.name_scope(\'conv4_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_2\n        with tf.name_scope(\'conv4_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,   name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_3\n        with tf.name_scope(\'conv4_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool4\n        self.pool4 = tf.nn.max_pool(self.conv4_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n        # conv5_1\n        with tf.name_scope(\'conv5_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_2\n        with tf.name_scope(\'conv5_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_3\n        with tf.name_scope(\'conv5_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        print(\'Shape of conv5_3\', self.conv5_3.get_shape())\n        self.phi_I = tf.einsum(\'ijkm,ijkn->imn\',self.conv5_3,self.conv5_3)\n        print(\'Shape of phi_I after einsum\', self.phi_I.get_shape())\n\n        \n        self.phi_I = tf.reshape(self.phi_I,[-1,512*512])\n        print(\'Shape of phi_I after reshape\', self.phi_I.get_shape())\n\n        self.phi_I = tf.divide(self.phi_I,784.0)  \n        print(\'Shape of phi_I after division\', self.phi_I.get_shape())\n\n        self.y_ssqrt = tf.multiply(tf.sign(self.phi_I),tf.sqrt(tf.abs(self.phi_I)+1e-12))\n        print(\'Shape of y_ssqrt\', self.y_ssqrt.get_shape())\n\n        self.z_l2 = tf.nn.l2_normalize(self.y_ssqrt, dim=1)\n        print(\'Shape of z_l2\', self.z_l2.get_shape())\n\n\n\n\n    def fc_layers(self):\n\n        with tf.name_scope(\'fc-new\') as scope:\n\n            fc3w = tf.get_variable(\'weights\', [512*512, 100], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            fc3b = tf.Variable(tf.constant(1.0, shape=[100], dtype=tf.float32), name=\'biases\', trainable=True)\n            self.fc3l = tf.nn.bias_add(tf.matmul(self.z_l2, fc3w), fc3b)\n            self.last_layer_parameters += [fc3w, fc3b]\n            self.parameters += [fc3w, fc3b]\n\n    def load_weights(self, sess):\n        weights = np.load(self.weight_file)\n        keys = sorted(weights.keys())\n        for i, k in enumerate(keys):\n            removed_layer_variables = [\'fc6_W\',\'fc6_b\',\'fc7_W\',\'fc7_b\',\'fc8_W\',\'fc8_b\']\n            if not k in removed_layer_variables:\n                print(k)\n                print("""",i, k, np.shape(weights[k]))\n                sess.run(self.parameters[i].assign(weights[k]))\n\nif __name__ == \'__main__\':\n\n    #with tf.device(\'/cpu:0\'):\n    train_data = h5py.File(\'../new_train.h5\', \'r\')\n    val_data = h5py.File(\'../new_val.h5\', \'r\')\n    \n\n    print(\'Input data read complete\')\n\n    X_train, Y_train = train_data[\'X\'], train_data[\'Y\']\n    X_val, Y_val = val_data[\'X\'], val_data[\'Y\']\n\n    print(""Data shapes -- (train, val, test)"", X_train.shape, X_val.shape)\n    X_train, Y_train = shuffle(X_train, Y_train)\n    \n    X_val, Y_val = shuffle(X_val, Y_val)\n    #print Y_train[0]\n    print(""Device placement on. Creating Session"")\n    \n    #sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n    sess = tf.Session()\n    #sess = tf.InteractiveSession()\n    #with tf.device(\'/gpu:0\'):\n    imgs = tf.placeholder(tf.float32, [None, 448, 448, 3])\n    target = tf.placeholder(""float"", [None, 100])\n    #print \'Creating graph\'\n    vgg = vgg16(imgs, \'vgg16_weights.npz\', sess)\n\n    \n    \n    #with tf.device(""/gpu:0""):\n    print(\'VGG network created\')\n    \n    \n    # Defining other ops using Tensorflow\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.fc3l, labels=target))\n    learning_rate_wft = tf.placeholder(tf.float32, shape=[])\n    learning_rate_woft = tf.placeholder(tf.float32, shape=[])\n\n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.9, momentum=0.9).minimize(loss)\n\n\n    correct_prediction = tf.equal(tf.argmax(vgg.fc3l,1), tf.argmax(target,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    num_correct_preds = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n\n    sess.run(tf.global_variables_initializer())\n\n    vgg.load_weights(sess)\n\n    batch_size = 32\n\n\n    print(\'Starting training\')\n\n    lr = 1.0\n    base_lr = 1.0\n    break_training_epoch = 15\n    for epoch in range(100):\n        avg_cost = 0.\n        total_batch = int(6000/batch_size)\n        X_train, Y_train = shuffle(X_train, Y_train)\n        \n\n        \n        # Uncomment following section if you want to break training at a particular epoch\n\n        \'\'\'\n        if epoch==break_training_epoch:\n            last_layer_weights = []\n            for v in vgg.parameters:\n                print(v)\n                if v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n                    print(\'Printing Trainable Variables :\', sess.run(v).shape)\n                    last_layer_weights.append(sess.run(v))\n            np.savez(\'last_layers_epoch_15.npz\',last_layer_weights)\n            print(""Last layer weights saved"")\n            break\n        \'\'\'\n\n        for i in range(total_batch):\n            batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_train[i*batch_size:i*batch_size+batch_size]\n            batch_xs = random_flip_right_to_left(batch_xs)\n\n            #if epoch <= finetune_step:\n            start = time.time()\n            sess.run(optimizer, feed_dict={imgs: batch_xs, target: batch_ys})\n            if i%20==0:\n                print(\'Last layer training, time to run optimizer for batch size:\', batch_size,\'is --> \',time.time()-start,\'seconds\')\n\n\n            cost = sess.run(loss, feed_dict={imgs: batch_xs, target: batch_ys})\n            if i % 100 == 0:\n                #print (\'Learning rate: \', (str(lr)))\n                if epoch <= finetune_step:\n                    print(""Training last layer of BCNN_DD"")\n\n                print(""Epoch:"", \'%03d\' % (epoch+1), ""Step:"", \'%03d\' % i,""Loss:"", str(cost))\n                print(""Training Accuracy -->"", sess.run(accuracy,feed_dict={imgs: batch_xs, target: batch_ys}))\n\n        val_batch_size = 10\n        total_val_count = len(X_val)\n        correct_val_count = 0\n        val_loss = 0.0\n        total_val_batch = int(total_val_count/val_batch_size)\n        for i in range(total_val_batch):\n            batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n            val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n            pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n            correct_val_count+=pred\n\n        print(""##############################"")\n        print(""Validation Loss -->"", val_loss)\n        print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n        print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n        print(""##############################"")\n\n        \n\n    test_data = h5py.File(\'../new_test.h5\', \'r\')\n    X_test, Y_test = test_data[\'X\'], test_data[\'Y\']\n    total_test_count = len(X_test)\n    correct_test_count = 0\n    test_batch_size = 10\n    total_test_batch = int(total_test_count/test_batch_size)\n    for i in range(total_test_batch):\n        batch_test_x, batch_test_y = X_test[i*test_batch_size:i*test_batch_size+test_batch_size], Y_test[i*test_batch_size:i*test_batch_size+test_batch_size]\n        \n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_test_x, target: batch_test_y})\n        correct_test_count+=pred\n\n    print(""##############################"")\n    print(""correct_test_count, total_test_count"", correct_test_count, total_test_count)\n    print(""Test Data Accuracy -->"", 100.0*correct_test_count/(1.0*total_test_count))\n    print(""##############################"")\n\n\n\n'"
core/bcnn_DD_woft_with_random_crops.py,113,"b'\'\'\'\nThis file is to train only the last fully connected layer of Bilineaar_CNN (DD).\nBilinear_CNN (DD) network needs images of input size [3x448x448].\nFor using the random crops, images are first resized to [3x488x488] using create_h5_dataset.py.\nDuring the training, images are randomly cropped to the size of [3x448x448] \nWeights for the last layer can be saved to a file and will be used during \nfinetuning the whole Bilinear_CNN (DD) network \n\'\'\'\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport tflearn\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\nimport os\nfrom tflearn.data_utils import shuffle\nimport pickle \nfrom tflearn.data_utils import image_preloader\nimport h5py\nimport math\nimport logging\nimport random\nimport time\n\n\ndef random_flip_right_to_left(image_batch):\n    result = []\n    #print(""Flipping images"")\n    #print(len(image_batch))\n    for n in range(len(image_batch)):\n        #print(image_batch[n].shape)\n        if bool(random.getrandbits(1)):\n            result.append(image_batch[n][:,::-1,:])\n        else:\n            result.append(image_batch[n])\n    return result\n\ndef random_crop(image_batch):\n    result = []\n    #print(""Cropping images"")\n    for n in range(image_batch.shape[0]):\n        #print(image_batch[n].shape)\n        start_x = random.randint(0,39)\n        start_y = random.randint(0,39)\n        result.append(image_batch[n][start_y:start_y+448,start_x:start_x+448,:])\n        #print(result[n].shape)\n    return result\n\n\n\nclass vgg16:\n    def __init__(self, imgs, weights=None, sess=None):\n        self.imgs = imgs\n        self.last_layer_parameters = []     ## Parameters in this list will be optimized when only last layer is being trained \n        self.parameters = []                ## Parameters in this list will be optimized when whole BCNN network is finetuned\n        self.convlayers()                   ## Create Convolutional layers\n        self.fc_layers()                    ## Create Fully connected layer\n        self.weight_file = weights          \n        #self.load_weights(weights, sess)\n\n\n    def convlayers(self):\n        \n        # zero-mean input\n        with tf.name_scope(\'preprocess\') as scope:\n            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n            images = self.imgs-mean\n            print(\'Adding Data Augmentation\')\n            #self.parameters = []\n            #self.last_layer_parameters = []\n            #images = tf.map_fn(lambda img: tf.image.random_flip_left_right(img),images)     ## Data augmentation\n\n\n        # conv1_1\n        with tf.name_scope(\'conv1_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv1_2\n        with tf.name_scope(\'conv1_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[64],  dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv1_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool1\n        self.pool1 = tf.nn.max_pool(self.conv1_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool1\')\n\n        # conv2_1\n        with tf.name_scope(\'conv2_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv2_2\n        with tf.name_scope(\'conv2_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv2_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool2\n        self.pool2 = tf.nn.max_pool(self.conv2_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool2\')\n\n        # conv3_1\n        with tf.name_scope(\'conv3_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_2\n        with tf.name_scope(\'conv3_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv3_3\n        with tf.name_scope(\'conv3_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                                   trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv3_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool3\n        self.pool3 = tf.nn.max_pool(self.conv3_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool3\')\n\n        # conv4_1\n        with tf.name_scope(\'conv4_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_2\n        with tf.name_scope(\'conv4_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,   name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv4_3\n        with tf.name_scope(\'conv4_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv4_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # pool4\n        self.pool4 = tf.nn.max_pool(self.conv4_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n        # conv5_1\n        with tf.name_scope(\'conv5_1\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1),  trainable=False, name=\'weights\')\n            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_1 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_2\n        with tf.name_scope(\'conv5_2\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_2 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        # conv5_3\n        with tf.name_scope(\'conv5_3\') as scope:\n            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                                     stddev=1e-1), trainable=False,  name=\'weights\')\n            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding=\'SAME\')\n            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                                  trainable=False, name=\'biases\')\n            out = tf.nn.bias_add(conv, biases)\n            self.conv5_3 = tf.nn.relu(out, name=scope)\n            self.parameters += [kernel, biases]\n\n        print(\'Shape of conv5_3\', self.conv5_3.get_shape())\n        self.phi_I = tf.einsum(\'ijkm,ijkn->imn\',self.conv5_3,self.conv5_3)\n        print(\'Shape of phi_I after einsum\', self.phi_I.get_shape())\n\n        \n        self.phi_I = tf.reshape(self.phi_I,[-1,512*512])\n        print(\'Shape of phi_I after reshape\', self.phi_I.get_shape())\n\n        self.phi_I = tf.divide(self.phi_I,784.0)  \n        print(\'Shape of phi_I after division\', self.phi_I.get_shape())\n\n        self.y_ssqrt = tf.multiply(tf.sign(self.phi_I),tf.sqrt(tf.abs(self.phi_I)+1e-12))\n        print(\'Shape of y_ssqrt\', self.y_ssqrt.get_shape())\n\n        self.z_l2 = tf.nn.l2_normalize(self.y_ssqrt, dim=1)\n        print(\'Shape of z_l2\', self.z_l2.get_shape())\n\n\n\n\n    def fc_layers(self):\n\n        with tf.name_scope(\'fc-new\') as scope:\n\n            fc3w = tf.get_variable(\'weights\', [512*512, 100], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            fc3b = tf.Variable(tf.constant(1.0, shape=[100], dtype=tf.float32), name=\'biases\', trainable=True)\n            self.fc3l = tf.nn.bias_add(tf.matmul(self.z_l2, fc3w), fc3b)\n            self.last_layer_parameters += [fc3w, fc3b]\n            self.parameters += [fc3w, fc3b]\n\n    def load_weights(self, sess):\n        weights = np.load(self.weight_file)\n        keys = sorted(weights.keys())\n        for i, k in enumerate(keys):\n            removed_layer_variables = [\'fc6_W\',\'fc6_b\',\'fc7_W\',\'fc7_b\',\'fc8_W\',\'fc8_b\']\n            if not k in removed_layer_variables:\n                print(k)\n                print("""",i, k, np.shape(weights[k]))\n                sess.run(self.parameters[i].assign(weights[k]))\n\nif __name__ == \'__main__\':\n\n    #with tf.device(\'/cpu:0\'):\n    train_data = h5py.File(\'../new_train_488.h5\', \'r\')\n    val_data = h5py.File(\'../new_val.h5\', \'r\')\n    \n\n    print(\'Input data read complete\')\n\n    X_train, Y_train = train_data[\'X\'], train_data[\'Y\']\n    X_val, Y_val = val_data[\'X\'], val_data[\'Y\']\n\n    print(""Data shapes -- (train, val, test)"", X_train.shape, X_val.shape)\n    X_train, Y_train = shuffle(X_train, Y_train)\n    \n    X_val, Y_val = shuffle(X_val, Y_val)\n    #print Y_train[0]\n    print(""Device placement on. Creating Session"")\n    \n    #sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n    sess = tf.Session()\n    #sess = tf.InteractiveSession()\n    #with tf.device(\'/gpu:0\'):\n    imgs = tf.placeholder(tf.float32, [None, 448, 448, 3])\n    target = tf.placeholder(""float"", [None, 100])\n    #print \'Creating graph\'\n    vgg = vgg16(imgs, \'vgg16_weights.npz\', sess)\n\n    \n    #print X_train.shape\n    \n    \n    \n    #with tf.device(""/gpu:0""):\n    print(\'VGG network created\')\n    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.probs, labels=target))\n    \n    # Defining other ops using Tensorflow\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.fc3l, labels=target))\n    learning_rate_wft = tf.placeholder(tf.float32, shape=[])\n    learning_rate_woft = tf.placeholder(tf.float32, shape=[])\n    \n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.9, momentum=0.9).minimize(loss)\n\n    correct_prediction = tf.equal(tf.argmax(vgg.fc3l,1), tf.argmax(target,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    num_correct_preds = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n\n    sess.run(tf.global_variables_initializer())\n\n    vgg.load_weights(sess)\n\n    batch_size = 32\n\n    #print ""Trainable"", tf.trainable_variables()[0]\n    print(\'Starting training\')\n\n    lr = 1.0\n    base_lr = 1.0\n    finetune_step = 50\n    for epoch in range(50):\n        avg_cost = 0.\n        total_batch = int(6000/batch_size)\n        X_train, Y_train = shuffle(X_train, Y_train)\n    \n        # Uncomment following section if you want to break training at a particular epoch\n\n        \'\'\'\n        if epoch==10:\n            last_layer_weights = []\n            for v in vgg.parameters:\n                print(v)\n                if v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n                    print(\'Printing Trainable Variables :\', sess.run(v).shape)\n                    last_layer_weights.append(sess.run(v))\n            np.savez(\'last_layers_epoch_20_crop.npz\',last_layer_weights)\n            print(""Last layer weights saved"")\n            break\n        \'\'\'\n\n        for i in range(total_batch):\n            batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_train[i*batch_size:i*batch_size+batch_size]\n            \n            batch_xs = random_crop(batch_xs)\n            batch_xs = random_flip_right_to_left(batch_xs)\n\n            #if epoch <= finetune_step:\n            start = time.time()\n            sess.run(optimizer, feed_dict={imgs: batch_xs, target: batch_ys})\n            if i%20==0:\n                print(\'Last layer training, time to run optimizer for batch size 32:\',time.time()-start,\'seconds\')\n\n            cost = sess.run(loss, feed_dict={imgs: batch_xs, target: batch_ys})\n\n            if i % 100 == 0:\n                #print (\'Learning rate: \', (str(lr)))\n                if epoch <= finetune_step:\n                    print(""Training last layer of BCNN_DD"")\n                else:\n                    print(""Fine tuning all BCNN_DD"")\n\n                print(""Epoch:"", \'%03d\' % (epoch+1), ""Step:"", \'%03d\' % i,""Loss:"", str(cost))\n                #print(""Training Accuracy -->"", accuracy.eval(feed_dict={imgs: batch_xs, target: batch_ys}, session=sess))\n                print(""Training Accuracy -->"", sess.run(accuracy,feed_dict={imgs: batch_xs, target: batch_ys}))\n\n\n        val_batch_size = 10\n        total_val_count = len(X_val)\n        correct_val_count = 0\n        val_loss = 0.0\n        total_val_batch = int(total_val_count/val_batch_size)\n        for i in range(total_val_batch):\n            batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n            val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n            pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n            correct_val_count+=pred\n\n        print(""##############################"")\n        print(""Validation Loss -->"", val_loss)\n        print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n        print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n        print(""##############################"")\n        \n\n    test_data = h5py.File(\'../new_test.h5\', \'r\')\n    X_test, Y_test = test_data[\'X\'], test_data[\'Y\']\n    total_test_count = len(X_test)\n    correct_test_count = 0\n    test_batch_size = 10\n    total_test_batch = int(total_test_count/test_batch_size)\n    for i in range(total_test_batch):\n        batch_test_x, batch_test_y = X_test[i*test_batch_size:i*test_batch_size+test_batch_size], Y_test[i*test_batch_size:i*test_batch_size+test_batch_size]\n        \n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_test_x, target: batch_test_y})\n        correct_test_count+=pred\n\n    print(""##############################"")\n    print(""correct_test_count, total_test_count"", correct_test_count, total_test_count)\n    print(""Test Data Accuracy -->"", 100.0*correct_test_count/(1.0*total_test_count))\n    print(""##############################"")\n\n\n\n'"
core/bcnn_finetuning.py,101,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\n#from scipy.misc import imread, imresize\nimport tflearn\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\nimport os\nfrom tflearn.data_utils import shuffle\n\nimport pickle \nfrom tflearn.data_utils import image_preloader\nimport h5py\nimport math\n#import logging\nimport random\nimport time\n\n\n\n\ndef random_flip_right_to_left(image_batch):\n    \'\'\'\n    This function will flip the images randomly.\n    Input: batch of images [batch, height, width, channels]\n    Output: batch of images flipped randomly [batch, height, width, channels]\n    \'\'\'\n    result = []\n    for n in range(image_batch.shape[0]):\n        if bool(random.getrandbits(1)):     ## With 0.5 probability flip the image\n            result.append(image_batch[n][:,::-1,:])\n        else:\n            result.append(image_batch[n])\n    return result\n\n\n\nclass vgg16:\n    def __init__(self, imgs, weights=None, sess=None):\n        self.imgs = imgs\n        self.last_layer_parameters = []     ## Parameters in this list will be optimized when only last layer is being trained \n        self.parameters = []                ## Parameters in this list will be optimized when whole BCNN network is finetuned\n        self.convlayers()                   ## Create Convolutional layers\n        self.fc_layers()                    ## Create Fully connected layer\n        self.weight_file = weights          \n\n\n    def convlayers(self):\n        \n        # zero-mean input\n        with tf.name_scope(\'preprocess\') as scope:\n            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n            images = self.imgs-mean\n            print(\'Adding Data Augmentation\')\n\n\n        # conv1_1\n        with tf.variable_scope(""conv1_1""):\n            weights = tf.get_variable(""W"", [3,3,3,64], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [64], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv1_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv1_2\n        with tf.variable_scope(""conv1_2""):\n            weights = tf.get_variable(""W"", [3,3,64,64], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [64], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv1_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv1_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # pool1\n        self.pool1 = tf.nn.max_pool(self.conv1_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool1\')\n\n        # conv2_1\n        with tf.variable_scope(""conv2_1""):\n            weights = tf.get_variable(""W"", [3,3,64,128], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [128], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv2_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n\n        # conv2_2\n        with tf.variable_scope(""conv2_2""):\n            weights = tf.get_variable(""W"", [3,3,128,128], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [128], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv2_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv2_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # pool2\n        self.pool2 = tf.nn.max_pool(self.conv2_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool2\')\n\n        # conv3_1\n        with tf.variable_scope(""conv3_1""):\n            weights = tf.get_variable(""W"", [3,3,128,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv3_2\n        with tf.variable_scope(""conv3_2""):\n            weights = tf.get_variable(""W"", [3,3,256,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv3_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # conv3_3\n        with tf.variable_scope(""conv3_3""):\n            weights = tf.get_variable(""W"", [3,3,256,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv3_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_3 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # pool3\n        self.pool3 = tf.nn.max_pool(self.conv3_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool3\')\n\n        # conv4_1\n        with tf.variable_scope(""conv4_1""):\n            weights = tf.get_variable(""W"", [3,3,256,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool3, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv4_2\n        with tf.variable_scope(""conv4_2""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv4_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv4_3\n        with tf.variable_scope(""conv4_3""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv4_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_3 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # pool4\n        self.pool4 = tf.nn.max_pool(self.conv4_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n        # conv5_1\n        with tf.variable_scope(""conv5_1""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool4, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv5_2\n        with tf.variable_scope(""conv5_2""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv5_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n            \n\n        # conv5_3\n        with tf.variable_scope(""conv5_3""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv5_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_3 = tf.nn.relu(conv + biases)\n\n            self.parameters += [weights, biases]\n            self.special_parameters = [weights,biases]\n\n\n        self.conv5_3 = tf.transpose(self.conv5_3, perm=[0,3,1,2])       \'\'\' Reshape conv5_3 from [batch_size, height, width, number_of_filters] \n                                                                        to [batch_size, number_of_filters, height, width]\'\'\'\n\n        self.conv5_3 = tf.reshape(self.conv5_3,[-1,512,784])            \'\'\' Reshape conv5_3 from [batch_size, number_of_filters, height*width]\n                                                                        \'\'\'\n\n        \n        \n        conv5_3_T = tf.transpose(self.conv5_3, perm=[0,2,1])            \'\'\' A temporary variable which holds the transpose of conv5_3 \n                                                                        \'\'\'\n\n        self.phi_I = tf.matmul(self.conv5_3, conv5_3_T)                 \'\'\'Matrix multiplication [batch_size,512,784] x [batch_size,784,512] \'\'\'\n\n    \n        self.phi_I = tf.reshape(self.phi_I,[-1,512*512])                \'\'\'Reshape from [batch_size,512,512] to [batch_size, 512*512] \'\'\'\n        print(\'Shape of phi_I after reshape\', self.phi_I.get_shape())\n\n        self.phi_I = tf.divide(self.phi_I,784.0)  \n        print(\'Shape of phi_I after division\', self.phi_I.get_shape())  \n\n        self.y_ssqrt = tf.multiply(tf.sign(self.phi_I),tf.sqrt(tf.abs(self.phi_I)+1e-12))       \'\'\'Take signed square root of phi_I\'\'\'\n        print(\'Shape of y_ssqrt\', self.y_ssqrt.get_shape())\n\n        self.z_l2 = tf.nn.l2_normalize(self.y_ssqrt, dim=1)     \'\'\'Apply l2 normalization\'\'\'\n        print(\'Shape of z_l2\', self.z_l2.get_shape())\n\n\n\n\n    def fc_layers(self):\n\n        with tf.variable_scope(\'fc-new\') as scope:\n            fc3w = tf.get_variable(\'W\', [512*512, 100], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            #fc3b = tf.Variable(tf.constant(1.0, shape=[100], dtype=tf.float32), name=\'biases\', trainable=True)\n            fc3b = tf.get_variable(""b"", [100], initializer=tf.constant_initializer(0.1), trainable=True)\n            self.fc3l = tf.nn.bias_add(tf.matmul(self.z_l2, fc3w), fc3b)\n            self.last_layer_parameters += [fc3w, fc3b]\n\n    def load_initial_weights(self, session):\n\n        \'\'\'weight_dict contains weigths of VGG16 layers\'\'\'\n        weights_dict = np.load(self.weight_file, encoding = \'bytes\')\n\n        \n        \'\'\'Loop over all layer names stored in the weights dict\n           Load only conv-layers. Skip fc-layers in VGG16\'\'\'\n        vgg_layers = [\'conv1_1\',\'conv1_2\',\'conv2_1\',\'conv2_2\',\'conv3_1\',\'conv3_2\',\'conv3_3\',\'conv4_1\',\'conv4_2\',\'conv4_3\',\'conv5_1\',\'conv5_2\',\'conv5_3\']\n        \n        for op_name in vgg_layers:\n            with tf.variable_scope(op_name, reuse = True):\n                \n              # Loop over list of weights/biases and assign them to their corresponding tf variable\n                # Biases\n              \n              var = tf.get_variable(\'b\', trainable = True)\n              print(\'Adding weights to\',var.name)\n              session.run(var.assign(weights_dict[op_name+\'_b\']))\n                  \n            # Weights\n              var = tf.get_variable(\'W\', trainable = True)\n              print(\'Adding weights to\',var.name)\n              session.run(var.assign(weights_dict[op_name+\'_W\']))\n\n        with tf.variable_scope(\'fc-new\', reuse = True):\n            \'\'\'\n            Load fc-layer weights trained in the first step. \n            Use file .py to train last layer\n            \'\'\'\n            last_layer_weights = np.load(\'last_layers_epoch_10.npz\')\n            print(\'Last layer weights: last_layers_epoch_10.npz\')\n            var = tf.get_variable(\'W\', trainable = True)\n            print(\'Adding weights to\',var.name)\n            session.run(var.assign(last_layer_weights[\'arr_0\'][0]))\n            var = tf.get_variable(\'b\', trainable = True)\n            print(\'Adding weights to\',var.name)\n            session.run(var.assign(last_layer_weights[\'arr_0\'][1]))\n\n\n\nif __name__ == \'__main__\':\n\n    \'\'\'\n    Load Training and Validation Data\n    \'\'\'\n    train_data = h5py.File(\'../new_train.h5\', \'r\')\n    val_data = h5py.File(\'../new_val.h5\', \'r\')\n    \n    print(\'Input data read complete\')\n\n    X_train, Y_train = train_data[\'X\'], train_data[\'Y\']\n    X_val, Y_val = val_data[\'X\'], val_data[\'Y\']\n    print(""Data shapes -- (train, val, test)"", X_train.shape, X_val.shape)\n\n    \'\'\'Shuffle the data\'\'\'\n    X_train, Y_train = shuffle(X_train, Y_train)\n    X_val, Y_val = shuffle(X_val, Y_val)\n    print(""Data shapes -- (train, val, test)"", X_train.shape, X_val.shape)\n    \n    \n    \n    sess = tf.Session()     ## Start session to create training graph\n\n    imgs = tf.placeholder(tf.float32, [None, 448, 448, 3])\n    target = tf.placeholder(""float"", [None, 100])\n\n    #print \'Creating graph\'\n    vgg = vgg16(imgs, \'vgg16_weights.npz\', sess)\n\n    \n    print(\'VGG network created\')\n    \n\n    # Defining other ops using Tensorflow\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.fc3l, labels=target))\n\n\n    print([_.name for _ in vgg.parameters])\n\n    \n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9).minimize(loss)\n    \n    check_op = tf.add_check_numerics_ops()\n\n\n    correct_prediction = tf.equal(tf.argmax(vgg.fc3l,1), tf.argmax(target,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    num_correct_preds = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n\n    sess.run(tf.global_variables_initializer())\n\n    \n    vgg.load_initial_weights(sess)\n    print([_.name for _ in vgg.parameters])\n\n    \n    batch_size = 16\n\n    for v in tf.trainable_variables():\n        print(""Trainable variables"", v)\n    print(\'Starting training\')\n\n    lr = 0.001\n    finetune_step = -1\n\n    for i in range(total_val_batch):\n        batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n        val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n        correct_val_count+=pred\n\n    print(""##############################"")\n    print(""Validation Loss -->"", val_loss)\n    print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n    print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n    print(""##############################"")\n\n    validation_accuracy_buffer = []\n    for epoch in range(100):\n        avg_cost = 0.\n        total_batch = int(6000/batch_size)\n        X_train, Y_train = shuffle(X_train, Y_train)\n\n\n        for i in range(total_batch):\n            batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_train[i*batch_size:i*batch_size+batch_size]\n            batch_xs = random_flip_right_to_left(batch_xs)\n\n            \n            start = time.time()\n            sess.run([optimizer,check_op], feed_dict={imgs: batch_xs, target: batch_ys})\n            if i%20==0:\n                print(\'Full BCNN finetuning, time to run optimizer for batch size 16:\',time.time()-start,\'seconds\')\n\n\n            cost = sess.run(loss, feed_dict={imgs: batch_xs, target: batch_ys})\n            \n            if i % 20 == 0:\n                print (\'Learning rate: \', (str(lr)))\n                if epoch <= finetune_step:\n                    print(""Training last layer of BCNN_DD"")\n                else:\n                    print(""Fine tuning all BCNN_DD"")\n\n                print(""Epoch:"", \'%03d\' % (epoch+1), ""Step:"", \'%03d\' % i,""Loss:"", str(cost))\n                print(""Training Accuracy -->"", accuracy.eval(feed_dict={imgs: batch_xs, target: batch_ys}, session=sess))\n                #print(sess.run(vgg.fc3l, feed_dict={imgs: batch_xs, target: batch_ys}))\n                \n\n        val_batch_size = 10\n        total_val_count = len(X_val)\n        correct_val_count = 0\n        val_loss = 0.0\n        total_val_batch = int(total_val_count/val_batch_size)\n        for i in range(total_val_batch):\n            batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n            val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n            pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n            correct_val_count+=pred\n\n        print(""##############################"")\n        print(""Validation Loss -->"", val_loss)\n        print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n        print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n        print(""##############################"")\n\n        if epoch>40:\n\n            validation_accuracy_buffer.append(100.0*correct_val_count/(1.0*total_val_count))\n            ## Check if the validation accuracy has stopped increasing\n            if len(validation_accuracy_buffer)>10:\n                index_of_max_val_acc = np.argmax(validation_accuracy_buffer)\n                if index_of_max_val_acc==0:\n                    break\n                else:\n                    del validation_accuracy_buffer[0]\n\n\n    test_data = h5py.File(\'../new_test.h5\', \'r\')\n    X_test, Y_test = test_data[\'X\'], test_data[\'Y\']\n    total_test_count = len(X_test)\n    correct_test_count = 0\n    test_batch_size = 10\n    total_test_batch = int(total_test_count/test_batch_size)\n    for i in range(total_test_batch):\n        batch_test_x, batch_test_y = X_test[i*test_batch_size:i*test_batch_size+test_batch_size], Y_test[i*test_batch_size:i*test_batch_size+test_batch_size]\n        \n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_test_x, target: batch_test_y})\n        correct_test_count+=pred\n\n    print(""##############################"")\n    print(""correct_test_count, total_test_count"", correct_test_count, total_test_count)\n    print(""Test Data Accuracy -->"", 100.0*correct_test_count/(1.0*total_test_count))\n    print(""##############################"")\n\n\n\n'"
core/bcnn_finetuning_with_random_crops.py,109,"b'from __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\n#from scipy.misc import imread, imresize\nimport tflearn\nfrom tflearn.data_preprocessing import ImagePreprocessing\nfrom tflearn.data_augmentation import ImageAugmentation\nimport os\nfrom tflearn.data_utils import shuffle\n\nimport pickle \nfrom tflearn.data_utils import image_preloader\nimport h5py\nimport math\n\nimport random\nimport time\n\n\n\n\n\n\n\n\ndef random_flip_right_to_left(image_batch):\n    result = []\n    #print(""Flipping images"")\n    #print(len(image_batch))\n    for n in range(len(image_batch)):\n        #print(image_batch[n].shape)\n        if bool(random.getrandbits(1)):\n            result.append(image_batch[n][:,::-1,:])\n        else:\n            result.append(image_batch[n])\n    return result\n\ndef random_crop(image_batch):\n    result = []\n    #print(""Cropping images"")\n    for n in range(image_batch.shape[0]):\n        #print(image_batch[n].shape)\n        start_x = random.randint(0,39)\n        start_y = random.randint(0,39)\n        result.append(image_batch[n][start_y:start_y+448,start_x:start_x+448,:])\n        #print(result[n].shape)\n    return result\n\n\nclass vgg16:\n    def __init__(self, imgs, weights=None, sess=None):\n        self.imgs = imgs\n        self.last_layer_parameters = []     ## Parameters in this list will be optimized when only last layer is being trained \n        self.parameters = []                ## Parameters in this list will be optimized when whole BCNN network is finetuned\n        self.convlayers()                   ## Create Convolutional layers\n        self.fc_layers()                    ## Create Fully connected layer\n        self.weight_file = weights          \n        #self.load_weights(weights, sess)\n\n\n    def convlayers(self):\n        \n        # zero-mean input\n        with tf.name_scope(\'preprocess\') as scope:\n            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n            images = self.imgs-mean\n            print(\'Adding Data Augmentation\')\n            #self.parameters = []\n            #self.last_layer_parameters = []\n            #images = tf.map_fn(lambda img: tf.image.random_flip_left_right(img),images)     ## Data augmentation\n\n\n        # conv1_1\n        with tf.variable_scope(""conv1_1""):\n            weights = tf.get_variable(""W"", [3,3,3,64], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [64], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv1_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv1_2\n        with tf.variable_scope(""conv1_2""):\n            weights = tf.get_variable(""W"", [3,3,64,64], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [64], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv1_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv1_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # pool1\n        self.pool1 = tf.nn.max_pool(self.conv1_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool1\')\n\n        # conv2_1\n        with tf.variable_scope(""conv2_1""):\n            weights = tf.get_variable(""W"", [3,3,64,128], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [128], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv2_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n\n        # conv2_2\n        with tf.variable_scope(""conv2_2""):\n            weights = tf.get_variable(""W"", [3,3,128,128], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [128], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv2_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv2_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # pool2\n        self.pool2 = tf.nn.max_pool(self.conv2_2,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool2\')\n\n        # conv3_1\n        with tf.variable_scope(""conv3_1""):\n            weights = tf.get_variable(""W"", [3,3,128,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv3_2\n        with tf.variable_scope(""conv3_2""):\n            weights = tf.get_variable(""W"", [3,3,256,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv3_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # conv3_3\n        with tf.variable_scope(""conv3_3""):\n            weights = tf.get_variable(""W"", [3,3,256,256], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [256], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv3_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv3_3 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # pool3\n        self.pool3 = tf.nn.max_pool(self.conv3_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool3\')\n\n        # conv4_1\n        with tf.variable_scope(""conv4_1""):\n            weights = tf.get_variable(""W"", [3,3,256,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool3, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv4_2\n        with tf.variable_scope(""conv4_2""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv4_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv4_3\n        with tf.variable_scope(""conv4_3""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv4_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv4_3 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n        # pool4\n        self.pool4 = tf.nn.max_pool(self.conv4_3,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding=\'SAME\',\n                               name=\'pool4\')\n\n        # conv5_1\n        with tf.variable_scope(""conv5_1""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.pool4, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_1 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n\n\n        # conv5_2\n        with tf.variable_scope(""conv5_2""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv5_1, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_2 = tf.nn.relu(conv + biases)\n            self.parameters += [weights, biases]\n            \n\n        # conv5_3\n        with tf.variable_scope(""conv5_3""):\n            weights = tf.get_variable(""W"", [3,3,512,512], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n             # Create variable named ""biases"".\n            biases = tf.get_variable(""b"", [512], initializer=tf.constant_initializer(0.1), trainable=True)\n            conv = tf.nn.conv2d(self.conv5_2, weights, strides=[1, 1, 1, 1], padding=\'SAME\')\n            self.conv5_3 = tf.nn.relu(conv + biases)\n\n            self.parameters += [weights, biases]\n            self.special_parameters = [weights,biases]\n        \'\'\'# Variables created here will be named ""conv1/weights"", ""conv1/biases"".\n            self.conv5_3 = conv_relu(self.conv5_2, [3, 3, 512, 512], [512])\n\n        print(\'Shape of conv5_3\', self.conv5_3.get_shape())\'\'\'\n\n        #self.phi_I = tf.einsum(\'ijkm,ijkn->imn\',self.conv5_3,self.conv5_3)\n        self.conv5_3 = tf.transpose(self.conv5_3, perm=[0,3,1,2])\n\n        self.conv5_3 = tf.reshape(self.conv5_3,[-1,512,784])\n\n        \n        \n        conv5_3_T = tf.transpose(self.conv5_3, perm=[0,2,1])\n\n        self.phi_I = tf.matmul(self.conv5_3, conv5_3_T)\n\n        #self.phi_I = tf.reshape(bcnn,[-1,512*512])\n        #print(\'Shape of phi_I after einsum\', self.phi_I.get_shape())\n\n        #self.phi_I = tf.tensordot(self.conv5_3, self.conv5_3, [[1,2],[1,2]])\n        self.phi_I = tf.reshape(self.phi_I,[-1,512*512])\n        print(\'Shape of phi_I after reshape\', self.phi_I.get_shape())\n\n        self.phi_I = tf.divide(self.phi_I,784.0)  \n        print(\'Shape of phi_I after division\', self.phi_I.get_shape())\n\n        self.y_ssqrt = tf.multiply(tf.sign(self.phi_I),tf.sqrt(tf.abs(self.phi_I)+1e-12))\n        print(\'Shape of y_ssqrt\', self.y_ssqrt.get_shape())\n\n        self.z_l2 = tf.nn.l2_normalize(self.y_ssqrt, dim=1)\n        print(\'Shape of z_l2\', self.z_l2.get_shape())\n\n\n\n\n    def fc_layers(self):\n\n        with tf.variable_scope(\'fc-new\') as scope:\n            fc3w = tf.get_variable(\'W\', [512*512, 100], initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n            #fc3b = tf.Variable(tf.constant(1.0, shape=[100], dtype=tf.float32), name=\'biases\', trainable=True)\n            fc3b = tf.get_variable(""b"", [100], initializer=tf.constant_initializer(0.1), trainable=True)\n            self.fc3l = tf.nn.bias_add(tf.matmul(self.z_l2, fc3w), fc3b)\n            self.last_layer_parameters += [fc3w, fc3b]\n\n\n    def load_initial_weights(self, session):\n        """"""\n        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come \n        as a dict of lists (e.g. weights[\'conv1\'] is a list) and not as dict of \n        dicts (e.g. weights[\'conv1\'] is a dict with keys \'weights\' & \'biases\') we\n        need a special load function\n        """"""\n        # Load the weights into memory\n        weights_dict = np.load(self.weight_file, encoding = \'bytes\')\n        #print(weights_dict.keys())\n        \n        # Loop over all layer names stored in the weights dict\n        vgg_layers = [\'conv1_1\',\'conv1_2\',\'conv2_1\',\'conv2_2\',\'conv3_1\',\'conv3_2\',\'conv3_3\',\'conv4_1\',\'conv4_2\',\'conv4_3\',\'conv5_1\',\'conv5_2\',\'conv5_3\']\n        for op_name in vgg_layers:\n\n            with tf.variable_scope(op_name, reuse = True):\n                \n              # Loop over list of weights/biases and assign them to their corresponding tf variable\n                # Biases\n              \n              var = tf.get_variable(\'b\', trainable = True)\n              print(\'Adding weights to\',var.name)\n              session.run(var.assign(weights_dict[op_name+\'_b\']))\n                  \n            # Weights\n              var = tf.get_variable(\'W\', trainable = True)\n              print(\'Adding weights to\',var.name)\n              session.run(var.assign(weights_dict[op_name+\'_W\']))\n\n        with tf.variable_scope(\'fc-new\', reuse = True):\n            #last_layer_weights = np.load(\'last_layers_epoch_10.npz\')\n            #print(\'Last layer weights: last_layers_epoch_10.npz\')\n            last_layer_weights = np.load(\'last_layers_epoch_20_crop.npz\')\n            print(\'Last layer weights: last_layers_epoch_20_crop.npz\')\n            \n            var = tf.get_variable(\'W\', trainable = True)\n            print(\'Adding weights to\',var.name)\n            session.run(var.assign(last_layer_weights[\'arr_0\'][0]))\n            var = tf.get_variable(\'b\', trainable = True)\n            print(\'Adding weights to\',var.name)\n            session.run(var.assign(last_layer_weights[\'arr_0\'][1]))\n\n\n\nif __name__ == \'__main__\':\n\n    #with tf.device(\'/cpu:0\'):\n    train_data = h5py.File(\'../new_train_488.h5\', \'r\')\n    val_data = h5py.File(\'../new_val.h5\', \'r\')\n    \n\n    print(\'Input data read complete\')\n\n    X_train, Y_train = train_data[\'X\'], train_data[\'Y\']\n    X_val, Y_val = val_data[\'X\'], val_data[\'Y\']\n\n    print(""Data shapes -- (train, val, test)"", X_train.shape, X_val.shape)\n    X_train, Y_train = shuffle(X_train, Y_train)\n    \n    X_val, Y_val = shuffle(X_val, Y_val)\n    #print Y_train[0]\n    print(""Device placement on. Creating Session"")\n    \n    \n    sess = tf.Session()\n    #sess = tf.InteractiveSession()\n    #with tf.device(\'/gpu:0\'):\n    imgs = tf.placeholder(tf.float32, [None, 448, 448, 3])\n    target = tf.placeholder(""float"", [None, 100])\n    #print \'Creating graph\'\n    vgg = vgg16(imgs, \'vgg16_weights.npz\', sess)\n\n    \n    \n    \n    \n    \n    #with tf.device(""/gpu:0""):\n    print(\'VGG network created\')\n    \n    \n    # Defining other ops using Tensorflow\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=vgg.fc3l, labels=target))\n    \n    print([_.name for _ in vgg.parameters])\n\n    \n    optimizer = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.9).minimize(loss)\n    \n    check_op = tf.add_check_numerics_ops()\n    \n    correct_prediction = tf.equal(tf.argmax(vgg.fc3l,1), tf.argmax(target,1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    num_correct_preds = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n\n    \n    sess.run(tf.global_variables_initializer())\n\n    #vgg.load_weights(sess)\n    vgg.load_initial_weights(sess)\n    print([_.name for _ in vgg.parameters])\n\n    \n    batch_size = 16\n\n    for v in tf.trainable_variables():\n        print(""Trainable variables"", v)\n    print(\'Starting training\')\n\n    lr = 0.001\n    finetune_step = -1\n\n    val_batch_size = 10\n    total_val_count = len(X_val)\n    correct_val_count = 0\n    val_loss = 0.0\n    total_val_batch = int(total_val_count/val_batch_size)\n    for i in range(total_val_batch):\n        batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n        val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n        correct_val_count+=pred\n\n    print(""##############################"")\n    print(""Validation Loss -->"", val_loss)\n    print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n    print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n    print(""##############################"")\n\n    validation_accuracy_buffer = []\n    for epoch in range(100):\n        avg_cost = 0.\n        total_batch = int(6000/batch_size)\n        X_train, Y_train = shuffle(X_train, Y_train)\n        \n\n        for i in range(total_batch):\n            batch_xs, batch_ys = X_train[i*batch_size:i*batch_size+batch_size], Y_train[i*batch_size:i*batch_size+batch_size]\n            batch_xs = random_crop(batch_xs)\n            batch_xs = random_flip_right_to_left(batch_xs)\n\n        \n            start = time.time()\n            sess.run([optimizer,check_op], feed_dict={imgs: batch_xs, target: batch_ys})\n            if i%20==0:\n                print(\'Full BCNN finetuning, time to run optimizer for batch size 16:\',time.time()-start,\'seconds\')\n            \n\n            cost = sess.run(loss, feed_dict={imgs: batch_xs, target: batch_ys})\n            \n            if i % 20 == 0:\n                print (\'Learning rate: \', (str(lr)))\n                if epoch <= finetune_step:\n                    print(""Training last layer of BCNN_DD"")\n                else:\n                    print(""Fine tuning all BCNN_DD"")\n\n                print(""Epoch:"", \'%03d\' % (epoch+1), ""Step:"", \'%03d\' % i,""Loss:"", str(cost))\n                print(""Training Accuracy -->"", accuracy.eval(feed_dict={imgs: batch_xs, target: batch_ys}, session=sess))\n                #print(sess.run(vgg.fc3l, feed_dict={imgs: batch_xs, target: batch_ys}))\n                \n\n\n        val_batch_size = 10\n        total_val_count = len(X_val)\n        correct_val_count = 0\n        val_loss = 0.0\n        total_val_batch = int(total_val_count/val_batch_size)\n        for i in range(total_val_batch):\n            batch_val_x, batch_val_y = X_val[i*val_batch_size:i*val_batch_size+val_batch_size], Y_val[i*val_batch_size:i*val_batch_size+val_batch_size]\n            val_loss += sess.run(loss, feed_dict={imgs: batch_val_x, target: batch_val_y})\n\n            pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_val_x, target: batch_val_y})\n            correct_val_count+=pred\n\n        print(""##############################"")\n        print(""Validation Loss -->"", val_loss)\n        print(""correct_val_count, total_val_count"", correct_val_count, total_val_count)\n        print(""Validation Data Accuracy -->"", 100.0*correct_val_count/(1.0*total_val_count))\n        print(""##############################"")\n        \n\n        if epoch>40:\n            validation_accuracy_buffer.append(100.0*correct_val_count/(1.0*total_val_count))\n\n            ## Check if the validation accuracy has stopped increasing\n            if len(validation_accuracy_buffer)>10:\n                index_of_max_val_acc = np.argmax(validation_accuracy_buffer)\n                if index_of_max_val_acc==0:\n                    break\n                else:\n                    del validation_accuracy_buffer[0]\n\n        \n\n    test_data = h5py.File(\'../new_test.h5\', \'r\')\n    X_test, Y_test = test_data[\'X\'], test_data[\'Y\']\n    total_test_count = len(X_test)\n    correct_test_count = 0\n    test_batch_size = 10\n    total_test_batch = int(total_test_count/test_batch_size)\n    for i in range(total_test_batch):\n        batch_test_x, batch_test_y = X_test[i*test_batch_size:i*test_batch_size+test_batch_size], Y_test[i*test_batch_size:i*test_batch_size+test_batch_size]\n        \n        pred = sess.run(num_correct_preds, feed_dict = {imgs: batch_test_x, target: batch_test_y})\n        correct_test_count+=pred\n\n    print(""##############################"")\n    print(""correct_test_count, total_test_count"", correct_test_count, total_test_count)\n    print(""Test Data Accuracy -->"", 100.0*correct_test_count/(1.0*total_test_count))\n    print(""##############################"")\n\n\n\n'"
utils/create_h5_dataset.py,0,"b'from tflearn.data_utils import build_hdf5_image_dataset\nimport h5py\n\nnew_train = ""/home/adoke/tf_tutorial/aircrafts_new/new_train_val/new_train.txt""\nnew_val = ""/home/adoke/tf_tutorial/aircrafts_new/new_train_val/new_val.txt""\nnew_test = ""/home/adoke/tf_tutorial/aircrafts_new/from_start/a3_variants_test.txt""\n\n# image_shape option can be set to different values to create images of different sizes\nbuild_hdf5_image_dataset(new_val, image_shape=(224, 224), mode=\'file\', output_path=\'new_val_224.h5\', categorical_labels=True, normalize=False)\nprint \'Done creating new_val.h5\'\nbuild_hdf5_image_dataset(new_test, image_shape=(224, 224), mode=\'file\', output_path=\'new_test_224.h5\', categorical_labels=True, normalize=False)\nprint \'Done creating new_test.h5\'\nbuild_hdf5_image_dataset(new_train, image_shape=(488, 488), mode=\'file\', output_path=\'new_train_488.h5\', categorical_labels=True, normalize=False)\nprint \'Done creating new_train_488.h5\'\n\n'"
utils/extract_vgg_weights2dict.py,0,"b'\'\'\'\nThis code is used to extract weights of VGG16 pretrained model.\nWeights of all layers are stored in a dictionary.\nThese weights are used during training the Bilinear CNN.\n\'\'\'\n\n\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.data_preprocessing import ImagePreprocessing\nimport os\nfrom tflearn.data_utils import shuffle\nimport numpy as np\nimport pickle \nfrom tflearn.data_utils import image_preloader\n\ndef vgg16_base(input):\n\n    x = tflearn.conv_2d(input, 64, 3, activation=\'relu\', scope=\'conv1_1\',trainable=False)\n    x = tflearn.conv_2d(x, 64, 3, activation=\'relu\', scope=\'conv1_2\')\n    x = tflearn.max_pool_2d(x, 2, strides=2, name=\'maxpool1\')\n\n    x = tflearn.conv_2d(x, 128, 3, activation=\'relu\', scope=\'conv2_1\')\n    x = tflearn.conv_2d(x, 128, 3, activation=\'relu\', scope=\'conv2_2\')\n    x = tflearn.max_pool_2d(x, 2, strides=2, name=\'maxpool2\')\n\n    x = tflearn.conv_2d(x, 256, 3, activation=\'relu\', scope=\'conv3_1\')\n    x = tflearn.conv_2d(x, 256, 3, activation=\'relu\', scope=\'conv3_2\')\n    x = tflearn.conv_2d(x, 256, 3, activation=\'relu\', scope=\'conv3_3\')\n    x = tflearn.max_pool_2d(x, 2, strides=2, name=\'maxpool3\')\n\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv4_1\')\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv4_2\')\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv4_3\')\n    x = tflearn.max_pool_2d(x, 2, strides=2, name=\'maxpool4\')\n\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv5_1\')\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv5_2\')\n    x = tflearn.conv_2d(x, 512, 3, activation=\'relu\', scope=\'conv5_3\')\n    x = tflearn.max_pool_2d(x, 2, strides=2, name=\'maxpool5\')\n\n    x = tflearn.fully_connected(x, 4096, activation=\'relu\', scope=\'fc6\')\n    x = tflearn.dropout(x, 0.5, name=\'dropout1\')\n\n    x = tflearn.fully_connected(x, 4096, activation=\'relu\', scope=\'fc7\')\n    x = tflearn.dropout(x, 0.5, name=\'dropout2\')\n\n    #x = tflearn.fully_connected(x, num_class, activation=\'softmax\', scope=\'fc8\') \n    x = tflearn.fully_connected(x, 100, activation=\'softmax\', scope=\'fc8\', restore=False)\n    return x\n\n\n\n\n\nnum_classes = 100 # num of your dataset\n\n# VGG preprocessing\nimg_prep = ImagePreprocessing()\nimg_prep.add_featurewise_zero_center(mean=[123.68, 116.779, 103.939],\n                                     per_channel=True)\n\n# VGG Network\nx = tflearn.input_data(shape=[None, 224, 224, 3], name=\'input\',\n                       data_preprocessing=img_prep)\nsoftmax = vgg16_base(x)\n\nsgd = tflearn.SGD(learning_rate=0.001, lr_decay=0.96, decay_step=500)\nregression = tflearn.regression(softmax, optimizer=sgd,\n                                loss=\'categorical_crossentropy\')\n\nmodel = tflearn.DNN(regression, checkpoint_path=\'vgg_dummy\',\n                    best_checkpoint_path=\'vgg_dummy\',max_checkpoints=3, tensorboard_verbose=2,\n                    tensorboard_dir=""./logs"")\n\n\nmodel.load(""/home/adoke/tf_tutorial/aircrafts_new/new_train_val/vgg16.tflearn"", weights_only=True)\n\n\nvgg_weights_dict = {}\nvgg_layers = [\'conv1_1\',\'conv1_2\',\'conv2_1\',\'conv2_2\',\'conv3_1\',\'conv3_2\',\'conv3_3\',\'conv4_1\',\'conv4_2\',\'conv4_3\',\'conv5_1\',\'conv5_2\',\'conv5_3\',\'fc6\',\'fc7\']\n\nfor layer_name in vgg_layers:\n  print layer_name\n  base_layer = tflearn.variables.get_layer_variables_by_name(layer_name)\n  vgg_weights_dict[layer_name] = [model.get_weights(base_layer[0]),model.get_weights(base_layer[1])]\n    \npickle.dump( vgg_weights_dict, open( ""./vgg_weights.p"", ""wb"" ) )  \n\n\n'"
