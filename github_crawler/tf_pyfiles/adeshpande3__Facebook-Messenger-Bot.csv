file_path,api_count,code
Seq2Seq.py,19,"b'import tensorflow as tf\nimport numpy as np\nimport sys\nfrom random import randint\nimport datetime\nfrom sklearn.utils import shuffle\nimport pickle\nimport os\n# Removes an annoying Tensorflow warning\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\ndef createTrainingMatrices(conversationFileName, wList, maxLen):\n    conversationDictionary = np.load(conversationFileName).item()\n    numExamples = len(conversationDictionary)\n    xTrain = np.zeros((numExamples, maxLen), dtype=\'int32\')\n    yTrain = np.zeros((numExamples, maxLen), dtype=\'int32\')\n    for index,(key,value) in enumerate(conversationDictionary.iteritems()):\n        # Will store integerized representation of strings here (initialized as padding)\n        encoderMessage = np.full((maxLen), wList.index(\'<pad>\'), dtype=\'int32\')\n        decoderMessage = np.full((maxLen), wList.index(\'<pad>\'), dtype=\'int32\')\n        # Getting all the individual words in the strings\n        keySplit = key.split()\n        valueSplit = value.split()\n        keyCount = len(keySplit)\n        valueCount = len(valueSplit)\n        # Throw out sequences that are too long or are empty\n        if (keyCount > (maxLen - 1) or valueCount > (maxLen - 1) or valueCount == 0 or keyCount == 0):\n            continue\n        # Integerize the encoder string\n        for keyIndex, word in enumerate(keySplit):\n            try:\n                encoderMessage[keyIndex] = wList.index(word)\n            except ValueError:\n                # TODO: This isnt really the right way to handle this scenario\n                encoderMessage[keyIndex] = 0\n        encoderMessage[keyIndex + 1] = wList.index(\'<EOS>\')\n        # Integerize the decoder string\n        for valueIndex, word in enumerate(valueSplit):\n            try:\n                decoderMessage[valueIndex] = wList.index(word)\n            except ValueError:\n                decoderMessage[valueIndex] = 0\n        decoderMessage[valueIndex + 1] = wList.index(\'<EOS>\')\n        xTrain[index] = encoderMessage\n        yTrain[index] = decoderMessage\n    # Remove rows with all zeros\n    yTrain = yTrain[~np.all(yTrain == 0, axis=1)]\n    xTrain = xTrain[~np.all(xTrain == 0, axis=1)]\n    numExamples = xTrain.shape[0]\n    return numExamples, xTrain, yTrain\n\ndef getTrainingBatch(localXTrain, localYTrain, localBatchSize, maxLen):\n    num = randint(0,numTrainingExamples - localBatchSize - 1)\n    arr = localXTrain[num:num + localBatchSize]\n    labels = localYTrain[num:num + localBatchSize]\n    # Reversing the order of encoder string apparently helps as per 2014 paper\n    reversedList = list(arr)\n    for index,example in enumerate(reversedList):\n        reversedList[index] = list(reversed(example))\n\n    # Lagged labels are for the training input into the decoder\n    laggedLabels = []\n    EOStokenIndex = wordList.index(\'<EOS>\')\n    padTokenIndex = wordList.index(\'<pad>\')\n    for example in labels:\n        eosFound = np.argwhere(example==EOStokenIndex)[0]\n        shiftedExample = np.roll(example,1)\n        shiftedExample[0] = EOStokenIndex\n        # The EOS token was already at the end, so no need for pad\n        if (eosFound != (maxLen - 1)):\n            shiftedExample[eosFound+1] = padTokenIndex\n        laggedLabels.append(shiftedExample)\n\n    # Need to transpose these\n    reversedList = np.asarray(reversedList).T.tolist()\n    labels = labels.T.tolist()\n    laggedLabels = np.asarray(laggedLabels).T.tolist()\n    return reversedList, labels, laggedLabels\n\ndef translateToSentences(inputs, wList, encoder=False):\n    EOStokenIndex = wList.index(\'<EOS>\')\n    padTokenIndex = wList.index(\'<pad>\')\n    numStrings = len(inputs[0])\n    numLengthOfStrings = len(inputs)\n    listOfStrings = [\'\'] * numStrings\n    for mySet in inputs:\n        for index,num in enumerate(mySet):\n            if (num != EOStokenIndex and num != padTokenIndex):\n                if (encoder):\n                    # Encodings are in reverse!\n                    listOfStrings[index] = wList[num] + "" "" + listOfStrings[index]\n                else:\n                    listOfStrings[index] = listOfStrings[index] + "" "" + wList[num]\n    listOfStrings = [string.strip() for string in listOfStrings]\n    return listOfStrings\n\ndef getTestInput(inputMessage, wList, maxLen):\n    encoderMessage = np.full((maxLen), wList.index(\'<pad>\'), dtype=\'int32\')\n    inputSplit = inputMessage.lower().split()\n    for index,word in enumerate(inputSplit):\n        try:\n            encoderMessage[index] = wList.index(word)\n        except ValueError:\n            continue\n    encoderMessage[index + 1] = wList.index(\'<EOS>\')\n    encoderMessage = encoderMessage[::-1]\n    encoderMessageList=[]\n    for num in encoderMessage:\n        encoderMessageList.append([num])\n    return encoderMessageList\n\ndef idsToSentence(ids, wList):\n    EOStokenIndex = wList.index(\'<EOS>\')\n    padTokenIndex = wList.index(\'<pad>\')\n    myStr = """"\n    listOfResponses=[]\n    for num in ids:\n        if (num[0] == EOStokenIndex or num[0] == padTokenIndex):\n            listOfResponses.append(myStr)\n            myStr = """"\n        else:\n            myStr = myStr + wList[num[0]] + "" ""\n    if myStr:\n        listOfResponses.append(myStr)\n    listOfResponses = [i for i in listOfResponses if i]\n    return listOfResponses\n\n# Hyperparamters\nbatchSize = 24\nmaxEncoderLength = 15\nmaxDecoderLength = maxEncoderLength\nlstmUnits = 112\nembeddingDim = lstmUnits\nnumLayersLSTM = 3\nnumIterations = 500000\n\n# Loading in all the data structures\nwith open(""wordList.txt"", ""rb"") as fp:\n    wordList = pickle.load(fp)\n\nvocabSize = len(wordList)\n\n# If you\'ve run the entirety of word2vec.py then these lines will load in\n# the embedding matrix.\nif (os.path.isfile(\'embeddingMatrix.npy\')):\n    wordVectors = np.load(\'embeddingMatrix.npy\')\n    wordVecDimensions = wordVectors.shape[1]\nelse:\n    question = \'Since we cant find an embedding matrix, how many dimensions do you want your word vectors to be?: \'\n    wordVecDimensions = int(input(question))\n\n# Add two entries to the word vector matrix. One to represent padding tokens,\n# and one to represent an end of sentence token\npadVector = np.zeros((1, wordVecDimensions), dtype=\'int32\')\nEOSVector = np.ones((1, wordVecDimensions), dtype=\'int32\')\nif (os.path.isfile(\'embeddingMatrix.npy\')):\n    wordVectors = np.concatenate((wordVectors,padVector), axis=0)\n    wordVectors = np.concatenate((wordVectors,EOSVector), axis=0)\n\n# Need to modify the word list as well\nwordList.append(\'<pad>\')\nwordList.append(\'<EOS>\')\nvocabSize = vocabSize + 2\n\nif (os.path.isfile(\'Seq2SeqXTrain.npy\') and os.path.isfile(\'Seq2SeqYTrain.npy\')):\n    xTrain = np.load(\'Seq2SeqXTrain.npy\')\n    yTrain = np.load(\'Seq2SeqYTrain.npy\')\n    print \'Finished loading training matrices\'\n    numTrainingExamples = xTrain.shape[0]\nelse:\n    numTrainingExamples, xTrain, yTrain = createTrainingMatrices(\'conversationDictionary.npy\', wordList, maxEncoderLength)\n    np.save(\'Seq2SeqXTrain.npy\', xTrain)\n    np.save(\'Seq2SeqYTrain.npy\', yTrain)\n    print \'Finished creating training matrices\'\n\ntf.reset_default_graph()\n\n# Create the placeholders\nencoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxEncoderLength)]\ndecoderLabels = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\ndecoderInputs = [tf.placeholder(tf.int32, shape=(None,)) for i in range(maxDecoderLength)]\nfeedPrevious = tf.placeholder(tf.bool)\n\nencoderLSTM = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits, state_is_tuple=True)\n\n#encoderLSTM = tf.nn.rnn_cell.MultiRNNCell([singleCell]*numLayersLSTM, state_is_tuple=True)\n# Architectural choice of of whether or not to include ^\n\ndecoderOutputs, decoderFinalState = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(encoderInputs, decoderInputs, encoderLSTM,\n                                                            vocabSize, vocabSize, embeddingDim, feed_previous=feedPrevious)\n\ndecoderPrediction = tf.argmax(decoderOutputs, 2)\n\nlossWeights = [tf.ones_like(l, dtype=tf.float32) for l in decoderLabels]\nloss = tf.contrib.legacy_seq2seq.sequence_loss(decoderOutputs, decoderLabels, lossWeights, vocabSize)\noptimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n\nsess = tf.Session()\nsaver = tf.train.Saver()\n# If you\'re loading in a saved model, uncomment the following line and comment out line 202\n#saver.restore(sess, tf.train.latest_checkpoint(\'models/\'))\nsess.run(tf.global_variables_initializer())\n\n# Uploading results to Tensorboard\ntf.summary.scalar(\'Loss\', loss)\nmerged = tf.summary.merge_all()\nlogdir = ""tensorboard/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"") + ""/""\nwriter = tf.summary.FileWriter(logdir, sess.graph)\n\n# Some test strings that we\'ll use as input at intervals during training\nencoderTestStrings = [""whats up"",\n\t\t\t\t\t""hi"",\n\t\t\t\t\t""hey how are you"",\n\t\t\t\t\t""what are you up to"",\n\t\t\t\t\t""that dodgers game was awesome""\n\t\t\t\t\t]\n\nzeroVector = np.zeros((1), dtype=\'int32\')\n\nfor i in range(numIterations):\n\n    encoderTrain, decoderTargetTrain, decoderInputTrain = getTrainingBatch(xTrain, yTrain, batchSize, maxEncoderLength)\n    feedDict = {encoderInputs[t]: encoderTrain[t] for t in range(maxEncoderLength)}\n    feedDict.update({decoderLabels[t]: decoderTargetTrain[t] for t in range(maxDecoderLength)})\n    feedDict.update({decoderInputs[t]: decoderInputTrain[t] for t in range(maxDecoderLength)})\n    feedDict.update({feedPrevious: False})\n\n    curLoss, _, pred = sess.run([loss, optimizer, decoderPrediction], feed_dict=feedDict)\n\n    if (i % 50 == 0):\n        print(\'Current loss:\', curLoss, \'at iteration\', i)\n        summary = sess.run(merged, feed_dict=feedDict)\n        writer.add_summary(summary, i)\n    if (i % 25 == 0 and i != 0):\n        num = randint(0,len(encoderTestStrings) - 1)\n        print encoderTestStrings[num]\n        inputVector = getTestInput(encoderTestStrings[num], wordList, maxEncoderLength);\n        feedDict = {encoderInputs[t]: inputVector[t] for t in range(maxEncoderLength)}\n        feedDict.update({decoderLabels[t]: zeroVector for t in range(maxDecoderLength)})\n        feedDict.update({decoderInputs[t]: zeroVector for t in range(maxDecoderLength)})\n        feedDict.update({feedPrevious: True})\n        ids = (sess.run(decoderPrediction, feed_dict=feedDict))\n        print idsToSentence(ids, wordList)\n\n    if (i % 10000 == 0 and i != 0):\n        savePath = saver.save(sess, ""models/pretrained_seq2seq.ckpt"", global_step=i)\n'"
Word2Vec.py,11,"b'import tensorflow as tf\nimport numpy as np\nimport re\nfrom collections import Counter\nimport sys\nimport math\nfrom random import randint\nimport pickle\nimport os\n\n# This Word2Vec implementation is largely based on this paper\n# https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n# It\'s a bit old, but Word2Vec is still SOTA and relatively simple, so I\'m going with it\n\n# Check out Tensorflow\'s documentation which is pretty good for Word2Vec\n# https://www.tensorflow.org/tutorials/word2vec\n\nwordVecDimensions = 100\nbatchSize = 128\nnumNegativeSample = 64\nwindowSize = 5\nnumIterations = 100000\n\n# This function just takes in the conversation data and makes it\n# into one huge string, and then uses a Counter to identify words\n# and the number of occurences\ndef processDataset(filename):\n    openedFile = open(filename, \'r\')\n    allLines = openedFile.readlines()\n    myStr = """"\n    for line in allLines:\n        myStr += line\n    finalDict = Counter(myStr.split())\n    return myStr, finalDict\n\ndef createTrainingMatrices(dictionary, corpus):\n    allUniqueWords = list(dictionary.keys())\n    allWords = corpus.split()\n    numTotalWords = len(allWords)\n    xTrain=[]\n    yTrain=[]\n    for i in range(numTotalWords):\n        if i % 100000 == 0:\n            print \'Finished %d/%d total words\' % (i, numTotalWords)\n        wordsAfter = allWords[i + 1:i + windowSize + 1]\n        wordsBefore = allWords[max(0, i - windowSize):i]\n        wordsAdded = wordsAfter + wordsBefore\n        for word in wordsAdded:\n            xTrain.append(allUniqueWords.index(allWords[i]))\n            yTrain.append(allUniqueWords.index(word))\n    return xTrain, yTrain\n\ndef getTrainingBatch():\n    num = randint(0,numTrainingExamples - batchSize - 1)\n    arr = xTrain[num:num + batchSize]\n    labels = yTrain[num:num + batchSize]\n    return arr, labels[:,np.newaxis]\n\ncontinueWord2Vec = True\n# Loading the data structures if they are present in the directory\nif (os.path.isfile(\'Word2VecXTrain.npy\') and os.path.isfile(\'Word2VecYTrain.npy\') and os.path.isfile(\'wordList.txt\')):\n    xTrain = np.load(\'Word2VecXTrain.npy\')\n    yTrain = np.load(\'Word2VecYTrain.npy\')\n    print \'Finished loading training matrices\'\n    with open(""wordList.txt"", ""rb"") as fp:\n        wordList = pickle.load(fp)\n    print \'Finished loading word list\'\n\nelse:\n    fullCorpus, datasetDictionary = processDataset(\'conversationData.txt\')\n    print \'Finished parsing and cleaning dataset\'\n    wordList = list(datasetDictionary.keys())\n    createOwnVectors = raw_input(\'Do you want to create your own vectors through Word2Vec (y/n)?\')\n    if (createOwnVectors == \'y\'):\n        xTrain, yTrain  = createTrainingMatrices(datasetDictionary, fullCorpus)\n        print \'Finished creating training matrices\'\n        np.save(\'Word2VecXTrain.npy\', xTrain)\n        np.save(\'Word2VecYTrain.npy\', yTrain)\n    else:\n        continueWord2Vec = False\n    with open(""wordList.txt"", ""wb"") as fp:\n        pickle.dump(wordList, fp)\n\n# If you do not want to create your own word vectors and you\'d just like to\n# have Tensorflow\'s seq2seq take care of that, then you don\'t need to run\n# anything below this line.\nif (continueWord2Vec == False):\n    sys.exit()\n\nnumTrainingExamples = len(xTrain)\nvocabSize = len(wordList)\n\nsess = tf.Session()\nembeddingMatrix = tf.Variable(tf.random_uniform([vocabSize, wordVecDimensions], -1.0, 1.0))\nnceWeights = tf.Variable(tf.truncated_normal([vocabSize, wordVecDimensions], stddev=1.0 / math.sqrt(wordVecDimensions)))\nnceBiases = tf.Variable(tf.zeros([vocabSize]))\n\ninputs = tf.placeholder(tf.int32, shape=[batchSize])\noutputs = tf.placeholder(tf.int32, shape=[batchSize, 1])\n\nembed = tf.nn.embedding_lookup(embeddingMatrix, inputs)\n\nloss = tf.reduce_mean(\n  tf.nn.nce_loss(weights=nceWeights,\n                 biases=nceBiases,\n                 labels=outputs,\n                 inputs=embed,\n                 num_sampled=numNegativeSample,\n                 num_classes=vocabSize))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n\nsess.run(tf.global_variables_initializer())\nfor i in range(numIterations):\n    trainInputs, trainLabels = getTrainingBatch()\n    _, curLoss = sess.run([optimizer, loss], feed_dict={inputs: trainInputs, outputs: trainLabels})\n    if (i % 10000 == 0):\n        print (\'Current loss is:\', curLoss)\nprint \'Saving the word embedding matrix\'\nembedMatrix = embeddingMatrix.eval(session=sess)\nnp.save(\'embeddingMatrix.npy\', embedMatrix)\n'"
createDataset.py,0,"b'import pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom datetime import datetime\n\nfbData = raw_input(\'Do you have Facebook data to parse through (y/n)?\')\ngoogleData = raw_input(\'Do you have Google Hangouts data to parse through (y/n)?\')\nlinkedInData = raw_input(\'Do you have LinkedIn data to parse through (y/n)?\')\nwhatsAppData = raw_input(\'Do you have whatsAppData to parse through (y/n)?\')\ndiscordData = raw_input(\'Do you have discordData to parse through (y/n)?\')\n\ndef getWhatsAppDataCSV(personName):\n    df = pd.read_csv(\'whatsapp_chats.csv\')\n    responseDictionary = dict()\n    receivedMessages = df[df[\'From\'] != personName]\n    sentMessages = df[df[\'From\'] == personName]\n    combined = pd.concat([sentMessages, receivedMessages])\n    otherPersonsMessage, myMessage = """",""""\n    firstMessage = True\n    for index, row in combined.iterrows():\n        if (row[\'From\'] != personName):\n            if myMessage and otherPersonsMessage:\n                otherPersonsMessage = cleanMessage(otherPersonsMessage)\n                myMessage = cleanMessage(myMessage)\n                responseDictionary[otherPersonsMessage.rstrip()] = myMessage.rstrip()\n                otherPersonsMessage, myMessage = """",""""\n            otherPersonsMessage = otherPersonsMessage + str(row[\'Content\']) + "" ""\n        else:\n            if (firstMessage):\n                firstMessage = False\n                # Don\'t include if I am the person initiating the convo\n                continue\n            myMessage = myMessage + str(row[\'Content\']) + "" ""\n    return responseDictionary\n\ndef getWhatsAppDataTXT(personName):\n    # Putting all the file names in a list\n    allFiles = []\n    # Edit these file and directory names if you have them saved somewhere else\n    for filename in os.listdir(\'WhatsAppChatLogs\'):\n        if filename.endswith("".txt""):\n            allFiles.append(\'WhatsAppChatLogs/\' + filename)\n\n    responseDictionary = dict()\n    """"""\n        The key is the other person\'s message, and the value is my response\n        Going through each file, and recording everyone\'s messages to me, and my\n        responses\n    """"""\n    for currentFile in allFiles:\n        myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n        with open(currentFile, \'r\') as openedFile:\n            allLines = openedFile.readlines()\n        for index,line in enumerate(allLines):\n            # The sender\'s name is separated by a \']\' or \'-\' and a \': \' (The whitespace is important)\n            leftDelimPattern = re.compile(r\'[\\]\\-]\')\n            # A pattern to match either `]` or `-`\n            leftDelim = leftDelimPattern.search(line)\n            leftDelim = leftDelim.start() if leftDelim else -1\n            rightColon = line.find(\': \')\n\n            # Find messages that I sent\n            if (line[leftDelim + 1:rightColon].strip() == personName):\n                if not myMessage:\n                    # Want to find the first message that I send (if I send\n                    # multiple in a row)\n                    startMessageIndex = index - 1\n                myMessage += line[rightColon + 1:].strip()\n\n            elif myMessage:\n                # Now go and see what message the other person sent by looking at\n                # previous messages\n                for counter in range(startMessageIndex, 0, -1):\n                    currentLine = allLines[counter]\n                    # Extracting the values of left and right delimiters\n                    leftDelim = leftDelimPattern.search(currentLine)\n                    leftDelim = leftDelim.start() if leftDelim else -1\n                    rightColon = line.find(\': \')\n                    if (leftDelim < 0 or rightColon < 0):\n                        # In case the message above isn\'t in the right format\n                        myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n                        break\n                    if not currentSpeaker:\n                        # The first speaker not named me\n                        currentSpeaker = currentLine[leftDelim + 1:rightColon].strip()\n                    elif (currentSpeaker != currentLine[leftDelim + 1:rightColon].strip()):\n                        # A different person started speaking, so now I know that\n                        # the first person\'s message is done\n                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n                        myMessage = cleanMessage(myMessage)\n                        responseDictionary[otherPersonsMessage] = myMessage\n                        break\n                    otherPersonsMessage = currentLine[rightColon + 1:].strip() + otherPersonsMessage\n                myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n    return responseDictionary\n\ndef getWhatsAppData():\n    personName = raw_input(\'Enter your full WhatsApp name: \')\n    if os.path.isfile(\'whatsapp_chats.csv\'):\n        return getWhatsAppDataCSV(personName)\n    else:\n        return getWhatsAppDataTXT(personName)\n\n\ndef getGoogleHangoutsData():\n    personName = raw_input(\'Enter your full Hangouts name: \')\n    # Putting all the file names in a list\n    allFiles = []\n    # Edit these file and directory names if you have them saved somewhere else\n    for filename in os.listdir(\'GoogleTextForm\'):\n        if filename.endswith("".txt""):\n            allFiles.append(\'GoogleTextForm/\' + filename)\n\n    responseDictionary = dict()\n    """"""\n        The key is the other person\'s message, and the value is my response\n        Going through each file, and recording everyone\'s messages to me, and my\n        responses\n    """"""\n    for currentFile in allFiles:\n        myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n        with open(currentFile, \'r\') as openedFile:\n            allLines = openedFile.readlines()\n        for index,lines in enumerate(allLines):\n            # The sender\'s name is separated by < and >\n            leftBracket = lines.find(\'<\')\n            rightBracket = lines.find(\'>\')\n\n            # Find messages that I sent\n            if (lines[leftBracket + 1:rightBracket] == personName):\n                if not myMessage:\n                    # Want to find the first message that I send (if I send\n                    # multiple in a row)\n                    startMessageIndex = index - 1\n                myMessage += lines[rightBracket + 1:]\n\n            elif myMessage:\n                # Now go and see what message the other person sent by looking at\n                # previous messages\n                for counter in range(startMessageIndex, 0, -1):\n                    currentLine = allLines[counter]\n                    # In case the message above isn\'t in the right format\n                    if (currentLine.find(\'<\') < 0 or currentLine.find(\'>\') < 0):\n                        myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n                        break\n                    if not currentSpeaker:\n                        # The first speaker not named me\n                        currentSpeaker = currentLine[currentLine.find(\'<\') + 1:currentLine.find(\'>\')]\n                    elif (currentSpeaker != currentLine[currentLine.find(\'<\') + 1:currentLine.find(\'>\')]):\n                        # A different person started speaking, so now I know that\n                        # the first person\'s message is done\n                        otherPersonsMessage = cleanMessage(otherPersonsMessage)\n                        myMessage = cleanMessage(myMessage)\n                        responseDictionary[otherPersonsMessage] = myMessage\n                        break\n                    otherPersonsMessage = currentLine[currentLine.find(\'>\') + 1:] + otherPersonsMessage\n                myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n    return responseDictionary\n\ndef getFacebookData():\n    personName = raw_input(\'Enter your full Facebook name: \')\n    responseDictionary = dict()\n    with open(\'fbMessages.txt\', \'r\') as fbFile:\n        allLines = fbFile.readlines()\n    myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n    for index,lines in enumerate(allLines):\n        rightBracket = lines.find(\']\') + 2\n        justMessage = lines[rightBracket:]\n        colon = justMessage.find(\':\')\n        # Find messages that I sent\n        if (justMessage[:colon] == personName):\n            if not myMessage:\n                # Want to find the first message that I send (if I send multiple\n                # in a row)\n                startMessageIndex = index - 1\n            myMessage += justMessage[colon + 2:]\n\n        elif myMessage:\n            # Now go and see what message the other person sent by looking at\n            # previous messages\n            for counter in range(startMessageIndex, 0, -1):\n                currentLine = allLines[counter]\n                rightBracket = currentLine.find(\']\') + 2\n                justMessage = currentLine[rightBracket:]\n                colon = justMessage.find(\':\')\n                if not currentSpeaker:\n                    # The first speaker not named me\n                    currentSpeaker = justMessage[:colon]\n                elif (currentSpeaker != justMessage[:colon] and otherPersonsMessage):\n                    # A different person started speaking, so now I know that the\n                    # first person\'s message is done\n                    otherPersonsMessage = cleanMessage(otherPersonsMessage)\n                    myMessage = cleanMessage(myMessage)\n                    responseDictionary[otherPersonsMessage] = myMessage\n                    break\n                otherPersonsMessage = justMessage[colon + 2:] + otherPersonsMessage\n            myMessage, otherPersonsMessage, currentSpeaker = """","""",""""\n    return responseDictionary\n\ndef getLinkedInData():\n    personName = raw_input(\'Enter your full LinkedIn name: \')\n    df = pd.read_csv(\'Inbox.csv\')\n    dateTimeConverter = lambda x: datetime.strptime(x,\'%B %d, %Y, %I:%M %p\')\n    responseDictionary = dict()\n    peopleContacted = df[\'From\'].unique().tolist()\n    for person in peopleContacted:\n        receivedMessages = df[df[\'From\'] == person]\n        sentMessages = df[df[\'To\'] == person]\n        if (len(sentMessages) == 0 or len(receivedMessages) == 0):\n            # There was no actual conversation\n            continue\n        combined = pd.concat([sentMessages, receivedMessages])\n        combined[\'Date\'] = combined[\'Date\'].apply(dateTimeConverter)\n        combined = combined.sort([\'Date\'])\n        otherPersonsMessage, myMessage = """",""""\n        firstMessage = True\n        for index, row in combined.iterrows():\n            if (row[\'From\'] != personName):\n                if myMessage and otherPersonsMessage:\n                    otherPersonsMessage = cleanMessage(otherPersonsMessage)\n                    myMessage = cleanMessage(myMessage)\n                    responseDictionary[otherPersonsMessage.rstrip()] = myMessage.rstrip()\n                    otherPersonsMessage, myMessage = """",""""\n                otherPersonsMessage = otherPersonsMessage + str(row[\'Content\']) + "" ""\n            else:\n                if (firstMessage):\n                    firstMessage = False\n                    # Don\'t include if I am the person initiating the convo\n                    continue\n                myMessage = myMessage + str(row[\'Content\']) + "" ""\n    return responseDictionary\n\ndef getDiscordData():\n    personName = raw_input(\'Enter your full Discord name: \')\n    # Putting all the file names in a list\n    allFiles = []\n    # Edit these file and directory names if you have them saved somewhere else\n    for filename in os.listdir(\'DiscordChatLogs\'):\n        if filename.endswith("".txt""):\n            allFiles.append(\'DiscordChatLogs/\' + filename)\n    responseDictionary = dict()\n    """"""\n        The key is the other person\'s message, and the value is my response\n        Going through each file, and recording everyone\'s messages to me, and my\n        responses\n    """"""\n    for currentFile in allFiles:\n        with open(currentFile, \'r\') as openedFile:\n            allLines = openedFile.readlines()\n        data = \'\'.join(allLines)\n        otherPersonsMessage, myMessage = """",""""\n        response_sets = re.findall(r\'\\[.+\\] (?!\' + re.escape(personName) + r\').+\\n(.+)\\n{2}(?:\\[.+\\] \' + re.escape(personName) + r\'\\n(.+)\\n{2})\', data)\n        for response_set in response_sets:\n            otherPersonsMessage = response_set[0]\n            myMessage = response_set[1]\n            responseDictionary[otherPersonsMessage] = cleanMessage(myMessage)\n            otherPersonsMessage, myMessage = """",""""\n    return responseDictionary\n\ndef cleanMessage(message):\n    # Remove new lines within message\n    cleanedMessage = message.replace(\'\\n\',\' \').lower()\n    # Deal with some weird tokens\n    cleanedMessage = cleanedMessage.replace(""\\xc2\\xa0"", """")\n    # Remove punctuation\n    cleanedMessage = re.sub(\'([.,!?])\',\'\', cleanedMessage)\n    # Remove multiple spaces in message\n    cleanedMessage = re.sub(\' +\',\' \', cleanedMessage)\n    return cleanedMessage\n\ncombinedDictionary = {}\nif (googleData == \'y\'):\n    print \'Getting Google Hangout Data\'\n    combinedDictionary.update(getGoogleHangoutsData())\nif (fbData == \'y\'):\n    print \'Getting Facebook Data\'\n    combinedDictionary.update(getFacebookData())\nif (linkedInData == \'y\'):\n    print \'Getting LinkedIn Data\'\n    combinedDictionary.update(getLinkedInData())\nif (whatsAppData == \'y\'):\n    print \'Getting whatsApp Data\'\n    combinedDictionary.update(getWhatsAppData())\nif (discordData == \'y\'):\n    print \'Getting Discord Data\'\n    combinedDictionary.update(getDiscordData())\nprint \'Total len of dictionary\', len(combinedDictionary)\n\nprint(\'Saving conversation data dictionary\')\nnp.save(\'conversationDictionary.npy\', combinedDictionary)\n\nconversationFile = open(\'conversationData.txt\', \'w\')\nfor key, value in combinedDictionary.items():\n    if (not key.strip() or not value.strip()):\n        # If there are empty strings\n        continue\n    conversationFile.write(key.strip() + value.strip())\n'"
