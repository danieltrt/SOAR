file_path,api_count,code
future_encoders.py,0,"b'""""""\nThis module merges two files from Scikit-Learn 0.20 to make a few encoders\navailable for users using an earlier version:\n    * sklearn/preprocessing/data.py (OneHotEncoder and CategoricalEncoder)\n    * sklearn/compose/_column_transformer.py (ColumnTransformer)\nI just copy/pasted the contents, fixed the imports and __all__, and also\ncopied the definitions of three pipeline functions whose signature changes\nin 0.20: _fit_one_transformer, _transform_one and _fit_transform_one.\nThe original authors are listed below.\n----\nThe :mod:`sklearn.compose._column_transformer` module implements utilities\nto work with heterogeneous data and to apply different transformers to\ndifferent columns.\n""""""\n# Authors: Andreas Mueller <amueller@ais.uni-bonn.de>\n#          Joris Van den Bossche <jorisvandenbossche@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport numbers\nimport warnings\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.externals import six\nfrom sklearn.utils import Bunch, check_array\nfrom sklearn.externals.joblib.parallel import delayed, Parallel\nfrom sklearn.utils.metaestimators import _BaseComposition\nfrom sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES\nfrom sklearn.pipeline import _name_estimators\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing.label import LabelEncoder\n\nfrom itertools import chain\n\n\n# weight and fit_params are not used but it allows _fit_one_transformer,\n# _transform_one and _fit_transform_one to have the same signature to\n#  factorize the code in ColumnTransformer\ndef _fit_one_transformer(transformer, X, y, weight=None, **fit_params):\n    return transformer.fit(X, y)\n\n\ndef _transform_one(transformer, X, y, weight, **fit_params):\n    res = transformer.transform(X)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res\n    return res * weight\n\n\ndef _fit_transform_one(transformer, X, y, weight, **fit_params):\n    if hasattr(transformer, \'fit_transform\'):\n        res = transformer.fit_transform(X, y, **fit_params)\n    else:\n        res = transformer.fit(X, y, **fit_params).transform(X)\n    # if we have a weight for this transformer, multiply output\n    if weight is None:\n        return res, transformer\n    return res * weight, transformer\n\n\nBOUNDS_THRESHOLD = 1e-7\n\n\nzip = six.moves.zip\nmap = six.moves.map\nrange = six.moves.range\n\n__all__ = [\n    \'OneHotEncoder\',\n    \'OrdinalEncoder\',\n    \'ColumnTransformer\',\n    \'make_column_transformer\'\n]\n\n\ndef _argmax(arr_or_spmatrix, axis=None):\n    return arr_or_spmatrix.argmax(axis=axis)\n\n\ndef _handle_zeros_in_scale(scale, copy=True):\n    \'\'\' Makes sure that whenever scale is zero, we handle it correctly.\n\n    This happens in most scalers when we have constant features.\'\'\'\n\n    # if we are fitting on 1D arrays, scale might be a scalar\n    if np.isscalar(scale):\n        if scale == .0:\n            scale = 1.\n        return scale\n    elif isinstance(scale, np.ndarray):\n        if copy:\n            # New array to avoid side-effects\n            scale = scale.copy()\n        scale[scale == 0.0] = 1.0\n        return scale\n\n\ndef _transform_selected(X, transform, selected=""all"", copy=True):\n    """"""Apply a transform function to portion of selected features\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n        Dense array or sparse matrix.\n\n    transform : callable\n        A callable transform(X) -> X_transformed\n\n    copy : boolean, optional\n        Copy X even if it could be avoided.\n\n    selected: ""all"" or array of indices or mask\n        Specify which features to apply the transform to.\n\n    Returns\n    -------\n    X : array or sparse matrix, shape=(n_samples, n_features_new)\n    """"""\n    X = check_array(X, accept_sparse=\'csc\', copy=copy, dtype=FLOAT_DTYPES)\n\n    if isinstance(selected, six.string_types) and selected == ""all"":\n        return transform(X)\n\n    if len(selected) == 0:\n        return X\n\n    n_features = X.shape[1]\n    ind = np.arange(n_features)\n    sel = np.zeros(n_features, dtype=bool)\n    sel[np.asarray(selected)] = True\n    not_sel = np.logical_not(sel)\n    n_selected = np.sum(sel)\n\n    if n_selected == 0:\n        # No features selected.\n        return X\n    elif n_selected == n_features:\n        # All features selected.\n        return transform(X)\n    else:\n        X_sel = transform(X[:, ind[sel]])\n        X_not_sel = X[:, ind[not_sel]]\n\n        if sparse.issparse(X_sel) or sparse.issparse(X_not_sel):\n            return sparse.hstack((X_sel, X_not_sel))\n        else:\n            return np.hstack((X_sel, X_not_sel))\n\n\nclass _BaseEncoder(BaseEstimator, TransformerMixin):\n    """"""\n    Base class for encoders that includes the code to categorize and\n    transform the input features.\n\n    """"""\n\n    def _fit(self, X, handle_unknown=\'error\'):\n\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, \'dtype\') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n\n        if self.categories != \'auto\':\n            for cats in self.categories:\n                if not np.all(np.sort(cats) == np.array(cats)):\n                    raise ValueError(""Unsorted categories are not yet ""\n                                     ""supported"")\n            if len(self.categories) != n_features:\n                raise ValueError(""Shape mismatch: if n_values is an array,""\n                                 "" it has to be of shape (n_features,)."")\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == \'auto\':\n                le.fit(Xi)\n            else:\n                if handle_unknown == \'error\':\n                    valid_mask = np.in1d(Xi, self.categories[i])\n                    if not np.all(valid_mask):\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (""Found unknown categories {0} in column {1}""\n                               "" during fit"".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(self.categories[i])\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n    def _transform(self, X, handle_unknown=\'error\'):\n\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, \'dtype\') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        _, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            Xi = X[:, i]\n            valid_mask = np.in1d(Xi, self.categories_[i])\n\n            if not np.all(valid_mask):\n                if handle_unknown == \'error\':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (""Found unknown categories {0} in column {1}""\n                           "" during transform"".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    Xi = Xi.copy()\n                    Xi[~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n        return X_int, X_mask\n\n\nWARNING_MSG = (\n    ""The handling of integer data will change in the future. Currently, the ""\n    ""categories are determined based on the range [0, max(values)], while ""\n    ""in the future they will be determined based on the unique values.\\n""\n    ""If you want the future behaviour, you can specify \\""categories=\'auto\'\\"".""\n)\n\n\nclass OneHotEncoder(_BaseEncoder):\n    """"""Encode categorical integer features as a one-hot numeric array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka \'one-of-K\' or \'dummy\')\n    encoding scheme. This creates a binary column for each category and\n    returns a sparse matrix or dense array.\n\n    By default, the encoder derives the categories based on the unique values\n    in each feature. Alternatively, you can also specify the `categories`\n    manually.\n    The OneHotEncoder previously assumed that the input features take on\n    values in the range [0, max(values)). This behaviour is deprecated.\n\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n\n    Note: a one-hot encoding of y labels should use a LabelBinarizer\n    instead.\n\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n    Parameters\n    ----------\n    categories : \'auto\' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n\n        - \'auto\' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories must be sorted and should not mix\n          strings and numeric values.\n\n        The used categories can be found in the ``categories_`` attribute.\n\n    sparse : boolean, default=True\n        Will return sparse matrix if set True else will return an array.\n\n    dtype : number type, default=np.float\n        Desired dtype of output.\n\n    handle_unknown : \'error\' (default) or \'ignore\'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this parameter\n        is set to \'ignore\' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros. In the inverse transform, an unknown category\n        will be denoted as None.\n\n    n_values : \'auto\', int or array of ints\n        Number of values per feature.\n\n        - \'auto\' : determine value range from training data.\n        - int : number of categorical values per feature.\n                Each feature value should be in ``range(n_values)``\n        - array : ``n_values[i]`` is the number of categorical values in\n                  ``X[:, i]``. Each feature value should be\n                  in ``range(n_values[i])``\n\n        .. deprecated:: 0.20\n            The `n_values` keyword is deprecated and will be removed in 0.22.\n            Use `categories` instead.\n\n    categorical_features : ""all"" or array of indices or mask\n        Specify what features are treated as categorical.\n\n        - \'all\' (default): All features are treated as categorical.\n        - array of indices: Array of categorical feature indices.\n        - mask: Array of length n_features and with dtype=bool.\n\n        Non-categorical features are always stacked to the right of the matrix.\n\n        .. deprecated:: 0.20\n            The `categorical_features` keyword is deprecated and will be\n            removed in 0.22.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order corresponding with output of ``transform``).\n\n    active_features_ : array\n        Indices for active features, meaning values that actually occur\n        in the training set. Only available when n_values is ``\'auto\'``.\n\n        .. deprecated:: 0.20\n\n    feature_indices_ : array of shape (n_features,)\n        Indices to feature ranges.\n        Feature ``i`` in the original data is mapped to features\n        from ``feature_indices_[i]`` to ``feature_indices_[i+1]``\n        (and then potentially masked by `active_features_` afterwards)\n\n        .. deprecated:: 0.20\n\n    n_values_ : array of shape (n_features,)\n        Maximum number of values per feature.\n\n        .. deprecated:: 0.20\n\n    Examples\n    --------\n    Given a dataset with two features, we let the encoder find the unique\n    values per feature and transform the data to a binary one-hot encoding.\n\n    >>> from sklearn.preprocessing import OneHotEncoder\n    >>> enc = OneHotEncoder(handle_unknown=\'ignore\')\n    >>> X = [[\'Male\', 1], [\'Female\', 3], [\'Female\', 2]]\n    >>> enc.fit(X)\n    ... # doctest: +ELLIPSIS\n    OneHotEncoder(categories=\'auto\', dtype=<... \'numpy.float64\'>,\n           handle_unknown=\'ignore\', sparse=True)\n\n    >>> enc.categories_\n    [array([\'Female\', \'Male\'], dtype=object), array([1, 2, 3], dtype=object)]\n    >>> enc.transform([[\'Female\', 1], [\'Male\', 4]]).toarray()\n    array([[ 1.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  0.,  0.,  0.]])\n    >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n    array([[\'Male\', 1],\n           [None, 2]], dtype=object)\n\n    See also\n    --------\n    sklearn.preprocessing.OrdinalEncoder : performs an ordinal (integer)\n      encoding of the categorical features.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all\n      fashion.\n    sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of\n      iterables and a multilabel format, e.g. a (samples x classes) binary\n      matrix indicating the presence of a class label.\n    """"""\n\n    def __init__(self, n_values=None, categorical_features=None,\n                 categories=None, sparse=True, dtype=np.float64,\n                 handle_unknown=\'error\'):\n        self._categories = categories\n        if categories is None:\n            self.categories = \'auto\'\n        else:\n            self.categories = categories\n        self.sparse = sparse\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n        if n_values is not None:\n            pass\n            # warnings.warn(""Deprecated"", DeprecationWarning)\n        else:\n            n_values = ""auto""\n        self._deprecated_n_values = n_values\n\n        if categorical_features is not None:\n            pass\n            # warnings.warn(""Deprecated"", DeprecationWarning)\n        else:\n            categorical_features = ""all""\n        self._deprecated_categorical_features = categorical_features\n\n    # Deprecated keywords\n\n    @property\n    def n_values(self):\n        warnings.warn(""The \'n_values\' parameter is deprecated."",\n                      DeprecationWarning)\n        return self._deprecated_n_values\n\n    @n_values.setter\n    def n_values(self, value):\n        warnings.warn(""The \'n_values\' parameter is deprecated."",\n                      DeprecationWarning)\n        self._deprecated_n_values = value\n\n    @property\n    def categorical_features(self):\n        warnings.warn(""The \'categorical_features\' parameter is deprecated."",\n                      DeprecationWarning)\n        return self._deprecated_categorical_features\n\n    @categorical_features.setter\n    def categorical_features(self, value):\n        warnings.warn(""The \'categorical_features\' parameter is deprecated."",\n                      DeprecationWarning)\n        self._deprecated_categorical_features = value\n\n    # Deprecated attributes\n\n    @property\n    def active_features_(self):\n        check_is_fitted(self, \'categories_\')\n        warnings.warn(""The \'active_features_\' attribute is deprecated."",\n                      DeprecationWarning)\n        return self._active_features_\n\n    @property\n    def feature_indices_(self):\n        check_is_fitted(self, \'categories_\')\n        warnings.warn(""The \'feature_indices_\' attribute is deprecated."",\n                      DeprecationWarning)\n        return self._feature_indices_\n\n    @property\n    def n_values_(self):\n        check_is_fitted(self, \'categories_\')\n        warnings.warn(""The \'n_values_\' attribute is deprecated."",\n                      DeprecationWarning)\n        return self._n_values_\n\n    def _handle_deprecations(self, X):\n\n        user_set_categories = False\n\n        if self._categories is not None:\n            self._legacy_mode = False\n            user_set_categories = True\n\n        elif self._deprecated_n_values != \'auto\':\n            msg = (\n                ""Passing \'n_values\' is deprecated and will be removed in a ""\n                ""future release. You can use the \'categories\' keyword instead.""\n                "" \'n_values=n\' corresponds to \'n_values=[range(n)]\'."")\n            warnings.warn(msg, DeprecationWarning)\n\n            # we internally translate this to the correct categories\n            # and don\'t use legacy mode\n            X = check_array(X, dtype=np.int)\n\n            if isinstance(self._deprecated_n_values, numbers.Integral):\n                n_features = X.shape[1]\n                self.categories = [\n                    list(range(self._deprecated_n_values))\n                    for _ in range(n_features)]\n                n_values = np.empty(n_features, dtype=np.int)\n                n_values.fill(self._deprecated_n_values)\n            else:\n                try:\n                    n_values = np.asarray(self._deprecated_n_values, dtype=int)\n                    self.categories = [list(range(i))\n                                       for i in self._deprecated_n_values]\n                except (ValueError, TypeError):\n                    raise TypeError(\n                        ""Wrong type for parameter `n_values`. Expected \'auto\',""\n                        "" int or array of ints, got %r"".format(type(X)))\n\n            self._n_values_ = n_values\n            n_values = np.hstack([[0], n_values])\n            indices = np.cumsum(n_values)\n            self._feature_indices_ = indices\n\n            self._legacy_mode = False\n\n        else:  # n_values = \'auto\'\n            if self.handle_unknown == \'ignore\':\n                # no change in behaviour, no need to raise deprecation warning\n                self._legacy_mode = False\n            else:\n\n                # check if we have integer or categorical input\n                try:\n                    X = check_array(X, dtype=np.int)\n                except ValueError:\n                    self._legacy_mode = False\n                else:\n                    warnings.warn(WARNING_MSG, DeprecationWarning)\n                    self._legacy_mode = True\n\n        if (not isinstance(self._deprecated_categorical_features,\n                           six.string_types)\n                or (isinstance(self._deprecated_categorical_features,\n                               six.string_types)\n                    and self._deprecated_categorical_features != \'all\')):\n            if user_set_categories:\n                raise ValueError(\n                    ""The \'categorical_features\' keyword is deprecated, and ""\n                    ""cannot be used together with specifying \'categories\'."")\n            warnings.warn(""The \'categorical_features\' keyword is deprecated."",\n                          DeprecationWarning)\n            self._legacy_mode = True\n\n    def fit(self, X, y=None):\n        """"""Fit OneHotEncoder to X.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n\n        Returns\n        -------\n        self\n        """"""\n        if self.handle_unknown not in [\'error\', \'ignore\']:\n            template = (""handle_unknown should be either \'error\' or ""\n                        ""\'ignore\', got %s"")\n            raise ValueError(template % self.handle_unknown)\n\n        self._handle_deprecations(X)\n\n        if self._legacy_mode:\n            # TODO not with _transform_selected ??\n            self._legacy_fit_transform(X)\n            return self\n        else:\n            self._fit(X, handle_unknown=self.handle_unknown)\n            return self\n\n    def _legacy_fit_transform(self, X):\n        """"""Assumes X contains only categorical features.""""""\n        self_n_values = self._deprecated_n_values\n        dtype = getattr(X, \'dtype\', None)\n        X = check_array(X, dtype=np.int)\n        if np.any(X < 0):\n            raise ValueError(""X needs to contain only non-negative integers."")\n        n_samples, n_features = X.shape\n        if (isinstance(self_n_values, six.string_types) and\n                self_n_values == \'auto\'):\n            n_values = np.max(X, axis=0) + 1\n        elif isinstance(self_n_values, numbers.Integral):\n            if (np.max(X, axis=0) >= self_n_values).any():\n                raise ValueError(""Feature out of bounds for n_values=%d""\n                                 % self_n_values)\n            n_values = np.empty(n_features, dtype=np.int)\n            n_values.fill(self_n_values)\n        else:\n            try:\n                n_values = np.asarray(self_n_values, dtype=int)\n            except (ValueError, TypeError):\n                raise TypeError(""Wrong type for parameter `n_values`. Expected""\n                                "" \'auto\', int or array of ints, got %r""\n                                % type(X))\n            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:\n                raise ValueError(""Shape mismatch: if n_values is an array,""\n                                 "" it has to be of shape (n_features,)."")\n\n        self._n_values_ = n_values\n        self.categories_ = [np.arange(n_val - 1, dtype=dtype)\n                            for n_val in n_values]\n        n_values = np.hstack([[0], n_values])\n        indices = np.cumsum(n_values)\n        self._feature_indices_ = indices\n\n        column_indices = (X + indices[:-1]).ravel()\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)\n        data = np.ones(n_samples * n_features)\n        out = sparse.coo_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n\n        if (isinstance(self_n_values, six.string_types) and\n                self_n_values == \'auto\'):\n            mask = np.array(out.sum(axis=0)).ravel() != 0\n            active_features = np.where(mask)[0]\n            out = out[:, active_features]\n            self._active_features_ = active_features\n\n            self.categories_ = [\n                np.unique(X[:, i]).astype(dtype) if dtype else np.unique(X[:, i])\n                for i in range(n_features)]\n            #import pdb; pdb.set_trace()\n\n        return out if self.sparse else out.toarray()\n\n    def fit_transform(self, X, y=None):\n        """"""Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to self.fit(X).transform(X), but more convenient and more\n        efficient. See fit for the parameters, transform for the return value.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            Input array of type int.\n        """"""\n        if self.handle_unknown not in [\'error\', \'ignore\']:\n            template = (""handle_unknown should be either \'error\' or ""\n                        ""\'ignore\', got %s"")\n            raise ValueError(template % self.handle_unknown)\n\n        self._handle_deprecations(X)\n\n        if self._legacy_mode:\n            return _transform_selected(X, self._legacy_fit_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)\n        else:\n            return self.fit(X).transform(X)\n\n    def _legacy_transform(self, X):\n        """"""Assumes X contains only categorical features.""""""\n        self_n_values = self._deprecated_n_values\n        X = check_array(X, dtype=np.int)\n        if np.any(X < 0):\n            raise ValueError(""X needs to contain only non-negative integers."")\n        n_samples, n_features = X.shape\n\n        indices = self._feature_indices_\n        if n_features != indices.shape[0] - 1:\n            raise ValueError(""X has different shape than during fitting.""\n                             "" Expected %d, got %d.""\n                             % (indices.shape[0] - 1, n_features))\n\n        # We use only those categorical features of X that are known using fit.\n        # i.e lesser than n_values_ using mask.\n        # This means, if self.handle_unknown is ""ignore"", the row_indices and\n        # col_indices corresponding to the unknown categorical feature are\n        # ignored.\n        mask = (X < self._n_values_).ravel()\n        if np.any(~mask):\n            if self.handle_unknown not in [\'error\', \'ignore\']:\n                raise ValueError(""handle_unknown should be either error or ""\n                                 ""unknown got %s"" % self.handle_unknown)\n            if self.handle_unknown == \'error\':\n                raise ValueError(""unknown categorical feature present %s ""\n                                 ""during transform."" % X.ravel()[~mask])\n\n        column_indices = (X + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(np.sum(mask))\n        out = sparse.coo_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if (isinstance(self_n_values, six.string_types) and\n                self_n_values == \'auto\'):\n            out = out[:, self._active_features_]\n\n        return out if self.sparse else out.toarray()\n\n    def _transform_new(self, X):\n        """"""New implementation assuming categorical input""""""\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, \'dtype\') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n\n        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        feature_indices = np.cumsum(n_values)\n\n        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n        indptr = X_mask.sum(axis=1).cumsum()\n        indptr = np.insert(indptr, 0, 0)\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, feature_indices[-1]),\n                                dtype=self.dtype)\n        if not self.sparse:\n            return out.toarray()\n        else:\n            return out\n\n    def transform(self, X):\n        """"""Transform X using one-hot encoding.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        """"""\n        if not self._legacy_mode:\n            return self._transform_new(X)\n        else:\n            return _transform_selected(X, self._legacy_transform,\n                                       self._deprecated_categorical_features,\n                                       copy=True)\n\n    def inverse_transform(self, X):\n        """"""Convert back the data to the original representation.\n\n        In case unknown categories are encountered (all zero\'s in the\n        one-hot encoding), ``None`` is used to represent this category.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n            The transformed data.\n\n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Inverse transformed array.\n\n        """"""\n        # if self._legacy_mode:\n        #     raise ValueError(""only supported for categorical features"")\n\n        check_is_fitted(self, \'categories_\')\n        X = check_array(X, accept_sparse=\'csr\')\n\n        n_samples, _ = X.shape\n        n_features = len(self.categories_)\n        n_transformed_features = sum([len(cats) for cats in self.categories_])\n\n        # validate shape of passed X\n        msg = (""Shape of the passed X data is not correct. Expected {0} ""\n               ""columns, got {1}."")\n        if X.shape[1] != n_transformed_features:\n            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n\n        # create resulting array of appropriate dtype\n        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n        X_tr = np.empty((n_samples, n_features), dtype=dt)\n\n        j = 0\n        found_unknown = {}\n\n        for i in range(n_features):\n            n_categories = len(self.categories_[i])\n            sub = X[:, j:j + n_categories]\n\n            # for sparse X argmax returns 2D matrix, ensure 1D array\n            labels = np.asarray(_argmax(sub, axis=1)).flatten()\n            X_tr[:, i] = self.categories_[i][labels]\n\n            if self.handle_unknown == \'ignore\':\n                # ignored unknown categories: we have a row of all zero\'s\n                unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n                if unknown.any():\n                    found_unknown[i] = unknown\n\n            j += n_categories\n\n        # if ignored are found: potentially need to upcast result to\n        # insert None values\n        if found_unknown:\n            if X_tr.dtype != object:\n                X_tr = X_tr.astype(object)\n\n            for idx, mask in found_unknown.items():\n                X_tr[mask, idx] = None\n\n        return X_tr\n\n\nclass OrdinalEncoder(_BaseEncoder):\n    """"""Encode categorical features as an integer array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are converted to ordinal integers. This results in\n   a single column of integers (0 to n_categories - 1) per feature.\n\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n    Parameters\n    ----------\n    categories : \'auto\' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n\n        - \'auto\' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories must be sorted and should not mix\n          strings and numeric values.\n\n        The used categories can be found in the ``categories_`` attribute.\n\n    dtype : number type, default np.float64\n        Desired dtype of output.\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order corresponding with output of ``transform``).\n\n    Examples\n    --------\n    Given a dataset with two features, we let the encoder find the unique\n    values per feature and transform the data to a binary one-hot encoding.\n\n    >>> from sklearn.preprocessing import OrdinalEncoder\n    >>> enc = OrdinalEncoder()\n    >>> X = [[\'Male\', 1], [\'Female\', 3], [\'Female\', 2]]\n    >>> enc.fit(X)\n    ... # doctest: +ELLIPSIS\n    OrdinalEncoder(categories=\'auto\', dtype=<... \'numpy.float64\'>)\n    >>> enc.categories_\n    [array([\'Female\', \'Male\'], dtype=object), array([1, 2, 3], dtype=object)]\n    >>> enc.transform([[\'Female\', 3], [\'Male\', 1]])\n    array([[ 0.,  2.],\n           [ 1.,  0.]])\n\n    >>> enc.inverse_transform([[1, 0], [0, 1]])\n    array([[\'Male\', 1],\n           [\'Female\', 2]], dtype=object)\n\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      categorical features.\n    sklearn.preprocessing.LabelEncoder : encodes target labels with values\n      between 0 and n_classes-1.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    """"""\n\n    def __init__(self, categories=\'auto\', dtype=np.float64):\n        self.categories = categories\n        self.dtype = dtype\n\n    def fit(self, X, y=None):\n        """"""Fit the OrdinalEncoder to X.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n\n        Returns\n        -------\n        self\n\n        """"""\n        self._fit(X)\n\n        return self\n\n    def transform(self, X):\n        """"""Transform X to ordinal codes.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n\n        """"""\n        X_int, _ = self._transform(X)\n        return X_int.astype(self.dtype, copy=False)\n\n    def inverse_transform(self, X):\n        """"""Convert back the data to the original representation.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n            The transformed data.\n\n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Inverse transformed array.\n\n        """"""\n        check_is_fitted(self, \'categories_\')\n        X = check_array(X, accept_sparse=\'csr\')\n\n        n_samples, _ = X.shape\n        n_features = len(self.categories_)\n\n        # validate shape of passed X\n        msg = (""Shape of the passed X data is not correct. Expected {0} ""\n               ""columns, got {1}."")\n        if X.shape[1] != n_features:\n            raise ValueError(msg.format(n_features, X.shape[1]))\n\n        # create resulting array of appropriate dtype\n        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n        X_tr = np.empty((n_samples, n_features), dtype=dt)\n\n        for i in range(n_features):\n            labels = X[:, i].astype(\'int64\')\n            X_tr[:, i] = self.categories_[i][labels]\n\n        return X_tr\n\n\n_ERR_MSG_1DCOLUMN = (""1D data passed to a transformer that expects 2D data. ""\n                     ""Try to specify the column selection as a list of one ""\n                     ""item instead of a scalar."")\n\n\nclass ColumnTransformer(_BaseComposition, TransformerMixin):\n    """"""Applies transformers to columns of an array or pandas DataFrame.\n\n    EXPERIMENTAL: some behaviors may change between releases without\n    deprecation.\n\n    This estimator allows different columns or column subsets of the input\n    to be transformed separately and the results combined into a single\n    feature space.\n    This is useful for heterogeneous or columnar data, to combine several\n    feature extraction mechanisms or transformations into a single transformer.\n\n    Read more in the :ref:`User Guide <column_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    transformers : list of tuples\n        List of (name, transformer, column(s)) tuples specifying the\n        transformer objects to be applied to subsets of the data.\n\n        name : string\n            Like in Pipeline and FeatureUnion, this allows the transformer and\n            its parameters to be set using ``set_params`` and searched in grid\n            search.\n        transformer : estimator or {\'passthrough\', \'drop\'}\n            Estimator must support `fit` and `transform`. Special-cased\n            strings \'drop\' and \'passthrough\' are accepted as well, to\n            indicate to drop the columns or to pass them through untransformed,\n            respectively.\n        column(s) : string or int, array-like of string or int, slice, \\\nboolean mask array or callable\n            Indexes the data on its second axis. Integers are interpreted as\n            positional columns, while strings can reference DataFrame columns\n            by name.  A scalar string or int should be used where\n            ``transformer`` expects X to be a 1d array-like (vector),\n            otherwise a 2d array will be passed to the transformer.\n            A callable is passed the input data `X` and can return any of the\n            above.\n\n    remainder : {\'drop\', \'passthrough\'} or estimator, default \'drop\'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``\'drop\'``).\n        By specifying ``remainder=\'passthrough\'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support `fit` and `transform`.\n\n    sparse_threshold : float, default = 0.3\n        If the transformed output consists of a mix of sparse and dense data,\n        it will be stacked as a sparse matrix if the density is lower than this\n        value. Use ``sparse_threshold=0`` to always return dense.\n        When the transformed output consists of all sparse or all dense data,\n        the stacked result will be sparse or dense, respectively, and this\n        keyword will be ignored.\n\n    n_jobs : int, optional\n        Number of jobs to run in parallel (default 1).\n\n    transformer_weights : dict, optional\n        Multiplicative weights for features per transformer. The output of the\n        transformer is multiplied by these weights. Keys are transformer names,\n        values the weights.\n\n    Attributes\n    ----------\n    transformers_ : list\n        The collection of fitted transformers as tuples of\n        (name, fitted_transformer, column). `fitted_transformer` can be an\n        estimator, \'drop\', or \'passthrough\'. If there are remaining columns,\n        the final element is a tuple of the form:\n        (\'remainder\', transformer, remaining_columns) corresponding to the\n        ``remainder`` parameter. If there are remaining columns, then\n        ``len(transformers_)==len(transformers)+1``, otherwise\n        ``len(transformers_)==len(transformers)``.\n\n    named_transformers_ : Bunch object, a dictionary with attribute access\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n    sparse_output_ : boolean\n        Boolean flag indicating wether the output of ``transform`` is a\n        sparse matrix or a dense numpy array, which depends on the output\n        of the individual transformers and the `sparse_threshold` keyword.\n\n    Notes\n    -----\n    The order of the columns in the transformed feature matrix follows the\n    order of how the columns are specified in the `transformers` list.\n    Columns of the original feature matrix that are not specified are\n    dropped from the resulting transformed feature matrix, unless specified\n    in the `passthrough` keyword. Those columns specified with `passthrough`\n    are added at the right to the output of the transformers.\n\n    See also\n    --------\n    sklearn.compose.make_column_transformer : convenience function for\n        combining the outputs of multiple transformer objects applied to\n        column subsets of the original feature space.\n\n    Examples\n    --------\n    >>> from sklearn.compose import ColumnTransformer\n    >>> from sklearn.preprocessing import Normalizer\n    >>> ct = ColumnTransformer(\n    ...     [(""norm1"", Normalizer(norm=\'l1\'), [0, 1]),\n    ...      (""norm2"", Normalizer(norm=\'l1\'), slice(2, 4))])\n    >>> X = np.array([[0., 1., 2., 2.],\n    ...               [1., 1., 0., 1.]])\n    >>> # Normalizer scales each row of X to unit norm. A separate scaling\n    >>> # is applied for the two first and two last elements of each\n    >>> # row independently.\n    >>> ct.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE\n    array([[0. , 1. , 0.5, 0.5],\n           [0.5, 0.5, 0. , 1. ]])\n\n    """"""\n\n    def __init__(self, transformers, remainder=\'drop\', sparse_threshold=0.3,\n                 n_jobs=1, transformer_weights=None):\n        self.transformers = transformers\n        self.remainder = remainder\n        self.sparse_threshold = sparse_threshold\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n\n    @property\n    def _transformers(self):\n        """"""\n        Internal list of transformer only containing the name and\n        transformers, dropping the columns. This is for the implementation\n        of get_params via BaseComposition._get_params which expects lists\n        of tuples of len 2.\n        """"""\n        return [(name, trans) for name, trans, _ in self.transformers]\n\n    @_transformers.setter\n    def _transformers(self, value):\n        self.transformers = [\n            (name, trans, col) for ((name, trans), (_, _, col))\n            in zip(value, self.transformers)]\n\n    def get_params(self, deep=True):\n        """"""Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : boolean, optional\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : mapping of string to any\n            Parameter names mapped to their values.\n        """"""\n        return self._get_params(\'_transformers\', deep=deep)\n\n    def set_params(self, **kwargs):\n        """"""Set the parameters of this estimator.\n\n        Valid parameter keys can be listed with ``get_params()``.\n\n        Returns\n        -------\n        self\n        """"""\n        self._set_params(\'_transformers\', **kwargs)\n        return self\n\n    def _iter(self, X=None, fitted=False, replace_strings=False):\n        """"""Generate (name, trans, column, weight) tuples\n        """"""\n        if fitted:\n            transformers = self.transformers_\n        else:\n            transformers = self.transformers\n            if self._remainder[2] is not None:\n                transformers = chain(transformers, [self._remainder])\n        get_weight = (self.transformer_weights or {}).get\n\n        for name, trans, column in transformers:\n            sub = None if X is None else _get_column(X, column)\n\n            if replace_strings:\n                # replace \'passthrough\' with identity transformer and\n                # skip in case of \'drop\'\n                if trans == \'passthrough\':\n                    trans = FunctionTransformer(\n                        validate=False, accept_sparse=True,\n                        check_inverse=False)\n                elif trans == \'drop\':\n                    continue\n\n            yield (name, trans, sub, get_weight(name))\n\n    def _validate_transformers(self):\n        if not self.transformers:\n            return\n\n        names, transformers, _ = zip(*self.transformers)\n\n        # validate names\n        self._validate_names(names)\n\n        # validate estimators\n        for t in transformers:\n            if t in (\'drop\', \'passthrough\'):\n                continue\n            if (not (hasattr(t, ""fit"") or hasattr(t, ""fit_transform"")) or not\n                    hasattr(t, ""transform"")):\n                raise TypeError(""All estimators should implement fit and ""\n                                ""transform, or can be \'drop\' or \'passthrough\' ""\n                                ""specifiers. \'%s\' (type %s) doesn\'t."" %\n                                (t, type(t)))\n\n    def _validate_remainder(self, X):\n        """"""\n        Validates ``remainder`` and defines ``_remainder`` targeting\n        the remaining columns.\n        """"""\n        is_transformer = ((hasattr(self.remainder, ""fit"")\n                           or hasattr(self.remainder, ""fit_transform""))\n                          and hasattr(self.remainder, ""transform""))\n        if (self.remainder not in (\'drop\', \'passthrough\')\n                and not is_transformer):\n            raise ValueError(\n                ""The remainder keyword needs to be one of \'drop\', ""\n                ""\'passthrough\', or estimator. \'%s\' was passed instead"" %\n                self.remainder)\n\n        n_columns = X.shape[1]\n        cols = []\n        for _, _, columns in self.transformers:\n            cols.extend(_get_column_indices(X, columns))\n        remaining_idx = sorted(list(set(range(n_columns)) - set(cols))) or None\n\n        self._remainder = (\'remainder\', self.remainder, remaining_idx)\n\n    @property\n    def named_transformers_(self):\n        """"""Access the fitted transformer by name.\n\n        Read-only attribute to access any transformer by given name.\n        Keys are transformer names and values are the fitted transformer\n        objects.\n\n        """"""\n        # Use Bunch object to improve autocomplete\n        return Bunch(**dict([(name, trans) for name, trans, _\n                             in self.transformers_]))\n\n    def get_feature_names(self):\n        """"""Get feature names from all transformers.\n\n        Returns\n        -------\n        feature_names : list of strings\n            Names of the features produced by transform.\n        """"""\n        check_is_fitted(self, \'transformers_\')\n        feature_names = []\n        for name, trans, _, _ in self._iter(fitted=True):\n            if trans == \'drop\':\n                continue\n            elif trans == \'passthrough\':\n                raise NotImplementedError(\n                    ""get_feature_names is not yet supported when using ""\n                    ""a \'passthrough\' transformer."")\n            elif not hasattr(trans, \'get_feature_names\'):\n                raise AttributeError(""Transformer %s (type %s) does not ""\n                                     ""provide get_feature_names.""\n                                     % (str(name), type(trans).__name__))\n            feature_names.extend([name + ""__"" + f for f in\n                                  trans.get_feature_names()])\n        return feature_names\n\n    def _update_fitted_transformers(self, transformers):\n        # transformers are fitted; excludes \'drop\' cases\n        transformers = iter(transformers)\n        transformers_ = []\n\n        transformer_iter = self.transformers\n        if self._remainder[2] is not None:\n            transformer_iter = chain(transformer_iter, [self._remainder])\n\n        for name, old, column in transformer_iter:\n            if old == \'drop\':\n                trans = \'drop\'\n            elif old == \'passthrough\':\n                # FunctionTransformer is present in list of transformers,\n                # so get next transformer, but save original string\n                next(transformers)\n                trans = \'passthrough\'\n            else:\n                trans = next(transformers)\n            transformers_.append((name, trans, column))\n\n        # sanity check that transformers is exhausted\n        assert not list(transformers)\n        self.transformers_ = transformers_\n\n    def _validate_output(self, result):\n        """"""\n        Ensure that the output of each transformer is 2D. Otherwise\n        hstack can raise an error or produce incorrect results.\n        """"""\n        names = [name for name, _, _, _ in self._iter(replace_strings=True)]\n        for Xs, name in zip(result, names):\n            if not getattr(Xs, \'ndim\', 0) == 2:\n                raise ValueError(\n                    ""The output of the \'{0}\' transformer should be 2D (scipy ""\n                    ""matrix, array, or pandas DataFrame)."".format(name))\n\n    def _fit_transform(self, X, y, func, fitted=False):\n        """"""\n        Private function to fit and/or transform on demand.\n\n        Return value (transformers and/or transformed X data) depends\n        on the passed function.\n        ``fitted=True`` ensures the fitted transformers are used.\n        """"""\n        try:\n            return Parallel(n_jobs=self.n_jobs)(\n                delayed(func)(clone(trans) if not fitted else trans,\n                              X_sel, y, weight)\n                for _, trans, X_sel, weight in self._iter(\n                    X=X, fitted=fitted, replace_strings=True))\n        except ValueError as e:\n            if ""Expected 2D array, got 1D array instead"" in str(e):\n                raise ValueError(_ERR_MSG_1DCOLUMN)\n            else:\n                raise\n\n    def fit(self, X, y=None):\n        """"""Fit all transformers using X.\n\n        Parameters\n        ----------\n        X : array-like or DataFrame of shape [n_samples, n_features]\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like, shape (n_samples, ...), optional\n            Targets for supervised learning.\n\n        Returns\n        -------\n        self : ColumnTransformer\n            This estimator\n\n        """"""\n        # we use fit_transform to make sure to set sparse_output_ (for which we\n        # need the transformed data) to have consistent output type in predict\n        self.fit_transform(X, y=y)\n        return self\n\n    def fit_transform(self, X, y=None):\n        """"""Fit all transformers, transform the data and concatenate results.\n\n        Parameters\n        ----------\n        X : array-like or DataFrame of shape [n_samples, n_features]\n            Input data, of which specified subsets are used to fit the\n            transformers.\n\n        y : array-like, shape (n_samples, ...), optional\n            Targets for supervised learning.\n\n        Returns\n        -------\n        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        """"""\n        self._validate_remainder(X)\n        self._validate_transformers()\n\n        result = self._fit_transform(X, y, _fit_transform_one)\n\n        if not result:\n            self._update_fitted_transformers([])\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        Xs, transformers = zip(*result)\n\n        # determine if concatenated output will be sparse or not\n        if all(sparse.issparse(X) for X in Xs):\n            self.sparse_output_ = True\n        elif any(sparse.issparse(X) for X in Xs):\n            nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)\n            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)\n                        else X.size for X in Xs)\n            density = nnz / total\n            self.sparse_output_ = density < self.sparse_threshold\n        else:\n            self.sparse_output_ = False\n\n        self._update_fitted_transformers(transformers)\n        self._validate_output(Xs)\n\n        return self._hstack(list(Xs))\n\n    def transform(self, X):\n        """"""Transform X separately by each transformer, concatenate results.\n\n        Parameters\n        ----------\n        X : array-like or DataFrame of shape [n_samples, n_features]\n            The data to be transformed by subset.\n\n        Returns\n        -------\n        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n            hstack of results of transformers. sum_n_components is the\n            sum of n_components (output dimension) over transformers. If\n            any result is a sparse matrix, everything will be converted to\n            sparse matrices.\n\n        """"""\n        check_is_fitted(self, \'transformers_\')\n\n        Xs = self._fit_transform(X, None, _transform_one, fitted=True)\n        self._validate_output(Xs)\n\n        if not Xs:\n            # All transformers are None\n            return np.zeros((X.shape[0], 0))\n\n        return self._hstack(list(Xs))\n\n    def _hstack(self, Xs):\n        """"""Stacks Xs horizontally.\n\n        This allows subclasses to control the stacking behavior, while reusing\n        everything else from ColumnTransformer.\n\n        Parameters\n        ----------\n        Xs : List of numpy arrays, sparse arrays, or DataFrames\n        """"""\n        if self.sparse_output_:\n            return sparse.hstack(Xs).tocsr()\n        else:\n            Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]\n            return np.hstack(Xs)\n\n\ndef _check_key_type(key, superclass):\n    """"""\n    Check that scalar, list or slice is of a certain type.\n\n    This is only used in _get_column and _get_column_indices to check\n    if the `key` (column specification) is fully integer or fully string-like.\n\n    Parameters\n    ----------\n    key : scalar, list, slice, array-like\n        The column specification to check\n    superclass : int or six.string_types\n        The type for which to check the `key`\n\n    """"""\n    if isinstance(key, superclass):\n        return True\n    if isinstance(key, slice):\n        return (isinstance(key.start, (superclass, type(None))) and\n                isinstance(key.stop, (superclass, type(None))))\n    if isinstance(key, list):\n        return all(isinstance(x, superclass) for x in key)\n    if hasattr(key, \'dtype\'):\n        if superclass is int:\n            return key.dtype.kind == \'i\'\n        else:\n            # superclass = six.string_types\n            return key.dtype.kind in (\'O\', \'U\', \'S\')\n    return False\n\n\ndef _get_column(X, key):\n    """"""\n    Get feature column(s) from input data X.\n\n    Supported input types (X): numpy arrays, sparse arrays and DataFrames\n\n    Supported key types (key):\n    - scalar: output is 1D\n    - lists, slices, boolean masks: output is 2D\n    - callable that returns any of the above\n\n    Supported key data types:\n\n    - integer or boolean mask (positional):\n        - supported for arrays, sparse matrices and dataframes\n    - string (key-based):\n        - only supported for dataframes\n        - So no keys other than strings are allowed (while in principle you\n          can use any hashable object as key).\n\n    """"""\n    if callable(key):\n        key = key(X)\n\n    # check whether we have string column names or integers\n    if _check_key_type(key, int):\n        column_names = False\n    elif _check_key_type(key, six.string_types):\n        column_names = True\n    elif hasattr(key, \'dtype\') and np.issubdtype(key.dtype, np.bool_):\n        # boolean mask\n        column_names = False\n        if hasattr(X, \'loc\'):\n            # pandas boolean masks don\'t work with iloc, so take loc path\n            column_names = True\n    else:\n        raise ValueError(""No valid specification of the columns. Only a ""\n                         ""scalar, list or slice of all integers or all ""\n                         ""strings, or boolean mask is allowed"")\n\n    if column_names:\n        if hasattr(X, \'loc\'):\n            # pandas dataframes\n            return X.loc[:, key]\n        else:\n            raise ValueError(""Specifying the columns using strings is only ""\n                             ""supported for pandas DataFrames"")\n    else:\n        if hasattr(X, \'iloc\'):\n            # pandas dataframes\n            return X.iloc[:, key]\n        else:\n            # numpy arrays, sparse arrays\n            return X[:, key]\n\n\ndef _get_column_indices(X, key):\n    """"""\n    Get feature column indices for input data X and key.\n\n    For accepted values of `key`, see the docstring of _get_column\n\n    """"""\n    n_columns = X.shape[1]\n\n    if callable(key):\n        key = key(X)\n\n    if _check_key_type(key, int):\n        if isinstance(key, int):\n            return [key]\n        elif isinstance(key, slice):\n            return list(range(n_columns)[key])\n        else:\n            return list(key)\n\n    elif _check_key_type(key, six.string_types):\n        try:\n            all_columns = list(X.columns)\n        except AttributeError:\n            raise ValueError(""Specifying the columns using strings is only ""\n                             ""supported for pandas DataFrames"")\n        if isinstance(key, six.string_types):\n            columns = [key]\n        elif isinstance(key, slice):\n            start, stop = key.start, key.stop\n            if start is not None:\n                start = all_columns.index(start)\n            if stop is not None:\n                # pandas indexing with strings is endpoint included\n                stop = all_columns.index(stop) + 1\n            else:\n                stop = n_columns + 1\n            return list(range(n_columns)[slice(start, stop)])\n        else:\n            columns = list(key)\n\n        return [all_columns.index(col) for col in columns]\n\n    elif hasattr(key, \'dtype\') and np.issubdtype(key.dtype, np.bool_):\n        # boolean mask\n        return list(np.arange(n_columns)[key])\n    else:\n        raise ValueError(""No valid specification of the columns. Only a ""\n                         ""scalar, list or slice of all integers or all ""\n                         ""strings, or boolean mask is allowed"")\n\n\ndef _get_transformer_list(estimators):\n    """"""\n    Construct (name, trans, column) tuples from list\n\n    """"""\n    transformers = [trans[1] for trans in estimators]\n    columns = [trans[0] for trans in estimators]\n    names = [trans[0] for trans in _name_estimators(transformers)]\n\n    transformer_list = list(zip(names, transformers, columns))\n    return transformer_list\n\n\ndef make_column_transformer(*transformers, **kwargs):\n    """"""Construct a ColumnTransformer from the given transformers.\n\n    This is a shorthand for the ColumnTransformer constructor; it does not\n    require, and does not permit, naming the transformers. Instead, they will\n    be given names automatically based on their types. It also does not allow\n    weighting.\n\n    Parameters\n    ----------\n    *transformers : tuples of column selections and transformers\n\n    remainder : {\'drop\', \'passthrough\'} or estimator, default \'drop\'\n        By default, only the specified columns in `transformers` are\n        transformed and combined in the output, and the non-specified\n        columns are dropped. (default of ``\'drop\'``).\n        By specifying ``remainder=\'passthrough\'``, all remaining columns that\n        were not specified in `transformers` will be automatically passed\n        through. This subset of columns is concatenated with the output of\n        the transformers.\n        By setting ``remainder`` to be an estimator, the remaining\n        non-specified columns will use the ``remainder`` estimator. The\n        estimator must support `fit` and `transform`.\n\n    n_jobs : int, optional\n        Number of jobs to run in parallel (default 1).\n\n    Returns\n    -------\n    ct : ColumnTransformer\n\n    See also\n    --------\n    sklearn.compose.ColumnTransformer : Class that allows combining the\n        outputs of multiple transformer objects used on column subsets\n        of the data into a single feature space.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    >>> from sklearn.compose import make_column_transformer\n    >>> make_column_transformer(\n    ...     ([\'numerical_column\'], StandardScaler()),\n    ...     ([\'categorical_column\'], OneHotEncoder()))\n    ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    ColumnTransformer(n_jobs=1, remainder=\'drop\', sparse_threshold=0.3,\n             transformer_weights=None,\n             transformers=[(\'standardscaler\',\n                            StandardScaler(...),\n                            [\'numerical_column\']),\n                           (\'onehotencoder\',\n                            OneHotEncoder(...),\n                            [\'categorical_column\'])])\n\n    """"""\n    n_jobs = kwargs.pop(\'n_jobs\', 1)\n    remainder = kwargs.pop(\'remainder\', \'drop\')\n    if kwargs:\n        raise TypeError(\'Unknown keyword arguments: ""{}""\'\n                        .format(list(kwargs.keys())[0]))\n    transformer_list = _get_transformer_list(transformers)\n    return ColumnTransformer(transformer_list, n_jobs=n_jobs,\n                             remainder=remainder)'"
tensorflow_graph_in_jupyter.py,1,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\n# This module defines the show_graph() function to visualize a TensorFlow graph within Jupyter.\n\n# As far as I can tell, this code was originally written by Alex Mordvintsev at:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n\n# The original code only worked on Chrome (because of the use of <link rel=""import""...>, but the version below\n# uses Polyfill (copied from this StackOverflow answer: https://stackoverflow.com/a/41463991/38626)\n# so that it can work on other browsers as well.\n\nimport numpy as np\nimport tensorflow as tf\nfrom IPython.display import clear_output, Image, display, HTML\n\ndef strip_consts(graph_def, max_const_size=32):\n    """"""Strip large constant values from graph_def.""""""\n    strip_def = tf.GraphDef()\n    for n0 in graph_def.node:\n        n = strip_def.node.add() \n        n.MergeFrom(n0)\n        if n.op == \'Const\':\n            tensor = n.attr[\'value\'].tensor\n            size = len(tensor.tensor_content)\n            if size > max_const_size:\n                tensor.tensor_content = b""<stripped %d bytes>""%size\n    return strip_def\n\ndef show_graph(graph_def, max_const_size=32):\n    """"""Visualize TensorFlow graph.""""""\n    if hasattr(graph_def, \'as_graph_def\'):\n        graph_def = graph_def.as_graph_def()\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n    code = """"""\n        <script src=""//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js""></script>\n        <script>\n          function load() {{\n            document.getElementById(""{id}"").pbtxt = {data};\n          }}\n        </script>\n        <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>\n        <div style=""height:600px"">\n          <tf-graph-basic id=""{id}""></tf-graph-basic>\n        </div>\n    """""".format(data=repr(str(strip_def)), id=\'graph\'+str(np.random.rand()))\n\n    iframe = """"""\n        <iframe seamless style=""width:1200px;height:620px;border:0"" srcdoc=""{}""></iframe>\n    """""".format(code.replace(\'""\', \'&quot;\'))\n    display(HTML(iframe))\n'"
docker/jupyter_notebook_config.py,0,"b'import os\nimport subprocess\n\ndef export_script_and_view(model, os_path, contents_manager):\n    if model[""type""] != ""notebook"":\n        return\n    dir_name, file_name = os.path.split(os_path)\n    file_base, file_ext = os.path.splitext(file_name)\n    if file_base.startswith(""Untitled""):\n        return\n    export_name = file_base if file_ext == "".ipynb"" else file_name\n    subprocess.check_call([""jupyter"", ""nbconvert"", ""--to"", ""script"", file_name, ""--output"", export_name + ""_script""], cwd=dir_name)\n    subprocess.check_call([""jupyter"", ""nbconvert"", ""--to"", ""html"", file_name, ""--output"", export_name + ""_view""], cwd=dir_name)\n\nc.FileContentsManager.post_save_hook = export_script_and_view\n'"
