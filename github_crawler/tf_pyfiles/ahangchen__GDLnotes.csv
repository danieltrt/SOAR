file_path,api_count,code
src/__init__.py,0,b''
src/distrib.py,19,"b'import tensorflow as tf\n\n# Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(""ps_hosts"", """",\n                           ""Comma-separated list of hostname:port pairs"")\ntf.app.flags.DEFINE_string(""worker_hosts"", """",\n                           ""Comma-separated list of hostname:port pairs"")\n\n# Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(""job_name"", """", ""One of \'ps\', \'worker\'"")\ntf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split("","")\n    worker_hosts = FLAGS.worker_hosts.split("","")\n\n    # Create a cluster from the parameter server and worker hosts.\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n\n    # Create and start a server for the local task.\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == ""ps"":\n        server.join()\n    elif FLAGS.job_name == ""worker"":\n\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=""/job:worker/task:%d"" % FLAGS.task_index,\n                cluster=cluster)):\n\n            # TODO Build model...\n            loss = tf.Variable()\n            global_step = tf.Variable(0)\n\n            train_op = tf.train.AdagradOptimizer(0.01).minimize(\n                loss, global_step=global_step)\n\n            saver = tf.train.Saver()\n            summary_op = tf.merge_all_summaries()\n            init_op = tf.global_variables_initializer()\n\n        # Create a ""supervisor"", which oversees the training process.\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=""/tmp/train_logs"",\n                                 init_op=init_op,\n                                 summary_op=summary_op,\n                                 saver=saver,\n                                 global_step=global_step,\n                                 save_model_secs=600)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step = 0\n            while not sv.should_stop() and step < 1000000:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                _, step = sess.run([train_op, global_step])\n\n        # Ask for all the services to stop.\n        sv.stop()\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
src/num_stable.py,0,b'# Numerical stability\n\na = 1000000000\nfor i in xrange(1000000):\n    a += 1e-6\nprint(a - 1000000000)\n\na = 1\nfor i in xrange(1000000):\n    a += 1e-6\nprint a - 1'
src/soft_max.py,0,"b'""""""Softmax.""""""\n\nscores = [3.0, 1.0, 0.2]\n\nimport numpy as np\n\n\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis=0)\n\n\nprint(softmax(scores))\n\n# Plot softmax curves\nimport matplotlib.pyplot as plt\n\nx = np.arange(-2.0, 6.0, 0.1)\nscores = np.vstack([x, np.ones_like(x), 0.2 * np.ones_like(x)])\n\nplt.plot(x, softmax(scores).T, linewidth=2)\nplt.show()\n'"
src/app/__init__.py,0,b''
src/convnet/__init__.py,0,b''
src/convnet/cnn_board.py,86,"b'from __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom convnet.conv_mnist import maxpool2d\nfrom util.board import variable_summary\nfrom util.mnist import format_mnist\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps, drop=False, lrd=False, get_grad=False, norm_list=None):\n    batch_size = basic_hps[\'batch_size\']\n    patch_size = basic_hps[\'patch_size\']\n    depth = basic_hps[\'depth\']\n    first_hidden_num = 192\n    second_hidden_num = basic_hps[\'num_hidden\']\n    num_channels = 1\n    layer_cnt = basic_hps[\'layer_sum\']\n    loss_collect = list()\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        with tf.name_scope(\'input\'):\n            with tf.name_scope(\'data\'):\n                tf_train_dataset = tf.placeholder(\n                    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n                variable_summary(tf_train_dataset)\n            with tf.name_scope(\'label\'):\n                tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n                variable_summary(tf_train_labels)\n\n        # Variables.\n        # the third parameter must be same as the last layer depth\n        with tf.name_scope(\'input_cnn_filter\'):\n            with tf.name_scope(\'input_weight\'):\n                input_weights = tf.Variable(tf.truncated_normal(\n                    [patch_size, patch_size, num_channels, depth], stddev=0.1), name=\'input_weight\')\n                variable_summary(input_weights)\n            with tf.name_scope(\'input_biases\'):\n                input_biases = tf.Variable(tf.zeros([depth]), name=\'input_biases\')\n                variable_summary(input_weights)\n\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth * (i + 2)])) for i in range(mid_layer_cnt)]\n        for i in range(mid_layer_cnt):\n            variable_summary(layer_biases)\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[first_hidden_num]))\n        with tf.name_scope(\'first_nn\'):\n            with tf.name_scope(\'weights\'):\n                first_nn_weights = tf.Variable(tf.truncated_normal(\n                    [first_hidden_num, second_hidden_num], stddev=0.1))\n                variable_summary(first_nn_weights)\n            with tf.name_scope(\'biases\'):\n                first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n                variable_summary(first_nn_weights)\n        with tf.name_scope(\'second_nn\'):\n            with tf.name_scope(\'weights\'):\n                second_nn_weights = tf.Variable(tf.truncated_normal(\n                    [second_hidden_num, num_labels], stddev=0.1))\n                variable_summary(second_nn_weights)\n            with tf.name_scope(\'biases\'):\n                second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n                variable_summary(second_nn_biases)\n\n        # Model.\n        def model(data, init=True):\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            with tf.name_scope(\'first_cnn\'):\n                conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding=\'SAME\')\n                if init:\n                    print(\'init\')\n                    variable_summary(conv)\n            with tf.name_scope(\'first_max_pool\'):\n                conv = maxpool2d(conv)\n                if init:\n                    variable_summary(conv)\n            hidden = tf.nn.relu6(conv + input_biases)\n            if init:\n                tf.summary.histogram(\'first_act\', hidden)\n            if drop and init:\n                with tf.name_scope(\'first_drop\'):\n                    hidden = tf.nn.dropout(hidden, 0.8, name=\'drop1\')\n                    tf.summary.histogram(\'first_drop\', hidden)\n            for i in range(mid_layer_cnt):\n                with tf.name_scope(\'cnn{i}\'.format(i=i)):\n                    if init:\n                        # avoid filter shape larger than input shape\n                        hid_shape = hidden.get_shape()\n                        # print(hid_shape)\n                        filter_w = patch_size / (i + 1)\n                        filter_h = patch_size / (i + 1)\n                        # print(filter_w)\n                        # print(filter_h)\n                        if filter_w > hid_shape[1]:\n                            filter_w = int(hid_shape[1])\n                        if filter_h > hid_shape[2]:\n                            filter_h = int(hid_shape[2])\n                        with tf.name_scope(\'weight\'.format(i=i)):\n                            layer_weight = tf.Variable(tf.truncated_normal(\n                                shape=[filter_w, filter_h, depth * (i + 1), depth * (i + 2)], stddev=0.1))\n                            variable_summary(layer_weight)\n                        layer_weights.append(layer_weight)\n                    if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                        # print(""is not large data"")\n                        stride_ps[i + 1] = [1, 1, 1, 1]\n                    # print(stride_ps[i + 1])\n                    # print(len(stride_ps))\n                    # print(i + 1)\n                    with tf.name_scope(\'conv2d\'):\n                        conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding=\'SAME\')\n                        if init:\n                            variable_summary(conv)\n                    with tf.name_scope(\'maxpool2d\'):\n                        if not large_data_size(conv):\n                            print(\'not large\')\n                            conv = maxpool2d(conv, 1, 1)\n                            if init:\n                                variable_summary(conv)\n                        else:\n                            conv = maxpool2d(conv)\n                            if init:\n                                variable_summary(conv)\n                    with tf.name_scope(\'act\'):\n                        hidden = tf.nn.relu6(conv + layer_biases[i])\n                        if init:\n                            variable_summary(conv)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                with tf.name_scope(\'output\'):\n                    output_size = shape_mul\n                    with tf.name_scope(\'weights\'):\n                        output_weights.append(tf.Variable(tf.truncated_normal([output_size, first_hidden_num], stddev=0.1)))\n                        variable_summary(output_weights)\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n            with tf.name_scope(\'output_act\'):\n                hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n                if init:\n                    tf.summary.histogram(\'output_act\', hidden)\n            if drop and init:\n                with tf.name_scope(\'output_drop\'):\n                    hidden = tf.nn.dropout(hidden, 0.5)\n                    tf.summary.histogram(\'output_drop\', hidden)\n            with tf.name_scope(\'output_wx_b\'):\n                hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n                if init:\n                    tf.summary.histogram(\'output_wx_b\', hidden)\n            if drop and init:\n                with tf.name_scope(\'final_drop\'):\n                    hidden = tf.nn.dropout(hidden, 0.5)\n                    tf.summary.histogram(\'final_drop\', hidden)\n            with tf.name_scope(\'final_wx_b\'):\n                hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n                if init:\n                    tf.summary.histogram(\'final_wx_b\', hidden)\n            return hidden\n\n        # Training computation.\n        with tf.name_scope(\'logits\'):\n            logits = model(tf_train_dataset)\n            tf.summary.histogram(\'logits\', logits)\n        with tf.name_scope(\'loss\'):\n            loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n            tf.summary.histogram(\'loss\', loss)\n        # Optimizer.\n\n        with tf.name_scope(\'train\'):\n            if lrd:\n                cur_step = tf.Variable(0)  # count the number of steps taken.\n                starter_learning_rate = 0.06\n                learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 600, 0.1, staircase=True)\n                optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n            else:\n                optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        with tf.name_scope(\'train_predict\'):\n            train_prediction = tf.nn.softmax(logits)\n            variable_summary(train_prediction)\n        # with tf.name_scope(\'valid_predict\'):\n        #     valid_prediction = tf.nn.softmax(model(tf_valid_dataset, init=False))\n        #     variable_summary(valid_prediction, \'valid_predict\')\n        # with tf.name_scope(\'test_predict\'):\n        #     test_prediction = tf.nn.softmax(model(tf_test_dataset, init=False))\n        #     variable_summary(test_prediction, \'test_predict\')\n        merged = tf.summary.merge_all()\n    summary_flag = True\n    summary_dir = \'summary\'\n    if tf.gfile.Exists(summary_dir):\n        tf.gfile.DeleteRecursively(summary_dir)\n    tf.gfile.MakeDirs(summary_dir)\n\n    num_steps = 5001\n    with tf.Session(graph=graph) as session:\n        train_writer = tf.summary.FileWriter(summary_dir + \'/train\',\n                                             session.graph)\n        valid_writer = tf.summary.FileWriter(summary_dir + \'/valid\')\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        mean_loss = 0\n        for step in range(num_steps):\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n            run_metadata = tf.RunMetadata()\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            if summary_flag:\n                summary, _, l, predictions = session.run(\n                    [merged, optimizer, loss, train_prediction], options=run_options, feed_dict=feed_dict)\n            else:\n                _, l, predictions = session.run(\n                    [optimizer, loss, train_prediction], options=run_options, feed_dict=feed_dict)\n            mean_loss += l\n            if step % 5 == 0:\n                mean_loss /= 5.0\n                loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step % 50 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (step, l))\n                    # print(\'Validation accuracy: %.1f%%\' % accuracy(\n                    #     valid_prediction.eval(), valid_labels))\n                    if step % 100 == 0 and summary_flag:\n                        train_writer.add_run_metadata(run_metadata, \'step%03d\' % step)\n                        train_writer.add_summary(summary, step)\n                        print(\'Adding run metadata for\', step)\n                if summary_flag:\n                    valid_writer.add_summary(summary, step)\n            if summary_flag:\n                train_writer.add_summary(summary, step)\n        train_writer.close()\n        valid_writer.close()\n        # print(\'Test accuracy: %.1f%%\' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef hp_train():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        \'batch_size\': 32,\n        \'patch_size\': 5,\n        \'depth\': 16,\n        \'num_hidden\': 64,\n        \'layer_sum\': 2\n    }\n    stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers[\'layer_sum\'])]\n    conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset,\n               test_labels,\n               image_size, num_labels, basic_hypers, stride_params, drop=True, lrd=False)\n\n\nif __name__ == \'__main__\':\n    hp_train()\n'"
src/convnet/conv_mnist.py,94,"b""import numpy as np\nimport tensorflow as tf\n\nfrom neural.full_connect import accuracy\nfrom not_mnist.img_pickle import load_pickle\n\n\ndef reformat(dataset, labels, image_size, num_labels, num_channels):\n    dataset = dataset.reshape(\n        (-1, image_size, image_size, num_channels)).astype(np.float32)\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef load_reformat_not_mnist(image_size, num_labels, num_channels):\n    pickle_file = '../not_mnist/notMNIST_clean.pickle'\n    save = load_pickle(pickle_file)\n    train_dataset = save['train_dataset']\n    train_labels = save['train_labels']\n    valid_dataset = save['valid_dataset']\n    valid_labels = save['valid_labels']\n    test_dataset = save['test_dataset']\n    test_labels = save['test_labels']\n    del save  # hint to help gc free up memory\n    print('Training set', train_dataset.shape, train_labels.shape)\n    print('Validation set', valid_dataset.shape, valid_labels.shape)\n    print('Test set', test_dataset.shape, test_labels.shape)\n    train_dataset, train_labels = reformat(train_dataset, train_labels, image_size, num_labels, num_channels)\n    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels, image_size, num_labels, num_channels)\n    test_dataset, test_labels = reformat(test_dataset, test_labels, image_size, num_labels, num_channels)\n    valid_dataset = valid_dataset[: 1000]\n    valid_labels = valid_labels[: 1000]\n    test_dataset = test_dataset[: 1000]\n    test_labels = test_labels[: 1000]\n    print('Training set', train_dataset.shape, train_labels.shape)\n    print('Validation set', valid_dataset.shape, valid_labels.shape)\n    print('Test set', test_dataset.shape, test_labels.shape)\n    return train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels\n\n\ndef maxpool2d(data, k=2, s=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool(data, ksize=[1, k, k, 1], strides=[1, s, s, 1],\n                          padding='SAME')\n\n\ndef conv_train():\n    batch_size = 16\n    patch_size = 5\n    depth = 16\n    num_hidden = 64\n    num_channels = 1\n\n    graph = tf.Graph()\n\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        layer1_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        layer1_biases = tf.Variable(tf.zeros([depth]))\n        layer2_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, depth, depth], stddev=0.1))\n        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n        layer3_weights = tf.Variable(tf.truncated_normal(\n            [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n        layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        layer4_weights = tf.Variable(tf.truncated_normal(\n            [num_hidden, num_labels], stddev=0.1))\n        layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data):\n            conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n            hidden = tf.nn.relu(conv + layer1_biases)\n            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n            hidden = tf.nn.relu(conv + layer2_biases)\n            shape = hidden.get_shape().as_list()\n            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n            return tf.matmul(hidden, layer4_weights) + layer4_biases\n\n        # Training computation.\n        logits = model(tf_train_dataset)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 1001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        for step in range(num_steps):\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 50 == 0:\n                print('Minibatch loss at step %d: %f' % (step, l))\n                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n                print('Validation accuracy: %.1f%%' % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef conv_max_pool_train():\n    batch_size = 16\n    patch_size = 5\n    depth = 16\n    num_hidden = 64\n    num_channels = 1\n\n    graph = tf.Graph()\n\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        layer1_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        layer1_biases = tf.Variable(tf.zeros([depth]))\n        layer2_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, depth, depth], stddev=0.1))\n        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n        layer3_weights = tf.Variable(tf.truncated_normal(\n            [64, num_hidden], stddev=0.1))\n        layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        layer4_weights = tf.Variable(tf.truncated_normal(\n            [num_hidden, num_labels], stddev=0.1))\n        layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data):\n            conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + layer1_biases)\n            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + layer2_biases)\n            shape = hidden.get_shape().as_list()\n            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n            return tf.matmul(hidden, layer4_weights) + layer4_biases\n        # Training computation.\n        logits = model(tf_train_dataset)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 1001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        for step in range(num_steps):\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 50 == 0:\n                print('Minibatch loss at step %d: %f' % (step, l))\n                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n                print('Validation accuracy: %.1f%%' % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef better_conv_train(drop=False, lrd=False):\n    batch_size = 16\n    patch_size = 5\n    depth = 16\n    num_hidden = 64\n    num_channels = 1\n\n    graph = tf.Graph()\n\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        layer1_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        layer1_biases = tf.Variable(tf.zeros([depth]))\n        layer2_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, depth, depth], stddev=0.1))\n        layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n        layer3_weights = tf.Variable(tf.truncated_normal(\n            [64, num_hidden], stddev=0.1))\n        layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        layer4_weights = tf.Variable(tf.truncated_normal(\n            [num_hidden, num_labels], stddev=0.1))\n        layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data):\n            conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + layer1_biases)\n            if drop:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + layer2_biases)\n            if drop:\n                hidden = tf.nn.dropout(hidden, 0.7)\n            shape = hidden.get_shape().as_list()\n            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n            if drop:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            return tf.matmul(hidden, layer4_weights) + layer4_biases\n        # Training computation.\n        logits = model(tf_train_dataset)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        if lrd:\n            cur_step = tf.Variable(0)  # count the number of steps taken.\n            starter_learning_rate = 0.1\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 10000, 0.96, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 5001\n    losses = []\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        for step in range(num_steps):\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            losses.append(l)\n            if step % 50 == 0:\n                print('Minibatch loss at step %d: %f' % (step, l))\n                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n                print('Validation accuracy: %.1f%%' % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n        print(losses)\n        # for i_l in losses:\n        #     print(i_l)\n\n\nif __name__ == '__main__':\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        load_reformat_not_mnist(image_size, num_labels, 1)\n    # conv_max_pool_train()\n    # conv_train()\n    better_conv_train(lrd=True)\n"""
src/convnet/hyper_conv_mnist.py,39,"b'from __future__ import print_function\n\nimport os\n\nimport tensorflow as tf\n\nfrom convnet.conv_mnist import maxpool2d\nfrom neural.full_connect import accuracy\nfrom util.mnist import format_mnist\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps, drop=False, lrd=False):\n    batch_size = basic_hps[\'batch_size\']\n    patch_size = basic_hps[\'patch_size\']\n    depth = basic_hps[\'depth\']\n    first_hidden_num = basic_hps[\'num_hidden\']\n    second_hidden_num = first_hidden_num / 2 + 1\n    num_channels = 1\n    layer_cnt = basic_hps[\'layer_sum\']\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        # the third parameter must be same as the last layer depth\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth * (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[first_hidden_num]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, model_drop=True, init=True):\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding=\'SAME\')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu6(conv + input_biases)\n            if drop and model_drop:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                print(hidden)\n                if init:\n                    # avoid filter shape larger than input shape\n                    hid_shape = hidden.get_shape()\n                    # print(hid_shape)\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    # print(filter_w)\n                    # print(filter_h)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(\n                        shape=[filter_w, filter_h, depth * (i + 1), depth * (i + 2)], stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    # print(""is not large data"")\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                # print(stride_ps[i + 1])\n                # print(len(stride_ps))\n                # print(i + 1)\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding=\'SAME\')\n                if not large_data_size(conv):\n                    print(\'not large\')\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                hidden = tf.nn.relu6(conv + layer_biases[i])\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, first_hidden_num], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if drop and model_drop:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if drop and model_drop:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        # Optimizer.\n        if lrd:\n            cur_step = tf.Variable(0)  # count the number of steps taken.\n            starter_learning_rate = 0.1\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 600, 0.1, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.AdagradOptimizer(0.06).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset, model_drop=False, init=False))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset, model_drop=False, init=False))\n        saver = tf.train.Saver()\n    # on step 1750, run over 55000 train images\n    num_steps = 1750 * 3\n\n    save_path = \'conv_mnist\'\n    save_flag = True\n    with tf.Session(graph=graph) as session:\n        if os.path.exists(save_path) and save_flag:\n            # Restore variables from disk.\n            saver.restore(session, save_path)\n        else:\n            tf.global_variables_initializer().run()\n            print(\'Initialized\')\n        end_train = False\n        mean_loss = 0\n        for step in range(num_steps):\n            if end_train:\n                break\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 10 == 0:\n                mean_loss /= 10.0\n                if step % 200 == 0:\n                    print(\'Minibatch loss at step %d: %f\' % (step, mean_loss))\n                    print(\'Validation accuracy: %.1f%%\' % accuracy(\n                        valid_prediction.eval(), valid_labels))\n                mean_loss = 0\n        if save_flag:\n            saver.save(session, save_path)\n        print(\'Test accuracy: %.1f%%\' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef hp_train():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        \'batch_size\': 32,\n        \'patch_size\': 5,\n        \'depth\': 16,\n        \'num_hidden\': 64,\n        \'layer_sum\': 2\n    }\n    stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers[\'layer_sum\'])]\n    conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset,\n               test_labels,\n               image_size, num_labels, basic_hypers, stride_params, drop=True, lrd=False)\n\n\nif __name__ == \'__main__\':\n    hp_train()\n'"
src/neural/__init__.py,0,b''
src/neural/digit_nn.py,19,"b'import random\nimport numpy as np\nimport tensorflow as tf\n\n\ndef div(xt):\n    label1 = int(abs(xt[0]) < 0.5)\n    label2 = int(abs(xt[1]) < 0.5)\n    return label1 + label2\n\n\ndef train_data():\n    inputs = [[random.uniform(-1, 1), random.uniform(-1, 1)] for i in range(100000)]\n    labels = np.asarray([div(x_t) for x_t in inputs])\n    labels = (np.arange(3) == labels[:, None]).astype(np.float32)\n\n    print(inputs[0])\n    print(div(inputs[0]))\n    print(labels[0])\n    return inputs, labels\n\n\ndef accuracy(predictions, train_labels):\n    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(train_labels, 1)) / predictions.shape[0]\n\n\ndef dig_nn(dataset, train_labels, batch_size, data_count, label_count):\n    graph = tf.Graph()\n    with graph.as_default():\n        tf_train_dataset = tf.placeholder(tf.float32,\n                                          shape=(batch_size, data_count))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, label_count))\n        hidden_node_count = [10, 10]\n        wi = tf.Variable(tf.truncated_normal([data_count, hidden_node_count[0]]))\n        bi = tf.Variable(tf.zeros([hidden_node_count[0]]))\n\n        y1 = tf.matmul(tf_train_dataset, wi) + bi\n        h1 = tf.nn.relu(y1)\n\n        w0 = tf.Variable(tf.truncated_normal([hidden_node_count[0], hidden_node_count[1]]))\n        b0 = tf.Variable(tf.zeros([hidden_node_count[1]]))\n\n        y2 = tf.matmul(h1, w0) + b0\n        h2 = tf.nn.relu(y2)\n\n        wo = tf.Variable(tf.truncated_normal([hidden_node_count[1], label_count]))\n        bo = tf.Variable(tf.zeros([label_count]))\n\n        logits = tf.matmul(h2, wo) + bo\n        train_prediction = tf.nn.softmax(logits)\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n    num_steps = 1000\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            batch_data = dataset[step * batch_size: (step + 1) * batch_size]\n            batch_labels = train_labels[step * batch_size: (step + 1) * batch_size]\n\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 10 == 0:\n                print(\'=\' * 80)\n                cur_first_data = dataset[step * batch_size: (step + 1) * batch_size][0]\n                print(\'current first data [%f, %f]\' % (cur_first_data[0], cur_first_data[1]))\n                print(\'current first predict: [%f, %f, %f]\' % (predictions[0][0], predictions[0][1], predictions[0][2]))\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n\nif __name__ == \'__main__\':\n    inputs, labels = train_data()\n    dig_nn(inputs, labels, 100, 2, 3)\n'"
src/neural/full_connect.py,60,"b'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom not_mnist.img_pickle import load_pickle\n\n\ndef reformat(dataset, labels, image_size, num_labels):\n    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef accuracy(predictions, labels):\n    return 100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n\n\ndef tf_logist():\n    # With gradient descent training, even this much data is prohibitive.\n    # Subset the training data for faster turnaround.\n    train_subset = 10000\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        # Load the training, validation and test data into constants that are\n        # attached to the graph.\n        tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n        tf_train_labels = tf.constant(train_labels[:train_subset])\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        # These are the parameters that we are going to be training. The weight\n        # matrix will be initialized using random valued following a (truncated)\n        # normal distribution. The biases get initialized to zero.\n        weights = tf.Variable(\n            tf.truncated_normal([image_size * image_size, num_labels]))\n        biases = tf.Variable(tf.zeros([num_labels]))\n\n        # Training computation.\n        # We multiply the inputs with the weight matrix, and add biases. We compute\n        # the softmax and cross-entropy (it\'s one operation in TensorFlow, because\n        # it\'s very common, and it can be optimized). We take the average of this\n        # cross-entropy across all training example: that\'s our loss.\n        logits = tf.matmul(tf_train_dataset, weights) + biases\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        # We are going to find the minimum of this loss using gradient descent.\n        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        # These are not part of training, but merely here so that we can report\n        # accuracy figures as we train.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(\n            tf.matmul(tf_valid_dataset, weights) + biases)\n        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n\n    num_steps = 801\n\n    with tf.Session(graph=graph) as session:\n        # This is a one-time operation which ensures the parameters get initialized as\n        # we described in the graph: random weights for the matrix, zeros for the\n        # biases.\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        for step in range(num_steps):\n            # Run the computations. We tell .run() that we want to run the optimizer,\n            # and get the loss value and the training predictions returned as numpy\n            # arrays.\n            _, l, predictions = session.run([optimizer, loss, train_prediction])\n            if step % 100 == 0:\n                print(\'Loss at step %d: %f\' % (step, l))\n                print(\'Training accuracy: %.1f%%\' % accuracy(\n                    predictions, train_labels[:train_subset, :]))\n                # Calling .eval() on valid_prediction is basically like calling run(), but\n                # just to get that one numpy array. Note that it recomputes all its graph\n                # dependencies.\n                print(\'Validation accuracy: %.1f%%\' % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(\'Test accuracy: %.1f%%\' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef tf_sgd():\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data. For the training data, we use a placeholder that will be fed\n        # at run time with a training minibatch.\n        tf_train_dataset = tf.placeholder(tf.float32,\n                                          shape=(batch_size, image_size * image_size))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        # Variables.\n        weights = tf.Variable(\n            tf.truncated_normal([image_size * image_size, num_labels]))\n        biases = tf.Variable(tf.zeros([num_labels]))\n\n        # Training computation.\n        logits = tf.matmul(tf_train_dataset, weights) + biases\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(\n            tf.matmul(tf_valid_dataset, weights) + biases)\n        test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n\n    num_steps = 3001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            # Pick an offset within the training data, which has been randomized.\n            # Note: we could use better randomization across epochs.\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            # Generate a minibatch.\n            batch_data = train_dataset[offset:(offset + batch_size), :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            # Prepare a dictionary telling the session where to feed the minibatch.\n            # The key of the dictionary is the placeholder node of the graph to be fed,\n            # and the value is the numpy array to feed to it.\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 500 == 0:\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n                print(""Validation accuracy: %.1f%%"" % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n\n\ndef tf_sgd_relu_nn():\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data. For the training data, we use a placeholder that will be fed\n        # at run time with a training minibatch.\n        tf_train_dataset = tf.placeholder(tf.float32,\n                                          shape=(batch_size, image_size * image_size))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        hidden_node_count = 1024\n        # Variables.\n        weights1 = tf.Variable(\n            tf.truncated_normal([image_size * image_size, hidden_node_count]))\n        biases1 = tf.Variable(tf.zeros([hidden_node_count]))\n\n        weights2 = tf.Variable(\n            tf.truncated_normal([hidden_node_count, num_labels]))\n        biases2 = tf.Variable(tf.zeros([num_labels]))\n\n        # Training computation.\n        ys = tf.matmul(tf_train_dataset, weights1) + biases1\n        hidden = tf.nn.relu(ys)\n        logits = tf.matmul(hidden, weights2) + biases2\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n\n        # Optimizer.\n        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(\n            tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n        test_prediction = tf.nn.softmax(\n            tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n\n    num_steps = 3001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            # Pick an offset within the training data, which has been randomized.\n            # Note: we could use better randomization across epochs.\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            # Generate a minibatch.\n            batch_data = train_dataset[offset:(offset + batch_size), :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            # Prepare a dictionary telling the session where to feed the minibatch.\n            # The key of the dictionary is the placeholder node of the graph to be fed,\n            # and the value is the numpy array to feed to it.\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 500 == 0:\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n                print(""Validation accuracy: %.1f%%"" % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n\n\ndef load_reformat_not_mnist(image_size, num_labels):\n    pickle_file = \'../not_mnist/notMNIST_clean.pickle\'\n    save = load_pickle(pickle_file)\n    train_dataset = save[\'train_dataset\']\n    train_labels = save[\'train_labels\']\n    valid_dataset = save[\'valid_dataset\']\n    valid_labels = save[\'valid_labels\']\n    test_dataset = save[\'test_dataset\']\n    test_labels = save[\'test_labels\']\n    del save  # hint to help gc free up memory\n    print(\'Training set\', train_dataset.shape, train_labels.shape)\n    print(\'Validation set\', valid_dataset.shape, valid_labels.shape)\n    print(\'Test set\', test_dataset.shape, test_labels.shape)\n    train_dataset, train_labels = reformat(train_dataset, train_labels, image_size, num_labels)\n    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels, image_size, num_labels)\n    test_dataset, test_labels = reformat(test_dataset, test_labels, image_size, num_labels)\n    print(\'Training set\', train_dataset.shape, train_labels.shape)\n    print(\'Validation set\', valid_dataset.shape, valid_labels.shape)\n    print(\'Test set\', test_dataset.shape, test_labels.shape)\n    return train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels\n\nif __name__ == \'__main__\':\n    # First reload the data we generated in 1_notmnist.ipynb.\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        load_reformat_not_mnist(image_size, num_labels)\n\n    # tf_logist()\n    # tf_sgd()\n    tf_sgd_relu_nn()\n'"
src/neural/nn_overfit.py,79,"b'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom neural.full_connect import load_reformat_not_mnist, accuracy\n\n\ndef tf_better_nn(offset_range=-1, regular=False, drop_out=False, lrd=False):\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data. For the training data, we use a placeholder that will be fed\n        # at run time with a training minibatch.\n        tf_train_dataset = tf.placeholder(tf.float32,\n                                          shape=(batch_size, image_size * image_size))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        hidden_node_count = 1024\n        # Variables.\n        weights1 = tf.Variable(\n            tf.truncated_normal([image_size * image_size, hidden_node_count]))\n        biases1 = tf.Variable(tf.zeros([hidden_node_count]))\n\n        weights2 = tf.Variable(\n            tf.truncated_normal([hidden_node_count, num_labels]))\n        biases2 = tf.Variable(tf.zeros([num_labels]))\n\n        # Training computation. right most\n        ys = tf.matmul(tf_train_dataset, weights1) + biases1\n        hidden = tf.nn.relu(ys)\n        h_fc = hidden\n\n        valid_y0 = tf.matmul(tf_valid_dataset, weights1) + biases1\n        valid_hidden1 = tf.nn.relu(valid_y0)\n\n        test_y0 = tf.matmul(tf_test_dataset, weights1) + biases1\n        test_hidden1 = tf.nn.relu(test_y0)\n\n        # enable DropOut\n        keep_prob = tf.placeholder(tf.float32)\n        if drop_out:\n            hidden_drop = tf.nn.dropout(hidden, keep_prob)\n            h_fc = hidden_drop\n\n        # left most\n        logits = tf.matmul(h_fc, weights2) + biases2\n        # only drop out when train\n        logits_predict = tf.matmul(hidden, weights2) + biases2\n        valid_predict = tf.matmul(valid_hidden1, weights2) + biases2\n        test_predict = tf.matmul(test_hidden1, weights2) + biases2\n        # loss\n        l2_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(biases2)\n        # enable regularization\n        if not regular:\n            l2_loss = 0\n        beta = 0.002\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta * l2_loss\n\n        # Optimizer.\n        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n        if lrd:\n            cur_step = tf.Variable(0)  # count the number of steps taken.\n            starter_learning_rate = 0.1\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 10000, 0.96, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits_predict)\n        valid_prediction = tf.nn.softmax(valid_predict)\n        test_prediction = tf.nn.softmax(test_predict)\n\n    num_steps = 30001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            # Pick an offset within the training data, which has been randomized.\n            # Note: we could use better randomization across epochs.\n            if offset_range == -1:\n                offset_range = train_labels.shape[0] - batch_size\n\n            offset = (step * batch_size) % offset_range\n            # Generate a minibatch.\n            batch_data = train_dataset[offset:(offset + batch_size), :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            # Prepare a dictionary telling the session where to feed the minibatch.\n            # The key of the dictionary is the placeholder node of the graph to be fed,\n            # and the value is the numpy array to feed to it.\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels, keep_prob: 0.5}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 500 == 0:\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n                print(""Validation accuracy: %.1f%%"" % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n\n\ndef tf_deep_nn(regular=False, drop_out=False, lrd=False, layer_cnt=2):\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        hidden_node_count = 1024\n        # start weight\n        hidden_stddev = np.sqrt(2.0 / 784)\n        weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_node_count], stddev=hidden_stddev))\n        biases1 = tf.Variable(tf.zeros([hidden_node_count]))\n        # middle weight\n        weights = []\n        biases = []\n        hidden_cur_cnt = hidden_node_count\n        for i in range(layer_cnt - 2):\n            if hidden_cur_cnt > 2:\n                hidden_next_cnt = int(hidden_cur_cnt / 2)\n            else:\n                hidden_next_cnt = 2\n            hidden_stddev = np.sqrt(2.0 / hidden_cur_cnt)\n            weights.append(tf.Variable(tf.truncated_normal([hidden_cur_cnt, hidden_next_cnt], stddev=hidden_stddev)))\n            biases.append(tf.Variable(tf.zeros([hidden_next_cnt])))\n            hidden_cur_cnt = hidden_next_cnt\n        # first wx + b\n        y0 = tf.matmul(tf_train_dataset, weights1) + biases1\n        # first relu\n        hidden = tf.nn.relu(y0)\n        hidden_drop = hidden\n        # first DropOut\n        keep_prob = 0.5\n        if drop_out:\n            hidden_drop = tf.nn.dropout(hidden, keep_prob)\n        # first wx+b for valid\n        valid_y0 = tf.matmul(tf_valid_dataset, weights1) + biases1\n        valid_hidden = tf.nn.relu(valid_y0)\n        # first wx+b for test\n        test_y0 = tf.matmul(tf_test_dataset, weights1) + biases1\n        test_hidden = tf.nn.relu(test_y0)\n\n        # middle layer\n        for i in range(layer_cnt - 2):\n            y1 = tf.matmul(hidden_drop, weights[i]) + biases[i]\n            hidden_drop = tf.nn.relu(y1)\n            if drop_out:\n                keep_prob += 0.5 * i / (layer_cnt + 1)\n                hidden_drop = tf.nn.dropout(hidden_drop, keep_prob)\n\n            y0 = tf.matmul(hidden, weights[i]) + biases[i]\n            hidden = tf.nn.relu(y0)\n\n            valid_y0 = tf.matmul(valid_hidden, weights[i]) + biases[i]\n            valid_hidden = tf.nn.relu(valid_y0)\n\n            test_y0 = tf.matmul(test_hidden, weights[i]) + biases[i]\n            test_hidden = tf.nn.relu(test_y0)\n\n        # last weight\n        weights2 = tf.Variable(tf.truncated_normal([hidden_cur_cnt, num_labels], stddev=hidden_stddev / 2))\n        biases2 = tf.Variable(tf.zeros([num_labels]))\n        # last wx + b\n        logits = tf.matmul(hidden_drop, weights2) + biases2\n\n        # predicts\n        logits_predict = tf.matmul(hidden, weights2) + biases2\n        valid_predict = tf.matmul(valid_hidden, weights2) + biases2\n        test_predict = tf.matmul(test_hidden, weights2) + biases2\n\n        l2_loss = 0\n        # enable regularization\n        if regular:\n            l2_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n            for i in range(len(weights)):\n                l2_loss += tf.nn.l2_loss(weights[i])\n                # l2_loss += tf.nn.l2_loss(biases[i])\n            beta = 0.25 / batch_size\n            beta = 1e-5\n            l2_loss *= beta\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + l2_loss\n\n        # Optimizer.\n        if lrd:\n            cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n            starter_learning_rate = 0.4\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 100000, 0.96, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits_predict)\n        valid_prediction = tf.nn.softmax(valid_predict)\n        test_prediction = tf.nn.softmax(test_predict)\n\n    num_steps = 20001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            offset_range = train_labels.shape[0] - batch_size\n            offset = (step * batch_size) % offset_range\n            batch_data = train_dataset[offset:(offset + batch_size), :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 500 == 0:\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n                print(""Validation accuracy: %.1f%%"" % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n\nif __name__ == \'__main__\':\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        load_reformat_not_mnist(image_size, num_labels)\n    # tf_better_nn(regular=True)\n    # tf_better_nn(offset_range=1000)\n    # tf_better_nn(offset_range=1000, drop_out=True)\n    # tf_better_nn(lrd=True)\n    tf_deep_nn(layer_cnt=6, lrd=True, drop_out=True, regular=True)\n'"
src/not_mnist/__init__.py,0,b''
src/not_mnist/clean_overlap.py,0,"b""import cPickle as pickle\nimport os\nimport numpy as np\n\nfrom not_mnist.img_pickle import load_pickle, save_obj\n\nimage_size = 28  # Pixel width and height.\n\n\ndef img_diff(pix_s1, pix_s2):  # by pixels\n    dif_cnt = 0\n    height = image_size\n    width = image_size\n    total = width * height\n    for x in range(height):\n        for y in range(width):\n            if pix_s1[x][y] != pix_s2[x][y]:\n                dif_cnt += 1\n    return float(dif_cnt) / float(total)\n\n\ndef test_img_diff():\n    img1 = [[x for x in range(20)] for y in range(28)]\n    img2 = [[x for x in range(20)] for y in range(28)]\n    print(img_diff(img1, img2))\n\n\ndef img_in(img, imgs):\n    for i, img2 in enumerate(imgs):\n        if img_diff(img, img2) < 0.1:\n            return True\n    return False\n\n\ndef BKDRHash(string):\n    seed = 131\n    hash = 0\n    for ch in string:\n        hash = hash * seed + ord(ch)\n    return hash & 0x7FFFFFFF\n\n\ndef img_hash(pix_s):\n    seed = 131\n    v_hash = 0\n    for row in pix_s:\n        for p in row:\n            v_hash = v_hash * seed + int(p * 255)\n    return v_hash & 0x7FFFFFFF\n\n\ndef imgs_except(left, right):\n    return filter(lambda img: not img_in(img, right), left)\n\n\ndef test_imgs_diff():\n    img1 = [[x for x in range(20)] for y in range(28)]\n    img2 = [[x for x in range(20)] for y in range(28)]\n    img3 = [[x for x in range(20)] for y in range(28)]\n\n    print(len(imgs_except([img2, img3], [img1])))\n\n\ndef imgs_idx_except(left, right):\n    except_idxs = []\n    imgs = []\n    for i in range(len(left)):\n        print('compare left[%d] to right' % i)\n        # about 2-3 seconds for one compare between left[i] and all right\n        if img_in(left[i], right):\n            except_idxs.append(i)\n            imgs.append(left[i])\n    return except_idxs, imgs\n\n\ndef imgs_idx_hash_except(left, right):\n    except_idxs = []\n    right_hashes = [img_hash(img) for img in right]\n    print len(right_hashes)\n    for i in range(len(left)):\n        if img_hash(left[i]) in right_hashes:\n            print('compare left[%d] to right found the same' % i)\n            except_idxs.append(i)\n    res = np.delete(left, except_idxs, axis=0)\n    return except_idxs, res\n\n\ndef list_except(objs, idxs):\n    new_objs = []\n    for i in range(len(objs)):\n        if i not in idxs:\n            new_objs.append(objs[i])\n    return new_objs\n\n\ndef clean():\n    datasets = load_pickle('notMNIST.pickle')\n    test_dataset = datasets['test_dataset']\n    test_labels = datasets['test_labels']\n    print('test_dataset:%d' % len(test_dataset))\n    print('test_labels:%d' % len(test_labels))\n\n    except_valid_idx, valid_dataset = imgs_idx_hash_except(datasets['valid_dataset'], test_dataset)\n    valid_labels = np.delete(datasets['valid_labels'], except_valid_idx)\n    print('valid_dataset:%d' % len(valid_dataset))\n    print('valid_labels:%d' % len(valid_labels))\n\n    # except with valid_dataset\n    except_train_idx, train_dataset = imgs_idx_hash_except(datasets['train_dataset'], valid_dataset)\n    train_labels = np.delete(datasets['train_labels'], except_train_idx)\n    # except with test_dataset\n    except_train_idx, train_dataset = imgs_idx_hash_except(train_dataset, test_dataset)\n    train_labels = np.delete(train_labels, except_train_idx)\n\n    print('train_dataset:%d' % len(train_dataset))\n    print('train_labels:%d' % len(train_labels))\n    print('valid_dataset:%d' % len(valid_dataset))\n    print('valid_labels:%d' % len(valid_labels))\n    print('test_dataset:%d' % len(test_dataset))\n    print('test_labels:%d' % len(test_labels))\n\n    pickle_file = 'notMNIST_clean.pickle'\n    save = {\n        'train_dataset': train_dataset,\n        'train_labels': train_labels,\n        'valid_dataset': valid_dataset,\n        'valid_labels': valid_labels,\n        'test_dataset': test_dataset,\n        'test_labels': test_labels,\n    }\n    save_obj(pickle_file, save)\n\n\nif __name__ == '__main__':\n    clean()\n"""
src/not_mnist/extract.py,0,"b""from __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\n\n# from six.moves.urllib.request import urlretrieve\n# from six.moves import cPickle as pickle\n\nnum_classes = 10\nnp.random.seed(133)\n\n\ndef maybe_extract(filename, force=False):\n    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n    if os.path.isdir(root) and not force:\n        # You may override by setting force=True.\n        print('%s already present - Skipping extraction of %s.' % (root, filename))\n    else:\n        print('Extracting data for %s. This may take a while. Please wait.' % root)\n        tar = tarfile.open(filename)\n        sys.stdout.flush()\n        tar.extractall()\n        tar.close()\n    data_folders = [\n        os.path.join(root, d) for d in sorted(os.listdir(root))\n        if os.path.isdir(os.path.join(root, d))]\n    if len(data_folders) != num_classes:\n        raise Exception(\n            'Expected %d folders, one per class. Found %d instead.' % (\n                num_classes, len(data_folders)))\n    print(data_folders)\n    return data_folders\n\nif __name__ == '__main__':\n    train_folders = maybe_extract('notMNIST_large.tar.gz')\n    test_folders = maybe_extract('notMNIST_small.tar.gz')\n"""
src/not_mnist/img_pickle.py,0,"b'from __future__ import print_function\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import ndimage\n# from six.moves.urllib.request import urlretrieve\n# from six.moves import cPickle as pickle\nimport cPickle as pickle\n\nimage_size = 28  # Pixel width and height.\npixel_depth = 255.0  # Number of levels per pixel.\n\n\ndef load_letter(folder, min_num_images):\n    """"""Load the data for a single letter label.""""""\n    image_files = os.listdir(folder)\n    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n                         dtype=np.float32)\n    print(folder)\n    for image_index, image in enumerate(image_files):\n        image_file = os.path.join(folder, image)\n        try:\n            image_data = (ndimage.imread(image_file).astype(float) -\n                          pixel_depth / 2) / pixel_depth\n            if image_data.shape != (image_size, image_size):\n                raise Exception(\'Unexpected image shape: %s\' % str(image_data.shape))\n            dataset[image_index, :, :] = image_data\n        except IOError as e:\n            print(\'Could not read:\', image_file, \':\', e, \'- it\\\'s ok, skipping.\')\n\n    num_images = image_index + 1\n    dataset = dataset[0:num_images, :, :]\n    if num_images < min_num_images:\n        raise Exception(\'Many fewer images than expected: %d < %d\' %\n                        (num_images, min_num_images))\n\n    print(\'Full dataset tensor:\', dataset.shape)\n    print(\'Mean:\', np.mean(dataset))\n    print(\'Standard deviation:\', np.std(dataset))\n    return dataset\n\n\ndef maybe_pickle(data_folders, min_num_images_per_class, force=False):\n    dataset_names = []\n    for folder in data_folders:\n        set_filename = folder + \'.pickle\'\n        dataset_names.append(set_filename)\n        if os.path.exists(set_filename) and not force:\n            # You may override by setting force=True.\n            print(\'%s already present - Skipping pickling.\' % set_filename)\n        else:\n            print(\'Pickling %s.\' % set_filename)\n            dataset = load_letter(folder, min_num_images_per_class)\n            try:\n                with open(set_filename, \'wb\') as f:\n                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n\n            except Exception as e:\n                print(\'Unable to save data to\', set_filename, \':\', e)\n\n    return dataset_names\n\n\ndef show_imgs(imgs, show_max=-1):\n    show_cnt = show_max\n    if show_max == -1:\n        show_cnt = len(imgs)\n\n    for image_index in xrange(show_cnt):\n        # they are binary images, if RGBs, don\'t add cmap=""Graeys""\n        plt.imshow(imgs[image_index], cmap=""Greys"")\n        plt.show()\n\n\ndef load_pickle(pickle_name):\n    # load a pickle file to memory\n    if os.path.exists(pickle_name):\n        return pickle.load(open(pickle_name, ""r""))\n    return None\n\n\ndef save_obj(pickle_file, obj):\n    try:\n        f = open(pickle_file, \'wb\')\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n        f.close()\n    except Exception as e:\n        print(\'Unable to save data to\', pickle_file, \':\', e)\n        raise\n    statinfo = os.stat(pickle_file)\n    print(\'Compressed pickle size:\', statinfo.st_size)\n\nif __name__ == \'__main__\':\n    train_folders = [\'notMNIST_large/A\', \'notMNIST_large/B\', \'notMNIST_large/C\', \'notMNIST_large/D\', \'notMNIST_large/E\',\n                     \'notMNIST_large/F\', \'notMNIST_large/G\', \'notMNIST_large/H\', \'notMNIST_large/I\', \'notMNIST_large/J\']\n    test_folders = [\'notMNIST_small/A\', \'notMNIST_small/B\', \'notMNIST_small/C\', \'notMNIST_small/D\', \'notMNIST_small/E\',\n                    \'notMNIST_small/F\', \'notMNIST_small/G\', \'notMNIST_small/H\', \'notMNIST_small/I\', \'notMNIST_small/J\']\n    train_datasets = maybe_pickle(train_folders, 45000)\n    test_datasets = maybe_pickle(test_folders, 1800)\n\n    for i in range(1): # only load a.pickle\n        imgs = load_pickle(train_datasets[i])\n        show_imgs(imgs, 3)\n\n'"
src/not_mnist/load_data.py,0,"b'from __future__ import print_function\n\nimport os\nimport sys\n\n# from six.moves.urllib.request import urlretrieve\n# from six.moves import cPickle as pickle\nfrom urllib import urlretrieve\n\n# %matplotlib inline\n\n# url = \'http://commondatastorage.googleapis.com/books1000/\'\n# if the url above can\'t work, use this:\n\nlast_percent_reported = None\n\n# First, we\'ll download the dataset to our local machine.\n# The data consists of characters rendered in a variety of fonts on a 28x28 image.\n# The labels are limited to \'A\' through \'J\' (10 classes).\n# The training set has about 500k and the testset 19000 labelled examples.\n# Given these sizes, it should be possible to train models quickly on any machine.\n\n\ndef download_progress_hook(count, blockSize, totalSize):\n    """"""A hook to report the progress of a download. This is mostly intended for users with\n    slow internet connections. Reports every 1% change in download progress.\n    """"""\n    global last_percent_reported\n    percent = int(count * blockSize * 100 / totalSize)\n\n    if last_percent_reported != percent:\n        if percent % 5 == 0:\n            sys.stdout.write(""%s%%"" % percent)\n            sys.stdout.flush()\n        else:\n            sys.stdout.write(""."")\n            sys.stdout.flush()\n\n        last_percent_reported = percent\n\n\ndef maybe_download(filename, expected_bytes, url=\'https://commondatastorage.googleapis.com/books1000/\', force=False, ):\n    """"""Download a file if not present, and make sure it\'s the right size.""""""\n    if force or not os.path.exists(filename):\n        print(\'Attempting to download:\', filename)\n        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n        print(\'\\nDownload Complete!\')\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print(\'Found and verified\', filename)\n    else:\n        raise Exception(\n            \'Failed to verify \' + filename + \'. Can you get to it with a browser?\')\n    return filename\n\n\nif __name__ == \'__main__\':\n    train_filename = maybe_download(\'notMNIST_large.tar.gz\', 247336696)\n    test_filename = maybe_download(\'notMNIST_small.tar.gz\', 8458043)\n\n'"
src/not_mnist/logistic_train.py,0,"b'from __future__ import print_function\n\nimport os\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom not_mnist.img_pickle import load_pickle, save_obj\n\n\ndef load_train():\n    datasets = load_pickle(\'notMNIST_clean.pickle\')\n    train_dataset = datasets[\'train_dataset\']\n    train_labels = datasets[\'train_labels\']\n    valid_dataset = datasets[\'valid_dataset\']\n    valid_labels = datasets[\'valid_labels\']\n\n    classifier_name = \'classifier.pickle\'\n\n    if os.path.exists(classifier_name):\n        classifier = load_pickle(classifier_name)\n    else:\n        classifier = LogisticRegression()\n        classifier.fit(train_dataset.reshape(train_dataset.shape[0], -1), train_labels)\n        save_obj(classifier_name, classifier)\n\n    # simple valid\n    valid_idx_s = 3000\n    valid_idx_e = 3014\n    x = classifier.predict(valid_dataset.reshape(valid_dataset.shape[0], -1)[valid_idx_s: valid_idx_e])\n    print(x)\n    print(valid_labels[valid_idx_s:valid_idx_e])\n\n    # whole valid\n    x = classifier.predict(valid_dataset.reshape(valid_dataset.shape[0], -1))\n    fail_cnt = 0\n    for i, pred in enumerate(x):\n        if pred != valid_labels[i]:\n            fail_cnt += 1\n    print(""success rate:"" + str((1 - float(fail_cnt) / len(x)) * 100) + ""%"")\n\nif __name__ == \'__main__\':\n    load_train()\n'"
src/not_mnist/merge_prune.py,0,"b'from __future__ import print_function\n\nimport os\n\nimport numpy as np\n# from six.moves.urllib.request import urlretrieve\n# from six.moves import cPickle as pickle\nimport cPickle as pickle\n\nfrom not_mnist.img_pickle import maybe_pickle, save_obj\n\nimage_size = 28  # Pixel width and height.\n\n\ndef make_arrays(nb_rows, img_size):\n    if nb_rows:\n        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n        labels = np.ndarray(nb_rows, dtype=np.int32)\n    else:\n        dataset, labels = None, None\n    return dataset, labels\n\n\ndef merge_datasets(pickle_files, train_size, valid_size=0):\n    num_classes = len(pickle_files)\n    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n    train_dataset, train_labels = make_arrays(train_size, image_size)\n    vsize_per_class = valid_size // num_classes\n    tsize_per_class = train_size // num_classes\n\n    start_v, start_t = 0, 0\n    end_v, end_t = vsize_per_class, tsize_per_class\n    end_l = vsize_per_class + tsize_per_class\n    for label, pickle_file in enumerate(pickle_files):\n        try:\n            with open(pickle_file, \'rb\') as f:\n                letter_set = pickle.load(f)\n                # let\'s shuffle the letters to have random validation and training set\n                np.random.shuffle(letter_set)\n                if valid_dataset is not None:  # None for test dataSet\n                    valid_letter = letter_set[:vsize_per_class, :, :]\n                    valid_dataset[start_v:end_v, :, :] = valid_letter\n                    valid_labels[start_v:end_v] = label\n                    start_v += vsize_per_class\n                    end_v += vsize_per_class\n\n                train_letter = letter_set[vsize_per_class:end_l, :, :]\n                train_dataset[start_t:end_t, :, :] = train_letter\n                train_labels[start_t:end_t] = label\n                start_t += tsize_per_class\n                end_t += tsize_per_class\n        except Exception as e:\n            print(\'Unable to process data from\', pickle_file, \':\', e)\n            raise\n\n    return valid_dataset, valid_labels, train_dataset, train_labels\n\n\ndef randomize(dataset, labels):\n    permutation = np.random.permutation(labels.shape[0])\n    shuffled_dataset = dataset[permutation, :, :]\n    shuffled_labels = labels[permutation]\n    return shuffled_dataset, shuffled_labels\n\n\ndef merge_prune(train_folders, test_folders):\n    train_datasets = maybe_pickle(train_folders, 45000)\n    test_datasets = maybe_pickle(test_folders, 1800)\n\n    train_size = 200000\n    valid_size = 10000\n    test_size = 10000\n\n    valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n        train_datasets, train_size, valid_size)\n    _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n\n    print(\'Training:\', train_dataset.shape, train_labels.shape)\n    print(\'Validation:\', valid_dataset.shape, valid_labels.shape)\n    print(\'Testing:\', test_dataset.shape, test_labels.shape)\n\n    train_dataset, train_labels = randomize(train_dataset, train_labels)\n    test_dataset, test_labels = randomize(test_dataset, test_labels)\n    valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n\n    pickle_file = \'notMNIST.pickle\'\n    save = {\n        \'train_dataset\': train_dataset,\n        \'train_labels\': train_labels,\n        \'valid_dataset\': valid_dataset,\n        \'valid_labels\': valid_labels,\n        \'test_dataset\': test_dataset,\n        \'test_labels\': test_labels,\n    }\n    save_obj(pickle_file, save)\n\n\nif __name__ == ""__main__"":\n    train_folders = [\'notMNIST_large/A\', \'notMNIST_large/B\', \'notMNIST_large/C\', \'notMNIST_large/D\', \'notMNIST_large/E\',\n                     \'notMNIST_large/F\', \'notMNIST_large/G\', \'notMNIST_large/H\', \'notMNIST_large/I\', \'notMNIST_large/J\']\n    test_folders = [\'notMNIST_small/A\', \'notMNIST_small/B\', \'notMNIST_small/C\', \'notMNIST_small/D\', \'notMNIST_small/E\',\n                    \'notMNIST_small/F\', \'notMNIST_small/G\', \'notMNIST_small/H\', \'notMNIST_small/I\', \'notMNIST_small/J\']\n    merge_prune(train_folders, test_folders)\n'"
src/not_mnist/pick.py,0,"b'import os\n\nimport cPickle as pickle\n\n\ndef save_obj(pickle_file, obj):\n    try:\n        f = open(pickle_file, \'wb\')\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n        f.close()\n    except Exception as e:\n        print(\'Unable to save data to\', pickle_file, \':\', e)\n        raise\n    statinfo = os.stat(pickle_file)\n    print(\'Compressed pickle size:\', statinfo.st_size)\n\n\ndef load_pickle(pickle_name):\n    # load a pickle file to memory\n    if os.path.exists(pickle_name):\n        return pickle.load(open(pickle_name, ""r""))\n    return None\n'"
src/not_mnist/schedule.py,0,"b""from not_mnist.clean_overlap import clean\nfrom not_mnist.extract import maybe_extract\nfrom not_mnist.img_pickle import maybe_pickle, save_obj\nfrom not_mnist.load_data import maybe_download\nfrom not_mnist.logistic_train import load_train\nfrom not_mnist.merge_prune import merge_datasets, randomize, merge_prune\n\ntrain_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\ntest_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n\ntrain_folders = maybe_extract(train_filename)\ntest_folders = maybe_extract(test_filename)\n\ntrain_datasets = maybe_pickle(train_folders, 45000)\ntest_datasets = maybe_pickle(test_folders, 1800)\n\ntrain_size = 200000\nvalid_size = 10000\ntest_size = 10000\n\nvalid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n    train_datasets, train_size, valid_size)\n_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n\nmerge_prune(train_folders, test_folders)\n\nprint('Training:', train_dataset.shape, train_labels.shape)\nprint('Validation:', valid_dataset.shape, valid_labels.shape)\nprint('Testing:', test_dataset.shape, test_labels.shape)\nclean()\nload_train()\n"""
src/optimize/__init__.py,0,b''
src/optimize/cnn_half_optimize.py,36,"b""from __future__ import print_function\n\nimport random\n\nimport tensorflow as tf\n\nfrom convnet.conv_mnist import maxpool2d\nfrom neural.full_connect import accuracy\nfrom util.mnist import format_mnist\nfrom util.request import half_trend_hyper\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps):\n    batch_size = basic_hps['batch_size']\n    patch_size = basic_hps['patch_size']\n    depth = basic_hps['depth']\n    if depth < 2:\n        depth = 2\n    num_hidden = basic_hps['num_hidden']\n    if num_hidden < 8:\n        num_hidden = 8\n\n    num_channels = 1\n    layer_cnt = basic_hps['layer_sum']\n    starter_learning_rate = basic_hps['starter_learning_rate']\n    loss_collect = list()\n    first_hidden_num = num_hidden\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding='SAME')\n            print(conv)\n            conv = maxpool2d(conv)\n            print(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    hid_shape = hidden.get_shape()\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding='SAME')\n                print(conv)\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                print(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 1001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        mean_loss = 0\n        for step in range(num_steps):\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 10 == 0 and step != 0:\n                mean_loss /= 10.0\n                if step % 50 == 0:\n                    loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step % 500 == 0:\n                    print('Minibatch loss at step %d: %f' % (step, l))\n                    print('Validation accuracy: %.1f%%' % accuracy(\n                        valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n        hypers = half_trend_hyper([batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n        print(hypers)\n        for i in range(len(hypers)):\n            if hypers[i] <= 1.0:\n                hypers[i] = 1\n            else:\n                hypers[i] = int(hypers[i])\n        hypers.append(starter_learning_rate)\n    return hypers\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print('all hp is one, change:')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(3, 10)\n        hps[5] = 0.1\n        hps[3] = 2\n        hps[2] *= 10\n        print(hps)\n    return True\n\n\ndef fit_better():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        'batch_size': 10,\n        'patch_size': 10,\n        'depth': 10,\n        'num_hidden': 10,\n        'layer_sum': 3,\n        'starter_learning_rate': 0.1\n    }\n    if basic_hypers['patch_size'] > 28:\n        basic_hypers['patch_size'] = 28\n    if basic_hypers['layer_sum'] > 3:\n        basic_hypers['layer_sum'] = 3\n    print('=' * 80)\n    print(basic_hypers)\n\n    stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers['layer_sum'])]\n    better_hps = conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n                                 image_size, num_labels, basic_hypers, stride_params)\n    while valid_hp(better_hps):\n        basic_hypers = {\n            'batch_size': better_hps[0],\n            'patch_size': better_hps[4],\n            'depth': better_hps[1],\n            'num_hidden': better_hps[2],\n            'layer_sum': better_hps[3],\n            'starter_learning_rate': better_hps[5]\n        }\n        # if basic_hypers['batch_size'] < 10:\n        #     basic_hypers['batch_size'] = 10\n        if basic_hypers['patch_size'] > 28:\n            basic_hypers['patch_size'] = 28\n        if basic_hypers['layer_sum'] > 3:\n            basic_hypers['layer_sum'] = 3\n        if basic_hypers['patch_size'] <= basic_hypers['layer_sum']:\n            basic_hypers['patch_size'] = basic_hypers['layer_sum']\n        print('=' * 80)\n        print(basic_hypers)\n        stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers['layer_sum'])]\n        better_hps = conv_train(\n            train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n            image_size, num_labels, basic_hypers, stride_params)\n    else:\n        print('can not find better hypers')\n\n\nif __name__ == '__main__':\n    fit_better()\n"""
src/optimize/cnn_long_optimize.py,34,"b""from __future__ import print_function\n\nimport random\n\nfrom convnet.conv_mnist import maxpool2d\nfrom neural.full_connect import accuracy\nfrom util.file_helper import read2mem\nfrom util.mnist import format_mnist\n\nimport tensorflow as tf\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps):\n    batch_size = basic_hps['batch_size']\n    patch_size = basic_hps['patch_size']\n    depth = basic_hps['depth']\n    num_hidden = basic_hps['num_hidden']\n    num_channels = 1\n    layer_cnt = basic_hps['layer_sum']\n    starter_learning_rate = basic_hps['starter_learning_rate']\n    loss_collect = list()\n    first_hidden_num = basic_hps['num_hidden']\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    hid_shape = hidden.get_shape()\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding='SAME')\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n    num_steps = 1001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        end_train = False\n        mean_loss = 0\n        for step in range(num_steps):\n            if end_train:\n                break\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 10 == 0:\n                mean_loss /= 5.0\n                mean_loss = 0\n                if step % 100 == 0:\n                    loss_collect.append(mean_loss)\n                    print('Minibatch loss at step %d: %f' % (step, l))\n                    print('Validation accuracy: %.1f%%' % accuracy(\n                        valid_prediction.eval(), valid_labels))\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print('all hp is one, change:')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(0, 10)\n        print(hps)\n    return True\n\n\ndef etc_hp():\n    hps = [list() for _ in range(5)]\n    hps[0] = read2mem('/home/cwh/Mission/coding/slides/hp2trend/hp2trend_hps0.txt').split()\n    hps[1] = read2mem('/home/cwh/Mission/coding/slides/hp2trend/hp2trend_hps1.txt').split()\n    hps[2] = read2mem('/home/cwh/Mission/coding/slides/hp2trend/hp2trend_hps2.txt').split()\n    hps[3] = read2mem('/home/cwh/Mission/coding/slides/hp2trend/hp2trend_hps3.txt').split()\n    hps[4] = read2mem('/home/cwh/Mission/coding/slides/hp2trend/hp2trend_hps4.txt').split()\n    format_hps = [[hps[0][i], hps[1][i], hps[2][i], hps[3][i], hps[4][i]] for i in range(len(hps[0]))]\n    return format_hps\n\n\ndef fit_better():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    better_hps_list = etc_hp()\n    for better_hps in better_hps_list:\n        basic_hypers = {\n            'batch_size': int(float(better_hps[0])),\n            'patch_size': int(float(better_hps[4])),\n            'depth': int(float(better_hps[1])),\n            'num_hidden': int(float(better_hps[2])),\n            'layer_sum': int(float(better_hps[3])),\n            'starter_learning_rate': 0.1\n        }\n        # if basic_hypers['batch_size'] < 10:\n        #     basic_hypers['batch_size'] = 10\n        if basic_hypers['patch_size'] > 28:\n            basic_hypers['patch_size'] = 28\n        print('=' * 80)\n        print(basic_hypers)\n        stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers['layer_sum'])]\n        conv_train(\n            train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n            image_size, num_labels, basic_hypers, stride_params)\n    else:\n        print('can not find better hypers')\n\n\nif __name__ == '__main__':\n    fit_better()\n"""
src/optimize/cnn_optimize.py,39,"b'from __future__ import print_function\n\nimport random\n\nfrom convnet.conv_mnist import maxpool2d, load_reformat_not_mnist\nfrom neural.full_connect import accuracy\nfrom util.mnist import format_mnist\nfrom util.request import fit_loss, better_hyper\n\nimport tensorflow as tf\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps, lrd=False, get_grad=False):\n    batch_size = basic_hps[\'batch_size\']\n    patch_size = basic_hps[\'patch_size\']\n    depth = basic_hps[\'depth\']\n    num_hidden = basic_hps[\'num_hidden\']\n    num_channels = 1\n    layer_cnt = basic_hps[\'layer_sum\']\n    loss_collect = list()\n    first_hidden_num = basic_hps[\'num_hidden\']\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding=\'SAME\')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    # avoid filter shape larger than input shape\n                    hid_shape = hidden.get_shape()\n                    # print(hid_shape)\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    # print(filter_w)\n                    # print(filter_h)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    # print(""is not large data"")\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                # print(stride_ps[i + 1])\n                # print(len(stride_ps))\n                # print(i + 1)\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding=\'SAME\')\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        # Optimizer.\n        starter_learning_rate = 0.1\n        if lrd:\n            cur_step = tf.Variable(0)  # count the number of steps taken.\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 10000, 0.96, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 3001\n    start_fit = 600\n    init_loss = []\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        end_train = False\n        mean_loss = 0\n        for step in range(num_steps):\n            if end_train:\n                break\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 5 == 0:\n                mean_loss /= 5.0\n                loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step >= start_fit:\n                    # print(loss_collect)\n                    if step == start_fit:\n                        res = fit_loss(1, [batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n                    else:\n                        res = fit_loss(0, [batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n                    if get_grad:\n                        better_hyper([batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n                    loss_collect.remove(loss_collect[0])\n                    ret = res[\'ret\']\n                    if ret == 1 and not get_grad:\n                        print(\'ret is end train when step is {step}\'.format(step=step))\n                        init_loss.append(loss_collect)\n                        end_train = True\n\n                        if step % 50 == 0:\n                            print(\'Minibatch loss at step %d: %f\' % (step, l))\n                            print(\'Validation accuracy: %.1f%%\' % accuracy(\n                                valid_prediction.eval(), valid_labels))\n\n        print(\'Test accuracy: %.1f%%\' % accuracy(test_prediction.eval(), test_labels))\n        if end_train:\n            hypers = better_hyper([batch_size, depth, num_hidden, layer_cnt, patch_size], init_loss[0])\n            print(hypers)\n            for i in range(len(hypers)):\n                if hypers[i] <= 1.0:\n                    hypers[i] = 1\n                else:\n                    hypers[i] = int(hypers[i])\n        else:\n            hypers = [batch_size, depth, num_hidden, layer_cnt, patch_size]\n    return end_train, hypers\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print(\'all hp is one, change:\')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(0, 10)\n        print(hps)\n    return True\n\n\ndef fit_better():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        \'batch_size\': 10,\n        \'patch_size\': 10,\n        \'depth\': 10,\n        \'num_hidden\': 10,\n        \'layer_sum\': 3\n    }\n    if basic_hypers[\'patch_size\'] > 28:\n        basic_hypers[\'patch_size\'] = 28\n\n    print(\'=\' * 80)\n    print(basic_hypers)\n\n    stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers[\'layer_sum\'])]\n    ret, better_hps = conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n                                 image_size, num_labels, basic_hypers, stride_params, lrd=True)\n    while ret and valid_hp(better_hps):\n        basic_hypers = {\n            \'batch_size\': better_hps[0],\n            \'patch_size\': better_hps[4],\n            \'depth\': better_hps[1],\n            \'num_hidden\': better_hps[2],\n            \'layer_sum\': better_hps[3]\n        }\n        # if basic_hypers[\'batch_size\'] < 10:\n        #     basic_hypers[\'batch_size\'] = 10\n        if basic_hypers[\'patch_size\'] > 28:\n            basic_hypers[\'patch_size\'] = 28\n        print(\'=\' * 80)\n        print(basic_hypers)\n        stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers[\'layer_sum\'])]\n        ret, better_hps = conv_train(\n            train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n            image_size, num_labels, basic_hypers, stride_params, lrd=True)\n    else:\n        print(\'can not find better hypers\')\n\n\nif __name__ == \'__main__\':\n    fit_better()\n'"
src/optimize/cnn_prophet.py,39,"b'from __future__ import print_function\n\nimport random\n\nfrom convnet.conv_mnist import maxpool2d, load_reformat_not_mnist\nfrom neural.full_connect import accuracy\nfrom util import file_helper\nfrom util.mnist import format_mnist\nfrom util.request import fit_loss, better_hyper, predict_future, fit_more\n\nimport tensorflow as tf\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps, lrd=False):\n    batch_size = basic_hps[\'batch_size\']\n    patch_size = basic_hps[\'patch_size\']\n    depth = basic_hps[\'depth\']\n    num_hidden = basic_hps[\'num_hidden\']\n    num_channels = 1\n    layer_cnt = basic_hps[\'layer_sum\']\n    loss_collect = list()\n    first_hidden_num = basic_hps[\'num_hidden\']\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding=\'SAME\')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    # avoid filter shape larger than input shape\n                    hid_shape = hidden.get_shape()\n                    # print(hid_shape)\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    # print(filter_w)\n                    # print(filter_h)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    # print(""is not large data"")\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                # print(stride_ps[i + 1])\n                # print(len(stride_ps))\n                # print(i + 1)\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding=\'SAME\')\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        # Optimizer.\n        starter_learning_rate = 0.1\n        if lrd:\n            cur_step = tf.Variable(0)  # count the number of steps taken.\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 10000, 0.96, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 3001\n    start_fit = 600\n    init_loss = []\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        end_train = False\n        mean_loss = 0\n        for step in range(num_steps):\n            if end_train:\n                break\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 5 == 0:\n                mean_loss /= 5.0\n                loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step >= start_fit:\n                    # print(loss_collect)\n                    if step == start_fit:\n                        res = fit_more(1, [batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n                    else:\n                        res = fit_more(0, [batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n                    loss_collect.remove(loss_collect[0])\n                    ret = res[\'ret\']\n                    if ret == 1:\n                        print(\'ret is end train when step is {step}\'.format(step=step))\n                        init_loss.append(loss_collect)\n                        more_index = predict_future([batch_size, depth, num_hidden, layer_cnt, patch_size], init_loss[0])\n                        print(\'more index: %d\' % more_index)\n                        for i in range(more_index):\n                            offset = ((step + i + 1) * batch_size) % (train_labels.shape[0] - batch_size)\n                            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n                            batch_labels = train_labels[offset:(offset + batch_size), :]\n                            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n                            _, l, predictions = session.run(\n                                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n                            loss_collect.append(l)\n                            file_helper.write(\'/home/cwh/coding/python/NN/line.txt\', str(loss_collect[20]))\n                            loss_collect.remove(loss_collect[0])\n                        for loss in loss_collect[21:]:\n                            file_helper.write(\'/home/cwh/coding/python/NN/line.txt\', str(loss))\n                        end_train = True\n\n                        file_helper.write(\'/home/cwh/coding/python/NN/line.txt\', \'===\')\n                    if step % 50 == 0:\n                        print(\'Minibatch loss at step %d: %f\' % (step, l))\n                        print(\'Validation accuracy: %.1f%%\' % accuracy(\n                            valid_prediction.eval(), valid_labels))\n\n        print(\'Test accuracy: %.1f%%\' % accuracy(test_prediction.eval(), test_labels))\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print(\'all hp is one, change:\')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(0, 10)\n        print(hps)\n    return True\n\n\ndef fit_predict():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        \'batch_size\': 10,\n        \'patch_size\': 10,\n        \'depth\': 10,\n        \'num_hidden\': 10,\n        \'layer_sum\': 3\n    }\n    while(True):\n        basic_hypers[\'batch_size\'] = random.randint(basic_hypers[\'batch_size\'] / 2, basic_hypers[\'batch_size\'] * 2)\n        basic_hypers[\'patch_size\'] = random.randint(basic_hypers[\'patch_size\'] / 2, basic_hypers[\'patch_size\'] * 2)\n        basic_hypers[\'depth\'] = random.randint(basic_hypers[\'depth\'] / 2, basic_hypers[\'depth\'] * 2)\n        basic_hypers[\'num_hidden\'] = random.randint(basic_hypers[\'num_hidden\'] / 2, basic_hypers[\'num_hidden\'] * 2)\n        basic_hypers[\'layer_sum\'] = random.randint(basic_hypers[\'layer_sum\'] / 2, basic_hypers[\'layer_sum\'] * 2)\n        if basic_hypers[\'patch_size\'] > 28:\n            basic_hypers[\'patch_size\'] = 28\n        print(\'=\' * 80)\n        print(basic_hypers)\n\n        stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers[\'layer_sum\'])]\n        conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n                                     image_size, num_labels, basic_hypers, stride_params, lrd=True)\n\nif __name__ == \'__main__\':\n    fit_predict()\n'"
src/optimize/cnn_step_optimize.py,36,"b""from __future__ import print_function\n\nimport random\n\nimport tensorflow as tf\n\nfrom convnet.conv_mnist import maxpool2d\nfrom neural.full_connect import accuracy\nfrom util.mnist import format_mnist\nfrom util.request import better_trend_hyper\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps):\n    batch_size = basic_hps['batch_size']\n    patch_size = basic_hps['patch_size']\n    depth = basic_hps['depth']\n    if depth < 2:\n        depth = 2\n    num_hidden = basic_hps['num_hidden']\n    if num_hidden < 8:\n        num_hidden = 8\n\n    num_channels = 1\n    layer_cnt = basic_hps['layer_sum']\n    starter_learning_rate = basic_hps['starter_learning_rate']\n    loss_collect = list()\n    first_hidden_num = num_hidden\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding='SAME')\n            print(conv)\n            conv = maxpool2d(conv)\n            print(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    hid_shape = hidden.get_shape()\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding='SAME')\n                print(conv)\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                print(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 1001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        mean_loss = 0\n        for step in range(num_steps):\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 10 == 0:\n                mean_loss /= 10.0\n                if step % 50 == 0:\n                    loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step % 200 == 0:\n                    print('Minibatch loss at step %d: %f' % (step, l))\n                    print('Validation accuracy: %.1f%%' % accuracy(\n                        valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n        hypers = better_trend_hyper([batch_size, depth, num_hidden, layer_cnt, patch_size], loss_collect)\n        print(hypers)\n        for i in range(len(hypers)):\n            if hypers[i] <= 1.0:\n                hypers[i] = 1\n            else:\n                hypers[i] = int(hypers[i])\n        hypers.append(starter_learning_rate)\n    return hypers\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print('all hp is one, change:')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(3, 10)\n        hps[5] = 0.1\n        hps[3] = 2\n        hps[2] *= 10\n        print(hps)\n    return True\n\n\ndef fit_better():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        'batch_size': 10,\n        'patch_size': 10,\n        'depth': 10,\n        'num_hidden': 10,\n        'layer_sum': 3,\n        'starter_learning_rate': 0.1\n    }\n    if basic_hypers['patch_size'] > 28:\n        basic_hypers['patch_size'] = 28\n    if basic_hypers['layer_sum'] > 3:\n        basic_hypers['layer_sum'] = 3\n    print('=' * 80)\n    print(basic_hypers)\n\n    stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers['layer_sum'])]\n    better_hps = conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n                                 image_size, num_labels, basic_hypers, stride_params)\n    while valid_hp(better_hps):\n        basic_hypers = {\n            'batch_size': better_hps[0],\n            'patch_size': better_hps[4],\n            'depth': better_hps[1],\n            'num_hidden': better_hps[2],\n            'layer_sum': better_hps[3],\n            'starter_learning_rate': better_hps[5]\n        }\n        # if basic_hypers['batch_size'] < 10:\n        #     basic_hypers['batch_size'] = 10\n        if basic_hypers['patch_size'] > 28:\n            basic_hypers['patch_size'] = 28\n        if basic_hypers['layer_sum'] > 3:\n            basic_hypers['layer_sum'] = 3\n        print('=' * 80)\n        print(basic_hypers)\n        stride_params = [[1, 2, 2, 1] for _ in range(basic_hypers['layer_sum'])]\n        better_hps = conv_train(\n            train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n            image_size, num_labels, basic_hypers, stride_params)\n    else:\n        print('can not find better hypers')\n\n\nif __name__ == '__main__':\n    fit_better()\n"""
src/optimize/random_param_cnn.py,36,"b""from __future__ import print_function\n\nimport random\n\nfrom convnet.conv_mnist import maxpool2d\nfrom neural.full_connect import accuracy\nfrom util import file_helper\nfrom util.mnist import format_mnist\n\nimport tensorflow as tf\n\n\ndef large_data_size(data):\n    return data.get_shape()[1] > 1 and data.get_shape()[2] > 1\n\n\ndef conv_train(train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image_size,\n               num_labels, basic_hps, stride_ps):\n    batch_size = basic_hps['batch_size']\n    patch_size = basic_hps['patch_size']\n    depth = basic_hps['depth']\n    num_hidden = basic_hps['num_hidden']\n    num_channels = 1\n    layer_cnt = basic_hps['layer_sum']\n    starter_learning_rate = basic_hps['starter_learning_rate']\n    loss_collect = list()\n    first_hidden_num = basic_hps['num_hidden']\n    second_hidden_num = first_hidden_num / 2 + 1\n\n    graph = tf.Graph()\n    with graph.as_default():\n        # Input data.\n        tf_train_dataset = tf.placeholder(\n            tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        input_weights = tf.Variable(tf.truncated_normal(\n            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n        input_biases = tf.Variable(tf.zeros([depth]))\n        mid_layer_cnt = layer_cnt - 1\n        layer_weights = list()\n        layer_biases = [tf.Variable(tf.constant(1.0, shape=[depth / (i + 2)])) for i in range(mid_layer_cnt)]\n        output_weights = list()\n        output_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n        first_nn_weights = tf.Variable(tf.truncated_normal(\n            [first_hidden_num, second_hidden_num], stddev=0.1))\n        second_nn_weights = tf.Variable(tf.truncated_normal(\n            [second_hidden_num, num_labels], stddev=0.1))\n        first_nn_biases = tf.Variable(tf.constant(1.0, shape=[second_hidden_num]))\n        second_nn_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n\n        # Model.\n        def model(data, init=False):\n            # Variables.\n            if not large_data_size(data) or not large_data_size(input_weights):\n                stride_ps[0] = [1, 1, 1, 1]\n            conv = tf.nn.conv2d(data, input_weights, stride_ps[0], use_cudnn_on_gpu=True, padding='SAME')\n            conv = maxpool2d(conv)\n            hidden = tf.nn.relu(conv + input_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.8)\n            for i in range(mid_layer_cnt):\n                # print(hidden)\n                if init:\n                    hid_shape = hidden.get_shape()\n                    filter_w = patch_size / (i + 1)\n                    filter_h = patch_size / (i + 1)\n                    if filter_w > hid_shape[1]:\n                        filter_w = int(hid_shape[1])\n                    if filter_h > hid_shape[2]:\n                        filter_h = int(hid_shape[2])\n                    layer_weight = tf.Variable(tf.truncated_normal(shape=[filter_w, filter_h, depth / (i + 1), depth / (i + 2)],\n                                                                   stddev=0.1))\n                    layer_weights.append(layer_weight)\n                if not large_data_size(hidden) or not large_data_size(layer_weights[i]):\n                    stride_ps[i + 1] = [1, 1, 1, 1]\n                conv = tf.nn.conv2d(hidden, layer_weights[i], stride_ps[i + 1], use_cudnn_on_gpu=True, padding='SAME')\n                if not large_data_size(conv):\n                    conv = maxpool2d(conv, 1, 1)\n                else:\n                    conv = maxpool2d(conv)\n                hidden = tf.nn.relu(conv + layer_biases[i])\n                if init:\n                    hidden = tf.nn.dropout(hidden, 0.8)\n\n            shapes = hidden.get_shape().as_list()\n            shape_mul = 1\n            for s in shapes[1:]:\n                shape_mul *= s\n\n            if init:\n                output_size = shape_mul\n                output_weights.append(tf.Variable(tf.truncated_normal([output_size, num_hidden], stddev=0.1)))\n            reshape = tf.reshape(hidden, [shapes[0], shape_mul])\n\n            hidden = tf.nn.relu6(tf.matmul(reshape, output_weights[0]) + output_biases)\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, first_nn_weights) + first_nn_biases\n            if init:\n                hidden = tf.nn.dropout(hidden, 0.5)\n            hidden = tf.matmul(hidden, second_nn_weights) + second_nn_biases\n            return hidden\n\n        # Training computation.\n        logits = model(tf_train_dataset, init=True)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n        optimizer = tf.train.AdagradOptimizer(starter_learning_rate).minimize(loss)\n\n        train_prediction = tf.nn.softmax(logits)\n        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n    num_steps = 2001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print('Initialized')\n        end_train = False\n        mean_loss = 0\n        for step in range(num_steps):\n            if end_train:\n                break\n            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            mean_loss += l\n            if step % 5 == 0:\n                mean_loss /= 5.0\n                loss_collect.append(mean_loss)\n                mean_loss = 0\n                if step % 50 == 0:\n                    print('Minibatch loss at step %d: %f' % (step, l))\n                    print('Validation accuracy: %.1f%%' % accuracy(\n                        valid_prediction.eval(), valid_labels))\n        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n        hypers = [batch_size, depth, num_hidden, layer_cnt, patch_size]\n    return hypers, loss_collect\n\n\ndef valid_hp(hps):\n    one_hp_cnt = 0\n    for hp in hps:\n        if hp <= 1:\n            one_hp_cnt += 1\n    if one_hp_cnt == len(hps):\n        print('all hp is one, change:')\n        for i in range(len(hps)):\n            hps[i] *= random.randint(0, 10)\n        print(hps)\n    return True\n\n\ndef random_hp():\n    image_size = 28\n    num_labels = 10\n    train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = \\\n        format_mnist()\n    pick_size = 2048\n    valid_dataset = valid_dataset[0: pick_size, :, :, :]\n    valid_labels = valid_labels[0: pick_size, :]\n    test_dataset = test_dataset[0: pick_size, :, :, :]\n    test_labels = test_labels[0: pick_size, :]\n    basic_hypers = {\n        'batch_size': 10,\n        'patch_size': 10,\n        'depth': 10,\n        'num_hidden': 10,\n        'layer_sum': 3,\n        'starter_learning_rate': 0.1\n    }\n    if basic_hypers['patch_size'] > 28:\n        basic_hypers['patch_size'] = 28\n\n    print('=' * 80)\n    print(basic_hypers)\n\n    for _ in range(200):\n        random_hypers = {\n            'batch_size': basic_hypers['batch_size'] + random.randint(-9, 30),\n            'patch_size': basic_hypers['patch_size'] + random.randint(-5, 15),\n            'depth': basic_hypers['depth'] + random.randint(-9, 10),\n            'num_hidden': basic_hypers['num_hidden'] * random.randint(1, 10),\n            'layer_sum': basic_hypers['layer_sum'] + random.randint(-1, 2),\n            'starter_learning_rate': basic_hypers['starter_learning_rate'] * random.uniform(0, 5)\n        }\n        # if basic_hypers['batch_size'] < 10:\n        #     basic_hypers['batch_size'] = 10\n        if random_hypers['patch_size'] > 28:\n            random_hypers['patch_size'] = 28\n        print('=' * 80)\n        print(random_hypers)\n        stride_params = [[1, 2, 2, 1] for _ in range(random_hypers['layer_sum'])]\n        hps, loss_es = conv_train(\n            train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels,\n            image_size, num_labels, random_hypers, stride_params)\n        file_helper.write('hps', str(hps))\n        file_helper.write('losses', str(loss_es))\n\nif __name__ == '__main__':\n    random_hp()\n"""
src/rnn/__init__.py,0,b''
src/rnn/bigram_lstm.py,36,"b'# coding=utf-8\nimport random\nimport string\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    f = zipfile.ZipFile(filename)\n    for name in f.namelist():\n        return tf.compat.as_str(f.read(name))\n    f.close()\n\n\ndata_set = load_pickle(\'text8_text.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    text = read_data(filename)\n    print(\'Data size %d\' % len(text))\n    save_obj(\'text8_text.pickle\', text)\nelse:\n    text = data_set\n\n# Create a small validation set.\nvalid_size = 1000\nvalid_text = text[:valid_size]\ntrain_text = text[valid_size:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(valid_size, valid_text[:64])\n\nvocabulary_size = (len(string.ascii_lowercase) + 1) * (len(string.ascii_lowercase) + 1)  # [a-z] + \' \'\n\nidx2bi = {}\nbi2idx = {}\nidx = 0\nfor i in \' \' + string.ascii_lowercase:\n    for j in \' \' + string.ascii_lowercase:\n        idx2bi[idx] = i + j\n        bi2idx[i + j] = idx\n        idx += 1\n\n\ndef bi2id(char):\n    if char in bi2idx.keys():\n        return bi2idx[char]\n    else:\n        print(\'Unexpected character: %s\' % char)\n        return 0\n\n\ndef id2bi(dictid):\n    if 0 <= dictid < len(idx2bi):\n        return idx2bi[dictid]\n    else:\n        return \'  \'\n\n\nprint(bi2id(\'ad\'), bi2id(\'zf\'), bi2id(\'  \'), bi2id(\'r \'), bi2id(\'\xc3\xaf\'))\nprint(id2bi(31), id2bi(708), id2bi(0), id2bi(486))\n\nbatch_size = 64\nnum_unrollings = 10\n\n\nclass BigramBatchGenerator(object):\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size = len(text)\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // batch_size\n        # print \'self._text_size, batch_size, segment\', self._text_size, batch_size, segment\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        # print self._cursor\n        self._last_batch = self._next_batch()\n\n    def _next_batch(self):\n        """"""Generate a single batch from the current cursor position in the data.""""""\n        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n        for b in range(self._batch_size):\n            batch[b, bi2id(self._text[self._cursor[b]:self._cursor[b] + 2])] = 1.0\n            self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n        return batch\n\n    def next(self):\n        """"""Generate the next array of batches from the data. The array consists of\n        the last batch of the previous array, followed by num_unrollings new ones.\n        """"""\n        batches = [self._last_batch]\n        for step in range(self._num_unrollings):\n            batches.append(self._next_batch())\n        self._last_batch = batches[-1]\n        return batches\n\n\ndef characters(probabilities):\n    """"""Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (mostl likely) character representation.""""""\n    return [id2bi(c) for c in np.argmax(probabilities, 1)]\n\n\ndef batches2string(batches):\n    """"""Convert a sequence of batches back into their (most likely) string\n    representation.""""""\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, characters(b))]\n    return s\n\n\ntrain_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\nvalid_batches = BigramBatchGenerator(valid_text, 1, 1)\n\nprint(batches2string(train_batches.next()))\nprint(batches2string(train_batches.next()))\nprint(batches2string(valid_batches.next()))\nprint(batches2string(valid_batches.next()))\n\n\ndef logprob(predictions, labels):\n    # prevent negative probability\n    """"""Log-probability of the true labels in a predicted batch.""""""\n    predictions[predictions < 1e-10] = 1e-10\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\n\ndef sample_distribution(distribution):\n    """"""Sample one element from a distribution assumed to be an array of normalized\n    probabilities.\n    """"""\n    # \xe5\x8f\x96\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\xa8\xe4\xba\x8e\xe8\xaf\x84\xe4\xbc\xb0\xef\xbc\x8c\xe6\x89\x80\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe6\xaf\x94\xe4\xbe\x8b\xe9\x9a\x8f\xe6\x9c\xba\n    r = random.uniform(0, 1)\n    s = 0\n    for i in range(len(distribution)):\n        s += distribution[i]\n        if s >= r:\n            return i\n    return len(distribution) - 1\n\n\ndef sample(prediction):\n    """"""Turn a (column) prediction into 1-hot encoded samples.""""""\n    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n    p[0, sample_distribution(prediction[0])] = 1.0\n    return p\n\n\ndef random_distribution():\n    """"""Generate a random column of probabilities.""""""\n    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n    return b / np.sum(b, 1)[:, None]\n\n\nnum_nodes = 64\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Parameters:\n    # Input, Forget, Memory, Output gate: input, previous output, and bias.\n    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n\n    # Variables saving state across unrollings.\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    # Classifier weights and biases.\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n\n    def _slice(_x, n, dim):\n        return _x[:, n * dim:(n + 1) * dim]\n\n\n    # Definition of the cell computation.\n    def lstm_cell(i, o, state):\n\n        ifco_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n\n        input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n        forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n        update = _slice(ifco_gates, 2, num_nodes)\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n        return output_gate * tf.tanh(state), state\n\n\n    # Input data.\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(\n            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n    # print(\'#######\', train_inputs)\n    # print(\'#######\', train_labels)\n\n    # Unrolled LSTM loop.\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n\n    # State saving across unrollings.\n    with tf.control_dependencies([saved_output.assign(output),\n                                  saved_state.assign(state)]):\n        # Classifier.\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                logits=logits, labels=tf.concat(train_labels, 0)))\n\n    # Optimizer.\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(\n        10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(\n        zip(gradients, v), global_step=global_step)\n\n    # Predictions.\n    train_prediction = tf.nn.softmax(logits)\n\n    # Sampling and validation eval: batch 1, no unrolling.\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(\n        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(\n        sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                  saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n\nnum_steps = 7001\nsummary_frequency = 100\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    mean_loss = 0\n    for step in range(num_steps):\n        batches = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batches[i]\n        _, l, predictions, lr = session.run(\n            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss /= summary_frequency\n            # The mean loss is an estimate of the loss over the last few batches.\n            print(\n                \'Average loss at step %d: %f learning rate: %f\' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batches)[1:])\n            print(\'Minibatch perplexity: %.2f\' % float(\n                np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                # Generate some samples.\n                print(\'=\' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print(\'=\' * 80)\n            # Measure validation set perplexity.\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print(\'Validation set perplexity: %.2f\' % float(np.exp(\n                valid_logprob / valid_size)))\n'"
src/rnn/cbow.py,21,"b'import zipfile\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport math\nimport collections\n\nfrom matplotlib import pylab\nfrom sklearn.manifold import TSNE\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    """"""Extract the first file enclosed in a zip file as a list of words""""""\n    with zipfile.ZipFile(filename) as f:\n        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n\ndef build_dataset(words, vocabulary_size):\n    count = [[\'UNK\', -1]]\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary[\'UNK\']\n            unk_count = unk_count + 1\n        data.append(index)\n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reverse_dictionary\n\n\ndef generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    context_size = 2 * skip_window\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n    batchs = np.ndarray(shape=(context_size, batch_size), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n\n    # use data of batch_size to create train_data-label set of batch_size // num_skips * num_skips\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        for j in range(num_skips):\n            labels[i * num_skips + j, 0] = buffer[target]\n            met_target = False\n            for bj in range(context_size):\n                if bj == target:\n                    met_target = True\n                if met_target:\n                    batchs[bj, i * num_skips + j] = buffer[bj + 1]\n                else:\n                    batchs[bj, i * num_skips + j] = buffer[bj]\n\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    # print(\'generate batch\')\n    # print(batchs)\n    return batchs, labels\n\nvocabulary_size = 50000\ndata_set = load_pickle(\'text8_data.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    words = read_data(filename)\n    print(\'Data size %d\' % len(words))\n    data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n    print(\'Most common words (+UNK)\', count[:5])\n    print(\'Sample data\', data[:10])\n    del words  # Hint to reduce memory.\n    data_set = {\n        \'data\': data, \'count\': count, \'dictionary\': dictionary, \'reverse_dictionary\': reverse_dictionary,\n    }\n    save_obj(\'text8_data.pickle\', data_set)\nelse:\n    data = data_set[\'data\']\n    count = data_set[\'count\']\n    dictionary = data_set[\'dictionary\']\n    reverse_dictionary = data_set[\'reverse_dictionary\']\n\n# split data\ndata_index = 0\n\nprint(\'data:\', [reverse_dictionary[di] for di in data[:8]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    test_size = 8\n    batch, labels = generate_batch(batch_size=test_size, num_skips=num_skips, skip_window=skip_window)\n    print(\'\\nwith num_skips = %d and skip_window = %d:\' % (num_skips, skip_window))\n    print(\'    batch:\', [reverse_dictionary[bi] for bi in batch.reshape(-1)])\n    print(\'    labels:\', [reverse_dictionary[li] for li in labels.reshape(-1)])\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1  # How many words to consider left and right.\nnum_skips = 2  # How many times to reuse an input to generate a label.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16  # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64  # Number of negative examples to sample.\n\n# tensor: Train a skip-gram model, word2vec\ngraph = tf.Graph()\n\nwith graph.as_default():\n    # Input data.\n    train_dataset = tf.placeholder(tf.int32, shape=[2 * skip_window, batch_size])\n    train_labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, shape=[2 * skip_window, batch_size], dtype=tf.int32)\n\n    # Variables.\n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Model.\n    # Look up embeddings for inputs.\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    # sum up vectors on first dimensions, as context vectors\n    embed_sum = tf.reduce_sum(embed, 0)\n\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases,\n                                   train_labels, embed_sum,\n                                   num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(\n        normalized_embeddings, valid_dataset)\n    # sum up vectors\n    valid_embeddings_sum = tf.reduce_sum(valid_embeddings, 0)\n    similarity = tf.matmul(valid_embeddings_sum, tf.transpose(normalized_embeddings))\n\n# flow\nnum_steps = 100001\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    average_loss = 0\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(\n            batch_size, num_skips, skip_window)\n        # print(batch_data.shape)\n        # print(batch_labels.shape)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss /= 2000\n            # The average loss is an estimate of the loss over the last 2000 batches.\n            print(\'Average loss at step %d: %f\' % (step, average_loss))\n            average_loss = 0\n        # note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = \'Nearest to %s:\' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = \'%s %s,\' % (log, close_word)\n                print(log)\n    final_embeddings = normalized_embeddings.eval()\n    save_obj(\'text8_embed.pickle\', final_embeddings)\n\nnum_points = 400\n\ntsne = TSNE(perplexity=30, n_components=2, init=\'pca\', n_iter=5000)\ntwo_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points + 1, :])\n\n\ndef plot(embeddings, labels):\n    assert embeddings.shape[0] >= len(labels), \'More labels than embeddings\'\n    pylab.figure(figsize=(15, 15))  # in inches\n    for i, label in enumerate(labels):\n        x, y = embeddings[i, :]\n        pylab.scatter(x, y)\n        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\',\n                       ha=\'right\', va=\'bottom\')\n    pylab.show()\n\n\nwords = [reverse_dictionary[i] for i in range(1, num_points + 1)]\nplot(two_d_embeddings, words)\n'"
src/rnn/data_utils.py,3,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utilities for downloading data from WMT, tokenizing, vocabularies.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport re\nimport tarfile\n\nfrom six.moves import urllib\n\nfrom tensorflow.python.platform import gfile\nimport tensorflow as tf\n\n# Special vocabulary symbols - we always put them at the start.\n_PAD = b""_PAD""\n_GO = b""_GO""\n_EOS = b""_EOS""\n_UNK = b""_UNK""\n_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n\nPAD_ID = 0\nGO_ID = 1\nEOS_ID = 2\nUNK_ID = 3\n\n# Regular expressions used to tokenize.\n_WORD_SPLIT = re.compile(b""([.,!?\\""\':;)(])"")\n_DIGIT_RE = re.compile(br""\\d"")\n\n# URLs for WMT data.\n_WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/training-giga-fren.tar""\n_WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/dev-v2.tgz""\n\n\ndef maybe_download(directory, filename, url):\n  """"""Download filename from url unless it\'s already in directory.""""""\n  if not os.path.exists(directory):\n    print(""Creating directory %s"" % directory)\n    os.mkdir(directory)\n  filepath = os.path.join(directory, filename)\n  if not os.path.exists(filepath):\n    print(""Downloading %s to %s"" % (url, filepath))\n    filepath, _ = urllib.request.urlretrieve(url, filepath)\n    statinfo = os.stat(filepath)\n    print(""Successfully downloaded"", filename, statinfo.st_size, ""bytes"")\n  return filepath\n\n\ndef gunzip_file(gz_path, new_path):\n  """"""Unzips from gz_path into new_path.""""""\n  print(""Unpacking %s to %s"" % (gz_path, new_path))\n  with gzip.open(gz_path, ""rb"") as gz_file:\n    with open(new_path, ""wb"") as new_file:\n      for line in gz_file:\n        new_file.write(line)\n\n\ndef get_wmt_enfr_train_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  train_path = os.path.join(directory, ""giga-fren.release2.fixed"")\n  if not (gfile.Exists(train_path +"".fr"") and gfile.Exists(train_path +"".en"")):\n    corpus_file = maybe_download(directory, ""training-giga-fren.tar"",\n                                 _WMT_ENFR_TRAIN_URL)\n    print(""Extracting tar file %s"" % corpus_file)\n    with tarfile.open(corpus_file, ""r"") as corpus_tar:\n      corpus_tar.extractall(directory)\n    gunzip_file(train_path + "".fr.gz"", train_path + "".fr"")\n    gunzip_file(train_path + "".en.gz"", train_path + "".en"")\n  return train_path\n\n\ndef get_wmt_enfr_dev_set(directory):\n  """"""Download the WMT en-fr training corpus to directory unless it\'s there.""""""\n  dev_name = ""newstest2013""\n  dev_path = os.path.join(directory, dev_name)\n  if not (gfile.Exists(dev_path + "".fr"") and gfile.Exists(dev_path + "".en"")):\n    dev_file = maybe_download(directory, ""dev-v2.tgz"", _WMT_ENFR_DEV_URL)\n    print(""Extracting tgz file %s"" % dev_file)\n    with tarfile.open(dev_file, ""r:gz"") as dev_tar:\n      fr_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".fr"")\n      en_dev_file = dev_tar.getmember(""dev/"" + dev_name + "".en"")\n      fr_dev_file.name = dev_name + "".fr""  # Extract without ""dev/"" prefix.\n      en_dev_file.name = dev_name + "".en""\n      dev_tar.extract(fr_dev_file, directory)\n      dev_tar.extract(en_dev_file, directory)\n  return dev_path\n\n\ndef basic_tokenizer(sentence):\n  """"""Very basic tokenizer: split the sentence into a list of tokens.""""""\n  words = []\n  for space_separated_fragment in sentence.strip().split():\n    words.extend(_WORD_SPLIT.split(space_separated_fragment))\n  return [w for w in words if w]\n\n\ndef create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n                      tokenizer=None, normalize_digits=True):\n  """"""Create vocabulary file (if it does not exist yet) from data file.\n\n  Data file is assumed to contain one sentence per line. Each sentence is\n  tokenized and digits are normalized (if normalize_digits is set).\n  Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n  We write it to vocabulary_path in a one-token-per-line format, so that later\n  token in the first line gets id=0, second line gets id=1, and so on.\n\n  Args:\n    vocabulary_path: path where the vocabulary will be created.\n    data_path: data file that will be used to create vocabulary.\n    max_vocabulary_size: limit on the size of the created vocabulary.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(vocabulary_path):\n    print(""Creating vocabulary %s from data %s"" % (vocabulary_path, data_path))\n    vocab = {}\n    with gfile.GFile(data_path, mode=""rb"") as f:\n      counter = 0\n      for line in f:\n        counter += 1\n        if counter % 100000 == 0:\n          print(""  processing line %d"" % counter)\n        line = tf.compat.as_bytes(line)\n        tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n        for w in tokens:\n          word = _DIGIT_RE.sub(b""0"", w) if normalize_digits else w\n          if word in vocab:\n            vocab[word] += 1\n          else:\n            vocab[word] = 1\n      vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n      if len(vocab_list) > max_vocabulary_size:\n        vocab_list = vocab_list[:max_vocabulary_size]\n      with gfile.GFile(vocabulary_path, mode=""wb"") as vocab_file:\n        for w in vocab_list:\n          vocab_file.write(w + b""\\n"")\n\n\ndef initialize_vocabulary(vocabulary_path):\n  """"""Initialize vocabulary from file.\n\n  We assume the vocabulary is stored one-item-per-line, so a file:\n    dog\n    cat\n  will result in a vocabulary {""dog"": 0, ""cat"": 1}, and this function will\n  also return the reversed-vocabulary [""dog"", ""cat""].\n\n  Args:\n    vocabulary_path: path to the file containing the vocabulary.\n\n  Returns:\n    a pair: the vocabulary (a dictionary mapping string to integers), and\n    the reversed vocabulary (a list, which reverses the vocabulary mapping).\n\n  Raises:\n    ValueError: if the provided vocabulary_path does not exist.\n  """"""\n  if gfile.Exists(vocabulary_path):\n    rev_vocab = []\n    with gfile.GFile(vocabulary_path, mode=""rb"") as f:\n      rev_vocab.extend(f.readlines())\n    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]\n    vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n    return vocab, rev_vocab\n  else:\n    raise ValueError(""Vocabulary file %s not found."", vocabulary_path)\n\n\ndef sentence_to_token_ids(sentence, vocabulary,\n                          tokenizer=None, normalize_digits=True):\n  """"""Convert a string to list of integers representing token-ids.\n\n  For example, a sentence ""I have a dog"" may become tokenized into\n  [""I"", ""have"", ""a"", ""dog""] and with vocabulary {""I"": 1, ""have"": 2,\n  ""a"": 4, ""dog"": 7""} this function will return [1, 2, 4, 7].\n\n  Args:\n    sentence: the sentence in bytes format to convert to token-ids.\n    vocabulary: a dictionary mapping tokens to integers.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n\n  Returns:\n    a list of integers, the token-ids for the sentence.\n  """"""\n\n  if tokenizer:\n    words = tokenizer(sentence)\n  else:\n    words = basic_tokenizer(sentence)\n  if not normalize_digits:\n    return [vocabulary.get(w, UNK_ID) for w in words]\n  # Normalize digits by 0 before looking words up in the vocabulary.\n  return [vocabulary.get(_DIGIT_RE.sub(b""0"", w), UNK_ID) for w in words]\n\n\ndef data_to_token_ids(data_path, target_path, vocabulary_path,\n                      tokenizer=None, normalize_digits=True):\n  """"""Tokenize data file and turn into token-ids using given vocabulary file.\n\n  This function loads data line-by-line from data_path, calls the above\n  sentence_to_token_ids, and saves the result to target_path. See comment\n  for sentence_to_token_ids on the details of token-ids format.\n\n  Args:\n    data_path: path to the data file in one-sentence-per-line format.\n    target_path: path where the file with token-ids will be created.\n    vocabulary_path: path to the vocabulary file.\n    tokenizer: a function to use to tokenize each sentence;\n      if None, basic_tokenizer will be used.\n    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n  """"""\n  if not gfile.Exists(target_path):\n    print(""Tokenizing data in %s"" % data_path)\n    vocab, _ = initialize_vocabulary(vocabulary_path)\n    with gfile.GFile(data_path, mode=""rb"") as data_file:\n      with gfile.GFile(target_path, mode=""w"") as tokens_file:\n        counter = 0\n        for line in data_file:\n          counter += 1\n          if counter % 100000 == 0:\n            print(""  tokenizing line %d"" % counter)\n          token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,\n                                            tokenizer, normalize_digits)\n          tokens_file.write("" "".join([str(tok) for tok in token_ids]) + ""\\n"")\n\n\ndef prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n  """"""Get WMT data into data_dir, create vocabularies and tokenize data.\n\n  Args:\n    data_dir: directory in which the data sets will be stored.\n    en_vocabulary_size: size of the English vocabulary to create and use.\n    fr_vocabulary_size: size of the French vocabulary to create and use.\n    tokenizer: a function to use to tokenize each data sentence;\n      if None, basic_tokenizer will be used.\n\n  Returns:\n    A tuple of 6 elements:\n      (1) path to the token-ids for English training data-set,\n      (2) path to the token-ids for French training data-set,\n      (3) path to the token-ids for English development data-set,\n      (4) path to the token-ids for French development data-set,\n      (5) path to the English vocabulary file,\n      (6) path to the French vocabulary file.\n  """"""\n  # Get wmt data to the specified directory.\n  train_path = get_wmt_enfr_train_set(data_dir)\n  dev_path = get_wmt_enfr_dev_set(data_dir)\n\n  from_train_path = train_path + "".en""\n  to_train_path = train_path + "".fr""\n  from_dev_path = dev_path + "".en""\n  to_dev_path = dev_path + "".fr""\n  return prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, en_vocabulary_size,\n                      fr_vocabulary_size, tokenizer)\n\n\ndef prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path, from_vocabulary_size,\n                 to_vocabulary_size, tokenizer=None):\n  """"""Preapre all necessary files that are required for the training.\n\n    Args:\n      data_dir: directory in which the data sets will be stored.\n      from_train_path: path to the file that includes ""from"" training samples.\n      to_train_path: path to the file that includes ""to"" training samples.\n      from_dev_path: path to the file that includes ""from"" dev samples.\n      to_dev_path: path to the file that includes ""to"" dev samples.\n      from_vocabulary_size: size of the ""from language"" vocabulary to create and use.\n      to_vocabulary_size: size of the ""to language"" vocabulary to create and use.\n      tokenizer: a function to use to tokenize each data sentence;\n        if None, basic_tokenizer will be used.\n\n    Returns:\n      A tuple of 6 elements:\n        (1) path to the token-ids for ""from language"" training data-set,\n        (2) path to the token-ids for ""to language"" training data-set,\n        (3) path to the token-ids for ""from language"" development data-set,\n        (4) path to the token-ids for ""to language"" development data-set,\n        (5) path to the ""from language"" vocabulary file,\n        (6) path to the ""to language"" vocabulary file.\n    """"""\n  # Create vocabularies of the appropriate sizes.\n  to_vocab_path = os.path.join(data_dir, ""vocab%d.to"" % to_vocabulary_size)\n  from_vocab_path = os.path.join(data_dir, ""vocab%d.from"" % from_vocabulary_size)\n  create_vocabulary(to_vocab_path, to_train_path , to_vocabulary_size, tokenizer)\n  create_vocabulary(from_vocab_path, from_train_path , from_vocabulary_size, tokenizer)\n\n  # Create token ids for the training data.\n  to_train_ids_path = to_train_path + ("".ids%d"" % to_vocabulary_size)\n  from_train_ids_path = from_train_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_train_path, to_train_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_train_path, from_train_ids_path, from_vocab_path, tokenizer)\n\n  # Create token ids for the development data.\n  to_dev_ids_path = to_dev_path + ("".ids%d"" % to_vocabulary_size)\n  from_dev_ids_path = from_dev_path + ("".ids%d"" % from_vocabulary_size)\n  data_to_token_ids(to_dev_path, to_dev_ids_path, to_vocab_path, tokenizer)\n  data_to_token_ids(from_dev_path, from_dev_ids_path, from_vocab_path, tokenizer)\n\n  return (from_train_ids_path, to_train_ids_path,\n          from_dev_ids_path, to_dev_ids_path,\n          from_vocab_path, to_vocab_path)\n'"
src/rnn/embed_bigram_lstm.py,46,"b'# coding=utf-8\nimport random\nimport string\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    f = zipfile.ZipFile(filename)\n    for name in f.namelist():\n        return tf.compat.as_str(f.read(name))\n    f.close()\n\n\ndata_set = load_pickle(\'text8_text.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    text = read_data(filename)\n    print(\'Data size %d\' % len(text))\n    save_obj(\'text8_text.pickle\', text)\nelse:\n    text = data_set\n\n# Create a small validation set.\nvalid_size = 1000\nvalid_text = text[:valid_size]\ntrain_text = text[valid_size:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(valid_size, valid_text[:64])\n\n# Utility functions to map characters to vocabulary IDs and back.\nvocabulary_size = len(string.ascii_lowercase) + 1  # [a-z] + \' \'\n# ascii code for character\nfirst_letter = ord(string.ascii_lowercase[0])\n\n\ndef char2id(char):\n    if char in string.ascii_lowercase:\n        return ord(char) - first_letter + 1\n    elif char == \' \':\n        return 0\n    else:\n        print(\'Unexpected character: %s\' % char)\n        return 0\n\n\ndef id2char(dictid):\n    if dictid > 0:\n        return chr(dictid + first_letter - 1)\n    else:\n        return \' \'\n\n\nprint(char2id(\'a\'), char2id(\'z\'), char2id(\' \'), char2id(\'\xc3\xaf\'))\nprint(id2char(1), id2char(26), id2char(0))\n\nbi_voc_size = vocabulary_size * vocabulary_size\n\n\nclass BiBatchGenerator(object):\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size_in_chars = len(text)\n        self._text_size = self._text_size_in_chars // 2  # in bigrams\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // batch_size\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        self._last_batch = self._next_batch()\n\n    def _next_batch(self):\n        batch = np.zeros(shape=self._batch_size, dtype=np.int)\n        # print \'batch idx %i\' %\n        for b in range(self._batch_size):\n            char_idx = self._cursor[b] * 2\n            ch1 = char2id(self._text[char_idx])\n            if self._text_size_in_chars - 1 == char_idx:\n                ch2 = 0\n            else:\n                ch2 = char2id(self._text[char_idx + 1])\n            batch[b] = ch1 * vocabulary_size + ch2\n            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n        return batch\n\n    def next(self):\n        batches = [self._last_batch]\n        for step in range(self._num_unrollings):\n            batches.append(self._next_batch())\n        self._last_batch = batches[-1]\n        return batches\n\n\ndef bi2str(encoding):\n    return id2char(encoding // vocabulary_size) + id2char(encoding % vocabulary_size)\n\n\ndef bigrams(encodings):\n    return [bi2str(e) for e in encodings]\n\n\ndef bibatches2string(batches):\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, bigrams(b))]\n    return s\n\n\nbi_onehot = np.zeros((bi_voc_size, bi_voc_size))\nnp.fill_diagonal(bi_onehot, 1)\n\n\ndef bigramonehot(encodings):\n    return [bi_onehot[e] for e in encodings]\n\n\ntrain_batches = BiBatchGenerator(train_text, 8, 8)\nvalid_batches = BiBatchGenerator(valid_text, 1, 1)\n\nbatch = train_batches.next()\nprint(batch)\nprint(bibatches2string(batch))\n# print bigramonehot(batch)\nprint (bibatches2string(train_batches.next()))\nprint (bibatches2string(valid_batches.next()))\nprint (bibatches2string(valid_batches.next()))\n\n\ndef logprob(predictions, labels):\n    """"""Log-probability of the true labels in a predicted batch.""""""\n    predictions[predictions < 1e-10] = 1e-10\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\n\ndef sample_distribution(distribution):\n    """"""Sample one element from a distribution assumed to be an array of normalized\n    probabilities.\n    """"""\n    r = random.uniform(0, 1)\n    s = 0\n    for i in range(len(distribution)):\n        s += distribution[i]\n        if s >= r:\n            return i\n    return len(distribution) - 1\n\n\ndef sample(prediction, size=vocabulary_size):\n    """"""Turn a (column) prediction into 1-hot encoded samples.""""""\n    p = np.zeros(shape=[1, size], dtype=np.float)\n    p[0, sample_distribution(prediction[0])] = 1.0\n    return p\n\n\ndef one_hot_voc(prediction, size=vocabulary_size):\n    p = np.zeros(shape=[1, size], dtype=np.float)\n    p[0, prediction[0]] = 1.0\n    return p\n\n\ndef random_distribution(size=vocabulary_size):\n    """"""Generate a random column of probabilities.""""""\n    b = np.random.uniform(0.0, 1.0, size=[1, size])\n    return b / np.sum(b, 1)[:, None]\n\n\ndef create_lstm_graph_bi(num_nodes, num_unrollings, batch_size, embedding_size=bi_voc_size):\n    with tf.Graph().as_default() as g:\n        # input to all gates\n        x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1), name=\'x\')\n        # memory of all gates\n        m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name=\'m\')\n        # biases all gates\n        biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n        # Variables saving state across unrollings.\n        saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n        saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n        # Classifier weights and biases.\n        w = tf.Variable(tf.truncated_normal([num_nodes, bi_voc_size], -0.1, 0.1))\n        b = tf.Variable(tf.zeros([bi_voc_size]))\n        # embeddings for all possible bigrams\n        embeddings = tf.Variable(tf.random_uniform([bi_voc_size, embedding_size], -1.0, 1.0), name=\'embeddings\')\n        # one hot encoding for labels in\n        np_embeds = np.zeros((bi_voc_size, bi_voc_size))\n        np.fill_diagonal(np_embeds, 1)\n        bigramonehot = tf.constant(np.reshape(np_embeds, -1), dtype=tf.float32, shape=[bi_voc_size, bi_voc_size],\n                                   name=\'bigramonehot\')\n        tf_keep_prob = tf.placeholder(tf.float32, name=\'tf_keep_prob\')\n\n        # Definition of the cell computation.\n        def lstm_cell(i, o, state):\n            # apply dropout to the input\n            i = tf.nn.dropout(i, tf_keep_prob)\n            mult = tf.matmul(i, x) + tf.matmul(o, m) + biases\n            input_gate = tf.sigmoid(mult[:, :num_nodes])\n            forget_gate = tf.sigmoid(mult[:, num_nodes:num_nodes * 2])\n            update = mult[:, num_nodes * 3:num_nodes * 4]\n            state = forget_gate * state + input_gate * tf.tanh(update)\n            output_gate = tf.sigmoid(mult[:, num_nodes * 3:])\n            output = tf.nn.dropout(output_gate * tf.tanh(state), tf_keep_prob)\n            return output, state\n\n        # Input data. [num_unrollings, batch_size] -> one hot encoding removed, we send just bigram ids\n        tf_train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size], name=\'tf_train_data\')\n        train_data = list()\n        for i in tf.split(tf_train_data, num_unrollings + 1, 0):\n            train_data.append(tf.squeeze(i))\n        train_inputs = train_data[:num_unrollings]\n        train_labels = list()\n        for l in train_data[1:]:\n            # train_labels.append(tf.nn.embedding_lookup(embeddings, l))\n            train_labels.append(tf.gather(bigramonehot, l))\n            # train_labels.append(tf.reshape(l, [batch_size,1]))  # labels are inputs shifted by one time step.\n\n        # Unrolled LSTM loop.\n        outputs = list()\n        output = saved_output\n        state = saved_state\n        # python loop used: tensorflow does not support sequential operations yet\n        for i in train_inputs:  # having a loop simulates having time\n            # embed input bigrams -> [batch_size, embedding_size]\n            output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)\n            outputs.append(output)\n\n        # State saving across unrollings, control_dependencies makes sure that output and state are computed\n        with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n            logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n                                                                          labels=tf.concat(train_labels, 0)\n                                                                          ), name=\'loss\')\n        # Optimizer.\n        global_step = tf.Variable(0, name=\'global_step\')\n        learning_rate = tf.train.exponential_decay(10.0, global_step, 500, 0.9, staircase=True, name=\'learning_rate\')\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate, name=\'optimizer\')\n        gradients, v = zip(*optimizer.compute_gradients(loss))\n        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n\n        # here we predict the embedding\n        # train_prediction = tf.argmax(tf.nn.softmax(logits), 1, name=\'train_prediction\')\n        train_prediction = tf.nn.softmax(logits, name=\'train_prediction\')\n\n        # Sampling and validation eval: batch 1, no unrolling.\n        sample_input = tf.placeholder(tf.int32, shape=[1], name=\'sample_input\')\n        saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]), name=\'saved_sample_output\')\n        saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]), name=\'saved_sample_state\')\n        reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n                                      saved_sample_state.assign(tf.zeros([1, num_nodes])), name=\'reset_sample_state\')\n        embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n        sample_output, sample_state = lstm_cell(embed_sample_input, saved_sample_output, saved_sample_state)\n\n        with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                      saved_sample_state.assign(sample_state)]):\n            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b), name=\'sample_prediction\')\n        return g\n\n\n# test graph\ncreate_lstm_graph_bi(64, 10, 128, 32)\n\n\ndef bitrain(g, num_steps, summary_frequency, num_unrollings, batch_size):\n    # initalize batch generators\n    train_batches = BiBatchGenerator(train_text, batch_size, num_unrollings)\n    valid_batches = BiBatchGenerator(valid_text, 1, 1)\n    optimizer = g.get_tensor_by_name(\'optimizer:0\')\n    loss = g.get_tensor_by_name(\'loss:0\')\n    train_prediction = g.get_tensor_by_name(\'train_prediction:0\')\n    learning_rate = g.get_tensor_by_name(\'learning_rate:0\')\n    tf_train_data = g.get_tensor_by_name(\'tf_train_data:0\')\n    sample_prediction = g.get_tensor_by_name(\'sample_prediction:0\')\n    # similarity = g.get_tensor_by_name(\'similarity:0\')\n    reset_sample_state = g.get_operation_by_name(\'reset_sample_state\')\n    sample_input = g.get_tensor_by_name(\'sample_input:0\')\n    embeddings = g.get_tensor_by_name(\'embeddings:0\')\n    keep_prob = g.get_tensor_by_name(\'tf_keep_prob:0\')\n    with tf.Session(graph=g) as session:\n        tf.global_variables_initializer().run()\n        print(\'Initialized\')\n        mean_loss = 0\n        for step in range(num_steps):\n            batches = train_batches.next()\n            # print bibatches2string(batches)\n            # print np.array(batches)\n            # feed_dict = dict()\n            # for i in range(num_unrollings + 1):\n            #  feed_dict[train_data[i]] = batches[i]\n            # tf_train_data =\n            _, l, lr, predictions = session.run([optimizer, loss, learning_rate, train_prediction],\n                                                feed_dict={tf_train_data: batches, keep_prob: 0.6})\n            mean_loss += l\n            if step % summary_frequency == 0:\n                if step > 0:\n                    mean_loss = mean_loss / summary_frequency\n                # The mean loss is an estimate of the loss over the last few batches.\n                print (\'Average loss at step %d: %f learning rate: %f\' % (step, mean_loss, lr))\n                mean_loss = 0\n                labels = list(batches)[1:]\n                labels = np.concatenate([bigramonehot(l) for l in labels])\n                # print predictions\n                # print labels\n                # print labels.shape[0]\n                print(\'Minibatch perplexity: %.2f\' % float(np.exp(logprob(predictions, labels))))\n                if step % (summary_frequency * 10) == 0:\n                    # Generate some samples.\n                    print(\'=\' * 80)\n                    # print embeddings.eval()\n                    for _ in range(5):\n                        # print random_distribution(bi_voc_size)\n                        feed = np.argmax(sample(random_distribution(bi_voc_size), bi_voc_size))\n                        sentence = bi2str(feed)\n                        reset_sample_state.run()\n                        for _ in range(49):\n                            # prediction = similarity.eval({sample_input: [feed]})\n                            # nearest = (-prediction[0]).argsort()[0]\n                            prediction = sample_prediction.eval({sample_input: [feed], keep_prob: 1.0})\n                            # print prediction\n                            feed = np.argmax(sample(prediction, bi_voc_size))\n                            # feed = np.argmax(prediction[0])\n                            sentence += bi2str(feed)\n                        print(sentence)\n                    print(\'=\' * 80)\n                # Measure validation set perplexity.\n                reset_sample_state.run()\n                valid_logprob = 0\n                for _ in range(valid_size):\n                    b = valid_batches.next()\n                    predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n                    # print(predictions)\n                    valid_logprob = valid_logprob + logprob(predictions, one_hot_voc(b[1], bi_voc_size))\n                print(\'Validation set perplexity: %.2f\' % float(np.exp(valid_logprob / valid_size)))\n\n\ngraph = create_lstm_graph_bi(512, 32, 32, 128)\nbitrain(graph, 4001, 100, 32, 32)\n'"
src/rnn/lstm.py,45,"b'# coding=utf-8\nimport random\nimport string\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    f = zipfile.ZipFile(filename)\n    for name in f.namelist():\n        return tf.compat.as_str(f.read(name))\n    f.close()\n\n\ndata_set = load_pickle(\'text8_text.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    text = read_data(filename)\n    print(\'Data size %d\' % len(text))\n    save_obj(\'text8_text.pickle\', text)\nelse:\n    text = data_set\n\n# Create a small validation set.\nvalid_size = 1000\nvalid_text = text[:valid_size]\ntrain_text = text[valid_size:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(valid_size, valid_text[:64])\n\n# Utility functions to map characters to vocabulary IDs and back.\nvocabulary_size = len(string.ascii_lowercase) + 1  # [a-z] + \' \'\n# ascii code for character\nfirst_letter = ord(string.ascii_lowercase[0])\n\n\ndef char2id(char):\n    if char in string.ascii_lowercase:\n        return ord(char) - first_letter + 1\n    elif char == \' \':\n        return 0\n    else:\n        print(\'Unexpected character: %s\' % char)\n        return 0\n\n\ndef id2char(dictid):\n    if dictid > 0:\n        return chr(dictid + first_letter - 1)\n    else:\n        return \' \'\n\n\nprint(char2id(\'a\'), char2id(\'z\'), char2id(\' \'), char2id(\'\xc3\xaf\'))\nprint(id2char(1), id2char(26), id2char(0))\n\n# Function to generate a training batch for the LSTM model.\nbatch_size = 64\nnum_unrollings = 10\n\n\nclass BatchGenerator(object):\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size = len(text)\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // batch_size\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        self._last_batch = self._next_batch()\n\n    def _next_batch(self):\n        """"""Generate a single batch from the current cursor position in the data.""""""\n        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n        for b in range(self._batch_size):\n            # same id, same index of second dimension\n            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n        return batch\n\n    def next(self):\n        """"""Generate the next array of batches from the data. The array consists of\n        the last batch of the previous array, followed by num_unrollings new ones.\n        """"""\n        batches = [self._last_batch]\n        for step in range(self._num_unrollings):\n            batches.append(self._next_batch())\n        self._last_batch = batches[-1]\n        return batches\n\n\ndef characters(probabilities):\n    """"""Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.""""""\n    # argmax for the most likely character\n    return [id2char(c) for c in np.argmax(probabilities, 1)]\n\n\ndef batches2string(batches):\n    """"""Convert a sequence of batches back into their (most likely) string\n    representation.""""""\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, characters(b))]\n    return s\n\n\ntrain_batches = BatchGenerator(train_text, batch_size, num_unrollings)\nvalid_batches = BatchGenerator(valid_text, 1, 1)\n\nprint(batches2string(train_batches.next()))\nprint(batches2string(train_batches.next()))\nprint(batches2string(valid_batches.next()))\nprint(batches2string(valid_batches.next()))\n\n\ndef logprob(predictions, labels):\n    # prevent negative probability\n    """"""Log-probability of the true labels in a predicted batch.""""""\n    predictions[predictions < 1e-10] = 1e-10\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\n\ndef sample_distribution(distribution):\n    """"""Sample one element from a distribution assumed to be an array of normalized\n    probabilities.\n    """"""\n    # \xe5\x8f\x96\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\xa8\xe4\xba\x8e\xe8\xaf\x84\xe4\xbc\xb0\xef\xbc\x8c\xe6\x89\x80\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe6\xaf\x94\xe4\xbe\x8b\xe9\x9a\x8f\xe6\x9c\xba\n    r = random.uniform(0, 1)\n    s = 0\n    for i in range(len(distribution)):\n        s += distribution[i]\n        if s >= r:\n            return i\n    return len(distribution) - 1\n\n\ndef sample(prediction):\n    """"""Turn a (column) prediction into 1-hot encoded samples.""""""\n    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n    p[0, sample_distribution(prediction[0])] = 1.0\n    return p\n\n\ndef random_distribution():\n    """"""Generate a random column of probabilities.""""""\n    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n    return b / np.sum(b, 1)[:, None]\n\n\n# Simple LSTM Model.\nnum_nodes = 64\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Parameters:\n    # Input gate: input, previous output, and bias.\n    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, num_nodes]))\n    # Forget gate: input, previous output, and bias.\n    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, num_nodes]))\n    # Memory cell: input, state and bias.\n    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, num_nodes]))\n    # Output gate: input, previous output, and bias.\n    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, num_nodes]))\n    # Variables saving state across unrollings.\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    # Classifier weights and biases.\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Definition of the cell computation.\n    def lstm_cell(i, o, state):\n        """"""Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n        Note that in this formulation, we omit the various connections between the\n        previous state and the gates.""""""\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n        return output_gate * tf.tanh(state), state\n\n    # Input data.\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(\n            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n\n    # Unrolled LSTM loop.\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n\n    # State saving across unrollings.\n    with tf.control_dependencies([saved_output.assign(output),\n                                  saved_state.assign(state)]):\n        # Classifier.\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(\n                logits=logits, labels=tf.concat(train_labels, 0)))\n\n    # Optimizer.\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(\n        10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(\n        zip(gradients, v), global_step=global_step)\n\n    # Predictions.\n    train_prediction = tf.nn.softmax(logits)\n\n    # Sampling and validation eval: batch 1, no unrolling.\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(\n        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(\n        sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                  saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n\nnum_steps = 7001\nsummary_frequency = 100\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    mean_loss = 0\n    for step in range(num_steps):\n        batches = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batches[i]\n        _, l, predictions, lr = session.run(\n            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss /= summary_frequency\n            # The mean loss is an estimate of the loss over the last few batches.\n            print(\n                \'Average loss at step %d: %f learning rate: %f\' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batches)[1:])\n            print(\'Minibatch perplexity: %.2f\' % float(\n                np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                # Generate some samples.\n                print(\'=\' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print(\'=\' * 80)\n            # Measure validation set perplexity.\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print(\'Validation set perplexity: %.2f\' % float(np.exp(\n                valid_logprob / valid_size)))\n'"
src/rnn/lstm_regular.py,36,"b'# coding=utf-8\n\nimport numpy as np\nimport tensorflow as tf\n\nMAX_DATA_SIZE = 10000000\n\n# Network Parameters\n# \xe6\xaf\x8f\xe6\xac\xa1\xe8\xae\xad\xe7\xbb\x835\xe7\xbb\x84\xe6\x95\xb0\xe6\x8d\xae\nbatch_cnt_per_step = 5\nbatch_size = 10  # 10 num to predict one num\nEMBEDDING_SIZE = 1\n\n\ndef raw_data():\n    return [1.0 / (i + 1) for i in range(MAX_DATA_SIZE)]\n    # return [1.0001 ** (- i) for i in range(MAX_DATA_SIZE)]\n    # return [1.0 / 1.0000001 ** (i + 1) for i in range(MAX_DATA_SIZE)]\n\n\ndef piece_data(raw_data, i, piece_size):\n    return raw_data[i: piece_size + i]\n\n\ndef piece_label(raw_data, i, piece_size):\n    return raw_data[i + 1: piece_size + i + 1]\n\n\ndef data_idx():\n    return [i for i in range(MAX_DATA_SIZE / 100 * 9)], [j + MAX_DATA_SIZE / 100 * 9 for j in\n                                                         range(MAX_DATA_SIZE / 100)]\n    # return [i for i in range(900)], [j + 90 for j in range(100)]\n\n\nclass TrainBatch(object):\n    def __init__(self):\n        self.cur_idx = 0\n        self.cur_test_idx = 0\n        self.raw = raw_data()\n        self.train_idxs, test_idxs = data_idx()\n        self.X_train = [piece_data(self.raw, i, batch_size) for i in self.train_idxs]\n        self.y_train = [piece_label(self.raw, i, batch_size) for i in self.train_idxs]\n\n    def next_train(self):\n        self.cur_idx += 1\n        cur_train_data = np.array(\n            self.X_train[batch_cnt_per_step * (self.cur_idx - 1): batch_cnt_per_step * self.cur_idx])\n        cur_train_label = np.array(\n            self.y_train[batch_cnt_per_step * (self.cur_idx - 1): batch_cnt_per_step * self.cur_idx])\n        # print(\'*\' * 80)\n        # print(cur_train_data)\n        # print(\'=\' * 80)\n        # print(cur_train_label)\n        # print(\'*\' * 80)\n        # print(self.cur_idx)\n        return cur_train_data.reshape((batch_cnt_per_step, batch_size, EMBEDDING_SIZE)), \\\n               cur_train_label.reshape((batch_cnt_per_step, batch_size, EMBEDDING_SIZE))\n\n\n# Simple LSTM Model.\nnum_nodes = 64\ntrain_batch = TrainBatch()\n\n\ndef logprob(predictions, labels):\n    # prevent negative probability\n    """"""Log-probability of the true labels in a predicted batch.""""""\n    predictions[predictions < 1e-10] = 1e-10\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Parameters:\n    # Input, Forget, Memory, Output gate: input, previous output, and bias.\n    ifcox = tf.Variable(tf.truncated_normal([EMBEDDING_SIZE, num_nodes * 4], -0.1, 0.1))\n    ifcom = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n    ifcob = tf.Variable(tf.zeros([1, num_nodes * 4]))\n\n    # Variables saving state across unrollings.\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    # Classifier weights and biases.\n    w = tf.Variable(tf.truncated_normal([num_nodes, EMBEDDING_SIZE], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([EMBEDDING_SIZE]))\n\n\n    def _slice(_x, n, dim):\n        return _x[:, n * dim:(n + 1) * dim]\n\n    # Definition of the cell computation.\n    def lstm_cell(cur_input, last_output, last_state, drop):\n        if drop:\n            cur_input = tf.nn.dropout(cur_input, 0.8)\n        ifco_gates = tf.matmul(cur_input, ifcox) + tf.matmul(last_output, ifcom) + ifcob\n        input_gate = tf.sigmoid(_slice(ifco_gates, 0, num_nodes))\n        forget_gate = tf.sigmoid(_slice(ifco_gates, 1, num_nodes))\n        update = _slice(ifco_gates, 2, num_nodes)\n        last_state = forget_gate * last_state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(_slice(ifco_gates, 3, num_nodes))\n        output_gate *= tf.tanh(last_state)\n        if drop:\n            output_gate = tf.nn.dropout(output_gate, 0.8)\n        return output_gate, last_state\n\n\n    # Input data.\n    train_inputs = [tf.placeholder(tf.float32, shape=[batch_size, EMBEDDING_SIZE]) for _ in range(batch_cnt_per_step)]\n    train_labels = [tf.placeholder(tf.float32, shape=[batch_size, EMBEDDING_SIZE]) for _ in range(batch_cnt_per_step)]\n    # print(\'#######\', train_inputs)\n    # print(\'#######\', train_labels)\n\n    # Unrolled LSTM loop.\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    #######################################################################################\n    # This is multi lstm layer\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state, True)\n        outputs.append(output)\n    #######################################################################################\n\n    # State saving\n    with tf.control_dependencies([saved_output.assign(output),\n                                  saved_state.assign(state)]):\n        # Classifier.\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        print(logits)\n        print(tf.concat(train_labels, 0))\n        loss = tf.reduce_mean(tf.square(tf.sub(logits, tf.concat(train_labels, 0))))\n\n    # Optimizer.\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(\n        1.0, global_step, 100, 0.9, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(\n        zip(gradients, v), global_step=global_step)\n\n    # Predictions, not softmax for no label\n    train_prediction = logits\n\n    # Sampling and validation eval: batch 1, no unrolling.\n    sample_input = tf.placeholder(tf.float32, shape=[batch_size, EMBEDDING_SIZE])\n    saved_sample_output = tf.Variable(tf.zeros([batch_size, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([batch_size, num_nodes]))\n    reset_sample_state = tf.group(\n        saved_sample_output.assign(tf.zeros([batch_size, num_nodes])),\n        saved_sample_state.assign(tf.zeros([batch_size, num_nodes])))\n    sample_output, sample_state = lstm_cell(\n        sample_input, saved_sample_output, saved_sample_state, False)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                  saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.xw_plus_b(sample_output, w, b)\n\nnum_steps = 3501  # \xe4\xb8\x8a\xe9\x99\x9089000\nsum_freq = 50\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    mean_loss = 0\n    for step in range(num_steps):\n        # prepare and feed train data\n        input_s, label_s = train_batch.next_train()\n        feed_dict = dict()\n        for i in range(batch_cnt_per_step):\n            feed_dict[train_inputs[i]] = input_s[i]\n        for i in range(batch_cnt_per_step):\n            feed_dict[train_labels[i]] = label_s[i]\n\n        # train\n        _, l, predictions, lr = session.run(\n            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        predictions = predictions.reshape((batch_cnt_per_step, batch_size, EMBEDDING_SIZE))\n        mean_loss += l\n        if step % sum_freq == 0 and step != 0:\n            if step > 0:\n                mean_loss /= sum_freq\n            # The mean loss is an estimate of the loss over the last few batches.\n            print(\n                \'Average loss at step %d: %f learning rate: %f\' % (step, mean_loss, lr))\n            mean_loss = 0\n            print(\'Minibatch perplexity: %.2f\' % float(\n                np.exp(logprob(predictions, label_s))))\n            if step % (sum_freq * 10) == 0:\n                # Generate some samples.\n                print((\'=\' * 40) + \'valid\' + \'=\' * 40)\n\n                feeds, feed_labels = train_batch.next_train()\n                for i in range(batch_cnt_per_step / 3):\n                    feed = feeds[i]\n                    feed_label = feed_labels[i]\n                    f_prediction = sample_prediction.eval({sample_input: feed})\n                    print (\'label:\')\n                    print(feed_label)\n                    print(\'predict:\')\n                    print(f_prediction)\n                    print(\'Minibatch perplexity: %.2f\' % float(np.exp(logprob(f_prediction, feed_label))))\n                print(\'=\' * 80)\n'"
src/rnn/seq2seq.py,3,"b'# coding=utf-8\nimport math\nimport string\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\nimport seq2seq_model\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    f = zipfile.ZipFile(filename)\n    for name in f.namelist():\n        return tf.compat.as_str(f.read(name))\n    f.close()\n\n\ndata_set = load_pickle(\'text8_text.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    text = read_data(filename)\n    print(\'Data size %d\' % len(text))\n    save_obj(\'text8_text.pickle\', text)\nelse:\n    text = data_set\n\n# Create a small validation set.\nvalid_size = 100\nvalid_text = text[:valid_size]\ntrain_text = text[valid_size:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(valid_size, valid_text[:64])\n\nvocabulary_size = 35  # len(string.ascii_lowercase) + 2 # [a-z] + \' \'\nfirst_letter = ord(string.ascii_lowercase[0])\n\n\ndef char2id(char):\n    if char in string.ascii_lowercase:\n        return ord(char) - first_letter + 5\n    elif char == \' \':\n        return 4\n    elif char == \'!\':\n        return 31\n    else:\n        print(\'Unexpected character: %s\' % char)\n        return 0\n\n\ndef id2char(dictid):\n    if dictid == 31:\n        return \'!\'\n    elif dictid > 4:\n        return chr(dictid + first_letter - 5)\n    elif dictid == 4:\n        return \' \'\n    else:\n        return \'@\'\n\n\nprint(char2id(\'a\'), char2id(\'z\'), char2id(\' \'), char2id(\'!\'))\nprint(id2char(5), id2char(30), id2char(4), id2char(31))\nbatch_size = 64\nnum_unrollings = 19\n\n\nclass BatchGenerator(object):\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size = len(text)\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // num_unrollings\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        self._last_batch = self._next_batch(0)\n\n    def _next_batch(self, step):\n        """"""Generate a single batch from the current cursor position in the data.""""""\n        batch = \'\'\n        # print(\'text size\', self._text_size)\n        for b in range(self._num_unrollings):\n            # print(self._cursor[step])\n            self._cursor[step] %= self._text_size\n            batch += self._text[self._cursor[step]]\n            self._cursor[step] += 1\n        return batch\n\n    def next(self):\n        """"""Generate the next array of batches from the data. The array consists of\n        the last batch of the previous array, followed by num_unrollings new ones.\n        """"""\n        batches = [self._last_batch]\n        for step in range(self._batch_size):\n            batches.append(self._next_batch(step))\n        self._last_batch = batches[-1]\n        return batches\n\n\ndef characters(probabilities):\n    """"""Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.""""""\n    return [id2char(c) for c in np.argmax(probabilities, 1)]\n\n\ndef ids(probabilities):\n    """"""Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.""""""\n    return [str(c) for c in np.argmax(probabilities, 1)]\n\n\ndef batches2id(batches):\n    """"""Convert a sequence of batches back into their (most likely) string\n    representation.""""""\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, ids(b))]\n    return s\n\ntrain_batches = BatchGenerator(train_text, batch_size, num_unrollings)\nvalid_batches = BatchGenerator(valid_text, 1, num_unrollings)\n\n\ndef rev_id(forward):\n    temp = forward.split(\' \')\n    backward = []\n    for i in range(len(temp)):\n        backward += temp[i][::-1] + \' \'\n    return map(lambda x: char2id(x), backward[:-1] + [\'!\'])\n\n\ndef create_model(sess, forward_only):\n    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n                                       target_vocab_size=vocabulary_size,\n                                       buckets=[(20, 21)],\n                                       size=256,\n                                       num_layers=4,\n                                       max_gradient_norm=5.0,\n                                       batch_size=batch_size,\n                                       learning_rate=1.0,\n                                       learning_rate_decay_factor=0.9,\n                                       use_lstm=True,\n                                       forward_only=forward_only)\n    return model\n\n\nwith tf.Session() as sess:\n    model = create_model(sess, False)\n    sess.run(tf.global_variables_initializer())\n    num_steps = 30001\n\n    # This is the training loop.\n    step_time, loss = 0.0, 0.0\n    current_step = 0\n    previous_losses = []\n    step_ckpt = 100\n    valid_ckpt = 500\n\n    for step in range(1, num_steps):\n        model.batch_size = batch_size\n        train_batches_next = train_batches.next()\n        batches = train_batches_next\n        train_sets = []\n        batch_encs = map(lambda x: map(lambda y: char2id(y), list(x)), batches)\n        batch_decs = map(lambda x: rev_id(x), batches)\n        for i in range(len(batch_encs)):\n            train_sets.append((batch_encs[i], batch_decs[i]))\n\n        # Get a batch and make a step.\n        encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n        _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n\n        loss += step_loss / step_ckpt\n\n        # Once in a while, we save checkpoint, print statistics, and run evals.\n        if step % step_ckpt == 0:\n            # Print statistics for the previous epoch.\n            perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n            print (""global step %d learning rate %.4f perplexity ""\n                   ""%.2f"" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n            # Decrease learning rate if no improvement was seen over last 3 times.\n            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n                sess.run(model.learning_rate_decay_op)\n            previous_losses.append(loss)\n\n            loss = 0.0\n\n            if step % valid_ckpt == 0:\n                v_loss = 0.0\n\n                model.batch_size = 1\n                batches = [\'the quick brown fox\']\n                test_sets = []\n                batch_encs = map(lambda x: map(lambda y: char2id(y), list(x)), batches)\n                # batch_decs = map(lambda x: rev_id(x), batches)\n                test_sets.append((batch_encs[0], []))\n                # Get a 1-element batch to feed the sentence to the model.\n                encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n                # Get output logits for the sentence.\n                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n\n                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n                # If there is an EOS symbol in outputs, cut them at that point.\n\n                if char2id(\'!\') in outputs:\n                    outputs = outputs[:outputs.index(char2id(\'!\'))]\n\n                print(\'>>>>>>>>> \', batches[0], \' -> \', \'\'.join(map(lambda x: id2char(x), outputs)))\n\n                for _ in range(valid_size):\n                    model.batch_size = 1\n                    v_batches = valid_batches.next()\n                    valid_sets = []\n                    v_batch_encs = map(lambda x: map(lambda y: char2id(y), list(x)), v_batches)\n                    v_batch_decs = map(lambda x: rev_id(x), v_batches)\n                    for i in range(len(v_batch_encs)):\n                        valid_sets.append((v_batch_encs[i], v_batch_decs[i]))\n                    encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n                    v_loss += eval_loss / valid_size\n\n                eval_ppx = math.exp(v_loss) if v_loss < 300 else float(\'inf\')\n                print(""  valid eval:  perplexity %.2f"" % (eval_ppx))\n\n    # reuse variable -> subdivide into two boxes\n    model.batch_size = 1  # We decode one sentence at a time.\n    batches = [\'the quick brown fox\']\n    test_sets = []\n    batch_encs = map(lambda x: map(lambda y: char2id(y), list(x)), batches)\n    # batch_decs = map(lambda x: rev_id(x), batches)\n    test_sets.append((batch_encs[0], []))\n    # Get a 1-element batch to feed the sentence to the model.\n    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n    # Get output logits for the sentence.\n    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n    print (\'## : \', outputs)\n    # If there is an EOS symbol in outputs, cut them at that point.\n    if char2id(\'!\') in outputs:\n        outputs = outputs[:outputs.index(char2id(\'!\'))]\n\n    print(batches[0], \' -> \', \'\'.join(map(lambda x: id2char(x), outputs)))\n'"
src/rnn/seq2seq_model.py,27,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport random\n\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nimport data_utils\n\n\nclass Seq2SeqModel(object):\n  """"""Sequence-to-sequence model with attention and for multiple buckets.\n\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n  or into the seq2seq library for complete model implementation.\n  This class also allows to use GRU cells in addition to LSTM cells, and\n  sampled softmax to handle large output vocabulary size. A single-layer\n  version of this model, but with bi-directional encoder, was presented in\n    http://arxiv.org/abs/1409.0473\n  and sampled softmax is described in Section 3 of the following paper.\n    http://arxiv.org/abs/1412.2007\n  """"""\n\n  def __init__(self,\n               source_vocab_size,\n               target_vocab_size,\n               buckets,\n               size,\n               num_layers,\n               max_gradient_norm,\n               batch_size,\n               learning_rate,\n               learning_rate_decay_factor,\n               use_lstm=False,\n               num_samples=512,\n               forward_only=False,\n               dtype=tf.float32):\n    """"""Create the model.\n\n    Args:\n      source_vocab_size: size of the source vocabulary.\n      target_vocab_size: size of the target vocabulary.\n      buckets: a list of pairs (I, O), where I specifies maximum input length\n        that will be processed in that bucket, and O specifies maximum output\n        length. Training instances that have inputs longer than I or outputs\n        longer than O will be pushed to the next bucket and padded accordingly.\n        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n      size: number of units in each layer of the model.\n      num_layers: number of layers in the model.\n      max_gradient_norm: gradients will be clipped to maximally this norm.\n      batch_size: the size of the batches used during training;\n        the model construction is independent of batch_size, so it can be\n        changed after initialization if this is convenient, e.g., for decoding.\n      learning_rate: learning rate to start with.\n      learning_rate_decay_factor: decay learning rate by this much when needed.\n      use_lstm: if true, we use LSTM cells instead of GRU cells.\n      num_samples: number of samples for sampled softmax.\n      forward_only: if set, we do not construct the backward pass in the model.\n      dtype: the data type to use to store internal variables.\n    """"""\n    self.source_vocab_size = source_vocab_size\n    self.target_vocab_size = target_vocab_size\n    self.buckets = buckets\n    self.batch_size = batch_size\n    self.learning_rate = tf.Variable(\n        float(learning_rate), trainable=False, dtype=dtype)\n    self.learning_rate_decay_op = self.learning_rate.assign(\n        self.learning_rate * learning_rate_decay_factor)\n    self.global_step = tf.Variable(0, trainable=False)\n\n    # If we use sampled softmax, we need an output projection.\n    output_projection = None\n    softmax_loss_function = None\n    # Sampled softmax only makes sense if we sample less than vocabulary size.\n    if num_samples > 0 and num_samples < self.target_vocab_size:\n      w_t = tf.get_variable(""proj_w"", [self.target_vocab_size, size], dtype=dtype)\n      w = tf.transpose(w_t)\n      b = tf.get_variable(""proj_b"", [self.target_vocab_size], dtype=dtype)\n      output_projection = (w, b)\n\n      def sampled_loss(labels, logits):\n        labels = tf.reshape(labels, [-1, 1])\n        # We need to compute the sampled_softmax_loss using 32bit floats to\n        # avoid numerical instabilities.\n        local_w_t = tf.cast(w_t, tf.float32)\n        local_b = tf.cast(b, tf.float32)\n        local_inputs = tf.cast(logits, tf.float32)\n        return tf.cast(\n            tf.nn.sampled_softmax_loss(\n                weights=local_w_t,\n                biases=local_b,\n                labels=labels,\n                inputs=local_inputs,\n                num_sampled=num_samples,\n                num_classes=self.target_vocab_size),\n            dtype)\n      softmax_loss_function = sampled_loss\n\n    # Create the internal multi-layer cell for our RNN.\n    def single_cell():\n      return tf.contrib.rnn.GRUCell(size)\n    if use_lstm:\n      def single_cell():\n        return tf.contrib.rnn.BasicLSTMCell(size)\n    cell = single_cell()\n    if num_layers > 1:\n      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])\n\n    # The seq2seq function: we use embedding for the input and attention.\n    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n          encoder_inputs,\n          decoder_inputs,\n          cell,\n          num_encoder_symbols=source_vocab_size,\n          num_decoder_symbols=target_vocab_size,\n          embedding_size=size,\n          output_projection=output_projection,\n          feed_previous=do_decode,\n          dtype=dtype)\n\n    # Feeds for inputs.\n    self.encoder_inputs = []\n    self.decoder_inputs = []\n    self.target_weights = []\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""encoder{0}"".format(i)))\n    for i in xrange(buckets[-1][1] + 1):\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n                                                name=""decoder{0}"".format(i)))\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\n                                                name=""weight{0}"".format(i)))\n\n    # Our targets are decoder inputs shifted by one.\n    targets = [self.decoder_inputs[i + 1]\n               for i in xrange(len(self.decoder_inputs) - 1)]\n\n    # Training outputs and losses.\n    if forward_only:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n          softmax_loss_function=softmax_loss_function)\n      # If we use output projection, we need to project outputs for decoding.\n      if output_projection is not None:\n        for b in xrange(len(buckets)):\n          self.outputs[b] = [\n              tf.matmul(output, output_projection[0]) + output_projection[1]\n              for output in self.outputs[b]\n          ]\n    else:\n      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n          self.encoder_inputs, self.decoder_inputs, targets,\n          self.target_weights, buckets,\n          lambda x, y: seq2seq_f(x, y, False),\n          softmax_loss_function=softmax_loss_function)\n\n    # Gradients and SGD update operation for training the model.\n    params = tf.trainable_variables()\n    if not forward_only:\n      self.gradient_norms = []\n      self.updates = []\n      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n      for b in xrange(len(buckets)):\n        gradients = tf.gradients(self.losses[b], params)\n        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n                                                         max_gradient_norm)\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(clipped_gradients, params), global_step=self.global_step))\n\n    self.saver = tf.train.Saver(tf.global_variables())\n\n  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n           bucket_id, forward_only):\n    """"""Run a step of the model feeding the given inputs.\n\n    Args:\n      session: tensorflow session to use.\n      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n      target_weights: list of numpy float vectors to feed as target weights.\n      bucket_id: which bucket of the model to use.\n      forward_only: whether to do the backward step or only forward.\n\n    Returns:\n      A triple consisting of gradient norm (or None if we did not do backward),\n      average perplexity, and the outputs.\n\n    Raises:\n      ValueError: if length of encoder_inputs, decoder_inputs, or\n        target_weights disagrees with bucket size for the specified bucket_id.\n    """"""\n    # Check if the sizes match.\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    if len(encoder_inputs) != encoder_size:\n      raise ValueError(""Encoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(encoder_inputs), encoder_size))\n    if len(decoder_inputs) != decoder_size:\n      raise ValueError(""Decoder length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(decoder_inputs), decoder_size))\n    if len(target_weights) != decoder_size:\n      raise ValueError(""Weights length must be equal to the one in bucket,""\n                       "" %d != %d."" % (len(target_weights), decoder_size))\n\n    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n    input_feed = {}\n    for l in xrange(encoder_size):\n      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n    for l in xrange(decoder_size):\n      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n      input_feed[self.target_weights[l].name] = target_weights[l]\n\n    # Since our targets are decoder inputs shifted by one, we need one more.\n    last_target = self.decoder_inputs[decoder_size].name\n    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n    # Output feed: depends on whether we do a backward step or not.\n    if not forward_only:\n      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                     self.gradient_norms[bucket_id],  # Gradient norm.\n                     self.losses[bucket_id]]  # Loss for this batch.\n    else:\n      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n      for l in xrange(decoder_size):  # Output logits.\n        output_feed.append(self.outputs[bucket_id][l])\n\n    outputs = session.run(output_feed, input_feed)\n    if not forward_only:\n      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n    else:\n      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\n  def get_batch(self, data, bucket_id):\n    """"""Get a random batch of data from the specified bucket, prepare for step.\n\n    To feed data in step(..) it must be a list of batch-major vectors, while\n    data here contains single length-major cases. So the main logic of this\n    function is to re-index data cases to be in the proper format for feeding.\n\n    Args:\n      data: a tuple of size len(self.buckets) in which each element contains\n        lists of pairs of input and output data that we use to create a batch.\n      bucket_id: integer, which bucket to get the batch for.\n\n    Returns:\n      The triple (encoder_inputs, decoder_inputs, target_weights) for\n      the constructed batch that has the proper format to call step(...) later.\n    """"""\n    encoder_size, decoder_size = self.buckets[bucket_id]\n    encoder_inputs, decoder_inputs = [], []\n\n    # Get a random batch of encoder and decoder inputs from data,\n    # pad them if needed, reverse encoder inputs and add GO to decoder.\n    for _ in xrange(self.batch_size):\n      encoder_input, decoder_input = random.choice(data[bucket_id])\n\n      # Encoder inputs are padded and then reversed.\n      encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n      # Decoder inputs get an extra ""GO"" symbol, and are padded then.\n      decoder_pad_size = decoder_size - len(decoder_input) - 1\n      decoder_inputs.append([data_utils.GO_ID] + decoder_input +\n                            [data_utils.PAD_ID] * decoder_pad_size)\n\n    # Now we create batch-major vectors from the data selected above.\n    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n\n    # Batch encoder inputs are just re-indexed encoder_inputs.\n    for length_idx in xrange(encoder_size):\n      batch_encoder_inputs.append(\n          np.array([encoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n    for length_idx in xrange(decoder_size):\n      batch_decoder_inputs.append(\n          np.array([decoder_inputs[batch_idx][length_idx]\n                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n\n      # Create target_weights to be 0 for targets that are padding.\n      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n      for batch_idx in xrange(self.batch_size):\n        # We set weight to 0 if the corresponding target is a PAD symbol.\n        # The corresponding target is decoder_input shifted by 1 forward.\n        if length_idx < decoder_size - 1:\n          target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n          batch_weight[batch_idx] = 0.0\n      batch_weights.append(batch_weight)\n    return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n'"
src/rnn/singlew_lstm.py,35,"b'# coding=utf-8\nimport random\nimport string\nimport zipfile\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom not_mnist.img_pickle import save_obj, load_pickle\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    f = zipfile.ZipFile(filename)\n    for name in f.namelist():\n        return tf.compat.as_str(f.read(name))\n    f.close()\n\n\ndata_set = load_pickle(\'text8_text.pickle\')\nif data_set is None:\n    # load data\n    url = \'http://mattmahoney.net/dc/\'\n    filename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n    # read data\n    text = read_data(filename)\n    print(\'Data size %d\' % len(text))\n    save_obj(\'text8_text.pickle\', text)\nelse:\n    text = data_set\n\n# Create a small validation set.\nvalid_size = 1000\nvalid_text = text[:valid_size]\ntrain_text = text[valid_size:]\ntrain_size = len(train_text)\nprint(train_size, train_text[:64])\nprint(valid_size, valid_text[:64])\n\n# Utility functions to map characters to vocabulary IDs and back.\nvocabulary_size = len(string.ascii_lowercase) + 1  # [a-z] + \' \'\n# ascii code for character\nfirst_letter = ord(string.ascii_lowercase[0])\n\n\ndef char2id(char):\n    if char in string.ascii_lowercase:\n        return ord(char) - first_letter + 1\n    elif char == \' \':\n        return 0\n    else:\n        print(\'Unexpected character: %s\' % char)\n        return 0\n\n\ndef id2char(dictid):\n    if dictid > 0:\n        return chr(dictid + first_letter - 1)\n    else:\n        return \' \'\n\n\nprint(char2id(\'a\'), char2id(\'z\'), char2id(\' \'), char2id(\'\xc3\xaf\'))\nprint(id2char(1), id2char(26), id2char(0))\n\n# Function to generate a training batch for the LSTM model.\nbatch_size = 64\nnum_unrollings = 10\n\n\nclass BatchGenerator(object):\n    def __init__(self, text, batch_size, num_unrollings):\n        self._text = text\n        self._text_size = len(text)\n        self._batch_size = batch_size\n        self._num_unrollings = num_unrollings\n        segment = self._text_size // batch_size\n        self._cursor = [offset * segment for offset in range(batch_size)]\n        self._last_batch = self._next_batch()\n\n    def _next_batch(self):\n        """"""Generate a single batch from the current cursor position in the data.""""""\n        # take character from text on cursor[b]\n        # set to 1 for the taken character\n        # so we have a matrix of 1/0 as input, an one hot encoding\n        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n        for b in range(self._batch_size):\n            # same id, same index of second dimension\n            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n        return batch\n\n    def next(self):\n        """"""Generate the next array of batches from the data. The array consists of\n        the last batch of the previous array, followed by num_unrollings new ones.\n        """"""\n        batches = [self._last_batch]\n        for step in range(self._num_unrollings):\n            batches.append(self._next_batch())\n        self._last_batch = batches[-1]\n        return batches\n\n\ndef characters(probabilities):\n    """"""Turn a 1-hot encoding or a probability distribution over the possible\n    characters back into its (most likely) character representation.""""""\n    # argmax for the most likely character\n    return [id2char(c) for c in np.argmax(probabilities, 1)]\n\n\ndef batches2string(batches):\n    """"""Convert a sequence of batches back into their (most likely) string\n    representation.""""""\n    s = [\'\'] * batches[0].shape[0]\n    for b in batches:\n        s = [\'\'.join(x) for x in zip(s, characters(b))]\n    return s\n\n\ntrain_batches = BatchGenerator(train_text, batch_size, num_unrollings)\nvalid_batches = BatchGenerator(valid_text, 1, 1)\n\nprint(batches2string(train_batches.next()))\nprint(batches2string(train_batches.next()))\nprint(batches2string(valid_batches.next()))\nprint(batches2string(valid_batches.next()))\n\n\ndef logprob(predictions, labels):\n    # prevent negative probability\n    """"""Log-probability of the true labels in a predicted batch.""""""\n    predictions[predictions < 1e-10] = 1e-10\n    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n\n\ndef sample_distribution(distribution):\n    """"""Sample one element from a distribution assumed to be an array of normalized\n    probabilities.\n    """"""\n    # \xe5\x8f\x96\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\xa8\xe4\xba\x8e\xe8\xaf\x84\xe4\xbc\xb0\xef\xbc\x8c\xe6\x89\x80\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe6\xaf\x94\xe4\xbe\x8b\xe9\x9a\x8f\xe6\x9c\xba\n    r = random.uniform(0, 1)\n    s = 0\n    for i in range(len(distribution)):\n        s += distribution[i]\n        if s >= r:\n            return i\n    return len(distribution) - 1\n\n\ndef sample(prediction):\n    """"""Turn a (column) prediction into 1-hot encoded samples.""""""\n    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n    p[0, sample_distribution(prediction[0])] = 1.0\n    return p\n\n\ndef random_distribution():\n    """"""Generate a random column of probabilities.""""""\n    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n    return b / np.sum(b, 1)[:, None]\n\n\n# Simple LSTM Model.\nnum_nodes = 64\n\ngraph = tf.Graph()\nwith graph.as_default():\n    gate_count = 4\n    # Parameters:\n    # Gates: input, previous output, and bias.\n    input_weights = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * gate_count], -0.1, 0.1))\n    output_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * gate_count], -0.1, 0.1))\n    bias = tf.Variable(tf.zeros([1, num_nodes * gate_count]))\n    # Variables saving state across unrollings.\n    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n    # Classifier weights and biases.\n    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n    b = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Definition of the cell computation.\n    def lstm_cell(i, o, state):\n        """"""Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n        Note that in this formulation, we omit the various connections between the\n        previous state and the gates.""""""\n        # large weight, 1/4 parameters for each gate, matrix multiply once, take 1/4 output results as a gate\n        values = tf.split(tf.matmul(i, input_weights) + tf.matmul(o, output_weights) + bias, gate_count, 1)\n        input_gate = tf.sigmoid(values[0])\n        forget_gate = tf.sigmoid(values[1])\n        update = values[2]\n        state = forget_gate * state + input_gate * tf.tanh(update)\n        output_gate = tf.sigmoid(values[3])\n        return output_gate * tf.tanh(state), state\n\n    # Input data.\n    train_data = list()\n    for _ in range(num_unrollings + 1):\n        train_data.append(\n            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n    train_inputs = train_data[:num_unrollings]\n    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n\n    # Unrolled LSTM loop.\n    outputs = list()\n    output = saved_output\n    state = saved_state\n    for i in train_inputs:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n\n    # State saving across unrollings.\n    with tf.control_dependencies([saved_output.assign(output),\n                                  saved_state.assign(state)]):\n        # Classifier.\n        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n        loss = tf.reduce_mean(\n            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels, 0)))\n\n    # Optimizer.\n    global_step = tf.Variable(0)\n    learning_rate = tf.train.exponential_decay(\n        10.0, global_step, 5000, 0.1, staircase=True)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    gradients, v = zip(*optimizer.compute_gradients(loss))\n    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n    optimizer = optimizer.apply_gradients(\n        zip(gradients, v), global_step=global_step)\n\n    # Predictions.\n    train_prediction = tf.nn.softmax(logits)\n\n    # Sampling and validation eval: batch 1, no unrolling.\n    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n    reset_sample_state = tf.group(\n        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n    sample_output, sample_state = lstm_cell(\n        sample_input, saved_sample_output, saved_sample_state)\n    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n                                  saved_sample_state.assign(sample_state)]):\n        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n\nnum_steps = 7001\nsummary_frequency = 100\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    mean_loss = 0\n    for step in range(num_steps):\n        batches = train_batches.next()\n        feed_dict = dict()\n        for i in range(num_unrollings + 1):\n            feed_dict[train_data[i]] = batches[i]\n        _, l, predictions, lr = session.run(\n            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n        mean_loss += l\n        if step % summary_frequency == 0:\n            if step > 0:\n                mean_loss /= summary_frequency\n            # The mean loss is an estimate of the loss over the last few batches.\n            print(\n                \'Average loss at step %d: %f learning rate: %f\' % (step, mean_loss, lr))\n            mean_loss = 0\n            labels = np.concatenate(list(batches)[1:])\n            print(\'Minibatch perplexity: %.2f\' % float(\n                np.exp(logprob(predictions, labels))))\n            if step % (summary_frequency * 10) == 0:\n                # Generate some samples.\n                print(\'=\' * 80)\n                for _ in range(5):\n                    feed = sample(random_distribution())\n                    sentence = characters(feed)[0]\n                    reset_sample_state.run()\n                    for _ in range(79):\n                        prediction = sample_prediction.eval({sample_input: feed})\n                        feed = sample(prediction)\n                        sentence += characters(feed)[0]\n                    print(sentence)\n                print(\'=\' * 80)\n            # Measure validation set perplexity.\n            reset_sample_state.run()\n            valid_logprob = 0\n            for _ in range(valid_size):\n                b = valid_batches.next()\n                predictions = sample_prediction.eval({sample_input: b[0]})\n                valid_logprob = valid_logprob + logprob(predictions, b[1])\n            print(\'Validation set perplexity: %.2f\' % float(np.exp(\n                valid_logprob / valid_size)))'"
src/rnn/word2vec.py,21,"b'import zipfile\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport math\nimport collections\n\nfrom matplotlib import pylab\nfrom sklearn.manifold import TSNE\n\nfrom not_mnist.img_pickle import save_obj\nfrom not_mnist.load_data import maybe_download\n\n\ndef read_data(filename):\n    """"""Extract the first file enclosed in a zip file as a list of words""""""\n    with zipfile.ZipFile(filename) as f:\n        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n\ndef build_dataset(words):\n    count = [[\'UNK\', -1]]\n    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # dictionary[\'UNK\']\n            unk_count = unk_count + 1\n        data.append(index)\n    count[0][1] = unk_count\n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reverse_dictionary\n\n\ndef generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.float32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    return batch, labels\n\n\n# load data\nurl = \'http://mattmahoney.net/dc/\'\nfilename = maybe_download(\'text8.zip\', 31344016, url=url)\n\n# read data\nwords = read_data(filename)\nprint(\'Data size %d\' % len(words))\n\nvocabulary_size = 50000\ncontext_size = 1\n\ndata, count, dictionary, reverse_dictionary = build_dataset(words)\nprint(\'Most common words (+UNK)\', count[:5])\nprint(\'Sample data\', data[:10])\ndel words  # Hint to reduce memory.\n\n# split data\ndata_index = 0\n\nprint(\'data:\', [reverse_dictionary[di] for di in data[:8]])\n\nfor num_skips, skip_window in [(2, 1), (4, 2)]:\n    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n    print(\'\\nwith num_skips = %d and skip_window = %d:\' % (num_skips, skip_window))\n    print(\'    batch:\', [reverse_dictionary[bi] for bi in batch])\n    print(\'    labels:\', [reverse_dictionary[li] for li in labels.reshape(8)])\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\nskip_window = 1  # How many words to consider left and right.\nnum_skips = 2  # How many times to reuse an input to generate a label.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16  # Random set of words to evaluate similarity on.\nvalid_window = 100  # Only pick dev samples in the head of the distribution.\nvalid_examples = np.array(random.sample(range(valid_window), valid_size))\nnum_sampled = 64  # Number of negative examples to sample.\n\n# tensor: Train a skip-gram model, word2vec\ngraph = tf.Graph()\n\nwith graph.as_default(), tf.device(\'/gpu:0\'):\n    # Input data.\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Variables.\n    embeddings = tf.Variable(\n        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    softmax_weights = tf.Variable(\n        tf.truncated_normal([vocabulary_size, embedding_size],\n                            stddev=1.0 / math.sqrt(embedding_size)))\n    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Model.\n    # Look up embeddings for inputs.\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n    # Compute the softmax loss, using a sample of the negative labels each time.\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, train_labels, embed,\n                                   num_sampled, vocabulary_size))\n\n    # Optimizer.\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings / norm\n    valid_embeddings = tf.nn.embedding_lookup(\n        normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n\n# flow\nnum_steps = 100001\nconfig = tf.ConfigProto(allow_soft_placement=True)\nwith tf.Session(graph=graph, config=config) as session:\n    tf.global_variables_initializer().run()\n    print(\'Initialized\')\n    average_loss = 0\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(\n            batch_size, num_skips, skip_window)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss /= 2000\n            # The average loss is an estimate of the loss over the last 2000 batches.\n            print(\'Average loss at step %d: %f\' % (step, average_loss))\n            average_loss = 0\n        # note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log = \'Nearest to %s:\' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = \'%s %s,\' % (log, close_word)\n                print(log)\n    final_embeddings = normalized_embeddings.eval()\n    save_obj(\'text8_embed.pickle\', final_embeddings)\n\nnum_points = 400\n\ntsne = TSNE(perplexity=30, n_components=2, init=\'pca\', n_iter=5000)\ntwo_d_embeddings = tsne.fit_transform(final_embeddings[1:num_points + 1, :])\n\n\ndef plot(embeddings, labels):\n    assert embeddings.shape[0] >= len(labels), \'More labels than embeddings\'\n    pylab.figure(figsize=(15, 15))  # in inches\n    for i, label in enumerate(labels):\n        x, y = embeddings[i, :]\n        pylab.scatter(x, y)\n        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=\'offset points\',\n                       ha=\'right\', va=\'bottom\')\n    pylab.show()\n\n\nwords = [reverse_dictionary[i] for i in range(1, num_points + 1)]\nplot(two_d_embeddings, words)\n'"
src/skflow/__init__.py,0,b''
src/skflow/skflow_cnn.py,22,"b'#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n""""""Convolutional Neural Network Estimator for MNIST, built with tf.layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\nfrom tensorflow.contrib.learn import SKCompat\nfrom tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef cnn_model_fn(features, labels, mode):\n    """"""Model function for CNN.""""""\n    # Input Layer\n    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n    # MNIST images are 28x28 pixels, and have one color channel\n    input_layer = tf.reshape(features, [-1, 28, 28, 1])\n\n    # Convolutional Layer #1\n    # Computes 32 features using a 5x5 filter with ReLU activation.\n    # Padding is added to preserve width and height.\n    # Input Tensor Shape: [batch_size, 28, 28, 1]\n    # Output Tensor Shape: [batch_size, 28, 28, 32]\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding=""same"",\n        activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    # First max pooling layer with a 2x2 filter and stride of 2\n    # Input Tensor Shape: [batch_size, 28, 28, 32]\n    # Output Tensor Shape: [batch_size, 14, 14, 32]\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    # Convolutional Layer #2\n    # Computes 64 features using a 5x5 filter.\n    # Padding is added to preserve width and height.\n    # Input Tensor Shape: [batch_size, 14, 14, 32]\n    # Output Tensor Shape: [batch_size, 14, 14, 64]\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding=""same"",\n        activation=tf.nn.relu)\n\n    # Pooling Layer #2\n    # Second max pooling layer with a 2x2 filter and stride of 2\n    # Input Tensor Shape: [batch_size, 14, 14, 64]\n    # Output Tensor Shape: [batch_size, 7, 7, 64]\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    # Flatten tensor into a batch of vectors\n    # Input Tensor Shape: [batch_size, 7, 7, 64]\n    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n    # Dense Layer\n    # Densely connected layer with 1024 neurons\n    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n    # Output Tensor Shape: [batch_size, 1024]\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n    # Add dropout operation; 0.6 probability that element will be kept\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=mode == learn.ModeKeys.TRAIN)\n\n    # Logits layer\n    # Input Tensor Shape: [batch_size, 1024]\n    # Output Tensor Shape: [batch_size, 10]\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    loss = None\n    train_op = None\n\n    # Calculate Loss (for both TRAIN and EVAL modes)\n    if mode != learn.ModeKeys.INFER:\n        onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n        loss = tf.losses.softmax_cross_entropy(\n            onehot_labels=onehot_labels, logits=logits)\n\n    # Configure the Training Op (for TRAIN mode)\n    if mode == learn.ModeKeys.TRAIN:\n        train_op = tf.contrib.layers.optimize_loss(\n            loss=loss,\n            global_step=tf.contrib.framework.get_global_step(),\n            learning_rate=0.001,\n            optimizer=""SGD"")\n\n    # Generate Predictions\n    predictions = {\n        ""classes"": tf.argmax(\n            input=logits, axis=1),\n        ""probabilities"": tf.nn.softmax(\n            logits, name=""softmax_tensor"")\n    }\n\n    # Return a ModelFnOps object\n    return model_fn_lib.ModelFnOps(\n        mode=mode, predictions=predictions, loss=loss, train_op=train_op)\n\n\ndef main(unused_argv):\n    # Load training and eval data\n    mnist = learn.datasets.load_dataset(""mnist"")\n    train_data = mnist.train.images  # Returns np.array\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n    eval_data = mnist.test.images  # Returns np.array\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n\n    # Create the Estimator\n    mnist_classifier = SKCompat(learn.Estimator(\n        model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model""))\n\n    # Set up logging for predictions\n    # Log the values in the ""Softmax"" tensor with label ""probabilities""\n    tensors_to_log = {""probabilities"": ""softmax_tensor""}\n    logging_hook = tf.train.LoggingTensorHook(\n        tensors=tensors_to_log, every_n_iter=50)\n\n    # Train the model\n    mnist_classifier.fit(\n        x=train_data,\n        y=train_labels,\n        batch_size=100,\n        steps=20000,\n        monitors=[logging_hook])\n\n    # Configure the accuracy metric for evaluation\n    metrics = {\n        ""accuracy"":\n            learn.MetricSpec(\n                metric_fn=tf.metrics.accuracy, prediction_key=""classes""),\n    }\n\n    # Evaluate the model and print results\n    eval_results = mnist_classifier.evaluate(\n        x=eval_data, y=eval_labels, metrics=metrics)\n    print(eval_results)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
src/skflow/skflow_rnn.py,6,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sklearn import metrics, preprocessing\n\nimport tensorflow as tf\n# tensorflow 0.8\nfrom tensorflow.contrib import learn\n\n# Parameters\nlearning_rate = 0.1\ntraining_steps = 3000\nbatch_size = 128\n\n# Network Parameters\nn_input = 28  # MNIST data input (img shape: 28*28)\nn_steps = 28  # timesteps\nn_hidden = 128  # hidden layer num of features\nn_classes = 10  # MNIST total classes (0-9 digits)\n\n### Download and load MNIST data.\nmnist = learn.datasets.load_dataset(\'mnist\')\n\nX_train = mnist.train.images\ny_train = mnist.train.labels\nX_test = mnist.test.images\ny_test = mnist.test.labels\n\n# It\'s useful to scale to ensure Stochastic Gradient Descent will do the right thing\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n\ndef rnn_model(X, y):\n    X = tf.reshape(X, [-1, n_steps, n_input])  # (batch_size, n_steps, n_input)\n    # # permute n_steps and batch_size\n    X = tf.transpose(X, [1, 0, 2])\n    # # Reshape to prepare input to hidden activation\n    X = tf.reshape(X, [-1, n_input])  # (n_steps*batch_size, n_input)\n    # # Split data because rnn cell needs a list of inputs for the RNN inner loop\n    X = tf.split(0, n_steps, X)  # n_steps * (batch_size, n_input)\n\n    # Define a GRU cell with tensorflow\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n    # Get lstm cell output\n    output, encoding = tf.nn.rnn(lstm_cell, X, dtype=tf.float32)\n    print(X)\n    print(encoding)\n    print(output)\n\n    return learn.models.logistic_regression(encoding, y)\n\n\nclassifier = learn.TensorFlowEstimator(model_fn=rnn_model, n_classes=n_classes,\n                                       batch_size=batch_size,\n                                       steps=training_steps,\n                                       learning_rate=learning_rate)\n\nclassifier.fit(X_train, y_train, logdir=""/tmp/mnist_rnn"")\nscore = metrics.accuracy_score(y_test, classifier.predict(X_test))\nprint(\'Accuracy: {0:f}\'.format(score))\n'"
src/util/__init__.py,0,b''
src/util/board.py,18,"b'import tensorflow as tf\n\n\ndef variable_summaries(var, name):\n    """"""Attach a lot of summaries to a Tensor.""""""\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.scalar_summary(\'mean/\' + name, mean)\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n        tf.scalar_summary(\'sttdev/\' + name, stddev)\n        tf.scalar_summary(\'max/\' + name, tf.reduce_max(var))\n        tf.scalar_summary(\'min/\' + name, tf.reduce_min(var))\n        tf.histogram_summary(name, var)\n\n\ndef variable_summary(var):\n    """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n    with tf.name_scope(\'summaries\'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar(\'mean\', mean)\n        with tf.name_scope(\'stddev\'):\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar(\'stddev\', stddev)\n        tf.summary.scalar(\'max\', tf.reduce_max(var))\n        tf.summary.scalar(\'min\', tf.reduce_min(var))\n        tf.summary.histogram(\'histogram\', var)'"
src/util/file_helper.py,0,"b'def write(path, content):\n    with open(path, ""a+"") as dst_file:\n        dst_file.write(content + \'\\n\')\n\n\ndef read2mem(path):\n    with open(path) as f:\n        content = \'\'\n        while 1:\n            try:\n                lines = f.readlines(100)\n            except UnicodeDecodeError:\n                f.close()\n                continue\n            if not lines:\n                break\n            for line in lines:\n                content += line\n    return content\n\n\ndef read_lines(path):\n    with open(path) as f:\n        content = list()\n        while 1:\n            try:\n                lines = f.readlines(100)\n            except UnicodeDecodeError:\n                f.close()\n                continue\n            if not lines:\n                break\n            for line in lines:\n                content.append(line)\n    return content\n'"
src/util/mnist.py,0,"b'from tensorflow.examples.tutorials.mnist import input_data\n\nfrom not_mnist.pick import save_obj\nfrom not_mnist.pick import load_pickle\nimport numpy as np\n\nmnist_path = \'mnist\'\n\n\ndef img_reshape(data, length):\n    img_size = 28\n    depth = 1\n    # print(len(data))\n    # print(length)\n    return np.array(data).reshape(length, img_size, img_size, depth)\n\n\ndef label_reshape(data, length):\n    label_size = 10\n    # print(len(data))\n    # print(length)\n    return np.array(data).reshape(length, label_size)\n\n\ndef format_mnist():\n    mnist = load_pickle(mnist_path)\n    if mnist is None:\n        mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n        save_obj(mnist_path, mnist)\n    train_length = len(mnist.train.labels)\n    valid_length = len(mnist.validation.labels)\n    test_length = len(mnist.test.labels)\n    return img_reshape(mnist.train.images, train_length), label_reshape(mnist.train.labels, train_length), \\\n           img_reshape(mnist.validation.images, valid_length), label_reshape(mnist.validation.labels, valid_length), \\\n           img_reshape(mnist.test.images, test_length), label_reshape(mnist.test.labels, test_length)\n\n\nif __name__ == \'__main__\':\n    format_mnist()\n'"
src/util/request.py,0,"b""# coding=utf-8\nimport json\nimport urllib\nimport urllib2\n\n\nfit_url = 'http://127.0.0.1:8000/fit/'\nfit_trend_url = 'http://127.0.0.1:8000/fit/trend/'\nbetter_hp_trend_url = 'http://127.0.0.1:8000/fit/trend/'\nhp2trend_url = 'http://127.0.0.1:8000/hp2trend/'\nhalf_trend_url = 'http://127.0.0.1:8000/half_trend/'\nfit2_url = 'http://127.0.0.1:8000/fit2/'\nbetter_hp_url = 'http://127.0.0.1:8000/hyper/'\npredict_loss_url = 'http://127.0.0.1:8000/predict/'\n\n\ndef fit_loss(reset, hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss, 'reset': reset})\n    req = urllib2.Request(fit_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    return res\n\n\ndef fit_more(reset, hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss, 'reset': reset})\n    req = urllib2.Request(fit2_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    return res\n\n\ndef better_hyper(hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss})\n    req = urllib2.Request(better_hp_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    better_hypers = res['msg']\n    # print(res)\n    return better_hypers\n\n\ndef predict_future(hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss})\n    req = urllib2.Request(predict_loss_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    more_index = res['msg']\n    # print(res)\n    return more_index\n\n\ndef fit_trend(hypers, loss_es):\n    sample_loss = list()\n    for i in range(len(loss_es)):\n        if i % 10 == 0:\n            sample_loss.append(loss_es[i])\n    data = urllib.urlencode({'hyper': hypers, 'loss': sample_loss, 'reset': 0})\n    req = urllib2.Request(fit_trend_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    return res['ret']\n\n\ndef better_trend_hyper(hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss})\n    req = urllib2.Request(hp2trend_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    better_hypers = res['msg']\n    # print(res)\n    return better_hypers\n\n\ndef half_trend_hyper(hypers, loss):\n    data = urllib.urlencode({'hyper': hypers, 'loss': loss})\n    req = urllib2.Request(half_trend_url, data)\n    response = urllib2.urlopen(req)\n    res = json.loads(response.read())\n    better_hypers = res['msg']\n    # print(res)\n    return better_hypers\n"""
src/app/caltech/__init__.py,0,b''
src/app/caltech/data.py,0,"b""# coding=utf-8\nimport os\nimport random\n\nfrom util.file_helper import read_lines\n\ncaltch_path = '/Users/cwh/Mission/scut/lesson/PR/project/PR_dataset/'\ncaltech_train_path = caltch_path + 'train/'\ncaltech_test_path = caltch_path + 'test/'\ncaltech_train_pos_path = caltech_train_path + 'pos/'\ncaltech_train_neg_path = caltech_train_path + 'neg/'\ncaltech_test_pos_path = caltech_test_path + 'pos/'\ncaltech_test_neg_path = caltech_test_path + 'neg/'\n\n\ndef read_caltech():\n    train_features = list()\n    train_labels = list()\n    test_features = list()\n    test_labels = list()\n    i = 0\n    max_read = 15000\n    for files in os.listdir(caltech_train_pos_path):\n        if i > max_read:\n            break\n        i += 1\n        path = os.path.join(caltech_train_pos_path, files)\n        features = read_lines(path)\n        # print(path)\n        train_features.append(features)\n        train_features.append(features)\n        train_features.append(features)\n        train_labels.append([1, 0])\n        train_labels.append([1, 0])\n        train_labels.append([1, 0])\n    pos_train_cnt = len(train_labels) / 3\n    print(pos_train_cnt)\n    i = 0\n    for files in os.listdir(caltech_train_neg_path):\n        if i > max_read:\n            break\n        i += 1\n        path = os.path.join(caltech_train_neg_path, files)\n        # print(path)\n        features = read_lines(path)\n        train_features.append(features)\n        train_labels.append([0, 1])\n    print(len(train_labels) - pos_train_cnt * 3)\n    i = 0\n    for files in os.listdir(caltech_test_pos_path):\n        if i > max_read:\n            break\n        i += 1\n        path = os.path.join(caltech_test_pos_path, files)\n        # print(path)\n        features = read_lines(path)\n        test_features.append(features)\n        test_labels.append([1, 0])\n    pos_test_cnt = len(test_labels) / 3\n    print(pos_test_cnt)\n    i = 0\n    for files in os.listdir(caltech_test_neg_path):\n        if i > max_read:\n            break\n        i += 1\n        path = os.path.join(caltech_test_neg_path, files)\n        # print(path)\n        features = read_lines(path)\n        test_features.append(features)\n        test_labels.append([0, 1])\n    print(len(test_labels) - pos_test_cnt)\n    train_tuples = zip(train_features, train_labels)\n    random.shuffle(train_tuples)\n    train_features = [train_tuple[0] for train_tuple in train_tuples]\n    train_labels = [train_tuple[1] for train_tuple in train_tuples]\n    test_tuples = zip(test_features, test_labels)\n    random.shuffle(test_tuples)\n    test_features = [test_tuple[0] for test_tuple in test_tuples]\n    test_labels = [test_tuple[1] for test_tuple in test_tuples]\n\n    return train_features, train_labels, test_features, test_labels\n\n\nif __name__ == '__main__':\n    read_caltech()\n"""
src/app/caltech/dnn_caltech.py,44,"b'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom app.caltech.data import read_caltech\nfrom neural.full_connect import accuracy\n\n\ndef tf_deep_nn(regular=False, drop_out=False, lrd=False, layer_cnt=2):\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, feature_dim))\n        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n        tf_valid_dataset = tf.constant(valid_dataset)\n        tf_test_dataset = tf.constant(test_dataset)\n\n        hidden_node_count = 32\n        # start weight\n        hidden_stddev = np.sqrt(2.0 / 100)\n        weights1 = tf.Variable(tf.truncated_normal([feature_dim, hidden_node_count], stddev=hidden_stddev))\n        biases1 = tf.Variable(tf.zeros([hidden_node_count]))\n        # middle weight\n        weights = []\n        biases = []\n        hidden_cur_cnt = hidden_node_count\n        for i in range(layer_cnt - 2):\n            if hidden_cur_cnt > 2:\n                hidden_next_cnt = int(hidden_cur_cnt / 2)\n            else:\n                hidden_next_cnt = 2\n            hidden_stddev = np.sqrt(2.0 / hidden_cur_cnt / 10)\n            weights.append(tf.Variable(tf.truncated_normal([hidden_cur_cnt, hidden_next_cnt], stddev=hidden_stddev)))\n            biases.append(tf.Variable(tf.zeros([hidden_next_cnt])))\n            hidden_cur_cnt = hidden_next_cnt\n        # first wx + b\n        y0 = tf.matmul(tf_train_dataset, weights1) + biases1\n        # first sigmoid\n        hidden = tf.nn.sigmoid(y0)\n        # hidden = y0\n        hidden_drop = hidden\n        # first DropOut\n        keep_prob = 0.5\n        if drop_out:\n            hidden_drop = tf.nn.dropout(hidden, keep_prob)\n        # first wx+b for valid\n        valid_y0 = tf.matmul(tf_valid_dataset, weights1) + biases1\n        valid_hidden = tf.nn.sigmoid(valid_y0)\n        # valid_hidden = valid_y0\n        # first wx+b for test\n        test_y0 = tf.matmul(tf_test_dataset, weights1) + biases1\n        test_hidden = tf.nn.sigmoid(test_y0)\n        # test_hidden = test_y0\n\n        # middle layer\n        for i in range(layer_cnt - 2):\n            y1 = tf.matmul(hidden_drop, weights[i]) + biases[i]\n            hidden_drop = tf.nn.sigmoid(y1)\n            if drop_out:\n                keep_prob += 0.5 * i / (layer_cnt + 1)\n                hidden_drop = tf.nn.dropout(hidden_drop, keep_prob)\n\n            y0 = tf.matmul(hidden, weights[i]) + biases[i]\n            hidden = tf.nn.sigmoid(y0)\n            # hidden = y0\n\n            valid_y0 = tf.matmul(valid_hidden, weights[i]) + biases[i]\n            valid_hidden = tf.nn.sigmoid(valid_y0)\n            # valid_hidden = valid_y0\n\n            test_y0 = tf.matmul(test_hidden, weights[i]) + biases[i]\n            test_hidden = tf.nn.sigmoid(test_y0)\n            # test_hidden = test_y0\n\n        # last weight\n        weights2 = tf.Variable(tf.truncated_normal([hidden_cur_cnt, num_labels], stddev=hidden_stddev / 2))\n        biases2 = tf.Variable(tf.zeros([num_labels]))\n        # last wx + b\n        logits = tf.matmul(hidden_drop, weights2) + biases2\n\n        # predicts\n        logits_predict = tf.matmul(hidden, weights2) + biases2\n        valid_predict = tf.matmul(valid_hidden, weights2) + biases2\n        test_predict = tf.matmul(test_hidden, weights2) + biases2\n\n        l2_loss = 0\n        # enable regularization\n        if regular:\n            l2_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n            for i in range(len(weights)):\n                l2_loss += tf.nn.l2_loss(weights[i])\n                # l2_loss += tf.nn.l2_loss(biases[i])\n\n            beta = 1e-2\n            l2_loss *= beta\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + l2_loss\n\n        # Optimizer.\n        if lrd:\n            cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n            starter_learning_rate = 0.4\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 500, 0.75, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n        else:\n            optimizer = tf.train.AdamOptimizer(0.5).minimize(loss)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits_predict)\n        valid_prediction = tf.nn.softmax(valid_predict)\n        test_prediction = tf.nn.softmax(test_predict)\n\n    num_steps = 8001\n\n    with tf.Session(graph=graph) as session:\n        tf.global_variables_initializer().run()\n        print(""Initialized"")\n        for step in range(num_steps):\n            offset_range = train_labels.shape[0] - batch_size\n            offset = (step * batch_size) % offset_range\n            batch_data = train_dataset[offset:(offset + batch_size), :]\n            batch_labels = train_labels[offset:(offset + batch_size), :]\n            feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n            _, l, predictions = session.run(\n                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n            if step % 50 == 0:\n                print(""Minibatch loss at step %d: %f"" % (step, l))\n                print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))\n                print(""Validation accuracy: %.1f%%"" % accuracy(\n                    valid_prediction.eval(), valid_labels))\n        print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))\n\n\nif __name__ == \'__main__\':\n    feature_dim = 2330\n    num_labels = 2\n    train_dataset, train_labels, test_dataset, test_labels = read_caltech()\n    valid_dataset = train_dataset[:99]\n    valid_labels = train_labels[:99]\n    train_dataset = np.asarray(train_dataset, dtype=np.float32)\n    train_labels = np.asarray(train_labels, dtype=np.int32)\n    valid_dataset = np.asarray(valid_dataset, dtype=np.float32)\n    valid_labels = np.asarray(valid_labels, dtype=np.int32)\n    test_dataset = np.asarray(test_dataset, dtype=np.float32)\n    test_labels = np.asarray(test_labels, dtype=np.int32)\n    tf_deep_nn(layer_cnt=3, lrd=True, drop_out=False, regular=True)\n'"
src/app/caltech/dnn_caltech_board.py,53,"b'from __future__ import print_function\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom app.caltech.data import read_caltech\nfrom util.board import variable_summary\n\n\ndef recall_rate(predictions, labels, is_test=False):\n    threshold = 0.0\n    i_predictions = np.subtract(predictions, np.asarray([[threshold, 0] for _ in range(len(labels))]))\n    predict_succ = np.argmax(i_predictions, 1) == np.argmax(labels, 1)\n    pos_samples = np.argmax(labels, 1) == 0\n    res = 100.0 * np.sum(np.logical_and(predict_succ, pos_samples)) / np.sum(pos_samples)\n    if is_test:\n        for i in range(20):\n            threshold = -1.0 + i * 0.1\n            i_predictions = np.subtract(predictions, np.asarray([[threshold, 0] for _ in range(len(labels))]))\n            predict_succ = np.argmax(i_predictions, 1) == np.argmax(labels, 1)\n            pos_samples = np.argmax(labels, 1) == 0\n            res = 100.0 * np.sum(np.logical_and(predict_succ, pos_samples)) / np.sum(pos_samples)\n            print(\'recall: %f%%\' % res)\n    return res\n\n\ndef accuracy(predictions, labels, is_test=False):\n    if is_test:\n        for i in range(20):\n            threshold = -1.0 + i * 0.1\n            i_predictions = np.subtract(predictions, np.asarray([[threshold, 0] for _ in range(len(labels))]))\n            res = 100.0 * np.sum(np.argmax(i_predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n            print(\'precision: %f%%\' % res)\n    else:\n        threshold = 0.0\n        i_predictions = np.subtract(predictions, np.asarray([[threshold, 0] for _ in range(len(labels))]))\n        res = 100.0 * np.sum(np.argmax(i_predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0]\n    return res\n\n\ndef tf_deep_nn(regular=False, test=False):\n    batch_size = 128\n\n    graph = tf.Graph()\n    with graph.as_default():\n        with tf.name_scope(\'input\'):\n            with tf.name_scope(\'feature\'):\n                tf_train_dataset = tf.placeholder(tf.float32, shape=(None, feature_dim))\n                variable_summary(tf_train_dataset)\n            with tf.name_scope(\'label\'):\n                tf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n                variable_summary(tf_train_labels)\n\n        hidden_node_count = 32\n        # start weight\n        hidden_stddev = np.sqrt(2.0 / 100)\n        with tf.name_scope(\'hidden1\'):\n            with tf.name_scope(\'weight\'):\n                weights1 = tf.Variable(tf.truncated_normal([feature_dim, hidden_node_count], stddev=hidden_stddev))\n                # variable_summary(weights1)\n            with tf.name_scope(\'biases\'):\n                biases1 = tf.Variable(tf.zeros([hidden_node_count]))\n                # variable_summary(biases1)\n            # first wx + b\n            with tf.name_scope(\'wx_b\'):\n                y0 = tf.matmul(tf_train_dataset, weights1) + biases1\n                # variable_summary(y0)\n            # first sigmoid\n            with tf.name_scope(\'sigmoid\'):\n                hidden = tf.nn.sigmoid(y0)\n                # variable_summary(hidden)\n\n        with tf.name_scope(\'hidden2\'):\n            hidden_cur_cnt = hidden_node_count\n            hidden_next_cnt = int(hidden_cur_cnt / 2)\n            with tf.name_scope(\'weight\'):\n                weights2 = tf.Variable(tf.truncated_normal([hidden_cur_cnt, hidden_next_cnt], stddev=hidden_stddev))\n                # variable_summary(weights2)\n            with tf.name_scope(\'biases\'):\n                biases2 = tf.Variable(tf.zeros([hidden_next_cnt]))\n                # variable_summary(biases2)\n            # first wx + b\n            with tf.name_scope(\'wx_b\'):\n                y0 = tf.matmul(hidden, weights2) + biases2\n                # variable_summary(y0)\n            # first sigmoid\n            with tf.name_scope(\'sigmoid\'):\n                hidden = tf.nn.sigmoid(y0)\n                # variable_summary(hidden)\n\n        # last weight\n        with tf.name_scope(\'hidden_3\'):\n            with tf.name_scope(\'weight\'):\n                weights3 = tf.Variable(tf.truncated_normal([hidden_next_cnt, num_labels], stddev=hidden_stddev / 2))\n                # variable_summary(weights3)\n            with tf.name_scope(\'biases\'):\n                biases3 = tf.Variable(tf.zeros([num_labels]))\n                # variable_summary(biases3)\n            # last wx + b\n            with tf.name_scope(\'wx_b\'):\n                logits = tf.matmul(hidden, weights3) + biases3\n                # variable_summary(biases3)\n            with tf.name_scope(\'sigmoid\'):\n                logits = tf.nn.sigmoid(logits)\n                # variable_summary(logits)\n\n        logits_predict = logits\n\n        l2_loss = 0\n        # enable regularization\n        with tf.name_scope(\'loss\'):\n            if regular:\n                with tf.name_scope(\'l2_norm\'):\n                    l2_loss = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)\n                    beta = 1e-2\n                    l2_loss *= beta\n\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n            loss += l2_loss\n            with tf.name_scope(\'summaries\'):\n                tf.summary.histogram(\'histogram\', loss)\n        # Optimizer.\n        with tf.name_scope(\'Gradient\'):\n            cur_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n            starter_learning_rate = 0.4\n            learning_rate = tf.train.exponential_decay(starter_learning_rate, cur_step, 500, 0.75, staircase=True)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=cur_step)\n\n        # Predictions for the training, validation, and test data.\n        train_prediction = tf.nn.softmax(logits_predict)\n\n        saver = tf.train.Saver()\n\n        merged = tf.summary.merge_all()\n    summary_flag = True\n    summary_dir = \'summary\'\n    if tf.gfile.Exists(summary_dir):\n        tf.gfile.DeleteRecursively(summary_dir)\n    tf.gfile.MakeDirs(summary_dir)\n    num_steps = 5001\n\n    save_path = \'ct_save.ckpt\'\n    save_flag = True\n\n    with tf.Session(graph=graph) as session:\n        train_writer = tf.summary.FileWriter(summary_dir + \'/train\',\n                                             session.graph)\n        test_writer = tf.summary.FileWriter(summary_dir + \'/test\')\n        if os.path.exists(save_path + \'.index\') and save_flag:\n            # Restore variables from disk.\n            saver.restore(session, \'./\' + save_path)\n            print(\'restore\')\n        else:\n            tf.global_variables_initializer().run()\n            print(""Initialized"")\n        if not test:\n            for step in range(num_steps):\n                offset_range = train_labels.shape[0] - batch_size\n                offset = (step * batch_size) % offset_range\n                batch_data = train_dataset[offset:(offset + batch_size), :]\n                batch_labels = train_labels[offset:(offset + batch_size), :]\n                feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n                if summary_flag:\n                    summary, _, l, predictions = session.run(\n                        [merged, optimizer, loss, train_prediction], feed_dict=feed_dict)\n                else:\n                    _, l, predictions = session.run(\n                        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n                if step % 50 == 0:\n                    print(""Minibatch loss at step %d: %f"" % (step, l))\n                    print(""Minibatch accuracy: %.1f%%, recall: %.1f%%"" % (\n                        accuracy(predictions, batch_labels), recall_rate(predictions, batch_labels))\n                          )\n                    print(""Validate accuracy: %.1f%%, recall: %.1f%%"" % (accuracy(\n                        train_prediction.eval(\n                            feed_dict={tf_train_dataset: valid_dataset, tf_train_labels: valid_labels}),\n                        valid_labels), recall_rate(\n                        train_prediction.eval(\n                            feed_dict={tf_train_dataset: valid_dataset,\n                                       tf_train_labels: valid_labels}),\n                        valid_labels)))\n                    if summary_flag:\n                        test_writer.add_summary(summary, step)\n                if summary_flag:\n                    train_writer.add_summary(summary, step)\n        print(""Test accuracy: %.1f%%, recall: %.1f%%"" % (accuracy(\n            train_prediction.eval(\n                feed_dict={tf_train_dataset: test_dataset, tf_train_labels: test_labels}),\n            test_labels, is_test=True), recall_rate(\n            train_prediction.eval(\n                feed_dict={tf_train_dataset: test_dataset,\n                           tf_train_labels: test_labels}),\n            test_labels, is_test=True)))\n\n        if save_flag and not test:\n            saver.save(session, save_path)\n\n\nif __name__ == \'__main__\':\n    feature_dim = 2330\n    num_labels = 2\n    raw_dataset, raw_labels, test_dataset, test_labels = read_caltech()\n    raw_train_size = len(raw_dataset)\n\n    train_dataset = raw_dataset[: raw_train_size / 2]\n    train_labels = raw_labels[: raw_train_size / 2]\n\n    valid_dataset = raw_dataset[raw_train_size / 2:]\n    valid_labels = raw_labels[raw_train_size / 2:]\n    train_dataset = np.asarray(train_dataset, dtype=np.float32)\n    train_labels = np.asarray(train_labels, dtype=np.int32)\n    valid_dataset = np.asarray(valid_dataset, dtype=np.float32)\n    valid_labels = np.asarray(valid_labels, dtype=np.int32)\n    test_dataset = np.asarray(test_dataset, dtype=np.float32)\n    test_labels = np.asarray(test_labels, dtype=np.int32)\n    tf_deep_nn(regular=True, test=True)\n\n# tensorboard --logdir=src/app/caltech/summary\n'"
src/app/neural_style/__init__.py,0,b''
src/app/neural_style/neural_style.py,0,"b'# Copyright (c) 2015-2016 Anish Athalye. Released under GPLv3.\n\nimport os\n\nimport numpy as np\nimport scipy.misc\n\nfrom stylize import stylize\n\nimport math\nfrom argparse import ArgumentParser\n\n# default arguments\nCONTENT_WEIGHT = 5e0\nSTYLE_WEIGHT = 1e2\nTV_WEIGHT = 1e2\nLEARNING_RATE = 1e1\nSTYLE_SCALE = 1.0\nITERATIONS = 1000\nVGG_PATH = \'imagenet-vgg-verydeep-19.mat\'\n\n\ndef build_parser():\n    parser = ArgumentParser()\n    parser.add_argument(\'--content\',\n            dest=\'content\', help=\'content image\',\n            metavar=\'CONTENT\', required=True)\n    parser.add_argument(\'--styles\',\n            dest=\'styles\',\n            nargs=\'+\', help=\'one or more style images\',\n            metavar=\'STYLE\', required=True)\n    parser.add_argument(\'--output\',\n            dest=\'output\', help=\'output path\',\n            metavar=\'OUTPUT\', required=True)\n    parser.add_argument(\'--iterations\', type=int,\n            dest=\'iterations\', help=\'iterations (default %(default)s)\',\n            metavar=\'ITERATIONS\', default=ITERATIONS)\n    parser.add_argument(\'--print-iterations\', type=int,\n            dest=\'print_iterations\', help=\'statistics printing frequency\',\n            metavar=\'PRINT_ITERATIONS\')\n    parser.add_argument(\'--checkpoint-output\',\n            dest=\'checkpoint_output\', help=\'checkpoint output format, e.g. output%%s.jpg\',\n            metavar=\'OUTPUT\')\n    parser.add_argument(\'--checkpoint-iterations\', type=int,\n            dest=\'checkpoint_iterations\', help=\'checkpoint frequency\',\n            metavar=\'CHECKPOINT_ITERATIONS\')\n    parser.add_argument(\'--width\', type=int,\n            dest=\'width\', help=\'output width\',\n            metavar=\'WIDTH\')\n    parser.add_argument(\'--style-scales\', type=float,\n            dest=\'style_scales\',\n            nargs=\'+\', help=\'one or more style scales\',\n            metavar=\'STYLE_SCALE\')\n    parser.add_argument(\'--network\',\n            dest=\'network\', help=\'path to network parameters (default %(default)s)\',\n            metavar=\'VGG_PATH\', default=VGG_PATH)\n    parser.add_argument(\'--content-weight\', type=float,\n            dest=\'content_weight\', help=\'content weight (default %(default)s)\',\n            metavar=\'CONTENT_WEIGHT\', default=CONTENT_WEIGHT)\n    parser.add_argument(\'--style-weight\', type=float,\n            dest=\'style_weight\', help=\'style weight (default %(default)s)\',\n            metavar=\'STYLE_WEIGHT\', default=STYLE_WEIGHT)\n    parser.add_argument(\'--style-blend-weights\', type=float,\n            dest=\'style_blend_weights\', help=\'style blending weights\',\n            nargs=\'+\', metavar=\'STYLE_BLEND_WEIGHT\')\n    parser.add_argument(\'--tv-weight\', type=float,\n            dest=\'tv_weight\', help=\'total variation regularization weight (default %(default)s)\',\n            metavar=\'TV_WEIGHT\', default=TV_WEIGHT)\n    parser.add_argument(\'--learning-rate\', type=float,\n            dest=\'learning_rate\', help=\'learning rate (default %(default)s)\',\n            metavar=\'LEARNING_RATE\', default=LEARNING_RATE)\n    parser.add_argument(\'--initial\',\n            dest=\'initial\', help=\'initial image\',\n            metavar=\'INITIAL\')\n    return parser\n\n\ndef main():\n    parser = build_parser()\n    options = parser.parse_args()\n\n    if not os.path.isfile(options.network):\n        parser.error(""Network %s does not exist. (Did you forget to download it?)"" % options.network)\n\n    content_image = imread(options.content)\n    style_images = [imread(style) for style in options.styles]\n\n    width = options.width\n    if width is not None:\n        new_shape = (int(math.floor(float(content_image.shape[0]) /\n                content_image.shape[1] * width)), width)\n        content_image = scipy.misc.imresize(content_image, new_shape)\n    target_shape = content_image.shape\n    for i in range(len(style_images)):\n        style_scale = STYLE_SCALE\n        if options.style_scales is not None:\n            style_scale = options.style_scales[i]\n        style_images[i] = scipy.misc.imresize(style_images[i], style_scale *\n                target_shape[1] / style_images[i].shape[1])\n\n    style_blend_weights = options.style_blend_weights\n    if style_blend_weights is None:\n        # default is equal weights\n        style_blend_weights = [1.0/len(style_images) for _ in style_images]\n    else:\n        total_blend_weight = sum(style_blend_weights)\n        style_blend_weights = [weight/total_blend_weight\n                               for weight in style_blend_weights]\n\n    initial = options.initial\n    if initial is not None:\n        initial = scipy.misc.imresize(imread(initial), content_image.shape[:2])\n\n    if options.checkpoint_output and ""%s"" not in options.checkpoint_output:\n        parser.error(""To save intermediate images, the checkpoint output ""\n                     ""parameter must contain `%s` (e.g. `foo%s.jpg`)"")\n\n    for iteration, image in stylize(\n        network=options.network,\n        initial=initial,\n        content=content_image,\n        styles=style_images,\n        iterations=options.iterations,\n        content_weight=options.content_weight,\n        style_weight=options.style_weight,\n        style_blend_weights=style_blend_weights,\n        tv_weight=options.tv_weight,\n        learning_rate=options.learning_rate,\n        print_iterations=options.print_iterations,\n        checkpoint_iterations=options.checkpoint_iterations\n    ):\n        output_file = None\n        if iteration is not None:\n            if options.checkpoint_output:\n                output_file = options.checkpoint_output % iteration\n        else:\n            output_file = options.output\n        if output_file:\n            imsave(output_file, image)\n\n\ndef imread(path):\n    return scipy.misc.imread(path).astype(np.float)\n\n\ndef imsave(path, img):\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    scipy.misc.imsave(path, img)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/app/neural_style/stylize.py,19,"b'# Copyright (c) 2015-2016 Anish Athalye. Released under GPLv3.\n\nimport vgg\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom sys import stderr\n\nCONTENT_LAYER = \'relu4_2\'\nSTYLE_LAYERS = (\'relu1_1\', \'relu2_1\', \'relu3_1\', \'relu4_1\', \'relu5_1\')\n\n\ntry:\n    reduce\nexcept NameError:\n    from functools import reduce\n\n\ndef stylize(network, initial, content, styles, iterations,\n        content_weight, style_weight, style_blend_weights, tv_weight,\n        learning_rate, print_iterations=None, checkpoint_iterations=None):\n    """"""\n    Stylize images.\n\n    This function yields tuples (iteration, image); `iteration` is None\n    if this is the final image (the last iteration).  Other tuples are yielded\n    every `checkpoint_iterations` iterations.\n\n    :rtype: iterator[tuple[int|None,image]]\n    """"""\n    shape = (1,) + content.shape\n    style_shapes = [(1,) + style.shape for style in styles]\n    content_features = {}\n    style_features = [{} for _ in styles]\n\n    # compute content features in feedforward mode\n    g = tf.Graph()\n    with g.as_default(), g.device(\'/gpu:0\'), tf.Session() as sess:\n        image = tf.placeholder(\'float\', shape=shape)\n        net, mean_pixel = vgg.net(network, image)\n        content_pre = np.array([vgg.preprocess(content, mean_pixel)])\n        content_features[CONTENT_LAYER] = net[CONTENT_LAYER].eval(\n                feed_dict={image: content_pre})\n\n    # compute style features in feedforward mode\n    for i in range(len(styles)):\n        g = tf.Graph()\n        with g.as_default(), g.device(\'/gpu:0\'), tf.Session() as sess:\n            image = tf.placeholder(\'float\', shape=style_shapes[i])\n            net, _ = vgg.net(network, image)\n            style_pre = np.array([vgg.preprocess(styles[i], mean_pixel)])\n            for layer in STYLE_LAYERS:\n                features = net[layer].eval(feed_dict={image: style_pre})\n                features = np.reshape(features, (-1, features.shape[3]))\n                gram = np.matmul(features.T, features) / features.size\n                style_features[i][layer] = gram\n\n    # make stylized image using backpropogation\n    g = tf.Graph()\n    with g.as_default(), g.device(\'/gpu:0\'):\n        if initial is None:\n            noise = np.random.normal(size=shape, scale=np.std(content) * 0.1)\n            initial = tf.random_normal(shape) * 0.256\n        else:\n            initial = np.array([vgg.preprocess(initial, mean_pixel)])\n            initial = initial.astype(\'float32\')\n        image = tf.Variable(initial)\n        net, _ = vgg.net(network, image)\n\n        # content loss\n        content_loss = content_weight * (2 * tf.nn.l2_loss(\n                net[CONTENT_LAYER] - content_features[CONTENT_LAYER]) /\n                content_features[CONTENT_LAYER].size)\n        # style loss\n        style_loss = 0\n        for i in range(len(styles)):\n            style_losses = []\n            for style_layer in STYLE_LAYERS:\n                layer = net[style_layer]\n                _, height, width, number = map(lambda i: i.value, layer.get_shape())\n                size = height * width * number\n                feats = tf.reshape(layer, (-1, number))\n                gram = tf.matmul(tf.transpose(feats), feats) / size\n                style_gram = style_features[i][style_layer]\n                style_losses.append(2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size)\n            style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses)\n        # total variation denoising\n        tv_y_size = _tensor_size(image[:,1:,:,:])\n        tv_x_size = _tensor_size(image[:,:,1:,:])\n        tv_loss = tv_weight * 2 * (\n                (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) /\n                    tv_y_size) +\n                (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) /\n                    tv_x_size))\n        # overall loss\n        loss = content_loss + style_loss + tv_loss\n\n        # optimizer setup\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n        def print_progress(i, last=False):\n            stderr.write(\'Iteration %d/%d\\n\' % (i + 1, iterations))\n            if last or (print_iterations and i % print_iterations == 0):\n                stderr.write(\'  content loss: %g\\n\' % content_loss.eval())\n                stderr.write(\'    style loss: %g\\n\' % style_loss.eval())\n                stderr.write(\'       tv loss: %g\\n\' % tv_loss.eval())\n                stderr.write(\'    total loss: %g\\n\' % loss.eval())\n\n        # optimization\n        best_loss = float(\'inf\')\n        best = None\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n            for i in range(iterations):\n                last_step = (i == iterations - 1)\n                print_progress(i, last=last_step)\n                train_step.run()\n\n                if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step:\n                    this_loss = loss.eval()\n                    if this_loss < best_loss:\n                        best_loss = this_loss\n                        best = image.eval()\n                    yield (\n                        (None if last_step else i),\n                        vgg.unprocess(best.reshape(shape[1:]), mean_pixel)\n                    )\n\n\ndef _tensor_size(tensor):\n    from operator import mul\n    return reduce(mul, (d.value for d in tensor.get_shape()), 1)\n'"
src/app/neural_style/vgg.py,4,"b""# Copyright (c) 2015-2016 Anish Athalye. Released under GPLv3.\n\nimport tensorflow as tf\nimport numpy as np\nimport scipy.io\n\n\ndef net(data_path, input_image):\n    layers = (\n        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n\n        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n\n        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n\n        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n\n        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n        'relu5_3', 'conv5_4', 'relu5_4'\n    )\n\n    data = scipy.io.loadmat(data_path)\n    mean = data['normalization'][0][0][0]\n    mean_pixel = np.mean(mean, axis=(0, 1))\n    weights = data['layers'][0]\n\n    net = {}\n    current = input_image\n    for i, name in enumerate(layers):\n        kind = name[:4]\n        if kind == 'conv':\n            kernels, bias = weights[i][0][0][0][0]\n            # matconvnet: weights are [width, height, in_channels, out_channels]\n            # tensorflow: weights are [height, width, in_channels, out_channels]\n            kernels = np.transpose(kernels, (1, 0, 2, 3))\n            bias = bias.reshape(-1)\n            current = _conv_layer(current, kernels, bias)\n        elif kind == 'relu':\n            current = tf.nn.relu(current)\n        elif kind == 'pool':\n            current = _pool_layer(current)\n        net[name] = current\n\n    assert len(net) == len(layers)\n    return net, mean_pixel\n\n\ndef _conv_layer(input, weights, bias):\n    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1),\n            padding='SAME')\n    return tf.nn.bias_add(conv, bias)\n\n\ndef _pool_layer(input):\n    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1),\n            padding='SAME')\n\n\ndef preprocess(image, mean_pixel):\n    return image - mean_pixel\n\n\ndef unprocess(image, mean_pixel):\n    return image + mean_pixel\n"""
