file_path,api_count,code
vehicle_detection_main.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# ----------------------------------------------\n# --- Author         : Ahmet Ozlu\n# --- Mail           : ahmetozlu93@gmail.com\n# --- Date           : 27th January 2018\n# ----------------------------------------------\n\n# Imports\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\nimport cv2\nimport numpy as np\nimport csv\nimport time\nfrom packaging import version\n\nfrom collections import defaultdict\nfrom io import StringIO\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n# Object detection imports\nfrom utils import label_map_util\nfrom utils import visualization_utils as vis_util\n\n# initialize .csv\nwith open(\'traffic_measurement.csv\', \'w\') as f:\n    writer = csv.writer(f)\n    csv_line = \\\n        \'Vehicle Type/Size, Vehicle Color, Vehicle Movement Direction, Vehicle Speed (km/h)\'\n    writer.writerows([csv_line.split(\',\')])\n\nif version.parse(tf.__version__) < version.parse(\'1.4.0\'):\n    raise ImportError(\'Please upgrade your tensorflow installation to v1.4.* or later!\'\n                      )\n\n# input video\ncap = cv2.VideoCapture(\'sub-1504619634606.mp4\')\n\n# Variables\ntotal_passed_vehicle = 0  # using it to count vehicles\n\n# By default I use an ""SSD with Mobilenet"" model here. See the detection model zoo (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies.\n# What model to download.\nMODEL_NAME = \'ssd_mobilenet_v1_coco_2018_01_28\'\nMODEL_FILE = MODEL_NAME + \'.tar.gz\'\nDOWNLOAD_BASE = \\\n    \'http://download.tensorflow.org/models/object_detection/\'\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = MODEL_NAME + \'/frozen_inference_graph.pb\'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = os.path.join(\'data\', \'mscoco_label_map.pbtxt\')\n\nNUM_CLASSES = 90\n\n# Download Model\n# uncomment if you have not download the model yet\n# Load a (frozen) Tensorflow model into memory.\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_CKPT, \'rb\') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name=\'\')\n\n# Loading label map\n# Label maps map indices to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here I use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map,\n        max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\n\n# Helper code\ndef load_image_into_numpy_array(image):\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape((im_height, im_width,\n            3)).astype(np.uint8)\n\n\n# Detection\ndef object_detection_function():\n    total_passed_vehicle = 0\n    speed = \'waiting...\'\n    direction = \'waiting...\'\n    size = \'waiting...\'\n    color = \'waiting...\'\n    with detection_graph.as_default():\n        with tf.Session(graph=detection_graph) as sess:\n\n            # Definite input and output Tensors for detection_graph\n            image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n\n            # Each box represents a part of the image where a particular object was detected.\n            detection_boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n\n            # Each score represent how level of confidence for each of the objects.\n            # Score is shown on the result image, together with the class label.\n            detection_scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n            detection_classes = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n            num_detections = detection_graph.get_tensor_by_name(\'num_detections:0\')\n\n            # for all the frames that are extracted from input video\n            while cap.isOpened():\n                (ret, frame) = cap.read()\n\n                if not ret:\n                    print (\'end of the video file...\')\n                    break\n\n                input_frame = frame\n\n                # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n                image_np_expanded = np.expand_dims(input_frame, axis=0)\n\n                # Actual detection.\n                (boxes, scores, classes, num) = \\\n                    sess.run([detection_boxes, detection_scores,\n                             detection_classes, num_detections],\n                             feed_dict={image_tensor: image_np_expanded})\n\n                # Visualization of the results of a detection.\n                (counter, csv_line) = \\\n                    vis_util.visualize_boxes_and_labels_on_image_array(\n                    cap.get(1),\n                    input_frame,\n                    np.squeeze(boxes),\n                    np.squeeze(classes).astype(np.int32),\n                    np.squeeze(scores),\n                    category_index,\n                    use_normalized_coordinates=True,\n                    line_thickness=4,\n                    )\n\n                total_passed_vehicle = total_passed_vehicle + counter\n\n                # insert information text to video frame\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                cv2.putText(\n                    input_frame,\n                    \'Detected Vehicles: \' + str(total_passed_vehicle),\n                    (10, 35),\n                    font,\n                    0.8,\n                    (0, 0xFF, 0xFF),\n                    2,\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    )\n\n                # when the vehicle passed over line and counted, make the color of ROI line green\n                if counter == 1:\n                    cv2.line(input_frame, (0, 200), (640, 200), (0, 0xFF, 0), 5)\n                else:\n                    cv2.line(input_frame, (0, 200), (640, 200), (0, 0, 0xFF), 5)\n\n                # insert information text to video frame\n                cv2.rectangle(input_frame, (10, 275), (230, 337), (180, 132, 109), -1)\n                cv2.putText(\n                    input_frame,\n                    \'ROI Line\',\n                    (545, 190),\n                    font,\n                    0.6,\n                    (0, 0, 0xFF),\n                    2,\n                    cv2.LINE_AA,\n                    )\n                cv2.putText(\n                    input_frame,\n                    \'LAST PASSED VEHICLE INFO\',\n                    (11, 290),\n                    font,\n                    0.5,\n                    (0xFF, 0xFF, 0xFF),\n                    1,\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    )\n                cv2.putText(\n                    input_frame,\n                    \'-Movement Direction: \' + direction,\n                    (14, 302),\n                    font,\n                    0.4,\n                    (0xFF, 0xFF, 0xFF),\n                    1,\n                    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                    )\n                cv2.putText(\n                    input_frame,\n                    \'-Speed(km/h): \' + speed,\n                    (14, 312),\n                    font,\n                    0.4,\n                    (0xFF, 0xFF, 0xFF),\n                    1,\n                    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                    )\n                cv2.putText(\n                    input_frame,\n                    \'-Color: \' + color,\n                    (14, 322),\n                    font,\n                    0.4,\n                    (0xFF, 0xFF, 0xFF),\n                    1,\n                    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                    )\n                cv2.putText(\n                    input_frame,\n                    \'-Vehicle Size/Type: \' + size,\n                    (14, 332),\n                    font,\n                    0.4,\n                    (0xFF, 0xFF, 0xFF),\n                    1,\n                    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n                    )\n\n                cv2.imshow(\'vehicle detection\', input_frame)\n\n                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                    break\n\n                if csv_line != \'not_available\':\n                    with open(\'traffic_measurement.csv\', \'a\') as f:\n                        writer = csv.writer(f)\n                        (size, color, direction, speed) = \\\n                            csv_line.split(\',\')\n                        writer.writerows([csv_line.split(\',\')])\n            cap.release()\n            cv2.destroyAllWindows()\n\n\nobject_detection_function()\t\t\n'"
custom_vehicle_training/generate_tfrecord.py,5,"b'""""""\nUsage:\n  # From tensorflow/models/\n  # Create train data:\n  python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=train.record\n  # Create test data:\n  python generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=test.record\n""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport io\nimport pandas as pd\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom object_detection.utils import dataset_util\nfrom collections import namedtuple, OrderedDict\n\nflags = tf.app.flags\nflags.DEFINE_string(\'csv_input\', \'\', \'Path to the CSV input\')\nflags.DEFINE_string(\'output_path\', \'\', \'Path to output TFRecord\')\nFLAGS = flags.FLAGS\n\n\n# Edit this function to train on your custom dataset\ndef class_text_to_int(row_label):\n    if row_label == \'motorcycle\':\n        return 1\n    elif row_label == \'car\':\n    \treturn 2\n    elif row_label == \'bus\':\n    \treturn 3   \n    elif row_label == \'truck\':\n    \treturn 4     \n    else:\n        None\n\n\ndef split(df, group):\n    data = namedtuple(\'data\', [\'filename\', \'object\'])\n    gb = df.groupby(group)\n    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n\n\ndef create_tf_example(group, path):\n    with tf.gfile.GFile(os.path.join(path, \'{}\'.format(group.filename)), \'rb\') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    filename = group.filename.encode(\'utf8\')\n    image_format = b\'jpg\'\n    xmins = []\n    xmaxs = []\n    ymins = []\n    ymaxs = []\n    classes_text = []\n    classes = []\n\n    for index, row in group.object.iterrows():\n        xmins.append(row[\'xmin\'] / width)\n        xmaxs.append(row[\'xmax\'] / width)\n        ymins.append(row[\'ymin\'] / height)\n        ymaxs.append(row[\'ymax\'] / height)\n        classes_text.append(row[\'class\'].encode(\'utf8\'))\n        classes.append(class_text_to_int(row[\'class\']))\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/height\': dataset_util.int64_feature(height),\n        \'image/width\': dataset_util.int64_feature(width),\n        \'image/filename\': dataset_util.bytes_feature(filename),\n        \'image/source_id\': dataset_util.bytes_feature(filename),\n        \'image/encoded\': dataset_util.bytes_feature(encoded_jpg),\n        \'image/format\': dataset_util.bytes_feature(image_format),\n        \'image/object/bbox/xmin\': dataset_util.float_list_feature(xmins),\n        \'image/object/bbox/xmax\': dataset_util.float_list_feature(xmaxs),\n        \'image/object/bbox/ymin\': dataset_util.float_list_feature(ymins),\n        \'image/object/bbox/ymax\': dataset_util.float_list_feature(ymaxs),\n        \'image/object/class/text\': dataset_util.bytes_list_feature(classes_text),\n        \'image/object/class/label\': dataset_util.int64_list_feature(classes),\n    }))\n    return tf_example\n\n\ndef main(_):\n    writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n    path = os.path.join(os.getcwd(), \'images\')\n    examples = pd.read_csv(FLAGS.csv_input)\n    grouped = split(examples, \'filename\')\n    for group in grouped:\n        tf_example = create_tf_example(group, path)\n        writer.write(tf_example.SerializeToString())\n\n    writer.close()\n    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n    print(\'Successfully created the TFRecords: {}\'.format(output_path))\n\n\nif __name__ == \'__main__\':\n\ttf.app.run()\n'"
custom_vehicle_training/object_detection/core/post_processing_test.py,88,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_models.object_detection.core.post_processing.""""""\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection.core import post_processing\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import test_case\n\n\nclass MulticlassNonMaxSuppressionTest(test_case.TestCase):\n\n  def test_multiclass_nms_select_with_shared_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  # TODO(bhattad): Remove conditional after CMLE moves to TF 1.9\n\n  def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    num_keypoints = 6\n    keypoints = tf.tile(\n        tf.reshape(tf.range(8), [8, 1, 1]),\n        [1, num_keypoints, 2])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_keypoints_tensor = tf.tile(\n        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),\n        [1, num_keypoints, 2])\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes,\n        scores,\n        score_thresh,\n        iou_thresh,\n        max_output_size,\n        additional_fields={fields.BoxListFields.keypoints: keypoints})\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_keypoints,\n       exp_nms_keypoints) = sess.run([\n           nms.get(),\n           nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes),\n           nms.get_field(fields.BoxListFields.keypoints),\n           exp_nms_keypoints_tensor\n       ])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_keypoints, exp_nms_keypoints)\n\n  def test_multiclass_nms_with_shared_boxes_given_keypoint_heatmaps(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n\n    num_boxes = tf.shape(boxes)[0]\n    heatmap_height = 5\n    heatmap_width = 5\n    num_keypoints = 17\n    keypoint_heatmaps = tf.ones(\n        [num_boxes, heatmap_height, heatmap_width, num_keypoints],\n        dtype=tf.float32)\n\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_keypoint_heatmaps = np.ones(\n        (4, heatmap_height, heatmap_width, num_keypoints), dtype=np.float32)\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes,\n        scores,\n        score_thresh,\n        iou_thresh,\n        max_output_size,\n        additional_fields={\n            fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps\n        })\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_keypoint_heatmaps) = sess.run(\n           [nms.get(),\n            nms.get_field(fields.BoxListFields.scores),\n            nms.get_field(fields.BoxListFields.classes),\n            nms.get_field(fields.BoxListFields.keypoint_heatmaps)])\n\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_keypoint_heatmaps, exp_nms_keypoint_heatmaps)\n\n  def test_multiclass_nms_with_additional_fields(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n\n    coarse_boxes_key = \'coarse_boxes\'\n    coarse_boxes = tf.constant([[0.1, 0.1, 1.1, 1.1],\n                                [0.1, 0.2, 1.1, 1.2],\n                                [0.1, -0.2, 1.1, 1.0],\n                                [0.1, 10.1, 1.1, 11.1],\n                                [0.1, 10.2, 1.1, 11.2],\n                                [0.1, 100.1, 1.1, 101.1],\n                                [0.1, 1000.1, 1.1, 1002.1],\n                                [0.1, 1000.1, 1.1, 1002.2]], tf.float32)\n\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[0, 10, 1, 11],\n                                [0, 0, 1, 1],\n                                [0, 1000, 1, 1002],\n                                [0, 100, 1, 101]], dtype=np.float32)\n\n    exp_nms_coarse_corners = np.array([[0.1, 10.1, 1.1, 11.1],\n                                       [0.1, 0.1, 1.1, 1.1],\n                                       [0.1, 1000.1, 1.1, 1002.1],\n                                       [0.1, 100.1, 1.1, 101.1]],\n                                      dtype=np.float32)\n\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes,\n        scores,\n        score_thresh,\n        iou_thresh,\n        max_output_size,\n        additional_fields={coarse_boxes_key: coarse_boxes})\n\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_coarse_corners) = sess.run(\n           [nms.get(),\n            nms.get_field(fields.BoxListFields.scores),\n            nms.get_field(fields.BoxListFields.classes),\n            nms.get_field(coarse_boxes_key)])\n\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_coarse_corners, exp_nms_coarse_corners)\n\n  def test_multiclass_nms_select_with_shared_boxes_given_masks(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    num_classes = 2\n    mask_height = 3\n    mask_width = 3\n    masks = tf.tile(\n        tf.reshape(tf.range(8), [8, 1, 1, 1]),\n        [1, num_classes, mask_height, mask_width])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n    exp_nms_masks_tensor = tf.tile(\n        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),\n        [1, mask_height, mask_width])\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size, masks=masks)\n    with self.test_session() as sess:\n      (nms_corners_output,\n       nms_scores_output,\n       nms_classes_output,\n       nms_masks,\n       exp_nms_masks) = sess.run([nms.get(),\n                                  nms.get_field(fields.BoxListFields.scores),\n                                  nms.get_field(fields.BoxListFields.classes),\n                                  nms.get_field(fields.BoxListFields.masks),\n                                  exp_nms_masks_tensor])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n      self.assertAllEqual(nms_masks, exp_nms_masks)\n\n  def test_multiclass_nms_select_with_clip_window(self):\n    boxes = tf.constant([[[0, 0, 10, 10]],\n                         [[1, 1, 11, 11]]], tf.float32)\n    scores = tf.constant([[.9], [.75]])\n    clip_window = tf.constant([5, 4, 8, 7], tf.float32)\n    score_thresh = 0.0\n    iou_thresh = 0.5\n    max_output_size = 100\n\n    exp_nms_corners = [[5, 4, 8, 7]]\n    exp_nms_scores = [.9]\n    exp_nms_classes = [0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes,\n        scores,\n        score_thresh,\n        iou_thresh,\n        max_output_size,\n        clip_window=clip_window)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_clip_window_change_coordinate_frame(self):\n    boxes = tf.constant([[[0, 0, 10, 10]],\n                         [[1, 1, 11, 11]]], tf.float32)\n    scores = tf.constant([[.9], [.75]])\n    clip_window = tf.constant([5, 4, 8, 7], tf.float32)\n    score_thresh = 0.0\n    iou_thresh = 0.5\n    max_output_size = 100\n\n    exp_nms_corners = [[0, 0, 1, 1]]\n    exp_nms_scores = [.9]\n    exp_nms_classes = [0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes,\n        scores,\n        score_thresh,\n        iou_thresh,\n        max_output_size,\n        clip_window=clip_window,\n        change_coordinate_frame=True)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_per_class_cap(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_size_per_class = 2\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 1000, 1, 1002]]\n    exp_nms_scores = [.95, .9, .85]\n    exp_nms_classes = [0, 0, 1]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_size_per_class)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_select_with_total_cap(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_size_per_class = 4\n    max_total_size = 2\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1]]\n    exp_nms_scores = [.95, .9]\n    exp_nms_classes = [0, 0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_size_per_class,\n        max_total_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_multiclass_nms_threshold_then_select_with_shared_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1]],\n                         [[0, 0.1, 1, 1.1]],\n                         [[0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002]],\n                         [[0, 1000, 1, 1002.1]]], tf.float32)\n    scores = tf.constant([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 3\n\n    exp_nms = [[0, 10, 1, 11],\n               [0, 0, 1, 1],\n               [0, 100, 1, 101]]\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_output = sess.run(nms.get())\n      self.assertAllClose(nms_output, exp_nms)\n\n  def test_multiclass_nms_select_with_separate_boxes(self):\n    boxes = tf.constant([[[0, 0, 1, 1], [0, 0, 4, 5]],\n                         [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                         [[0, 10, 1, 11], [0, 10, 1, 11]],\n                         [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                         [[0, 100, 1, 101], [0, 100, 1, 101]],\n                         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]],\n                        tf.float32)\n    scores = tf.constant([[.9, 0.01], [.75, 0.05],\n                          [.6, 0.01], [.95, 0],\n                          [.5, 0.01], [.3, 0.01],\n                          [.01, .85], [.01, .5]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[0, 10, 1, 11],\n                       [0, 0, 1, 1],\n                       [0, 999, 2, 1004],\n                       [0, 100, 1, 101]]\n    exp_nms_scores = [.95, .9, .85, .3]\n    exp_nms_classes = [0, 0, 1, 0]\n\n    nms, _ = post_processing.multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh, max_output_size)\n    with self.test_session() as sess:\n      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(\n          [nms.get(), nms.get_field(fields.BoxListFields.scores),\n           nms.get_field(fields.BoxListFields.classes)])\n      self.assertAllClose(nms_corners_output, exp_nms_corners)\n      self.assertAllClose(nms_scores_output, exp_nms_scores)\n      self.assertAllClose(nms_classes_output, exp_nms_classes)\n\n  def test_batch_multiclass_nms_with_batch_size_1(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]],\n                          [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0],\n                           [.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 10, 1, 11],\n                        [0, 0, 1, 1],\n                        [0, 999, 2, 1004],\n                        [0, 100, 1, 101]]]\n    exp_nms_scores = [[.95, .9, .85, .3]]\n    exp_nms_classes = [[0, 0, 1, 0]]\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertEqual(num_detections, [4])\n\n  def test_batch_multiclass_nms_with_batch_size_2(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n\n  def test_batch_multiclass_nms_with_per_batch_clip_window(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    clip_window = tf.constant([0., 0., 200., 200.])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.5, .3, 0, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [0, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        clip_window=clip_window)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 2])\n\n  def test_batch_multiclass_nms_with_per_image_clip_window(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    clip_window = tf.constant([[0., 0., 5., 5.],\n                               [0., 0., 200., 200.]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.9, 0., 0., 0.],\n                               [.5, .3, 0, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [0, 0, 0, 0]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        clip_window=clip_window)\n\n    self.assertIsNone(nmsed_masks)\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(),\n                        exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(),\n                        exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(),\n                        exp_nms_classes.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [1, 2])\n\n  def test_batch_multiclass_nms_with_masks(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    masks = tf.constant([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                          [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                          [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                          [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                         [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                          [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                          [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                          [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]],\n                        tf.float32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_masks = np.array([[[[6, 7], [8, 9]],\n                               [[0, 1], [2, 3]],\n                               [[0, 0], [0, 0]],\n                               [[0, 0], [0, 0]]],\n                              [[[13, 14], [15, 16]],\n                               [[8, 9], [10, 11]],\n                               [[10, 11], [12, 13]],\n                               [[0, 0], [0, 0]]]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        masks=masks)\n\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(), exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(), exp_nms_classes.shape)\n    self.assertAllEqual(nmsed_masks.shape.as_list(), exp_nms_masks.shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_additional_fields(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    additional_fields = {\n        \'keypoints\': tf.constant(\n            [[[[6, 7], [8, 9]],\n              [[0, 1], [2, 3]],\n              [[0, 0], [0, 0]],\n              [[0, 0], [0, 0]]],\n             [[[13, 14], [15, 16]],\n              [[8, 9], [10, 11]],\n              [[10, 11], [12, 13]],\n              [[0, 0], [0, 0]]]],\n            tf.float32)\n    }\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_additional_fields = {\n        \'keypoints\': np.array([[[[0, 0], [0, 0]],\n                                [[6, 7], [8, 9]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]],\n                               [[[10, 11], [12, 13]],\n                                [[13, 14], [15, 16]],\n                                [[8, 9], [10, 11]],\n                                [[0, 0], [0, 0]]]])\n    }\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        additional_fields=additional_fields)\n\n    self.assertIsNone(nmsed_masks)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), exp_nms_corners.shape)\n    self.assertAllEqual(nmsed_scores.shape.as_list(), exp_nms_scores.shape)\n    self.assertAllEqual(nmsed_classes.shape.as_list(), exp_nms_classes.shape)\n    self.assertEqual(len(nmsed_additional_fields),\n                     len(exp_nms_additional_fields))\n    for key in exp_nms_additional_fields:\n      self.assertAllEqual(nmsed_additional_fields[key].shape.as_list(),\n                          exp_nms_additional_fields[key].shape)\n    self.assertEqual(num_detections.shape.as_list(), [2])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_additional_fields,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_additional_fields, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      for key in exp_nms_additional_fields:\n        self.assertAllClose(nmsed_additional_fields[key],\n                            exp_nms_additional_fields[key])\n      self.assertAllClose(num_detections, [2, 3])\n\n  def test_batch_multiclass_nms_with_dynamic_batch_size(self):\n    boxes_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 4))\n    scores_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2))\n    masks_placeholder = tf.placeholder(tf.float32, shape=(None, None, 2, 2, 2))\n\n    boxes = np.array([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                       [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                       [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                       [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                      [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                       [[0, 100, 1, 101], [0, 100, 1, 101]],\n                       [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                       [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]])\n    scores = np.array([[[.9, 0.01], [.75, 0.05],\n                        [.6, 0.01], [.95, 0]],\n                       [[.5, 0.01], [.3, 0.01],\n                        [.01, .85], [.01, .5]]])\n    masks = np.array([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                       [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                       [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                       [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                      [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                       [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                       [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                       [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]])\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = np.array([[[0, 10, 1, 11],\n                                 [0, 0, 1, 1],\n                                 [0, 0, 0, 0],\n                                 [0, 0, 0, 0]],\n                                [[0, 999, 2, 1004],\n                                 [0, 10.1, 1, 11.1],\n                                 [0, 100, 1, 101],\n                                 [0, 0, 0, 0]]])\n    exp_nms_scores = np.array([[.95, .9, 0, 0],\n                               [.85, .5, .3, 0]])\n    exp_nms_classes = np.array([[0, 0, 0, 0],\n                                [1, 0, 0, 0]])\n    exp_nms_masks = np.array([[[[6, 7], [8, 9]],\n                               [[0, 1], [2, 3]],\n                               [[0, 0], [0, 0]],\n                               [[0, 0], [0, 0]]],\n                              [[[13, 14], [15, 16]],\n                               [[8, 9], [10, 11]],\n                               [[10, 11], [12, 13]],\n                               [[0, 0], [0, 0]]]])\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes_placeholder, scores_placeholder, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        masks=masks_placeholder)\n\n    self.assertIsNone(nmsed_additional_fields)\n    # Check static shapes\n    self.assertAllEqual(nmsed_boxes.shape.as_list(), [None, 4, 4])\n    self.assertAllEqual(nmsed_scores.shape.as_list(), [None, 4])\n    self.assertAllEqual(nmsed_classes.shape.as_list(), [None, 4])\n    self.assertAllEqual(nmsed_masks.shape.as_list(), [None, 4, 2, 2])\n    self.assertEqual(num_detections.shape.as_list(), [None])\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections],\n                                  feed_dict={boxes_placeholder: boxes,\n                                             scores_placeholder: scores,\n                                             masks_placeholder: masks})\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [2, 3])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_masks_and_num_valid_boxes(self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    masks = tf.constant([[[[[0, 1], [2, 3]], [[1, 2], [3, 4]]],\n                          [[[2, 3], [4, 5]], [[3, 4], [5, 6]]],\n                          [[[4, 5], [6, 7]], [[5, 6], [7, 8]]],\n                          [[[6, 7], [8, 9]], [[7, 8], [9, 10]]]],\n                         [[[[8, 9], [10, 11]], [[9, 10], [11, 12]]],\n                          [[[10, 11], [12, 13]], [[11, 12], [13, 14]]],\n                          [[[12, 13], [14, 15]], [[13, 14], [15, 16]]],\n                          [[[14, 15], [16, 17]], [[15, 16], [17, 18]]]]],\n                        tf.float32)\n    num_valid_boxes = tf.constant([1, 1], tf.int32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 0, 1, 1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]],\n                       [[0, 10.1, 1, 11.1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]]]\n    exp_nms_scores = [[.9, 0, 0, 0],\n                      [.5, 0, 0, 0]]\n    exp_nms_classes = [[0, 0, 0, 0],\n                       [0, 0, 0, 0]]\n    exp_nms_masks = [[[[0, 1], [2, 3]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]]],\n                     [[[8, 9], [10, 11]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]],\n                      [[0, 0], [0, 0]]]]\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        num_valid_boxes=num_valid_boxes, masks=masks)\n\n    self.assertIsNone(nmsed_additional_fields)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_masks, num_detections])\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      self.assertAllClose(num_detections, [1, 1])\n      self.assertAllClose(nmsed_masks, exp_nms_masks)\n\n  def test_batch_multiclass_nms_with_additional_fields_and_num_valid_boxes(\n      self):\n    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],\n                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],\n                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],\n                          [[0, 10, 1, 11], [0, 10, 1, 11]]],\n                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],\n                          [[0, 100, 1, 101], [0, 100, 1, 101]],\n                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],\n                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],\n                        tf.float32)\n    scores = tf.constant([[[.9, 0.01], [.75, 0.05],\n                           [.6, 0.01], [.95, 0]],\n                          [[.5, 0.01], [.3, 0.01],\n                           [.01, .85], [.01, .5]]])\n    additional_fields = {\n        \'keypoints\': tf.constant(\n            [[[[6, 7], [8, 9]],\n              [[0, 1], [2, 3]],\n              [[0, 0], [0, 0]],\n              [[0, 0], [0, 0]]],\n             [[[13, 14], [15, 16]],\n              [[8, 9], [10, 11]],\n              [[10, 11], [12, 13]],\n              [[0, 0], [0, 0]]]],\n            tf.float32)\n    }\n    num_valid_boxes = tf.constant([1, 1], tf.int32)\n    score_thresh = 0.1\n    iou_thresh = .5\n    max_output_size = 4\n\n    exp_nms_corners = [[[0, 0, 1, 1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]],\n                       [[0, 10.1, 1, 11.1],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0],\n                        [0, 0, 0, 0]]]\n    exp_nms_scores = [[.9, 0, 0, 0],\n                      [.5, 0, 0, 0]]\n    exp_nms_classes = [[0, 0, 0, 0],\n                       [0, 0, 0, 0]]\n    exp_nms_additional_fields = {\n        \'keypoints\': np.array([[[[6, 7], [8, 9]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]],\n                               [[[13, 14], [15, 16]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]],\n                                [[0, 0], [0, 0]]]])\n    }\n\n    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,\n     nmsed_additional_fields, num_detections\n    ) = post_processing.batch_multiclass_non_max_suppression(\n        boxes, scores, score_thresh, iou_thresh,\n        max_size_per_class=max_output_size, max_total_size=max_output_size,\n        num_valid_boxes=num_valid_boxes,\n        additional_fields=additional_fields)\n\n    self.assertIsNone(nmsed_masks)\n\n    with self.test_session() as sess:\n      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_additional_fields,\n       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,\n                                   nmsed_additional_fields, num_detections])\n\n      self.assertAllClose(nmsed_boxes, exp_nms_corners)\n      self.assertAllClose(nmsed_scores, exp_nms_scores)\n      self.assertAllClose(nmsed_classes, exp_nms_classes)\n      for key in exp_nms_additional_fields:\n        self.assertAllClose(nmsed_additional_fields[key],\n                            exp_nms_additional_fields[key])\n      self.assertAllClose(num_detections, [1, 1])\n\n  # TODO(bhattad): Remove conditional after CMLE moves to TF 1.9\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/metrics/coco_evaluation_test.py,54,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_models.object_detection.metrics.coco_evaluation.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom object_detection.core import standard_fields\nfrom object_detection.metrics import coco_evaluation\n\n\ndef _get_categories_list():\n  return [{\n      \'id\': 1,\n      \'name\': \'person\'\n  }, {\n      \'id\': 2,\n      \'name\': \'dog\'\n  }, {\n      \'id\': 3,\n      \'name\': \'cat\'\n  }]\n\n\nclass CocoDetectionEvaluationTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    """"""Tests that mAP is calculated correctly on GT and Detections.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image2\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image2\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image3\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image3\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsSkipCrowd(self):\n    """"""Tests computing mAP with is_crowd GT boxes skipped.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n                np.array([[100., 100., 200., 200.], [99., 99., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes:\n                np.array([1, 2]),\n            standard_fields.InputDataFields.groundtruth_is_crowd:\n                np.array([0, 1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n                np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n                np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsEmptyCrowd(self):\n    """"""Tests computing mAP with empty is_crowd array passed in.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes:\n                np.array([1]),\n            standard_fields.InputDataFields.groundtruth_is_crowd:\n                np.array([])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n                np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n                np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n                np.array([1])\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n\n  def testRejectionOnDuplicateGroundtruth(self):\n    """"""Tests that groundtruth cannot be added more than once for an image.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    #  Add groundtruth\n    image_key1 = \'img1\'\n    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],\n                                  dtype=float)\n    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)\n    coco_evaluator.add_single_ground_truth_image_info(image_key1, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes1,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels1\n    })\n    groundtruth_lists_len = len(coco_evaluator._groundtruth_list)\n\n    # Add groundtruth with the same image id.\n    coco_evaluator.add_single_ground_truth_image_info(image_key1, {\n        standard_fields.InputDataFields.groundtruth_boxes:\n            groundtruth_boxes1,\n        standard_fields.InputDataFields.groundtruth_classes:\n            groundtruth_class_labels1\n    })\n    self.assertEqual(groundtruth_lists_len,\n                     len(coco_evaluator._groundtruth_list))\n\n  def testRejectionOnDuplicateDetections(self):\n    """"""Tests that detections cannot be added more than once for an image.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    #  Add groundtruth\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[99., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1])\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    detections_lists_len = len(coco_evaluator._detection_boxes_list)\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',  # Note that this image id was previously added.\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1])\n        })\n    self.assertEqual(detections_lists_len,\n                     len(coco_evaluator._detection_boxes_list))\n\n  def testExceptionRaisedWithMissingGroundtruth(self):\n    """"""Tests that exception is raised for detection with missing groundtruth.""""""\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    with self.assertRaises(ValueError):\n      coco_evaluator.add_single_detected_image_info(\n          image_id=\'image1\',\n          detections_dict={\n              standard_fields.DetectionResultFields.detection_boxes:\n                  np.array([[100., 100., 200., 200.]]),\n              standard_fields.DetectionResultFields.detection_scores:\n                  np.array([.8]),\n              standard_fields.DetectionResultFields.detection_classes:\n                  np.array([1])\n          })\n\n\nclass CocoEvaluationPyFuncTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image1\',\n                   groundtruth_boxes: np.array([[100., 100., 200., 200.]]),\n                   groundtruth_classes: np.array([1]),\n                   detection_boxes: np.array([[100., 100., 200., 200.]]),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image2\',\n                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),\n                   groundtruth_classes: np.array([3]),\n                   detection_boxes: np.array([[50., 50., 100., 100.]]),\n                   detection_scores: np.array([.7]),\n                   detection_classes: np.array([3])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image3\',\n                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),\n                   groundtruth_classes: np.array([2]),\n                   detection_boxes: np.array([[25., 25., 50., 50.]]),\n                   detection_scores: np.array([.9]),\n                   detection_classes: np.array([2])\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsIsAnnotated(self):\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    is_annotated = tf.placeholder(tf.bool, shape=())\n    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        \'is_annotated\': is_annotated,\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image1\',\n                   groundtruth_boxes: np.array([[100., 100., 200., 200.]]),\n                   groundtruth_classes: np.array([1]),\n                   is_annotated: True,\n                   detection_boxes: np.array([[100., 100., 200., 200.]]),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image2\',\n                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),\n                   groundtruth_classes: np.array([3]),\n                   is_annotated: True,\n                   detection_boxes: np.array([[50., 50., 100., 100.]]),\n                   detection_scores: np.array([.7]),\n                   detection_classes: np.array([3])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image3\',\n                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),\n                   groundtruth_classes: np.array([2]),\n                   is_annotated: True,\n                   detection_boxes: np.array([[25., 25., 50., 50.]]),\n                   detection_scores: np.array([.9]),\n                   detection_classes: np.array([2])\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image4\',\n                   groundtruth_boxes: np.zeros((0, 4)),\n                   groundtruth_classes: np.zeros((0)),\n                   is_annotated: False,  # Note that this image isn\'t annotated.\n                   detection_boxes: np.array([[25., 25., 50., 50.],\n                                              [25., 25., 70., 50.],\n                                              [25., 25., 80., 50.],\n                                              [25., 25., 90., 50.]]),\n                   detection_scores: np.array([0.6, 0.7, 0.8, 0.9]),\n                   detection_classes: np.array([1, 2, 2, 3])\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsPadded(self):\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id:\n                  \'image1\',\n              groundtruth_boxes:\n                  np.array([[100., 100., 200., 200.], [-1, -1, -1, -1]]),\n              groundtruth_classes:\n                  np.array([1, -1]),\n              detection_boxes:\n                  np.array([[100., 100., 200., 200.], [0., 0., 0., 0.]]),\n              detection_scores:\n                  np.array([.8, 0.]),\n              detection_classes:\n                  np.array([1, -1])\n          })\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id:\n                  \'image2\',\n              groundtruth_boxes:\n                  np.array([[50., 50., 100., 100.], [-1, -1, -1, -1]]),\n              groundtruth_classes:\n                  np.array([3, -1]),\n              detection_boxes:\n                  np.array([[50., 50., 100., 100.], [0., 0., 0., 0.]]),\n              detection_scores:\n                  np.array([.7, 0.]),\n              detection_classes:\n                  np.array([3, -1])\n          })\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id:\n                  \'image3\',\n              groundtruth_boxes:\n                  np.array([[25., 25., 50., 50.], [10., 10., 15., 15.]]),\n              groundtruth_classes:\n                  np.array([2, 2]),\n              detection_boxes:\n                  np.array([[25., 25., 50., 50.], [10., 10., 15., 15.]]),\n              detection_scores:\n                  np.array([.95, .9]),\n              detection_classes:\n                  np.array([2, 2])\n          })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 0.83333331)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsBatched(self):\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    batch_size = 3\n    image_id = tf.placeholder(tf.string, shape=(batch_size))\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n    detection_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))\n    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(update_op,\n               feed_dict={\n                   image_id: [\'image1\', \'image2\', \'image3\'],\n                   groundtruth_boxes: np.array([[[100., 100., 200., 200.]],\n                                                [[50., 50., 100., 100.]],\n                                                [[25., 25., 50., 50.]]]),\n                   groundtruth_classes: np.array([[1], [3], [2]]),\n                   detection_boxes: np.array([[[100., 100., 200., 200.]],\n                                              [[50., 50., 100., 100.]],\n                                              [[25., 25., 50., 50.]]]),\n                   detection_scores: np.array([[.8], [.7], [.9]]),\n                   detection_classes: np.array([[1], [3], [2]])\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsPaddedBatches(self):\n    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(\n        _get_categories_list())\n    batch_size = 3\n    image_id = tf.placeholder(tf.string, shape=(batch_size))\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n    num_gt_boxes_per_image = tf.placeholder(tf.int32, shape=(None))\n    detection_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))\n    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))\n    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n    num_det_boxes_per_image = tf.placeholder(tf.int32, shape=(None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        detection_fields.detection_boxes: detection_boxes,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes,\n        \'num_groundtruth_boxes_per_image\': num_gt_boxes_per_image,\n        \'num_det_boxes_per_image\': num_det_boxes_per_image\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionBoxes_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id: [\'image1\', \'image2\', \'image3\'],\n              groundtruth_boxes:\n                  np.array([[[100., 100., 200., 200.], [-1, -1, -1, -1]],\n                            [[50., 50., 100., 100.], [-1, -1, -1, -1]],\n                            [[25., 25., 50., 50.], [10., 10., 15., 15.]]]),\n              groundtruth_classes:\n                  np.array([[1, -1], [3, -1], [2, 2]]),\n              num_gt_boxes_per_image:\n                  np.array([1, 1, 2]),\n              detection_boxes:\n                  np.array([[[100., 100., 200., 200.],\n                             [0., 0., 0., 0.],\n                             [0., 0., 0., 0.]],\n                            [[50., 50., 100., 100.],\n                             [0., 0., 0., 0.],\n                             [0., 0., 0., 0.]],\n                            [[25., 25., 50., 50.],\n                             [10., 10., 15., 15.],\n                             [10., 10., 15., 15.]]]),\n              detection_scores:\n                  np.array([[.8, 0., 0.], [.7, 0., 0.], [.95, .9, 0.9]]),\n              detection_classes:\n                  np.array([[1, -1, -1], [3, -1, -1], [2, 2, 2]]),\n              num_det_boxes_per_image:\n                  np.array([1, 1, 3]),\n          })\n\n    # Check the number of bounding boxes added.\n    self.assertEqual(len(coco_evaluator._groundtruth_list), 4)\n    self.assertEqual(len(coco_evaluator._detection_boxes_list), 5)\n\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@1\'], 0.83333331)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionBoxes_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_boxes_list)\n    self.assertFalse(coco_evaluator._image_ids)\n\n\nclass CocoMaskEvaluationTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    coco_evaluator = coco_evaluation.CocoMaskEvaluator(_get_categories_list())\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image1\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 100, 100], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image1\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[100., 100., 200., 200.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 100, 100], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image2\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image2\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[50., 50., 100., 100.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_ground_truth_image_info(\n        image_id=\'image3\',\n        groundtruth_dict={\n            standard_fields.InputDataFields.groundtruth_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),\n            standard_fields.InputDataFields.groundtruth_instance_masks:\n            np.pad(np.ones([1, 25, 25], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    coco_evaluator.add_single_detected_image_info(\n        image_id=\'image3\',\n        detections_dict={\n            standard_fields.DetectionResultFields.detection_boxes:\n            np.array([[25., 25., 50., 50.]]),\n            standard_fields.DetectionResultFields.detection_scores:\n            np.array([.8]),\n            standard_fields.DetectionResultFields.detection_classes:\n            np.array([1]),\n            standard_fields.DetectionResultFields.detection_masks:\n            np.pad(np.ones([1, 25, 25], dtype=np.uint8),\n                   ((0, 0), (10, 10), (10, 10)), mode=\'constant\')\n        })\n    metrics = coco_evaluator.evaluate()\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP\'], 1.0)\n    coco_evaluator.clear()\n    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)\n    self.assertFalse(coco_evaluator._image_ids_with_detections)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._detection_masks_list)\n\n\nclass CocoMaskEvaluationPyFuncTest(tf.test.TestCase):\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):\n    coco_evaluator = coco_evaluation.CocoMaskEvaluator(_get_categories_list())\n    image_id = tf.placeholder(tf.string, shape=())\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))\n    groundtruth_masks = tf.placeholder(tf.uint8, shape=(None, None, None))\n    detection_scores = tf.placeholder(tf.float32, shape=(None))\n    detection_classes = tf.placeholder(tf.float32, shape=(None))\n    detection_masks = tf.placeholder(tf.uint8, shape=(None, None, None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        input_data_fields.groundtruth_instance_masks: groundtruth_masks,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes,\n        detection_fields.detection_masks: detection_masks,\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionMasks_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id:\n                  \'image1\',\n              groundtruth_boxes:\n                  np.array([[100., 100., 200., 200.], [50., 50., 100., 100.]]),\n              groundtruth_classes:\n                  np.array([1, 2]),\n              groundtruth_masks:\n                  np.stack([\n                      np.pad(\n                          np.ones([100, 100], dtype=np.uint8), ((10, 10),\n                                                                (10, 10)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([50, 50], dtype=np.uint8), ((0, 70), (0, 70)),\n                          mode=\'constant\')\n                  ]),\n              detection_scores:\n                  np.array([.9, .8]),\n              detection_classes:\n                  np.array([2, 1]),\n              detection_masks:\n                  np.stack([\n                      np.pad(\n                          np.ones([50, 50], dtype=np.uint8), ((0, 70), (0, 70)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([100, 100], dtype=np.uint8), ((10, 10),\n                                                                (10, 10)),\n                          mode=\'constant\'),\n                  ])\n          })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image2\',\n                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),\n                   groundtruth_classes: np.array([1]),\n                   groundtruth_masks: np.pad(np.ones([1, 50, 50],\n                                                     dtype=np.uint8),\n                                             ((0, 0), (10, 10), (10, 10)),\n                                             mode=\'constant\'),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1]),\n                   detection_masks: np.pad(np.ones([1, 50, 50], dtype=np.uint8),\n                                           ((0, 0), (10, 10), (10, 10)),\n                                           mode=\'constant\')\n               })\n      sess.run(update_op,\n               feed_dict={\n                   image_id: \'image3\',\n                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),\n                   groundtruth_classes: np.array([1]),\n                   groundtruth_masks: np.pad(np.ones([1, 25, 25],\n                                                     dtype=np.uint8),\n                                             ((0, 0), (10, 10), (10, 10)),\n                                             mode=\'constant\'),\n                   detection_scores: np.array([.8]),\n                   detection_classes: np.array([1]),\n                   detection_masks: np.pad(np.ones([1, 25, 25],\n                                                   dtype=np.uint8),\n                                           ((0, 0), (10, 10), (10, 10)),\n                                           mode=\'constant\')\n               })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._image_ids_with_detections)\n    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)\n    self.assertFalse(coco_evaluator._detection_masks_list)\n\n  def testGetOneMAPWithMatchingGroundtruthAndDetectionsBatched(self):\n    coco_evaluator = coco_evaluation.CocoMaskEvaluator(_get_categories_list())\n    batch_size = 3\n    image_id = tf.placeholder(tf.string, shape=(batch_size))\n    groundtruth_boxes = tf.placeholder(tf.float32, shape=(batch_size, None, 4))\n    groundtruth_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n    groundtruth_masks = tf.placeholder(\n        tf.uint8, shape=(batch_size, None, None, None))\n    detection_scores = tf.placeholder(tf.float32, shape=(batch_size, None))\n    detection_classes = tf.placeholder(tf.float32, shape=(batch_size, None))\n    detection_masks = tf.placeholder(\n        tf.uint8, shape=(batch_size, None, None, None))\n\n    input_data_fields = standard_fields.InputDataFields\n    detection_fields = standard_fields.DetectionResultFields\n    eval_dict = {\n        input_data_fields.key: image_id,\n        input_data_fields.groundtruth_boxes: groundtruth_boxes,\n        input_data_fields.groundtruth_classes: groundtruth_classes,\n        input_data_fields.groundtruth_instance_masks: groundtruth_masks,\n        detection_fields.detection_scores: detection_scores,\n        detection_fields.detection_classes: detection_classes,\n        detection_fields.detection_masks: detection_masks,\n    }\n\n    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(eval_dict)\n\n    _, update_op = eval_metric_ops[\'DetectionMasks_Precision/mAP\']\n\n    with self.test_session() as sess:\n      sess.run(\n          update_op,\n          feed_dict={\n              image_id: [\'image1\', \'image2\', \'image3\'],\n              groundtruth_boxes:\n                  np.array([[[100., 100., 200., 200.]],\n                            [[50., 50., 100., 100.]],\n                            [[25., 25., 50., 50.]]]),\n              groundtruth_classes:\n                  np.array([[1], [1], [1]]),\n              groundtruth_masks:\n                  np.stack([\n                      np.pad(\n                          np.ones([1, 100, 100], dtype=np.uint8),\n                          ((0, 0), (0, 0), (0, 0)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([1, 50, 50], dtype=np.uint8),\n                          ((0, 0), (25, 25), (25, 25)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([1, 25, 25], dtype=np.uint8),\n                          ((0, 0), (37, 38), (37, 38)),\n                          mode=\'constant\')\n                  ],\n                           axis=0),\n              detection_scores:\n                  np.array([[.8], [.8], [.8]]),\n              detection_classes:\n                  np.array([[1], [1], [1]]),\n              detection_masks:\n                  np.stack([\n                      np.pad(\n                          np.ones([1, 100, 100], dtype=np.uint8),\n                          ((0, 0), (0, 0), (0, 0)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([1, 50, 50], dtype=np.uint8),\n                          ((0, 0), (25, 25), (25, 25)),\n                          mode=\'constant\'),\n                      np.pad(\n                          np.ones([1, 25, 25], dtype=np.uint8),\n                          ((0, 0), (37, 38), (37, 38)),\n                          mode=\'constant\')\n                  ],\n                           axis=0)\n          })\n    metrics = {}\n    for key, (value_op, _) in eval_metric_ops.iteritems():\n      metrics[key] = value_op\n    metrics = sess.run(metrics)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.50IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP@.75IOU\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Precision/mAP (small)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@1\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@10\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (large)\'], 1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (medium)\'],\n                           1.0)\n    self.assertAlmostEqual(metrics[\'DetectionMasks_Recall/AR@100 (small)\'], 1.0)\n    self.assertFalse(coco_evaluator._groundtruth_list)\n    self.assertFalse(coco_evaluator._image_ids_with_detections)\n    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)\n    self.assertFalse(coco_evaluator._detection_masks_list)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/metrics/coco_tools_test.py,10,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_model.object_detection.metrics.coco_tools.""""""\nimport json\nimport os\nimport re\nimport numpy as np\n\nfrom pycocotools import mask\n\nimport tensorflow as tf\n\nfrom object_detection.metrics import coco_tools\n\n\nclass CocoToolsTest(tf.test.TestCase):\n\n  def setUp(self):\n    groundtruth_annotations_list = [\n        {\n            \'id\': 1,\n            \'image_id\': \'first\',\n            \'category_id\': 1,\n            \'bbox\': [100., 100., 100., 100.],\n            \'area\': 100.**2,\n            \'iscrowd\': 0\n        },\n        {\n            \'id\': 2,\n            \'image_id\': \'second\',\n            \'category_id\': 1,\n            \'bbox\': [50., 50., 50., 50.],\n            \'area\': 50.**2,\n            \'iscrowd\': 0\n        },\n    ]\n    image_list = [{\'id\': \'first\'}, {\'id\': \'second\'}]\n    category_list = [{\'id\': 0, \'name\': \'person\'},\n                     {\'id\': 1, \'name\': \'cat\'},\n                     {\'id\': 2, \'name\': \'dog\'}]\n    self._groundtruth_dict = {\n        \'annotations\': groundtruth_annotations_list,\n        \'images\': image_list,\n        \'categories\': category_list\n    }\n\n    self._detections_list = [\n        {\n            \'image_id\': \'first\',\n            \'category_id\': 1,\n            \'bbox\': [100., 100., 100., 100.],\n            \'score\': .8\n        },\n        {\n            \'image_id\': \'second\',\n            \'category_id\': 1,\n            \'bbox\': [50., 50., 50., 50.],\n            \'score\': .7\n        },\n    ]\n\n  def testCocoWrappers(self):\n    groundtruth = coco_tools.COCOWrapper(self._groundtruth_dict)\n    detections = groundtruth.LoadAnnotations(self._detections_list)\n    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections)\n    summary_metrics, _ = evaluator.ComputeMetrics()\n    self.assertAlmostEqual(1.0, summary_metrics[\'Precision/mAP\'])\n\n  def testExportGroundtruthToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    groundtruth_boxes = [np.array([[100, 100, 200, 200]], np.float),\n                         np.array([[50, 50, 100, 100]], np.float)]\n    groundtruth_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'groundtruth.json\')\n    result = coco_tools.ExportGroundtruthToCOCO(\n        image_ids,\n        groundtruth_boxes,\n        groundtruth_classes,\n        categories,\n        output_path=output_path)\n    self.assertDictEqual(result, self._groundtruth_dict)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      # The json output should have floats written to 4 digits of precision.\n      matcher = re.compile(r\'""bbox"":\\s+\\[\\n\\s+\\d+.\\d\\d\\d\\d,\', re.MULTILINE)\n      self.assertTrue(matcher.findall(written_result))\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportDetectionsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detections_boxes = [np.array([[100, 100, 200, 200]], np.float),\n                        np.array([[50, 50, 100, 100]], np.float)]\n    detections_scores = [np.array([.8], np.float), np.array([.7], np.float)]\n    detections_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'detections.json\')\n    result = coco_tools.ExportDetectionsToCOCO(\n        image_ids,\n        detections_boxes,\n        detections_scores,\n        detections_classes,\n        categories,\n        output_path=output_path)\n    self.assertListEqual(result, self._detections_list)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      # The json output should have floats written to 4 digits of precision.\n      matcher = re.compile(r\'""bbox"":\\s+\\[\\n\\s+\\d+.\\d\\d\\d\\d,\', re.MULTILINE)\n      self.assertTrue(matcher.findall(written_result))\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportSegmentsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detection_masks = [np.array(\n        [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],\n        dtype=np.uint8), np.array(\n            [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],\n            dtype=np.uint8)]\n\n    for i, detection_mask in enumerate(detection_masks):\n      detection_masks[i] = detection_mask[:, :, :, None]\n\n    detection_scores = [np.array([.8], np.float), np.array([.7], np.float)]\n    detection_classes = [np.array([1], np.int32), np.array([1], np.int32)]\n\n    categories = [{\'id\': 0, \'name\': \'person\'},\n                  {\'id\': 1, \'name\': \'cat\'},\n                  {\'id\': 2, \'name\': \'dog\'}]\n    output_path = os.path.join(tf.test.get_temp_dir(), \'segments.json\')\n    result = coco_tools.ExportSegmentsToCOCO(\n        image_ids,\n        detection_masks,\n        detection_scores,\n        detection_classes,\n        categories,\n        output_path=output_path)\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      written_result = json.loads(written_result)\n      mask_load = mask.decode([written_result[0][\'segmentation\']])\n      self.assertTrue(np.allclose(mask_load, detection_masks[0]))\n      self.assertAlmostEqual(result, written_result)\n\n  def testExportKeypointsToCOCO(self):\n    image_ids = [\'first\', \'second\']\n    detection_keypoints = [\n        np.array(\n            [[[100, 200], [300, 400], [500, 600]],\n             [[50, 150], [250, 350], [450, 550]]], dtype=np.int32),\n        np.array(\n            [[[110, 210], [310, 410], [510, 610]],\n             [[60, 160], [260, 360], [460, 560]]], dtype=np.int32)]\n\n    detection_scores = [np.array([.8, 0.2], np.float),\n                        np.array([.7, 0.3], np.float)]\n    detection_classes = [np.array([1, 1], np.int32), np.array([1, 1], np.int32)]\n\n    categories = [{\'id\': 1, \'name\': \'person\', \'num_keypoints\': 3},\n                  {\'id\': 2, \'name\': \'cat\'},\n                  {\'id\': 3, \'name\': \'dog\'}]\n\n    output_path = os.path.join(tf.test.get_temp_dir(), \'keypoints.json\')\n    result = coco_tools.ExportKeypointsToCOCO(\n        image_ids,\n        detection_keypoints,\n        detection_scores,\n        detection_classes,\n        categories,\n        output_path=output_path)\n\n    with tf.gfile.GFile(output_path, \'r\') as f:\n      written_result = f.read()\n      written_result = json.loads(written_result)\n      self.assertAlmostEqual(result, written_result)\n\n  def testSingleImageDetectionBoxesExport(self):\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5],\n                      [.5, .5, 1, 1]], dtype=np.float32)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)\n    coco_boxes = np.array([[0, 0, 1, 1],\n                           [0, 0, .5, .5],\n                           [.5, .5, .5, .5]], dtype=np.float32)\n    coco_annotations = coco_tools.ExportSingleImageDetectionBoxesToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        detection_boxes=boxes,\n        detection_classes=classes,\n        detection_scores=scores)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertAlmostEqual(annotation[\'score\'], scores[i])\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n\n  def testSingleImageDetectionMaskExport(self):\n    masks = np.array(\n        [[[1, 1,], [1, 1]],\n         [[0, 0], [0, 1]],\n         [[0, 0], [0, 0]]], dtype=np.uint8)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)\n    coco_annotations = coco_tools.ExportSingleImageDetectionMasksToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        detection_classes=classes,\n        detection_scores=scores,\n        detection_masks=masks)\n    expected_counts = [\'04\', \'31\', \'4\']\n    for i, mask_annotation in enumerate(coco_annotations):\n      self.assertEqual(mask_annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          mask_annotation[\'segmentation\']), masks[i])))\n      self.assertEqual(mask_annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(mask_annotation[\'category_id\'], classes[i])\n      self.assertAlmostEqual(mask_annotation[\'score\'], scores[i])\n\n  def testSingleImageGroundtruthExport(self):\n    masks = np.array(\n        [[[1, 1,], [1, 1]],\n         [[0, 0], [0, 1]],\n         [[0, 0], [0, 0]]], dtype=np.uint8)\n    boxes = np.array([[0, 0, 1, 1],\n                      [0, 0, .5, .5],\n                      [.5, .5, 1, 1]], dtype=np.float32)\n    coco_boxes = np.array([[0, 0, 1, 1],\n                           [0, 0, .5, .5],\n                           [.5, .5, .5, .5]], dtype=np.float32)\n    classes = np.array([1, 2, 3], dtype=np.int32)\n    is_crowd = np.array([0, 1, 0], dtype=np.int32)\n    next_annotation_id = 1\n    expected_counts = [\'04\', \'31\', \'4\']\n\n    # Tests exporting without passing in is_crowd (for backward compatibility).\n    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        next_annotation_id=next_annotation_id,\n        groundtruth_boxes=boxes,\n        groundtruth_classes=classes,\n        groundtruth_masks=masks)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          annotation[\'segmentation\']), masks[i])))\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertEqual(annotation[\'id\'], i + next_annotation_id)\n\n    # Tests exporting with is_crowd.\n    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(\n        image_id=\'first_image\',\n        category_id_set=set([1, 2, 3]),\n        next_annotation_id=next_annotation_id,\n        groundtruth_boxes=boxes,\n        groundtruth_classes=classes,\n        groundtruth_masks=masks,\n        groundtruth_is_crowd=is_crowd)\n    for i, annotation in enumerate(coco_annotations):\n      self.assertEqual(annotation[\'segmentation\'][\'counts\'],\n                       expected_counts[i])\n      self.assertTrue(np.all(np.equal(mask.decode(\n          annotation[\'segmentation\']), masks[i])))\n      self.assertTrue(np.all(np.isclose(annotation[\'bbox\'], coco_boxes[i])))\n      self.assertEqual(annotation[\'image_id\'], \'first_image\')\n      self.assertEqual(annotation[\'category_id\'], classes[i])\n      self.assertEqual(annotation[\'iscrowd\'], is_crowd[i])\n      self.assertEqual(annotation[\'id\'], i + next_annotation_id)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py,1,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for ssd resnet v1 FPN feature extractors.""""""\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor_testbase\n\n\nclass SSDResnet50V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet50v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return ssd_resnet_v1_fpn_feature_extractor.SSDResnet50V1FpnFeatureExtractor(\n        is_training, depth_multiplier, min_depth, pad_to_multiple,\n        self.conv_hyperparams_fn, use_explicit_padding=use_explicit_padding)\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_50\'\n\n\nclass SSDResnet101V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet101v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return (\n        ssd_resnet_v1_fpn_feature_extractor.SSDResnet101V1FpnFeatureExtractor(\n            is_training,\n            depth_multiplier,\n            min_depth,\n            pad_to_multiple,\n            self.conv_hyperparams_fn,\n            use_explicit_padding=use_explicit_padding))\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_101\'\n\n\nclass SSDResnet152V1FeatureExtractorTest(\n    ssd_resnet_v1_fpn_feature_extractor_testbase.\n    SSDResnetFPNFeatureExtractorTestBase):\n  """"""SSDResnet152v1Fpn feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return (\n        ssd_resnet_v1_fpn_feature_extractor.SSDResnet152V1FpnFeatureExtractor(\n            is_training,\n            depth_multiplier,\n            min_depth,\n            pad_to_multiple,\n            self.conv_hyperparams_fn,\n            use_explicit_padding=use_explicit_padding))\n\n  def _resnet_scope_name(self):\n    return \'resnet_v1_152\'\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py,1,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for ssd resnet v1 feature extractors.""""""\nimport tensorflow as tf\n\nfrom object_detection.models import ssd_resnet_v1_ppn_feature_extractor\nfrom object_detection.models import ssd_resnet_v1_ppn_feature_extractor_testbase\n\n\nclass SSDResnet50V1PpnFeatureExtractorTest(\n    ssd_resnet_v1_ppn_feature_extractor_testbase.\n    SSDResnetPpnFeatureExtractorTestBase):\n  """"""SSDResnet50v1 feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return ssd_resnet_v1_ppn_feature_extractor.SSDResnet50V1PpnFeatureExtractor(\n        is_training,\n        depth_multiplier,\n        min_depth,\n        pad_to_multiple,\n        self.conv_hyperparams_fn,\n        use_explicit_padding=use_explicit_padding)\n\n  def _scope_name(self):\n    return \'resnet_v1_50\'\n\n\nclass SSDResnet101V1PpnFeatureExtractorTest(\n    ssd_resnet_v1_ppn_feature_extractor_testbase.\n    SSDResnetPpnFeatureExtractorTestBase):\n  """"""SSDResnet101v1 feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return (\n        ssd_resnet_v1_ppn_feature_extractor.SSDResnet101V1PpnFeatureExtractor(\n            is_training,\n            depth_multiplier,\n            min_depth,\n            pad_to_multiple,\n            self.conv_hyperparams_fn,\n            use_explicit_padding=use_explicit_padding))\n\n  def _scope_name(self):\n    return \'resnet_v1_101\'\n\n\nclass SSDResnet152V1PpnFeatureExtractorTest(\n    ssd_resnet_v1_ppn_feature_extractor_testbase.\n    SSDResnetPpnFeatureExtractorTestBase):\n  """"""SSDResnet152v1 feature extractor test.""""""\n\n  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,\n                                use_explicit_padding=False):\n    min_depth = 32\n    is_training = True\n    return (\n        ssd_resnet_v1_ppn_feature_extractor.SSDResnet152V1PpnFeatureExtractor(\n            is_training,\n            depth_multiplier,\n            min_depth,\n            pad_to_multiple,\n            self.conv_hyperparams_fn,\n            use_explicit_padding=use_explicit_padding))\n\n  def _scope_name(self):\n    return \'resnet_v1_152\'\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/utils/context_manager_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_models.object_detection.utils.context_manager.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom object_detection.utils import context_manager\n\n\nclass ContextManagerTest(tf.test.TestCase):\n\n  def test_identity_context_manager(self):\n    with context_manager.IdentityContextManager() as identity_context:\n      self.assertIsNone(identity_context)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
custom_vehicle_training/object_detection/utils/ops.py,202,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A module for helper tensorflow ops.""""""\nimport math\nimport numpy as np\nimport six\n\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields as fields\nfrom object_detection.utils import shape_utils\nfrom object_detection.utils import static_shape\n\n\ndef expanded_shape(orig_shape, start_dim, num_dims):\n  """"""Inserts multiple ones into a shape vector.\n\n  Inserts an all-1 vector of length num_dims at position start_dim into a shape.\n  Can be combined with tf.reshape to generalize tf.expand_dims.\n\n  Args:\n    orig_shape: the shape into which the all-1 vector is added (int32 vector)\n    start_dim: insertion position (int scalar)\n    num_dims: length of the inserted all-1 vector (int scalar)\n  Returns:\n    An int32 vector of length tf.size(orig_shape) + num_dims.\n  """"""\n  with tf.name_scope(\'ExpandedShape\'):\n    start_dim = tf.expand_dims(start_dim, 0)  # scalar to rank-1\n    before = tf.slice(orig_shape, [0], start_dim)\n    add_shape = tf.ones(tf.reshape(num_dims, [1]), dtype=tf.int32)\n    after = tf.slice(orig_shape, start_dim, [-1])\n    new_shape = tf.concat([before, add_shape, after], 0)\n    return new_shape\n\n\ndef normalized_to_image_coordinates(normalized_boxes, image_shape,\n                                    parallel_iterations=32):\n  """"""Converts a batch of boxes from normal to image coordinates.\n\n  Args:\n    normalized_boxes: a float32 tensor of shape [None, num_boxes, 4] in\n      normalized coordinates.\n    image_shape: a float32 tensor of shape [4] containing the image shape.\n    parallel_iterations: parallelism for the map_fn op.\n\n  Returns:\n    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containing\n      the boxes in image coordinates.\n  """"""\n  x_scale = tf.cast(image_shape[2], tf.float32)\n  y_scale = tf.cast(image_shape[1], tf.float32)\n  def _to_absolute_coordinates(normalized_boxes):\n    y_min, x_min, y_max, x_max = tf.split(\n        value=normalized_boxes, num_or_size_splits=4, axis=1)\n    y_min = y_scale * y_min\n    y_max = y_scale * y_max\n    x_min = x_scale * x_min\n    x_max = x_scale * x_max\n    scaled_boxes = tf.concat([y_min, x_min, y_max, x_max], 1)\n    return scaled_boxes\n\n  absolute_boxes = shape_utils.static_or_dynamic_map_fn(\n      _to_absolute_coordinates,\n      elems=(normalized_boxes),\n      dtype=tf.float32,\n      parallel_iterations=parallel_iterations,\n      back_prop=True)\n  return absolute_boxes\n\n\ndef meshgrid(x, y):\n  """"""Tiles the contents of x and y into a pair of grids.\n\n  Multidimensional analog of numpy.meshgrid, giving the same behavior if x and y\n  are vectors. Generally, this will give:\n\n  xgrid(i1, ..., i_m, j_1, ..., j_n) = x(j_1, ..., j_n)\n  ygrid(i1, ..., i_m, j_1, ..., j_n) = y(i_1, ..., i_m)\n\n  Keep in mind that the order of the arguments and outputs is reverse relative\n  to the order of the indices they go into, done for compatibility with numpy.\n  The output tensors have the same shapes.  Specifically:\n\n  xgrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n  ygrid.get_shape() = y.get_shape().concatenate(x.get_shape())\n\n  Args:\n    x: A tensor of arbitrary shape and rank. xgrid will contain these values\n       varying in its last dimensions.\n    y: A tensor of arbitrary shape and rank. ygrid will contain these values\n       varying in its first dimensions.\n  Returns:\n    A tuple of tensors (xgrid, ygrid).\n  """"""\n  with tf.name_scope(\'Meshgrid\'):\n    x = tf.convert_to_tensor(x)\n    y = tf.convert_to_tensor(y)\n    x_exp_shape = expanded_shape(tf.shape(x), 0, tf.rank(y))\n    y_exp_shape = expanded_shape(tf.shape(y), tf.rank(y), tf.rank(x))\n\n    xgrid = tf.tile(tf.reshape(x, x_exp_shape), y_exp_shape)\n    ygrid = tf.tile(tf.reshape(y, y_exp_shape), x_exp_shape)\n    new_shape = y.get_shape().concatenate(x.get_shape())\n    xgrid.set_shape(new_shape)\n    ygrid.set_shape(new_shape)\n\n    return xgrid, ygrid\n\n\ndef fixed_padding(inputs, kernel_size, rate=1):\n  """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n  pad_total = kernel_size_effective - 1\n  pad_beg = pad_total // 2\n  pad_end = pad_total - pad_beg\n  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n                                  [pad_beg, pad_end], [0, 0]])\n  return padded_inputs\n\n\ndef pad_to_multiple(tensor, multiple):\n  """"""Returns the tensor zero padded to the specified multiple.\n\n  Appends 0s to the end of the first and second dimension (height and width) of\n  the tensor until both dimensions are a multiple of the input argument\n  \'multiple\'. E.g. given an input tensor of shape [1, 3, 5, 1] and an input\n  multiple of 4, PadToMultiple will append 0s so that the resulting tensor will\n  be of shape [1, 4, 8, 1].\n\n  Args:\n    tensor: rank 4 float32 tensor, where\n            tensor -> [batch_size, height, width, channels].\n    multiple: the multiple to pad to.\n\n  Returns:\n    padded_tensor: the tensor zero padded to the specified multiple.\n  """"""\n  if multiple == 1:\n    return tensor\n\n  tensor_shape = tensor.get_shape()\n  batch_size = static_shape.get_batch_size(tensor_shape)\n  tensor_height = static_shape.get_height(tensor_shape)\n  tensor_width = static_shape.get_width(tensor_shape)\n  tensor_depth = static_shape.get_depth(tensor_shape)\n\n  if batch_size is None:\n    batch_size = tf.shape(tensor)[0]\n\n  if tensor_height is None:\n    tensor_height = tf.shape(tensor)[1]\n    padded_tensor_height = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_height) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_height = int(\n        math.ceil(float(tensor_height) / multiple) * multiple)\n\n  if tensor_width is None:\n    tensor_width = tf.shape(tensor)[2]\n    padded_tensor_width = tf.to_int32(\n        tf.ceil(tf.to_float(tensor_width) / tf.to_float(multiple))) * multiple\n  else:\n    padded_tensor_width = int(\n        math.ceil(float(tensor_width) / multiple) * multiple)\n\n  if tensor_depth is None:\n    tensor_depth = tf.shape(tensor)[3]\n\n  # Use tf.concat instead of tf.pad to preserve static shape\n  if padded_tensor_height != tensor_height:\n    height_pad = tf.zeros([\n        batch_size, padded_tensor_height - tensor_height, tensor_width,\n        tensor_depth\n    ])\n    tensor = tf.concat([tensor, height_pad], 1)\n  if padded_tensor_width != tensor_width:\n    width_pad = tf.zeros([\n        batch_size, padded_tensor_height, padded_tensor_width - tensor_width,\n        tensor_depth\n    ])\n    tensor = tf.concat([tensor, width_pad], 2)\n\n  return tensor\n\n\ndef padded_one_hot_encoding(indices, depth, left_pad):\n  """"""Returns a zero padded one-hot tensor.\n\n  This function converts a sparse representation of indices (e.g., [4]) to a\n  zero padded one-hot representation (e.g., [0, 0, 0, 0, 1] with depth = 4 and\n  left_pad = 1). If `indices` is empty, the result will simply be a tensor of\n  shape (0, depth + left_pad). If depth = 0, then this function just returns\n  `None`.\n\n  Args:\n    indices: an integer tensor of shape [num_indices].\n    depth: depth for the one-hot tensor (integer).\n    left_pad: number of zeros to left pad the one-hot tensor with (integer).\n\n  Returns:\n    padded_onehot: a tensor with shape (num_indices, depth + left_pad). Returns\n      `None` if the depth is zero.\n\n  Raises:\n    ValueError: if `indices` does not have rank 1 or if `left_pad` or `depth are\n      either negative or non-integers.\n\n  TODO(rathodv): add runtime checks for depth and indices.\n  """"""\n  if depth < 0 or not isinstance(depth, six.integer_types):\n    raise ValueError(\'`depth` must be a non-negative integer.\')\n  if left_pad < 0 or not isinstance(left_pad, six.integer_types):\n    raise ValueError(\'`left_pad` must be a non-negative integer.\')\n  if depth == 0:\n    return None\n\n  rank = len(indices.get_shape().as_list())\n  if rank != 1:\n    raise ValueError(\'`indices` must have rank 1, but has rank=%s\' % rank)\n\n  def one_hot_and_pad():\n    one_hot = tf.cast(tf.one_hot(tf.cast(indices, tf.int64), depth,\n                                 on_value=1, off_value=0), tf.float32)\n    return tf.pad(one_hot, [[0, 0], [left_pad, 0]], mode=\'CONSTANT\')\n  result = tf.cond(tf.greater(tf.size(indices), 0), one_hot_and_pad,\n                   lambda: tf.zeros((depth + left_pad, 0)))\n  return tf.reshape(result, [-1, depth + left_pad])\n\n\ndef dense_to_sparse_boxes(dense_locations, dense_num_boxes, num_classes):\n  """"""Converts bounding boxes from dense to sparse form.\n\n  Args:\n    dense_locations:  a [max_num_boxes, 4] tensor in which only the first k rows\n      are valid bounding box location coordinates, where k is the sum of\n      elements in dense_num_boxes.\n    dense_num_boxes: a [max_num_classes] tensor indicating the counts of\n       various bounding box classes e.g. [1, 0, 0, 2] means that the first\n       bounding box is of class 0 and the second and third bounding boxes are\n       of class 3. The sum of elements in this tensor is the number of valid\n       bounding boxes.\n    num_classes: number of classes\n\n  Returns:\n    box_locations: a [num_boxes, 4] tensor containing only valid bounding\n       boxes (i.e. the first num_boxes rows of dense_locations)\n    box_classes: a [num_boxes] tensor containing the classes of each bounding\n       box (e.g. dense_num_boxes = [1, 0, 0, 2] => box_classes = [0, 3, 3]\n  """"""\n\n  num_valid_boxes = tf.reduce_sum(dense_num_boxes)\n  box_locations = tf.slice(dense_locations,\n                           tf.constant([0, 0]), tf.stack([num_valid_boxes, 4]))\n  tiled_classes = [tf.tile([i], tf.expand_dims(dense_num_boxes[i], 0))\n                   for i in range(num_classes)]\n  box_classes = tf.concat(tiled_classes, 0)\n  box_locations.set_shape([None, 4])\n  return box_locations, box_classes\n\n\ndef indices_to_dense_vector(indices,\n                            size,\n                            indices_value=1.,\n                            default_value=0,\n                            dtype=tf.float32):\n  """"""Creates dense vector with indices set to specific value and rest to zeros.\n\n  This function exists because it is unclear if it is safe to use\n    tf.sparse_to_dense(indices, [size], 1, validate_indices=False)\n  with indices which are not ordered.\n  This function accepts a dynamic size (e.g. tf.shape(tensor)[0])\n\n  Args:\n    indices: 1d Tensor with integer indices which are to be set to\n        indices_values.\n    size: scalar with size (integer) of output Tensor.\n    indices_value: values of elements specified by indices in the output vector\n    default_value: values of other elements in the output vector.\n    dtype: data type.\n\n  Returns:\n    dense 1D Tensor of shape [size] with indices set to indices_values and the\n        rest set to default_value.\n  """"""\n  size = tf.to_int32(size)\n  zeros = tf.ones([size], dtype=dtype) * default_value\n  values = tf.ones_like(indices, dtype=dtype) * indices_value\n\n  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],\n                           [zeros, values])\n\n\ndef reduce_sum_trailing_dimensions(tensor, ndims):\n  """"""Computes sum across all dimensions following first `ndims` dimensions.""""""\n  return tf.reduce_sum(tensor, axis=tuple(range(ndims, tensor.shape.ndims)))\n\n\ndef retain_groundtruth(tensor_dict, valid_indices):\n  """"""Retains groundtruth by valid indices.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n    valid_indices: a tensor with valid indices for the box-level groundtruth.\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth for valid_indices.\n\n  Raises:\n    ValueError: If the shape of valid_indices is invalid.\n    ValueError: field fields.InputDataFields.groundtruth_boxes is\n      not present in tensor_dict.\n  """"""\n  input_shape = valid_indices.get_shape().as_list()\n  if not (len(input_shape) == 1 or\n          (len(input_shape) == 2 and input_shape[1] == 1)):\n    raise ValueError(\'The shape of valid_indices is invalid.\')\n  valid_indices = tf.reshape(valid_indices, [-1])\n  valid_dict = {}\n  if fields.InputDataFields.groundtruth_boxes in tensor_dict:\n    # Prevents reshape failure when num_boxes is 0.\n    num_boxes = tf.maximum(tf.shape(\n        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0], 1)\n    for key in tensor_dict:\n      if key in [fields.InputDataFields.groundtruth_boxes,\n                 fields.InputDataFields.groundtruth_classes,\n                 fields.InputDataFields.groundtruth_keypoints,\n                 fields.InputDataFields.groundtruth_instance_masks]:\n        valid_dict[key] = tf.gather(tensor_dict[key], valid_indices)\n      # Input decoder returns empty tensor when these fields are not provided.\n      # Needs to reshape into [num_boxes, -1] for tf.gather() to work.\n      elif key in [fields.InputDataFields.groundtruth_is_crowd,\n                   fields.InputDataFields.groundtruth_area,\n                   fields.InputDataFields.groundtruth_difficult,\n                   fields.InputDataFields.groundtruth_label_types]:\n        valid_dict[key] = tf.reshape(\n            tf.gather(tf.reshape(tensor_dict[key], [num_boxes, -1]),\n                      valid_indices), [-1])\n      # Fields that are not associated with boxes.\n      else:\n        valid_dict[key] = tensor_dict[key]\n  else:\n    raise ValueError(\'%s not present in input tensor dict.\' % (\n        fields.InputDataFields.groundtruth_boxes))\n  return valid_dict\n\n\ndef retain_groundtruth_with_positive_classes(tensor_dict):\n  """"""Retains only groundtruth with positive class ids.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n      fields.InputDataFields.groundtruth_difficult\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth with positive\n    classes.\n\n  Raises:\n    ValueError: If groundtruth_classes tensor is not in tensor_dict.\n  """"""\n  if fields.InputDataFields.groundtruth_classes not in tensor_dict:\n    raise ValueError(\'`groundtruth classes` not in tensor_dict.\')\n  keep_indices = tf.where(tf.greater(\n      tensor_dict[fields.InputDataFields.groundtruth_classes], 0))\n  return retain_groundtruth(tensor_dict, keep_indices)\n\n\ndef replace_nan_groundtruth_label_scores_with_ones(label_scores):\n  """"""Replaces nan label scores with 1.0.\n\n  Args:\n    label_scores: a tensor containing object annoation label scores.\n\n  Returns:\n    a tensor where NaN label scores have been replaced by ones.\n  """"""\n  return tf.where(\n      tf.is_nan(label_scores), tf.ones(tf.shape(label_scores)), label_scores)\n\n\ndef filter_groundtruth_with_crowd_boxes(tensor_dict):\n  """"""Filters out groundtruth with boxes corresponding to crowd.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  if fields.InputDataFields.groundtruth_is_crowd in tensor_dict:\n    is_crowd = tensor_dict[fields.InputDataFields.groundtruth_is_crowd]\n    is_not_crowd = tf.logical_not(is_crowd)\n    is_not_crowd_indices = tf.where(is_not_crowd)\n    tensor_dict = retain_groundtruth(tensor_dict, is_not_crowd_indices)\n  return tensor_dict\n\n\ndef filter_groundtruth_with_nan_box_coordinates(tensor_dict):\n  """"""Filters out groundtruth with no bounding boxes.\n\n  Args:\n    tensor_dict: a dictionary of following groundtruth tensors -\n      fields.InputDataFields.groundtruth_boxes\n      fields.InputDataFields.groundtruth_classes\n      fields.InputDataFields.groundtruth_keypoints\n      fields.InputDataFields.groundtruth_instance_masks\n      fields.InputDataFields.groundtruth_is_crowd\n      fields.InputDataFields.groundtruth_area\n      fields.InputDataFields.groundtruth_label_types\n\n  Returns:\n    a dictionary of tensors containing only the groundtruth that have bounding\n    boxes.\n  """"""\n  groundtruth_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]\n  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.to_int32(\n      tf.is_nan(groundtruth_boxes)), reduction_indices=[1]), 0)\n  valid_indicator_vector = tf.logical_not(nan_indicator_vector)\n  valid_indices = tf.where(valid_indicator_vector)\n\n  return retain_groundtruth(tensor_dict, valid_indices)\n\n\ndef normalize_to_target(inputs,\n                        target_norm_value,\n                        dim,\n                        epsilon=1e-7,\n                        trainable=True,\n                        scope=\'NormalizeToTarget\',\n                        summarize=True):\n  """"""L2 normalizes the inputs across the specified dimension to a target norm.\n\n  This op implements the L2 Normalization layer introduced in\n  Liu, Wei, et al. ""SSD: Single Shot MultiBox Detector.""\n  and Liu, Wei, Andrew Rabinovich, and Alexander C. Berg.\n  ""Parsenet: Looking wider to see better."" and is useful for bringing\n  activations from multiple layers in a convnet to a standard scale.\n\n  Note that the rank of `inputs` must be known and the dimension to which\n  normalization is to be applied should be statically defined.\n\n  TODO(jonathanhuang): Add option to scale by L2 norm of the entire input.\n\n  Args:\n    inputs: A `Tensor` of arbitrary size.\n    target_norm_value: A float value that specifies an initial target norm or\n      a list of floats (whose length must be equal to the depth along the\n      dimension to be normalized) specifying a per-dimension multiplier\n      after normalization.\n    dim: The dimension along which the input is normalized.\n    epsilon: A small value to add to the inputs to avoid dividing by zero.\n    trainable: Whether the norm is trainable or not\n    scope: Optional scope for variable_scope.\n    summarize: Whether or not to add a tensorflow summary for the op.\n\n  Returns:\n    The input tensor normalized to the specified target norm.\n\n  Raises:\n    ValueError: If dim is smaller than the number of dimensions in \'inputs\'.\n    ValueError: If target_norm_value is not a float or a list of floats with\n      length equal to the depth along the dimension to be normalized.\n  """"""\n  with tf.variable_scope(scope, \'NormalizeToTarget\', [inputs]):\n    if not inputs.get_shape():\n      raise ValueError(\'The input rank must be known.\')\n    input_shape = inputs.get_shape().as_list()\n    input_rank = len(input_shape)\n    if dim < 0 or dim >= input_rank:\n      raise ValueError(\n          \'dim must be non-negative but smaller than the input rank.\')\n    if not input_shape[dim]:\n      raise ValueError(\'input shape should be statically defined along \'\n                       \'the specified dimension.\')\n    depth = input_shape[dim]\n    if not (isinstance(target_norm_value, float) or\n            (isinstance(target_norm_value, list) and\n             len(target_norm_value) == depth) and\n            all([isinstance(val, float) for val in target_norm_value])):\n      raise ValueError(\'target_norm_value must be a float or a list of floats \'\n                       \'with length equal to the depth along the dimension to \'\n                       \'be normalized.\')\n    if isinstance(target_norm_value, float):\n      initial_norm = depth * [target_norm_value]\n    else:\n      initial_norm = target_norm_value\n    target_norm = tf.contrib.framework.model_variable(\n        name=\'weights\', dtype=tf.float32,\n        initializer=tf.constant(initial_norm, dtype=tf.float32),\n        trainable=trainable)\n    if summarize:\n      mean = tf.reduce_mean(target_norm)\n      mean = tf.Print(mean, [\'NormalizeToTarget:\', mean])\n      tf.summary.scalar(tf.get_variable_scope().name, mean)\n    lengths = epsilon + tf.sqrt(tf.reduce_sum(tf.square(inputs), dim, True))\n    mult_shape = input_rank*[1]\n    mult_shape[dim] = depth\n    return tf.reshape(target_norm, mult_shape) * tf.truediv(inputs, lengths)\n\n\ndef batch_position_sensitive_crop_regions(images,\n                                          boxes,\n                                          crop_size,\n                                          num_spatial_bins,\n                                          global_pool,\n                                          parallel_iterations=64):\n  """"""Position sensitive crop with batches of images and boxes.\n\n  This op is exactly like `position_sensitive_crop_regions` below but operates\n  on batches of images and boxes. See `position_sensitive_crop_regions` function\n  below for the operation applied per batch element.\n\n  Args:\n    images: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 3-D tensor of shape `[batch, num_boxes, 4]`. Each box is specified in\n      normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value\n      of `y` is mapped to the image coordinate at `y * (image_height - 1)`, so\n      as the `[0, 1]` interval of normalized image height is mapped to\n      `[0, image_height - 1] in image height coordinates. We do allow y1 > y2,\n      in which case the sampled crop is an up-down flipped version of the\n      original image. The width dimension is treated similarly.\n    crop_size: See `position_sensitive_crop_regions` below.\n    num_spatial_bins: See `position_sensitive_crop_regions` below.\n    global_pool: See `position_sensitive_crop_regions` below.\n    parallel_iterations: Number of batch items to process in parallel.\n\n  Returns:\n  """"""\n  def _position_sensitive_crop_fn(inputs):\n    images, boxes = inputs\n    return position_sensitive_crop_regions(\n        images,\n        boxes,\n        crop_size=crop_size,\n        num_spatial_bins=num_spatial_bins,\n        global_pool=global_pool)\n\n  return shape_utils.static_or_dynamic_map_fn(\n      _position_sensitive_crop_fn,\n      elems=[images, boxes],\n      dtype=tf.float32,\n      parallel_iterations=parallel_iterations)\n\n\ndef position_sensitive_crop_regions(image,\n                                    boxes,\n                                    crop_size,\n                                    num_spatial_bins,\n                                    global_pool):\n  """"""Position-sensitive crop and pool rectangular regions from a feature grid.\n\n  The output crops are split into `spatial_bins_y` vertical bins\n  and `spatial_bins_x` horizontal bins. For each intersection of a vertical\n  and a horizontal bin the output values are gathered by performing\n  `tf.image.crop_and_resize` (bilinear resampling) on a a separate subset of\n  channels of the image. This reduces `depth` by a factor of\n  `(spatial_bins_y * spatial_bins_x)`.\n\n  When global_pool is True, this function implements a differentiable version\n  of position-sensitive RoI pooling used in\n  [R-FCN detection system](https://arxiv.org/abs/1605.06409).\n\n  When global_pool is False, this function implements a differentiable version\n  of position-sensitive assembling operation used in\n  [instance FCN](https://arxiv.org/abs/1603.08678).\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.\n      A 3-D tensor of shape `[image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32`.\n      A 2-D tensor of shape `[num_boxes, 4]`. Each box is specified in\n      normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value\n      of `y` is mapped to the image coordinate at `y * (image_height - 1)`, so\n      as the `[0, 1]` interval of normalized image height is mapped to\n      `[0, image_height - 1] in image height coordinates. We do allow y1 > y2,\n      in which case the sampled crop is an up-down flipped version of the\n      original image. The width dimension is treated similarly.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    num_spatial_bins: A list of two integers `[spatial_bins_y, spatial_bins_x]`.\n      Represents the number of position-sensitive bins in y and x directions.\n      Both values should be >= 1. `crop_height` should be divisible by\n      `spatial_bins_y`, and similarly for width.\n      The number of image channels should be divisible by\n      (spatial_bins_y * spatial_bins_x).\n      Suggested value from R-FCN paper: [3, 3].\n    global_pool: A boolean variable.\n      If True, we perform average global pooling on the features assembled from\n        the position-sensitive score maps.\n      If False, we keep the position-pooled features without global pooling\n        over the spatial coordinates.\n      Note that using global_pool=True is equivalent to but more efficient than\n        running the function with global_pool=False and then performing global\n        average pooling.\n\n  Returns:\n    position_sensitive_features: A 4-D tensor of shape\n      `[num_boxes, K, K, crop_channels]`,\n      where `crop_channels = depth / (spatial_bins_y * spatial_bins_x)`,\n      where K = 1 when global_pool is True (Average-pooled cropped regions),\n      and K = crop_size when global_pool is False.\n  Raises:\n    ValueError: Raised in four situations:\n      `num_spatial_bins` is not >= 1;\n      `num_spatial_bins` does not divide `crop_size`;\n      `(spatial_bins_y*spatial_bins_x)` does not divide `depth`;\n      `bin_crop_size` is not square when global_pool=False due to the\n        constraint in function space_to_depth.\n  """"""\n  total_bins = 1\n  bin_crop_size = []\n\n  for (num_bins, crop_dim) in zip(num_spatial_bins, crop_size):\n    if num_bins < 1:\n      raise ValueError(\'num_spatial_bins should be >= 1\')\n\n    if crop_dim % num_bins != 0:\n      raise ValueError(\'crop_size should be divisible by num_spatial_bins\')\n\n    total_bins *= num_bins\n    bin_crop_size.append(crop_dim // num_bins)\n\n  if not global_pool and bin_crop_size[0] != bin_crop_size[1]:\n    raise ValueError(\'Only support square bin crop size for now.\')\n\n  ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=1)\n  spatial_bins_y, spatial_bins_x = num_spatial_bins\n\n  # Split each box into spatial_bins_y * spatial_bins_x bins.\n  position_sensitive_boxes = []\n  for bin_y in range(spatial_bins_y):\n    step_y = (ymax - ymin) / spatial_bins_y\n    for bin_x in range(spatial_bins_x):\n      step_x = (xmax - xmin) / spatial_bins_x\n      box_coordinates = [ymin + bin_y * step_y,\n                         xmin + bin_x * step_x,\n                         ymin + (bin_y + 1) * step_y,\n                         xmin + (bin_x + 1) * step_x,\n                        ]\n      position_sensitive_boxes.append(tf.stack(box_coordinates, axis=1))\n\n  image_splits = tf.split(value=image, num_or_size_splits=total_bins, axis=2)\n\n  image_crops = []\n  for (split, box) in zip(image_splits, position_sensitive_boxes):\n    if split.shape.is_fully_defined() and box.shape.is_fully_defined():\n      crop = tf.squeeze(\n          matmul_crop_and_resize(\n              tf.expand_dims(split, axis=0), tf.expand_dims(box, axis=0),\n              bin_crop_size),\n          axis=0)\n    else:\n      crop = tf.image.crop_and_resize(\n          tf.expand_dims(split, 0), box,\n          tf.zeros(tf.shape(boxes)[0], dtype=tf.int32), bin_crop_size)\n    image_crops.append(crop)\n\n  if global_pool:\n    # Average over all bins.\n    position_sensitive_features = tf.add_n(image_crops) / len(image_crops)\n    # Then average over spatial positions within the bins.\n    position_sensitive_features = tf.reduce_mean(\n        position_sensitive_features, [1, 2], keep_dims=True)\n  else:\n    # Reorder height/width to depth channel.\n    block_size = bin_crop_size[0]\n    if block_size >= 2:\n      image_crops = [tf.space_to_depth(\n          crop, block_size=block_size) for crop in image_crops]\n\n    # Pack image_crops so that first dimension is for position-senstive boxes.\n    position_sensitive_features = tf.stack(image_crops, axis=0)\n\n    # Unroll the position-sensitive boxes to spatial positions.\n    position_sensitive_features = tf.squeeze(\n        tf.batch_to_space_nd(position_sensitive_features,\n                             block_shape=[1] + num_spatial_bins,\n                             crops=tf.zeros((3, 2), dtype=tf.int32)),\n        squeeze_dims=[0])\n\n    # Reorder back the depth channel.\n    if block_size >= 2:\n      position_sensitive_features = tf.depth_to_space(\n          position_sensitive_features, block_size=block_size)\n\n  return position_sensitive_features\n\n\ndef reframe_box_masks_to_image_masks(box_masks, boxes, image_height,\n                                     image_width):\n  """"""Transforms the box masks back to full image masks.\n\n  Embeds masks in bounding boxes of larger masks whose shapes correspond to\n  image shape.\n\n  Args:\n    box_masks: A tf.float32 tensor of size [num_masks, mask_height, mask_width].\n    boxes: A tf.float32 tensor of size [num_masks, 4] containing the box\n           corners. Row i contains [ymin, xmin, ymax, xmax] of the box\n           corresponding to mask i. Note that the box corners are in\n           normalized coordinates.\n    image_height: Image height. The output mask will have the same height as\n                  the image height.\n    image_width: Image width. The output mask will have the same width as the\n                 image width.\n\n  Returns:\n    A tf.float32 tensor of size [num_masks, image_height, image_width].\n  """"""\n  # TODO(rathodv): Make this a public function.\n  def reframe_box_masks_to_image_masks_default():\n    """"""The default function when there are more than 0 box masks.""""""\n    def transform_boxes_relative_to_boxes(boxes, reference_boxes):\n      boxes = tf.reshape(boxes, [-1, 2, 2])\n      min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)\n      max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)\n      transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)\n      return tf.reshape(transformed_boxes, [-1, 4])\n\n    box_masks_expanded = tf.expand_dims(box_masks, axis=3)\n    num_boxes = tf.shape(box_masks_expanded)[0]\n    unit_boxes = tf.concat(\n        [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)\n    reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)\n    return tf.image.crop_and_resize(\n        image=box_masks_expanded,\n        boxes=reverse_boxes,\n        box_ind=tf.range(num_boxes),\n        crop_size=[image_height, image_width],\n        extrapolation_value=0.0)\n  image_masks = tf.cond(\n      tf.shape(box_masks)[0] > 0,\n      reframe_box_masks_to_image_masks_default,\n      lambda: tf.zeros([0, image_height, image_width, 1], dtype=tf.float32))\n  return tf.squeeze(image_masks, axis=3)\n\n\ndef merge_boxes_with_multiple_labels(boxes,\n                                     classes,\n                                     confidences,\n                                     num_classes,\n                                     quantization_bins=10000):\n  """"""Merges boxes with same coordinates and returns K-hot encoded classes.\n\n  Args:\n    boxes: A tf.float32 tensor with shape [N, 4] holding N boxes. Only\n      normalized coordinates are allowed.\n    classes: A tf.int32 tensor with shape [N] holding class indices.\n      The class index starts at 0.\n    confidences: A tf.float32 tensor with shape [N] holding class confidences.\n    num_classes: total number of classes to use for K-hot encoding.\n    quantization_bins: the number of bins used to quantize the box coordinate.\n\n  Returns:\n    merged_boxes: A tf.float32 tensor with shape [N\', 4] holding boxes,\n      where N\' <= N.\n    class_encodings: A tf.int32 tensor with shape [N\', num_classes] holding\n      K-hot encodings for the merged boxes.\n    confidence_encodings: A tf.float32 tensor with shape [N\', num_classes]\n      holding encodings of confidences for the merged boxes.\n    merged_box_indices: A tf.int32 tensor with shape [N\'] holding original\n      indices of the boxes.\n  """"""\n  boxes_shape = tf.shape(boxes)\n  classes_shape = tf.shape(classes)\n  confidences_shape = tf.shape(confidences)\n  box_class_shape_assert = shape_utils.assert_shape_equal_along_first_dimension(\n      boxes_shape, classes_shape)\n  box_confidence_shape_assert = (\n      shape_utils.assert_shape_equal_along_first_dimension(\n          boxes_shape, confidences_shape))\n  box_dimension_assert = tf.assert_equal(boxes_shape[1], 4)\n  box_normalized_assert = shape_utils.assert_box_normalized(boxes)\n\n  with tf.control_dependencies(\n      [box_class_shape_assert, box_confidence_shape_assert,\n       box_dimension_assert, box_normalized_assert]):\n    quantized_boxes = tf.to_int64(boxes * (quantization_bins - 1))\n    ymin, xmin, ymax, xmax = tf.unstack(quantized_boxes, axis=1)\n    hashcodes = (\n        ymin +\n        xmin * quantization_bins +\n        ymax * quantization_bins * quantization_bins +\n        xmax * quantization_bins * quantization_bins * quantization_bins)\n    unique_hashcodes, unique_indices = tf.unique(hashcodes)\n    num_boxes = tf.shape(boxes)[0]\n    num_unique_boxes = tf.shape(unique_hashcodes)[0]\n    merged_box_indices = tf.unsorted_segment_min(\n        tf.range(num_boxes), unique_indices, num_unique_boxes)\n    merged_boxes = tf.gather(boxes, merged_box_indices)\n\n    def map_box_encodings(i):\n      """"""Produces box K-hot and score encodings for each class index.""""""\n      box_mask = tf.equal(\n          unique_indices, i * tf.ones(num_boxes, dtype=tf.int32))\n      box_mask = tf.reshape(box_mask, [-1])\n      box_indices = tf.boolean_mask(classes, box_mask)\n      box_confidences = tf.boolean_mask(confidences, box_mask)\n      box_class_encodings = tf.sparse_to_dense(\n          box_indices, [num_classes], 1, validate_indices=False)\n      box_confidence_encodings = tf.sparse_to_dense(\n          box_indices, [num_classes], box_confidences, validate_indices=False)\n      return box_class_encodings, box_confidence_encodings\n\n    class_encodings, confidence_encodings = tf.map_fn(\n        map_box_encodings,\n        tf.range(num_unique_boxes),\n        back_prop=False,\n        dtype=(tf.int32, tf.float32))\n\n    merged_boxes = tf.reshape(merged_boxes, [-1, 4])\n    class_encodings = tf.reshape(class_encodings, [-1, num_classes])\n    confidence_encodings = tf.reshape(confidence_encodings, [-1, num_classes])\n    merged_box_indices = tf.reshape(merged_box_indices, [-1])\n    return (merged_boxes, class_encodings, confidence_encodings,\n            merged_box_indices)\n\n\ndef nearest_neighbor_upsampling(input_tensor, scale=None, height_scale=None,\n                                width_scale=None):\n  """"""Nearest neighbor upsampling implementation.\n\n  Nearest neighbor upsampling function that maps input tensor with shape\n  [batch_size, height, width, channels] to [batch_size, height * scale\n  , width * scale, channels]. This implementation only uses reshape and\n  broadcasting to make it TPU compatible.\n\n  Args:\n    input_tensor: A float32 tensor of size [batch, height_in, width_in,\n      channels].\n    scale: An integer multiple to scale resolution of input data in both height\n      and width dimensions.\n    height_scale: An integer multiple to scale the height of input image. This\n      option when provided overrides `scale` option.\n    width_scale: An integer multiple to scale the width of input image. This\n      option when provided overrides `scale` option.\n  Returns:\n    data_up: A float32 tensor of size\n      [batch, height_in*scale, width_in*scale, channels].\n\n  Raises:\n    ValueError: If both scale and height_scale or if both scale and width_scale\n      are None.\n  """"""\n  if not scale and (height_scale is None or width_scale is None):\n    raise ValueError(\'Provide either `scale` or `height_scale` and\'\n                     \' `width_scale`.\')\n  with tf.name_scope(\'nearest_neighbor_upsampling\'):\n    h_scale = scale if height_scale is None else height_scale\n    w_scale = scale if width_scale is None else width_scale\n    (batch_size, height, width,\n     channels) = shape_utils.combined_static_and_dynamic_shape(input_tensor)\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, height, 1, width, 1, channels]) * tf.ones(\n            [1, 1, h_scale, 1, w_scale, 1], dtype=input_tensor.dtype)\n    return tf.reshape(output_tensor,\n                      [batch_size, height * h_scale, width * w_scale, channels])\n\n\ndef matmul_gather_on_zeroth_axis(params, indices, scope=None):\n  """"""Matrix multiplication based implementation of tf.gather on zeroth axis.\n\n  TODO(rathodv, jonathanhuang): enable sparse matmul option.\n\n  Args:\n    params: A float32 Tensor. The tensor from which to gather values.\n      Must be at least rank 1.\n    indices: A Tensor. Must be one of the following types: int32, int64.\n      Must be in range [0, params.shape[0])\n    scope: A name for the operation (optional).\n\n  Returns:\n    A Tensor. Has the same type as params. Values from params gathered\n    from indices given by indices, with shape indices.shape + params.shape[1:].\n  """"""\n  with tf.name_scope(scope, \'MatMulGather\'):\n    params_shape = shape_utils.combined_static_and_dynamic_shape(params)\n    indices_shape = shape_utils.combined_static_and_dynamic_shape(indices)\n    params2d = tf.reshape(params, [params_shape[0], -1])\n    indicator_matrix = tf.one_hot(indices, params_shape[0])\n    gathered_result_flattened = tf.matmul(indicator_matrix, params2d)\n    return tf.reshape(gathered_result_flattened,\n                      tf.stack(indices_shape + params_shape[1:]))\n\n\ndef matmul_crop_and_resize(image, boxes, crop_size, scope=None):\n  """"""Matrix multiplication based implementation of the crop and resize op.\n\n  Extracts crops from the input image tensor and bilinearly resizes them\n  (possibly with aspect ratio change) to a common output size specified by\n  crop_size. This is more general than the crop_to_bounding_box op which\n  extracts a fixed size slice from the input image and does not allow\n  resizing or aspect ratio change.\n\n  Returns a tensor with crops from the input image at positions defined at\n  the bounding box locations in boxes. The cropped boxes are all resized\n  (with bilinear interpolation) to a fixed size = `[crop_height, crop_width]`.\n  The result is a 5-D tensor `[batch, num_boxes, crop_height, crop_width,\n  depth]`.\n\n  Running time complexity:\n    O((# channels) * (# boxes) * (crop_size)^2 * M), where M is the number\n  of pixels of the longer edge of the image.\n\n  Note that this operation is meant to replicate the behavior of the standard\n  tf.image.crop_and_resize operation but there are a few differences.\n  Specifically:\n    1) The extrapolation value (the values that are interpolated from outside\n      the bounds of the image window) is always zero\n    2) Only XLA supported operations are used (e.g., matrix multiplication).\n    3) There is no `box_indices` argument --- to run this op on multiple images,\n      one must currently call this op independently on each image.\n    4) All shapes and the `crop_size` parameter are assumed to be statically\n      defined.  Moreover, the number of boxes must be strictly nonzero.\n\n  Args:\n    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,\n      `int16`, `int32`, `int64`, `half`, \'bfloat16\', `float32`, `float64`.\n      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.\n      Both `image_height` and `image_width` need to be positive.\n    boxes: A `Tensor` of type `float32` or \'bfloat16\'.\n      A 3-D tensor of shape `[batch, num_boxes, 4]`. The boxes are specified in\n      normalized coordinates and are of the form `[y1, x1, y2, x2]`. A\n      normalized coordinate value of `y` is mapped to the image coordinate at\n      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image\n      height is mapped to `[0, image_height - 1] in image height coordinates.\n      We do allow y1 > y2, in which case the sampled crop is an up-down flipped\n      version of the original image. The width dimension is treated similarly.\n      Normalized coordinates outside the `[0, 1]` range are allowed, in which\n      case we use `extrapolation_value` to extrapolate the input image values.\n    crop_size: A list of two integers `[crop_height, crop_width]`. All\n      cropped image patches are resized to this size. The aspect ratio of the\n      image content is not preserved. Both `crop_height` and `crop_width` need\n      to be positive.\n    scope: A name for the operation (optional).\n\n  Returns:\n    A 5-D tensor of shape `[batch, num_boxes, crop_height, crop_width, depth]`\n\n  Raises:\n    ValueError: if image tensor does not have shape\n      `[batch, image_height, image_width, depth]` and all dimensions statically\n      defined.\n    ValueError: if boxes tensor does not have shape `[batch, num_boxes, 4]`\n      where num_boxes > 0.\n    ValueError: if crop_size is not a list of two positive integers\n  """"""\n  img_shape = image.shape.as_list()\n  boxes_shape = boxes.shape.as_list()\n  _, img_height, img_width, _ = img_shape\n  if not isinstance(crop_size, list) or len(crop_size) != 2:\n    raise ValueError(\'`crop_size` must be a list of length 2\')\n  dimensions = img_shape + crop_size + boxes_shape\n  if not all([isinstance(dim, int) for dim in dimensions]):\n    raise ValueError(\'all input shapes must be statically defined\')\n  if len(boxes_shape) != 3 or boxes_shape[2] != 4:\n    raise ValueError(\'`boxes` should have shape `[batch, num_boxes, 4]`\')\n  if len(img_shape) != 4:\n    raise ValueError(\'image should have shape \'\n                     \'`[batch, image_height, image_width, depth]`\')\n  num_crops = boxes_shape[0]\n  if not num_crops > 0:\n    raise ValueError(\'number of boxes must be > 0\')\n  if not (crop_size[0] > 0 and crop_size[1] > 0):\n    raise ValueError(\'`crop_size` must be a list of two positive integers.\')\n\n  def _lin_space_weights(num, img_size):\n    if num > 1:\n      start_weights = tf.linspace(img_size - 1.0, 0.0, num)\n      stop_weights = img_size - 1 - start_weights\n    else:\n      start_weights = tf.constant(num * [.5 * (img_size - 1)], dtype=tf.float32)\n      stop_weights = tf.constant(num * [.5 * (img_size - 1)], dtype=tf.float32)\n    return (start_weights, stop_weights)\n\n  with tf.name_scope(scope, \'MatMulCropAndResize\'):\n    y1_weights, y2_weights = _lin_space_weights(crop_size[0], img_height)\n    x1_weights, x2_weights = _lin_space_weights(crop_size[1], img_width)\n    y1_weights = tf.cast(y1_weights, boxes.dtype)\n    y2_weights = tf.cast(y2_weights, boxes.dtype)\n    x1_weights = tf.cast(x1_weights, boxes.dtype)\n    x2_weights = tf.cast(x2_weights, boxes.dtype)\n    [y1, x1, y2, x2] = tf.unstack(boxes, axis=2)\n\n    # Pixel centers of input image and grid points along height and width\n    image_idx_h = tf.constant(\n        np.reshape(np.arange(img_height), (1, 1, 1, img_height)),\n        dtype=boxes.dtype)\n    image_idx_w = tf.constant(\n        np.reshape(np.arange(img_width), (1, 1, 1, img_width)),\n        dtype=boxes.dtype)\n    grid_pos_h = tf.expand_dims(\n        tf.einsum(\'ab,c->abc\', y1, y1_weights) + tf.einsum(\n            \'ab,c->abc\', y2, y2_weights),\n        axis=3)\n    grid_pos_w = tf.expand_dims(\n        tf.einsum(\'ab,c->abc\', x1, x1_weights) + tf.einsum(\n            \'ab,c->abc\', x2, x2_weights),\n        axis=3)\n\n    # Create kernel matrices of pairwise kernel evaluations between pixel\n    # centers of image and grid points.\n    kernel_h = tf.nn.relu(1 - tf.abs(image_idx_h - grid_pos_h))\n    kernel_w = tf.nn.relu(1 - tf.abs(image_idx_w - grid_pos_w))\n\n    # Compute matrix multiplication between the spatial dimensions of the image\n    # and height-wise kernel using einsum.\n    intermediate_image = tf.einsum(\'abci,aiop->abcop\', kernel_h, image)\n    # Compute matrix multiplication between the spatial dimensions of the\n    # intermediate_image and width-wise kernel using einsum.\n    return tf.einsum(\'abno,abcop->abcnp\', kernel_w, intermediate_image)\n\n\ndef native_crop_and_resize(image, boxes, crop_size, scope=None):\n  """"""Same as `matmul_crop_and_resize` but uses tf.image.crop_and_resize.""""""\n  def get_box_inds(proposals):\n    proposals_shape = proposals.get_shape().as_list()\n    if any(dim is None for dim in proposals_shape):\n      proposals_shape = tf.shape(proposals)\n    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)\n    multiplier = tf.expand_dims(\n        tf.range(start=0, limit=proposals_shape[0]), 1)\n    return tf.reshape(ones_mat * multiplier, [-1])\n\n  with tf.name_scope(scope, \'CropAndResize\'):\n    cropped_regions = tf.image.crop_and_resize(\n        image, tf.reshape(boxes, [-1] + boxes.shape.as_list()[2:]),\n        get_box_inds(boxes), crop_size)\n    final_shape = tf.concat([tf.shape(boxes)[:2],\n                             tf.shape(cropped_regions)[1:]], axis=0)\n    return tf.reshape(cropped_regions, final_shape)\n\n\ndef expected_classification_loss_under_sampling(\n    batch_cls_targets, cls_losses, unmatched_cls_losses,\n    desired_negative_sampling_ratio, min_num_negative_samples):\n  """"""Computes classification loss by background/foreground weighting.\n\n  The weighting is such that the effective background/foreground weight ratio\n  is the desired_negative_sampling_ratio. if p_i is the foreground probability\n  of anchor a_i, L(a_i) is the anchors loss, N is the number of anchors, M\n  is the sum of foreground probabilities across anchors, and K is the desired\n  ratio between the number of negative and positive samples, then the total loss\n  L is calculated as:\n\n  beta = K*M/(N-M)\n  L = sum_{i=1}^N [p_i * L_p(a_i) + beta * (1 - p_i) * L_n(a_i)]\n  where L_p(a_i) is the loss against target assuming the anchor was matched,\n  otherwise zero, and L_n(a_i) is the loss against the background target\n  assuming the anchor was unmatched, otherwise zero.\n\n  Args:\n    batch_cls_targets: A tensor with shape [batch_size, num_anchors, num_classes\n      + 1], where 0\'th index is the background class, containing the class\n      distrubution for the target assigned to a given anchor.\n    cls_losses: Float tensor of shape [batch_size, num_anchors] representing\n      anchorwise classification losses.\n    unmatched_cls_losses: loss for each anchor against the unmatched class\n      target.\n    desired_negative_sampling_ratio: The desired background/foreground weight\n      ratio.\n    min_num_negative_samples: Minimum number of effective negative samples.\n      Used only when there are no positive examples.\n\n  Returns:\n    The classification loss.\n  """"""\n  num_anchors = tf.cast(tf.shape(batch_cls_targets)[1], tf.float32)\n\n  # find the p_i\n  foreground_probabilities = 1 - batch_cls_targets[:, :, 0]\n\n  foreground_sum = tf.reduce_sum(foreground_probabilities, axis=-1)\n\n  # for each anchor, expected_j is the expected number of positive anchors\n  # given that this anchor was sampled as negative.\n  tiled_foreground_sum = tf.tile(\n      tf.reshape(foreground_sum, [-1, 1]),\n      [1, tf.cast(num_anchors, tf.int32)])\n  expected_j = tiled_foreground_sum - foreground_probabilities\n\n  k = desired_negative_sampling_ratio\n\n  # compute beta\n  expected_negatives = tf.to_float(num_anchors) - expected_j\n  desired_negatives = k * expected_j\n  desired_negatives = tf.where(\n      tf.greater(desired_negatives, expected_negatives), expected_negatives,\n      desired_negatives)\n\n  # probability that an anchor is sampled for the loss computation given that it\n  # is negative.\n  beta = desired_negatives / expected_negatives\n\n  # where the foreground sum is zero, use a minimum negative weight.\n  min_negative_weight = 1.0 * min_num_negative_samples / num_anchors\n  beta = tf.where(\n      tf.equal(tiled_foreground_sum, 0),\n      min_negative_weight * tf.ones_like(beta), beta)\n\n  foreground_weights = foreground_probabilities\n  background_weights = (1 - foreground_weights) * beta\n\n  weighted_foreground_losses = foreground_weights * cls_losses\n  weighted_background_losses = background_weights * unmatched_cls_losses\n\n  cls_losses = tf.reduce_sum(\n      weighted_foreground_losses, axis=-1) + tf.reduce_sum(\n          weighted_background_losses, axis=-1)\n\n  return cls_losses\n'"
custom_vehicle_training/object_detection/utils/vrd_evaluation_test.py,4,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tensorflow_models.object_detection.utils.vrd_evaluation.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom object_detection.core import standard_fields\nfrom object_detection.utils import vrd_evaluation\n\n\nclass VRDRelationDetectionEvaluatorTest(tf.test.TestCase):\n\n  def test_vrdrelation_evaluator(self):\n    self.vrd_eval = vrd_evaluation.VRDRelationDetectionEvaluator()\n\n    image_key1 = \'img1\'\n    groundtruth_box_tuples1 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples1 = np.array(\n        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    groundtruth_verified_labels1 = np.array([1, 2, 3, 4, 5], dtype=int)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key1, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples1,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples1,\n            standard_fields.InputDataFields.groundtruth_image_classes:\n                groundtruth_verified_labels1\n        })\n\n    image_key2 = \'img2\'\n    groundtruth_box_tuples2 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples2 = np.array(\n        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key2, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples2,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples2,\n        })\n\n    image_key3 = \'img3\'\n    groundtruth_box_tuples3 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples3 = np.array(\n        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key3, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples3,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples3,\n        })\n\n    image_key = \'img1\'\n    detected_box_tuples = np.array(\n        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2]),\n         ([0.5, 0, 1, 1], [1, 1, 3, 3])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    detected_class_tuples = np.array(\n        [(1, 2, 5), (1, 2, 3), (1, 6, 3)], dtype=vrd_evaluation.label_data_type)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    self.vrd_eval.add_single_detected_image_info(\n        image_key, {\n            standard_fields.DetectionResultFields.detection_boxes:\n                detected_box_tuples,\n            standard_fields.DetectionResultFields.detection_scores:\n                detected_scores,\n            standard_fields.DetectionResultFields.detection_classes:\n                detected_class_tuples\n        })\n\n    metrics = self.vrd_eval.evaluate()\n\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_weightedAP@0.5IOU\'],\n                           0.25)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_mAP@0.5IOU\'],\n                           0.1666666666666666)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_AP@0.5IOU/3\'],\n                           0.3333333333333333)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_AP@0.5IOU/4\'], 0)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_Recall@50@0.5IOU\'],\n                           0.25)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Relationships_Recall@100@0.5IOU\'],\n                           0.25)\n    self.vrd_eval.clear()\n    self.assertFalse(self.vrd_eval._image_ids)\n\n\nclass VRDPhraseDetectionEvaluatorTest(tf.test.TestCase):\n\n  def test_vrdphrase_evaluator(self):\n    self.vrd_eval = vrd_evaluation.VRDPhraseDetectionEvaluator()\n\n    image_key1 = \'img1\'\n    groundtruth_box_tuples1 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples1 = np.array(\n        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    groundtruth_verified_labels1 = np.array([1, 2, 3, 4, 5], dtype=int)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key1, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples1,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples1,\n            standard_fields.InputDataFields.groundtruth_image_classes:\n                groundtruth_verified_labels1\n        })\n\n    image_key2 = \'img2\'\n    groundtruth_box_tuples2 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples2 = np.array(\n        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key2, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples2,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples2,\n        })\n\n    image_key3 = \'img3\'\n    groundtruth_box_tuples3 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples3 = np.array(\n        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key3, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                groundtruth_box_tuples3,\n            standard_fields.InputDataFields.groundtruth_classes:\n                groundtruth_class_tuples3,\n        })\n\n    image_key = \'img1\'\n    detected_box_tuples = np.array(\n        [([0, 0.3, 0.5, 0.5], [0.3, 0.3, 1.0, 1.0]),\n         ([0, 0, 1.2, 1.2], [0.0, 0.0, 2.0, 2.0]),\n         ([0.5, 0, 1, 1], [1, 1, 3, 3])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    detected_class_tuples = np.array(\n        [(1, 2, 5), (1, 2, 3), (1, 6, 3)], dtype=vrd_evaluation.label_data_type)\n    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)\n    self.vrd_eval.add_single_detected_image_info(\n        image_key, {\n            standard_fields.DetectionResultFields.detection_boxes:\n                detected_box_tuples,\n            standard_fields.DetectionResultFields.detection_scores:\n                detected_scores,\n            standard_fields.DetectionResultFields.detection_classes:\n                detected_class_tuples\n        })\n\n    metrics = self.vrd_eval.evaluate()\n\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_weightedAP@0.5IOU\'], 0.25)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_mAP@0.5IOU\'],\n                           0.1666666666666666)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_AP@0.5IOU/3\'],\n                           0.3333333333333333)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_AP@0.5IOU/4\'], 0)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_Recall@50@0.5IOU\'], 0.25)\n    self.assertAlmostEqual(metrics[\'VRDMetric_Phrases_Recall@100@0.5IOU\'], 0.25)\n\n    self.vrd_eval.clear()\n    self.assertFalse(self.vrd_eval._image_ids)\n\n\nclass VRDDetectionEvaluationTest(tf.test.TestCase):\n\n  def setUp(self):\n\n    self.vrd_eval = vrd_evaluation._VRDDetectionEvaluation(\n        matching_iou_threshold=0.5)\n\n    image_key1 = \'img1\'\n    groundtruth_box_tuples1 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2]), ([0, 0, 1, 1], [1, 2, 2, 3])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples1 = np.array(\n        [(1, 2, 3), (1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key1, groundtruth_box_tuples1, groundtruth_class_tuples1)\n\n    image_key2 = \'img2\'\n    groundtruth_box_tuples2 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples2 = np.array(\n        [(1, 4, 3)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key2, groundtruth_box_tuples2, groundtruth_class_tuples2)\n\n    image_key3 = \'img3\'\n    groundtruth_box_tuples3 = np.array(\n        [([0, 0, 1, 1], [1, 1, 2, 2])], dtype=vrd_evaluation.vrd_box_data_type)\n    groundtruth_class_tuples3 = np.array(\n        [(1, 2, 4)], dtype=vrd_evaluation.label_data_type)\n    self.vrd_eval.add_single_ground_truth_image_info(\n        image_key3, groundtruth_box_tuples3, groundtruth_class_tuples3)\n\n    image_key = \'img1\'\n    detected_box_tuples = np.array(\n        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2])],\n        dtype=vrd_evaluation.vrd_box_data_type)\n    detected_class_tuples = np.array(\n        [(1, 2, 3), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)\n    detected_scores = np.array([0.7, 0.8], dtype=float)\n    self.vrd_eval.add_single_detected_image_info(\n        image_key, detected_box_tuples, detected_scores, detected_class_tuples)\n\n    metrics = self.vrd_eval.evaluate()\n    expected_weighted_average_precision = 0.25\n    expected_mean_average_precision = 0.16666666666666\n    expected_precision = np.array([1., 0.5], dtype=float)\n    expected_recall = np.array([0.25, 0.25], dtype=float)\n    expected_recall_50 = 0.25\n    expected_recall_100 = 0.25\n    expected_median_rank_50 = 0\n    expected_median_rank_100 = 0\n\n    self.assertAlmostEqual(expected_weighted_average_precision,\n                           metrics.weighted_average_precision)\n    self.assertAlmostEqual(expected_mean_average_precision,\n                           metrics.mean_average_precision)\n    self.assertAlmostEqual(expected_mean_average_precision,\n                           metrics.mean_average_precision)\n\n    self.assertAllClose(expected_precision, metrics.precisions)\n    self.assertAllClose(expected_recall, metrics.recalls)\n    self.assertAlmostEqual(expected_recall_50, metrics.recall_50)\n    self.assertAlmostEqual(expected_recall_100, metrics.recall_100)\n    self.assertAlmostEqual(expected_median_rank_50, metrics.median_rank_50)\n    self.assertAlmostEqual(expected_median_rank_100, metrics.median_rank_100)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
