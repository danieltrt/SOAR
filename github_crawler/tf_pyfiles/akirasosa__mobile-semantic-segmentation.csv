file_path,api_count,code
config.py,0,"b""IMG_DIR = 'data/raw'\n\n"""
coreml_converter.py,1,"b'import re\n\nimport onnx\nimport torch\nfrom onnx import onnx_pb\nfrom onnx_coreml import convert\nfrom onnx_tf.backend import prepare\n\nfrom nets.ImgWrapNet import ImgWrapNet\n\n# %%\nIMG_SIZE = 224\n\nTMP_ONNX = \'tmp/tmp.onnx\'\nWEIGHT_PATH = \'outputs/train_unet/0-best.pth\'\nML_MODEL = re.sub(\'\\.pth$\', \'.mlmodel\', WEIGHT_PATH)\nTF_MODEL = re.sub(\'\\.pth$\', \'.pb\', WEIGHT_PATH)\n\n# %%\n# Convert to ONNX once\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nmodel = ImgWrapNet(torch.load(WEIGHT_PATH, map_location=\'cpu\'))\nmodel.to(device)\n\ntorch.onnx.export(model,\n                  torch.randn(1, 3, IMG_SIZE, IMG_SIZE),\n                  TMP_ONNX)\n\n# %%\n# Print out ONNX model to confirm the number of output layer\nonnx_model = onnx.load(TMP_ONNX)\nprint(onnx_model)\n\n# %%\n# Convert ONNX to CoreML model\nmodel_file = open(TMP_ONNX, \'rb\')\nmodel_proto = onnx_pb.ModelProto()\nmodel_proto.ParseFromString(model_file.read())\n# 595 is the identifier of output.\ncoreml_model = convert(model_proto,\n                       image_input_names=[\'0\'],\n                       image_output_names=[\'595\'])\ncoreml_model.save(ML_MODEL)\n\n# %%\n# tf_rep = prepare(onnx_model)  # prepare tf representation\n# tf_rep.export_graph(TF_MODEL)  # export the model\n'"
dataset.py,0,"b""import random\nimport re\nfrom glob import glob\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nfrom config import IMG_DIR\n\n\ndef _mask_to_img(mask_file):\n    img_file = re.sub('^{}/masks'.format(IMG_DIR), '{}/images'.format(IMG_DIR), mask_file)\n    img_file = re.sub('\\.ppm$', '.jpg', img_file)\n    return img_file\n\n\ndef _img_to_mask(img_file):\n    mask_file = re.sub('^{}/images'.format(IMG_DIR), '{}/masks'.format(IMG_DIR), img_file)\n    mask_file = re.sub('\\.jpg$', '.ppm', mask_file)\n    return mask_file\n\n\ndef get_img_files():\n    mask_files = sorted(glob('{}/masks/*.ppm'.format(IMG_DIR)))\n    return np.array([_mask_to_img(f) for f in mask_files])\n\n\nclass MaskDataset(Dataset):\n    def __init__(self, img_files, transform, mask_transform=None, mask_axis=0):\n        self.img_files = img_files\n        self.mask_files = [_img_to_mask(f) for f in img_files]\n        self.transform = transform\n        if mask_transform is None:\n            self.mask_transform = transform\n        else:\n            self.mask_transform = mask_transform\n        self.mask_axis = mask_axis\n\n    def __getitem__(self, idx):\n        img = cv2.imread(self.img_files[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        mask = cv2.imread(self.mask_files[idx])\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n        mask = mask[:, :, self.mask_axis]\n\n        seed = random.randint(0, 2 ** 32)\n\n        # Apply transform to img\n        random.seed(seed)\n        img = Image.fromarray(img)\n        img = self.transform(img)\n\n        # Apply same transform to mask\n        random.seed(seed)\n        mask = Image.fromarray(mask)\n        mask = self.mask_transform(mask)\n\n        return img, mask\n\n    def __len__(self):\n        return len(self.img_files)\n\n\nif __name__ == '__main__':\n    pass\n    #\n    # mask = cv2.imread('{}/masks/Aaron_Peirsol_0001.ppm'.format(IMG_DIR))\n    # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n    # mask = mask[:, :, 0]\n    # print(mask.shape)\n    # plt.imshow(mask)\n    # plt.show()\n"""
eval_unet.py,0,"b'import logging\nimport os\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, Resize, ToTensor\n\nfrom dataset import MaskDataset, get_img_files\nfrom nets.MobileNetV2_unet import MobileNetV2_unet\n\nnp.random.seed(1)\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(1)\n\n# %%\nN_CV = 5\nIMG_SIZE = 224\n\nRANDOM_STATE = 1\n\nEXPERIMENT = \'train_unet\'\nOUT_DIR = \'outputs/{}\'.format(EXPERIMENT)\n\n\n# %%\ndef get_data_loaders(val_files):\n    val_transform = Compose([\n        Resize((IMG_SIZE, IMG_SIZE)),\n        ToTensor(),\n    ])\n\n    val_loader = DataLoader(MaskDataset(val_files, val_transform),\n                            batch_size=1,\n                            shuffle=False,\n                            pin_memory=True,\n                            num_workers=4)\n\n    return val_loader\n\n\ndef evaluate():\n    img_size = (IMG_SIZE, IMG_SIZE)\n    n_shown = 0\n\n    image_files = get_img_files()\n    kf = KFold(n_splits=N_CV, random_state=RANDOM_STATE, shuffle=True)\n\n    device = torch.device(""cuda:1"" if torch.cuda.is_available() else ""cpu"")\n\n    for n, (train_idx, val_idx) in enumerate(kf.split(image_files)):\n        val_files = image_files[val_idx]\n        data_loader = get_data_loaders(val_files)\n\n        model = MobileNetV2_unet()\n        model.load_state_dict(torch.load(\'{}/{}-best.pth\'.format(OUT_DIR, n)))\n        model.to(device)\n        model.eval()\n\n        with torch.no_grad():\n            for inputs, labels in data_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs)\n\n                for i, l, o in zip(inputs, labels, outputs):\n                    i = i.cpu().numpy().transpose((1, 2, 0)) * 255\n                    l = l.cpu().numpy().reshape(*img_size) * 255\n                    o = o.cpu().numpy().reshape(int(IMG_SIZE / 2), int(IMG_SIZE / 2)) * 255\n\n                    i = cv2.resize(i.astype(np.uint8), img_size)\n                    l = cv2.resize(l.astype(np.uint8), img_size)\n                    o = cv2.resize(o.astype(np.uint8), img_size)\n\n                    plt.subplot(131)\n                    plt.imshow(i)\n                    plt.subplot(132)\n                    plt.imshow(l)\n                    plt.subplot(133)\n                    plt.imshow(o)\n                    plt.show()\n                    n_shown += 1\n                    if n_shown > 10:\n                        return\n\n\nif __name__ == \'__main__\':\n    if not os.path.exists(OUT_DIR):\n        os.makedirs(OUT_DIR)\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n    if not logger.hasHandlers():\n        logger.addHandler(logging.FileHandler(filename=""outputs/{}.log"".format(EXPERIMENT)))\n\n    evaluate()\n'"
loss.py,0,"b""from torch.nn.functional import interpolate\n\n\ndef dice_loss(scale=None):\n    def fn(input, target):\n        smooth = 1.\n\n        if scale is not None:\n            scaled = interpolate(input, scale_factor=scale, mode='bilinear', align_corners=False)\n            iflat = scaled.view(-1)\n        else:\n            iflat = input.view(-1)\n\n        tflat = target.view(-1)\n        intersection = (iflat * tflat).sum()\n\n        return 1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n\n    return fn\n"""
train_unet.py,0,"b'import argparse\nimport logging\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import KFold\nfrom tensorboardX import SummaryWriter\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomRotation, RandomHorizontalFlip, ToTensor, \\\n    Resize, RandomAffine, ColorJitter\n\nfrom dataset import MaskDataset, get_img_files\nfrom loss import dice_loss\nfrom nets.MobileNetV2_unet import MobileNetV2_unet\nfrom trainer import Trainer\n\nnp.random.seed(1)\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(1)\n\n# %%\nN_CV = 5\nBATCH_SIZE = 8\nLR = 1e-4\n\nN_EPOCHS = 100\nIMG_SIZE = 224\nRANDOM_STATE = 1\n\nEXPERIMENT = \'train_unet\'\nOUT_DIR = \'outputs/{}\'.format(EXPERIMENT)\n\n\n# %%\ndef get_data_loaders(train_files, val_files, img_size=224):\n    train_transform = Compose([\n        ColorJitter(0.3, 0.3, 0.3, 0.3),\n        RandomResizedCrop(img_size, scale=(0.8, 1.2)),\n        RandomAffine(10.),\n        RandomRotation(13.),\n        RandomHorizontalFlip(),\n        ToTensor(),\n    ])\n    # train_mask_transform = Compose([\n    #     RandomResizedCrop(img_size, scale=(0.8, 1.2)),\n    #     RandomAffine(10.),\n    #     RandomRotation(13.),\n    #     RandomHorizontalFlip(),\n    #     ToTensor(),\n    # ])\n    val_transform = Compose([\n        Resize((img_size, img_size)),\n        ToTensor(),\n    ])\n\n    train_loader = DataLoader(MaskDataset(train_files, train_transform),\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              pin_memory=True,\n                              num_workers=4)\n    val_loader = DataLoader(MaskDataset(val_files, val_transform),\n                            batch_size=BATCH_SIZE,\n                            shuffle=False,\n                            pin_memory=True,\n                            num_workers=4)\n\n    return train_loader, val_loader\n\n\ndef save_best_model(cv, model, df_hist):\n    if df_hist[\'val_loss\'].tail(1).iloc[0] <= df_hist[\'val_loss\'].min():\n        torch.save(model.state_dict(), \'{}/{}-best.pth\'.format(OUT_DIR, cv))\n\n\ndef write_on_board(writer, df_hist):\n    row = df_hist.tail(1).iloc[0]\n\n    writer.add_scalars(\'{}/loss\'.format(EXPERIMENT), {\n        \'train\': row.train_loss,\n        \'val\': row.val_loss,\n    }, row.epoch)\n\n\ndef log_hist(df_hist):\n    last = df_hist.tail(1)\n    best = df_hist.sort_values(\'val_loss\').head(1)\n    summary = pd.concat((last, best)).reset_index(drop=True)\n    summary[\'name\'] = [\'Last\', \'Best\']\n    logger.debug(summary[[\'name\', \'epoch\', \'train_loss\', \'val_loss\']])\n    logger.debug(\'\')\n\n\ndef run_cv(img_size, pre_trained):\n    image_files = get_img_files()\n    kf = KFold(n_splits=N_CV, random_state=RANDOM_STATE, shuffle=True)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    for n, (train_idx, val_idx) in enumerate(kf.split(image_files)):\n        train_files = image_files[train_idx]\n        val_files = image_files[val_idx]\n\n        writer = SummaryWriter()\n\n        def on_after_epoch(m, df_hist):\n            save_best_model(n, m, df_hist)\n            write_on_board(writer, df_hist)\n            log_hist(df_hist)\n\n        criterion = dice_loss(scale=2)\n        data_loaders = get_data_loaders(train_files, val_files, img_size)\n        trainer = Trainer(data_loaders, criterion, device, on_after_epoch)\n\n        model = MobileNetV2_unet(pre_trained=pre_trained)\n        model.to(device)\n        optimizer = Adam(model.parameters(), lr=LR)\n\n        hist = trainer.train(model, optimizer, num_epochs=N_EPOCHS)\n        hist.to_csv(\'{}/{}-hist.csv\'.format(OUT_DIR, n), index=False)\n\n        writer.close()\n\n        break\n\n\nif __name__ == \'__main__\':\n    if not os.path.exists(OUT_DIR):\n        os.makedirs(OUT_DIR)\n\n    logger = logging.getLogger(""logger"")\n    logger.setLevel(logging.DEBUG)\n    if not logger.hasHandlers():\n        logger.addHandler(logging.FileHandler(filename=""outputs/{}.log"".format(EXPERIMENT)))\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--img_size\',\n        type=int,\n        default=224,\n        help=\'image size\',\n    )\n    parser.add_argument(\n        \'--pre_trained\',\n        type=str,\n        help=\'path of pre trained weight\',\n    )\n    args, _ = parser.parse_known_args()\n    print(args)\n    # run_cv(**vars(args))\n\n'"
trainer.py,0,"b""import pandas as pd\nimport torch\n\n\nclass Trainer:\n    def __init__(self, data_loaders, criterion, device, on_after_epoch=None):\n        self.data_loaders = data_loaders\n        self.criterion = criterion\n        self.device = device\n        self.history = []\n        self.on_after_epoch = on_after_epoch\n\n    def train(self, model, optimizer, num_epochs):\n        for epoch in range(num_epochs):\n            train_epoch_loss = self._train_on_epoch(model, optimizer)\n            val_epoch_loss = self._val_on_epoch(model, optimizer)\n\n            hist = {\n                'epoch': epoch,\n                'train_loss': train_epoch_loss,\n                'val_loss': val_epoch_loss,\n            }\n            self.history.append(hist)\n\n            if self.on_after_epoch is not None:\n                self.on_after_epoch(model, pd.DataFrame(self.history))\n\n        return pd.DataFrame(self.history)\n\n    def _train_on_epoch(self, model, optimizer):\n        model.train()\n        data_loader = self.data_loaders[0]\n        running_loss = 0.0\n\n        for inputs, labels in data_loader:\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n\n            optimizer.zero_grad()\n\n            with torch.set_grad_enabled(True):\n                outputs = model(inputs)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(data_loader.dataset)\n\n        return epoch_loss\n\n    def _val_on_epoch(self, model, optimizer):\n        model.eval()\n        data_loader = self.data_loaders[1]\n        running_loss = 0.0\n\n        for inputs, labels in data_loader:\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n\n            optimizer.zero_grad()\n\n            with torch.set_grad_enabled(False):\n                outputs = model(inputs)\n                loss = self.criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n\n        epoch_loss = running_loss / len(data_loader.dataset)\n\n        return epoch_loss\n"""
nets/ImgWrapNet.py,0,"b""import torch\nimport torch.nn as nn\n\nfrom nets.MobileNetV2_unet import MobileNetV2_unet\n\n\ndef _init_unet(state_dict):\n    unet = MobileNetV2_unet(pre_trained=None)\n    unet.load_state_dict(state_dict)\n    return unet\n\n\nclass ImgWrapNet(nn.Module):\n    def __init__(self, state_dict, scale=255.):\n        super().__init__()\n        self.scale = scale\n        self.unet = _init_unet(state_dict)\n\n    def forward(self, x):\n        x = x / self.scale\n        x = self.unet(x)\n        x = x * self.scale\n        x = torch.cat((x, x, x), dim=1)\n        return x\n\n\nif __name__ == '__main__':\n    WEIGHT_PATH = 'outputs/train_unet/0-best.pth'\n    net = ImgWrapNet(torch.load(WEIGHT_PATH, map_location='cpu'))\n    net(torch.randn(1, 3, 224, 224))\n"""
nets/MobileNetV2.py,0,"b'import math\n\nimport torch.nn as nn\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n'"
nets/MobileNetV2_unet.py,0,"b""import logging\nimport math\nimport sys\n\nimport torch\nimport torch.nn as nn\n\nfrom nets.MobileNetV2 import MobileNetV2, InvertedResidual\n\n\nclass MobileNetV2_unet(nn.Module):\n    def __init__(self, pre_trained='weights/mobilenet_v2.pth.tar'):\n        super(MobileNetV2_unet, self).__init__()\n\n        self.backbone = MobileNetV2()\n\n        self.dconv1 = nn.ConvTranspose2d(1280, 96, 4, padding=1, stride=2)\n        self.invres1 = InvertedResidual(192, 96, 1, 6)\n\n        self.dconv2 = nn.ConvTranspose2d(96, 32, 4, padding=1, stride=2)\n        self.invres2 = InvertedResidual(64, 32, 1, 6)\n\n        self.dconv3 = nn.ConvTranspose2d(32, 24, 4, padding=1, stride=2)\n        self.invres3 = InvertedResidual(48, 24, 1, 6)\n\n        self.dconv4 = nn.ConvTranspose2d(24, 16, 4, padding=1, stride=2)\n        self.invres4 = InvertedResidual(32, 16, 1, 6)\n\n        self.conv_last = nn.Conv2d(16, 3, 1)\n\n        self.conv_score = nn.Conv2d(3, 1, 1)\n\n        self._init_weights()\n\n        if pre_trained is not None:\n            self.backbone.load_state_dict(torch.load(pre_trained))\n\n    def forward(self, x):\n        for n in range(0, 2):\n            x = self.backbone.features[n](x)\n        x1 = x\n        logging.debug((x1.shape, 'x1'))\n\n        for n in range(2, 4):\n            x = self.backbone.features[n](x)\n        x2 = x\n        logging.debug((x2.shape, 'x2'))\n\n        for n in range(4, 7):\n            x = self.backbone.features[n](x)\n        x3 = x\n        logging.debug((x3.shape, 'x3'))\n\n        for n in range(7, 14):\n            x = self.backbone.features[n](x)\n        x4 = x\n        logging.debug((x4.shape, 'x4'))\n\n        for n in range(14, 19):\n            x = self.backbone.features[n](x)\n        x5 = x\n        logging.debug((x5.shape, 'x5'))\n\n        up1 = torch.cat([\n            x4,\n            self.dconv1(x)\n        ], dim=1)\n        up1 = self.invres1(up1)\n        logging.debug((up1.shape, 'up1'))\n\n        up2 = torch.cat([\n            x3,\n            self.dconv2(up1)\n        ], dim=1)\n        up2 = self.invres2(up2)\n        logging.debug((up2.shape, 'up2'))\n\n        up3 = torch.cat([\n            x2,\n            self.dconv3(up2)\n        ], dim=1)\n        up3 = self.invres3(up3)\n        logging.debug((up3.shape, 'up3'))\n\n        up4 = torch.cat([\n            x1,\n            self.dconv4(up3)\n        ], dim=1)\n        up4 = self.invres4(up4)\n        logging.debug((up4.shape, 'up4'))\n\n        x = self.conv_last(up4)\n        logging.debug((x.shape, 'conv_last'))\n\n        x = self.conv_score(x)\n        logging.debug((x.shape, 'conv_score'))\n\n        # x = interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        # logging.debug((x.shape, 'interpolate'))\n\n        x = torch.sigmoid(x)\n\n        return x\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nif __name__ == '__main__':\n    # Debug\n    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n    net = MobileNetV2_unet(pre_trained=None)\n    net(torch.randn(1, 3, 224, 224))\n"""
nets/__init__.py,0,b''
