file_path,api_count,code
evaluate.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport sys\nfrom src.evaluator import Evaluator\n\n\ndef main():\n    argv = sys.argv\n    if len(argv) == 3:\n        model_path, data_file = argv[1:]\n        evaluator = Evaluator(model_path, data_file)\n        evaluator.evaluate()\n    else:\n        print(\'Usage: ""python evaluate.py $model_path $data_file""\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport sys\nimport json5\nfrom pprint import pprint\nfrom src.utils import params\nfrom src.trainer import Trainer\n\n\ndef main():\n    argv = sys.argv\n    if len(argv) == 2:\n        arg_groups = params.parse(sys.argv[1])\n        for args, config in arg_groups:\n            trainer = Trainer(args)\n            states = trainer.train()\n            with open(\'models/log.jsonl\', \'a\') as f:\n                f.write(json5.dumps({\n                    \'data\': os.path.basename(args.data_dir),\n                    \'params\': config,\n                    \'state\': states,\n                }))\n                f.write(\'\\n\')\n    elif len(argv) == 3 and \'--dry\' in argv:\n        argv.remove(\'--dry\')\n        arg_groups = params.parse(sys.argv[1])\n        pprint([args.__dict__ for args, _ in arg_groups])\n    else:\n        print(\'Usage: ""python train.py configs/xxx.json5""\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
data/prepare_quora.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom tqdm import tqdm\n\n\nprint(\'processing quora\')\nos.makedirs(\'quora\', exist_ok=True)\n# use the partition on https://zhiguowang.github.io\nfor split in (\'train\', \'dev\', \'test\'):\n    with open(\'orig/Quora_question_pair_partition/{}.tsv\'.format(split)) as f, \\\n            open(\'quora/{}.txt\'.format(split), \'w\') as fout:\n        n_lines = 0\n        for _ in f:\n            n_lines += 1\n        f.seek(0)\n        for line in tqdm(f, total=n_lines, leave=False):\n            elements = line.rstrip().split(\'\\t\')\n            fout.write(\'{}\\t{}\\t{}\\n\'.format(elements[1], elements[2], int(elements[0])))\n'"
data/prepare_scitail.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport re\nimport os\nimport json\nfrom tqdm import tqdm\nfrom nltk.tokenize import TweetTokenizer\n\n\ntokenizer = TweetTokenizer()\nlabel_map = {\n    \'entailment\': 0,\n    \'neutral\': 1,\n    \'contradiction\': 2,\n}\n\n\ndef tokenize(string):\n    string = \' \'.join(tokenizer.tokenize(string))\n    string = re.sub(r""[-.#\\""/]"", "" "", string)\n    string = re.sub(r""\\\'(?!(s|m|ve|t|re|d|ll)( |$))"", "" "", string)\n    string = re.sub(r""\\\'s"", "" \\\'s"", string)\n    string = re.sub(r""\\\'m"", "" \\\'m"", string)\n    string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n    string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n    string = re.sub(r""\\\'re"", "" \\\'re"", string)\n    string = re.sub(r""\\\'d"", "" \\\'d"", string)\n    string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.strip()\n\n\nos.makedirs(\'scitail\', exist_ok=True)\n\n\nfor split in [\'train\', \'dev\', \'test\']:\n    print(\'processing SciTail\', split)\n    with open(\'orig/SciTailV1.1/snli_format/scitail_1.0_{}.txt\'.format(split)) as f, \\\n            open(\'scitail/{}.txt\'.format(split), \'w\', encoding=\'utf8\') as fout:\n        n_lines = 0\n        for _ in f:\n            n_lines += 1\n        f.seek(0)\n        for line in tqdm(f, total=n_lines, desc=split, leave=False):\n            sample = json.loads(line)\n            sentence1 = tokenize(sample[\'sentence1\'])\n            sentence2 = tokenize(sample[\'sentence2\'])\n            label = sample[""gold_label""]\n            assert label in label_map\n            label = label_map[label]\n            fout.write(\'{}\\t{}\\t{}\\n\'.format(sentence1, sentence2, label))\n'"
data/prepare_snli.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport json\nimport string\nimport numpy as np\nimport msgpack\nfrom collections import Counter\n\nin_dir = \'orig/SNLI\'\nout_dir = \'../models/snli/\'\ndata_dir = \'snli\'\nlabel_map = {2: \'0\', 1: \'1\', 0: \'2\'}\n\nos.makedirs(out_dir, exist_ok=True)\nos.makedirs(data_dir, exist_ok=True)\nwith open(os.path.join(in_dir, \'env\')) as f:\n    env = json.load(f)\n\nprint(\'convert embeddings ...\')\nemb = np.load(os.path.join(in_dir, \'emb_glove_300.npy\'))\nprint(len(emb))\nwith open(os.path.join(out_dir, \'embedding.msgpack\'), \'wb\') as f:\n    msgpack.dump(emb.tolist(), f)\n\nprint(\'convert_vocab ...\')\nw2idx = env[\'word_index\']\nprint(len(w2idx))\nidx2w = {i: w for w, i in w2idx.items()}\nwith open(os.path.join(out_dir, \'vocab.txt\'), \'w\') as f:\n    for index in range(len(idx2w)):\n        if index >= 2:\n            f.write(\'{}\\n\'.format(idx2w[index]))\nwith open(os.path.join(out_dir, \'target_map.txt\'), \'w\') as f:\n    for label in (0, 1, 2):\n        f.write(\'{}\\n\'.format(label))\n\n# save data files\npunctuactions = set(string.punctuation)\nfor split in [\'train\', \'dev\', \'test\']:\n    labels = Counter()\n    print(\'convert\', split, \'...\')\n    data = env[split]\n    with open(os.path.join(data_dir, f\'{split}.txt\'), \'w\') as f_out:\n        for sample in data:\n            a, b, label = sample\n            a = a[1:-1]\n            b = b[1:-1]\n            a = [w.lower() for w in a if w and w not in punctuactions]\n            b = [w.lower() for w in b if w and w not in punctuactions]\n            assert all(w in w2idx for w in a) and all(w in w2idx for w in b)\n            a = \' \'.join(a)\n            b = \' \'.join(b)\n            assert len(a) != 0 and len(b) != 0\n            labels.update({label: 1})\n            assert label in label_map\n            label = label_map[label]\n            f_out.write(f\'{a}\\t{b}\\t{label}\\n\')\n    print(\'labels:\', labels)\n'"
data/prepare_wikiqa.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom shutil import copyfile\n\n\ndef copy(src, tgt):\n    copyfile(os.path.abspath(src), os.path.abspath(tgt))\n\n\nos.makedirs(\'wikiqa\', exist_ok=True)\n\n\ncopy(\'orig/WikiQACorpus/WikiQA-dev-filtered.ref\', \'wikiqa/dev.ref\')\ncopy(\'orig/WikiQACorpus/WikiQA-test-filtered.ref\', \'wikiqa/test.ref\')\ncopy(\'orig/WikiQACorpus/emnlp-table/WikiQA.CNN.dev.rank\', \'wikiqa/dev.rank\')\ncopy(\'orig/WikiQACorpus/emnlp-table/WikiQA.CNN.test.rank\', \'wikiqa/test.rank\')\nfor split in [\'train\', \'dev\', \'test\']:\n    print(\'processing WikiQA\', split)\n    copy(\'orig/WikiQACorpus/WikiQA-{}.txt\'.format(split), \'wikiqa/{}.txt\'.format(split))\n'"
src/__init__.py,0,b''
src/evaluator.py,4,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom pprint import pprint\nimport tensorflow as tf\nfrom .model import Model\nfrom .interface import Interface\nfrom .utils.loader import load_data\n\n\nclass Evaluator:\n    def __init__(self, model_path, data_file):\n        self.model_path = model_path\n        self.data_file = data_file\n\n    def evaluate(self):\n        data = load_data(*os.path.split(self.data_file))\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth = True\n            config.allow_soft_placement = True\n            sess = tf.Session(config=config)\n            with sess.as_default():\n                model, checkpoint = Model.load(sess, self.model_path)\n                args = checkpoint[\'args\']\n                interface = Interface(args)\n                batches = interface.pre_process(data, training=False)\n                _, stats = model.evaluate(sess, batches)\n                pprint(stats)\n'"
src/interface.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport random\nimport msgpack\nfrom .utils.vocab import Vocab, Indexer\nfrom .utils.loader import load_data, load_embeddings\n\n\nclass Interface:\n    def __init__(self, args, log=None):\n        self.args = args\n        # build/load vocab and target map\n        vocab_file = os.path.join(args.output_dir, \'vocab.txt\')\n        target_map_file = os.path.join(args.output_dir, \'target_map.txt\')\n        if not os.path.exists(vocab_file):\n            data = load_data(self.args.data_dir)\n            self.target_map = Indexer.build((sample[\'target\'] for sample in data), log=log)\n            self.target_map.save(target_map_file)\n            self.vocab = Vocab.build((word for sample in data\n                                      for text in (sample[\'text1\'], sample[\'text2\'])\n                                      for word in text.split()[:self.args.max_len]),\n                                     lower=args.lower_case, min_df=self.args.min_df, log=log,\n                                     pretrained_embeddings=args.pretrained_embeddings,\n                                     dump_filtered=os.path.join(args.output_dir, \'filtered_words.txt\'))\n            self.vocab.save(vocab_file)\n        else:\n            self.target_map = Indexer.load(target_map_file)\n            self.vocab = Vocab.load(vocab_file)\n        args.num_classes = len(self.target_map)\n        args.num_vocab = len(self.vocab)\n        args.padding = Vocab.pad()\n\n    def load_embeddings(self):\n        """"""generate embeddings suited for the current vocab or load previously cached ones.""""""\n        embedding_file = os.path.join(self.args.output_dir, \'embedding.msgpack\')\n        if not os.path.exists(embedding_file):\n            embeddings = load_embeddings(self.args.pretrained_embeddings, self.vocab,\n                                         self.args.embedding_dim, mode=self.args.embedding_mode,\n                                         lower=self.args.lower_case)\n            with open(embedding_file, \'wb\') as f:\n                msgpack.dump(embeddings, f)\n        else:\n            with open(embedding_file, \'rb\') as f:\n                embeddings = msgpack.load(f)\n        return embeddings\n\n    def pre_process(self, data, training=True):\n        result = [self.process_sample(sample) for sample in data]\n        if training:\n            result = list(filter(lambda x: x[\'len1\'] < self.args.max_len and x[\'len2\'] < self.args.max_len, result))\n            if not self.args.sort_by_len:\n                return result\n            result = sorted(result, key=lambda x: (x[\'len1\'], x[\'len2\'], x[\'text1\']))\n        batch_size = self.args.batch_size\n        return [self.make_batch(result[i:i + batch_size]) for i in range(0, len(data), batch_size)]\n\n    def process_sample(self, sample, with_target=True):\n        text1 = sample[\'text1\']\n        text2 = sample[\'text2\']\n        if self.args.lower_case:\n            text1 = text1.lower()\n            text2 = text2.lower()\n        processed = {\n            \'text1\': [self.vocab.index(w) for w in text1.split()[:self.args.max_len]],\n            \'text2\': [self.vocab.index(w) for w in text2.split()[:self.args.max_len]],\n        }\n        processed[\'len1\'] = len(processed[\'text1\'])\n        processed[\'len2\'] = len(processed[\'text2\'])\n        if \'target\' in sample and with_target:\n            target = sample[\'target\']\n            assert target in self.target_map\n            processed[\'target\'] = self.target_map.index(target)\n        return processed\n\n    def shuffle_batch(self, data):\n        data = random.sample(data, len(data))\n        if self.args.sort_by_len:\n            return data\n        batch_size = self.args.batch_size\n        batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n        return list(map(self.make_batch, batches))\n\n    def make_batch(self, batch, with_target=True):\n        batch = {key: [sample[key] for sample in batch] for key in batch[0].keys()}\n        if \'target\' in batch and not with_target:\n            del batch[\'target\']\n        batch = {key: self.padding(value, min_len=self.args.min_len) if key.startswith(\'text\') else value\n                 for key, value in batch.items()}\n        return batch\n\n    @staticmethod\n    def padding(samples, min_len=1):\n        max_len = max(max(map(len, samples)), min_len)\n        batch = [sample + [Vocab.pad()] * (max_len - len(sample)) for sample in samples]\n        return batch\n\n    def post_process(self, output):\n        final_prediction = []\n        for prob in output:\n            idx = max(range(len(prob)), key=prob.__getitem__)\n            target = self.target_map[idx]\n            final_prediction.append(target)\n        return final_prediction\n'"
src/model.py,52,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport re\nimport sys\nimport random\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.lookup_ops import HashTable\nfrom tensorflow.python.ops.lookup_ops import TextFileIdTableInitializer\nfrom tensorflow.python.client import device_lib\n\nfrom .network import Network\nfrom .utils.vocab import Vocab\nfrom .utils.metrics import registry as metrics\n\n\nclass Model:\n    prefix = \'checkpoint\'\n    best_model_name = \'best\'\n\n    def __init__(self, args, session, updates=None):\n        self.args = args\n        self.sess = session\n\n        # updates\n        if not updates:\n            updates = 0\n        self.updates = updates\n        self.global_step = tf.get_variable(\'global_step\', shape=(), dtype=tf.float32,\n                                           initializer=tf.constant_initializer(updates), trainable=False)\n        self.step = tf.assign_add(self.global_step, 1)\n\n        # placeholders\n        table = HashTable(TextFileIdTableInitializer(filename=os.path.join(args.output_dir, \'vocab.txt\')),\n                          default_value=Vocab.unk())\n        self.q1_string = tf.placeholder(tf.string, [None, None], name=\'q1_str\')\n        self.q2_string = tf.placeholder(tf.string, [None, None], name=\'q2_str\')\n        self.q1 = tf.placeholder_with_default(table.lookup(self.q1_string), [None, None], name=\'q1\')\n        self.q2 = tf.placeholder_with_default(table.lookup(self.q2_string), [None, None], name=\'q2\')\n        self.q1_len = tf.placeholder(tf.int32, [None], name=\'q1_len\')\n        self.q2_len = tf.placeholder(tf.int32, [None], name=\'q2_len\')\n        self.y = tf.placeholder(tf.int32, [None], name=\'y\')\n        self.dropout_keep_prob = tf.placeholder(tf.float32, (), name=\'dropout_keep_prob\')\n\n        q1_mask = tf.expand_dims(tf.sequence_mask(self.q1_len, dtype=tf.float32), dim=-1)\n        q2_mask = tf.expand_dims(tf.sequence_mask(self.q2_len, dtype=tf.float32), dim=-1)\n\n        devices = self.get_available_gpus() or [\'/device:CPU:0\']\n        if not args.multi_gpu:\n            devices = devices[:1]\n        if len(devices) == 1:\n            splits = 1\n        else:\n            splits = [tf.shape(self.q1)[0] // len(devices)] * (len(devices) - 1) + [-1]  # handle uneven split\n        q1 = tf.split(self.q1, splits)\n        q2 = tf.split(self.q2, splits)\n        q1_mask = tf.split(q1_mask, splits)\n        q2_mask = tf.split(q2_mask, splits)\n        y = tf.split(self.y, splits)\n\n        # network\n        self.network = Network(args)\n\n        # optimizer\n        lr = tf.get_variable(\'lr\', shape=(), dtype=tf.float32,\n                             initializer=tf.constant_initializer(args.lr), trainable=False)\n        lr_next = tf.cond(self.global_step < args.lr_warmup_steps,\n                          true_fn=lambda: args.min_lr +\n                                          (args.lr - args.min_lr) / max(1, args.lr_warmup_steps) * self.global_step,\n                          false_fn=lambda: tf.maximum(args.min_lr, args.lr * args.lr_decay_rate ** tf.floor(\n                              (self.global_step - args.lr_warmup_steps) / args.lr_decay_steps)))\n        tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.assign(lr, lr_next, name=\'update_lr\'))\n        self.lr = lr\n        self.opt = tf.train.AdamOptimizer(learning_rate=lr, beta1=args.beta1, beta2=args.beta2)\n\n        # training graph\n        tower_names = [\'tower-{}\'.format(i) for i in range(len(devices))] if len(devices) > 1 else [\'\']\n        tower_logits = []\n        tower_grads = []\n        summaries = []\n        loss = 0\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n            for i, device in enumerate(devices):\n                with tf.device(device):\n                    with tf.name_scope(tower_names[i]) as scope:\n                        logits = self.network(q1[i], q2[i], q1_mask[i], q2_mask[i], self.dropout_keep_prob)\n                        tower_logits.append(logits)\n                        loss = self.get_loss(logits, y[i])\n                        tf.get_variable_scope().reuse_variables()\n                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                        grads = self.opt.compute_gradients(loss)\n                        tower_grads.append(grads)\n        gradients = []\n        variables = []\n        for grad_and_vars in zip(*tower_grads):\n            if grad_and_vars[0][0] is None:\n                msg = \'WARNING: trainable variable {} receives no grad.\\n\'.format(grad_and_vars[0][1].op.name)\n                sys.stderr.write(msg)\n                continue\n            grad = tf.stack([g for g, _ in grad_and_vars])\n            grad = tf.reduce_mean(grad, 0)\n            v = grad_and_vars[0][1]  # use the first tower\'s pointer to the (shared) variable\n            gradients.append(grad)\n            variables.append(v)\n\n        gradients, self.gnorm = tf.clip_by_global_norm(gradients, self.args.grad_clipping)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_op = self.opt.apply_gradients(zip(gradients, variables))\n        logits = tf.concat(tower_logits, 0)\n        self.prob = tf.nn.softmax(logits, dim=1, name=\'prob\')\n        self.pred = tf.argmax(input=logits, axis=1, name=\'pred\')\n        self.loss = tf.identity(loss, name=\'loss\')\n        summaries.append(tf.summary.scalar(\'training/lr\', lr))\n        summaries.append(tf.summary.scalar(\'training/gnorm\', self.gnorm))\n        summaries.append(tf.summary.scalar(\'training/loss\', self.loss))\n\n        # add summary\n        self.summary = tf.summary.merge(summaries)\n\n        # saver\n        self.saver = tf.train.Saver([var for var in tf.global_variables() if \'Adam\' not in var.name],\n                                    max_to_keep=args.max_checkpoints)\n\n    def update(self, sess, batch):\n        feed_dict = self.process_data(batch, training=True)\n        _, gnorm, loss, summary, lr = sess.run(\n            [self.train_op, self.gnorm, self.loss, self.summary, self.lr],\n            feed_dict=feed_dict\n        )\n        assert gnorm >= 0, \'encounter nan in gradients.\'\n        sess.run(self.step)\n        self.updates += 1\n        stats = {\n            \'updates\': self.updates,\n            \'loss\': loss,\n            \'lr\': lr,\n            \'gnorm\': gnorm,\n            \'summary\': summary,\n        }\n        return stats\n\n    def evaluate(self, sess, data):\n        predictions = []\n        targets = []\n        probabilities = []\n        losses = []\n        for batch in data:\n            feed_dict = self.process_data(batch, training=False)\n            loss, pred, prob = sess.run(\n                [self.loss, self.pred, self.prob],\n                feed_dict=feed_dict\n            )\n            losses.append(loss)\n            predictions.extend(pred.tolist())\n            targets.extend(feed_dict[self.y])\n            probabilities.extend(prob.tolist())\n        outputs = {\n            \'target\': targets,\n            \'prob\': probabilities,\n            \'pred\': predictions,\n            \'args\': self.args,\n        }\n        stats = {\n            \'updates\': self.updates,\n            \'loss\': sum(losses[:-1]) / (len(losses) - 1) if len(losses) > 1 else sum(losses),\n        }\n        for metric in self.args.watch_metrics:\n            if metric not in stats:  # multiple metrics could be computed by the same function\n                stats.update(metrics[metric](outputs))\n        assert \'score\' not in stats, \'metric name collides with ""score""\'\n        eval_score = stats[self.args.metric]\n        stats[\'score\'] = eval_score\n        return eval_score, stats  # first value is for early stopping\n\n    def predict(self, sess, batch):\n        feed_dict = self.process_data(batch, training=False)\n        return sess.run(self.prob, feed_dict=feed_dict)\n\n    def process_data(self, batch, training):\n        feed_dict = {\n            self.q1: batch[\'text1\'],\n            self.q2: batch[\'text2\'],\n            self.q1_len: batch[\'len1\'],\n            self.q2_len: batch[\'len2\'],\n            self.dropout_keep_prob: self.args.dropout_keep_prob,\n        }\n        if not training:\n            feed_dict[self.dropout_keep_prob] = 1.\n        if \'target\' in batch:\n            feed_dict[self.y] = batch[\'target\']\n        return feed_dict\n\n    @staticmethod\n    def get_loss(logits, target):\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target)\n        return tf.reduce_mean(losses)\n\n    def save(self, states, name=None):\n        self.saver.save(self.sess, os.path.join(self.args.summary_dir, self.prefix),\n                        global_step=self.updates)\n        if not name:\n            name = str(self.updates)\n        # noinspection PyTypeChecker\n        numpy_state = list(np.random.get_state())\n        numpy_state[1] = numpy_state[1].tolist()\n        params = {\n            \'updates\': self.updates,\n            \'args\': self.args,\n            \'random_state\': random.getstate(),\n            \'numpy_state\': numpy_state,\n        }\n        params.update(states)\n        with open(os.path.join(self.args.summary_dir, \'{}-{}.stat\'.format(self.prefix, name)), \'wb\') as f:\n            pickle.dump(params, f)\n\n    @classmethod\n    def load(cls, sess, model_path):\n        with open(model_path + \'.stat\', \'rb\') as f:\n            checkpoint = pickle.load(f)\n        args = checkpoint[\'args\']\n        args.summary_dir = os.path.dirname(model_path)\n        args.output_dir = os.path.dirname(args.summary_dir)\n        model = cls(args, sess, updates=checkpoint[\'updates\'])\n\n        init_vars = tf.train.list_variables(model_path)\n        model_vars = {re.match(""^(.*):\\\\d+$"", var.name).group(1): var for var in tf.global_variables()}\n        assignment_map = {name: model_vars[name] for name, _ in init_vars if name in model_vars}\n        tf.train.init_from_checkpoint(model_path, assignment_map)\n        sess.run(tf.global_variables_initializer())\n        return model, checkpoint\n\n    @staticmethod\n    def num_parameters(exclude_embed=False):\n        num_params = int(np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()]))\n        if exclude_embed:\n            emb_params = int(np.sum([np.prod(v.shape.as_list())\n                                     for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n                                                                scope=\'embedding\')]))\n            num_params -= emb_params\n        return num_params\n\n    def set_embeddings(self, sess, embeddings):\n        self.network.embedding.set_(sess, embeddings)\n\n    @staticmethod\n    def get_available_gpus():\n        local_device_protos = device_lib.list_local_devices()\n        return [x.name for x in local_device_protos if x.device_type == \'GPU\']\n'"
src/network.py,3,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\nfrom .modules.embedding import Embedding\nfrom .modules.encoder import Encoder\nfrom .modules.alignment import registry as alignment\nfrom .modules.fusion import registry as fusion\nfrom .modules.connection import registry as connection\nfrom .modules.pooling import pooling\nfrom .modules.prediction import registry as prediction\n\n\nclass Network:\n    def __init__(self, args):\n        self.embedding = Embedding(args)\n        self.blocks = [{\n            \'encoder\': Encoder(args),\n            \'alignment\': alignment[args.alignment](args),\n            \'fusion\': fusion[args.fusion](args),\n        } for _ in range(args.blocks)]\n        self.connection = connection[args.connection]\n        self.pooling = pooling\n        self.prediction = prediction[args.prediction](args)\n\n    def __call__(self, a, b, mask_a, mask_b, dropout_keep_prob):\n        a = self.embedding(a, dropout_keep_prob)\n        b = self.embedding(b, dropout_keep_prob)\n        res_a, res_b = a, b\n\n        for i, block in enumerate(self.blocks):\n            with tf.variable_scope(\'block-{}\'.format(i), reuse=tf.AUTO_REUSE):\n                if i > 0:\n                    a = self.connection(a, res_a, i)\n                    b = self.connection(b, res_b, i)\n                    res_a, res_b = a, b\n                a_enc = block[\'encoder\'](a, mask_a, dropout_keep_prob)\n                b_enc = block[\'encoder\'](b, mask_b, dropout_keep_prob)\n                a = tf.concat([a, a_enc], axis=-1)\n                b = tf.concat([b, b_enc], axis=-1)\n                align_a, align_b = block[\'alignment\'](a, b, mask_a, mask_b, dropout_keep_prob)\n                a = block[\'fusion\'](a, align_a, dropout_keep_prob)\n                b = block[\'fusion\'](b, align_b, dropout_keep_prob)\n        a = self.pooling(a, mask_a)\n        b = self.pooling(b, mask_b)\n        return self.prediction(a, b, dropout_keep_prob)\n'"
src/trainer.py,6,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport random\nimport json5\nimport numpy as np\nimport tensorflow as tf\nfrom datetime import datetime\nfrom pprint import pformat\nfrom .utils.loader import load_data\nfrom .utils.logger import Logger\nfrom .utils.params import validate_params\nfrom .model import Model\nfrom .interface import Interface\n\n\nclass Trainer:\n    def __init__(self, args):\n        self.args = args\n        self.log = Logger(self.args)\n\n    def train(self):\n        start_time = datetime.now()\n        train = load_data(self.args.data_dir, \'train\')\n        dev = load_data(self.args.data_dir, self.args.eval_file)\n        self.log(f\'train ({len(train)}) | {self.args.eval_file} ({len(dev)})\')\n\n        tf.reset_default_graph()\n        with tf.Graph().as_default():\n            config = tf.ConfigProto()\n            config.gpu_options.allow_growth = True\n            config.allow_soft_placement = True\n            sess = tf.Session(config=config)\n            with sess.as_default():\n                model, interface, states = self.build_model(sess)\n                train_batches = interface.pre_process(train)\n                dev_batches = interface.pre_process(dev, training=False)\n                self.log(\'setup complete: {}s.\'.format(str(datetime.now() - start_time).split(""."")[0]))\n                try:\n                    for epoch in range(states[\'start_epoch\'], self.args.epochs + 1):\n                        states[\'epoch\'] = epoch\n                        self.log.set_epoch(epoch)\n\n                        batches = interface.shuffle_batch(train_batches)\n                        for batch_id, batch in enumerate(batches):\n                            stats = model.update(sess, batch)\n                            self.log.update(stats)\n                            eval_per_updates = self.args.eval_per_updates \\\n                                if model.updates > self.args.eval_warmup_steps else self.args.eval_per_updates_warmup\n                            if model.updates % eval_per_updates == 0 \\\n                                    or (self.args.eval_epoch and batch_id + 1 == len(batches)):\n                                score, dev_stats = model.evaluate(sess, dev_batches)\n                                if score > states[\'best_eval\']:\n                                    states[\'best_eval\'], states[\'best_epoch\'], states[\'best_step\'] = \\\n                                        score, epoch, model.updates\n                                    if self.args.save:\n                                        model.save(states, name=model.best_model_name)\n                                self.log.log_eval(dev_stats)\n                                if self.args.save_all:\n                                    model.save(states)\n                                    model.save(states, name=\'last\')\n                                if model.updates - states[\'best_step\'] > self.args.early_stopping \\\n                                        and model.updates > self.args.min_steps:\n                                    raise EarlyStop(\'[Tolerance reached. Training is stopped early.]\')\n                            if stats[\'loss\'] > self.args.max_loss:\n                                raise EarlyStop(\'[Loss exceeds tolerance. Unstable training is stopped early.]\')\n                            if stats[\'lr\'] < self.args.min_lr - 1e-6:\n                                raise EarlyStop(\'[Learning rate has decayed below min_lr. Training is stopped early.]\')\n                        self.log.newline()\n                    self.log(\'Training complete.\')\n                except KeyboardInterrupt:\n                    self.log.newline()\n                    self.log(f\'Training interrupted. Stopped early.\')\n                except EarlyStop as e:\n                    self.log.newline()\n                    self.log(str(e))\n                self.log(f\'best dev score {states[""best_eval""]} at step {states[""best_step""]} \'\n                         f\'(epoch {states[""best_epoch""]}).\')\n                self.log(f\'best eval stats [{self.log.best_eval_str}]\')\n                training_time = str(datetime.now() - start_time).split(\'.\')[0]\n                self.log(f\'Training time: {training_time}.\')\n        states[\'start_time\'] = str(start_time).split(\'.\')[0]\n        states[\'training_time\'] = training_time\n        return states\n\n    def build_model(self, sess):\n        states = {}\n        interface = Interface(self.args, self.log)\n        self.log(f\'#classes: {self.args.num_classes}; #vocab: {self.args.num_vocab}\')\n        if self.args.seed:\n            random.seed(self.args.seed)\n            np.random.seed(self.args.seed)\n            tf.set_random_seed(self.args.seed)\n\n        model = Model(self.args, sess)\n        sess.run(tf.global_variables_initializer())\n        embeddings = interface.load_embeddings()\n        model.set_embeddings(sess, embeddings)\n\n        # set initial states\n        states[\'start_epoch\'] = 1\n        states[\'best_eval\'] = 0.\n        states[\'best_epoch\'] = 0\n        states[\'best_step\'] = 0\n\n        self.log(f\'trainable params: {model.num_parameters():,d}\')\n        self.log(f\'trainable params (exclude embeddings): {model.num_parameters(exclude_embed=True):,d}\')\n        validate_params(self.args)\n        with open(os.path.join(self.args.summary_dir, \'args.json5\'), \'w\') as f:\n            args = {k: v for k, v in vars(self.args).items() if not k.startswith(\'_\')}\n            json5.dump(args, f, indent=2)\n        self.log(pformat(vars(self.args), indent=2, width=120))\n        return model, interface, states\n\n\nclass EarlyStop(Exception):\n    pass\n'"
src/modules/__init__.py,23,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + tf.nn.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n\n\ndef get_weight(shape, gain=np.sqrt(2), weight_norm=True, fan_in=None, name=\'weight\'):\n    if fan_in is None:\n        fan_in = np.prod(shape[:-1])\n    std = gain / np.sqrt(fan_in)  # He init\n\n    w = tf.get_variable(name, shape=shape, initializer=tf.initializers.random_normal(0, std),\n                        dtype=tf.float32)\n    if weight_norm:\n        g = tf.get_variable(\'{}_g\'.format(name), shape=(1,) * (len(shape) - 1) + (shape[-1],),\n                            initializer=tf.ones_initializer)\n        w_norm = tf.sqrt(tf.reduce_sum(tf.square(w), axis=list(range(len(shape) - 1)), keep_dims=True))\n        w = w / tf.maximum(w_norm, 1e-7) * g\n    return w\n\n\ndef apply_bias(x, name=\'bias\'):\n    b = tf.get_variable(name, shape=[x.get_shape()[-1]], initializer=tf.zeros_initializer)\n    b = tf.cast(b, x.dtype)\n    b = tf.reshape(b, [1] * len(x.get_shape()[:-1]) + [x.get_shape().as_list()[-1]])\n    return x + b\n\n\ndef dense(x, units, activation=None, name=\'dense\'):\n    with tf.variable_scope(name):\n        fan_in = x.shape[-1].value\n        new_shape = tf.concat([tf.shape(x)[:-1], tf.constant([units])], axis=0)\n        x = tf.reshape(x, (-1, fan_in))\n        gain = np.sqrt(2) if activation is tf.nn.relu else 1\n        w = get_weight([fan_in, units], gain=gain)\n        out = apply_bias(tf.matmul(x, w))\n        out = tf.reshape(out, new_shape)\n        if activation:\n            if activation is tf.nn.relu:\n                activation = gelu\n            out = activation(out)\n        return out\n\n\ndef conv1d(x, filters, kernel_size, activation=None, name=\'conv1d\'):\n    with tf.variable_scope(name):\n        gain = np.sqrt(2) if activation is tf.nn.relu else 1\n        x = tf.expand_dims(x, 1)\n        w = get_weight([kernel_size, x.shape[-1].value, filters], gain=gain)\n        w = tf.expand_dims(w, 0)\n        out = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding=\'SAME\')\n        out = tf.squeeze(out, [1])\n        out = apply_bias(out)\n        if activation:\n            out = activation(out)\n        return out\n'"
src/modules/alignment.py,20,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport math\nimport tensorflow as tf\nfrom functools import partial\nfrom src.utils.registry import register\nfrom . import dense\n\nregistry = {}\nregister = partial(register, registry=registry)\n\n\n@register(\'identity\')\nclass Alignment:\n    def __init__(self, args):\n        self.args = args\n\n    def _attention(self, a, b, t, _):\n        return tf.matmul(a, b, transpose_b=True) * t\n\n    def __call__(self, a, b, mask_a, mask_b, dropout_keep_prob, name=\'alignment\'):\n        with tf.variable_scope(name):\n            temperature = tf.get_variable(\'temperature\', shape=(), dtype=tf.float32, trainable=True,\n                                          initializer=tf.constant_initializer(math.sqrt(1 / self.args.hidden_size)))\n            tf.summary.histogram(\'temperature\', temperature)\n            attention = self._attention(a, b, temperature, dropout_keep_prob)\n            attention_mask = tf.matmul(mask_a, mask_b, transpose_b=True)\n            attention = attention_mask * attention + (1 - attention_mask) * tf.float32.min\n            attention_a = tf.nn.softmax(attention, dim=1)\n            attention_b = tf.nn.softmax(attention, dim=2)\n            attention_a = tf.identity(attention_a, name=\'attention_a\')\n            attention_b = tf.identity(attention_b, name=\'attention_b\')\n            tf.summary.histogram(\'attention_a\', tf.boolean_mask(attention_a, tf.cast(attention_mask, tf.bool)))\n            tf.summary.histogram(\'attention_b\', tf.boolean_mask(attention_b, tf.cast(attention_mask, tf.bool)))\n\n            feature_b = tf.matmul(attention_a, a, transpose_a=True)\n            feature_a = tf.matmul(attention_b, b)\n            return feature_a, feature_b\n\n\n@register(\'linear\')\nclass MappedAlignment(Alignment):\n    def _attention(self, a, b, t, dropout_keep_prob):\n        with tf.variable_scope(f\'proj\'):\n            a = dense(tf.nn.dropout(a, dropout_keep_prob),\n                      self.args.hidden_size, activation=tf.nn.relu)\n            b = dense(tf.nn.dropout(b, dropout_keep_prob),\n                      self.args.hidden_size, activation=tf.nn.relu)\n            return super()._attention(a, b, t, dropout_keep_prob)\n'"
src/modules/connection.py,2,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport math\nimport tensorflow as tf\nfrom functools import partial\nfrom src.utils.registry import register\nfrom . import dense\nregistry = {}\nregister = partial(register, registry=registry)\n\n\n@register(\'none\')\ndef null_connection(x, _, __):\n    return x\n\n\n@register(\'residual\')\ndef residual(x, res, _):\n    if x.shape[-1] != res.shape[-1]:\n        x = dense(x, res.shape.as_list()[-1], name=\'residual_projection\')\n    return (x + res) * math.sqrt(0.5)\n\n\n@register(\'aug\')\ndef augmented_residual(x, res, i):\n    if i == 1:\n        x = tf.concat([res, x], axis=-1)  # res is embedding\n    elif i > 1:\n        hidden_size = int(x.shape[-1])\n        x = (res[:, :, -hidden_size:] + x) * math.sqrt(0.5)\n        x = tf.concat([res[:, :, :-hidden_size], x], axis=-1)  # former half of res is embedding\n    return x\n'"
src/modules/embedding.py,7,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\n\n\nclass Embedding:\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n\n    @staticmethod\n    def set_(sess, value):\n        with tf.variable_scope(\'embedding\', reuse=True):\n            embedding_matrix = tf.get_variable(\'embedding_matrix\')\n        embedding_matrix_input = tf.placeholder(embedding_matrix.dtype, embedding_matrix.get_shape())\n        sess.run(embedding_matrix.assign(embedding_matrix_input),\n                 feed_dict={\n                     embedding_matrix_input: value,\n                 })\n\n    def __call__(self, x, dropout_keep_prob):\n        with tf.variable_scope(\'embedding\', reuse=tf.AUTO_REUSE):\n            embedding_matrix = tf.get_variable(\'embedding_matrix\', dtype=tf.float32, trainable=False,\n                                               shape=(self.args.num_vocab, self.args.embedding_dim))\n            x = tf.nn.embedding_lookup(embedding_matrix, x)\n            x = tf.nn.dropout(x, dropout_keep_prob)\n            return x\n'"
src/modules/encoder.py,4,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\nfrom . import conv1d\n\n\nclass Encoder:\n    def __init__(self, args):\n        self.args = args\n\n    def __call__(self, x, mask, dropout_keep_prob, name=\'encoder\'):\n        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n            for i in range(self.args.enc_layers):\n                x = mask * x\n                if i > 0:\n                    x = tf.nn.dropout(x, dropout_keep_prob)\n                x = conv1d(x, self.args.hidden_size, kernel_size=self.args.kernel_size, activation=tf.nn.relu,\n                           name=f\'cnn_{i}\')\n            x = tf.nn.dropout(x, dropout_keep_prob)\n            return x\n'"
src/modules/fusion.py,10,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\nfrom functools import partial\nfrom src.utils.registry import register\nfrom . import dense\n\nregistry = {}\nregister = partial(register, registry=registry)\n\n\n@register(\'simple\')\nclass Fusion:\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n\n    def __call__(self, x, align, _):\n        with tf.variable_scope(\'align\', reuse=tf.AUTO_REUSE):\n            return dense(tf.concat([x, align], axis=-1), self.args.hidden_size,\n                         activation=tf.nn.relu)\n\n\n@register(\'full\')\nclass FullFusion:\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n\n    def __call__(self, x, align, dropout_keep_prob):\n        with tf.variable_scope(\'align\', reuse=tf.AUTO_REUSE):\n            x = tf.concat([\n                dense(tf.concat([x, align], axis=-1), self.args.hidden_size, activation=tf.nn.relu, name=\'orig\'),\n                dense(tf.concat([x, x - align], axis=-1), self.args.hidden_size, activation=tf.nn.relu, name=\'sub\'),\n                dense(tf.concat([x, x * align], axis=-1), self.args.hidden_size, activation=tf.nn.relu, name=\'mul\'),\n            ], axis=-1)\n            x = tf.nn.dropout(x, dropout_keep_prob)\n            x = dense(x, self.args.hidden_size, activation=tf.nn.relu, name=""proj"")\n            return x\n'"
src/modules/pooling.py,1,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\n\n\ndef pooling(x, mask):\n    return tf.reduce_max(mask * x + (1. - mask) * tf.float32.min, axis=1)\n'"
src/modules/prediction.py,7,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\nfrom functools import partial\nfrom src.utils.registry import register\nfrom . import dense\n\nregistry = {}\nregister = partial(register, registry=registry)\n\n\n@register(\'simple\')\nclass Prediction:\n    def __init__(self, args):\n        self.args = args\n\n    def _features(self, a, b):\n        return tf.concat([a, b], axis=-1)\n\n    def __call__(self, a, b, dropout_keep_prob, name=\'prediction\'):\n        x = self._features(a, b)\n        with tf.variable_scope(name):\n            x = tf.nn.dropout(x, dropout_keep_prob)\n            x = dense(x, self.args.hidden_size, activation=tf.nn.relu, name=\'dense_1\')\n            x = tf.nn.dropout(x, dropout_keep_prob)\n            x = dense(x, self.args.num_classes, activation=None, name=\'dense_2\')\n            return x\n\n\n@register(\'full\')\nclass AdvancedPrediction(Prediction):\n    def _features(self, a, b):\n        return tf.concat([a, b, a * b, a - b], axis=-1)\n\n\n@register(\'symmetric\')\nclass SymmetricPrediction(Prediction):\n    def _features(self, a, b):\n        return tf.concat([a, b, a * b, tf.abs(a - b)], axis=-1)\n'"
src/utils/__init__.py,0,b'\n'
src/utils/loader.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport numpy as np\n\n\ndef load_data(data_dir, split=None):\n    data = []\n    if split is None:\n        files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith(\'.txt\')]\n    else:\n        if not split.endswith(\'.txt\'):\n            split += \'.txt\'\n        files = [os.path.join(data_dir, f\'{split}\')]\n    for file in files:\n        with open(file) as f:\n            for line in f:\n                text1, text2, label = line.rstrip().split(\'\\t\')\n                data.append({\n                    \'text1\': text1,\n                    \'text2\': text2,\n                    \'target\': label,\n                })\n    return data\n\n\ndef load_embeddings(file, vocab, dim, lower, mode=\'freq\'):\n    embedding = np.zeros((len(vocab), dim))\n    count = np.zeros((len(vocab), 1))\n    with open(file) as f:\n        for line in f:\n            elems = line.rstrip().split()\n            if len(elems) != dim + 1:\n                continue\n            token = elems[0]\n            if lower and mode != \'strict\':\n                token = token.lower()\n            if token in vocab:\n                index = vocab.index(token)\n                vector = [float(x) for x in elems[1:]]\n                if mode == \'freq\' or mode == \'strict\':\n                    if not count[index]:\n                        embedding[index] = vector\n                        count[index] = 1.\n                elif mode == \'last\':\n                    embedding[index] = vector\n                    count[index] = 1.\n                elif mode == \'avg\':\n                    embedding[index] += vector\n                    count[index] += 1.\n                else:\n                    raise NotImplementedError(\'Unknown embedding loading mode: \' + mode)\n    if mode == \'avg\':\n        inverse_mask = np.where(count == 0, 1., 0.)\n        embedding /= count + inverse_mask\n    return embedding.tolist()\n\n\n'"
src/utils/logger.py,1,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport sys\nimport logging\n\n\nclass Logger:\n    def __init__(self, args):\n        log = logging.getLogger(args.summary_dir)\n        if not log.handlers:\n            log.setLevel(logging.DEBUG)\n            fh = logging.FileHandler(os.path.join(args.summary_dir, args.log_file))\n            fh.setLevel(logging.INFO)\n            ch = ProgressHandler()\n            ch.setLevel(logging.DEBUG)\n            formatter = logging.Formatter(fmt=\'%(asctime)s %(message)s\', datefmt=\'%m/%d/%Y %I:%M:%S\')\n            fh.setFormatter(formatter)\n            ch.setFormatter(formatter)\n            log.addHandler(fh)\n            log.addHandler(ch)\n        self.log = log\n        # setup TensorBoard\n        if args.tensorboard:\n            import tensorflow as tf\n            from tensorboardX import SummaryWriter\n            summary_dir = os.path.join(args.summary_dir, \'viz\')\n            self.writer = tf.summary.FileWriter(summary_dir)\n            self.eval_writer = SummaryWriter(summary_dir)\n            self.log.info(f\'TensorBoard activated.\')\n        else:\n            self.writer = None\n            self.eval_writer = None\n        self.log_per_updates = args.log_per_updates\n        self.grad_clipping = args.grad_clipping\n        self.clips = 0\n        self.train_meters = {}\n        self.epoch = None\n        self.best_eval = 0.\n        self.best_eval_str = \'\'\n\n    def set_epoch(self, epoch):\n        self(f\'Epoch: {epoch}\')\n        self.epoch = epoch\n\n    @staticmethod\n    def _format_number(x):\n        return f\'{x:.4f}\' if float(x) > 1e-3 else f\'{x:.4e}\'\n\n    def update(self, stats):\n        updates = stats.pop(\'updates\')\n        if updates % self.log_per_updates == 0:\n            summary = stats.pop(\'summary\')\n            if self.writer:\n                self.writer.add_summary(summary, updates)\n            self.clips += int(stats[\'gnorm\'] > self.grad_clipping)\n            stats_str = \' \'.join(f\'{key}: \' + self._format_number(val) for key, val in stats.items())\n            for key, val in stats.items():\n                if key not in self.train_meters:\n                    self.train_meters[key] = AverageMeter()\n                self.train_meters[key].update(val)\n            msg = f\'epoch {self.epoch} updates {updates} {stats_str}\'\n            if self.log_per_updates != 1:\n                msg = \'> \' + msg\n            self.log.info(msg)\n\n    def newline(self):\n        self.log.debug(\'\')\n\n    def log_eval(self, valid_stats):\n        self.newline()\n        updates = valid_stats.pop(\'updates\')\n        eval_score = valid_stats.pop(\'score\')\n        # report the exponential averaged training stats, while reporting the full dev set stats\n        if self.train_meters:\n            train_stats_str = \' \'.join(f\'{key}: \' + self._format_number(val) for key, val in self.train_meters.items())\n            train_stats_str += \' \' + f\'clip: {self.clips}\'\n            self.log.info(f\'train {train_stats_str}\')\n        valid_stats_str = \' \'.join(f\'{key}: \' + self._format_number(val) for key, val in valid_stats.items())\n        if eval_score > self.best_eval:\n            self.best_eval_str = valid_stats_str\n            self.best_eval = eval_score\n            valid_stats_str += \' [NEW BEST]\'\n        else:\n            valid_stats_str += f\' [BEST: {self._format_number(self.best_eval)}]\'\n        self.log.info(f\'valid {valid_stats_str}\')\n        if self.eval_writer:\n            for key in valid_stats.keys():\n                group = {\'valid\': valid_stats[key]}\n                if self.train_meters and key in self.train_meters:\n                    group[\'train\'] = float(self.train_meters[key])\n                self.eval_writer.add_scalars(f\'valid/{key}\', group, updates)\n        self.train_meters = {}\n        self.clips = 0\n\n    def __call__(self, msg):\n        self.log.info(msg)\n\n\nclass ProgressHandler(logging.Handler):\n    def __init__(self, level=logging.NOTSET):\n        super().__init__(level)\n\n    def emit(self, record):\n        log_entry = self.format(record)\n        if record.message.startswith(\'> \'):\n            sys.stdout.write(\'{}\\r\'.format(log_entry.rstrip()))\n            sys.stdout.flush()\n        else:\n            sys.stdout.write(\'{}\\n\'.format(log_entry))\n\n\nclass AverageMeter(object):\n    """"""Keep exponential weighted averages.""""""\n    def __init__(self, beta=0.99):\n        self.beta = beta\n        self.moment = 0.\n        self.value = 0.\n        self.t = 0.\n\n    def update(self, val):\n        self.t += 1\n        self.moment = self.beta * self.moment + (1 - self.beta) * val\n        # bias correction\n        self.value = self.moment / (1 - self.beta ** self.t)\n\n    def __format__(self, spec):\n        return format(self.value, spec)\n\n    def __float__(self):\n        return self.value\n'"
src/utils/metrics.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport subprocess\nfrom functools import partial\nimport numpy as np\nfrom sklearn import metrics\n\nfrom .registry import register\n\nregistry = {}\nregister = partial(register, registry=registry)\n\n\n@register(\'acc\')\ndef acc(outputs):\n    target = outputs[\'target\']\n    pred = outputs[\'pred\']\n    return {\n        \'acc\': metrics.accuracy_score(target, pred).item(),\n    }\n\n\n@register(\'f1\')\ndef f1(outputs):\n    target = outputs[\'target\']\n    pred = outputs[\'pred\']\n    return {\n        \'f1\': metrics.f1_score(target, pred).item(),\n    }\n\n\n@register(\'auc\')\ndef auc(outputs):\n    target = outputs[\'target\']\n    prob = np.array(outputs[\'prob\'])\n    return {\n        \'auc\': metrics.roc_auc_score(target, prob[:, 1]).item(),\n    }\n\n\n@register(\'map\')\n@register(\'mrr\')\ndef ranking(outputs):\n    args = outputs[\'args\']\n    prediction = [o[1] for o in outputs[\'prob\']]\n    ref_file = os.path.join(args.data_dir, \'{}.ref\'.format(args.eval_file))\n    rank_file = os.path.join(args.data_dir, \'{}.rank\'.format(args.eval_file))\n    tmp_file = os.path.join(args.summary_dir, \'tmp-pred.txt\')\n    with open(rank_file) as f:\n        prefix = []\n        for line in f:\n            prefix.append(line.strip().split())\n        assert len(prefix) == len(prediction), \\\n            \'prefix {}, while prediction {}\'.format(len(prefix), len(prediction))\n    with open(tmp_file, \'w\') as f:\n        for prefix, pred in zip(prefix, prediction):\n            prefix[-2] = str(pred)\n            f.write(\' \'.join(prefix) + \'\\n\')\n    sp = subprocess.Popen(\'./resources/trec_eval {} {} | egrep ""map|recip_rank""\'.format(ref_file, tmp_file),\n                          shell=True,\n                          stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = sp.communicate()\n    stdout, stderr = stdout.decode(), stderr.decode()\n    os.remove(tmp_file)\n    map_, mrr = [float(s[-6:]) for s in stdout.strip().split(\'\\n\')]\n    return {\n        \'map\': map_,\n        \'mrr\': mrr,\n    }\n'"
src/utils/params.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport math\nimport shutil\nfrom datetime import datetime\nimport json5\n\n\nclass Object:\n    """"""\n    @DynamicAttrs\n    """"""\n    pass\n\n\ndef parse(config_file):\n    root = os.path.dirname(config_file)  # __parent__ in config is a relative path\n    config_group = _load_param(\'\', config_file)\n    if type(config_group) is dict:\n        config_group = [config_group]\n    configs = []\n    for config in config_group:\n        try:\n            choice = config.pop(\'__iter__\')\n            assert len(choice) == 1, \'only support iterating over 1 variable\'\n            key, values = next(iter(choice.items()))\n        except KeyError:\n            key, value = config.popitem()\n            values = [value]\n        for value in values:\n            config[key] = value\n            repeat = config.get(\'__repeat__\', 1)\n            for index in range(repeat):\n                config_ = config.copy()\n                config_[\'__index__\'] = index\n                if repeat > 1:\n                    config_[\'name\'] += \'-\' + str(index)\n                args = _parse_args(root, config_)\n                configs.append((args, config_))\n    return configs\n\n\ndef _parse_args(root, config):\n    args = Object()\n    assert type(config) is dict\n    parents = config.get(\'__parents__\', [])\n    for parent in parents:\n        parent = _load_param(root, parent)\n        assert type(parent) is dict, \'only top-level configs can be a sequence\'\n        _add_param(args, parent)\n    _add_param(args, config)\n    _post_process(args)\n    return args\n\n\ndef _add_param(args, x: dict):\n    for k, v in x.items():\n        if type(v) is dict:\n            _add_param(args, v)\n        else:\n            k = _validate_param(k)\n            if hasattr(args, k):\n                previous_type = type(getattr(args, k))\n                current_type = type(v)\n                assert previous_type is current_type or (isinstance(None, previous_type)) or \\\n                    (previous_type is float and current_type is int), \\\n                    f\'param ""{k}"" of type {previous_type} can not be overwritten by type {current_type}\'\n            setattr(args, k, v)\n\n\ndef _load_param(root, file: str):\n    file = os.path.join(root, file)\n    if not file.endswith(\'.json5\'):\n        file += \'.json5\'\n    with open(file) as f:\n        config = json5.load(f)\n        return config\n\n\ndef _post_process(args: Object):\n    if not args.output_dir.startswith(\'models\'):\n        args.output_dir = os.path.join(\'models\', args.output_dir)\n    os.makedirs(args.output_dir, exist_ok=True)\n    if not args.name:\n        args.name = str(datetime.now())\n    args.summary_dir = os.path.join(args.output_dir, args.name)\n    if os.path.exists(args.summary_dir):\n        shutil.rmtree(args.summary_dir)\n    os.makedirs(args.summary_dir)\n    data_config_file = os.path.join(args.output_dir, \'data_config.json5\')\n    if os.path.exists(data_config_file):\n        with open(data_config_file) as f:\n            config = json5.load(f)\n            for k, v in config.items():\n                if not hasattr(args, k) or getattr(args, k) != v:\n                    print(\'ERROR: Data configurations are different. Please use another output_dir or \'\n                          \'remove the older one manually.\')\n                    exit()\n    else:\n        with open(data_config_file, \'w\') as f:\n            keys = [\'data_dir\', \'min_df\', \'max_vocab\', \'max_len\', \'min_len\', \'lower_case\',\n                    \'pretrained_embeddings\', \'embedding_mode\']\n            json5.dump({k: getattr(args, k) for k in keys}, f)\n    args.metric = args.metric.lower()\n    args.watch_metrics = [m.lower() for m in args.watch_metrics]\n    if args.metric not in args.watch_metrics:\n        args.watch_metrics.append(args.metric)\n    assert args.pretrained_embeddings, \'pretrained embeddings must be provided.\'\n\n    def samples2steps(n):\n        return int(math.ceil(n / args.batch_size))\n\n    if not hasattr(args, \'log_per_updates\'):\n        args.log_per_updates = samples2steps(args.log_per_samples)\n    if not hasattr(args, \'eval_per_updates\'):\n        args.eval_per_updates = samples2steps(args.eval_per_samples)\n    if not hasattr(args, \'eval_per_updates_warmup\'):\n        args.eval_per_updates_warmup = samples2steps(args.eval_per_samples_warmup)\n    if not hasattr(args, \'eval_warmup_steps\'):\n        args.eval_warmup_steps = samples2steps(args.eval_warmup_samples)\n    if not hasattr(args, \'min_steps\'):\n        args.min_steps = samples2steps(args.min_samples)\n    if not hasattr(args, \'early_stopping\'):\n        args.early_stopping = samples2steps(args.tolerance_samples)\n    if not hasattr(args, \'lr_warmup_steps\'):\n        args.lr_warmup_steps = samples2steps(args.lr_warmup_samples)\n    if not hasattr(args, \'lr_decay_steps\'):\n        args.lr_decay_steps = samples2steps(args.lr_decay_samples)\n\n\ndef validate_params(args):\n    """"""validate params after interface initialization""""""\n    assert args.num_classes == 2 or (\'f1\' not in args.watch_metrics and \'auc\' not in args.watch_metrics), \\\n        f\'F1 and AUC are only valid for 2 classes.\'\n    assert args.num_classes == 2 or \'ranking\' not in args.watch_metrics, \\\n        f\'ranking metrics are only valid for 2 classes.\'\n    assert args.num_vocab > 0\n\n\ndef _validate_param(name):\n    name = name.replace(\'-\', \'_\')\n    if not str.isidentifier(name):\n        raise ValueError(f\'Invalid param name: {name}\')\n    return name\n'"
src/utils/registry.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef register(name=None, registry=None):\n    def decorator(fn, registration_name=None):\n        module_name = registration_name or _default_name(fn)\n        if module_name in registry:\n            raise LookupError(f""module {module_name} already registered."")\n        registry[module_name] = fn\n        return fn\n    return lambda fn: decorator(fn, name)\n\n\ndef _default_name(obj_class):\n    return obj_class.__name__\n'"
src/utils/vocab.py,0,"b'# coding=utf-8\n# Copyright (C) 2019 Alibaba Group Holding Limited\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom collections import Counter\n\n\nclass Indexer:\n    def __init__(self):\n        self.w2id = {}\n        self.id2w = {}\n\n    @property\n    def n_spec(self):\n        return 0\n\n    def __len__(self):\n        return len(self.w2id)\n\n    def __getitem__(self, index):\n        if index not in self.id2w:\n            raise IndexError(f\'invalid index {index} in indices.\')\n        return self.id2w[index]\n\n    def __contains__(self, item):\n        return item in self.w2id\n\n    def index(self, symbol):\n        if symbol in self.w2id:\n            return self.w2id[symbol]\n        raise IndexError(f\'Unknown symbol {symbol}\')\n\n    def keys(self):\n        return self.w2id.keys()\n\n    def indices(self):\n        return self.id2w.keys()\n\n    def add_symbol(self, symbol):\n        if symbol not in self.w2id:\n            self.id2w[len(self.id2w)] = symbol\n            self.w2id[symbol] = len(self.w2id)\n\n    @classmethod\n    def build(cls, symbols, min_counts=1, dump_filtered=None, log=print):\n        counter = Counter(symbols)\n        symbols = sorted([t for t, c in counter.items() if c >= min_counts],\n                         key=counter.get, reverse=True)\n        log(f\'\'\'{len(symbols)} symbols found: {\' \'.join(symbols[:15]) + (\'...\' if len(symbols) > 15 else \'\')}\'\'\')\n        filtered = sorted(list(counter.keys() - set(symbols)), key=counter.get, reverse=True)\n        if filtered:\n            log(\'filtered classes:\')\n            if len(filtered) > 20:\n                log(\'{} ... {}\'.format(\' \'.join(filtered[:10]), \' \'.join(filtered[-10:])))\n            else:\n                log(\' \'.join(filtered))\n            if dump_filtered:\n                with open(dump_filtered, \'w\') as f:\n                    for name in filtered:\n                        f.write(f\'{name} {counter.get(name)}\\n\')\n        indexer = cls()\n        try:  # restore numeric order if labels are represented by integers already\n            symbols = list(map(int, symbols))\n            symbols.sort()\n            symbols = list(map(str, symbols))\n        except ValueError:\n            pass\n        for symbol in symbols:\n            if symbol:\n                indexer.add_symbol(symbol)\n        return indexer\n\n    def save(self, file):\n        with open(file, \'w\') as f:\n            for symbol, index in self.w2id.items():\n                if index < self.n_spec:\n                    continue\n                f.write(\'{}\\n\'.format(symbol))\n\n    @classmethod\n    def load(cls, file):\n        indexer = cls()\n        with open(file) as f:\n            for line in f:\n                symbol = line.rstrip()\n                assert len(symbol) > 0, \'Empty symbol encountered.\'\n                indexer.add_symbol(symbol)\n        return indexer\n\n\nclass RobustIndexer(Indexer):\n    def __init__(self, validate=True):\n        super().__init__()\n        self.w2id.update({self.unk_symbol(): self.unk()})\n        self.id2w = {i: w for w, i in self.w2id.items()}\n        if validate:\n            self.validate_spec()\n\n    @property\n    def n_spec(self):\n        return 1\n\n    def index(self, symbol):\n        return self.w2id[symbol] if symbol in self.w2id else self.unk()\n\n    @staticmethod\n    def unk():\n        return 0\n\n    @staticmethod\n    def unk_symbol():\n        return \'<UNK>\'\n\n    def validate_spec(self):\n        assert self.n_spec == len(self.w2id), f\'{self.n_spec}, {len(self.w2id)}\'\n        assert len(self.w2id) == max(self.id2w.keys()) + 1, ""empty indices found in special tokens""\n        assert len(self.w2id) == len(self.id2w), ""index conflict in special tokens""\n\n\nclass Vocab(RobustIndexer):\n    def __init__(self):\n        super().__init__(validate=False)\n        self.w2id.update({\n            self.pad_symbol(): self.pad(),\n        })\n        self.id2w = {i: w for w, i in self.w2id.items()}\n        self.validate_spec()\n\n    @classmethod\n    def build(cls, words, lower=False, min_df=1, max_tokens=float(\'inf\'), pretrained_embeddings=None,\n              dump_filtered=None, log=print):\n        if pretrained_embeddings:\n            wv_vocab = cls.load_embedding_vocab(pretrained_embeddings, lower)\n        else:\n            wv_vocab = set()\n        if lower:\n            words = (word.lower() for word in words)\n        counter = Counter(words)\n        candidate_tokens = sorted([t for t, c in counter.items() if t in wv_vocab or c >= min_df],\n                                  key=counter.get, reverse=True)\n        if len(candidate_tokens) > max_tokens:\n            tokens = []\n            for i, token in enumerate(candidate_tokens):\n                if i < max_tokens:\n                    tokens.append(token)\n                elif token in wv_vocab:\n                    tokens.append(token)\n        else:\n            tokens = candidate_tokens\n        total = sum(counter.values())\n        matched = sum(counter[t] for t in tokens)\n        stats = (len(tokens), len(counter), total - matched, total, (total - matched) / total * 100)\n        log(\'vocab coverage {}/{} | OOV occurrences {}/{} ({:.4f}%)\'.format(*stats))\n        tokens_set = set(tokens)\n        if pretrained_embeddings:\n            oop_samples = sorted(list(tokens_set - wv_vocab), key=counter.get, reverse=True)\n            log(\'Covered by pretrained vectors {:.4f}%. \'.format(len(tokens_set & wv_vocab) / len(tokens) * 100) +\n                (\'outside pretrained: \' + \' \'.join(oop_samples[:10]) + \' ...\' if len(oop_samples) > 10 else \'\')\n                if oop_samples else \'\')\n        log(\'top words:\\n{}\'.format(\' \'.join(tokens[:10])))\n        filtered = sorted(list(counter.keys() - set(tokens)), key=counter.get, reverse=True)\n        if filtered:\n            if len(filtered) > 20:\n                log(\'filtered words:\\n{} ... {}\'.format(\' \'.join(filtered[:10]), \' \'.join(filtered[-10:])))\n            else:\n                log(\'filtered words:\\n\' + \' \'.join(filtered))\n            if dump_filtered:\n                with open(dump_filtered, \'w\') as f:\n                    for name in filtered:\n                        f.write(f\'{name} {counter.get(name)}\\n\')\n\n        vocab = cls()\n        for token in tokens:\n            vocab.add_symbol(token)\n        return vocab\n\n    @staticmethod\n    def load_embedding_vocab(file, lower):\n        wv_vocab = set()\n        with open(file) as f:\n            for line in f:\n                token = line.rstrip().split(\' \')[0]\n                if lower:\n                    token = token.lower()\n                wv_vocab.add(token)\n        return wv_vocab\n\n    @staticmethod\n    def pad():\n        return 0\n\n    @staticmethod\n    def unk():\n        return 1\n\n    @property\n    def n_spec(self):\n        return 2\n\n    @staticmethod\n    def pad_symbol():\n        return \'<PAD>\'\n\n    char_map = {  # escape special characters for safe serialization\n        \'\\n\': \'<NEWLINE>\',\n    }\n\n    def save(self, file):\n        with open(file, \'w\') as f:\n            for symbol, index in self.w2id.items():\n                if index < self.n_spec:\n                    continue\n                symbol = self.char_map.get(symbol, symbol)\n                f.write(f\'{symbol}\\n\')\n\n    @classmethod\n    def load(cls, file):\n        vocab = cls()\n        reverse_char_map = {v: k for k, v in cls.char_map.items()}\n        with open(file) as f:\n            for line in f:\n                symbol = line.rstrip(\'\\n\')\n                symbol = reverse_char_map.get(symbol, symbol)\n                vocab.add_symbol(symbol)\n        return vocab\n'"
