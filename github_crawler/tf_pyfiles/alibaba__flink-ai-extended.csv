file_path,api_count,code
flink-ml-framework/python/setup.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport os\nimport re\nimport sys\nimport sysconfig\nimport platform\nimport subprocess\nfrom shutil import copyfile, copymode\nfrom distutils.version import LooseVersion\nfrom setuptools import setup, find_packages, Extension\nfrom setuptools.command.build_ext import build_ext\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, sourcedir=\'\'):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n\nclass CMakeBuild(build_ext):\n    def run(self):\n        try:\n            out = subprocess.check_output([\'cmake\', \'--version\'])\n        except OSError:\n            raise RuntimeError(\n                ""CMake must be installed to build the following extensions: "" +\n                "", "".join(e.name for e in self.extensions))\n\n        if platform.system() == ""Windows"":\n            cmake_version = LooseVersion(re.search(r\'version\\s*([\\d.]+)\',\n                                                   out.decode()).group(1))\n            if cmake_version < \'3.1.0\':\n                raise RuntimeError(""CMake >= 3.1.0 is required on Windows"")\n\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        extdir = os.path.abspath(\n            os.path.dirname(self.get_ext_fullpath(ext.name)))\n        cmake_args = [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\' + extdir,\n                      \'-DPYTHON_EXECUTABLE=\' + sys.executable]\n\n        cfg = \'Debug\' if self.debug else \'Release\'\n        build_args = [\'--config\', cfg]\n\n        if platform.system() == ""Windows"":\n            cmake_args += [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}\'.format(\n                cfg.upper(),\n                extdir)]\n            if sys.maxsize > 2**32:\n                cmake_args += [\'-A\', \'x64\']\n            build_args += [\'--\', \'/m\']\n        else:\n            cmake_args += [\'-DCMAKE_BUILD_TYPE=\' + cfg]\n            build_args += [\'--\', \'-j2\']\n            if platform.system() == ""Linux"":\n                build_args += [\'-lpthread\']\n\n        env = os.environ.copy()\n        env[\'CXXFLAGS\'] = \'{} -D_GLIBCXX_USE_CXX11_ABI=0 -DVERSION_INFO=\\\\""{}\\\\""\'.format(\n            env.get(\'CXXFLAGS\', \'\'),\n            self.distribution.get_version())\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        subprocess.check_call([\'cmake\', ext.sourcedir] + cmake_args,\n                              cwd=self.build_temp, env=env)\n        subprocess.check_call([\'cmake\', \'--build\', \'.\'] + build_args,\n                              cwd=self.build_temp)\n\nsetup(\n    name=\'flink_ml_framework\',\n    version=\'0.2.1\',\n    include_package_data=True,\n    packages=find_packages(),\n    ext_modules=[CMakeExtension(\'flink_ml_framework/flink_ml_framework\')],\n    cmdclass=dict(build_ext=CMakeBuild),\n    zip_safe=False,\n)\n'"
flink-ml-tensorflow/python/setup.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport os\nimport re\nimport sys\nimport sysconfig\nimport platform\nimport subprocess\nfrom shutil import copyfile, copymode\nfrom distutils.version import LooseVersion\nfrom setuptools import setup, find_packages, Extension\nfrom setuptools.command.build_ext import build_ext\n\nclass CMakeExtension(Extension):\n    def __init__(self, name, sourcedir=\'\'):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n\nclass CMakeBuild(build_ext):\n    def run(self):\n        try:\n            out = subprocess.check_output([\'cmake\', \'--version\'])\n        except OSError:\n            raise RuntimeError(\n                ""CMake must be installed to build the following extensions: "" +\n                "", "".join(e.name for e in self.extensions))\n\n        if platform.system() == ""Windows"":\n            cmake_version = LooseVersion(re.search(r\'version\\s*([\\d.]+)\',\n                                                   out.decode()).group(1))\n            if cmake_version < \'3.1.0\':\n                raise RuntimeError(""CMake >= 3.1.0 is required on Windows"")\n\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        extdir = os.path.abspath(\n            os.path.dirname(self.get_ext_fullpath(ext.name)))\n        cmake_args = [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\' + extdir,\n                      \'-DPYTHON_EXECUTABLE=\' + sys.executable]\n\n        cfg = \'Debug\' if self.debug else \'Release\'\n        build_args = [\'--config\', cfg]\n\n        if platform.system() == ""Windows"":\n            cmake_args += [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}\'.format(\n                cfg.upper(),\n                extdir)]\n            if sys.maxsize > 2**32:\n                cmake_args += [\'-A\', \'x64\']\n            build_args += [\'--\', \'/m\']\n        else:\n            cmake_args += [\'-DCMAKE_BUILD_TYPE=\' + cfg]\n            build_args += [\'--\', \'-j2\']\n            if platform.system() == ""Linux"":\n                build_args += [\'-lpthread\']\n\n        env = os.environ.copy()\n        env[\'CXXFLAGS\'] = \'{} -D_GLIBCXX_USE_CXX11_ABI=0 -DVERSION_INFO=\\\\""{}\\\\""\'.format(\n            env.get(\'CXXFLAGS\', \'\'),\n            self.distribution.get_version())\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        subprocess.check_call([\'cmake\', ext.sourcedir] + cmake_args,\n                              cwd=self.build_temp, env=env)\n        subprocess.check_call([\'cmake\', \'--build\', \'.\'] + build_args,\n                              cwd=self.build_temp)\n\nsetup(\n    name=\'flink_ml_tensorflow\',\n    version=\'0.2.1\',\n    include_package_data=True,\n    packages=find_packages(),\n    ext_modules=[CMakeExtension(\'flink_ml_tensorflow/flink_ml_tensorflow\')],\n    cmdclass=dict(build_ext=CMakeBuild),\n    zip_safe=False,\n    install_requires = [\'tensorflow==1.13.1\', \'tensorboard==1.13.1\', \'flink_ml_framework==0.2.1\'],\n)\n'"
flink-ml-framework/python/flink_ml_framework/__init__.py,0,b''
flink-ml-framework/python/flink_ml_framework/context.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom time import sleep\nimport json\nimport os\nfrom flink_ml_framework import node_pb2\nfrom flink_ml_framework import node_service_pb2_grpc\n\n\nclass Context(object):\n\n    def __str__(self):\n        return self.context_pb.__str__()\n\n    def __init__(self, context, channel):\n        self.mode = context.mode\n        self.roleName = context.roleName\n        self.index = context.index\n        self.roleParallelism = context.roleParallelism\n        self.properties = context.props\n        self.context_pb = context\n\n        self.userScript = context.userScript\n        self.identity = context.identity\n        self.funcName = context.funcName\n        self.failNum = context.failNum\n\n        self.outQueueName = context.outQueueName\n        self.inQueueName = context.inQueueName\n        self.outQueueMMapLen = context.outQueueMMapLen\n        self.inQueueMMapLen = context.inQueueMMapLen\n\n        self.channel = channel\n        self.stub = node_service_pb2_grpc.NodeServiceStub(self.channel)\n\n    def from_java(self):\n        return ""queue://"" + str(self.inQueueName) + "":"" + str(self.inQueueMMapLen)\n\n    def to_java(self):\n        return ""queue://"" + str(self.outQueueName) + "":"" + str(self.outQueueMMapLen)\n\n    def get_failed_num(self):\n        return self.failNum\n\n    def get_finish_workers(self):\n        response = self.stub.GetFinishWorker(node_pb2.NodeSimpleRequest(code=0))\n        return response.workers\n\n    def stop_job(self):\n        response = self.stub.FinishJob(node_pb2.NodeSimpleRequest(code=0))\n\n    def get_property(self, key):\n        return self.properties[key]\n\n    def get_role_parallelism_map(self):\n        return self.roleParallelism\n\n    def get_index(self):\n        return self.index\n\n    def get_role_name(self):\n        return self.roleName\n\n    def get_context_proto(self):\n        return self.context_pb\n'"
flink-ml-framework/python/flink_ml_framework/java_file.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport java_file_c\nimport struct\nimport json\n\n\nclass JavaFile(object):\n    def __init__(self, read_file, write_file):\n        self.read_file = read_file\n        self.write_file = write_file\n        self.java_file_c = java_file_c.JavaFile(read_file, write_file)\n\n    def read(self, data_len):\n        data = self.java_file_c.readBytes(data_len)\n        if 0 == len(data):\n            raise EOFError(""file reach end!"")\n        return data\n\n    def write(self, data, data_len):\n        return self.java_file_c.writeBytes(data, data_len)\n\n\nclass BytesRecorder(object):\n    def __init__(self, read_file, write_file):\n        self.java_file = JavaFile(read_file, write_file)\n\n    def read_record(self):\n        res = self.java_file.read(4)\n        data_len, = struct.unpack(""<i"", res)\n        return self.java_file.read(data_len)\n\n    def write_record(self, data):\n        data_len = len(data)\n        json_len = struct.pack(""<i"", data_len)\n        res = self.java_file.write(json_len, 4)\n        if res is False:\n            return False\n        res = self.java_file.write(data, data_len)\n        return res\n\n\nclass JsonRecorder(object):\n    def __init__(self, read_file, write_file):\n        self.java_file = JavaFile(read_file, write_file)\n\n    def read_record(self):\n        res = self.java_file.read(4)\n        data_len, = struct.unpack(""<i"", res)\n        data = self.java_file.read(data_len)\n        return json.loads(data)\n\n    def write_record(self, data):\n        json_data = json.dumps(data)\n        data_len = len(json_data)\n        json_len = struct.pack(""<i"", data_len)\n        res = self.java_file.write(json_len, 4)\n        if res is False:\n            return False\n        res = self.java_file.write(json_data, data_len)\n        return res\n'"
flink-ml-framework/python/flink_ml_framework/startup.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom __future__ import print_function\nimport sys\nimport importlib\nimport threading\nimport ctypes\nimport logging\nimport traceback\nfrom flink_ml_framework import context\nimport grpc\nfrom flink_ml_framework import node_pb2\nfrom flink_ml_framework import node_service_pb2_grpc\n\n\ndef parse_dir_script(script_path):\n    index = str(script_path).rindex(\'/\')\n    dir_str = script_path[0: index + 1]\n    script_name = script_path[index + 1: len(script_path) - 3]\n    return dir_str, script_name\n\n\ndef start_user_func(function, ml_context):\n    try:\n        function(ml_context)\n    except Exception as e:\n        logging.error(traceback.format_exc())\n        raise\n\n\ndef start_user_thread(function, ml_context):\n    local_t = threading.Thread(target=start_user_func, args=(function, ml_context,), name=""user_thread"")\n    local_t.setDaemon(True)\n    local_t.start()\n    return local_t\n\n\ndef terminate_thread(thread):\n    """"""Terminates a python thread from another thread.\n\n    :param thread: a threading.Thread instance\n    """"""\n    if not thread.isAlive():\n        return\n\n    exc = ctypes.py_object(SystemExit)\n    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n        ctypes.c_long(thread.ident), exc)\n    if res == 0:\n        raise ValueError(""nonexistent thread id"")\n    elif res > 1:\n        # """"""if it returns a number greater than one, you\'re in trouble,\n        # and you should call it again with exc=NULL to revert the effect""""""\n        ctypes.pythonapi.PyThreadState_SetAsyncExc(thread.ident, None)\n        raise SystemError(""PyThreadState_SetAsyncExc failed"")\n\n    thread.join()\n\n\ndef createContext(node_address):\n    channel = grpc.insecure_channel(node_address)\n    stub = node_service_pb2_grpc.NodeServiceStub(channel)\n    response = stub.GetContext(node_pb2.ContextRequest(message=\'\'))\n    context_proto = response.context\n    return context.Context(context_proto, channel)\n\n\nif __name__ == ""__main__"":\n    assert len(sys.argv) == 2, \'Invalid cmd line argument \' + str(sys.argv)\n\n    print (\'Running user func in process mode\')\n    sys.stdout.flush()\n\n    address = sys.argv[1]\n\n    context = createContext(address)\n\n    # setup default logging\n    logging.basicConfig(level=logging.INFO,\n                        format=\'%(asctime)s [\' + context.identity + \'-python-%(filename)s:%(lineno)d] %(levelname)s %(message)s\',\n                        datefmt=\'%Y-%m-%d %H:%M:%S\',\n                        )\n\n    print (""########## "" + context.userScript)\n    script_str = context.userScript\n    key = context.identity\n    func_name = context.funcName\n    dir_name = parse_dir_script(script_str)\n    sys.path.insert(0, dir_name[0])\n    user_py = importlib.import_module(dir_name[1])\n    func = getattr(user_py, func_name)\n    logging.info(key + \' calling user func \' + func_name)\n    func(context)\n    logging.info(key + "" python run finish"")\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/__init__.py,0,b''
flink-ml-tensorflow/python/flink_ml_tensorflow/gpu_info.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import nested_scopes\nfrom __future__ import print_function\n\nimport ctypes as ct\nimport logging\nimport platform\nimport random\nimport subprocess\nimport time\n\nMAX_RETRIES = 3  #: Maximum retries to allocate GPUs\n\n\ndef _get_gpu():\n    """"""*DEPRECATED*. Allocates first available GPU using cudaSetDevice(), or returns 0 otherwise.""""""\n    # Note: this code executes, but Tensorflow subsequently complains that the ""current context was not created by the StreamExecutor cuda_driver API""\n    system = platform.system()\n    if system == ""Linux"":\n        libcudart = ct.cdll.LoadLibrary(""libcudart.so"")\n    elif system == ""Darwin"":\n        libcudart = ct.cdll.LoadLibrary(""libcudart.dylib"")\n    elif system == ""Windows"":\n        libcudart = ct.windll.LoadLibrary(""libcudart.dll"")\n    else:\n        raise NotImplementedError(""Cannot identify system."")\n\n    device_count = ct.c_int()\n    libcudart.cudaGetDeviceCount(ct.byref(device_count))\n    gpu = 0\n    for i in range(device_count.value):\n        if (0 == libcudart.cudaSetDevice(i) and 0 == libcudart.cudaFree(0)):\n            gpu = i\n            break\n    return gpu\n\n\ndef get_gpus(num_gpu=1, worker_index=-1):\n    """"""Get list of free GPUs according to nvidia-smi.\n\n    This will retry for ``MAX_RETRIES`` times until the requested number of GPUs are available.\n\n    Args:\n      :num_gpu: number of GPUs desired.\n      :worker_index: index ""hint"" for allocation of available GPUs.\n\n    Returns:\n      Comma-delimited string of GPU ids, or raises an Exception if the requested number of GPUs could not be found.\n    """"""\n    try:\n        # get list of gpus (index, uuid)\n        list_gpus = subprocess.check_output([""nvidia-smi"", ""--list-gpus""]).decode()\n        logging.debug(""all GPUs:\\n{0}"".format(list_gpus))\n\n        # parse index and guid\n        gpus = [x for x in list_gpus.split(\'\\n\') if len(x) > 0]\n    except Exception as e:\n        gpus = []\n        return gpus\n\n    def parse_gpu(gpu_str):\n        cols = gpu_str.split(\' \')\n        return cols[5].split(\')\')[0], cols[1].split(\':\')[0]\n\n    gpu_list = [parse_gpu(gpu) for gpu in gpus]\n\n    free_gpus = []\n    retries = 0\n    while len(free_gpus) < num_gpu and retries < MAX_RETRIES:\n        smi_output = subprocess.check_output(\n            [""nvidia-smi"", ""--format=csv,noheader,nounits"", ""--query-compute-apps=gpu_uuid""]).decode()\n        logging.debug(""busy GPUs:\\n{0}"".format(smi_output))\n        busy_uuids = [x for x in smi_output.split(\'\\n\') if len(x) > 0]\n        for uuid, index in gpu_list:\n            if uuid not in busy_uuids:\n                free_gpus.append(index)\n\n        if len(free_gpus) < num_gpu:\n            logging.warn(""Unable to find available GPUs: requested={0}, available={1}"".format(num_gpu, len(free_gpus)))\n            retries += 1\n            time.sleep(30 * retries)\n            free_gpus = []\n\n    logging.info(""Available GPUs: {}"".format(free_gpus))\n\n    # if still can\'t find available GPUs, raise exception\n    if len(free_gpus) < num_gpu:\n        smi_output = subprocess.check_output(\n            [""nvidia-smi"", ""--format=csv"", ""--query-compute-apps=gpu_uuid,pid,process_name,used_gpu_memory""]).decode()\n        logging.info("": {0}"".format(smi_output))\n        raise Exception(""Unable to find {} free GPU(s)\\n{}"".format(num_gpu, smi_output))\n\n    # Get logical placement\n    num_available = len(free_gpus)\n    if worker_index == -1:\n        # use original random placement\n        random.shuffle(free_gpus)\n        proposed_gpus = free_gpus[:num_gpu]\n    else:\n        # ordered by worker index\n        if worker_index + num_gpu > num_available:\n            worker_index = worker_index % num_available\n        proposed_gpus = free_gpus[worker_index:(worker_index + num_gpu)]\n    logging.info(""Proposed GPUs: {}"".format(proposed_gpus))\n\n    # return \',\'.join(str(x) for x in proposed_gpus)\n    return proposed_gpus\n\n\n# Function to get the gpu information\ndef _get_free_gpu(max_gpu_utilization=40, min_free_memory=0.5, num_gpu=1):\n    """"""Get available GPUs according to utilization thresholds.\n\n    Args:\n      :max_gpu_utilization: percent utilization threshold to consider a GPU ""free""\n      :min_free_memory: percent free memory to consider a GPU ""free""\n      :num_gpu: number of requested GPUs\n\n    Returns:\n      A tuple of (available_gpus, minimum_free_memory), where available_gpus is a comma-delimited string of GPU ids, and minimum_free_memory\n      is the lowest amount of free memory available on the available_gpus.\n\n    """"""\n\n    def get_gpu_info():\n        # Get the gpu information\n        gpu_info = subprocess.check_output([""nvidia-smi"", ""--format=csv,noheader,nounits"",\n                                            ""--query-gpu=index,memory.total,memory.free,memory.used,utilization.gpu""]).decode()\n        gpu_info = gpu_info.split(\'\\n\')\n\n        gpu_info_array = []\n\n        # Check each gpu\n        for line in gpu_info:\n            if len(line) > 0:\n                gpu_id, total_memory, free_memory, used_memory, gpu_util = line.split(\',\')\n                gpu_memory_util = float(used_memory) / float(total_memory)\n                gpu_info_array.append((float(gpu_util), gpu_memory_util, gpu_id))\n\n        return (gpu_info_array)\n\n    # Read the gpu information multiple times\n    num_times_to_average = 5\n    current_array = []\n    for ind in range(num_times_to_average):\n        current_array.append(get_gpu_info())\n        time.sleep(1)\n\n    # Get number of gpus\n    num_gpus = len(current_array[0])\n\n    # Average the gpu information\n    avg_array = [(0, 0, str(x)) for x in range(num_gpus)]\n    for ind in range(num_times_to_average):\n        for gpu_ind in range(num_gpus):\n            avg_array[gpu_ind] = (avg_array[gpu_ind][0] + current_array[ind][gpu_ind][0],\n                                  avg_array[gpu_ind][1] + current_array[ind][gpu_ind][1], avg_array[gpu_ind][2])\n\n    for gpu_ind in range(num_gpus):\n        avg_array[gpu_ind] = (\n        float(avg_array[gpu_ind][0]) / num_times_to_average, float(avg_array[gpu_ind][1]) / num_times_to_average,\n        avg_array[gpu_ind][2])\n\n    avg_array.sort()\n\n    gpus_found = 0\n    gpus_to_use = """"\n    free_memory = 1.0\n    # Return the least utilized GPUs if it\'s utilized less than max_gpu_utilization and amount of free memory is at least min_free_memory\n    # Otherwise, run in cpu only mode\n    for current_gpu in avg_array:\n        if current_gpu[0] < max_gpu_utilization and (1 - current_gpu[1]) > min_free_memory:\n            if gpus_found == 0:\n                gpus_to_use = current_gpu[2]\n                free_memory = 1 - current_gpu[1]\n            else:\n                gpus_to_use = gpus_to_use + "","" + current_gpu[2]\n                free_memory = min(free_memory, 1 - current_gpu[1])\n\n            gpus_found = gpus_found + 1\n\n        if gpus_found == num_gpu:\n            break\n\n    return gpus_to_use, free_memory\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/table_sinks.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\nfrom pyflink.util.type_util import TypesUtil\nfrom pyflink.table.sinks import JavaTableSink\nfrom sink_funcs import LogSink\n\n\nclass LogTableStreamSink(JavaTableSink):\n    def __init__(self, sink_func=None):\n        if sink_func is None:\n            sink_func = LogSink()._j_sink_function\n        sink_clz_name = \'com.alibaba.flink.tensorflow.flink_op.sink.LogTableStreamSink\'\n        sink_clz = TypesUtil.class_for_name(sink_clz_name)\n        super(LogTableStreamSink, self).__init__(sink_clz(sink_func))\n\n\nclass LogInferAccSink(LogTableStreamSink):\n    def __init__(self):\n        sink_func = get_gateway().jvm.com.alibaba.flink.tensorflow.client.LogInferAccSink()\n        super(LogInferAccSink, self).__init__(sink_func=sink_func)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_TFConfig.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\n\n\nclass TFConfig(object):\n\n    def __init__(self, num_worker, num_ps, python_file, func, properties, env_path, zk_conn, zk_base_path):\n        self._num_worker = num_worker\n        self._num_ps = num_ps\n        self._python_file = python_file\n        self._func = func\n        self._properties = properties\n        self._env_path = env_path\n        self._zk_conn = zk_conn\n        self._zk_base_path = zk_base_path\n\n    def java_config(self):\n        return get_gateway().jvm.com.alibaba.flink.ml.tensorflow.client.TFConfig(self._num_worker,\n                                                                                 self._num_ps,\n                                                                                 self._properties,\n                                                                                 self._python_file,\n                                                                                 self._func,\n                                                                                 self._env_path)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_context.py,2,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport json\nimport os\nimport tensorflow as tf\nfrom flink_ml_tensorflow import tensorflow_on_flink_ops as flink_ops\nfrom flink_ml_framework.context import Context\n\n\nclass TFContext(Context):\n    def __init__(self, other):\n        if isinstance(other, Context):\n            self.__dict__ = other.__dict__.copy()\n\n    @staticmethod\n    def to_tf_cluster(cluster_str):\n        cluster_json = json.loads(cluster_str)\n        tf_cluster = {\'ps\': [], \'worker\': []}\n        jobs = cluster_json[\'job\']\n        for job in jobs:\n            if \'worker\' == job[\'name\']:\n                worker_num = len(job[\'tasks\'])\n                for i in range(worker_num):\n                    task = job[\'tasks\'][str(i)]\n                    ip = task[\'ip\']\n                    port = task[\'props\'][\'SYS:tf_port\']\n                    address = ip + "":"" + port\n                    tf_cluster[\'worker\'].append(address)\n                continue\n            if \'ps\' == job[\'name\']:\n                ps_num = len(job[\'tasks\'])\n                for i in range(ps_num):\n                    task = job[\'tasks\'][str(i)]\n                    ip = task[\'ip\']\n                    port = task[\'props\'][\'SYS:tf_port\']\n                    address = ip + "":"" + port\n                    tf_cluster[\'ps\'].append(address)\n        if 0 == len(tf_cluster[\'ps\']):\n            del tf_cluster[\'ps\']\n        return tf_cluster\n\n    def get_tf_cluster(self):\n        cluster_str = self.get_property(""cluster"")\n        return TFContext.to_tf_cluster(cluster_str)\n\n    @staticmethod\n    def cluster_to_estimator(cluster_str):\n        cluster = TFContext.to_tf_cluster(cluster_str)\n        worker_0 = cluster[\'worker\'][0]\n        del (cluster[\'worker\'][0])\n        if 0 == len(cluster[\'worker\']):\n            del (cluster[\'worker\'])\n        cluster[\'chief\'] = [worker_0]\n        return cluster\n\n    @staticmethod\n    def export_cluster_env(cluster_str, job_name, index):\n        cluster = TFContext.cluster_to_estimator(cluster_str)\n        if \'ps\' == job_name:\n            task_type = \'ps\'\n            task_index = index\n        elif \'worker\' == job_name:\n            if 0 == index:\n                task_type = \'chief\'\n                task_index = 0\n            else:\n                task_type = \'worker\'\n                task_index = index - 1\n\n        os.environ[\'TF_CONFIG\'] = json.dumps(\n            {\'cluster\': cluster,\n             \'task\': {\'type\': task_type, \'index\': task_index}})\n        print (os.environ[\'TF_CONFIG\'])\n        return cluster, task_type, task_index\n\n    def export_estimator_cluster(self):\n        cluster_str = self.properties[""cluster""]\n        return TFContext.export_cluster_env(cluster_str, self.roleName, self.index)\n\n    def example_input_dataset(self):\n        dataset = tf.data.TFRecordDataset(self.from_java())\n        dataset = dataset.map(lambda record: tf.parse_single_example(record, features=self.features))\n        return dataset\n\n    def output_writer_op(self, input_list):\n        path = self.to_java()\n        writer = flink_ops.FlinkTFRecordWriter(address=path)\n        write_op = writer.write(input_list)\n        close_op = writer.close()\n        return write_op, close_op\n\n    def flink_stream_dataset(self, compression_type=None, buffer_size=0, num_parallel_reads=None):\n        return flink_ops.FlinkStreamDataSet(self.from_java(), compression_type, buffer_size, num_parallel_reads)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_datastream.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\nfrom tensorflow_TFConfig import TFConfig\nfrom pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment\nfrom tensorflow_on_flink_stream.datastream import DataStream\nfrom tensorflow_on_flink_stream.datastreamsource import DataStreamSource\nfrom pyflink.table import types\n\n\ndef inference(num_worker, num_ps=0, python_file= None, func=None, properties=None, env_path=None, zk_conn=None, zk_base_path=None,\n              stream_env=None, input_ds=None, output_row_type=None):\n    """"""\n    Tensorflow inference for DataStream\n    :param num_worker: Number of workers\n    :param num_ps: Number of PS\n    :param python_file: The python file which is going to be run\n    :param func: The user-defined function that runs TF inference. If it\'s None, inference is run via Java API.\n    :param properties: User-defined properties\n    :param env_path: Path to the virtual env\n    :param zk_conn: The Zookeeper connection string\n    :param zk_base_path: The Zookeeper base path\n    :param stream_env: The StreamExecutionEnvironment. If it\'s None, this method will create one and execute the job\n                       at the end. Otherwise, caller is responsible to trigger the job execution\n    :param input_ds: The input DataStream\n    :param output_row_type: The RowType for the output DataStream. If it\'s None, a dummy sink will be added to the\n                      output DataStream. Otherwise, caller is responsible to add sink before executing the job.\n    :return: The output DataStream. Currently it\'s always of type Row.\n    """"""\n    tf_config = TFConfig(num_worker, num_ps, python_file, func, properties, env_path, zk_conn, zk_base_path)\n    if stream_env is None:\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n    if input_ds is not None:\n        if isinstance(input_ds, DataStreamSource):\n            input_ds = input_ds._j_datastream_source\n        else:\n            input_ds = input_ds._j_datastream\n    output_ds = get_gateway().jvm.com.alibaba.flink.ml.tensorflow.client.TFUtils.inference(stream_env._j_stream_execution_environment,\n                                                                                           input_ds,\n                                                                                           tf_config.java_config(),\n                                                                                           to_java_type_info(output_row_type))\n    stream_env.execute()\n    return DataStream(output_ds)\n\n\ndef train(num_worker, num_ps, python_file, func, properties=None, env_path=None, zk_conn=None, zk_base_path=None,\n          stream_env=None, input_ds=None, output_row_type=None):\n    """"""\n    Tensorflow training for DataStream\n    :param num_worker: Number of workers\n    :param num_ps: Number of PS\n    :param python_file: The python file which is going to be run\n    :param func: The user-defined function that runs TF training\n    :param properties: User-defined properties\n    :param env_path: Path to the virtual env\n    :param zk_conn: The Zookeeper connection string\n    :param zk_base_path: The Zookeeper base path\n    :param stream_env: The StreamExecutionEnvironment. If it\'s None, this method will create one and execute the job\n                       at the end. Otherwise, caller is responsible to trigger the job execution\n    :param input_ds: The input DataStream\n    :param output_row_type: The RowType for the output DataStream. If it\'s None, a dummy sink will be added to the\n                      output DataStream. Otherwise, caller is responsible to add sink before executing the job.\n    :return: The output DataStream. Currently it\'s always of type Row.\n    """"""\n    tf_config = TFConfig(num_worker, num_ps, python_file, func, properties, env_path, zk_conn, zk_base_path)\n    if stream_env is None:\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n    if input_ds is not None:\n        if isinstance(input_ds, DataStreamSource):\n            input_ds = input_ds._j_datastream_source\n        else:\n            input_ds = input_ds._j_datastream\n    output_ds = get_gateway().jvm.com.alibaba.flink.ml.tensorflow.client.TFUtils.train(stream_env._j_stream_execution_environment,\n                                                                                       input_ds,\n                                                                                       tf_config.java_config(),\n                                                                                       to_java_type_info(output_row_type))\n\n    stream_env.execute()\n    return DataStream(output_ds)\n\n\n\ndef to_java_type_info(output_row_type):\n    if output_row_type is None:\n        return None\n    return types._to_java_type(output_row_type)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_datastream_examples.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport os\nfrom pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment\nfrom tensorflow_on_flink_datastream import train, inference\nfrom tensorflow_on_flink_tfconf import TFCONSTANS\n\n\nclass datastreamTest(object):\n\n    @staticmethod\n    def addTrainStream():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        work_num = 2\n        ps_num = 1\n        python_file = os.getcwd() + ""/../../src/test/python/add.py""\n        func = ""map_func""\n        property = None\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        input_ds = None\n        output_row_type = None\n\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, input_ds, output_row_type)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, input_ds, output_row_type)\n\n    @staticmethod\n    def addTrainChiefAloneStream():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        work_num = 2\n        ps_num = 1\n        python_file = os.getcwd() + ""/../../src/test/python/add.py""\n        func = ""map_func""\n        property = {}\n        property[TFCONSTANS.TF_IS_CHIEF_ALONE] = ""true""\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        input_ds = None\n        output_row_type = None\n\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, input_ds, output_row_type)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, input_ds, output_row_type)\n\n\n\nif __name__ == \'__main__\':\n    datastreamTest.addTrainStream()\n    datastreamTest.addTrainChiefAloneStream()\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_mlconf.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\n\nclass MLCONSTANTS(object):\n    ml_constants = get_gateway().jvm.com.alibaba.flink.ml.util.MLConstants\n\n    SYS_PREFIX = str(ml_constants.SYS_PREFIX)\n    ENCODING_CLASS = str(ml_constants.ENCODING_CLASS)\n    DECODING_CLASS = str(ml_constants.DECODING_CLASS)\n    RECORD_READER_CLASS = str(ml_constants.RECORD_READER_CLASS)\n    RECORD_WRITER_CLASS = str(ml_constants.RECORD_WRITER_CLASS)\n    DATA_BRIDGE_CLASS = str(ml_constants.DATA_BRIDGE_CLASS)\n    ML_RUNNER_CLASS = str(ml_constants.ML_RUNNER_CLASS)\n\n    END_STATUS_NORMAL = str(ml_constants.END_STATUS_NORMAL)\n    END_STATUS_FLINK_ASK_TO_KILL = str(ml_constants.END_STATUS_FLINK_ASK_TO_KILL)\n    END_STATUS_TF_FAIL = str(ml_constants.END_STATUS_TF_FAIL)\n    DISPLAY_NAME_DUMMY_SINK = str(ml_constants.DISPLAY_NAME_DUMMY_SINK)\n    JOB_VERSION = str(ml_constants.JOB_VERSION)\n    PYTHON_FILES = str(ml_constants.PYTHON_FILES)\n    USER_ENTRY_PYTHON_FILE = str(ml_constants.USER_ENTRY_PYTHON_FILE)\n    SCRIPT_RUNNER_CLASS = str(ml_constants.SCRIPT_RUNNER_CLASS)\n    AM_STATE_CLASS = str(ml_constants.AM_STATE_CLASS)\n    AM_STATE_MACHINE_CLASS = str(ml_constants.AM_STATE_MACHINE_CLASS)\n    PYTHON_PATH = str(ml_constants.PYTHON_PATH)\n    PYTHONPATH_ENV = str(ml_constants.PYTHONPATH_ENV)\n    VIRTUAL_ENV_DIR = str(ml_constants.VIRTUAL_ENV_DIR)\n    LD_LIBRARY_PATH = str(ml_constants.LD_LIBRARY_PATH)\n    HADOOP_HDFS_HOME = str(ml_constants.HADOOP_HDFS_HOME)\n    LD_PRELOAD = str(ml_constants.LD_PRELOAD)\n    JAVA_HOME = str(ml_constants.JAVA_HOME)\n    HADOOP_CLASSPATH = str(ml_constants.HADOOP_CLASSPATH)\n    CLASSPATH = str(ml_constants.CLASSPATH)\n    CONFIG_CLUSTER_PATH = str(ml_constants.CONFIG_CLUSTER_PATH)\n    CONFIG_STORAGE_TYPE = str(ml_constants.CONFIG_STORAGE_TYPE)\n    STORAGE_MEMORY = str(ml_constants.STORAGE_MEMORY)\n    STORAGE_ZOOKEEPER = str(ml_constants.STORAGE_ZOOKEEPER)\n    STORAGE_CUSTOM = str(ml_constants.STORAGE_CUSTOM)\n    STORAGE_IMPL_CLASS = str(ml_constants.STORAGE_IMPL_CLASS)\n    CONFIG_ZOOKEEPER_CONNECT_STR = str(ml_constants.CONFIG_ZOOKEEPER_CONNECT_STR)\n    CONFIG_ZOOKEEPER_TIMEOUT = str(ml_constants.CONFIG_ZOOKEEPER_TIMEOUT)\n    CONFIG_TENSORFLOW_FLINK = str(ml_constants.CONFIG_TENSORFLOW_FLINK)\n    CONFIG_ZOOKEEPER_BASE_PATH = str(ml_constants.CONFIG_ZOOKEEPER_BASE_PATH)\n    CROSS_QUEUE_SIZE = str(ml_constants.CROSS_QUEUE_SIZE)\n    CONFIG_EVENT_REPORTER = str(ml_constants.CONFIG_EVENT_REPORTER)\n    CONFIG_JOB_NAME = str(ml_constants.CONFIG_JOB_NAME)\n    CONFIG_JOB_HAS_INPUT = str(ml_constants.CONFIG_JOB_HAS_INPUT)\n    INTUT_DEFAULT_NAME = str(ml_constants.INTUT_DEFAULT_NAME)\n    TIMEOUT = str(ml_constants.TIMEOUT)\n    INT_SIZE = str(ml_constants.INT_SIZE)\n    WORK_DIR = str(ml_constants.WORK_DIR)\n    STARTUP_SCRIPT = str(ml_constants.STARTUP_SCRIPT)\n    STARTUP_SCRIPT_FILE = str(ml_constants.STARTUP_SCRIPT_FILE)\n    ENV_PROPERTY_PREFIX = str(ml_constants.ENV_PROPERTY_PREFIX)\n    SYS_PROPERTY_PREFIX = str(ml_constants.SYS_PROPERTY_PREFIX)\n    REMOTE_CODE_ZIP_FILE = str(ml_constants.REMOTE_CODE_ZIP_FILE)\n    CODE_DIR_NAME = str(ml_constants.CODE_DIR_NAME)\n    CODE_DIR = str(ml_constants.CODE_DIR)\n    USE_DISTRIBUTE_CACHE = str(ml_constants.USE_DISTRIBUTE_CACHE)\n    PYTHON_SCRIPT_DIR = str(ml_constants.PYTHON_SCRIPT_DIR)\n    START_WITH_STARTUP = str(ml_constants.START_WITH_STARTUP)\n    FLINK_VERTEX_NAME = str(ml_constants.FLINK_VERTEX_NAME)\n    CHECKPOINT_DIR = str(ml_constants.CHECKPOINT_DIR)\n    EXPORT_DIR = str(ml_constants.EXPORT_DIR)\n\n    SERVER_RPC_CONTACT_TIMEOUT = str(ml_constants.SERVER_RPC_CONTACT_TIMEOUT)\n    SERVER_RPC_CONTACT_TIMEOUT_DEFAULT = str(ml_constants.SERVER_RPC_CONTACT_TIMEOUT_DEFAULT)\n\n    NODE_IDLE_TIMEOUT = str(ml_constants.NODE_IDLE_TIMEOUT)\n    NODE_IDLE_TIMEOUT_DEFAULT = str(ml_constants.NODE_IDLE_TIMEOUT_DEFAULT)\n\n    AM_REGISTRY_TIMEOUT = str(ml_constants.AM_REGISTRY_TIMEOUT)\n    AM_REGISTRY_TIMEOUT_DEFAULT = str(ml_constants.AM_REGISTRY_TIMEOUT_DEFAULT)\n\n    HEARTBEAT_TIMEOUT = str(ml_constants.HEARTBEAT_TIMEOUT)\n    HEARTBEAT_TIMEOUT_DEFAULT = str(ml_constants.HEARTBEAT_TIMEOUT_DEFAULT)\n\n    SEPERATOR_COMMA= str(ml_constants.SEPERATOR_COMMA)\n    FLINK_HOOK_CLASSNAMES = str(ml_constants.FLINK_HOOK_CLASSNAMES)\n\n    FAILOVER_STRATEGY = str(ml_constants.FAILOVER_STRATEGY)\n    FAILOVER_RESTART_ALL_STRATEGY = str(ml_constants.FAILOVER_RESTART_ALL_STRATEGY)\n    FAILOVER_RESTART_INDIVIDUAL_STRATEGY = str(ml_constants.FAILOVER_RESTART_INDIVIDUAL_STRATEGY)\n    FAILOVER_STRATEGY_DEFAULT = str(ml_constants.FAILOVER_STRATEGY_DEFAULT)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_ops.py,2,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom tensorflow.python.platform import resource_loader\nimport tensorflow as tf\nimport os\n\nqueue_library = os.path.join(resource_loader.get_data_files_path(),\n                             ""libflink_ops.so"")\nprint(queue_library)\nflink_ops = tf.load_op_library(queue_library)\nprint(""load libflink_ops.so success"")\n\n\nclass FlinkTFRecordWriter(object):\n    def __init__(self, address, container=\'\', shared_name=\'\', name=\'FlinkTFRecordWriter\'):\n        self.writer = flink_ops.flink_record_writer(address=address, container=container,\n                                                    shared_name=shared_name, name=name)\n\n    def write(self, tensor_list):\n        res = flink_ops.flink_record_write(writer_handle=self.writer, values=tensor_list)\n        return res\n\n    def close(self):\n        res = flink_ops.flink_record_close(writer_handle=self.writer)\n        return res\n\n\ndef encode_csv(input_list, field_delim=\',\'):\n    return flink_ops.encode_csv(records=input_list, field_delim=field_delim)\n\n\ndef encode_example(input_list, name_list):\n    return flink_ops.encode_example(records=input_list, names=name_list)\n\n\nclass FlinkStreamDataSet(tf.data.TFRecordDataset):\n    def __init__(self, filenames, compression_type=None, buffer_size=None, num_parallel_reads=None):\n        super(FlinkStreamDataSet, self).__init__(filenames, compression_type, buffer_size, num_parallel_reads)\n        super(FlinkStreamDataSet, self).repeat(1)\n\n    def repeat(self, count=None):\n        raise Exception(""repeat func can not set!"")\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_table.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# Based on Pyflink,which means all these interfaces are based on Flink1.9\n\nfrom pyflink.java_gateway import get_gateway\nfrom tensorflow_TFConfig import TFConfig\nfrom pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nfrom pyflink.table.table import Table\n\n\ndef inference(num_worker, num_ps=0, python_file=None, func=None, properties=None, env_path=None, zk_conn=None, zk_base_path=None,\n              stream_env=None, table_env=None, input_table=None, output_schema=None):\n    """"""\n    Tensorflow inference for Table\n    :param num_worker: Number of workers\n    :param num_ps: Number of PS\n    :param python_file: The python file which is going to be run\n    :param func: The user-defined function that runs TF inference. If it\'s None, inference is run via Java API.\n    :param properties: User-defined properties\n    :param env_path: Path to the virtual env\n    :param stream_env: The StreamExecutionEnvironment. If it\'s None, this method will create one and execute the job\n                       at the end. Otherwise, caller is responsible to trigger the job execution\n    :param table_env: The TableEnvironment\n    :param zk_conn: The Zookeeper connection string\n    :param zk_base_path: The Zookeeper base path\n    :param input_table: The input Table\n    :param output_schema: The TableSchema of the output Table. If it\'s None, a dummy sink will be added to the output\n                          Table. Otherwise, caller is responsible to add sink before executing the job.\n    :return: The output Table\n    """"""\n    tf_config = TFConfig(num_worker, num_ps, python_file, func, properties, env_path, zk_conn, zk_base_path)\n    if stream_env is None:\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n    if table_env is None:\n        table_env = StreamTableEnvironment.create(stream_env)\n    if input_table is not None:\n        input_table = input_table._j_table\n    if output_schema is not None:\n        output_schema = output_schema._j_table_schema\n    output_table = get_gateway().jvm.com.alibaba.flink.ml.tensorflow.client.TFUtils.inference(\n                                                                                           stream_env._j_stream_execution_environment,\n                                                                                           table_env._j_tenv,\n                                                                                           input_table,\n                                                                                           tf_config.java_config(),\n                                                                                           output_schema)\n    table_env.execute(job_name=""table inference"")\n    return Table(output_table)\n\n\ndef train(num_worker, num_ps, python_file, func, properties=None, env_path=None, zk_conn=None, zk_base_path=None,\n          stream_env=None, table_env=None, input_table=None, output_schema=None):\n    """"""\n    Tensorflow training for Table\n    :param num_worker: Number of workers\n    :param num_ps: Number of PS\n    :param python_file: The python file which is going to be run\n    :param func: The user-defined function that runs TF training\n    :param properties: User-defined properties\n    :param env_path: Path to the virtual env\n    :param zk_conn: The Zookeeper connection string\n    :param zk_base_path: The Zookeeper base path\n    :param stream_env: The StreamExecutionEnvironment. If it\'s None, this method will create one and execute the job\n                       at the end. Otherwise, caller is responsible to trigger the job execution\n    :param table_env: The TableEnvironment\n    :param input_table: The input Table\n    :param output_schema: The TableSchema of the output Table. If it\'s None, a dummy sink will be added to the output\n                          Table. Otherwise, caller is responsible to add sink before executing the job.\n    :return: The output Table\n    """"""\n    tf_config = TFConfig(num_worker, num_ps, python_file, func, properties, env_path, zk_conn, zk_base_path)\n    if stream_env is None:\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n    if table_env is None:\n        table_env = StreamTableEnvironment.create(stream_env)\n    if input_table is not None:\n        input_table = input_table._j_table\n    if output_schema is not None:\n        output_schema = output_schema._j_table_schema\n    output_table = get_gateway().jvm.com.alibaba.flink.ml.tensorflow.client.TFUtils.train(\n                                                                                       stream_env._j_stream_execution_environment,\n                                                                                       table_env._j_tenv, input_table,\n                                                                                       tf_config.java_config(),\n                                                                                       output_schema)\n    table_env.execute(job_name=""table train"")\n    return Table(output_table)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_table_examples.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nimport os\nfrom pyflink.datastream.stream_execution_environment import StreamExecutionEnvironment\nfrom pyflink.table import StreamTableEnvironment\nfrom pyflink.table.table import TableSchema\nfrom tensorflow_on_flink_table import train, inference\nfrom tensorflow_on_flink_tfconf import TFCONSTANS\nfrom tensorflow_on_flink_mlconf import MLCONSTANTS\nfrom pyflink.table.sources import CsvTableSource\nfrom pyflink.table.types import DataTypes\n\n\nclass tableTest(object):\n\n    @staticmethod\n    def addTrainTable():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        table_env = StreamTableEnvironment.create(stream_env)\n        work_num = 2\n        ps_num = 1\n        python_file = os.getcwd() + ""/../../src/test/python/add.py""\n        func = ""map_func""\n        property = None\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        input_tb = None\n        output_schema = None\n\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n              input_tb, output_schema)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n        #           input_tb, output_schema)\n\n    @staticmethod\n    def addTrainChiefAloneTable():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        table_env = StreamTableEnvironment.create(stream_env)\n        work_num = 2\n        ps_num = 1\n        python_file = os.getcwd() + ""/../../src/test/python/add.py""\n        func = ""map_func""\n        property = {}\n        property[TFCONSTANS.TF_IS_CHIEF_ALONE] = ""ture""\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        input_tb = None\n        output_schema = None\n\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n              input_tb, output_schema)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n        #           input_tb, output_schema)\n\n    @staticmethod\n    def inputOutputTable():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        table_env = StreamTableEnvironment.create(stream_env)\n        work_num = 2\n        ps_num = 1\n        python_file = os.getcwd() + ""/../../src/test/python/input_output.py""\n        property = {}\n        func = ""map_func""\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        property[MLCONSTANTS.ENCODING_CLASS] = ""com.alibaba.flink.ml.operator.coding.RowCSVCoding""\n        property[MLCONSTANTS.DECODING_CLASS] = ""com.alibaba.flink.ml.operator.coding.RowCSVCoding""\n        inputSb = ""INT_32"" + "","" + ""INT_64"" + "","" + ""FLOAT_32"" + "","" + ""FLOAT_64"" + "","" + ""STRING""\n        property[""SYS:csv_encode_types""] = inputSb\n        property[""SYS:csv_decode_types""] = inputSb\n        source_file = os.getcwd() + ""/../../src/test/resources/input.csv""\n        table_source = CsvTableSource(source_file,\n                                      [""a"", ""b"", ""c"", ""d"", ""e""],\n                                      [DataTypes.INT(),\n                                       DataTypes.INT(),\n                                       DataTypes.FLOAT(),\n                                       DataTypes.DOUBLE(),\n                                       DataTypes.STRING()])\n        table_env.register_table_source(""source"", table_source)\n        input_tb = table_env.scan(""source"")\n        output_schema = TableSchema([""a"", ""b"", ""c"", ""d"", ""e""],\n                                    [DataTypes.INT(),\n                                     DataTypes.INT(),\n                                     DataTypes.FLOAT(),\n                                     DataTypes.DOUBLE(),\n                                     DataTypes.STRING()]\n                                    )\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n              input_tb, output_schema)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n        #           input_tb, output_schema)\n\n    @staticmethod\n    def testWorkerZeroFinish():\n        stream_env = StreamExecutionEnvironment.get_execution_environment()\n        table_env = StreamTableEnvironment.create(stream_env)\n        work_num = 3\n        ps_num = 2\n        python_file = os.getcwd() + ""/../../src/test/python/worker_0_finish.py""\n        func = ""map_func""\n        property = None\n        env_path = None\n        zk_conn = None\n        zk_base_path = None\n        input_tb = None\n        output_schema = None\n\n        train(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n              input_tb, output_schema)\n        # inference(work_num, ps_num, python_file, func, property, env_path, zk_conn, zk_base_path, stream_env, table_env,\n        #           input_tb, output_schema)\n\n\nif __name__ == \'__main__\':\n    tableTest.addTrainTable()\n    tableTest.addTrainChiefAloneTable()\n    tableTest.testWorkerZeroFinish()\n    tableTest.inputOutputTable()'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_tfconf.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\n\nclass TFCONSTANS(object):\n\n    J_CONSTANTS = get_gateway().jvm.com.alibaba.flink.ml.tensorflow.util.TFConstants\n\n    TF_PORT = str(J_CONSTANTS.TF_PORT)\n    TF_INFERENCE_EXPORT_PATH = str(J_CONSTANTS.TF_INFERENCE_EXPORT_PATH)\n    TF_INFERENCE_INPUT_TENSOR_NAMES = str(J_CONSTANTS.TF_INFERENCE_INPUT_TENSOR_NAMES)\n    TF_INFERENCE_OUTPUT_TENSOR_NAMES = str(J_CONSTANTS.TF_INFERENCE_OUTPUT_TENSOR_NAMES)\n    TF_INFERENCE_OUTPUT_ROW_FIELDS = str(J_CONSTANTS.TF_INFERENCE_OUTPUT_ROW_FIELDS)\n    TF_INFERENCE_BATCH_SIZE = str(J_CONSTANTS.TF_INFERENCE_BATCH_SIZE)\n    TF_IS_CHIEF_ALONE = str(J_CONSTANTS.TF_IS_CHIEF_ALONE)\n    TF_IS_CHIEF_ROLE = str(J_CONSTANTS.TF_IS_CHIEF_ROLE)\n    TENSORBOART_PORT = str(J_CONSTANTS.TENSORBOART_PORT)\n    INPUT_TF_EXAMPLE_CONFIG = str(J_CONSTANTS.INPUT_TF_EXAMPLE_CONFIG)\n    OUTPUT_TF_EXAMPLE_CONFIG = str(J_CONSTANTS.OUTPUT_TF_EXAMPLE_CONFIG)\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tfrecord_source_func.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.stream.functions.source import JavaSourceFunction\nfrom pyflink.util.type_util import TypesUtil\n\n\nclass TFRSourceFunc(JavaSourceFunction):\n    def __init__(self, paths, epochs, out_row_type, converters):\n        src_func_clz_name = \'com.alibaba.flink.tensorflow.hadoop.io.TFRToRowSourceFunc\'\n        src_func_clz = TypesUtil.class_for_name(src_func_clz_name)\n        j_paths = TypesUtil._convert_py_list_to_java_array(\'java.lang.String\', paths)\n        j_converters = []\n        for converter in converters:\n            j_converters.append(converter.java_converter())\n        j_converters = TypesUtil._convert_py_list_to_java_array(\n            \'com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper$ScalarConverter\', j_converters)\n        j_row_type = TypesUtil.to_java_sql_type(out_row_type)\n        super(TFRSourceFunc, self).__init__(src_func_clz(j_paths, epochs, j_row_type, j_converters))\n'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tfrecord_table_source.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\nfrom pyflink.java_gateway import get_gateway\nfrom enum import unique, Enum\nfrom pyflink.util.type_util import TypesUtil\nfrom pyflink.table.table_source import JavaTableSource\n\n\n@unique\nclass ScalarConverter(Enum):\n    FIRST = 1\n    LAST = 2\n    MAX = 3\n    MIN = 4\n    ONE_HOT = 5\n\n    def java_converter(self):\n        if self is ScalarConverter.FIRST:\n            return get_gateway().jvm.com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper.ScalarConverter.FIRST\n        if self is ScalarConverter.LAST:\n            return get_gateway().jvm.com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper.ScalarConverter.LAST\n        if self is ScalarConverter.MAX:\n            return get_gateway().jvm.com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper.ScalarConverter.MAX\n        if self is ScalarConverter.MIN:\n            return get_gateway().jvm.com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper.ScalarConverter.MIN\n        if self is ScalarConverter.ONE_HOT:\n            return get_gateway().jvm.com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper.ScalarConverter.ONE_HOT\n        raise Exception(\'Unknown converter \' + self.name)\n\n\nclass TFRTableSource(JavaTableSource):\n    def __init__(self, paths, epochs, out_row_type, converters):\n        table_src_clz_name = \'com.alibaba.flink.tensorflow.hadoop.io.TFRToRowTableSource\'\n        table_src_clz = TypesUtil.class_for_name(table_src_clz_name)\n        j_paths = TypesUtil._convert_py_list_to_java_array(\'java.lang.String\', paths)\n        j_converters = []\n        for converter in converters:\n            j_converters.append(converter.java_converter())\n        j_converters = TypesUtil._convert_py_list_to_java_array(\n            \'com.alibaba.flink.tensorflow.hadoop.io.TFRExtractRowHelper$ScalarConverter\', j_converters)\n        j_row_type = TypesUtil.to_java_sql_type(out_row_type)\n        super(TFRTableSource, self).__init__(table_src_clz(j_paths, epochs, j_row_type, j_converters))\n\n    def register_table(self, table_env, name=None):\n        if name is None:\n            import time\n            name = \'tfr_table_src_\' + str(int(round(time.time() * 1000)))\n        table_env.register_table_source(name=name, table_source=self)\n        return table_env.scan(name)\n'"
flink-ml-examples/src/test/python/all_reduce_test.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function\nimport os\nimport sys\nimport torch\nimport torch.distributed as dist\nimport json\n\n"""""" All-Reduce example.""""""\n\n\ndef get_master_address(cluster_str):\n    cluster_json = json.loads(cluster_str)\n    worker_props = cluster_json[\'job\'][0][\'tasks\'][\'0\'][\'props\']\n    return worker_props[\'SYS:pytorch_master_ip\'], worker_props[\'SYS:pytorch_master_port\']\n\n\ndef map_func(context):\n    print (\'index:\', context.index)\n    print(""context:"", context)\n    cluster_str = context.properties[\'cluster\']\n    master_ip, master_port = get_master_address(cluster_str)\n    print(\'master:\', master_ip, master_port)\n    world_size = int(context.roleParallelism[\'worker\'])\n    distributed_flag = torch.distributed.is_available()\n    print(\'distributed_flag:\', distributed_flag)\n    if distributed_flag:\n        os.environ[\'MASTER_ADDR\'] = master_ip\n        os.environ[\'MASTER_PORT\'] = master_port\n        dist.init_process_group(dist.Backend.GLOO, rank=context.index, world_size=world_size)\n        tensor = torch.ones(1)\n        dist.all_reduce(tensor, op=dist.reduce_op.SUM)\n        print(\'Rank \', context.index, \' has data \', tensor[0])\n    sys.stdout.flush()\n    print (""job num:"", context.roleParallelism)\n    print (""world size:"", context.roleParallelism[\'worker\'])\n    sys.stdout.flush()\n'"
flink-ml-examples/src/test/python/greeter.py,0,"b""from __future__ import print_function\nimport sys\n\n\ndef map_func(context):\n    print('hello from greeter')\n    print('index:', context.get_index())\n    sys.stdout.flush()\n"""
flink-ml-examples/src/test/python/mnist_data_setup.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy\nimport tensorflow as tf\nimport math\nimport sys\nfrom tensorflow.contrib.learn.python.learn.datasets import mnist\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef write_mnist_data(input_images, input_labels, output, partitions):\n    with open(input_images, \'rb\') as f:\n        images = numpy.array(mnist.extract_images(f))\n\n    with open(input_labels, \'rb\') as f:\n        labels = numpy.array(mnist.extract_labels(f, one_hot=True))\n\n    shape = images.shape\n    print(""images.shape: {0}"".format(shape))\n    print(""labels.shape: {0}"".format(labels.shape))\n\n    images = images.reshape(shape[0], shape[1], shape[2])\n    num_per_part = int(math.ceil(float(shape[0]) / partitions))\n    seq = 0\n    filename = output + ""/"" + str(seq) + "".tfrecords""\n    writer = tf.python_io.TFRecordWriter(filename)\n\n    for i in range(shape[0]):\n        if i != 0 and i % num_per_part == 0:\n            writer.close()\n            seq += 1\n            filename = output + ""/"" + str(seq) + "".tfrecords""\n            writer = tf.python_io.TFRecordWriter(filename)\n        image_raw = images[i].tostring()\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'image_raw\': _bytes_feature(image_raw),\n            \'label\': _int64_feature(labels[i].astype(int))\n        }))\n        writer.write(example.SerializeToString())\n    writer.close()\n\n\nif __name__ == ""__main__"":\n\n    if \'args\' in globals():\n        global args\n        input = args.get(""input"")\n        output = args.get(""output"")\n        partitions = args.get(""partitions"")\n    else:\n        assert len(sys.argv) == 4, \'Invalid cmd line argument \' + str(sys.argv)\n        input = sys.argv[1]\n        output = sys.argv[2]\n        partitions = sys.argv[3]\n\n    write_mnist_data(input + ""/train-images-idx3-ubyte.gz"",\n                     input + ""/train-labels-idx1-ubyte.gz"",\n                     output + ""/train"", int(partitions))\n\n    write_mnist_data(input + ""/t10k-images-idx3-ubyte.gz"",\n                     input + ""/t10k-labels-idx1-ubyte.gz"",\n                     output + ""/test"", int(partitions))\n'"
flink-ml-examples/src/test/python/mnist_dist.py,41,"b'# Distributed MNIST on grid based on TensorFlow MNIST example\n\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport math\nimport numpy\nimport json\nimport sys\nfrom flink_ml_tensorflow.tensorflow_context import *\n\n\ndef export_saved_model(sess, export_dir, tag_set, signatures):\n    g = sess.graph\n    g._unsafe_unfinalize()\n    try:\n        tf.gfile.DeleteRecursively(export_dir)\n    except tf.errors.OpError:\n        pass\n    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n\n    signature_def_map = {}\n    for key, sig in signatures.items():\n        signature_def_map[key] = tf.saved_model.signature_def_utils.build_signature_def(\n            inputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                    sig[\'inputs\'].items()},\n            outputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                     sig[\'outputs\'].items()},\n            method_name=sig[\'method_name\'] if \'method_name\' in sig else key)\n\n        builder.add_meta_graph_and_variables(\n            sess,\n            tag_set.split(\',\'),\n            signature_def_map=signature_def_map,\n            clear_devices=True)\n\n        g.finalize()\n        builder.save()\n\n\nclass ExportHook(tf.train.SessionRunHook):\n    def __init__(self, export_dir, input_tensor, output_tensor):\n        self.export_dir = export_dir\n        self.input_tensor = input_tensor\n        self.output_tensor = output_tensor\n\n    def end(self, session):\n        print(""{} ======= Exporting to: {}"".format(datetime.now().isoformat(), self.export_dir))\n        signatures = {\n            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n                \'inputs\': {\'image\': self.input_tensor},\n                \'outputs\': {\'prediction\': self.output_tensor},\n                \'method_name\': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n            }\n        }\n        export_saved_model(session, self.export_dir, tf.saved_model.tag_constants.SERVING,\n                           signatures)\n        print(""{} ======= Done exporting"".format(datetime.now().isoformat()))\n\n\ndef decode(serialized_example):\n    feature = {\'image_raw\': tf.FixedLenFeature([], tf.string),\n               \'label\': tf.VarLenFeature(tf.int64)}\n    features = tf.parse_single_example(serialized_example, features=feature)\n    image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    label = features[\'label\'].values\n    return image, label\n\n\ndef input_iter(filename, batch_size, num_epochs):\n    if not num_epochs:\n        num_epochs = None\n\n    dataset = tf.data.TFRecordDataset(filename)\n    dataset = dataset.map(decode)\n    dataset = dataset.repeat(num_epochs)\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_one_shot_iterator()\n    return iterator\n\n\ndef map_fun(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n\n    props = context.properties\n    batch_size = int(props.get(""batch_size""))\n    input_dir = props.get(""input"")\n    epochs = int(props.get(""epochs""))\n    checkpoint_dir = props.get(""checkpoint_dir"")\n    export_dir = props.get(""export_dir"")\n    print (""input:"" + input_dir)\n    print (""checkpoint_dir:"" + checkpoint_dir)\n    print (""export_dir:"" + export_dir)\n    sys.stdout.flush()\n\n    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n    # if job_name == ""ps"":\n    #   time.sleep((num_worker + 1) * 5)\n\n    # Parameters\n    IMAGE_PIXELS = 28\n    hidden_units = 128\n\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n\n    def feed_dict(images, labels):\n        xs = numpy.array(images)\n        xs = xs.astype(numpy.float32)\n        xs = xs / 255.0\n        ys = numpy.array(labels)\n        ys = ys.astype(numpy.uint8)\n        return (xs, ys)\n\n    if job_name == ""ps"":\n        from time import sleep\n        while True:\n            sleep(1)\n\n    elif job_name == ""worker"":\n\n        # Assigns ops to the local worker by default.\n        with tf.device(\n                tf.train.replica_device_setter(worker_device=""/job:worker/task:"" + str(index), cluster=cluster)):\n\n            # Placeholders or QueueRunner/Readers for input data\n            x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS], name=""x"")\n            y_ = tf.placeholder(tf.float32, [None, 10], name=""y_"")\n\n            # Variables of the hidden layer\n            hid_w = tf.Variable(\n                tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units], stddev=1.0 / IMAGE_PIXELS),\n                name=""hid_w"")\n            hid_b = tf.Variable(tf.zeros([hidden_units]), name=""hid_b"")\n            hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n            hid = tf.nn.relu(hid_lin)\n\n            # Variables of the softmax layer\n            sm_w = tf.Variable(\n                tf.truncated_normal([hidden_units, 10], stddev=1.0 / math.sqrt(hidden_units)),\n                name=""sm_w"")\n            sm_b = tf.Variable(tf.zeros([10]), name=""sm_b"")\n            y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n\n            global_step = tf.train.get_or_create_global_step()\n\n            loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n            train_op = tf.train.AdagradOptimizer(0.01).minimize(loss, global_step=global_step)\n\n            # Test trained model\n            label = tf.argmax(y_, 1, name=""label"")\n            prediction = tf.argmax(y, 1, name=""prediction"")\n            correct_prediction = tf.equal(prediction, label)\n\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""accuracy"")\n\n            iter = input_iter(input_dir + ""/"" + str(index) + "".tfrecords"", batch_size, epochs)\n            next_batch = iter.get_next()\n\n            is_chief = (index == 0)\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                         device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n\n            # The MonitoredTrainingSession takes care of session initialization, restoring from\n            #  a checkpoint, and closing when done or an error occurs\n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief,\n                                                         checkpoint_dir=checkpoint_dir,\n                                                         stop_grace_period_secs=10, max_wait_secs=300,\n                                                         config=sess_config,\n                                                         chief_only_hooks=[ExportHook(export_dir, x, prediction)])\n            processed = 0\n            while not mon_sess.should_stop():\n                # Run a training step asynchronously\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                try:\n                    images, labels = mon_sess.run(next_batch)\n                    processed += images.shape[0]\n                except tf.errors.OutOfRangeError:\n                    break\n\n                batch_xs, batch_ys = feed_dict(images, labels)\n                feed = {x: batch_xs, y_: batch_ys}\n\n                if len(batch_xs) > 0 and not mon_sess.should_stop():\n                    _, step = mon_sess.run([train_op, global_step], feed_dict=feed)\n                    if step % 100 == 0:\n                        print(""{0}, Task {1} step: {2} accuracy: {3}"".format(\n                            datetime.now().isoformat(), index, step,\n                            mon_sess.run(accuracy, {x: batch_xs, y_: batch_ys})))\n                        sys.stdout.flush()\n\n            print(str(processed) + "" records processed."")\n            print(""{0} stopping MonitoredTrainingSession"".format(datetime.now().isoformat()))\n            mon_sess.close()\n\n    SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-examples/src/test/python/mnist_dist_with_input.py,38,"b'# Distributed MNIST on grid based on TensorFlow MNIST example\n\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport math\nimport numpy\nimport json\nimport sys\nfrom flink_ml_tensorflow.tensorflow_on_flink_ops import *\nfrom flink_ml_tensorflow.tensorflow_context import *\n\n\ndef export_saved_model(sess, export_dir, tag_set, signatures):\n    g = sess.graph\n    g._unsafe_unfinalize()\n    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n\n    signature_def_map = {}\n    for key, sig in signatures.items():\n        signature_def_map[key] = tf.saved_model.signature_def_utils.build_signature_def(\n            inputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                    sig[\'inputs\'].items()},\n            outputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                     sig[\'outputs\'].items()},\n            method_name=sig[\'method_name\'] if \'method_name\' in sig else key)\n\n        builder.add_meta_graph_and_variables(\n            sess,\n            tag_set.split(\',\'),\n            signature_def_map=signature_def_map,\n            clear_devices=True)\n\n        g.finalize()\n        builder.save()\n\n\nclass ExportHook(tf.train.SessionRunHook):\n    def __init__(self, export_dir, input_tensor, output_tensor):\n        self.export_dir = export_dir\n        self.input_tensor = input_tensor\n        self.output_tensor = output_tensor\n\n    def end(self, session):\n        print(""{} ======= Exporting to: {}"".format(datetime.now().isoformat(), self.export_dir))\n        signatures = {\n            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n                \'inputs\': {\'image\': self.input_tensor},\n                \'outputs\': {\'prediction\': self.output_tensor},\n                \'method_name\': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n            }\n        }\n        export_saved_model(session, self.export_dir, tf.saved_model.tag_constants.SERVING,\n                           signatures)\n        print(""{} ======= Done exporting"".format(datetime.now().isoformat()))\n\n\ndef decode(features):\n    image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    label = tf.one_hot(features[\'label\'], 10, on_value=1)\n    return image, label\n\n\ndef input_iter(context, batch_size):\n    features = {\'label\': tf.FixedLenFeature([], tf.int64), \'image_raw\': tf.FixedLenFeature([], tf.string)}\n    dataset = context.flink_stream_dataset()\n    dataset = dataset.map(lambda record: tf.parse_single_example(record, features=features))\n    dataset = dataset.map(decode)\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_one_shot_iterator()\n    return iterator\n\n\ndef map_fun(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    task_index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n\n    props = context.properties\n    batch_size = int(props.get(""batch_size""))\n    checkpoint_dir = props.get(""checkpoint_dir"")\n    export_dir = props.get(""export_dir"")\n\n    # Parameters\n    IMAGE_PIXELS = 28\n    hidden_units = 128\n\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=task_index)\n\n    def feed_dict(images, labels):\n        xs = numpy.array(images)\n        xs = xs.astype(numpy.float32)\n        xs = xs / 255.0\n        ys = numpy.array(labels)\n        ys = ys.astype(numpy.uint8)\n        return (xs, ys)\n\n    if job_name == ""ps"":\n        from time import sleep\n        while True:\n            sleep(1)\n    elif job_name == ""worker"":\n\n        # Assigns ops to the local worker by default.\n        with tf.device(\n                tf.train.replica_device_setter(worker_device=""/job:worker/task:"" + str(task_index), cluster=cluster)):\n\n            # Placeholders or QueueRunner/Readers for input data\n            x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS], name=""x"")\n            y_ = tf.placeholder(tf.float32, [None, 10], name=""y_"")\n\n            # Variables of the hidden layer\n            hid_w = tf.Variable(\n                tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units], stddev=1.0 / IMAGE_PIXELS),\n                name=""hid_w"")\n            hid_b = tf.Variable(tf.zeros([hidden_units]), name=""hid_b"")\n            hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n            hid = tf.nn.relu(hid_lin)\n\n            # Variables of the softmax layer\n            sm_w = tf.Variable(\n                tf.truncated_normal([hidden_units, 10], stddev=1.0 / math.sqrt(hidden_units)),\n                name=""sm_w"")\n            sm_b = tf.Variable(tf.zeros([10]), name=""sm_b"")\n            y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n\n            global_step = tf.train.get_or_create_global_step()\n\n            loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n            train_op = tf.train.AdagradOptimizer(0.01).minimize(loss, global_step=global_step)\n\n            # Test trained model\n            label = tf.argmax(y_, 1, name=""label"")\n            prediction = tf.argmax(y, 1, name=""prediction"")\n            correct_prediction = tf.equal(prediction, label)\n\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""accuracy"")\n\n            iter = input_iter(tf_context, batch_size)\n            next_batch = iter.get_next()\n\n            is_chief = (task_index == 0)\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                         device_filters=[""/job:ps"", ""/job:worker/task:%d"" % task_index])\n\n            # The MonitoredTrainingSession takes care of session initialization, restoring from\n            #  a checkpoint, and closing when done or an error occurs\n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief,\n                                                         checkpoint_dir=checkpoint_dir,\n                                                         stop_grace_period_secs=10, max_wait_secs=300,\n                                                         config=sess_config,\n                                                         chief_only_hooks=[ExportHook(export_dir, x,\n                                                                                      prediction)])\n            processed = 0\n            while not mon_sess.should_stop():\n                # Run a training step asynchronously\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n                try:\n                    images, labels = mon_sess.run(next_batch)\n                    processed += images.shape[0]\n                    # print mon_sess.run(next_batch)\n                except tf.errors.OutOfRangeError:\n                    break\n\n                batch_xs, batch_ys = feed_dict(images, labels)\n                feed = {x: batch_xs, y_: batch_ys}\n\n                if len(batch_xs) > 0 and not mon_sess.should_stop():\n                    _, step = mon_sess.run([train_op, global_step], feed_dict=feed)\n                    if step % 100 == 0:\n                        print(""{0}, Task {1} step: {2} accuracy: {3}"".format(\n                            datetime.now().isoformat(), task_index, step,\n                            mon_sess.run(accuracy, {x: batch_xs, y_: batch_ys})))\n                        sys.stdout.flush()\n\n            print(str(processed) + "" records processed."")\n            print(""{0} stopping MonitoredTrainingSession"".format(datetime.now().isoformat()))\n            mon_sess.close()\n\n    SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-examples/src/test/python/mnist_python_api.py,0,"b'import time\nfrom tensorflow_on_flink import tensorflow_on_flink_table\nfrom tensorflow_on_flink import tensorflow_on_flink_datastream\nfrom tensorflow_on_flink.tfrecord_table_source import ScalarConverter\nfrom tensorflow_on_flink.tfrecord_table_source import TFRTableSource\nfrom tensorflow_on_flink.table_sinks import LogTableStreamSink, LogInferAccSink\nfrom tensorflow_on_flink.tfrecord_source_func import TFRSourceFunc\nfrom tensorflow_on_flink.sink_funcs import LogSink\nimport mnist_dist\nimport mnist_dist_with_input\nimport mnist_table_inference\nfrom pyflink.environment import StreamExecutionEnvironment\nfrom pyflink.table.table_environment import TableEnvironment\nfrom pyflink.table.table_schema import TableSchema\nfrom pyflink.util.type_util import TypesUtil\nfrom pyflink.table.functions import JavaTableFunction\nfrom pyflink.sql.data_type import StringType, IntegerType, RowType, LongType\nfrom tensorflow_on_flink.tensorflow_on_flink_conf import *\nimport os\n\n\ndef get_root_path():\n    return get_gateway().jvm.com.alibaba.flink.tensorflow.util.TestUtil.getProjectRootPath()\n\n\nexport_path = get_root_path() + \'/examples/target/export/0\'\n\n\ndef build_props(version=None):\n    root_path = get_root_path()\n    if version is None:\n        version = str(int(round(time.time() * 1000)))\n    props = {""batch_size"": ""32"", ""input"": root_path + ""/examples/target/data/train/"", ""epochs"": ""1"",\n             ""checkpoint_dir"": root_path + ""/examples/target/ckpt/"" + version,\n             ""export_dir"": root_path + ""/examples/target/export/"" + version}\n    return props\n\n\ndef prep_data():\n    get_gateway().jvm.com.alibaba.flink.tensorflow.mnist.MnistDataUtil.prepareData()\n\n\ndef start_zk_server(port=2181):\n    return get_gateway().jvm.org.apache.curator.test.TestingServer(port, True)\n\n\ndef generate_model():\n    if not os.path.exists(export_path):\n        tensorflow_on_flink_table.train(num_worker=1, num_ps=1, func=mnist_dist.map_fun, properties=build_props(\'0\'))\n\n\ndef table_no_input():\n    prep_data()\n    testing_server = start_zk_server()\n    tensorflow_on_flink_table.train(num_worker=1, num_ps=1, func=mnist_dist.map_fun, properties=build_props(),\n                                    zk_conn=""localhost:2181"")\n    testing_server.stop()\n\n\ndef datastream_no_input():\n    prep_data()\n    testing_server = start_zk_server()\n    tensorflow_on_flink_datastream.train(num_worker=1, num_ps=1, func=mnist_dist.map_fun, properties=build_props())\n    testing_server.stop()\n\n\ndef datastream_inference_input():\n    prep_data()\n    testing_server = start_zk_server()\n    generate_model()\n    stream_env = StreamExecutionEnvironment.get_execution_environment()\n    test_data_path = ""file://"" + get_root_path() + ""/examples/target/data/test/""\n    paths = [test_data_path + ""0.tfrecords"", test_data_path + ""1.tfrecords""]\n    src_row_type = RowType([StringType(), IntegerType()], [""image_raw"", ""label""])\n    input_ds = stream_env.add_source(source_func=TFRSourceFunc(paths=paths, epochs=1, out_row_type=src_row_type,\n                                                               converters=[ScalarConverter.FIRST,\n                                                                           ScalarConverter.ONE_HOT]))\n    input_ds.set_parallelism(len(paths))\n    out_row_type = RowType([IntegerType(), IntegerType()], [\'label_org\', \'predict_label\'])\n    output_ds = tensorflow_on_flink_datastream.inference(num_worker=1, func=mnist_table_inference.map_fun,\n                                                         properties=build_props(\'0\'), stream_env=stream_env,\n                                                         input_ds=input_ds, output_row_type=out_row_type)\n    output_ds.add_sink(LogSink())\n    stream_env.execute()\n    testing_server.stop()\n\n\ndef datastream_with_input():\n    prep_data()\n    testing_server = start_zk_server()\n    stream_env = StreamExecutionEnvironment.get_execution_environment()\n    train_data_path = ""file://"" + get_root_path() + ""/examples/target/data/train/""\n    paths = [train_data_path + ""0.tfrecords"", train_data_path + ""1.tfrecords""]\n    src_row_type = RowType([StringType(), IntegerType()], [""image_raw"", ""label""])\n    input_ds = stream_env.add_source(TFRSourceFunc(paths=paths, epochs=1, out_row_type=src_row_type,\n                                                   converters=[ScalarConverter.FIRST,\n                                                               ScalarConverter.ONE_HOT]))\n    input_ds.set_parallelism(len(paths))\n    tensorflow_on_flink_datastream.train(num_worker=1, num_ps=1, func=mnist_dist_with_input.map_fun,\n                                         properties=build_props(), input_ds=input_ds, stream_env=stream_env)\n    stream_env.execute()\n    testing_server.stop()\n\n\ndef table_with_input():\n    prep_data()\n    testing_server = start_zk_server()\n    stream_env = StreamExecutionEnvironment.get_execution_environment()\n    table_env = TableEnvironment.get_table_environment(stream_env)\n    stream_env.set_parallelism(2)\n    train_data_path = ""file://"" + get_root_path() + ""/examples/target/data/train/""\n    paths = [train_data_path + ""0.tfrecords"", train_data_path + ""1.tfrecords""]\n    out_row_type = RowType([StringType(), IntegerType()], [""image_raw"", ""label""])\n    table_src = TFRTableSource(paths=paths, epochs=1, out_row_type=out_row_type,\n                               converters=[ScalarConverter.FIRST, ScalarConverter.ONE_HOT])\n    input_table = table_src.register_table(table_env=table_env)\n    tensorflow_on_flink_table.train(num_worker=1, num_ps=1, func=mnist_dist_with_input.map_fun,\n                                    properties=build_props(), stream_env=stream_env,\n                                    table_env=table_env, input_table=input_table)\n    table_env.generate_stream_graph()\n    stream_env.execute()\n    testing_server.stop()\n\n\ndef table_inference():\n    prep_data()\n    testing_server = start_zk_server()\n    generate_model()\n    stream_env = StreamExecutionEnvironment.get_execution_environment()\n    table_env = TableEnvironment.get_table_environment(stream_env)\n    stream_env.set_parallelism(2)\n    test_data_path = ""file://"" + get_root_path() + ""/examples/target/data/test/""\n    paths = [test_data_path + ""0.tfrecords"", test_data_path + ""1.tfrecords""]\n    src_row_type = RowType([StringType(), IntegerType()], [""image_raw"", ""label""])\n    table_src = TFRTableSource(paths=paths, epochs=1, out_row_type=src_row_type,\n                               converters=[ScalarConverter.FIRST, ScalarConverter.ONE_HOT])\n    input_table = table_src.register_table(table_env=table_env)\n    builder = TableSchema.Builder()\n    builder.column(name=\'label_org\', data_type=IntegerType()).column(name=\'predict_label\', data_type=IntegerType())\n    output_schema = builder.build()\n    output_table = tensorflow_on_flink_table.inference(num_worker=2, func=mnist_table_inference.map_fun,\n                                                       properties=build_props(\'0\'), stream_env=stream_env,\n                                                       table_env=table_env, input_table=input_table,\n                                                       output_schema=output_schema)\n    output_table.write_to_sink(LogTableStreamSink())\n    table_env.generate_stream_graph()\n    stream_env.execute()\n    testing_server.stop()\n\n\ndef java_inference_extract_func():\n    func_clz_name = \'com.alibaba.flink.tensorflow.client.MnistTFRExtractRowForJavaFunction\'\n    func_clz = TypesUtil.class_for_name(func_clz_name)\n    return JavaTableFunction(func_clz())\n\n\ndef table_java_inference():\n    prep_data()\n    testing_server = start_zk_server()\n    generate_model()\n    stream_env = StreamExecutionEnvironment.get_execution_environment()\n    table_env = TableEnvironment.get_table_environment(stream_env)\n    stream_env.set_parallelism(2)\n    train_data_path = ""file://"" + get_root_path() + ""/examples/target/data/test/""\n    paths = [train_data_path + ""0.tfrecords"", train_data_path + ""1.tfrecords""]\n    src_row_type = RowType([StringType(), IntegerType()], [""image_raw"", ""label""])\n    table_src = TFRTableSource(paths=paths, epochs=1, out_row_type=src_row_type,\n                               converters=[ScalarConverter.FIRST, ScalarConverter.ONE_HOT])\n    tfr_tbl_name = ""tfr_input_table""\n    table_env.register_table_source(tfr_tbl_name, table_src)\n    ext_func_name = ""tfr_extract""\n    table_env.register_function(ext_func_name, java_inference_extract_func())\n    out_cols = \'image,org_label\'\n    in_cols = \',\'.join(src_row_type.fields_names)\n    extracted = table_env.sql_query(\n        \'select {} from {}, LATERAL TABLE({}({})) as T({})\'.format(out_cols, tfr_tbl_name, ext_func_name, in_cols,\n                                                                   out_cols))\n    builder = TableSchema.Builder()\n    builder.column(name=\'real_label\', data_type=LongType()).column(name=\'predicted_label\', data_type=LongType())\n    output_schema = builder.build()\n    props = build_props(\'0\')\n    props[TF_INFERENCE_EXPORT_PATH] = export_path\n    props[TF_INFERENCE_INPUT_TENSOR_NAMES] = \'image\'\n    props[TF_INFERENCE_OUTPUT_TENSOR_NAMES] = \'prediction\'\n    props[TF_INFERENCE_OUTPUT_ROW_FIELDS] = \',\'.join([\'org_label\', \'prediction\'])\n    output_table = tensorflow_on_flink_table.inference(num_worker=2, properties=props, stream_env=stream_env,\n                                                       table_env=table_env, input_table=extracted,\n                                                       output_schema=output_schema)\n    output_table.write_to_sink(LogInferAccSink())\n    table_env.generate_stream_graph()\n    stream_env.execute()\n    testing_server.stop()\n\n\nif __name__ == ""__main__"":\n    table_no_input()\n    datastream_no_input()\n    datastream_with_input()\n    table_with_input()\n    table_inference()\n    table_java_inference()\n    datastream_inference_input()\n'"
flink-ml-examples/src/test/python/mnist_table_inference.py,13,"b'# Distributed MNIST on grid based on TensorFlow MNIST example\n\nfrom __future__ import print_function\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport math\nimport numpy\nimport json\nimport sys\nfrom flink_ml_tensorflow.tensorflow_on_flink_ops import *\nfrom flink_ml_tensorflow.tensorflow_context import *\n\n\ndef test_log(message):\n    print(message)\n    sys.stdout.flush()\n\n\ndef decode(features):\n    image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    label = tf.one_hot(features[\'label\'], 10, on_value=1)\n    return image, label\n\n\ndef feed_dict(images, labels):\n    xs = numpy.array(images)\n    xs = xs.astype(numpy.float32)\n    xs = xs / 255.0\n    ys = numpy.array(labels)\n    ys = ys.astype(numpy.uint8)\n    return (xs, ys)\n\n\ndef input_iter(context, batch_size):\n    features = {\'label\': tf.FixedLenFeature([], tf.int64), \'image_raw\': tf.FixedLenFeature([], tf.string)}\n    dataset = context.flink_stream_dataset()\n    dataset = dataset.map(lambda record: tf.parse_single_example(record, features=features))\n    dataset = dataset.map(decode)\n    dataset = dataset.batch(batch_size)\n    iterator = dataset.make_one_shot_iterator()\n    return iterator\n\n\ndef map_fun(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    task_index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n\n    props = context.properties\n    batch_size = int(props.get(""batch_size""))\n    epochs = int(props.get(""epochs""))\n    checkpoint_dir = props.get(""checkpoint_dir"")\n    export_dir = props.get(""export_dir"")\n\n    # Parameters\n    IMAGE_PIXELS = 28\n    hidden_units = 128\n\n    session = tf.Session()\n    signature_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n    input_key = \'image\'\n    output_key = \'prediction\'\n    # saved_model_dir = \'/home/chen/code/TensorFlowOnFlink/target/export/1539071075170/\'\n    test_log(""before load"")\n    meta_graph_def = tf.saved_model.loader.load(session, [tf.saved_model.tag_constants.SERVING], export_dir=export_dir)\n    test_log(""after load"")\n    signature = meta_graph_def.signature_def\n\n    x_tensor_name = signature[signature_key].inputs[input_key].name\n    test_log(x_tensor_name)\n    y_tensor_name = signature[signature_key].outputs[output_key].name\n    test_log(y_tensor_name)\n    x = session.graph.get_tensor_by_name(x_tensor_name)\n    y = session.graph.get_tensor_by_name(y_tensor_name)\n\n    # write_feed, write_op, close_op = context.getOutputWriterOp()\n    write_feed = tf.placeholder(dtype=tf.string)\n    write_op, close_op = tf_context.output_writer_op([write_feed])\n    iter = input_iter(tf_context, batch_size)\n    next_batch = iter.get_next()\n    prediction = tf.argmax(next_batch[1], 1, name=""prediction"")\n\n    while True:\n        try:\n            images, labels, labels_ = session.run([next_batch[0], next_batch[1], prediction])\n        except tf.errors.OutOfRangeError:\n            break\n        batch_xs, batch_ys = feed_dict(images, labels)\n        feed = {x: batch_xs}\n        # test_log(feed_data)\n        # input_res = session.run(x_, feed_dict={input_data: feed_data})\n        # print ""input_data"", input_res\n        y_res = session.run(y, feed_dict=feed)\n        # print ""y_res"", y_res, ""y_org"", labels_\n        # sys.stdout.flush()\n        for i in range(len(y_res)):\n            example = tf.train.Example(features=tf.train.Features(\n                feature={\n                    \'predict_label\': tf.train.Feature(int64_list=tf.train.Int64List(value=[y_res[i]])),\n                    \'label_org\': tf.train.Feature(int64_list=tf.train.Int64List(value=[labels_[i]])),\n                }))\n            # print ""write:"", i\n            sys.stdout.flush()\n            session.run(write_op, feed_dict={write_feed: example.SerializeToString()})\n\n    session.run(close_op)\n\n    SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/setup.py,0,"b'#!/usr/bin/env python\n\n# Setup script for PyPI; use CMakeFile.txt to build extension modules\n\nfrom setuptools import setup\nfrom distutils.command.install_headers import install_headers\nfrom pybind11 import __version__\nimport os\n\n# Prevent installation of pybind11 headers by setting\n# PYBIND11_USE_CMAKE.\nif os.environ.get(\'PYBIND11_USE_CMAKE\'):\n    headers = []\nelse:\n    headers = [\n        \'include/pybind11/detail/class.h\',\n        \'include/pybind11/detail/common.h\',\n        \'include/pybind11/detail/descr.h\',\n        \'include/pybind11/detail/init.h\',\n        \'include/pybind11/detail/internals.h\',\n        \'include/pybind11/detail/typeid.h\',\n        \'include/pybind11/attr.h\',\n        \'include/pybind11/buffer_info.h\',\n        \'include/pybind11/cast.h\',\n        \'include/pybind11/chrono.h\',\n        \'include/pybind11/common.h\',\n        \'include/pybind11/complex.h\',\n        \'include/pybind11/eigen.h\',\n        \'include/pybind11/embed.h\',\n        \'include/pybind11/eval.h\',\n        \'include/pybind11/functional.h\',\n        \'include/pybind11/iostream.h\',\n        \'include/pybind11/numpy.h\',\n        \'include/pybind11/operators.h\',\n        \'include/pybind11/options.h\',\n        \'include/pybind11/pybind11.h\',\n        \'include/pybind11/pytypes.h\',\n        \'include/pybind11/stl.h\',\n        \'include/pybind11/stl_bind.h\',\n    ]\n\n\nclass InstallHeaders(install_headers):\n    """"""Use custom header installer because the default one flattens subdirectories""""""\n    def run(self):\n        if not self.distribution.headers:\n            return\n\n        for header in self.distribution.headers:\n            subdir = os.path.dirname(os.path.relpath(header, \'include/pybind11\'))\n            install_dir = os.path.join(self.install_dir, subdir)\n            self.mkpath(install_dir)\n\n            (out, _) = self.copy_file(header, install_dir)\n            self.outfiles.append(out)\n\n\nsetup(\n    name=\'pybind11\',\n    version=__version__,\n    description=\'Seamless operability between C++11 and Python\',\n    author=\'Wenzel Jakob\',\n    author_email=\'wenzel.jakob@epfl.ch\',\n    url=\'https://github.com/pybind/pybind11\',\n    download_url=\'https://github.com/pybind/pybind11/tarball/v\' + __version__,\n    packages=[\'pybind11\'],\n    license=\'BSD\',\n    headers=headers,\n    cmdclass=dict(install_headers=InstallHeaders),\n    classifiers=[\n        \'Development Status :: 5 - Production/Stable\',\n        \'Intended Audience :: Developers\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Utilities\',\n        \'Programming Language :: C++\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.2\',\n        \'Programming Language :: Python :: 3.3\',\n        \'Programming Language :: Python :: 3.4\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'License :: OSI Approved :: BSD License\'\n    ],\n    keywords=\'C++11, Python bindings\',\n    long_description=""""""pybind11 is a lightweight header-only library that\nexposes C++ types in Python and vice versa, mainly to create Python bindings of\nexisting C++ code. Its goals and syntax are similar to the excellent\nBoost.Python by David Abrahams: to minimize boilerplate code in traditional\nextension modules by inferring type information using compile-time\nintrospection.\n\nThe main issue with Boost.Python-and the reason for creating such a similar\nproject-is Boost. Boost is an enormously large and complex suite of utility\nlibraries that works with almost every C++ compiler in existence. This\ncompatibility has its cost: arcane template tricks and workarounds are\nnecessary to support the oldest and buggiest of compiler specimens. Now that\nC++11-compatible compilers are widely available, this heavy machinery has\nbecome an excessively large and unnecessary dependency.\n\nThink of this library as a tiny self-contained version of Boost.Python with\neverything stripped away that isn\'t relevant for binding generation. Without\ncomments, the core header files only require ~4K lines of code and depend on\nPython (2.7 or 3.x, or PyPy2.7 >= 5.7) and the C++ standard library. This\ncompact implementation was possible thanks to some of the new C++11 language\nfeatures (specifically: tuples, lambda functions and variadic templates). Since\nits creation, this library has grown beyond Boost.Python in many ways, leading\nto dramatically simpler binding code in many common situations."""""")\n'"
flink-ml-framework/src/test/python/__init__.py,0,b''
flink-ml-framework/src/test/python/greeter.py,0,"b""from __future__ import print_function\nimport sys\n\n\ndef map_func(context):\n    print('hello from greeter')\n    print(context)\n    sys.stdout.flush()\n"""
flink-ml-framework/src/test/python/read_bytes_from_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n\n    bytes_recorder = BytesRecorder(context.from_java(), context.to_java())\n    try:\n        res = bytes_recorder.read_record()\n        print(""res"", res)\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-framework/src/test/python/read_from_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nimport struct\nfrom flink_ml_framework.java_file import JavaFile\n\n\ndef map_func(context):\n    java_file = JavaFile(context.from_java(), context.to_java())\n    try:\n        res = java_file.read(4)\n        # len = int(\'\'.join(reversed(res)).encode(\'hex\'), 16)\n        data_len, = struct.unpack(""<i"", res)\n        print(""res"", type(data_len), data_len)\n        data = java_file.read(data_len)\n        print(""data"", str(data))\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-framework/src/test/python/read_json_from_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n    bytes_recorder = BytesRecorder(context.from_java(), context.to_java())\n    try:\n        res = bytes_recorder.read_record()\n        print(""res"", res)\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-framework/src/test/python/write_bytes_to_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n\n    bytes_recorder = BytesRecorder(context.from_java(), context.to_java())\n    try:\n        res = bytes_recorder.write_record(bytes(""aaaaaaa""))\n        print(""res:"", res)\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-framework/src/test/python/write_json_to_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n\n    json_recorder = JsonRecorder(context.from_java(), context.to_java())\n    try:\n        res = json_recorder.write_record({""kk"": ""kkk""})\n        print(""res:"", res)\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-framework/src/test/python/write_to_java.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import JavaFile\nimport json\nimport struct\n\n\ndef map_func(context):\n\n    java_file = JavaFile(context.from_java(), context.to_java())\n    try:\n        json_object = {\'aa\': \'aa\'}\n        json_bytes = json.dumps(json_object)\n        json_len = struct.pack(""<i"", len(json_bytes))\n        res = java_file.write(json_len, 4)\n        print(""res:"", res)\n        res = java_file.write(json_bytes, len(json_bytes))\n        print(""res:"", res)\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-lib/src/test/python/add_saved_model.py,19,"b'import tensorflow as tf\nimport os\n\na = tf.placeholder(tf.float32, shape=None, name=""a"")\nb = tf.placeholder(tf.float32, shape=None, name=""b"")\nv = tf.Variable(dtype=tf.float32, initial_value=tf.constant(1.0), name=""v"")\nc = tf.add(a, b, name=""c"")\nd = tf.add(c, v)\nglobal_step = tf.contrib.framework.get_or_create_global_step()\nglobal_step_inc = tf.assign_add(global_step, 1)\nhooks = [tf.train.StopAtStepHook(last_step=2)]\nwith tf.Session() as mon_sess:\n    mon_sess.run(tf.initialize_all_variables())\n    for i in range(2):\n        print(mon_sess.run([d, global_step_inc], feed_dict={a: [1.0, 2.0, 3.0], b: [1.0, 2.0, 3.0]}))\n\n    signatures = {\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n            \'inputs\': {\'a\': a, ""b"": b},\n            \'outputs\': {""d"": d},\n            \'method_name\': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n        }\n    }\n\n    export_dir = os.path.dirname(os.path.abspath(__file__)) + ""../../../../target/test-classes/export""\n    try:\n        tf.gfile.DeleteRecursively(export_dir)\n    except tf.errors.OpError:\n        pass\n    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n\n    signature_def_map = {}\n    for key, sig in signatures.items():\n        signature_def_map[key] = tf.saved_model.signature_def_utils.build_signature_def(\n            inputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                    sig[\'inputs\'].items()},\n            outputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                     sig[\'outputs\'].items()},\n            method_name=sig[\'method_name\'] if \'method_name\' in sig else key)\n\n        builder.add_meta_graph_and_variables(\n            mon_sess,\n            tf.saved_model.tag_constants.SERVING.split(\',\'),\n            signature_def_map=signature_def_map,\n            clear_devices=True)\n        builder.save()\n'"
flink-ml-lib/src/test/python/build_model.py,24,"b'import tensorflow as tf\nimport os\n\na = tf.placeholder(tf.float32, shape=None, name=""a"")\nb = tf.placeholder(tf.float32, shape=None, name=""b"")\ne = tf.placeholder(tf.string, shape=None, name=""e"")\nee = tf.strings.to_number(\n    e,\n    out_type=tf.float32,\n    name=""ee""\n)\nv = tf.Variable(dtype=tf.float32, initial_value=tf.constant(1.0), name=""v"")\nc = tf.add(a, b, name=""c"")\nd = tf.add(c, v)\neee = tf.add(ee, v)\ne4 = tf.as_string(eee)\nglobal_step = tf.contrib.framework.get_or_create_global_step()\nglobal_step_inc = tf.assign_add(global_step, 1)\nhooks = [tf.train.StopAtStepHook(last_step=2)]\nwith tf.Session() as mon_sess:\n    mon_sess.run(tf.initialize_all_variables())\n    for i in range(2):\n        print(mon_sess.run([d, e4, global_step_inc], feed_dict={a: [1.0, 2.0, 3.0], b: [1.0, 2.0, 3.0],\n                                                                e: [""1.0"", ""2.0"", ""3.0""]}))\n\n    signatures = {\n        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: {\n            \'inputs\': {\'a\': a, ""b"": b, ""e"": e},\n            \'outputs\': {""d"": d, ""e4"": e4},\n            \'method_name\': tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n        }\n    }\n\n    export_dir = os.path.dirname(os.path.abspath(__file__)) + ""../../../../target/test-classes/export2""\n    try:\n        tf.gfile.DeleteRecursively(export_dir)\n    except tf.errors.OpError:\n        pass\n    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n\n    signature_def_map = {}\n    for key, sig in signatures.items():\n        signature_def_map[key] = tf.saved_model.signature_def_utils.build_signature_def(\n            inputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                    sig[\'inputs\'].items()},\n            outputs={name: tf.saved_model.utils.build_tensor_info(tensor) for name, tensor in\n                     sig[\'outputs\'].items()},\n            method_name=sig[\'method_name\'] if \'method_name\' in sig else key)\n\n        builder.add_meta_graph_and_variables(\n            mon_sess,\n            tf.saved_model.tag_constants.SERVING.split(\',\'),\n            signature_def_map=signature_def_map,\n            clear_devices=True)\n        builder.save()\n'"
flink-ml-operator/src/test/python/greeter.py,0,"b""from __future__ import print_function\nimport sys\n\n\ndef map_func(context):\n    print('hello from greeter')\n    print('index:', context.index)\n    sys.stdout.flush()\n"""
flink-ml-operator/src/test/python/input_output_json.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n    json_recorder = JsonRecorder(context.from_java(), context.to_java())\n    try:\n        while True:\n\n            data = json_recorder.read_record()\n            print(context.index, ""data:"", data)\n            sys.stdout.flush()\n            res = json_recorder.write_record(data)\n            print(context.index, ""res:"", res)\n            sys.stdout.flush()\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-operator/src/test/python/input_output_row.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n    bytes_recorder = BytesRecorder(context.from_java(), context.to_java())\n    try:\n        while True:\n\n            data = bytes_recorder.read_record()\n            print(context.index, ""data:"", data)\n            sys.stdout.flush()\n            res = bytes_recorder.write_record(data)\n            print(context.index, ""res:"", res)\n            sys.stdout.flush()\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-operator/src/test/python/output_json.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n    json_recorder = JsonRecorder(context.from_java(), context.to_java())\n    try:\n        for i in range(20):\n            res = json_recorder.write_record({""output"": ""output_"" + str(i)})\n            print(context.index, ""res:"", res)\n            sys.stdout.flush()\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-operator/src/test/python/output_row.py,0,"b'from __future__ import print_function\nimport sys\nimport traceback\nfrom flink_ml_framework.java_file import *\n\n\ndef map_func(context):\n\n    bytes_recorder = BytesRecorder(context.from_java(), context.to_java())\n    try:\n        for i in range(20):\n            res = bytes_recorder.write_record(""a,b,c,d"")\n            print(context.index, ""res:"", res)\n            sys.stdout.flush()\n    except Exception as e:\n        msg = traceback.format_exc()\n        print (msg)\n'"
flink-ml-pytorch/src/test/python/greeter.py,0,"b""from __future__ import print_function\nimport sys\n\ndef map_func(context):\n    print('hello from greeter')\n    print('index:', context.index)\n    sys.stdout.flush()"""
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_stream/__init__.py,0,b''
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_stream/datastream.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# pyflink doesn\'t contain the DataStream class right now, So here is the simplest python class of DataStream\n\nclass DataStream(object):\n    def __init__(self, j_datastream):\n        self._j_datastream = j_datastream'"
flink-ml-tensorflow/python/flink_ml_tensorflow/tensorflow_on_flink_stream/datastreamsource.py,0,"b'# Copyright 2019 The flink-ai-extended Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n# pyflink doesn\'t contain the DataStreamSource class right now, So here is the simplest python class of DataStreamSource\n\nclass DataStreamSource(object):\n    def __init__(self, j_datastream_source):\n        self._j_datastream_source = j_datastream_source'"
flink-ml-tensorflow/src/test/python/__init__.py,0,b''
flink-ml-tensorflow/src/test/python/add.py,14,"b'import tensorflow as tf\nimport sys\nimport time\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nfrom flink_ml_tensorflow.tensorflow_context import TFContext\n\n\ndef build_graph():\n    global a\n    a = tf.placeholder(tf.float32, shape=None, name=""a"")\n    b = tf.reduce_mean(a, name=""b"")\n    r_list = []\n    for i in range(1):\n        v = tf.Variable(dtype=tf.float32, initial_value=tf.constant(1.0), name=""v_"" + str(i))\n        c = tf.add(b, v, name=""c_"" + str(i))\n        add = tf.assign(v, c, name=""assign_"" + str(i))\n        sum = tf.summary.scalar(name=""sum_"" + str(i), tensor=c)\n        r_list.append(add)\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    global_step_inc = tf.assign_add(global_step, 1)\n    r_list.append(global_step_inc)\n    return r_list\n\n\ndef map_func(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (""cluster:"" + str(cluster_json))\n    print (""job name:"" + job_name)\n    print (""current index:"" + str(index))\n    sys.stdout.flush()\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    t = time.time()\n    if \'ps\' == job_name:\n        from time import sleep\n        while True:\n            sleep(1)\n    else:\n        with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n            train_ops = build_graph()\n            print(""python worker index:"" + str(index))\n            sys.stdout.flush()\n            try:\n                hooks = [tf.train.StopAtStepHook(last_step=2)]\n                with tf.train.MonitoredTrainingSession(master=server.target, config=sess_config,\n                                                       checkpoint_dir=""./target/tmp/s1/"" + str(t),\n                                                       hooks=hooks) as mon_sess:\n                    while not mon_sess.should_stop():\n                        print (mon_sess.run(train_ops, feed_dict={a: [1.0, 2.0, 3.0]}))\n                        sys.stdout.flush()\n                        time.sleep(1)\n            finally:\n                SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/add_withtb.py,14,"b'import tensorflow as tf\nimport sys\nimport time\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nfrom flink_ml_tensorflow.tensorflow_context import *\n\n\ndef build_graph():\n    global a\n    a = tf.placeholder(tf.float32, shape=None, name=""a"")\n    b = tf.reduce_mean(a, name=""b"")\n    r_list = []\n    for i in range(1):\n        v = tf.Variable(dtype=tf.float32, initial_value=tf.constant(1.0), name=""v_"" + str(i))\n        c = tf.add(b, v, name=""c_"" + str(i))\n        add = tf.assign(v, c, name=""assign_"" + str(i))\n        sum = tf.summary.scalar(name=""sum_"" + str(i), tensor=c)\n        r_list.append(add)\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    global_step_inc = tf.assign_add(global_step, 1)\n    r_list.append(global_step_inc)\n    return r_list\n\n\ndef map_func(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n    ckpt = tf_context.get_property(""checkpoint_dir"")\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    t = time.time()\n    if \'ps\' == job_name:\n        from time import sleep\n        while True:\n            sleep(1)\n    else:\n        with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n            train_ops = build_graph()\n            try:\n                hooks = [tf.train.StopAtStepHook(last_step=50)]\n                with tf.train.MonitoredTrainingSession(master=server.target, config=sess_config,\n                                                       checkpoint_dir=ckpt, hooks=hooks,\n                                                       save_summaries_steps=1) as mon_sess:\n                    while not mon_sess.should_stop():\n                        print (mon_sess.run(train_ops, feed_dict={a: [1.0, 2.0, 3.0]}))\n                        sys.stdout.flush()\n                        time.sleep(1)\n            finally:\n                SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_func(context)\n'"
flink-ml-tensorflow/src/test/python/csv_input_only.py,11,"b'from __future__ import print_function\nimport tensorflow as tf\nimport sys\nimport time\nimport json\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport tensorflow_on_flink.tensorflow_on_flink_ops as tff_ops\nimport traceback\nfrom flink_ml_tensorflow.tensorflow_context import TFContext\n\n\ndef map_fun(context):\n    print(tf.__version__)\n    sys.stdout.flush()\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    if \'ps\' == job_name:\n        from time import sleep\n        while True:\n            sleep(1)\n    else:\n        with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n            record_defaults = [[9], [tf.constant(value=9, dtype=tf.int64)], [9.0],\n                               [tf.constant(value=9.0, dtype=tf.float64)], [""9.0""]]\n            dataset = context.flinkStreamDataSet(buffer_size=0)\n            dataset = dataset.map(lambda record: tf.decode_csv(record, record_defaults=record_defaults))\n            dataset = dataset.batch(3)\n            iterator = dataset.make_one_shot_iterator()\n            input_records = iterator.get_next()\n\n            global_step = tf.contrib.framework.get_or_create_global_step()\n            global_step_inc = tf.assign_add(global_step, 1)\n            out_list = [input_records[0], input_records[2], input_records[4]]\n            out = tff_ops.encode_csv(input_list=out_list)\n            is_chief = (index == 0)\n            t = time.time()\n            try:\n                with tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief, config=sess_config,\n                                                       checkpoint_dir=""./target/tmp/input_output/"" + str(\n                                                           t)) as mon_sess:\n                    # while not mon_sess.should_stop():\n                    while True:\n                        print (index, mon_sess.run([global_step_inc, out]))\n                        sys.stdout.flush()\n                        # time.sleep(1)\n            except Exception as e:\n                print(\'traceback.print_exc():\')\n                traceback.print_exc()\n                sys.stdout.flush()\n            finally:\n                SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/env.py,0,"b""from __future__ import print_function\nimport os\n\nvar=os.environ['MYVAR']\nprint(var)"""
flink-ml-tensorflow/src/test/python/failover.py,0,"b'import sys\nimport time\nimport traceback\nimport logging\nimport logging.config\n\n\ndef map_func(context):\n    key = context.identity\n    index = context.index\n    fail_num = context.get_failed_num()\n    logging.info(key + "" fail num: "" + str(fail_num))\n    sys.stdout.flush()\n    if 1 == index and fail_num < 2:\n        time.sleep(5)\n        logging.info(key + "" failover!"")\n        sys.stdout.flush()\n        raise Exception(""fail over!"")\n\n    for i in range(5):\n        logging.info(key + "" run num: "" + str(i))\n        sys.stdout.flush()\n        time.sleep(5)\n\n\nif __name__ == ""__main__"":\n    pass\n'"
flink-ml-tensorflow/src/test/python/failover2.py,0,"b'import sys\nimport time\nimport logging\nimport logging.config\n\n\n# test failover with some nodes already finished\ndef map_func(context):\n    key = context.identity\n    index = context.index\n    fail_num = context.get_failed_num()\n    logging.info(key + "" fail num: "" + str(fail_num))\n    sys.stdout.flush()\n    if context.get_role_name() == ""worker"" and 0 == index and fail_num < 1:\n        time.sleep(8)\n        logging.info(key + "" failover!"")\n        sys.stdout.flush()\n        raise Exception(""fail over!"")\n\n    if context.get_role_name() == ""ps"":\n        while True:\n            time.sleep(1)\n    else:\n        for i in range(2):\n            logging.info(key + "" run num: "" + str(i))\n            sys.stdout.flush()\n            time.sleep(1)\n        logging.info(key + "" finished"")\n        sys.stdout.flush()\n\n\nif __name__ == ""__main__"":\n    pass\n'"
flink-ml-tensorflow/src/test/python/finish_worker_info.py,0,"b'import sys\nimport time\n\n\ndef map_func(context):\n    key = context.identity\n    index = context.index\n    for i in range(11 - index):\n        print(key + "" finish worker:"" + str(context.getFinishWorkerNode()))\n        print(key + "" finish worker:"" + str(len(context.getFinishWorkerNode())))\n        print(len(set(context.getFinishWorkerNode())))\n        if 0 == index and 5 == i:\n            context.stopJob()\n\n        time.sleep(1)\n\n\nif __name__ == ""__main__"":\n    pass\n'"
flink-ml-tensorflow/src/test/python/global_step_direct.py,11,"b'from __future__ import print_function\nfrom datetime import datetime\nimport tensorflow as tf\nimport sys\nimport time\nimport json\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport tensorflow_on_flink.tensorflow_on_flink_ops as tff_ops\nimport traceback\n\n\ndef log_speed(steps, start):\n    duration = time.time() - start\n    speed = steps / duration\n    print (""Read directly: "" + str(steps) + "" steps, at "" + \'%.2f\' % speed + "" steps/second"")\n    sys.stdout.flush()\n\n\ndef map_fun(context):\n    print(tf.__version__)\n    sys.stdout.flush()\n    tf.logging.set_verbosity(tf.logging.ERROR)\n    jobName = context.jobName\n    index = context.index\n    clusterStr = context.properties[""cluster""]\n    delim = context.properties[""SYS:delim""]\n    epochs = int(context.properties[""epochs""])\n    data_file = context.properties[""data.file""]\n    print (index, clusterStr)\n    sys.stdout.flush()\n    clusterJson = json.loads(clusterStr)\n    cluster = tf.train.ClusterSpec(cluster=clusterJson)\n    server = tf.train.Server(cluster, job_name=jobName, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n\n    with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n        filename_queue = tf.train.string_input_producer([data_file], num_epochs=epochs)\n        reader = tf.TextLineReader()\n        key, value = reader.read(filename_queue)\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        global_step_inc = tf.assign_add(global_step, 1)\n        is_chief = (index == 0)\n        print (datetime.now().isoformat() + "" started ------------------------------------"")\n        t = time.time()\n        total_step = 0\n        try:\n            with tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief, config=sess_config,\n                                                   checkpoint_dir=""./target/tmp/input_output/"" + str(t)) as mon_sess:\n                # while not mon_sess.should_stop():\n                while True:\n                    total_step, _, _ = mon_sess.run([global_step_inc, key, value])\n                    if (total_step % 10000 == 0):\n                        log_speed(total_step, t)\n        except Exception as e:\n            print(\'traceback.print_exc():\')\n            traceback.print_exc()\n            sys.stdout.flush()\n        finally:\n            print (datetime.now().isoformat() + "" ended --------------------------------------"")\n            log_speed(total_step, t)\n            SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/global_step_queue.py,9,"b'from __future__ import print_function\nfrom datetime import datetime\nimport tensorflow as tf\nimport sys\nimport time\nimport json\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport tensorflow_on_flink.tensorflow_on_flink_ops as tff_ops\nimport traceback\n\n\ndef log_speed(steps, start):\n    duration = time.time() - start\n    speed = steps / duration\n    print (""Read from queue: "" + str(steps) + "" steps, at "" + \'%.2f\' % speed + "" steps/second"")\n    sys.stdout.flush()\n\n\ndef map_fun(context):\n    print(tf.__version__)\n    sys.stdout.flush()\n    tf.logging.set_verbosity(tf.logging.ERROR)\n    jobName = context.jobName\n    index = context.index\n    clusterStr = context.properties[""cluster""]\n    delim = context.properties[""SYS:delim""]\n    print (index, clusterStr)\n    sys.stdout.flush()\n    clusterJson = json.loads(clusterStr)\n    cluster = tf.train.ClusterSpec(cluster=clusterJson)\n    server = tf.train.Server(cluster, job_name=jobName, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n        dataset = context.flinkStreamDataSet(buffer_size=0)\n        iterator = dataset.make_one_shot_iterator()\n        input_records = iterator.get_next()\n\n        global_step = tf.contrib.framework.get_or_create_global_step()\n        global_step_inc = tf.assign_add(global_step, 1)\n        is_chief = (index == 0)\n        print (datetime.now().isoformat() + "" started ------------------------------------"")\n        t = time.time()\n        total_step = 0\n        try:\n            with tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief, config=sess_config,\n                                                   checkpoint_dir=""./target/tmp/input_output/"" + str(t)) as mon_sess:\n                # while not mon_sess.should_stop():\n                while True:\n                    total_step, _ = mon_sess.run([global_step_inc, input_records])\n                    if (total_step % 10000 == 0):\n                        log_speed(total_step, t)\n        except Exception as e:\n            print(\'traceback.print_exc():\')\n            traceback.print_exc()\n            sys.stdout.flush()\n        finally:\n            print (datetime.now().isoformat() + "" ended --------------------------------------"")\n            log_speed(total_step, t)\n            SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/input_output.py,11,"b'from __future__ import print_function\nimport tensorflow as tf\nimport sys\nimport time\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nfrom flink_ml_tensorflow import tensorflow_on_flink_ops as tff_ops\nfrom flink_ml_tensorflow.tensorflow_context import *\nimport traceback\n\n\ndef map_func(context):\n    print(tf.__version__)\n    sys.stdout.flush()\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    if \'ps\' == job_name:\n        from time import sleep\n        while True:\n            sleep(1)\n    else:\n        with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n            record_defaults = [[9], [tf.constant(value=9, dtype=tf.int64)], [9.0],\n                               [tf.constant(value=9.0, dtype=tf.float64)], [""9.0""]]\n            dataset = tf_context.flink_stream_dataset(buffer_size=0)\n            dataset = dataset.map(lambda record: tf.decode_csv(record, record_defaults=record_defaults))\n            dataset = dataset.batch(3)\n            iterator = dataset.make_one_shot_iterator()\n            input_records = iterator.get_next()\n\n            global_step = tf.contrib.framework.get_or_create_global_step()\n            global_step_inc = tf.assign_add(global_step, 1)\n            out = tff_ops.encode_csv(input_list=input_records)\n            fw = tff_ops.FlinkTFRecordWriter(address=context.to_java())\n            w = fw.write([out])\n            is_chief = (index == 0)\n            t = time.time()\n            try:\n                with tf.train.MonitoredTrainingSession(master=server.target, is_chief=is_chief, config=sess_config,\n                                                       checkpoint_dir=""./target/tmp/input_output/"" + str(\n                                                           t)) as mon_sess:\n                    # while not mon_sess.should_stop():\n                    while True:\n                        print (index, mon_sess.run([global_step_inc, w]))\n                        sys.stdout.flush()\n                        #time.sleep(1)\n            except Exception as e:\n                print(\'traceback.print_exc():\')\n                traceback.print_exc()\n                sys.stdout.flush()\n                raise e\n            finally:\n                SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/simple_print.py,0,"b'from __future__ import print_function\nimport sys\n\n\ndef map_func(context):\n    for i in range(2):\n        print(""hello world!"")\n        sys.stdout.flush()\n\n\nif __name__ == ""__main__"":\n    pass\n'"
flink-ml-tensorflow/src/test/python/tensorboard.py,0,"b""# -*- coding: utf-8 -*-\n# -*- coding: utf-8 -*-\nimport re\nimport sys\n\n# del the dir path\nsys.path = sys.path[1:]\nfrom tensorboard.main import run_main\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(run_main())\n\n\n\n"""
flink-ml-tensorflow/src/test/python/with_output.py,10,"b'import tensorflow as tf\nimport sys\nimport time\nimport json\nfrom tensorflow.python.summary.writer.writer_cache import FileWriterCache as SummaryWriterCache\nimport tensorflow_on_flink.tensorflow_on_flink_ops as tff_ops\nfrom flink_ml_tensorflow.tensorflow_context import TFContext\n\ndef map_fun(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n    cluster = tf.train.ClusterSpec(cluster=cluster_json)\n    server = tf.train.Server(cluster, job_name=job_name, task_index=index)\n    sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                 device_filters=[""/job:ps"", ""/job:worker/task:%d"" % index])\n    if \'ps\' == job_name:\n        from time import sleep\n        while True:\n            sleep(1)\n    else:\n        with tf.device(tf.train.replica_device_setter(worker_device=\'/job:worker/task:\' + str(index), cluster=cluster)):\n\n            global_step = tf.contrib.framework.get_or_create_global_step()\n            global_step_inc = tf.assign_add(global_step, 1)\n            input_records = [tf.constant([1, 2, 3]),\n                             tf.constant([1.0, 2.0, 3.0]), tf.constant([\'1.0\', \'2.0\', \'3.0\'])]\n            out = tff_ops.encode_csv(input_list=input_records, field_delim=\'|\')\n            fw = tff_ops.FlinkTFRecordWriter(address=context.toFlink())\n            w = fw.write([out])\n            is_chief = (index == 0)\n            t = time.time()\n            try:\n                hooks = [tf.train.StopAtStepHook(last_step=50)]\n                with tf.train.MonitoredTrainingSession(master=server.target, config=sess_config, is_chief=is_chief,\n                                                       checkpoint_dir=""./target/tmp/with_output/""+str(t), hooks=hooks) as mon_sess:\n                    while not mon_sess.should_stop():\n                        print (index, mon_sess.run([global_step_inc, w]))\n                        sys.stdout.flush()\n                        time.sleep(1)\n            finally:\n                SummaryWriterCache.clear()\n\n\nif __name__ == ""__main__"":\n    map_fun(context)\n'"
flink-ml-tensorflow/src/test/python/worker_0_finish.py,0,"b'from __future__ import print_function\nimport sys\nimport time\nfrom flink_ml_tensorflow.tensorflow_context import TFContext\n\ndef map_func(context):\n    tf_context = TFContext(context)\n    job_name = tf_context.get_role_name()\n    index = tf_context.get_index()\n    cluster_json = tf_context.get_tf_cluster()\n    print (cluster_json)\n    sys.stdout.flush()\n    if ""worker"" == job_name and 0 == index:\n        time.sleep(3)\n        print(""worker 0 finish!"")\n        sys.stdout.flush()\n    else:\n        while True:\n            print(""hello world!"")\n            sys.stdout.flush()\n            time.sleep(3)\n\n\nif __name__ == ""__main__"":\n    pass\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/docs/benchmark.py,0,"b'import random\nimport os\nimport time\nimport datetime as dt\n\nnfns = 4  # Functions per class\nnargs = 4  # Arguments per function\n\n\ndef generate_dummy_code_pybind11(nclasses=10):\n    decl = """"\n    bindings = """"\n\n    for cl in range(nclasses):\n        decl += ""class cl%03i;\\n"" % cl\n    decl += \'\\n\'\n\n    for cl in range(nclasses):\n        decl += ""class cl%03i {\\n"" % cl\n        decl += ""public:\\n""\n        bindings += \'    py::class_<cl%03i>(m, ""cl%03i"")\\n\' % (cl, cl)\n        for fn in range(nfns):\n            ret = random.randint(0, nclasses - 1)\n            params  = [random.randint(0, nclasses - 1) for i in range(nargs)]\n            decl += ""    cl%03i *fn_%03i("" % (ret, fn)\n            decl += "", "".join(""cl%03i *"" % p for p in params)\n            decl += "");\\n""\n            bindings += \'        .def(""fn_%03i"", &cl%03i::fn_%03i)\\n\' % \\\n                (fn, cl, fn)\n        decl += ""};\\n\\n""\n        bindings += \'        ;\\n\'\n\n    result = ""#include <pybind11/pybind11.h>\\n\\n""\n    result += ""namespace py = pybind11;\\n\\n""\n    result += decl + \'\\n\'\n    result += ""PYBIND11_MODULE(example, m) {\\n""\n    result += bindings\n    result += ""}""\n    return result\n\n\ndef generate_dummy_code_boost(nclasses=10):\n    decl = """"\n    bindings = """"\n\n    for cl in range(nclasses):\n        decl += ""class cl%03i;\\n"" % cl\n    decl += \'\\n\'\n\n    for cl in range(nclasses):\n        decl += ""class cl%03i {\\n"" % cl\n        decl += ""public:\\n""\n        bindings += \'    py::class_<cl%03i>(""cl%03i"")\\n\' % (cl, cl)\n        for fn in range(nfns):\n            ret = random.randint(0, nclasses - 1)\n            params  = [random.randint(0, nclasses - 1) for i in range(nargs)]\n            decl += ""    cl%03i *fn_%03i("" % (ret, fn)\n            decl += "", "".join(""cl%03i *"" % p for p in params)\n            decl += "");\\n""\n            bindings += \'        .def(""fn_%03i"", &cl%03i::fn_%03i, py::return_value_policy<py::manage_new_object>())\\n\' % \\\n                (fn, cl, fn)\n        decl += ""};\\n\\n""\n        bindings += \'        ;\\n\'\n\n    result = ""#include <boost/python.hpp>\\n\\n""\n    result += ""namespace py = boost::python;\\n\\n""\n    result += decl + \'\\n\'\n    result += ""BOOST_PYTHON_MODULE(example) {\\n""\n    result += bindings\n    result += ""}""\n    return result\n\n\nfor codegen in [generate_dummy_code_pybind11, generate_dummy_code_boost]:\n    print (""{"")\n    for i in range(0, 10):\n        nclasses = 2 ** i\n        with open(""test.cpp"", ""w"") as f:\n            f.write(codegen(nclasses))\n        n1 = dt.datetime.now()\n        os.system(""g++ -Os -shared -rdynamic -undefined dynamic_lookup ""\n            ""-fvisibility=hidden -std=c++14 test.cpp -I include ""\n            ""-I /System/Library/Frameworks/Python.framework/Headers -o test.so"")\n        n2 = dt.datetime.now()\n        elapsed = (n2 - n1).total_seconds()\n        size = os.stat(\'test.so\').st_size\n        print(""   {%i, %f, %i},"" % (nclasses * nfns, elapsed, size))\n    print (""}"")\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# pybind11 documentation build configuration file, created by\n# sphinx-quickstart on Sun Oct 11 19:23:48 2015.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nimport shlex\nimport subprocess\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'breathe\']\n\nbreathe_projects = {\'pybind11\': \'.build/doxygenxml/\'}\nbreathe_default_project = \'pybind11\'\nbreathe_domain_by_extension = {\'h\': \'cpp\'}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'.templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'pybind11\'\ncopyright = \'2017, Wenzel Jakob\'\nauthor = \'Wenzel Jakob\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'2.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'2.2.3\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = [\'.build\', \'release.rst\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\ndefault_role = \'any\'\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\n#pygments_style = \'monokai\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\nif not on_rtd:  # only import and set the theme if we\'re building docs locally\n    import sphinx_rtd_theme\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n    html_context = {\n        \'css_files\': [\n            \'_static/theme_overrides.css\'\n        ]\n    }\nelse:\n    html_context = {\n        \'css_files\': [\n            \'//media.readthedocs.org/css/sphinx_rtd_theme.css\',            \n            \'//media.readthedocs.org/css/readthedocs-doc-embed.css\',    \n            \'_static/theme_overrides.css\'\n        ]\n    }\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# Now only \'ja\' uses this config value\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pybind11doc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n\'preamble\': \'\\DeclareUnicodeCharacter{00A0}{}\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  (master_doc, \'pybind11.tex\', \'pybind11 Documentation\',\n   \'Wenzel Jakob\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = \'pybind11-logo.png\'\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pybind11\', \'pybind11 Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  (master_doc, \'pybind11\', \'pybind11 Documentation\',\n   author, \'pybind11\', \'One line description of project.\',\n   \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\nprimary_domain = \'cpp\'\nhighlight_language = \'cpp\'\n\n\ndef generate_doxygen_xml(app):\n    build_dir = os.path.join(app.confdir, \'.build\')\n    if not os.path.exists(build_dir):\n        os.mkdir(build_dir)\n\n    try:\n        subprocess.call([\'doxygen\', \'--version\'])\n        retcode = subprocess.call([\'doxygen\'], cwd=app.confdir)\n        if retcode < 0:\n            sys.stderr.write(""doxygen error code: {}\\n"".format(-retcode))\n    except OSError as e:\n        sys.stderr.write(""doxygen execution failed: {}\\n"".format(e))\n\n\ndef setup(app):\n    """"""Add hook for building doxygen xml when needed""""""\n    app.connect(""builder-inited"", generate_doxygen_xml)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/pybind11/__init__.py,0,"b'from ._version import version_info, __version__  # noqa: F401 imported but unused\n\n\ndef get_include(user=False):\n    from distutils.dist import Distribution\n    import os\n    import sys\n\n    # Are we running in a virtual environment?\n    virtualenv = hasattr(sys, \'real_prefix\') or \\\n        sys.prefix != getattr(sys, ""base_prefix"", sys.prefix)\n\n    if virtualenv:\n        return os.path.join(sys.prefix, \'include\', \'site\',\n                            \'python\' + sys.version[:3])\n    else:\n        dist = Distribution({\'name\': \'pybind11\'})\n        dist.parse_config_files()\n\n        dist_cobj = dist.get_command_obj(\'install\', create=True)\n\n        # Search for packages in user\'s home directory?\n        if user:\n            dist_cobj.user = user\n            dist_cobj.prefix = """"\n        dist_cobj.finalize_options()\n\n        return os.path.dirname(dist_cobj.install_headers)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/pybind11/__main__.py,0,"b""from __future__ import print_function\n\nimport argparse\nimport sys\nimport sysconfig\n\nfrom . import get_include\n\n\ndef print_includes():\n    dirs = [sysconfig.get_path('include'),\n            sysconfig.get_path('platinclude'),\n            get_include(),\n            get_include(True)]\n\n    # Make unique but preserve order\n    unique_dirs = []\n    for d in dirs:\n        if d not in unique_dirs:\n            unique_dirs.append(d)\n\n    print(' '.join('-I' + d for d in unique_dirs))\n\n\ndef main():\n    parser = argparse.ArgumentParser(prog='python -m pybind11')\n    parser.add_argument('--includes', action='store_true',\n                        help='Include flags for both pybind11 and Python headers.')\n    args = parser.parse_args()\n    if not sys.argv[1:]:\n        parser.print_help()\n    if args.includes:\n        print_includes()\n\n\nif __name__ == '__main__':\n    main()\n"""
flink-ml-framework/python/lib/pybind11-2.2.4/pybind11/_version.py,0,"b""version_info = (2, 2, 4)\n__version__ = '.'.join(map(str, version_info))\n"""
flink-ml-framework/python/lib/pybind11-2.2.4/tests/conftest.py,0,"b'""""""pytest configuration\n\nExtends output capture as needed by pybind11: ignore constructors, optional unordered lines.\nAdds docstring and exceptions message sanitizers: ignore Python 2 vs 3 differences.\n""""""\n\nimport pytest\nimport textwrap\nimport difflib\nimport re\nimport sys\nimport contextlib\nimport platform\nimport gc\n\n_unicode_marker = re.compile(r\'u(\\\'[^\\\']*\\\')\')\n_long_marker = re.compile(r\'([0-9])L\')\n_hexadecimal = re.compile(r\'0x[0-9a-fA-F]+\')\n\n\ndef _strip_and_dedent(s):\n    """"""For triple-quote strings""""""\n    return textwrap.dedent(s.lstrip(\'\\n\').rstrip())\n\n\ndef _split_and_sort(s):\n    """"""For output which does not require specific line order""""""\n    return sorted(_strip_and_dedent(s).splitlines())\n\n\ndef _make_explanation(a, b):\n    """"""Explanation for a failed assert -- the a and b arguments are List[str]""""""\n    return [""--- actual / +++ expected""] + [line.strip(\'\\n\') for line in difflib.ndiff(a, b)]\n\n\nclass Output(object):\n    """"""Basic output post-processing and comparison""""""\n    def __init__(self, string):\n        self.string = string\n        self.explanation = []\n\n    def __str__(self):\n        return self.string\n\n    def __eq__(self, other):\n        # Ignore constructor/destructor output which is prefixed with ""###""\n        a = [line for line in self.string.strip().splitlines() if not line.startswith(""###"")]\n        b = _strip_and_dedent(other).splitlines()\n        if a == b:\n            return True\n        else:\n            self.explanation = _make_explanation(a, b)\n            return False\n\n\nclass Unordered(Output):\n    """"""Custom comparison for output without strict line ordering""""""\n    def __eq__(self, other):\n        a = _split_and_sort(self.string)\n        b = _split_and_sort(other)\n        if a == b:\n            return True\n        else:\n            self.explanation = _make_explanation(a, b)\n            return False\n\n\nclass Capture(object):\n    def __init__(self, capfd):\n        self.capfd = capfd\n        self.out = """"\n        self.err = """"\n\n    def __enter__(self):\n        self.capfd.readouterr()\n        return self\n\n    def __exit__(self, *_):\n        self.out, self.err = self.capfd.readouterr()\n\n    def __eq__(self, other):\n        a = Output(self.out)\n        b = other\n        if a == b:\n            return True\n        else:\n            self.explanation = a.explanation\n            return False\n\n    def __str__(self):\n        return self.out\n\n    def __contains__(self, item):\n        return item in self.out\n\n    @property\n    def unordered(self):\n        return Unordered(self.out)\n\n    @property\n    def stderr(self):\n        return Output(self.err)\n\n\n@pytest.fixture\ndef capture(capsys):\n    """"""Extended `capsys` with context manager and custom equality operators""""""\n    return Capture(capsys)\n\n\nclass SanitizedString(object):\n    def __init__(self, sanitizer):\n        self.sanitizer = sanitizer\n        self.string = """"\n        self.explanation = []\n\n    def __call__(self, thing):\n        self.string = self.sanitizer(thing)\n        return self\n\n    def __eq__(self, other):\n        a = self.string\n        b = _strip_and_dedent(other)\n        if a == b:\n            return True\n        else:\n            self.explanation = _make_explanation(a.splitlines(), b.splitlines())\n            return False\n\n\ndef _sanitize_general(s):\n    s = s.strip()\n    s = s.replace(""pybind11_tests."", ""m."")\n    s = s.replace(""unicode"", ""str"")\n    s = _long_marker.sub(r""\\1"", s)\n    s = _unicode_marker.sub(r""\\1"", s)\n    return s\n\n\ndef _sanitize_docstring(thing):\n    s = thing.__doc__\n    s = _sanitize_general(s)\n    return s\n\n\n@pytest.fixture\ndef doc():\n    """"""Sanitize docstrings and add custom failure explanation""""""\n    return SanitizedString(_sanitize_docstring)\n\n\ndef _sanitize_message(thing):\n    s = str(thing)\n    s = _sanitize_general(s)\n    s = _hexadecimal.sub(""0"", s)\n    return s\n\n\n@pytest.fixture\ndef msg():\n    """"""Sanitize messages and add custom failure explanation""""""\n    return SanitizedString(_sanitize_message)\n\n\n# noinspection PyUnusedLocal\ndef pytest_assertrepr_compare(op, left, right):\n    """"""Hook to insert custom failure explanation""""""\n    if hasattr(left, \'explanation\'):\n        return left.explanation\n\n\n@contextlib.contextmanager\ndef suppress(exception):\n    """"""Suppress the desired exception""""""\n    try:\n        yield\n    except exception:\n        pass\n\n\ndef gc_collect():\n    \'\'\' Run the garbage collector twice (needed when running\n    reference counting tests with PyPy) \'\'\'\n    gc.collect()\n    gc.collect()\n\n\ndef pytest_namespace():\n    """"""Add import suppression and test requirements to `pytest` namespace""""""\n    try:\n        import numpy as np\n    except ImportError:\n        np = None\n    try:\n        import scipy\n    except ImportError:\n        scipy = None\n    try:\n        from pybind11_tests.eigen import have_eigen\n    except ImportError:\n        have_eigen = False\n    pypy = platform.python_implementation() == ""PyPy""\n\n    skipif = pytest.mark.skipif\n    return {\n        \'suppress\': suppress,\n        \'requires_numpy\': skipif(not np, reason=""numpy is not installed""),\n        \'requires_scipy\': skipif(not np, reason=""scipy is not installed""),\n        \'requires_eigen_and_numpy\': skipif(not have_eigen or not np,\n                                           reason=""eigen and/or numpy are not installed""),\n        \'requires_eigen_and_scipy\': skipif(not have_eigen or not scipy,\n                                           reason=""eigen and/or scipy are not installed""),\n        \'unsupported_on_pypy\': skipif(pypy, reason=""unsupported on PyPy""),\n        \'unsupported_on_py2\': skipif(sys.version_info.major < 3,\n                                     reason=""unsupported on Python 2.x""),\n        \'gc_collect\': gc_collect\n    }\n\n\ndef _test_import_pybind11():\n    """"""Early diagnostic for test module initialization errors\n\n    When there is an error during initialization, the first import will report the\n    real error while all subsequent imports will report nonsense. This import test\n    is done early (in the pytest configuration file, before any tests) in order to\n    avoid the noise of having all tests fail with identical error messages.\n\n    Any possible exception is caught here and reported manually *without* the stack\n    trace. This further reduces noise since the trace would only show pytest internals\n    which are not useful for debugging pybind11 module issues.\n    """"""\n    # noinspection PyBroadException\n    try:\n        import pybind11_tests  # noqa: F401 imported but unused\n    except Exception as e:\n        print(""Failed to import pybind11_tests from pytest:"")\n        print(""  {}: {}"".format(type(e).__name__, e))\n        sys.exit(1)\n\n\n_test_import_pybind11()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_buffers.py,0,"b'import struct\nimport pytest\nfrom pybind11_tests import buffers as m\nfrom pybind11_tests import ConstructorStats\n\npytestmark = pytest.requires_numpy\n\nwith pytest.suppress(ImportError):\n    import numpy as np\n\n\ndef test_from_python():\n    with pytest.raises(RuntimeError) as excinfo:\n        m.Matrix(np.array([1, 2, 3]))  # trying to assign a 1D array\n    assert str(excinfo.value) == ""Incompatible buffer format!""\n\n    m3 = np.array([[1, 2, 3], [4, 5, 6]]).astype(np.float32)\n    m4 = m.Matrix(m3)\n\n    for i in range(m4.rows()):\n        for j in range(m4.cols()):\n            assert m3[i, j] == m4[i, j]\n\n    cstats = ConstructorStats.get(m.Matrix)\n    assert cstats.alive() == 1\n    del m3, m4\n    assert cstats.alive() == 0\n    assert cstats.values() == [""2x3 matrix""]\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0  # Don\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n\n# PyPy: Memory leak in the ""np.array(m, copy=False)"" call\n# https://bitbucket.org/pypy/pypy/issues/2444\n@pytest.unsupported_on_pypy\ndef test_to_python():\n    mat = m.Matrix(5, 5)\n    assert memoryview(mat).shape == (5, 5)\n\n    assert mat[2, 3] == 0\n    mat[2, 3] = 4\n    assert mat[2, 3] == 4\n\n    mat2 = np.array(mat, copy=False)\n    assert mat2.shape == (5, 5)\n    assert abs(mat2).sum() == 4\n    assert mat2[2, 3] == 4\n    mat2[2, 3] = 5\n    assert mat2[2, 3] == 5\n\n    cstats = ConstructorStats.get(m.Matrix)\n    assert cstats.alive() == 1\n    del mat\n    pytest.gc_collect()\n    assert cstats.alive() == 1\n    del mat2  # holds a mat reference\n    pytest.gc_collect()\n    assert cstats.alive() == 0\n    assert cstats.values() == [""5x5 matrix""]\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0  # Don\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n\n@pytest.unsupported_on_pypy\ndef test_inherited_protocol():\n    """"""SquareMatrix is derived from Matrix and inherits the buffer protocol""""""\n\n    matrix = m.SquareMatrix(5)\n    assert memoryview(matrix).shape == (5, 5)\n    assert np.asarray(matrix).shape == (5, 5)\n\n\n@pytest.unsupported_on_pypy\ndef test_pointer_to_member_fn():\n    for cls in [m.Buffer, m.ConstBuffer, m.DerivedBuffer]:\n        buf = cls()\n        buf.value = 0x12345678\n        value = struct.unpack(\'i\', bytearray(buf))[0]\n        assert value == 0x12345678\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_builtin_casters.py,0,"b'# Python < 3 needs this: coding=utf-8\nimport pytest\n\nfrom pybind11_tests import builtin_casters as m\nfrom pybind11_tests import UserType, IncType\n\n\ndef test_simple_string():\n    assert m.string_roundtrip(""const char *"") == ""const char *""\n\n\ndef test_unicode_conversion():\n    """"""Tests unicode conversion and error reporting.""""""\n    assert m.good_utf8_string() == u""Say utf8\xe2\x80\xbd \xf0\x9f\x8e\x82 \xf0\x9d\x90\x80""\n    assert m.good_utf16_string() == u""b\xe2\x80\xbd\xf0\x9f\x8e\x82\xf0\x9d\x90\x80z""\n    assert m.good_utf32_string() == u""a\xf0\x9d\x90\x80\xf0\x9f\x8e\x82\xe2\x80\xbdz""\n    assert m.good_wchar_string() == u""a\xe2\xb8\x98\xf0\x9d\x90\x80z""\n\n    with pytest.raises(UnicodeDecodeError):\n        m.bad_utf8_string()\n\n    with pytest.raises(UnicodeDecodeError):\n        m.bad_utf16_string()\n\n    # These are provided only if they actually fail (they don\'t when 32-bit and under Python 2.7)\n    if hasattr(m, ""bad_utf32_string""):\n        with pytest.raises(UnicodeDecodeError):\n            m.bad_utf32_string()\n    if hasattr(m, ""bad_wchar_string""):\n        with pytest.raises(UnicodeDecodeError):\n            m.bad_wchar_string()\n\n    assert m.u8_Z() == \'Z\'\n    assert m.u8_eacute() == u\'\xc3\xa9\'\n    assert m.u16_ibang() == u\'\xe2\x80\xbd\'\n    assert m.u32_mathbfA() == u\'\xf0\x9d\x90\x80\'\n    assert m.wchar_heart() == u\'\xe2\x99\xa5\'\n\n\ndef test_single_char_arguments():\n    """"""Tests failures for passing invalid inputs to char-accepting functions""""""\n    def toobig_message(r):\n        return ""Character code point not in range({0:#x})"".format(r)\n    toolong_message = ""Expected a character, but multi-character string found""\n\n    assert m.ord_char(u\'a\') == 0x61  # simple ASCII\n    assert m.ord_char_lv(u\'b\') == 0x62\n    assert m.ord_char(u\'\xc3\xa9\') == 0xE9  # requires 2 bytes in utf-8, but can be stuffed in a char\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_char(u\'\xc4\x80\') == 0x100  # requires 2 bytes, doesn\'t fit in a char\n    assert str(excinfo.value) == toobig_message(0x100)\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_char(u\'ab\')\n    assert str(excinfo.value) == toolong_message\n\n    assert m.ord_char16(u\'a\') == 0x61\n    assert m.ord_char16(u\'\xc3\xa9\') == 0xE9\n    assert m.ord_char16_lv(u\'\xc3\xaa\') == 0xEA\n    assert m.ord_char16(u\'\xc4\x80\') == 0x100\n    assert m.ord_char16(u\'\xe2\x80\xbd\') == 0x203d\n    assert m.ord_char16(u\'\xe2\x99\xa5\') == 0x2665\n    assert m.ord_char16_lv(u\'\xe2\x99\xa1\') == 0x2661\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_char16(u\'\xf0\x9f\x8e\x82\') == 0x1F382  # requires surrogate pair\n    assert str(excinfo.value) == toobig_message(0x10000)\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_char16(u\'aa\')\n    assert str(excinfo.value) == toolong_message\n\n    assert m.ord_char32(u\'a\') == 0x61\n    assert m.ord_char32(u\'\xc3\xa9\') == 0xE9\n    assert m.ord_char32(u\'\xc4\x80\') == 0x100\n    assert m.ord_char32(u\'\xe2\x80\xbd\') == 0x203d\n    assert m.ord_char32(u\'\xe2\x99\xa5\') == 0x2665\n    assert m.ord_char32(u\'\xf0\x9f\x8e\x82\') == 0x1F382\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_char32(u\'aa\')\n    assert str(excinfo.value) == toolong_message\n\n    assert m.ord_wchar(u\'a\') == 0x61\n    assert m.ord_wchar(u\'\xc3\xa9\') == 0xE9\n    assert m.ord_wchar(u\'\xc4\x80\') == 0x100\n    assert m.ord_wchar(u\'\xe2\x80\xbd\') == 0x203d\n    assert m.ord_wchar(u\'\xe2\x99\xa5\') == 0x2665\n    if m.wchar_size == 2:\n        with pytest.raises(ValueError) as excinfo:\n            assert m.ord_wchar(u\'\xf0\x9f\x8e\x82\') == 0x1F382  # requires surrogate pair\n        assert str(excinfo.value) == toobig_message(0x10000)\n    else:\n        assert m.ord_wchar(u\'\xf0\x9f\x8e\x82\') == 0x1F382\n    with pytest.raises(ValueError) as excinfo:\n        assert m.ord_wchar(u\'aa\')\n    assert str(excinfo.value) == toolong_message\n\n\ndef test_bytes_to_string():\n    """"""Tests the ability to pass bytes to C++ string-accepting functions.  Note that this is\n    one-way: the only way to return bytes to Python is via the pybind11::bytes class.""""""\n    # Issue #816\n    import sys\n    byte = bytes if sys.version_info[0] < 3 else str\n\n    assert m.strlen(byte(""hi"")) == 2\n    assert m.string_length(byte(""world"")) == 5\n    assert m.string_length(byte(""a\\x00b"")) == 3\n    assert m.strlen(byte(""a\\x00b"")) == 1  # C-string limitation\n\n    # passing in a utf8 encoded string should work\n    assert m.string_length(u\'\xf0\x9f\x92\xa9\'.encode(""utf8"")) == 4\n\n\n@pytest.mark.skipif(not hasattr(m, ""has_string_view""), reason=""no <string_view>"")\ndef test_string_view(capture):\n    """"""Tests support for C++17 string_view arguments and return values""""""\n    assert m.string_view_chars(""Hi"") == [72, 105]\n    assert m.string_view_chars(""Hi \xf0\x9f\x8e\x82"") == [72, 105, 32, 0xf0, 0x9f, 0x8e, 0x82]\n    assert m.string_view16_chars(""Hi \xf0\x9f\x8e\x82"") == [72, 105, 32, 0xd83c, 0xdf82]\n    assert m.string_view32_chars(""Hi \xf0\x9f\x8e\x82"") == [72, 105, 32, 127874]\n\n    assert m.string_view_return() == ""utf8 secret \xf0\x9f\x8e\x82""\n    assert m.string_view16_return() == ""utf16 secret \xf0\x9f\x8e\x82""\n    assert m.string_view32_return() == ""utf32 secret \xf0\x9f\x8e\x82""\n\n    with capture:\n        m.string_view_print(""Hi"")\n        m.string_view_print(""utf8 \xf0\x9f\x8e\x82"")\n        m.string_view16_print(""utf16 \xf0\x9f\x8e\x82"")\n        m.string_view32_print(""utf32 \xf0\x9f\x8e\x82"")\n    assert capture == """"""\n        Hi 2\n        utf8 \xf0\x9f\x8e\x82 9\n        utf16 \xf0\x9f\x8e\x82 8\n        utf32 \xf0\x9f\x8e\x82 7\n    """"""\n\n    with capture:\n        m.string_view_print(""Hi, ascii"")\n        m.string_view_print(""Hi, utf8 \xf0\x9f\x8e\x82"")\n        m.string_view16_print(""Hi, utf16 \xf0\x9f\x8e\x82"")\n        m.string_view32_print(""Hi, utf32 \xf0\x9f\x8e\x82"")\n    assert capture == """"""\n        Hi, ascii 9\n        Hi, utf8 \xf0\x9f\x8e\x82 13\n        Hi, utf16 \xf0\x9f\x8e\x82 12\n        Hi, utf32 \xf0\x9f\x8e\x82 11\n    """"""\n\n\ndef test_integer_casting():\n    """"""Issue #929 - out-of-range integer values shouldn\'t be accepted""""""\n    import sys\n    assert m.i32_str(-1) == ""-1""\n    assert m.i64_str(-1) == ""-1""\n    assert m.i32_str(2000000000) == ""2000000000""\n    assert m.u32_str(2000000000) == ""2000000000""\n    if sys.version_info < (3,):\n        assert m.i32_str(long(-1)) == ""-1""  # noqa: F821 undefined name \'long\'\n        assert m.i64_str(long(-1)) == ""-1""  # noqa: F821 undefined name \'long\'\n        assert m.i64_str(long(-999999999999)) == ""-999999999999""  # noqa: F821 undefined name\n        assert m.u64_str(long(999999999999)) == ""999999999999""  # noqa: F821 undefined name \'long\'\n    else:\n        assert m.i64_str(-999999999999) == ""-999999999999""\n        assert m.u64_str(999999999999) == ""999999999999""\n\n    with pytest.raises(TypeError) as excinfo:\n        m.u32_str(-1)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.u64_str(-1)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.i32_str(-3000000000)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.i32_str(3000000000)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    if sys.version_info < (3,):\n        with pytest.raises(TypeError) as excinfo:\n            m.u32_str(long(-1))  # noqa: F821 undefined name \'long\'\n        assert ""incompatible function arguments"" in str(excinfo.value)\n        with pytest.raises(TypeError) as excinfo:\n            m.u64_str(long(-1))  # noqa: F821 undefined name \'long\'\n        assert ""incompatible function arguments"" in str(excinfo.value)\n\n\ndef test_tuple(doc):\n    """"""std::pair <-> tuple & std::tuple <-> tuple""""""\n    assert m.pair_passthrough((True, ""test"")) == (""test"", True)\n    assert m.tuple_passthrough((True, ""test"", 5)) == (5, ""test"", True)\n    # Any sequence can be cast to a std::pair or std::tuple\n    assert m.pair_passthrough([True, ""test""]) == (""test"", True)\n    assert m.tuple_passthrough([True, ""test"", 5]) == (5, ""test"", True)\n    assert m.empty_tuple() == ()\n\n    assert doc(m.pair_passthrough) == """"""\n        pair_passthrough(arg0: Tuple[bool, str]) -> Tuple[str, bool]\n\n        Return a pair in reversed order\n    """"""\n    assert doc(m.tuple_passthrough) == """"""\n        tuple_passthrough(arg0: Tuple[bool, str, int]) -> Tuple[int, str, bool]\n\n        Return a triple in reversed order\n    """"""\n\n    assert m.rvalue_pair() == (""rvalue"", ""rvalue"")\n    assert m.lvalue_pair() == (""lvalue"", ""lvalue"")\n    assert m.rvalue_tuple() == (""rvalue"", ""rvalue"", ""rvalue"")\n    assert m.lvalue_tuple() == (""lvalue"", ""lvalue"", ""lvalue"")\n    assert m.rvalue_nested() == (""rvalue"", (""rvalue"", (""rvalue"", ""rvalue"")))\n    assert m.lvalue_nested() == (""lvalue"", (""lvalue"", (""lvalue"", ""lvalue"")))\n\n\ndef test_builtins_cast_return_none():\n    """"""Casters produced with PYBIND11_TYPE_CASTER() should convert nullptr to None""""""\n    assert m.return_none_string() is None\n    assert m.return_none_char() is None\n    assert m.return_none_bool() is None\n    assert m.return_none_int() is None\n    assert m.return_none_float() is None\n\n\ndef test_none_deferred():\n    """"""None passed as various argument types should defer to other overloads""""""\n    assert not m.defer_none_cstring(""abc"")\n    assert m.defer_none_cstring(None)\n    assert not m.defer_none_custom(UserType())\n    assert m.defer_none_custom(None)\n    assert m.nodefer_none_void(None)\n\n\ndef test_void_caster():\n    assert m.load_nullptr_t(None) is None\n    assert m.cast_nullptr_t() is None\n\n\ndef test_reference_wrapper():\n    """"""std::reference_wrapper for builtin and user types""""""\n    assert m.refwrap_builtin(42) == 420\n    assert m.refwrap_usertype(UserType(42)) == 42\n\n    with pytest.raises(TypeError) as excinfo:\n        m.refwrap_builtin(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        m.refwrap_usertype(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    a1 = m.refwrap_list(copy=True)\n    a2 = m.refwrap_list(copy=True)\n    assert [x.value for x in a1] == [2, 3]\n    assert [x.value for x in a2] == [2, 3]\n    assert not a1[0] is a2[0] and not a1[1] is a2[1]\n\n    b1 = m.refwrap_list(copy=False)\n    b2 = m.refwrap_list(copy=False)\n    assert [x.value for x in b1] == [1, 2]\n    assert [x.value for x in b2] == [1, 2]\n    assert b1[0] is b2[0] and b1[1] is b2[1]\n\n    assert m.refwrap_iiw(IncType(5)) == 5\n    assert m.refwrap_call_iiw(IncType(10), m.refwrap_iiw) == [10, 10, 10, 10]\n\n\ndef test_complex_cast():\n    """"""std::complex casts""""""\n    assert m.complex_cast(1) == ""1.0""\n    assert m.complex_cast(2j) == ""(0.0, 2.0)""\n\n\ndef test_bool_caster():\n    """"""Test bool caster implicit conversions.""""""\n    convert, noconvert = m.bool_passthrough, m.bool_passthrough_noconvert\n\n    def require_implicit(v):\n        pytest.raises(TypeError, noconvert, v)\n\n    def cant_convert(v):\n        pytest.raises(TypeError, convert, v)\n\n    # straight up bool\n    assert convert(True) is True\n    assert convert(False) is False\n    assert noconvert(True) is True\n    assert noconvert(False) is False\n\n    # None requires implicit conversion\n    require_implicit(None)\n    assert convert(None) is False\n\n    class A(object):\n        def __init__(self, x):\n            self.x = x\n\n        def __nonzero__(self):\n            return self.x\n\n        def __bool__(self):\n            return self.x\n\n    class B(object):\n        pass\n\n    # Arbitrary objects are not accepted\n    cant_convert(object())\n    cant_convert(B())\n\n    # Objects with __nonzero__ / __bool__ defined can be converted\n    require_implicit(A(True))\n    assert convert(A(True)) is True\n    assert convert(A(False)) is False\n\n\n@pytest.requires_numpy\ndef test_numpy_bool():\n    import numpy as np\n    convert, noconvert = m.bool_passthrough, m.bool_passthrough_noconvert\n\n    # np.bool_ is not considered implicit\n    assert convert(np.bool_(True)) is True\n    assert convert(np.bool_(False)) is False\n    assert noconvert(np.bool_(True)) is True\n    assert noconvert(np.bool_(False)) is False\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_call_policies.py,0,"b'import pytest\nfrom pybind11_tests import call_policies as m\nfrom pybind11_tests import ConstructorStats, UserType\n\n\ndef test_keep_alive_argument(capture):\n    n_inst = ConstructorStats.detail_reg_inst()\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.addChild(m.Child())\n        assert ConstructorStats.detail_reg_inst() == n_inst + 1\n    assert capture == """"""\n        Allocating child.\n        Releasing child.\n    """"""\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == ""Releasing parent.""\n\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.addChildKeepAlive(m.Child())\n        assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    assert capture == ""Allocating child.""\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n    """"""\n\n\ndef test_keep_alive_return_value(capture):\n    n_inst = ConstructorStats.detail_reg_inst()\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.returnChild()\n        assert ConstructorStats.detail_reg_inst() == n_inst + 1\n    assert capture == """"""\n        Allocating child.\n        Releasing child.\n    """"""\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == ""Releasing parent.""\n\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.returnChildKeepAlive()\n        assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    assert capture == ""Allocating child.""\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n    """"""\n\n\ndef test_keep_alive_single():\n    """"""Issue #1251 - patients are stored multiple times when given to the same nurse""""""\n\n    nurse, p1, p2 = UserType(), UserType(), UserType()\n    b = m.refcount(nurse)\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b, b]\n    m.add_patient(nurse, p1)\n    assert m.get_patients(nurse) == [p1, ]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b]\n    m.add_patient(nurse, p1)\n    assert m.get_patients(nurse) == [p1, ]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b]\n    m.add_patient(nurse, p1)\n    assert m.get_patients(nurse) == [p1, ]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b]\n    m.add_patient(nurse, p2)\n    assert m.get_patients(nurse) == [p1, p2]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b + 1]\n    m.add_patient(nurse, p2)\n    assert m.get_patients(nurse) == [p1, p2]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b + 1]\n    m.add_patient(nurse, p2)\n    m.add_patient(nurse, p1)\n    assert m.get_patients(nurse) == [p1, p2]\n    assert [m.refcount(nurse), m.refcount(p1), m.refcount(p2)] == [b, b + 1, b + 1]\n    del nurse\n    assert [m.refcount(p1), m.refcount(p2)] == [b, b]\n\n\n# https://bitbucket.org/pypy/pypy/issues/2447\n@pytest.unsupported_on_pypy\ndef test_alive_gc(capture):\n    n_inst = ConstructorStats.detail_reg_inst()\n    p = m.ParentGC()\n    p.addChildKeepAlive(m.Child())\n    assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    lst = [p]\n    lst.append(lst)   # creates a circular reference\n    with capture:\n        del p, lst\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n    """"""\n\n\ndef test_alive_gc_derived(capture):\n    class Derived(m.Parent):\n        pass\n\n    n_inst = ConstructorStats.detail_reg_inst()\n    p = Derived()\n    p.addChildKeepAlive(m.Child())\n    assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    lst = [p]\n    lst.append(lst)   # creates a circular reference\n    with capture:\n        del p, lst\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n    """"""\n\n\ndef test_alive_gc_multi_derived(capture):\n    class Derived(m.Parent, m.Child):\n        def __init__(self):\n            m.Parent.__init__(self)\n            m.Child.__init__(self)\n\n    n_inst = ConstructorStats.detail_reg_inst()\n    p = Derived()\n    p.addChildKeepAlive(m.Child())\n    # +3 rather than +2 because Derived corresponds to two registered instances\n    assert ConstructorStats.detail_reg_inst() == n_inst + 3\n    lst = [p]\n    lst.append(lst)   # creates a circular reference\n    with capture:\n        del p, lst\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n        Releasing child.\n    """"""\n\n\ndef test_return_none(capture):\n    n_inst = ConstructorStats.detail_reg_inst()\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.returnNullChildKeepAliveChild()\n        assert ConstructorStats.detail_reg_inst() == n_inst + 1\n    assert capture == """"\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == ""Releasing parent.""\n\n    with capture:\n        p = m.Parent()\n    assert capture == ""Allocating parent.""\n    with capture:\n        p.returnNullChildKeepAliveParent()\n        assert ConstructorStats.detail_reg_inst() == n_inst + 1\n    assert capture == """"\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == ""Releasing parent.""\n\n\ndef test_keep_alive_constructor(capture):\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    with capture:\n        p = m.Parent(m.Child())\n        assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    assert capture == """"""\n        Allocating child.\n        Allocating parent.\n    """"""\n    with capture:\n        del p\n        assert ConstructorStats.detail_reg_inst() == n_inst\n    assert capture == """"""\n        Releasing parent.\n        Releasing child.\n    """"""\n\n\ndef test_call_guard():\n    assert m.unguarded_call() == ""unguarded""\n    assert m.guarded_call() == ""guarded""\n\n    assert m.multiple_guards_correct_order() == ""guarded & guarded""\n    assert m.multiple_guards_wrong_order() == ""unguarded & guarded""\n\n    if hasattr(m, ""with_gil""):\n        assert m.with_gil() == ""GIL held""\n        assert m.without_gil() == ""GIL released""\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_callbacks.py,0,"b'import pytest\nfrom pybind11_tests import callbacks as m\n\n\ndef test_callbacks():\n    from functools import partial\n\n    def func1():\n        return ""func1""\n\n    def func2(a, b, c, d):\n        return ""func2"", a, b, c, d\n\n    def func3(a):\n        return ""func3({})"".format(a)\n\n    assert m.test_callback1(func1) == ""func1""\n    assert m.test_callback2(func2) == (""func2"", ""Hello"", ""x"", True, 5)\n    assert m.test_callback1(partial(func2, 1, 2, 3, 4)) == (""func2"", 1, 2, 3, 4)\n    assert m.test_callback1(partial(func3, ""partial"")) == ""func3(partial)""\n    assert m.test_callback3(lambda i: i + 1) == ""func(43) = 44""\n\n    f = m.test_callback4()\n    assert f(43) == 44\n    f = m.test_callback5()\n    assert f(number=43) == 44\n\n\ndef test_bound_method_callback():\n    # Bound Python method:\n    class MyClass:\n        def double(self, val):\n            return 2 * val\n\n    z = MyClass()\n    assert m.test_callback3(z.double) == ""func(43) = 86""\n\n    z = m.CppBoundMethodTest()\n    assert m.test_callback3(z.triple) == ""func(43) = 129""\n\n\ndef test_keyword_args_and_generalized_unpacking():\n\n    def f(*args, **kwargs):\n        return args, kwargs\n\n    assert m.test_tuple_unpacking(f) == ((""positional"", 1, 2, 3, 4, 5, 6), {})\n    assert m.test_dict_unpacking(f) == ((""positional"", 1), {""key"": ""value"", ""a"": 1, ""b"": 2})\n    assert m.test_keyword_args(f) == ((), {""x"": 10, ""y"": 20})\n    assert m.test_unpacking_and_keywords1(f) == ((1, 2), {""c"": 3, ""d"": 4})\n    assert m.test_unpacking_and_keywords2(f) == (\n        (""positional"", 1, 2, 3, 4, 5),\n        {""key"": ""value"", ""a"": 1, ""b"": 2, ""c"": 3, ""d"": 4, ""e"": 5}\n    )\n\n    with pytest.raises(TypeError) as excinfo:\n        m.test_unpacking_error1(f)\n    assert ""Got multiple values for keyword argument"" in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        m.test_unpacking_error2(f)\n    assert ""Got multiple values for keyword argument"" in str(excinfo.value)\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.test_arg_conversion_error1(f)\n    assert ""Unable to convert call argument"" in str(excinfo.value)\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.test_arg_conversion_error2(f)\n    assert ""Unable to convert call argument"" in str(excinfo.value)\n\n\ndef test_lambda_closure_cleanup():\n    m.test_cleanup()\n    cstats = m.payload_cstats()\n    assert cstats.alive() == 0\n    assert cstats.copy_constructions == 1\n    assert cstats.move_constructions >= 1\n\n\ndef test_cpp_function_roundtrip():\n    """"""Test if passing a function pointer from C++ -> Python -> C++ yields the original pointer""""""\n\n    assert m.test_dummy_function(m.dummy_function) == ""matches dummy_function: eval(1) = 2""\n    assert (m.test_dummy_function(m.roundtrip(m.dummy_function)) ==\n            ""matches dummy_function: eval(1) = 2"")\n    assert m.roundtrip(None, expect_none=True) is None\n    assert (m.test_dummy_function(lambda x: x + 2) ==\n            ""can\'t convert to function pointer: eval(1) = 3"")\n\n    with pytest.raises(TypeError) as excinfo:\n        m.test_dummy_function(m.dummy_function2)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        m.test_dummy_function(lambda x, y: x + y)\n    assert any(s in str(excinfo.value) for s in (""missing 1 required positional argument"",\n                                                 ""takes exactly 2 arguments""))\n\n\ndef test_function_signatures(doc):\n    assert doc(m.test_callback3) == ""test_callback3(arg0: Callable[[int], int]) -> str""\n    assert doc(m.test_callback4) == ""test_callback4() -> Callable[[int], int]""\n\n\ndef test_movable_object():\n    assert m.callback_with_movable(lambda _: None) is True\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_chrono.py,0,"b'from pybind11_tests import chrono as m\nimport datetime\n\n\ndef test_chrono_system_clock():\n\n    # Get the time from both c++ and datetime\n    date1 = m.test_chrono1()\n    date2 = datetime.datetime.today()\n\n    # The returned value should be a datetime\n    assert isinstance(date1, datetime.datetime)\n\n    # The numbers should vary by a very small amount (time it took to execute)\n    diff = abs(date1 - date2)\n\n    # There should never be a days/seconds difference\n    assert diff.days == 0\n    assert diff.seconds == 0\n\n    # We test that no more than about 0.5 seconds passes here\n    # This makes sure that the dates created are very close to the same\n    # but if the testing system is incredibly overloaded this should still pass\n    assert diff.microseconds < 500000\n\n\ndef test_chrono_system_clock_roundtrip():\n    date1 = datetime.datetime.today()\n\n    # Roundtrip the time\n    date2 = m.test_chrono2(date1)\n\n    # The returned value should be a datetime\n    assert isinstance(date2, datetime.datetime)\n\n    # They should be identical (no information lost on roundtrip)\n    diff = abs(date1 - date2)\n    assert diff.days == 0\n    assert diff.seconds == 0\n    assert diff.microseconds == 0\n\n\ndef test_chrono_duration_roundtrip():\n\n    # Get the difference between two times (a timedelta)\n    date1 = datetime.datetime.today()\n    date2 = datetime.datetime.today()\n    diff = date2 - date1\n\n    # Make sure this is a timedelta\n    assert isinstance(diff, datetime.timedelta)\n\n    cpp_diff = m.test_chrono3(diff)\n\n    assert cpp_diff.days == diff.days\n    assert cpp_diff.seconds == diff.seconds\n    assert cpp_diff.microseconds == diff.microseconds\n\n\ndef test_chrono_duration_subtraction_equivalence():\n\n    date1 = datetime.datetime.today()\n    date2 = datetime.datetime.today()\n\n    diff = date2 - date1\n    cpp_diff = m.test_chrono4(date2, date1)\n\n    assert cpp_diff.days == diff.days\n    assert cpp_diff.seconds == diff.seconds\n    assert cpp_diff.microseconds == diff.microseconds\n\n\ndef test_chrono_steady_clock():\n    time1 = m.test_chrono5()\n    assert isinstance(time1, datetime.timedelta)\n\n\ndef test_chrono_steady_clock_roundtrip():\n    time1 = datetime.timedelta(days=10, seconds=10, microseconds=100)\n    time2 = m.test_chrono6(time1)\n\n    assert isinstance(time2, datetime.timedelta)\n\n    # They should be identical (no information lost on roundtrip)\n    assert time1.days == time2.days\n    assert time1.seconds == time2.seconds\n    assert time1.microseconds == time2.microseconds\n\n\ndef test_floating_point_duration():\n    # Test using a floating point number in seconds\n    time = m.test_chrono7(35.525123)\n\n    assert isinstance(time, datetime.timedelta)\n\n    assert time.seconds == 35\n    assert 525122 <= time.microseconds <= 525123\n\n    diff = m.test_chrono_float_diff(43.789012, 1.123456)\n    assert diff.seconds == 42\n    assert 665556 <= diff.microseconds <= 665557\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_class.py,0,"b'import pytest\n\nfrom pybind11_tests import class_ as m\nfrom pybind11_tests import UserType, ConstructorStats\n\n\ndef test_repr():\n    # In Python 3.3+, repr() accesses __qualname__\n    assert ""pybind11_type"" in repr(type(UserType))\n    assert ""UserType"" in repr(UserType)\n\n\ndef test_instance(msg):\n    with pytest.raises(TypeError) as excinfo:\n        m.NoConstructor()\n    assert msg(excinfo.value) == ""m.class_.NoConstructor: No constructor defined!""\n\n    instance = m.NoConstructor.new_instance()\n\n    cstats = ConstructorStats.get(m.NoConstructor)\n    assert cstats.alive() == 1\n    del instance\n    assert cstats.alive() == 0\n\n\ndef test_docstrings(doc):\n    assert doc(UserType) == ""A `py::class_` type for testing""\n    assert UserType.__name__ == ""UserType""\n    assert UserType.__module__ == ""pybind11_tests""\n    assert UserType.get_value.__name__ == ""get_value""\n    assert UserType.get_value.__module__ == ""pybind11_tests""\n\n    assert doc(UserType.get_value) == """"""\n        get_value(self: m.UserType) -> int\n\n        Get value using a method\n    """"""\n    assert doc(UserType.value) == ""Get/set value using a property""\n\n    assert doc(m.NoConstructor.new_instance) == """"""\n        new_instance() -> m.class_.NoConstructor\n\n        Return an instance\n    """"""\n\n\ndef test_qualname(doc):\n    """"""Tests that a properly qualified name is set in __qualname__ (even in pre-3.3, where we\n    backport the attribute) and that generated docstrings properly use it and the module name""""""\n    assert m.NestBase.__qualname__ == ""NestBase""\n    assert m.NestBase.Nested.__qualname__ == ""NestBase.Nested""\n\n    assert doc(m.NestBase.__init__) == """"""\n        __init__(self: m.class_.NestBase) -> None\n    """"""\n    assert doc(m.NestBase.g) == """"""\n        g(self: m.class_.NestBase, arg0: m.class_.NestBase.Nested) -> None\n    """"""\n    assert doc(m.NestBase.Nested.__init__) == """"""\n        __init__(self: m.class_.NestBase.Nested) -> None\n    """"""\n    assert doc(m.NestBase.Nested.fn) == """"""\n        fn(self: m.class_.NestBase.Nested, arg0: int, arg1: m.class_.NestBase, arg2: m.class_.NestBase.Nested) -> None\n    """"""  # noqa: E501 line too long\n    assert doc(m.NestBase.Nested.fa) == """"""\n        fa(self: m.class_.NestBase.Nested, a: int, b: m.class_.NestBase, c: m.class_.NestBase.Nested) -> None\n    """"""  # noqa: E501 line too long\n    assert m.NestBase.__module__ == ""pybind11_tests.class_""\n    assert m.NestBase.Nested.__module__ == ""pybind11_tests.class_""\n\n\ndef test_inheritance(msg):\n    roger = m.Rabbit(\'Rabbit\')\n    assert roger.name() + "" is a "" + roger.species() == ""Rabbit is a parrot""\n    assert m.pet_name_species(roger) == ""Rabbit is a parrot""\n\n    polly = m.Pet(\'Polly\', \'parrot\')\n    assert polly.name() + "" is a "" + polly.species() == ""Polly is a parrot""\n    assert m.pet_name_species(polly) == ""Polly is a parrot""\n\n    molly = m.Dog(\'Molly\')\n    assert molly.name() + "" is a "" + molly.species() == ""Molly is a dog""\n    assert m.pet_name_species(molly) == ""Molly is a dog""\n\n    fred = m.Hamster(\'Fred\')\n    assert fred.name() + "" is a "" + fred.species() == ""Fred is a rodent""\n\n    assert m.dog_bark(molly) == ""Woof!""\n\n    with pytest.raises(TypeError) as excinfo:\n        m.dog_bark(polly)\n    assert msg(excinfo.value) == """"""\n        dog_bark(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: m.class_.Dog) -> str\n\n        Invoked with: <m.class_.Pet object at 0>\n    """"""\n\n    with pytest.raises(TypeError) as excinfo:\n        m.Chimera(""lion"", ""goat"")\n    assert ""No constructor defined!"" in str(excinfo.value)\n\n\ndef test_automatic_upcasting():\n    assert type(m.return_class_1()).__name__ == ""DerivedClass1""\n    assert type(m.return_class_2()).__name__ == ""DerivedClass2""\n    assert type(m.return_none()).__name__ == ""NoneType""\n    # Repeat these a few times in a random order to ensure no invalid caching is applied\n    assert type(m.return_class_n(1)).__name__ == ""DerivedClass1""\n    assert type(m.return_class_n(2)).__name__ == ""DerivedClass2""\n    assert type(m.return_class_n(0)).__name__ == ""BaseClass""\n    assert type(m.return_class_n(2)).__name__ == ""DerivedClass2""\n    assert type(m.return_class_n(2)).__name__ == ""DerivedClass2""\n    assert type(m.return_class_n(0)).__name__ == ""BaseClass""\n    assert type(m.return_class_n(1)).__name__ == ""DerivedClass1""\n\n\ndef test_isinstance():\n    objects = [tuple(), dict(), m.Pet(""Polly"", ""parrot"")] + [m.Dog(""Molly"")] * 4\n    expected = (True, True, True, True, True, False, False)\n    assert m.check_instances(objects) == expected\n\n\ndef test_mismatched_holder():\n    import re\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.mismatched_holder_1()\n    assert re.match(\'generic_type: type "".*MismatchDerived1"" does not have a non-default \'\n                    \'holder type while its base "".*MismatchBase1"" does\', str(excinfo.value))\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.mismatched_holder_2()\n    assert re.match(\'generic_type: type "".*MismatchDerived2"" has a non-default holder type \'\n                    \'while its base "".*MismatchBase2"" does not\', str(excinfo.value))\n\n\ndef test_override_static():\n    """"""#511: problem with inheritance + overwritten def_static""""""\n    b = m.MyBase.make()\n    d1 = m.MyDerived.make2()\n    d2 = m.MyDerived.make()\n\n    assert isinstance(b, m.MyBase)\n    assert isinstance(d1, m.MyDerived)\n    assert isinstance(d2, m.MyDerived)\n\n\ndef test_implicit_conversion_life_support():\n    """"""Ensure the lifetime of temporary objects created for implicit conversions""""""\n    assert m.implicitly_convert_argument(UserType(5)) == 5\n    assert m.implicitly_convert_variable(UserType(5)) == 5\n\n    assert ""outside a bound function"" in m.implicitly_convert_variable_fail(UserType(5))\n\n\ndef test_operator_new_delete(capture):\n    """"""Tests that class-specific operator new/delete functions are invoked""""""\n\n    class SubAliased(m.AliasedHasOpNewDelSize):\n        pass\n\n    with capture:\n        a = m.HasOpNewDel()\n        b = m.HasOpNewDelSize()\n        d = m.HasOpNewDelBoth()\n    assert capture == """"""\n        A new 8\n        B new 4\n        D new 32\n    """"""\n    sz_alias = str(m.AliasedHasOpNewDelSize.size_alias)\n    sz_noalias = str(m.AliasedHasOpNewDelSize.size_noalias)\n    with capture:\n        c = m.AliasedHasOpNewDelSize()\n        c2 = SubAliased()\n    assert capture == (\n        ""C new "" + sz_noalias + ""\\n"" +\n        ""C new "" + sz_alias + ""\\n""\n    )\n\n    with capture:\n        del a\n        pytest.gc_collect()\n        del b\n        pytest.gc_collect()\n        del d\n        pytest.gc_collect()\n    assert capture == """"""\n        A delete\n        B delete 4\n        D delete\n    """"""\n\n    with capture:\n        del c\n        pytest.gc_collect()\n        del c2\n        pytest.gc_collect()\n    assert capture == (\n        ""C delete "" + sz_noalias + ""\\n"" +\n        ""C delete "" + sz_alias + ""\\n""\n    )\n\n\ndef test_bind_protected_functions():\n    """"""Expose protected member functions to Python using a helper class""""""\n    a = m.ProtectedA()\n    assert a.foo() == 42\n\n    b = m.ProtectedB()\n    assert b.foo() == 42\n\n    class C(m.ProtectedB):\n        def __init__(self):\n            m.ProtectedB.__init__(self)\n\n        def foo(self):\n            return 0\n\n    c = C()\n    assert c.foo() == 0\n\n\ndef test_brace_initialization():\n    """""" Tests that simple POD classes can be constructed using C++11 brace initialization """"""\n    a = m.BraceInitialization(123, ""test"")\n    assert a.field1 == 123\n    assert a.field2 == ""test""\n\n    # Tests that a non-simple class doesn\'t get brace initialization (if the\n    # class defines an initializer_list constructor, in particular, it would\n    # win over the expected constructor).\n    b = m.NoBraceInitialization([123, 456])\n    assert b.vec == [123, 456]\n\n\n@pytest.unsupported_on_pypy\ndef test_class_refcount():\n    """"""Instances must correctly increase/decrease the reference count of their types (#1029)""""""\n    from sys import getrefcount\n\n    class PyDog(m.Dog):\n        pass\n\n    for cls in m.Dog, PyDog:\n        refcount_1 = getrefcount(cls)\n        molly = [cls(""Molly"") for _ in range(10)]\n        refcount_2 = getrefcount(cls)\n\n        del molly\n        pytest.gc_collect()\n        refcount_3 = getrefcount(cls)\n\n        assert refcount_1 == refcount_3\n        assert refcount_2 > refcount_1\n\n\ndef test_reentrant_implicit_conversion_failure(msg):\n    # ensure that there is no runaway reentrant implicit conversion (#1035)\n    with pytest.raises(TypeError) as excinfo:\n        m.BogusImplicitConversion(0)\n    assert msg(excinfo.value) == \'\'\'\n        __init__(): incompatible constructor arguments. The following argument types are supported:\n            1. m.class_.BogusImplicitConversion(arg0: m.class_.BogusImplicitConversion)\n\n        Invoked with: 0\n    \'\'\'\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_constants_and_functions.py,0,"b'from pybind11_tests import constants_and_functions as m\n\n\ndef test_constants():\n    assert m.some_constant == 14\n\n\ndef test_function_overloading():\n    assert m.test_function() == ""test_function()""\n    assert m.test_function(7) == ""test_function(7)""\n    assert m.test_function(m.MyEnum.EFirstEntry) == ""test_function(enum=1)""\n    assert m.test_function(m.MyEnum.ESecondEntry) == ""test_function(enum=2)""\n\n    assert m.test_function() == ""test_function()""\n    assert m.test_function(""abcd"") == ""test_function(char *)""\n    assert m.test_function(1, 1.0) == ""test_function(int, float)""\n    assert m.test_function(1, 1.0) == ""test_function(int, float)""\n    assert m.test_function(2.0, 2) == ""test_function(float, int)""\n\n\ndef test_bytes():\n    assert m.print_bytes(m.return_bytes()) == ""bytes[1 0 2 0]""\n\n\ndef test_exception_specifiers():\n    c = m.C()\n    assert c.m1(2) == 1\n    assert c.m2(3) == 1\n    assert c.m3(5) == 2\n    assert c.m4(7) == 3\n    assert c.m5(10) == 5\n    assert c.m6(14) == 8\n    assert c.m7(20) == 13\n    assert c.m8(29) == 21\n\n    assert m.f1(33) == 34\n    assert m.f2(53) == 55\n    assert m.f3(86) == 89\n    assert m.f4(140) == 144\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_copy_move.py,0,"b'import pytest\nfrom pybind11_tests import copy_move_policies as m\n\n\ndef test_lacking_copy_ctor():\n    with pytest.raises(RuntimeError) as excinfo:\n        m.lacking_copy_ctor.get_one()\n    assert ""the object is non-copyable!"" in str(excinfo.value)\n\n\ndef test_lacking_move_ctor():\n    with pytest.raises(RuntimeError) as excinfo:\n        m.lacking_move_ctor.get_one()\n    assert ""the object is neither movable nor copyable!"" in str(excinfo.value)\n\n\ndef test_move_and_copy_casts():\n    """"""Cast some values in C++ via custom type casters and count the number of moves/copies.""""""\n\n    cstats = m.move_and_copy_cstats()\n    c_m, c_mc, c_c = cstats[""MoveOnlyInt""], cstats[""MoveOrCopyInt""], cstats[""CopyOnlyInt""]\n\n    # The type move constructions/assignments below each get incremented: the move assignment comes\n    # from the type_caster load; the move construction happens when extracting that via a cast or\n    # loading into an argument.\n    assert m.move_and_copy_casts(3) == 18\n    assert c_m.copy_assignments + c_m.copy_constructions == 0\n    assert c_m.move_assignments == 2\n    assert c_m.move_constructions >= 2\n    assert c_mc.alive() == 0\n    assert c_mc.copy_assignments + c_mc.copy_constructions == 0\n    assert c_mc.move_assignments == 2\n    assert c_mc.move_constructions >= 2\n    assert c_c.alive() == 0\n    assert c_c.copy_assignments == 2\n    assert c_c.copy_constructions >= 2\n    assert c_m.alive() + c_mc.alive() + c_c.alive() == 0\n\n\ndef test_move_and_copy_loads():\n    """"""Call some functions that load arguments via custom type casters and count the number of\n    moves/copies.""""""\n\n    cstats = m.move_and_copy_cstats()\n    c_m, c_mc, c_c = cstats[""MoveOnlyInt""], cstats[""MoveOrCopyInt""], cstats[""CopyOnlyInt""]\n\n    assert m.move_only(10) == 10  # 1 move, c_m\n    assert m.move_or_copy(11) == 11  # 1 move, c_mc\n    assert m.copy_only(12) == 12  # 1 copy, c_c\n    assert m.move_pair((13, 14)) == 27  # 1 c_m move, 1 c_mc move\n    assert m.move_tuple((15, 16, 17)) == 48  # 2 c_m moves, 1 c_mc move\n    assert m.copy_tuple((18, 19)) == 37  # 2 c_c copies\n    # Direct constructions: 2 c_m moves, 2 c_mc moves, 1 c_c copy\n    # Extra moves/copies when moving pairs/tuples: 3 c_m, 3 c_mc, 2 c_c\n    assert m.move_copy_nested((1, ((2, 3, (4,)), 5))) == 15\n\n    assert c_m.copy_assignments + c_m.copy_constructions == 0\n    assert c_m.move_assignments == 6\n    assert c_m.move_constructions == 9\n    assert c_mc.copy_assignments + c_mc.copy_constructions == 0\n    assert c_mc.move_assignments == 5\n    assert c_mc.move_constructions == 8\n    assert c_c.copy_assignments == 4\n    assert c_c.copy_constructions == 6\n    assert c_m.alive() + c_mc.alive() + c_c.alive() == 0\n\n\n@pytest.mark.skipif(not m.has_optional, reason=\'no <optional>\')\ndef test_move_and_copy_load_optional():\n    """"""Tests move/copy loads of std::optional arguments""""""\n\n    cstats = m.move_and_copy_cstats()\n    c_m, c_mc, c_c = cstats[""MoveOnlyInt""], cstats[""MoveOrCopyInt""], cstats[""CopyOnlyInt""]\n\n    # The extra move/copy constructions below come from the std::optional move (which has to move\n    # its arguments):\n    assert m.move_optional(10) == 10  # c_m: 1 move assign, 2 move construct\n    assert m.move_or_copy_optional(11) == 11  # c_mc: 1 move assign, 2 move construct\n    assert m.copy_optional(12) == 12  # c_c: 1 copy assign, 2 copy construct\n    # 1 move assign + move construct moves each of c_m, c_mc, 1 c_c copy\n    # +1 move/copy construct each from moving the tuple\n    # +1 move/copy construct each from moving the optional (which moves the tuple again)\n    assert m.move_optional_tuple((3, 4, 5)) == 12\n\n    assert c_m.copy_assignments + c_m.copy_constructions == 0\n    assert c_m.move_assignments == 2\n    assert c_m.move_constructions == 5\n    assert c_mc.copy_assignments + c_mc.copy_constructions == 0\n    assert c_mc.move_assignments == 2\n    assert c_mc.move_constructions == 5\n    assert c_c.copy_assignments == 2\n    assert c_c.copy_constructions == 5\n    assert c_m.alive() + c_mc.alive() + c_c.alive() == 0\n\n\ndef test_private_op_new():\n    """"""An object with a private `operator new` cannot be returned by value""""""\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.private_op_new_value()\n    assert ""the object is neither movable nor copyable"" in str(excinfo.value)\n\n    assert m.private_op_new_reference().value == 1\n\n\ndef test_move_fallback():\n    """"""#389: rvp::move should fall-through to copy on non-movable objects""""""\n\n    m2 = m.get_moveissue2(2)\n    assert m2.value == 2\n    m1 = m.get_moveissue1(1)\n    assert m1.value == 1\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_docstring_options.py,0,"b'from pybind11_tests import docstring_options as m\n\n\ndef test_docstring_options():\n    # options.disable_function_signatures()\n    assert not m.test_function1.__doc__\n\n    assert m.test_function2.__doc__ == ""A custom docstring""\n\n    # docstring specified on just the first overload definition:\n    assert m.test_overloaded1.__doc__ == ""Overload docstring""\n\n    # docstring on both overloads:\n    assert m.test_overloaded2.__doc__ == ""overload docstring 1\\noverload docstring 2""\n\n    # docstring on only second overload:\n    assert m.test_overloaded3.__doc__ == ""Overload docstr""\n\n    # options.enable_function_signatures()\n    assert m.test_function3.__doc__ .startswith(""test_function3(a: int, b: int) -> None"")\n\n    assert m.test_function4.__doc__ .startswith(""test_function4(a: int, b: int) -> None"")\n    assert m.test_function4.__doc__ .endswith(""A custom docstring\\n"")\n\n    # options.disable_function_signatures()\n    # options.disable_user_defined_docstrings()\n    assert not m.test_function5.__doc__\n\n    # nested options.enable_user_defined_docstrings()\n    assert m.test_function6.__doc__ == ""A custom docstring""\n\n    # RAII destructor\n    assert m.test_function7.__doc__ .startswith(""test_function7(a: int, b: int) -> None"")\n    assert m.test_function7.__doc__ .endswith(""A custom docstring\\n"")\n\n    # Suppression of user-defined docstrings for non-function objects\n    assert not m.DocstringTestFoo.__doc__\n    assert not m.DocstringTestFoo.value_prop.__doc__\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_eigen.py,0,"b'import pytest\nfrom pybind11_tests import ConstructorStats\n\npytestmark = pytest.requires_eigen_and_numpy\n\nwith pytest.suppress(ImportError):\n    from pybind11_tests import eigen as m\n    import numpy as np\n\n    ref = np.array([[ 0.,  3,  0,  0,  0, 11],\n                    [22,  0,  0,  0, 17, 11],\n                    [ 7,  5,  0,  1,  0, 11],\n                    [ 0,  0,  0,  0,  0, 11],\n                    [ 0,  0, 14,  0,  8, 11]])\n\n\ndef assert_equal_ref(mat):\n    np.testing.assert_array_equal(mat, ref)\n\n\ndef assert_sparse_equal_ref(sparse_mat):\n    assert_equal_ref(sparse_mat.toarray())\n\n\ndef test_fixed():\n    assert_equal_ref(m.fixed_c())\n    assert_equal_ref(m.fixed_r())\n    assert_equal_ref(m.fixed_copy_r(m.fixed_r()))\n    assert_equal_ref(m.fixed_copy_c(m.fixed_c()))\n    assert_equal_ref(m.fixed_copy_r(m.fixed_c()))\n    assert_equal_ref(m.fixed_copy_c(m.fixed_r()))\n\n\ndef test_dense():\n    assert_equal_ref(m.dense_r())\n    assert_equal_ref(m.dense_c())\n    assert_equal_ref(m.dense_copy_r(m.dense_r()))\n    assert_equal_ref(m.dense_copy_c(m.dense_c()))\n    assert_equal_ref(m.dense_copy_r(m.dense_c()))\n    assert_equal_ref(m.dense_copy_c(m.dense_r()))\n\n\ndef test_partially_fixed():\n    ref2 = np.array([[0., 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]])\n    np.testing.assert_array_equal(m.partial_copy_four_rm_r(ref2), ref2)\n    np.testing.assert_array_equal(m.partial_copy_four_rm_c(ref2), ref2)\n    np.testing.assert_array_equal(m.partial_copy_four_rm_r(ref2[:, 1]), ref2[:, [1]])\n    np.testing.assert_array_equal(m.partial_copy_four_rm_c(ref2[0, :]), ref2[[0], :])\n    np.testing.assert_array_equal(m.partial_copy_four_rm_r(ref2[:, (0, 2)]), ref2[:, (0, 2)])\n    np.testing.assert_array_equal(\n        m.partial_copy_four_rm_c(ref2[(3, 1, 2), :]), ref2[(3, 1, 2), :])\n\n    np.testing.assert_array_equal(m.partial_copy_four_cm_r(ref2), ref2)\n    np.testing.assert_array_equal(m.partial_copy_four_cm_c(ref2), ref2)\n    np.testing.assert_array_equal(m.partial_copy_four_cm_r(ref2[:, 1]), ref2[:, [1]])\n    np.testing.assert_array_equal(m.partial_copy_four_cm_c(ref2[0, :]), ref2[[0], :])\n    np.testing.assert_array_equal(m.partial_copy_four_cm_r(ref2[:, (0, 2)]), ref2[:, (0, 2)])\n    np.testing.assert_array_equal(\n        m.partial_copy_four_cm_c(ref2[(3, 1, 2), :]), ref2[(3, 1, 2), :])\n\n    # TypeError should be raise for a shape mismatch\n    functions = [m.partial_copy_four_rm_r, m.partial_copy_four_rm_c,\n                 m.partial_copy_four_cm_r, m.partial_copy_four_cm_c]\n    matrix_with_wrong_shape = [[1, 2],\n                               [3, 4]]\n    for f in functions:\n        with pytest.raises(TypeError) as excinfo:\n            f(matrix_with_wrong_shape)\n        assert ""incompatible function arguments"" in str(excinfo.value)\n\n\ndef test_mutator_descriptors():\n    zr = np.arange(30, dtype=\'float32\').reshape(5, 6)  # row-major\n    zc = zr.reshape(6, 5).transpose()  # column-major\n\n    m.fixed_mutator_r(zr)\n    m.fixed_mutator_c(zc)\n    m.fixed_mutator_a(zr)\n    m.fixed_mutator_a(zc)\n    with pytest.raises(TypeError) as excinfo:\n        m.fixed_mutator_r(zc)\n    assert (\'(arg0: numpy.ndarray[float32[5, 6], flags.writeable, flags.c_contiguous]) -> None\'\n            in str(excinfo.value))\n    with pytest.raises(TypeError) as excinfo:\n        m.fixed_mutator_c(zr)\n    assert (\'(arg0: numpy.ndarray[float32[5, 6], flags.writeable, flags.f_contiguous]) -> None\'\n            in str(excinfo.value))\n    with pytest.raises(TypeError) as excinfo:\n        m.fixed_mutator_a(np.array([[1, 2], [3, 4]], dtype=\'float32\'))\n    assert (\'(arg0: numpy.ndarray[float32[5, 6], flags.writeable]) -> None\'\n            in str(excinfo.value))\n    zr.flags.writeable = False\n    with pytest.raises(TypeError):\n        m.fixed_mutator_r(zr)\n    with pytest.raises(TypeError):\n        m.fixed_mutator_a(zr)\n\n\ndef test_cpp_casting():\n    assert m.cpp_copy(m.fixed_r()) == 22.\n    assert m.cpp_copy(m.fixed_c()) == 22.\n    z = np.array([[5., 6], [7, 8]])\n    assert m.cpp_copy(z) == 7.\n    assert m.cpp_copy(m.get_cm_ref()) == 21.\n    assert m.cpp_copy(m.get_rm_ref()) == 21.\n    assert m.cpp_ref_c(m.get_cm_ref()) == 21.\n    assert m.cpp_ref_r(m.get_rm_ref()) == 21.\n    with pytest.raises(RuntimeError) as excinfo:\n        # Can\'t reference m.fixed_c: it contains floats, m.cpp_ref_any wants doubles\n        m.cpp_ref_any(m.fixed_c())\n    assert \'Unable to cast Python instance\' in str(excinfo.value)\n    with pytest.raises(RuntimeError) as excinfo:\n        # Can\'t reference m.fixed_r: it contains floats, m.cpp_ref_any wants doubles\n        m.cpp_ref_any(m.fixed_r())\n    assert \'Unable to cast Python instance\' in str(excinfo.value)\n    assert m.cpp_ref_any(m.ReturnTester.create()) == 1.\n\n    assert m.cpp_ref_any(m.get_cm_ref()) == 21.\n    assert m.cpp_ref_any(m.get_cm_ref()) == 21.\n\n\ndef test_pass_readonly_array():\n    z = np.full((5, 6), 42.0)\n    z.flags.writeable = False\n    np.testing.assert_array_equal(z, m.fixed_copy_r(z))\n    np.testing.assert_array_equal(m.fixed_r_const(), m.fixed_r())\n    assert not m.fixed_r_const().flags.writeable\n    np.testing.assert_array_equal(m.fixed_copy_r(m.fixed_r_const()), m.fixed_r_const())\n\n\ndef test_nonunit_stride_from_python():\n    counting_mat = np.arange(9.0, dtype=np.float32).reshape((3, 3))\n    second_row = counting_mat[1, :]\n    second_col = counting_mat[:, 1]\n    np.testing.assert_array_equal(m.double_row(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_col(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_complex(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_row(second_col), 2.0 * second_col)\n    np.testing.assert_array_equal(m.double_col(second_col), 2.0 * second_col)\n    np.testing.assert_array_equal(m.double_complex(second_col), 2.0 * second_col)\n\n    counting_3d = np.arange(27.0, dtype=np.float32).reshape((3, 3, 3))\n    slices = [counting_3d[0, :, :], counting_3d[:, 0, :], counting_3d[:, :, 0]]\n    for slice_idx, ref_mat in enumerate(slices):\n        np.testing.assert_array_equal(m.double_mat_cm(ref_mat), 2.0 * ref_mat)\n        np.testing.assert_array_equal(m.double_mat_rm(ref_mat), 2.0 * ref_mat)\n\n    # Mutator:\n    m.double_threer(second_row)\n    m.double_threec(second_col)\n    np.testing.assert_array_equal(counting_mat, [[0., 2, 2], [6, 16, 10], [6, 14, 8]])\n\n\ndef test_negative_stride_from_python(msg):\n    """"""Eigen doesn\'t support (as of yet) negative strides. When a function takes an Eigen matrix by\n    copy or const reference, we can pass a numpy array that has negative strides.  Otherwise, an\n    exception will be thrown as Eigen will not be able to map the numpy array.""""""\n\n    counting_mat = np.arange(9.0, dtype=np.float32).reshape((3, 3))\n    counting_mat = counting_mat[::-1, ::-1]\n    second_row = counting_mat[1, :]\n    second_col = counting_mat[:, 1]\n    np.testing.assert_array_equal(m.double_row(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_col(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_complex(second_row), 2.0 * second_row)\n    np.testing.assert_array_equal(m.double_row(second_col), 2.0 * second_col)\n    np.testing.assert_array_equal(m.double_col(second_col), 2.0 * second_col)\n    np.testing.assert_array_equal(m.double_complex(second_col), 2.0 * second_col)\n\n    counting_3d = np.arange(27.0, dtype=np.float32).reshape((3, 3, 3))\n    counting_3d = counting_3d[::-1, ::-1, ::-1]\n    slices = [counting_3d[0, :, :], counting_3d[:, 0, :], counting_3d[:, :, 0]]\n    for slice_idx, ref_mat in enumerate(slices):\n        np.testing.assert_array_equal(m.double_mat_cm(ref_mat), 2.0 * ref_mat)\n        np.testing.assert_array_equal(m.double_mat_rm(ref_mat), 2.0 * ref_mat)\n\n    # Mutator:\n    with pytest.raises(TypeError) as excinfo:\n        m.double_threer(second_row)\n    assert msg(excinfo.value) == """"""\n        double_threer(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: numpy.ndarray[float32[1, 3], flags.writeable]) -> None\n\n        Invoked with: """""" + repr(np.array([ 5.,  4.,  3.], dtype=\'float32\'))  # noqa: E501 line too long\n\n    with pytest.raises(TypeError) as excinfo:\n        m.double_threec(second_col)\n    assert msg(excinfo.value) == """"""\n        double_threec(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: numpy.ndarray[float32[3, 1], flags.writeable]) -> None\n\n        Invoked with: """""" + repr(np.array([ 7.,  4.,  1.], dtype=\'float32\'))  # noqa: E501 line too long\n\n\ndef test_nonunit_stride_to_python():\n    assert np.all(m.diagonal(ref) == ref.diagonal())\n    assert np.all(m.diagonal_1(ref) == ref.diagonal(1))\n    for i in range(-5, 7):\n        assert np.all(m.diagonal_n(ref, i) == ref.diagonal(i)), ""m.diagonal_n({})"".format(i)\n\n    assert np.all(m.block(ref, 2, 1, 3, 3) == ref[2:5, 1:4])\n    assert np.all(m.block(ref, 1, 4, 4, 2) == ref[1:, 4:])\n    assert np.all(m.block(ref, 1, 4, 3, 2) == ref[1:4, 4:])\n\n\ndef test_eigen_ref_to_python():\n    chols = [m.cholesky1, m.cholesky2, m.cholesky3, m.cholesky4]\n    for i, chol in enumerate(chols, start=1):\n        mymat = chol(np.array([[1., 2, 4], [2, 13, 23], [4, 23, 77]]))\n        assert np.all(mymat == np.array([[1, 0, 0], [2, 3, 0], [4, 5, 6]])), ""cholesky{}"".format(i)\n\n\ndef assign_both(a1, a2, r, c, v):\n    a1[r, c] = v\n    a2[r, c] = v\n\n\ndef array_copy_but_one(a, r, c, v):\n    z = np.array(a, copy=True)\n    z[r, c] = v\n    return z\n\n\ndef test_eigen_return_references():\n    """"""Tests various ways of returning references and non-referencing copies""""""\n\n    master = np.ones((10, 10))\n    a = m.ReturnTester()\n    a_get1 = a.get()\n    assert not a_get1.flags.owndata and a_get1.flags.writeable\n    assign_both(a_get1, master, 3, 3, 5)\n    a_get2 = a.get_ptr()\n    assert not a_get2.flags.owndata and a_get2.flags.writeable\n    assign_both(a_get1, master, 2, 3, 6)\n\n    a_view1 = a.view()\n    assert not a_view1.flags.owndata and not a_view1.flags.writeable\n    with pytest.raises(ValueError):\n        a_view1[2, 3] = 4\n    a_view2 = a.view_ptr()\n    assert not a_view2.flags.owndata and not a_view2.flags.writeable\n    with pytest.raises(ValueError):\n        a_view2[2, 3] = 4\n\n    a_copy1 = a.copy_get()\n    assert a_copy1.flags.owndata and a_copy1.flags.writeable\n    np.testing.assert_array_equal(a_copy1, master)\n    a_copy1[7, 7] = -44  # Shouldn\'t affect anything else\n    c1want = array_copy_but_one(master, 7, 7, -44)\n    a_copy2 = a.copy_view()\n    assert a_copy2.flags.owndata and a_copy2.flags.writeable\n    np.testing.assert_array_equal(a_copy2, master)\n    a_copy2[4, 4] = -22  # Shouldn\'t affect anything else\n    c2want = array_copy_but_one(master, 4, 4, -22)\n\n    a_ref1 = a.ref()\n    assert not a_ref1.flags.owndata and a_ref1.flags.writeable\n    assign_both(a_ref1, master, 1, 1, 15)\n    a_ref2 = a.ref_const()\n    assert not a_ref2.flags.owndata and not a_ref2.flags.writeable\n    with pytest.raises(ValueError):\n        a_ref2[5, 5] = 33\n    a_ref3 = a.ref_safe()\n    assert not a_ref3.flags.owndata and a_ref3.flags.writeable\n    assign_both(a_ref3, master, 0, 7, 99)\n    a_ref4 = a.ref_const_safe()\n    assert not a_ref4.flags.owndata and not a_ref4.flags.writeable\n    with pytest.raises(ValueError):\n        a_ref4[7, 0] = 987654321\n\n    a_copy3 = a.copy_ref()\n    assert a_copy3.flags.owndata and a_copy3.flags.writeable\n    np.testing.assert_array_equal(a_copy3, master)\n    a_copy3[8, 1] = 11\n    c3want = array_copy_but_one(master, 8, 1, 11)\n    a_copy4 = a.copy_ref_const()\n    assert a_copy4.flags.owndata and a_copy4.flags.writeable\n    np.testing.assert_array_equal(a_copy4, master)\n    a_copy4[8, 4] = 88\n    c4want = array_copy_but_one(master, 8, 4, 88)\n\n    a_block1 = a.block(3, 3, 2, 2)\n    assert not a_block1.flags.owndata and a_block1.flags.writeable\n    a_block1[0, 0] = 55\n    master[3, 3] = 55\n    a_block2 = a.block_safe(2, 2, 3, 2)\n    assert not a_block2.flags.owndata and a_block2.flags.writeable\n    a_block2[2, 1] = -123\n    master[4, 3] = -123\n    a_block3 = a.block_const(6, 7, 4, 3)\n    assert not a_block3.flags.owndata and not a_block3.flags.writeable\n    with pytest.raises(ValueError):\n        a_block3[2, 2] = -44444\n\n    a_copy5 = a.copy_block(2, 2, 2, 3)\n    assert a_copy5.flags.owndata and a_copy5.flags.writeable\n    np.testing.assert_array_equal(a_copy5, master[2:4, 2:5])\n    a_copy5[1, 1] = 777\n    c5want = array_copy_but_one(master[2:4, 2:5], 1, 1, 777)\n\n    a_corn1 = a.corners()\n    assert not a_corn1.flags.owndata and a_corn1.flags.writeable\n    a_corn1 *= 50\n    a_corn1[1, 1] = 999\n    master[0, 0] = 50\n    master[0, 9] = 50\n    master[9, 0] = 50\n    master[9, 9] = 999\n    a_corn2 = a.corners_const()\n    assert not a_corn2.flags.owndata and not a_corn2.flags.writeable\n    with pytest.raises(ValueError):\n        a_corn2[1, 0] = 51\n\n    # All of the changes made all the way along should be visible everywhere\n    # now (except for the copies, of course)\n    np.testing.assert_array_equal(a_get1, master)\n    np.testing.assert_array_equal(a_get2, master)\n    np.testing.assert_array_equal(a_view1, master)\n    np.testing.assert_array_equal(a_view2, master)\n    np.testing.assert_array_equal(a_ref1, master)\n    np.testing.assert_array_equal(a_ref2, master)\n    np.testing.assert_array_equal(a_ref3, master)\n    np.testing.assert_array_equal(a_ref4, master)\n    np.testing.assert_array_equal(a_block1, master[3:5, 3:5])\n    np.testing.assert_array_equal(a_block2, master[2:5, 2:4])\n    np.testing.assert_array_equal(a_block3, master[6:10, 7:10])\n    np.testing.assert_array_equal(a_corn1, master[0::master.shape[0] - 1, 0::master.shape[1] - 1])\n    np.testing.assert_array_equal(a_corn2, master[0::master.shape[0] - 1, 0::master.shape[1] - 1])\n\n    np.testing.assert_array_equal(a_copy1, c1want)\n    np.testing.assert_array_equal(a_copy2, c2want)\n    np.testing.assert_array_equal(a_copy3, c3want)\n    np.testing.assert_array_equal(a_copy4, c4want)\n    np.testing.assert_array_equal(a_copy5, c5want)\n\n\ndef assert_keeps_alive(cl, method, *args):\n    cstats = ConstructorStats.get(cl)\n    start_with = cstats.alive()\n    a = cl()\n    assert cstats.alive() == start_with + 1\n    z = method(a, *args)\n    assert cstats.alive() == start_with + 1\n    del a\n    # Here\'s the keep alive in action:\n    assert cstats.alive() == start_with + 1\n    del z\n    # Keep alive should have expired:\n    assert cstats.alive() == start_with\n\n\ndef test_eigen_keepalive():\n    a = m.ReturnTester()\n    cstats = ConstructorStats.get(m.ReturnTester)\n    assert cstats.alive() == 1\n    unsafe = [a.ref(), a.ref_const(), a.block(1, 2, 3, 4)]\n    copies = [a.copy_get(), a.copy_view(), a.copy_ref(), a.copy_ref_const(),\n              a.copy_block(4, 3, 2, 1)]\n    del a\n    assert cstats.alive() == 0\n    del unsafe\n    del copies\n\n    for meth in [m.ReturnTester.get, m.ReturnTester.get_ptr, m.ReturnTester.view,\n                 m.ReturnTester.view_ptr, m.ReturnTester.ref_safe, m.ReturnTester.ref_const_safe,\n                 m.ReturnTester.corners, m.ReturnTester.corners_const]:\n        assert_keeps_alive(m.ReturnTester, meth)\n\n    for meth in [m.ReturnTester.block_safe, m.ReturnTester.block_const]:\n        assert_keeps_alive(m.ReturnTester, meth, 4, 3, 2, 1)\n\n\ndef test_eigen_ref_mutators():\n    """"""Tests Eigen\'s ability to mutate numpy values""""""\n\n    orig = np.array([[1., 2, 3], [4, 5, 6], [7, 8, 9]])\n    zr = np.array(orig)\n    zc = np.array(orig, order=\'F\')\n    m.add_rm(zr, 1, 0, 100)\n    assert np.all(zr == np.array([[1., 2, 3], [104, 5, 6], [7, 8, 9]]))\n    m.add_cm(zc, 1, 0, 200)\n    assert np.all(zc == np.array([[1., 2, 3], [204, 5, 6], [7, 8, 9]]))\n\n    m.add_any(zr, 1, 0, 20)\n    assert np.all(zr == np.array([[1., 2, 3], [124, 5, 6], [7, 8, 9]]))\n    m.add_any(zc, 1, 0, 10)\n    assert np.all(zc == np.array([[1., 2, 3], [214, 5, 6], [7, 8, 9]]))\n\n    # Can\'t reference a col-major array with a row-major Ref, and vice versa:\n    with pytest.raises(TypeError):\n        m.add_rm(zc, 1, 0, 1)\n    with pytest.raises(TypeError):\n        m.add_cm(zr, 1, 0, 1)\n\n    # Overloads:\n    m.add1(zr, 1, 0, -100)\n    m.add2(zr, 1, 0, -20)\n    assert np.all(zr == orig)\n    m.add1(zc, 1, 0, -200)\n    m.add2(zc, 1, 0, -10)\n    assert np.all(zc == orig)\n\n    # a non-contiguous slice (this won\'t work on either the row- or\n    # column-contiguous refs, but should work for the any)\n    cornersr = zr[0::2, 0::2]\n    cornersc = zc[0::2, 0::2]\n\n    assert np.all(cornersr == np.array([[1., 3], [7, 9]]))\n    assert np.all(cornersc == np.array([[1., 3], [7, 9]]))\n\n    with pytest.raises(TypeError):\n        m.add_rm(cornersr, 0, 1, 25)\n    with pytest.raises(TypeError):\n        m.add_cm(cornersr, 0, 1, 25)\n    with pytest.raises(TypeError):\n        m.add_rm(cornersc, 0, 1, 25)\n    with pytest.raises(TypeError):\n        m.add_cm(cornersc, 0, 1, 25)\n    m.add_any(cornersr, 0, 1, 25)\n    m.add_any(cornersc, 0, 1, 44)\n    assert np.all(zr == np.array([[1., 2, 28], [4, 5, 6], [7, 8, 9]]))\n    assert np.all(zc == np.array([[1., 2, 47], [4, 5, 6], [7, 8, 9]]))\n\n    # You shouldn\'t be allowed to pass a non-writeable array to a mutating Eigen method:\n    zro = zr[0:4, 0:4]\n    zro.flags.writeable = False\n    with pytest.raises(TypeError):\n        m.add_rm(zro, 0, 0, 0)\n    with pytest.raises(TypeError):\n        m.add_any(zro, 0, 0, 0)\n    with pytest.raises(TypeError):\n        m.add1(zro, 0, 0, 0)\n    with pytest.raises(TypeError):\n        m.add2(zro, 0, 0, 0)\n\n    # integer array shouldn\'t be passable to a double-matrix-accepting mutating func:\n    zi = np.array([[1, 2], [3, 4]])\n    with pytest.raises(TypeError):\n        m.add_rm(zi)\n\n\ndef test_numpy_ref_mutators():\n    """"""Tests numpy mutating Eigen matrices (for returned Eigen::Ref<...>s)""""""\n\n    m.reset_refs()  # In case another test already changed it\n\n    zc = m.get_cm_ref()\n    zcro = m.get_cm_const_ref()\n    zr = m.get_rm_ref()\n    zrro = m.get_rm_const_ref()\n\n    assert [zc[1, 2], zcro[1, 2], zr[1, 2], zrro[1, 2]] == [23] * 4\n\n    assert not zc.flags.owndata and zc.flags.writeable\n    assert not zr.flags.owndata and zr.flags.writeable\n    assert not zcro.flags.owndata and not zcro.flags.writeable\n    assert not zrro.flags.owndata and not zrro.flags.writeable\n\n    zc[1, 2] = 99\n    expect = np.array([[11., 12, 13], [21, 22, 99], [31, 32, 33]])\n    # We should have just changed zc, of course, but also zcro and the original eigen matrix\n    assert np.all(zc == expect)\n    assert np.all(zcro == expect)\n    assert np.all(m.get_cm_ref() == expect)\n\n    zr[1, 2] = 99\n    assert np.all(zr == expect)\n    assert np.all(zrro == expect)\n    assert np.all(m.get_rm_ref() == expect)\n\n    # Make sure the readonly ones are numpy-readonly:\n    with pytest.raises(ValueError):\n        zcro[1, 2] = 6\n    with pytest.raises(ValueError):\n        zrro[1, 2] = 6\n\n    # We should be able to explicitly copy like this (and since we\'re copying,\n    # the const should drop away)\n    y1 = np.array(m.get_cm_const_ref())\n\n    assert y1.flags.owndata and y1.flags.writeable\n    # We should get copies of the eigen data, which was modified above:\n    assert y1[1, 2] == 99\n    y1[1, 2] += 12\n    assert y1[1, 2] == 111\n    assert zc[1, 2] == 99  # Make sure we aren\'t referencing the original\n\n\ndef test_both_ref_mutators():\n    """"""Tests a complex chain of nested eigen/numpy references""""""\n\n    m.reset_refs()  # In case another test already changed it\n\n    z = m.get_cm_ref()  # numpy -> eigen\n    z[0, 2] -= 3\n    z2 = m.incr_matrix(z, 1)  # numpy -> eigen -> numpy -> eigen\n    z2[1, 1] += 6\n    z3 = m.incr_matrix(z, 2)  # (numpy -> eigen)^3\n    z3[2, 2] += -5\n    z4 = m.incr_matrix(z, 3)  # (numpy -> eigen)^4\n    z4[1, 1] -= 1\n    z5 = m.incr_matrix(z, 4)  # (numpy -> eigen)^5\n    z5[0, 0] = 0\n    assert np.all(z == z2)\n    assert np.all(z == z3)\n    assert np.all(z == z4)\n    assert np.all(z == z5)\n    expect = np.array([[0., 22, 20], [31, 37, 33], [41, 42, 38]])\n    assert np.all(z == expect)\n\n    y = np.array(range(100), dtype=\'float64\').reshape(10, 10)\n    y2 = m.incr_matrix_any(y, 10)  # np -> eigen -> np\n    y3 = m.incr_matrix_any(y2[0::2, 0::2], -33)  # np -> eigen -> np slice -> np -> eigen -> np\n    y4 = m.even_rows(y3)  # numpy -> eigen slice -> (... y3)\n    y5 = m.even_cols(y4)  # numpy -> eigen slice -> (... y4)\n    y6 = m.incr_matrix_any(y5, 1000)  # numpy -> eigen -> (... y5)\n\n    # Apply same mutations using just numpy:\n    yexpect = np.array(range(100), dtype=\'float64\').reshape(10, 10)\n    yexpect += 10\n    yexpect[0::2, 0::2] -= 33\n    yexpect[0::4, 0::4] += 1000\n    assert np.all(y6 == yexpect[0::4, 0::4])\n    assert np.all(y5 == yexpect[0::4, 0::4])\n    assert np.all(y4 == yexpect[0::4, 0::2])\n    assert np.all(y3 == yexpect[0::2, 0::2])\n    assert np.all(y2 == yexpect)\n    assert np.all(y == yexpect)\n\n\ndef test_nocopy_wrapper():\n    # get_elem requires a column-contiguous matrix reference, but should be\n    # callable with other types of matrix (via copying):\n    int_matrix_colmajor = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], order=\'F\')\n    dbl_matrix_colmajor = np.array(int_matrix_colmajor, dtype=\'double\', order=\'F\', copy=True)\n    int_matrix_rowmajor = np.array(int_matrix_colmajor, order=\'C\', copy=True)\n    dbl_matrix_rowmajor = np.array(int_matrix_rowmajor, dtype=\'double\', order=\'C\', copy=True)\n\n    # All should be callable via get_elem:\n    assert m.get_elem(int_matrix_colmajor) == 8\n    assert m.get_elem(dbl_matrix_colmajor) == 8\n    assert m.get_elem(int_matrix_rowmajor) == 8\n    assert m.get_elem(dbl_matrix_rowmajor) == 8\n\n    # All but the second should fail with m.get_elem_nocopy:\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_nocopy(int_matrix_colmajor)\n    assert (\'get_elem_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.f_contiguous\' in str(excinfo.value))\n    assert m.get_elem_nocopy(dbl_matrix_colmajor) == 8\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_nocopy(int_matrix_rowmajor)\n    assert (\'get_elem_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.f_contiguous\' in str(excinfo.value))\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_nocopy(dbl_matrix_rowmajor)\n    assert (\'get_elem_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.f_contiguous\' in str(excinfo.value))\n\n    # For the row-major test, we take a long matrix in row-major, so only the third is allowed:\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_rm_nocopy(int_matrix_colmajor)\n    assert (\'get_elem_rm_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.c_contiguous\' in str(excinfo.value))\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_rm_nocopy(dbl_matrix_colmajor)\n    assert (\'get_elem_rm_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.c_contiguous\' in str(excinfo.value))\n    assert m.get_elem_rm_nocopy(int_matrix_rowmajor) == 8\n    with pytest.raises(TypeError) as excinfo:\n        m.get_elem_rm_nocopy(dbl_matrix_rowmajor)\n    assert (\'get_elem_rm_nocopy(): incompatible function arguments.\' in str(excinfo.value) and\n            \', flags.c_contiguous\' in str(excinfo.value))\n\n\ndef test_eigen_ref_life_support():\n    """"""Ensure the lifetime of temporary arrays created by the `Ref` caster\n\n    The `Ref` caster sometimes creates a copy which needs to stay alive. This needs to\n    happen both for directs casts (just the array) or indirectly (e.g. list of arrays).\n    """"""\n\n    a = np.full(shape=10, fill_value=8, dtype=np.int8)\n    assert m.get_elem_direct(a) == 8\n\n    list_of_a = [a]\n    assert m.get_elem_indirect(list_of_a) == 8\n\n\ndef test_special_matrix_objects():\n    assert np.all(m.incr_diag(7) == np.diag([1., 2, 3, 4, 5, 6, 7]))\n\n    asymm = np.array([[ 1.,  2,  3,  4],\n                      [ 5,  6,  7,  8],\n                      [ 9, 10, 11, 12],\n                      [13, 14, 15, 16]])\n    symm_lower = np.array(asymm)\n    symm_upper = np.array(asymm)\n    for i in range(4):\n        for j in range(i + 1, 4):\n            symm_lower[i, j] = symm_lower[j, i]\n            symm_upper[j, i] = symm_upper[i, j]\n\n    assert np.all(m.symmetric_lower(asymm) == symm_lower)\n    assert np.all(m.symmetric_upper(asymm) == symm_upper)\n\n\ndef test_dense_signature(doc):\n    assert doc(m.double_col) == """"""\n        double_col(arg0: numpy.ndarray[float32[m, 1]]) -> numpy.ndarray[float32[m, 1]]\n    """"""\n    assert doc(m.double_row) == """"""\n        double_row(arg0: numpy.ndarray[float32[1, n]]) -> numpy.ndarray[float32[1, n]]\n    """"""\n    assert doc(m.double_complex) == """"""\n        double_complex(arg0: numpy.ndarray[complex64[m, 1]]) -> numpy.ndarray[complex64[m, 1]]\n    """"""\n    assert doc(m.double_mat_rm) == """"""\n        double_mat_rm(arg0: numpy.ndarray[float32[m, n]]) -> numpy.ndarray[float32[m, n]]\n    """"""\n\n\ndef test_named_arguments():\n    a = np.array([[1.0, 2], [3, 4], [5, 6]])\n    b = np.ones((2, 1))\n\n    assert np.all(m.matrix_multiply(a, b) == np.array([[3.], [7], [11]]))\n    assert np.all(m.matrix_multiply(A=a, B=b) == np.array([[3.], [7], [11]]))\n    assert np.all(m.matrix_multiply(B=b, A=a) == np.array([[3.], [7], [11]]))\n\n    with pytest.raises(ValueError) as excinfo:\n        m.matrix_multiply(b, a)\n    assert str(excinfo.value) == \'Nonconformable matrices!\'\n\n    with pytest.raises(ValueError) as excinfo:\n        m.matrix_multiply(A=b, B=a)\n    assert str(excinfo.value) == \'Nonconformable matrices!\'\n\n    with pytest.raises(ValueError) as excinfo:\n        m.matrix_multiply(B=a, A=b)\n    assert str(excinfo.value) == \'Nonconformable matrices!\'\n\n\n@pytest.requires_eigen_and_scipy\ndef test_sparse():\n    assert_sparse_equal_ref(m.sparse_r())\n    assert_sparse_equal_ref(m.sparse_c())\n    assert_sparse_equal_ref(m.sparse_copy_r(m.sparse_r()))\n    assert_sparse_equal_ref(m.sparse_copy_c(m.sparse_c()))\n    assert_sparse_equal_ref(m.sparse_copy_r(m.sparse_c()))\n    assert_sparse_equal_ref(m.sparse_copy_c(m.sparse_r()))\n\n\n@pytest.requires_eigen_and_scipy\ndef test_sparse_signature(doc):\n    assert doc(m.sparse_copy_r) == """"""\n        sparse_copy_r(arg0: scipy.sparse.csr_matrix[float32]) -> scipy.sparse.csr_matrix[float32]\n    """"""  # noqa: E501 line too long\n    assert doc(m.sparse_copy_c) == """"""\n        sparse_copy_c(arg0: scipy.sparse.csc_matrix[float32]) -> scipy.sparse.csc_matrix[float32]\n    """"""  # noqa: E501 line too long\n\n\ndef test_issue738():\n    """"""Ignore strides on a length-1 dimension (even if they would be incompatible length > 1)""""""\n    assert np.all(m.iss738_f1(np.array([[1., 2, 3]])) == np.array([[1., 102, 203]]))\n    assert np.all(m.iss738_f1(np.array([[1.], [2], [3]])) == np.array([[1.], [12], [23]]))\n\n    assert np.all(m.iss738_f2(np.array([[1., 2, 3]])) == np.array([[1., 102, 203]]))\n    assert np.all(m.iss738_f2(np.array([[1.], [2], [3]])) == np.array([[1.], [12], [23]]))\n\n\ndef test_issue1105():\n    """"""Issue 1105: 1xN or Nx1 input arrays weren\'t accepted for eigen\n    compile-time row vectors or column vector""""""\n    assert m.iss1105_row(np.ones((1, 7)))\n    assert m.iss1105_col(np.ones((7, 1)))\n\n    # These should still fail (incompatible dimensions):\n    with pytest.raises(TypeError) as excinfo:\n        m.iss1105_row(np.ones((7, 1)))\n    assert ""incompatible function arguments"" in str(excinfo)\n    with pytest.raises(TypeError) as excinfo:\n        m.iss1105_col(np.ones((1, 7)))\n    assert ""incompatible function arguments"" in str(excinfo)\n\n\ndef test_custom_operator_new():\n    """"""Using Eigen types as member variables requires a class-specific\n    operator new with proper alignment""""""\n\n    o = m.CustomOperatorNew()\n    np.testing.assert_allclose(o.a, 0.0)\n    np.testing.assert_allclose(o.b.diagonal(), 1.0)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_enum.py,0,"b'import pytest\nfrom pybind11_tests import enums as m\n\n\ndef test_unscoped_enum():\n    assert str(m.UnscopedEnum.EOne) == ""UnscopedEnum.EOne""\n    assert str(m.UnscopedEnum.ETwo) == ""UnscopedEnum.ETwo""\n    assert str(m.EOne) == ""UnscopedEnum.EOne""\n    # __members__ property\n    assert m.UnscopedEnum.__members__ == \\\n        {""EOne"": m.UnscopedEnum.EOne, ""ETwo"": m.UnscopedEnum.ETwo}\n    # __members__ readonly\n    with pytest.raises(AttributeError):\n        m.UnscopedEnum.__members__ = {}\n    # __members__ returns a copy\n    foo = m.UnscopedEnum.__members__\n    foo[""bar""] = ""baz""\n    assert m.UnscopedEnum.__members__ == \\\n        {""EOne"": m.UnscopedEnum.EOne, ""ETwo"": m.UnscopedEnum.ETwo}\n\n    # no TypeError exception for unscoped enum ==/!= int comparisons\n    y = m.UnscopedEnum.ETwo\n    assert y == 2\n    assert y != 3\n\n    assert int(m.UnscopedEnum.ETwo) == 2\n    assert str(m.UnscopedEnum(2)) == ""UnscopedEnum.ETwo""\n\n    # order\n    assert m.UnscopedEnum.EOne < m.UnscopedEnum.ETwo\n    assert m.UnscopedEnum.EOne < 2\n    assert m.UnscopedEnum.ETwo > m.UnscopedEnum.EOne\n    assert m.UnscopedEnum.ETwo > 1\n    assert m.UnscopedEnum.ETwo <= 2\n    assert m.UnscopedEnum.ETwo >= 2\n    assert m.UnscopedEnum.EOne <= m.UnscopedEnum.ETwo\n    assert m.UnscopedEnum.EOne <= 2\n    assert m.UnscopedEnum.ETwo >= m.UnscopedEnum.EOne\n    assert m.UnscopedEnum.ETwo >= 1\n    assert not (m.UnscopedEnum.ETwo < m.UnscopedEnum.EOne)\n    assert not (2 < m.UnscopedEnum.EOne)\n\n\ndef test_scoped_enum():\n    assert m.test_scoped_enum(m.ScopedEnum.Three) == ""ScopedEnum::Three""\n    z = m.ScopedEnum.Two\n    assert m.test_scoped_enum(z) == ""ScopedEnum::Two""\n\n    # expected TypeError exceptions for scoped enum ==/!= int comparisons\n    with pytest.raises(TypeError):\n        assert z == 2\n    with pytest.raises(TypeError):\n        assert z != 3\n\n    # order\n    assert m.ScopedEnum.Two < m.ScopedEnum.Three\n    assert m.ScopedEnum.Three > m.ScopedEnum.Two\n    assert m.ScopedEnum.Two <= m.ScopedEnum.Three\n    assert m.ScopedEnum.Two <= m.ScopedEnum.Two\n    assert m.ScopedEnum.Two >= m.ScopedEnum.Two\n    assert m.ScopedEnum.Three >= m.ScopedEnum.Two\n\n\ndef test_implicit_conversion():\n    assert str(m.ClassWithUnscopedEnum.EMode.EFirstMode) == ""EMode.EFirstMode""\n    assert str(m.ClassWithUnscopedEnum.EFirstMode) == ""EMode.EFirstMode""\n\n    f = m.ClassWithUnscopedEnum.test_function\n    first = m.ClassWithUnscopedEnum.EFirstMode\n    second = m.ClassWithUnscopedEnum.ESecondMode\n\n    assert f(first) == 1\n\n    assert f(first) == f(first)\n    assert not f(first) != f(first)\n\n    assert f(first) != f(second)\n    assert not f(first) == f(second)\n\n    assert f(first) == int(f(first))\n    assert not f(first) != int(f(first))\n\n    assert f(first) != int(f(second))\n    assert not f(first) == int(f(second))\n\n    # noinspection PyDictCreation\n    x = {f(first): 1, f(second): 2}\n    x[f(first)] = 3\n    x[f(second)] = 4\n    # Hashing test\n    assert str(x) == ""{EMode.EFirstMode: 3, EMode.ESecondMode: 4}""\n\n\ndef test_binary_operators():\n    assert int(m.Flags.Read) == 4\n    assert int(m.Flags.Write) == 2\n    assert int(m.Flags.Execute) == 1\n    assert int(m.Flags.Read | m.Flags.Write | m.Flags.Execute) == 7\n    assert int(m.Flags.Read | m.Flags.Write) == 6\n    assert int(m.Flags.Read | m.Flags.Execute) == 5\n    assert int(m.Flags.Write | m.Flags.Execute) == 3\n    assert int(m.Flags.Write | 1) == 3\n\n    state = m.Flags.Read | m.Flags.Write\n    assert (state & m.Flags.Read) != 0\n    assert (state & m.Flags.Write) != 0\n    assert (state & m.Flags.Execute) == 0\n    assert (state & 1) == 0\n\n    state2 = ~state\n    assert state2 == -7\n    assert int(state ^ state2) == -1\n\n\ndef test_enum_to_int():\n    m.test_enum_to_int(m.Flags.Read)\n    m.test_enum_to_int(m.ClassWithUnscopedEnum.EMode.EFirstMode)\n    m.test_enum_to_uint(m.Flags.Read)\n    m.test_enum_to_uint(m.ClassWithUnscopedEnum.EMode.EFirstMode)\n    m.test_enum_to_long_long(m.Flags.Read)\n    m.test_enum_to_long_long(m.ClassWithUnscopedEnum.EMode.EFirstMode)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_eval.py,0,"b'import os\nfrom pybind11_tests import eval_ as m\n\n\ndef test_evals(capture):\n    with capture:\n        assert m.test_eval_statements()\n    assert capture == ""Hello World!""\n\n    assert m.test_eval()\n    assert m.test_eval_single_statement()\n\n    filename = os.path.join(os.path.dirname(__file__), ""test_eval_call.py"")\n    assert m.test_eval_file(filename)\n\n    assert m.test_eval_failure()\n    assert m.test_eval_file_failure()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_eval_call.py,0,"b""# This file is called from 'test_eval.py'\n\nif 'call_test2' in locals():\n    call_test2(y)  # noqa: F821 undefined name\n"""
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_exceptions.py,0,"b'import pytest\n\nfrom pybind11_tests import exceptions as m\nimport pybind11_cross_module_tests as cm\n\n\ndef test_std_exception(msg):\n    with pytest.raises(RuntimeError) as excinfo:\n        m.throw_std_exception()\n    assert msg(excinfo.value) == ""This exception was intentionally thrown.""\n\n\ndef test_error_already_set(msg):\n    with pytest.raises(RuntimeError) as excinfo:\n        m.throw_already_set(False)\n    assert msg(excinfo.value) == ""Unknown internal error occurred""\n\n    with pytest.raises(ValueError) as excinfo:\n        m.throw_already_set(True)\n    assert msg(excinfo.value) == ""foo""\n\n\ndef test_cross_module_exceptions():\n    with pytest.raises(RuntimeError) as excinfo:\n        cm.raise_runtime_error()\n    assert str(excinfo.value) == ""My runtime error""\n\n    with pytest.raises(ValueError) as excinfo:\n        cm.raise_value_error()\n    assert str(excinfo.value) == ""My value error""\n\n    with pytest.raises(ValueError) as excinfo:\n        cm.throw_pybind_value_error()\n    assert str(excinfo.value) == ""pybind11 value error""\n\n    with pytest.raises(TypeError) as excinfo:\n        cm.throw_pybind_type_error()\n    assert str(excinfo.value) == ""pybind11 type error""\n\n    with pytest.raises(StopIteration) as excinfo:\n        cm.throw_stop_iteration()\n\n\ndef test_python_call_in_catch():\n    d = {}\n    assert m.python_call_in_destructor(d) is True\n    assert d[""good""] is True\n\n\ndef test_exception_matches():\n    m.exception_matches()\n\n\ndef test_custom(msg):\n    # Can we catch a MyException?\n    with pytest.raises(m.MyException) as excinfo:\n        m.throws1()\n    assert msg(excinfo.value) == ""this error should go to a custom type""\n\n    # Can we translate to standard Python exceptions?\n    with pytest.raises(RuntimeError) as excinfo:\n        m.throws2()\n    assert msg(excinfo.value) == ""this error should go to a standard Python exception""\n\n    # Can we handle unknown exceptions?\n    with pytest.raises(RuntimeError) as excinfo:\n        m.throws3()\n    assert msg(excinfo.value) == ""Caught an unknown exception!""\n\n    # Can we delegate to another handler by rethrowing?\n    with pytest.raises(m.MyException) as excinfo:\n        m.throws4()\n    assert msg(excinfo.value) == ""this error is rethrown""\n\n    # Can we fall-through to the default handler?\n    with pytest.raises(RuntimeError) as excinfo:\n        m.throws_logic_error()\n    assert msg(excinfo.value) == ""this error should fall through to the standard handler""\n\n    # Can we handle a helper-declared exception?\n    with pytest.raises(m.MyException5) as excinfo:\n        m.throws5()\n    assert msg(excinfo.value) == ""this is a helper-defined translated exception""\n\n    # Exception subclassing:\n    with pytest.raises(m.MyException5) as excinfo:\n        m.throws5_1()\n    assert msg(excinfo.value) == ""MyException5 subclass""\n    assert isinstance(excinfo.value, m.MyException5_1)\n\n    with pytest.raises(m.MyException5_1) as excinfo:\n        m.throws5_1()\n    assert msg(excinfo.value) == ""MyException5 subclass""\n\n    with pytest.raises(m.MyException5) as excinfo:\n        try:\n            m.throws5()\n        except m.MyException5_1:\n            raise RuntimeError(""Exception error: caught child from parent"")\n    assert msg(excinfo.value) == ""this is a helper-defined translated exception""\n\n\ndef test_nested_throws(capture):\n    """"""Tests nested (e.g. C++ -> Python -> C++) exception handling""""""\n\n    def throw_myex():\n        raise m.MyException(""nested error"")\n\n    def throw_myex5():\n        raise m.MyException5(""nested error 5"")\n\n    # In the comments below, the exception is caught in the first step, thrown in the last step\n\n    # C++ -> Python\n    with capture:\n        m.try_catch(m.MyException5, throw_myex5)\n    assert str(capture).startswith(""MyException5: nested error 5"")\n\n    # Python -> C++ -> Python\n    with pytest.raises(m.MyException) as excinfo:\n        m.try_catch(m.MyException5, throw_myex)\n    assert str(excinfo.value) == ""nested error""\n\n    def pycatch(exctype, f, *args):\n        try:\n            f(*args)\n        except m.MyException as e:\n            print(e)\n\n    # C++ -> Python -> C++ -> Python\n    with capture:\n        m.try_catch(\n            m.MyException5, pycatch, m.MyException, m.try_catch, m.MyException, throw_myex5)\n    assert str(capture).startswith(""MyException5: nested error 5"")\n\n    # C++ -> Python -> C++\n    with capture:\n        m.try_catch(m.MyException, pycatch, m.MyException5, m.throws4)\n    assert capture == ""this error is rethrown""\n\n    # Python -> C++ -> Python -> C++\n    with pytest.raises(m.MyException5) as excinfo:\n        m.try_catch(m.MyException, pycatch, m.MyException, m.throws5)\n    assert str(excinfo.value) == ""this is a helper-defined translated exception""\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_factory_constructors.py,0,"b'import pytest\nimport re\n\nfrom pybind11_tests import factory_constructors as m\nfrom pybind11_tests.factory_constructors import tag\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_init_factory_basic():\n    """"""Tests py::init_factory() wrapper around various ways of returning the object""""""\n\n    cstats = [ConstructorStats.get(c) for c in [m.TestFactory1, m.TestFactory2, m.TestFactory3]]\n    cstats[0].alive()  # force gc\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    x1 = m.TestFactory1(tag.unique_ptr, 3)\n    assert x1.value == ""3""\n    y1 = m.TestFactory1(tag.pointer)\n    assert y1.value == ""(empty)""\n    z1 = m.TestFactory1(""hi!"")\n    assert z1.value == ""hi!""\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 3\n\n    x2 = m.TestFactory2(tag.move)\n    assert x2.value == ""(empty2)""\n    y2 = m.TestFactory2(tag.pointer, 7)\n    assert y2.value == ""7""\n    z2 = m.TestFactory2(tag.unique_ptr, ""hi again"")\n    assert z2.value == ""hi again""\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 6\n\n    x3 = m.TestFactory3(tag.shared_ptr)\n    assert x3.value == ""(empty3)""\n    y3 = m.TestFactory3(tag.pointer, 42)\n    assert y3.value == ""42""\n    z3 = m.TestFactory3(""bye"")\n    assert z3.value == ""bye""\n\n    with pytest.raises(TypeError) as excinfo:\n        m.TestFactory3(tag.null_ptr)\n    assert str(excinfo.value) == ""pybind11::init(): factory function returned nullptr""\n\n    assert [i.alive() for i in cstats] == [3, 3, 3]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 9\n\n    del x1, y2, y3, z3\n    assert [i.alive() for i in cstats] == [2, 2, 1]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 5\n    del x2, x3, y1, z1, z2\n    assert [i.alive() for i in cstats] == [0, 0, 0]\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    assert [i.values() for i in cstats] == [\n        [""3"", ""hi!""],\n        [""7"", ""hi again""],\n        [""42"", ""bye""]\n    ]\n    assert [i.default_constructions for i in cstats] == [1, 1, 1]\n\n\ndef test_init_factory_signature(msg):\n    with pytest.raises(TypeError) as excinfo:\n        m.TestFactory1(""invalid"", ""constructor"", ""arguments"")\n    assert msg(excinfo.value) == """"""\n        __init__(): incompatible constructor arguments. The following argument types are supported:\n            1. m.factory_constructors.TestFactory1(arg0: m.factory_constructors.tag.unique_ptr_tag, arg1: int)\n            2. m.factory_constructors.TestFactory1(arg0: str)\n            3. m.factory_constructors.TestFactory1(arg0: m.factory_constructors.tag.pointer_tag)\n            4. m.factory_constructors.TestFactory1(arg0: handle, arg1: int, arg2: handle)\n\n        Invoked with: \'invalid\', \'constructor\', \'arguments\'\n    """"""  # noqa: E501 line too long\n\n    assert msg(m.TestFactory1.__init__.__doc__) == """"""\n        __init__(*args, **kwargs)\n        Overloaded function.\n\n        1. __init__(self: m.factory_constructors.TestFactory1, arg0: m.factory_constructors.tag.unique_ptr_tag, arg1: int) -> None\n\n        2. __init__(self: m.factory_constructors.TestFactory1, arg0: str) -> None\n\n        3. __init__(self: m.factory_constructors.TestFactory1, arg0: m.factory_constructors.tag.pointer_tag) -> None\n\n        4. __init__(self: m.factory_constructors.TestFactory1, arg0: handle, arg1: int, arg2: handle) -> None\n    """"""  # noqa: E501 line too long\n\n\ndef test_init_factory_casting():\n    """"""Tests py::init_factory() wrapper with various upcasting and downcasting returns""""""\n\n    cstats = [ConstructorStats.get(c) for c in [m.TestFactory3, m.TestFactory4, m.TestFactory5]]\n    cstats[0].alive()  # force gc\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    # Construction from derived references:\n    a = m.TestFactory3(tag.pointer, tag.TF4, 4)\n    assert a.value == ""4""\n    b = m.TestFactory3(tag.shared_ptr, tag.TF4, 5)\n    assert b.value == ""5""\n    c = m.TestFactory3(tag.pointer, tag.TF5, 6)\n    assert c.value == ""6""\n    d = m.TestFactory3(tag.shared_ptr, tag.TF5, 7)\n    assert d.value == ""7""\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 4\n\n    # Shared a lambda with TF3:\n    e = m.TestFactory4(tag.pointer, tag.TF4, 8)\n    assert e.value == ""8""\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 5\n    assert [i.alive() for i in cstats] == [5, 3, 2]\n\n    del a\n    assert [i.alive() for i in cstats] == [4, 2, 2]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 4\n\n    del b, c, e\n    assert [i.alive() for i in cstats] == [1, 0, 1]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 1\n\n    del d\n    assert [i.alive() for i in cstats] == [0, 0, 0]\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    assert [i.values() for i in cstats] == [\n        [""4"", ""5"", ""6"", ""7"", ""8""],\n        [""4"", ""5"", ""8""],\n        [""6"", ""7""]\n    ]\n\n\ndef test_init_factory_alias():\n    """"""Tests py::init_factory() wrapper with value conversions and alias types""""""\n\n    cstats = [m.TestFactory6.get_cstats(), m.TestFactory6.get_alias_cstats()]\n    cstats[0].alive()  # force gc\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    a = m.TestFactory6(tag.base, 1)\n    assert a.get() == 1\n    assert not a.has_alias()\n    b = m.TestFactory6(tag.alias, ""hi there"")\n    assert b.get() == 8\n    assert b.has_alias()\n    c = m.TestFactory6(tag.alias, 3)\n    assert c.get() == 3\n    assert c.has_alias()\n    d = m.TestFactory6(tag.alias, tag.pointer, 4)\n    assert d.get() == 4\n    assert d.has_alias()\n    e = m.TestFactory6(tag.base, tag.pointer, 5)\n    assert e.get() == 5\n    assert not e.has_alias()\n    f = m.TestFactory6(tag.base, tag.alias, tag.pointer, 6)\n    assert f.get() == 6\n    assert f.has_alias()\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 6\n    assert [i.alive() for i in cstats] == [6, 4]\n\n    del a, b, e\n    assert [i.alive() for i in cstats] == [3, 3]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 3\n    del f, c, d\n    assert [i.alive() for i in cstats] == [0, 0]\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    class MyTest(m.TestFactory6):\n        def __init__(self, *args):\n            m.TestFactory6.__init__(self, *args)\n\n        def get(self):\n            return -5 + m.TestFactory6.get(self)\n\n    # Return Class by value, moved into new alias:\n    z = MyTest(tag.base, 123)\n    assert z.get() == 118\n    assert z.has_alias()\n\n    # Return alias by value, moved into new alias:\n    y = MyTest(tag.alias, ""why hello!"")\n    assert y.get() == 5\n    assert y.has_alias()\n\n    # Return Class by pointer, moved into new alias then original destroyed:\n    x = MyTest(tag.base, tag.pointer, 47)\n    assert x.get() == 42\n    assert x.has_alias()\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 3\n    assert [i.alive() for i in cstats] == [3, 3]\n    del x, y, z\n    assert [i.alive() for i in cstats] == [0, 0]\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    assert [i.values() for i in cstats] == [\n        [""1"", ""8"", ""3"", ""4"", ""5"", ""6"", ""123"", ""10"", ""47""],\n        [""hi there"", ""3"", ""4"", ""6"", ""move"", ""123"", ""why hello!"", ""move"", ""47""]\n    ]\n\n\ndef test_init_factory_dual():\n    """"""Tests init factory functions with dual main/alias factory functions""""""\n    from pybind11_tests.factory_constructors import TestFactory7\n\n    cstats = [TestFactory7.get_cstats(), TestFactory7.get_alias_cstats()]\n    cstats[0].alive()  # force gc\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    class PythFactory7(TestFactory7):\n        def get(self):\n            return 100 + TestFactory7.get(self)\n\n    a1 = TestFactory7(1)\n    a2 = PythFactory7(2)\n    assert a1.get() == 1\n    assert a2.get() == 102\n    assert not a1.has_alias()\n    assert a2.has_alias()\n\n    b1 = TestFactory7(tag.pointer, 3)\n    b2 = PythFactory7(tag.pointer, 4)\n    assert b1.get() == 3\n    assert b2.get() == 104\n    assert not b1.has_alias()\n    assert b2.has_alias()\n\n    c1 = TestFactory7(tag.mixed, 5)\n    c2 = PythFactory7(tag.mixed, 6)\n    assert c1.get() == 5\n    assert c2.get() == 106\n    assert not c1.has_alias()\n    assert c2.has_alias()\n\n    d1 = TestFactory7(tag.base, tag.pointer, 7)\n    d2 = PythFactory7(tag.base, tag.pointer, 8)\n    assert d1.get() == 7\n    assert d2.get() == 108\n    assert not d1.has_alias()\n    assert d2.has_alias()\n\n    # Both return an alias; the second multiplies the value by 10:\n    e1 = TestFactory7(tag.alias, tag.pointer, 9)\n    e2 = PythFactory7(tag.alias, tag.pointer, 10)\n    assert e1.get() == 9\n    assert e2.get() == 200\n    assert e1.has_alias()\n    assert e2.has_alias()\n\n    f1 = TestFactory7(tag.shared_ptr, tag.base, 11)\n    f2 = PythFactory7(tag.shared_ptr, tag.base, 12)\n    assert f1.get() == 11\n    assert f2.get() == 112\n    assert not f1.has_alias()\n    assert f2.has_alias()\n\n    g1 = TestFactory7(tag.shared_ptr, tag.invalid_base, 13)\n    assert g1.get() == 13\n    assert not g1.has_alias()\n    with pytest.raises(TypeError) as excinfo:\n        PythFactory7(tag.shared_ptr, tag.invalid_base, 14)\n    assert (str(excinfo.value) ==\n            ""pybind11::init(): construction failed: returned holder-wrapped instance is not an ""\n            ""alias instance"")\n\n    assert [i.alive() for i in cstats] == [13, 7]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 13\n\n    del a1, a2, b1, d1, e1, e2\n    assert [i.alive() for i in cstats] == [7, 4]\n    assert ConstructorStats.detail_reg_inst() == n_inst + 7\n    del b2, c1, c2, d2, f1, f2, g1\n    assert [i.alive() for i in cstats] == [0, 0]\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    assert [i.values() for i in cstats] == [\n        [""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""100"", ""11"", ""12"", ""13"", ""14""],\n        [""2"", ""4"", ""6"", ""8"", ""9"", ""100"", ""12""]\n    ]\n\n\ndef test_no_placement_new(capture):\n    """"""Prior to 2.2, `py::init<...>` relied on the type supporting placement\n    new; this tests a class without placement new support.""""""\n    with capture:\n        a = m.NoPlacementNew(123)\n\n    found = re.search(r\'^operator new called, returning (\\d+)\\n$\', str(capture))\n    assert found\n    assert a.i == 123\n    with capture:\n        del a\n        pytest.gc_collect()\n    assert capture == ""operator delete called on "" + found.group(1)\n\n    with capture:\n        b = m.NoPlacementNew()\n\n    found = re.search(r\'^operator new called, returning (\\d+)\\n$\', str(capture))\n    assert found\n    assert b.i == 100\n    with capture:\n        del b\n        pytest.gc_collect()\n    assert capture == ""operator delete called on "" + found.group(1)\n\n\ndef test_multiple_inheritance():\n    class MITest(m.TestFactory1, m.TestFactory2):\n        def __init__(self):\n            m.TestFactory1.__init__(self, tag.unique_ptr, 33)\n            m.TestFactory2.__init__(self, tag.move)\n\n    a = MITest()\n    assert m.TestFactory1.value.fget(a) == ""33""\n    assert m.TestFactory2.value.fget(a) == ""(empty2)""\n\n\ndef create_and_destroy(*args):\n    a = m.NoisyAlloc(*args)\n    print(""---"")\n    del a\n    pytest.gc_collect()\n\n\ndef strip_comments(s):\n    return re.sub(r\'\\s+#.*\', \'\', s)\n\n\ndef test_reallocations(capture, msg):\n    """"""When the constructor is overloaded, previous overloads can require a preallocated value.\n    This test makes sure that such preallocated values only happen when they might be necessary,\n    and that they are deallocated properly""""""\n\n    pytest.gc_collect()\n\n    with capture:\n        create_and_destroy(1)\n    assert msg(capture) == """"""\n        noisy new\n        noisy placement new\n        NoisyAlloc(int 1)\n        ---\n        ~NoisyAlloc()\n        noisy delete\n    """"""\n    with capture:\n        create_and_destroy(1.5)\n    assert msg(capture) == strip_comments(""""""\n        noisy new               # allocation required to attempt first overload\n        noisy delete            # have to dealloc before considering factory init overload\n        noisy new               # pointer factory calling ""new"", part 1: allocation\n        NoisyAlloc(double 1.5)  # ... part two, invoking constructor\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n    with capture:\n        create_and_destroy(2, 3)\n    assert msg(capture) == strip_comments(""""""\n        noisy new          # pointer factory calling ""new"", allocation\n        NoisyAlloc(int 2)  # constructor\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n    with capture:\n        create_and_destroy(2.5, 3)\n    assert msg(capture) == strip_comments(""""""\n        NoisyAlloc(double 2.5)  # construction (local func variable: operator_new not called)\n        noisy new               # return-by-value ""new"" part 1: allocation\n        ~NoisyAlloc()           # moved-away local func variable destruction\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n    with capture:\n        create_and_destroy(3.5, 4.5)\n    assert msg(capture) == strip_comments(""""""\n        noisy new               # preallocation needed before invoking placement-new overload\n        noisy placement new     # Placement new\n        NoisyAlloc(double 3.5)  # construction\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n    with capture:\n        create_and_destroy(4, 0.5)\n    assert msg(capture) == strip_comments(""""""\n        noisy new          # preallocation needed before invoking placement-new overload\n        noisy delete       # deallocation of preallocated storage\n        noisy new          # Factory pointer allocation\n        NoisyAlloc(int 4)  # factory pointer construction\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n    with capture:\n        create_and_destroy(5, ""hi"")\n    assert msg(capture) == strip_comments(""""""\n        noisy new            # preallocation needed before invoking first placement new\n        noisy delete         # delete before considering new-style constructor\n        noisy new            # preallocation for second placement new\n        noisy placement new  # Placement new in the second placement new overload\n        NoisyAlloc(int 5)    # construction\n        ---\n        ~NoisyAlloc()  # Destructor\n        noisy delete   # operator delete\n    """""")\n\n\n@pytest.unsupported_on_py2\ndef test_invalid_self():\n    """"""Tests invocation of the pybind-registered base class with an invalid `self` argument.  You\n    can only actually do this on Python 3: Python 2 raises an exception itself if you try.""""""\n    class NotPybindDerived(object):\n        pass\n\n    # Attempts to initialize with an invalid type passed as `self`:\n    class BrokenTF1(m.TestFactory1):\n        def __init__(self, bad):\n            if bad == 1:\n                a = m.TestFactory2(tag.pointer, 1)\n                m.TestFactory1.__init__(a, tag.pointer)\n            elif bad == 2:\n                a = NotPybindDerived()\n                m.TestFactory1.__init__(a, tag.pointer)\n\n    # Same as above, but for a class with an alias:\n    class BrokenTF6(m.TestFactory6):\n        def __init__(self, bad):\n            if bad == 1:\n                a = m.TestFactory2(tag.pointer, 1)\n                m.TestFactory6.__init__(a, tag.base, 1)\n            elif bad == 2:\n                a = m.TestFactory2(tag.pointer, 1)\n                m.TestFactory6.__init__(a, tag.alias, 1)\n            elif bad == 3:\n                m.TestFactory6.__init__(NotPybindDerived.__new__(NotPybindDerived), tag.base, 1)\n            elif bad == 4:\n                m.TestFactory6.__init__(NotPybindDerived.__new__(NotPybindDerived), tag.alias, 1)\n\n    for arg in (1, 2):\n        with pytest.raises(TypeError) as excinfo:\n            BrokenTF1(arg)\n        assert str(excinfo.value) == ""__init__(self, ...) called with invalid `self` argument""\n\n    for arg in (1, 2, 3, 4):\n        with pytest.raises(TypeError) as excinfo:\n            BrokenTF6(arg)\n        assert str(excinfo.value) == ""__init__(self, ...) called with invalid `self` argument""\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_iostream.py,0,"b'from pybind11_tests import iostream as m\nimport sys\n\nfrom contextlib import contextmanager\n\ntry:\n    # Python 3\n    from io import StringIO\nexcept ImportError:\n    # Python 2\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n\ntry:\n    # Python 3.4\n    from contextlib import redirect_stdout\nexcept ImportError:\n    @contextmanager\n    def redirect_stdout(target):\n        original = sys.stdout\n        sys.stdout = target\n        yield\n        sys.stdout = original\n\ntry:\n    # Python 3.5\n    from contextlib import redirect_stderr\nexcept ImportError:\n    @contextmanager\n    def redirect_stderr(target):\n        original = sys.stderr\n        sys.stderr = target\n        yield\n        sys.stderr = original\n\n\ndef test_captured(capsys):\n    msg = ""I\'ve been redirected to Python, I hope!""\n    m.captured_output(msg)\n    stdout, stderr = capsys.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n\n    m.captured_output_default(msg)\n    stdout, stderr = capsys.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n\n    m.captured_err(msg)\n    stdout, stderr = capsys.readouterr()\n    assert stdout == \'\'\n    assert stderr == msg\n\n\ndef test_captured_large_string(capsys):\n    # Make this bigger than the buffer used on the C++ side: 1024 chars\n    msg = ""I\'ve been redirected to Python, I hope!""\n    msg = msg * (1024 // len(msg) + 1)\n\n    m.captured_output_default(msg)\n    stdout, stderr = capsys.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n\n\ndef test_guard_capture(capsys):\n    msg = ""I\'ve been redirected to Python, I hope!""\n    m.guard_output(msg)\n    stdout, stderr = capsys.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n\n\ndef test_series_captured(capture):\n    with capture:\n        m.captured_output(""a"")\n        m.captured_output(""b"")\n    assert capture == ""ab""\n\n\ndef test_flush(capfd):\n    msg = ""(not flushed)""\n    msg2 = ""(flushed)""\n\n    with m.ostream_redirect():\n        m.noisy_function(msg, flush=False)\n        stdout, stderr = capfd.readouterr()\n        assert stdout == \'\'\n\n        m.noisy_function(msg2, flush=True)\n        stdout, stderr = capfd.readouterr()\n        assert stdout == msg + msg2\n\n        m.noisy_function(msg, flush=False)\n\n    stdout, stderr = capfd.readouterr()\n    assert stdout == msg\n\n\ndef test_not_captured(capfd):\n    msg = ""Something that should not show up in log""\n    stream = StringIO()\n    with redirect_stdout(stream):\n        m.raw_output(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n    assert stream.getvalue() == \'\'\n\n    stream = StringIO()\n    with redirect_stdout(stream):\n        m.captured_output(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'\'\n    assert stderr == \'\'\n    assert stream.getvalue() == msg\n\n\ndef test_err(capfd):\n    msg = ""Something that should not show up in log""\n    stream = StringIO()\n    with redirect_stderr(stream):\n        m.raw_err(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'\'\n    assert stderr == msg\n    assert stream.getvalue() == \'\'\n\n    stream = StringIO()\n    with redirect_stderr(stream):\n        m.captured_err(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'\'\n    assert stderr == \'\'\n    assert stream.getvalue() == msg\n\n\ndef test_multi_captured(capfd):\n    stream = StringIO()\n    with redirect_stdout(stream):\n        m.captured_output(""a"")\n        m.raw_output(""b"")\n        m.captured_output(""c"")\n        m.raw_output(""d"")\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'bd\'\n    assert stream.getvalue() == \'ac\'\n\n\ndef test_dual(capsys):\n    m.captured_dual(""a"", ""b"")\n    stdout, stderr = capsys.readouterr()\n    assert stdout == ""a""\n    assert stderr == ""b""\n\n\ndef test_redirect(capfd):\n    msg = ""Should not be in log!""\n    stream = StringIO()\n    with redirect_stdout(stream):\n        m.raw_output(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == msg\n    assert stream.getvalue() == \'\'\n\n    stream = StringIO()\n    with redirect_stdout(stream):\n        with m.ostream_redirect():\n            m.raw_output(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'\'\n    assert stream.getvalue() == msg\n\n    stream = StringIO()\n    with redirect_stdout(stream):\n        m.raw_output(msg)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == msg\n    assert stream.getvalue() == \'\'\n\n\ndef test_redirect_err(capfd):\n    msg = ""StdOut""\n    msg2 = ""StdErr""\n\n    stream = StringIO()\n    with redirect_stderr(stream):\n        with m.ostream_redirect(stdout=False):\n            m.raw_output(msg)\n            m.raw_err(msg2)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == msg\n    assert stderr == \'\'\n    assert stream.getvalue() == msg2\n\n\ndef test_redirect_both(capfd):\n    msg = ""StdOut""\n    msg2 = ""StdErr""\n\n    stream = StringIO()\n    stream2 = StringIO()\n    with redirect_stdout(stream):\n        with redirect_stderr(stream2):\n            with m.ostream_redirect():\n                m.raw_output(msg)\n                m.raw_err(msg2)\n    stdout, stderr = capfd.readouterr()\n    assert stdout == \'\'\n    assert stderr == \'\'\n    assert stream.getvalue() == msg\n    assert stream2.getvalue() == msg2\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_kwargs_and_defaults.py,0,"b'import pytest\nfrom pybind11_tests import kwargs_and_defaults as m\n\n\ndef test_function_signatures(doc):\n    assert doc(m.kw_func0) == ""kw_func0(arg0: int, arg1: int) -> str""\n    assert doc(m.kw_func1) == ""kw_func1(x: int, y: int) -> str""\n    assert doc(m.kw_func2) == ""kw_func2(x: int=100, y: int=200) -> str""\n    assert doc(m.kw_func3) == ""kw_func3(data: str=\'Hello world!\') -> None""\n    assert doc(m.kw_func4) == ""kw_func4(myList: List[int]=[13, 17]) -> str""\n    assert doc(m.kw_func_udl) == ""kw_func_udl(x: int, y: int=300) -> str""\n    assert doc(m.kw_func_udl_z) == ""kw_func_udl_z(x: int, y: int=0) -> str""\n    assert doc(m.args_function) == ""args_function(*args) -> tuple""\n    assert doc(m.args_kwargs_function) == ""args_kwargs_function(*args, **kwargs) -> tuple""\n    assert doc(m.KWClass.foo0) == \\\n        ""foo0(self: m.kwargs_and_defaults.KWClass, arg0: int, arg1: float) -> None""\n    assert doc(m.KWClass.foo1) == \\\n        ""foo1(self: m.kwargs_and_defaults.KWClass, x: int, y: float) -> None""\n\n\ndef test_named_arguments(msg):\n    assert m.kw_func0(5, 10) == ""x=5, y=10""\n\n    assert m.kw_func1(5, 10) == ""x=5, y=10""\n    assert m.kw_func1(5, y=10) == ""x=5, y=10""\n    assert m.kw_func1(y=10, x=5) == ""x=5, y=10""\n\n    assert m.kw_func2() == ""x=100, y=200""\n    assert m.kw_func2(5) == ""x=5, y=200""\n    assert m.kw_func2(x=5) == ""x=5, y=200""\n    assert m.kw_func2(y=10) == ""x=100, y=10""\n    assert m.kw_func2(5, 10) == ""x=5, y=10""\n    assert m.kw_func2(x=5, y=10) == ""x=5, y=10""\n\n    with pytest.raises(TypeError) as excinfo:\n        # noinspection PyArgumentList\n        m.kw_func2(x=5, y=10, z=12)\n    assert excinfo.match(\n        r\'(?s)^kw_func2\\(\\): incompatible.*Invoked with: kwargs: ((x=5|y=10|z=12)(, |$))\' + \'{3}$\')\n\n    assert m.kw_func4() == ""{13 17}""\n    assert m.kw_func4(myList=[1, 2, 3]) == ""{1 2 3}""\n\n    assert m.kw_func_udl(x=5, y=10) == ""x=5, y=10""\n    assert m.kw_func_udl_z(x=5) == ""x=5, y=0""\n\n\ndef test_arg_and_kwargs():\n    args = \'arg1_value\', \'arg2_value\', 3\n    assert m.args_function(*args) == args\n\n    args = \'a1\', \'a2\'\n    kwargs = dict(arg3=\'a3\', arg4=4)\n    assert m.args_kwargs_function(*args, **kwargs) == (args, kwargs)\n\n\ndef test_mixed_args_and_kwargs(msg):\n    mpa = m.mixed_plus_args\n    mpk = m.mixed_plus_kwargs\n    mpak = m.mixed_plus_args_kwargs\n    mpakd = m.mixed_plus_args_kwargs_defaults\n\n    assert mpa(1, 2.5, 4, 99.5, None) == (1, 2.5, (4, 99.5, None))\n    assert mpa(1, 2.5) == (1, 2.5, ())\n    with pytest.raises(TypeError) as excinfo:\n        assert mpa(1)\n    assert msg(excinfo.value) == """"""\n        mixed_plus_args(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: int, arg1: float, *args) -> tuple\n\n        Invoked with: 1\n    """"""  # noqa: E501 line too long\n    with pytest.raises(TypeError) as excinfo:\n        assert mpa()\n    assert msg(excinfo.value) == """"""\n        mixed_plus_args(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: int, arg1: float, *args) -> tuple\n\n        Invoked with:\n    """"""  # noqa: E501 line too long\n\n    assert mpk(-2, 3.5, pi=3.14159, e=2.71828) == (-2, 3.5, {\'e\': 2.71828, \'pi\': 3.14159})\n    assert mpak(7, 7.7, 7.77, 7.777, 7.7777, minusseven=-7) == (\n        7, 7.7, (7.77, 7.777, 7.7777), {\'minusseven\': -7})\n    assert mpakd() == (1, 3.14159, (), {})\n    assert mpakd(3) == (3, 3.14159, (), {})\n    assert mpakd(j=2.71828) == (1, 2.71828, (), {})\n    assert mpakd(k=42) == (1, 3.14159, (), {\'k\': 42})\n    assert mpakd(1, 1, 2, 3, 5, 8, then=13, followedby=21) == (\n        1, 1, (2, 3, 5, 8), {\'then\': 13, \'followedby\': 21})\n    # Arguments specified both positionally and via kwargs should fail:\n    with pytest.raises(TypeError) as excinfo:\n        assert mpakd(1, i=1)\n    assert msg(excinfo.value) == """"""\n        mixed_plus_args_kwargs_defaults(): incompatible function arguments. The following argument types are supported:\n            1. (i: int=1, j: float=3.14159, *args, **kwargs) -> tuple\n\n        Invoked with: 1; kwargs: i=1\n    """"""  # noqa: E501 line too long\n    with pytest.raises(TypeError) as excinfo:\n        assert mpakd(1, 2, j=1)\n    assert msg(excinfo.value) == """"""\n        mixed_plus_args_kwargs_defaults(): incompatible function arguments. The following argument types are supported:\n            1. (i: int=1, j: float=3.14159, *args, **kwargs) -> tuple\n\n        Invoked with: 1, 2; kwargs: j=1\n    """"""  # noqa: E501 line too long\n\n\ndef test_args_refcount():\n    """"""Issue/PR #1216 - py::args elements get double-inc_ref()ed when combined with regular\n    arguments""""""\n    refcount = m.arg_refcount_h\n\n    myval = 54321\n    expected = refcount(myval)\n    assert m.arg_refcount_h(myval) == expected\n    assert m.arg_refcount_o(myval) == expected + 1\n    assert m.arg_refcount_h(myval) == expected\n    assert refcount(myval) == expected\n\n    assert m.mixed_plus_args(1, 2.0, ""a"", myval) == (1, 2.0, (""a"", myval))\n    assert refcount(myval) == expected\n\n    assert m.mixed_plus_kwargs(3, 4.0, a=1, b=myval) == (3, 4.0, {""a"": 1, ""b"": myval})\n    assert refcount(myval) == expected\n\n    assert m.args_function(-1, myval) == (-1, myval)\n    assert refcount(myval) == expected\n\n    assert m.mixed_plus_args_kwargs(5, 6.0, myval, a=myval) == (5, 6.0, (myval,), {""a"": myval})\n    assert refcount(myval) == expected\n\n    assert m.args_kwargs_function(7, 8, myval, a=1, b=myval) == \\\n        ((7, 8, myval), {""a"": 1, ""b"": myval})\n    assert refcount(myval) == expected\n\n    exp3 = refcount(myval, myval, myval)\n    assert m.args_refcount(myval, myval, myval) == (exp3, exp3, exp3)\n    assert refcount(myval) == expected\n\n    # This function takes the first arg as a `py::object` and the rest as a `py::args`.  Unlike the\n    # previous case, when we have both positional and `py::args` we need to construct a new tuple\n    # for the `py::args`; in the previous case, we could simply inc_ref and pass on Python\'s input\n    # tuple without having to inc_ref the individual elements, but here we can\'t, hence the extra\n    # refs.\n    assert m.mixed_args_refcount(myval, myval, myval) == (exp3 + 3, exp3 + 3, exp3 + 3)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_local_bindings.py,0,"b'import pytest\n\nfrom pybind11_tests import local_bindings as m\n\n\ndef test_load_external():\n    """"""Load a `py::module_local` type that\'s only registered in an external module""""""\n    import pybind11_cross_module_tests as cm\n\n    assert m.load_external1(cm.ExternalType1(11)) == 11\n    assert m.load_external2(cm.ExternalType2(22)) == 22\n\n    with pytest.raises(TypeError) as excinfo:\n        assert m.load_external2(cm.ExternalType1(21)) == 21\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        assert m.load_external1(cm.ExternalType2(12)) == 12\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n\ndef test_local_bindings():\n    """"""Tests that duplicate `py::module_local` class bindings work across modules""""""\n\n    # Make sure we can load the second module with the conflicting (but local) definition:\n    import pybind11_cross_module_tests as cm\n\n    i1 = m.LocalType(5)\n    assert i1.get() == 4\n    assert i1.get3() == 8\n\n    i2 = cm.LocalType(10)\n    assert i2.get() == 11\n    assert i2.get2() == 12\n\n    assert not hasattr(i1, \'get2\')\n    assert not hasattr(i2, \'get3\')\n\n    # Loading within the local module\n    assert m.local_value(i1) == 5\n    assert cm.local_value(i2) == 10\n\n    # Cross-module loading works as well (on failure, the type loader looks for\n    # external module-local converters):\n    assert m.local_value(i2) == 10\n    assert cm.local_value(i1) == 5\n\n\ndef test_nonlocal_failure():\n    """"""Tests that attempting to register a non-local type in multiple modules fails""""""\n    import pybind11_cross_module_tests as cm\n\n    with pytest.raises(RuntimeError) as excinfo:\n        cm.register_nonlocal()\n    assert str(excinfo.value) == \'generic_type: type ""NonLocalType"" is already registered!\'\n\n\ndef test_duplicate_local():\n    """"""Tests expected failure when registering a class twice with py::local in the same module""""""\n    with pytest.raises(RuntimeError) as excinfo:\n        m.register_local_external()\n    import pybind11_tests\n    assert str(excinfo.value) == (\n        \'generic_type: type ""LocalExternal"" is already registered!\'\n        if hasattr(pybind11_tests, \'class_\') else \'test_class not enabled\')\n\n\ndef test_stl_bind_local():\n    import pybind11_cross_module_tests as cm\n\n    v1, v2 = m.LocalVec(), cm.LocalVec()\n    v1.append(m.LocalType(1))\n    v1.append(m.LocalType(2))\n    v2.append(cm.LocalType(1))\n    v2.append(cm.LocalType(2))\n\n    # Cross module value loading:\n    v1.append(cm.LocalType(3))\n    v2.append(m.LocalType(3))\n\n    assert [i.get() for i in v1] == [0, 1, 2]\n    assert [i.get() for i in v2] == [2, 3, 4]\n\n    v3, v4 = m.NonLocalVec(), cm.NonLocalVec2()\n    v3.append(m.NonLocalType(1))\n    v3.append(m.NonLocalType(2))\n    v4.append(m.NonLocal2(3))\n    v4.append(m.NonLocal2(4))\n\n    assert [i.get() for i in v3] == [1, 2]\n    assert [i.get() for i in v4] == [13, 14]\n\n    d1, d2 = m.LocalMap(), cm.LocalMap()\n    d1[""a""] = v1[0]\n    d1[""b""] = v1[1]\n    d2[""c""] = v2[0]\n    d2[""d""] = v2[1]\n    assert {i: d1[i].get() for i in d1} == {\'a\': 0, \'b\': 1}\n    assert {i: d2[i].get() for i in d2} == {\'c\': 2, \'d\': 3}\n\n\ndef test_stl_bind_global():\n    import pybind11_cross_module_tests as cm\n\n    with pytest.raises(RuntimeError) as excinfo:\n        cm.register_nonlocal_map()\n    assert str(excinfo.value) == \'generic_type: type ""NonLocalMap"" is already registered!\'\n\n    with pytest.raises(RuntimeError) as excinfo:\n        cm.register_nonlocal_vec()\n    assert str(excinfo.value) == \'generic_type: type ""NonLocalVec"" is already registered!\'\n\n    with pytest.raises(RuntimeError) as excinfo:\n        cm.register_nonlocal_map2()\n    assert str(excinfo.value) == \'generic_type: type ""NonLocalMap2"" is already registered!\'\n\n\ndef test_mixed_local_global():\n    """"""Local types take precedence over globally registered types: a module with a `module_local`\n    type can be registered even if the type is already registered globally.  With the module,\n    casting will go to the local type; outside the module casting goes to the global type.""""""\n    import pybind11_cross_module_tests as cm\n    m.register_mixed_global()\n    m.register_mixed_local()\n\n    a = []\n    a.append(m.MixedGlobalLocal(1))\n    a.append(m.MixedLocalGlobal(2))\n    a.append(m.get_mixed_gl(3))\n    a.append(m.get_mixed_lg(4))\n\n    assert [x.get() for x in a] == [101, 1002, 103, 1004]\n\n    cm.register_mixed_global_local()\n    cm.register_mixed_local_global()\n    a.append(m.MixedGlobalLocal(5))\n    a.append(m.MixedLocalGlobal(6))\n    a.append(cm.MixedGlobalLocal(7))\n    a.append(cm.MixedLocalGlobal(8))\n    a.append(m.get_mixed_gl(9))\n    a.append(m.get_mixed_lg(10))\n    a.append(cm.get_mixed_gl(11))\n    a.append(cm.get_mixed_lg(12))\n\n    assert [x.get() for x in a] == \\\n        [101, 1002, 103, 1004, 105, 1006, 207, 2008, 109, 1010, 211, 2012]\n\n\ndef test_internal_locals_differ():\n    """"""Makes sure the internal local type map differs across the two modules""""""\n    import pybind11_cross_module_tests as cm\n    assert m.local_cpp_types_addr() != cm.local_cpp_types_addr()\n\n\ndef test_stl_caster_vs_stl_bind(msg):\n    """"""One module uses a generic vector caster from `<pybind11/stl.h>` while the other\n    exports `std::vector<int>` via `py:bind_vector` and `py::module_local`""""""\n    import pybind11_cross_module_tests as cm\n\n    v1 = cm.VectorInt([1, 2, 3])\n    assert m.load_vector_via_caster(v1) == 6\n    assert cm.load_vector_via_binding(v1) == 6\n\n    v2 = [1, 2, 3]\n    assert m.load_vector_via_caster(v2) == 6\n    with pytest.raises(TypeError) as excinfo:\n        cm.load_vector_via_binding(v2) == 6\n    assert msg(excinfo.value) == """"""\n    load_vector_via_binding(): incompatible function arguments. The following argument types are supported:\n        1. (arg0: pybind11_cross_module_tests.VectorInt) -> int\n\n    Invoked with: [1, 2, 3]\n    """"""  # noqa: E501 line too long\n\n\ndef test_cross_module_calls():\n    import pybind11_cross_module_tests as cm\n\n    v1 = m.LocalVec()\n    v1.append(m.LocalType(1))\n    v2 = cm.LocalVec()\n    v2.append(cm.LocalType(2))\n\n    # Returning the self pointer should get picked up as returning an existing\n    # instance (even when that instance is of a foreign, non-local type).\n    assert m.return_self(v1) is v1\n    assert cm.return_self(v2) is v2\n    assert m.return_self(v2) is v2\n    assert cm.return_self(v1) is v1\n\n    assert m.LocalVec is not cm.LocalVec\n    # Returning a copy, on the other hand, always goes to the local type,\n    # regardless of where the source type came from.\n    assert type(m.return_copy(v1)) is m.LocalVec\n    assert type(m.return_copy(v2)) is m.LocalVec\n    assert type(cm.return_copy(v1)) is cm.LocalVec\n    assert type(cm.return_copy(v2)) is cm.LocalVec\n\n    # Test the example given in the documentation (which also tests inheritance casting):\n    mycat = m.Cat(""Fluffy"")\n    mydog = cm.Dog(""Rover"")\n    assert mycat.get_name() == ""Fluffy""\n    assert mydog.name() == ""Rover""\n    assert m.Cat.__base__.__name__ == ""Pet""\n    assert cm.Dog.__base__.__name__ == ""Pet""\n    assert m.Cat.__base__ is not cm.Dog.__base__\n    assert m.pet_name(mycat) == ""Fluffy""\n    assert m.pet_name(mydog) == ""Rover""\n    assert cm.pet_name(mycat) == ""Fluffy""\n    assert cm.pet_name(mydog) == ""Rover""\n\n    assert m.MixGL is not cm.MixGL\n    a = m.MixGL(1)\n    b = cm.MixGL(2)\n    assert m.get_gl_value(a) == 11\n    assert m.get_gl_value(b) == 12\n    assert cm.get_gl_value(a) == 101\n    assert cm.get_gl_value(b) == 102\n\n    c, d = m.MixGL2(3), cm.MixGL2(4)\n    with pytest.raises(TypeError) as excinfo:\n        m.get_gl_value(c)\n    assert ""incompatible function arguments"" in str(excinfo)\n    with pytest.raises(TypeError) as excinfo:\n        m.get_gl_value(d)\n    assert ""incompatible function arguments"" in str(excinfo)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_methods_and_attributes.py,0,"b'import pytest\nfrom pybind11_tests import methods_and_attributes as m\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_methods_and_attributes():\n    instance1 = m.ExampleMandA()\n    instance2 = m.ExampleMandA(32)\n\n    instance1.add1(instance2)\n    instance1.add2(instance2)\n    instance1.add3(instance2)\n    instance1.add4(instance2)\n    instance1.add5(instance2)\n    instance1.add6(32)\n    instance1.add7(32)\n    instance1.add8(32)\n    instance1.add9(32)\n    instance1.add10(32)\n\n    assert str(instance1) == ""ExampleMandA[value=320]""\n    assert str(instance2) == ""ExampleMandA[value=32]""\n    assert str(instance1.self1()) == ""ExampleMandA[value=320]""\n    assert str(instance1.self2()) == ""ExampleMandA[value=320]""\n    assert str(instance1.self3()) == ""ExampleMandA[value=320]""\n    assert str(instance1.self4()) == ""ExampleMandA[value=320]""\n    assert str(instance1.self5()) == ""ExampleMandA[value=320]""\n\n    assert instance1.internal1() == 320\n    assert instance1.internal2() == 320\n    assert instance1.internal3() == 320\n    assert instance1.internal4() == 320\n    assert instance1.internal5() == 320\n\n    assert instance1.overloaded() == ""()""\n    assert instance1.overloaded(0) == ""(int)""\n    assert instance1.overloaded(1, 1.0) == ""(int, float)""\n    assert instance1.overloaded(2.0, 2) == ""(float, int)""\n    assert instance1.overloaded(3,   3) == ""(int, int)""\n    assert instance1.overloaded(4., 4.) == ""(float, float)""\n    assert instance1.overloaded_const(-3) == ""(int) const""\n    assert instance1.overloaded_const(5, 5.0) == ""(int, float) const""\n    assert instance1.overloaded_const(6.0, 6) == ""(float, int) const""\n    assert instance1.overloaded_const(7,   7) == ""(int, int) const""\n    assert instance1.overloaded_const(8., 8.) == ""(float, float) const""\n    assert instance1.overloaded_float(1, 1) == ""(float, float)""\n    assert instance1.overloaded_float(1, 1.) == ""(float, float)""\n    assert instance1.overloaded_float(1., 1) == ""(float, float)""\n    assert instance1.overloaded_float(1., 1.) == ""(float, float)""\n\n    assert instance1.value == 320\n    instance1.value = 100\n    assert str(instance1) == ""ExampleMandA[value=100]""\n\n    cstats = ConstructorStats.get(m.ExampleMandA)\n    assert cstats.alive() == 2\n    del instance1, instance2\n    assert cstats.alive() == 0\n    assert cstats.values() == [""32""]\n    assert cstats.default_constructions == 1\n    assert cstats.copy_constructions == 3\n    assert cstats.move_constructions >= 1\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n\ndef test_copy_method():\n    """"""Issue #443: calling copied methods fails in Python 3""""""\n\n    m.ExampleMandA.add2c = m.ExampleMandA.add2\n    m.ExampleMandA.add2d = m.ExampleMandA.add2b\n    a = m.ExampleMandA(123)\n    assert a.value == 123\n    a.add2(m.ExampleMandA(-100))\n    assert a.value == 23\n    a.add2b(m.ExampleMandA(20))\n    assert a.value == 43\n    a.add2c(m.ExampleMandA(6))\n    assert a.value == 49\n    a.add2d(m.ExampleMandA(-7))\n    assert a.value == 42\n\n\ndef test_properties():\n    instance = m.TestProperties()\n\n    assert instance.def_readonly == 1\n    with pytest.raises(AttributeError):\n        instance.def_readonly = 2\n\n    instance.def_readwrite = 2\n    assert instance.def_readwrite == 2\n\n    assert instance.def_property_readonly == 2\n    with pytest.raises(AttributeError):\n        instance.def_property_readonly = 3\n\n    instance.def_property = 3\n    assert instance.def_property == 3\n\n\ndef test_static_properties():\n    assert m.TestProperties.def_readonly_static == 1\n    with pytest.raises(AttributeError) as excinfo:\n        m.TestProperties.def_readonly_static = 2\n    assert ""can\'t set attribute"" in str(excinfo)\n\n    m.TestProperties.def_readwrite_static = 2\n    assert m.TestProperties.def_readwrite_static == 2\n\n    assert m.TestProperties.def_property_readonly_static == 2\n    with pytest.raises(AttributeError) as excinfo:\n        m.TestProperties.def_property_readonly_static = 3\n    assert ""can\'t set attribute"" in str(excinfo)\n\n    m.TestProperties.def_property_static = 3\n    assert m.TestProperties.def_property_static == 3\n\n    # Static property read and write via instance\n    instance = m.TestProperties()\n\n    m.TestProperties.def_readwrite_static = 0\n    assert m.TestProperties.def_readwrite_static == 0\n    assert instance.def_readwrite_static == 0\n\n    instance.def_readwrite_static = 2\n    assert m.TestProperties.def_readwrite_static == 2\n    assert instance.def_readwrite_static == 2\n\n    # It should be possible to override properties in derived classes\n    assert m.TestPropertiesOverride().def_readonly == 99\n    assert m.TestPropertiesOverride.def_readonly_static == 99\n\n\ndef test_static_cls():\n    """"""Static property getter and setters expect the type object as the their only argument""""""\n\n    instance = m.TestProperties()\n    assert m.TestProperties.static_cls is m.TestProperties\n    assert instance.static_cls is m.TestProperties\n\n    def check_self(self):\n        assert self is m.TestProperties\n\n    m.TestProperties.static_cls = check_self\n    instance.static_cls = check_self\n\n\ndef test_metaclass_override():\n    """"""Overriding pybind11\'s default metaclass changes the behavior of `static_property`""""""\n\n    assert type(m.ExampleMandA).__name__ == ""pybind11_type""\n    assert type(m.MetaclassOverride).__name__ == ""type""\n\n    assert m.MetaclassOverride.readonly == 1\n    assert type(m.MetaclassOverride.__dict__[""readonly""]).__name__ == ""pybind11_static_property""\n\n    # Regular `type` replaces the property instead of calling `__set__()`\n    m.MetaclassOverride.readonly = 2\n    assert m.MetaclassOverride.readonly == 2\n    assert isinstance(m.MetaclassOverride.__dict__[""readonly""], int)\n\n\ndef test_no_mixed_overloads():\n    from pybind11_tests import debug_enabled\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.ExampleMandA.add_mixed_overloads1()\n    assert (str(excinfo.value) ==\n            ""overloading a method with both static and instance methods is not supported; "" +\n            (""compile in debug mode for more details"" if not debug_enabled else\n             ""error while attempting to bind static method ExampleMandA.overload_mixed1""\n             ""(arg0: float) -> str"")\n            )\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.ExampleMandA.add_mixed_overloads2()\n    assert (str(excinfo.value) ==\n            ""overloading a method with both static and instance methods is not supported; "" +\n            (""compile in debug mode for more details"" if not debug_enabled else\n             ""error while attempting to bind instance method ExampleMandA.overload_mixed2""\n             ""(self: pybind11_tests.methods_and_attributes.ExampleMandA, arg0: int, arg1: int)""\n             "" -> str"")\n            )\n\n\n@pytest.mark.parametrize(""access"", [""ro"", ""rw"", ""static_ro"", ""static_rw""])\ndef test_property_return_value_policies(access):\n    if not access.startswith(""static""):\n        obj = m.TestPropRVP()\n    else:\n        obj = m.TestPropRVP\n\n    ref = getattr(obj, access + ""_ref"")\n    assert ref.value == 1\n    ref.value = 2\n    assert getattr(obj, access + ""_ref"").value == 2\n    ref.value = 1  # restore original value for static properties\n\n    copy = getattr(obj, access + ""_copy"")\n    assert copy.value == 1\n    copy.value = 2\n    assert getattr(obj, access + ""_copy"").value == 1\n\n    copy = getattr(obj, access + ""_func"")\n    assert copy.value == 1\n    copy.value = 2\n    assert getattr(obj, access + ""_func"").value == 1\n\n\ndef test_property_rvalue_policy():\n    """"""When returning an rvalue, the return value policy is automatically changed from\n    `reference(_internal)` to `move`. The following would not work otherwise.""""""\n\n    instance = m.TestPropRVP()\n    o = instance.rvalue\n    assert o.value == 1\n\n    os = m.TestPropRVP.static_rvalue\n    assert os.value == 1\n\n\n# https://bitbucket.org/pypy/pypy/issues/2447\n@pytest.unsupported_on_pypy\ndef test_dynamic_attributes():\n    instance = m.DynamicClass()\n    assert not hasattr(instance, ""foo"")\n    assert ""foo"" not in dir(instance)\n\n    # Dynamically add attribute\n    instance.foo = 42\n    assert hasattr(instance, ""foo"")\n    assert instance.foo == 42\n    assert ""foo"" in dir(instance)\n\n    # __dict__ should be accessible and replaceable\n    assert ""foo"" in instance.__dict__\n    instance.__dict__ = {""bar"": True}\n    assert not hasattr(instance, ""foo"")\n    assert hasattr(instance, ""bar"")\n\n    with pytest.raises(TypeError) as excinfo:\n        instance.__dict__ = []\n    assert str(excinfo.value) == ""__dict__ must be set to a dictionary, not a \'list\'""\n\n    cstats = ConstructorStats.get(m.DynamicClass)\n    assert cstats.alive() == 1\n    del instance\n    assert cstats.alive() == 0\n\n    # Derived classes should work as well\n    class PythonDerivedDynamicClass(m.DynamicClass):\n        pass\n\n    for cls in m.CppDerivedDynamicClass, PythonDerivedDynamicClass:\n        derived = cls()\n        derived.foobar = 100\n        assert derived.foobar == 100\n\n        assert cstats.alive() == 1\n        del derived\n        assert cstats.alive() == 0\n\n\n# https://bitbucket.org/pypy/pypy/issues/2447\n@pytest.unsupported_on_pypy\ndef test_cyclic_gc():\n    # One object references itself\n    instance = m.DynamicClass()\n    instance.circular_reference = instance\n\n    cstats = ConstructorStats.get(m.DynamicClass)\n    assert cstats.alive() == 1\n    del instance\n    assert cstats.alive() == 0\n\n    # Two object reference each other\n    i1 = m.DynamicClass()\n    i2 = m.DynamicClass()\n    i1.cycle = i2\n    i2.cycle = i1\n\n    assert cstats.alive() == 2\n    del i1, i2\n    assert cstats.alive() == 0\n\n\ndef test_noconvert_args(msg):\n    a = m.ArgInspector()\n    assert msg(a.f(""hi"")) == """"""\n        loading ArgInspector1 argument WITH conversion allowed.  Argument value = hi\n    """"""\n    assert msg(a.g(""this is a"", ""this is b"")) == """"""\n        loading ArgInspector1 argument WITHOUT conversion allowed.  Argument value = this is a\n        loading ArgInspector1 argument WITH conversion allowed.  Argument value = this is b\n        13\n        loading ArgInspector2 argument WITH conversion allowed.  Argument value = (default arg inspector 2)\n    """"""  # noqa: E501 line too long\n    assert msg(a.g(""this is a"", ""this is b"", 42)) == """"""\n        loading ArgInspector1 argument WITHOUT conversion allowed.  Argument value = this is a\n        loading ArgInspector1 argument WITH conversion allowed.  Argument value = this is b\n        42\n        loading ArgInspector2 argument WITH conversion allowed.  Argument value = (default arg inspector 2)\n    """"""  # noqa: E501 line too long\n    assert msg(a.g(""this is a"", ""this is b"", 42, ""this is d"")) == """"""\n        loading ArgInspector1 argument WITHOUT conversion allowed.  Argument value = this is a\n        loading ArgInspector1 argument WITH conversion allowed.  Argument value = this is b\n        42\n        loading ArgInspector2 argument WITH conversion allowed.  Argument value = this is d\n    """"""\n    assert (a.h(""arg 1"") ==\n            ""loading ArgInspector2 argument WITHOUT conversion allowed.  Argument value = arg 1"")\n    assert msg(m.arg_inspect_func(""A1"", ""A2"")) == """"""\n        loading ArgInspector2 argument WITH conversion allowed.  Argument value = A1\n        loading ArgInspector1 argument WITHOUT conversion allowed.  Argument value = A2\n    """"""\n\n    assert m.floats_preferred(4) == 2.0\n    assert m.floats_only(4.0) == 2.0\n    with pytest.raises(TypeError) as excinfo:\n        m.floats_only(4)\n    assert msg(excinfo.value) == """"""\n        floats_only(): incompatible function arguments. The following argument types are supported:\n            1. (f: float) -> float\n\n        Invoked with: 4\n    """"""\n\n    assert m.ints_preferred(4) == 2\n    assert m.ints_preferred(True) == 0\n    with pytest.raises(TypeError) as excinfo:\n        m.ints_preferred(4.0)\n    assert msg(excinfo.value) == """"""\n        ints_preferred(): incompatible function arguments. The following argument types are supported:\n            1. (i: int) -> int\n\n        Invoked with: 4.0\n    """"""  # noqa: E501 line too long\n\n    assert m.ints_only(4) == 2\n    with pytest.raises(TypeError) as excinfo:\n        m.ints_only(4.0)\n    assert msg(excinfo.value) == """"""\n        ints_only(): incompatible function arguments. The following argument types are supported:\n            1. (i: int) -> int\n\n        Invoked with: 4.0\n    """"""\n\n\ndef test_bad_arg_default(msg):\n    from pybind11_tests import debug_enabled\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.bad_arg_def_named()\n    assert msg(excinfo.value) == (\n        ""arg(): could not convert default argument \'a: UnregisteredType\' in function ""\n        ""\'should_fail\' into a Python object (type not registered yet?)""\n        if debug_enabled else\n        ""arg(): could not convert default argument into a Python object (type not registered ""\n        ""yet?). Compile in debug mode for more information.""\n    )\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.bad_arg_def_unnamed()\n    assert msg(excinfo.value) == (\n        ""arg(): could not convert default argument \'UnregisteredType\' in function ""\n        ""\'should_fail\' into a Python object (type not registered yet?)""\n        if debug_enabled else\n        ""arg(): could not convert default argument into a Python object (type not registered ""\n        ""yet?). Compile in debug mode for more information.""\n    )\n\n\ndef test_accepts_none(msg):\n    a = m.NoneTester()\n    assert m.no_none1(a) == 42\n    assert m.no_none2(a) == 42\n    assert m.no_none3(a) == 42\n    assert m.no_none4(a) == 42\n    assert m.no_none5(a) == 42\n    assert m.ok_none1(a) == 42\n    assert m.ok_none2(a) == 42\n    assert m.ok_none3(a) == 42\n    assert m.ok_none4(a) == 42\n    assert m.ok_none5(a) == 42\n\n    with pytest.raises(TypeError) as excinfo:\n        m.no_none1(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.no_none2(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.no_none3(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.no_none4(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n    with pytest.raises(TypeError) as excinfo:\n        m.no_none5(None)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    # The first one still raises because you can\'t pass None as a lvalue reference arg:\n    with pytest.raises(TypeError) as excinfo:\n        assert m.ok_none1(None) == -1\n    assert msg(excinfo.value) == """"""\n        ok_none1(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: m.methods_and_attributes.NoneTester) -> int\n\n        Invoked with: None\n    """"""\n\n    # The rest take the argument as pointer or holder, and accept None:\n    assert m.ok_none2(None) == -1\n    assert m.ok_none3(None) == -1\n    assert m.ok_none4(None) == -1\n    assert m.ok_none5(None) == -1\n\n\ndef test_str_issue(msg):\n    """"""#283: __str__ called on uninitialized instance when constructor arguments invalid""""""\n\n    assert str(m.StrIssue(3)) == ""StrIssue[3]""\n\n    with pytest.raises(TypeError) as excinfo:\n        str(m.StrIssue(""no"", ""such"", ""constructor""))\n    assert msg(excinfo.value) == """"""\n        __init__(): incompatible constructor arguments. The following argument types are supported:\n            1. m.methods_and_attributes.StrIssue(arg0: int)\n            2. m.methods_and_attributes.StrIssue()\n\n        Invoked with: \'no\', \'such\', \'constructor\'\n    """"""\n\n\ndef test_unregistered_base_implementations():\n    a = m.RegisteredDerived()\n    a.do_nothing()\n    assert a.rw_value == 42\n    assert a.ro_value == 1.25\n    a.rw_value += 5\n    assert a.sum() == 48.25\n    a.increase_value()\n    assert a.rw_value == 48\n    assert a.ro_value == 1.5\n    assert a.sum() == 49.5\n    assert a.rw_value_prop == 48\n    a.rw_value_prop += 1\n    assert a.rw_value_prop == 49\n    a.increase_value()\n    assert a.ro_value_prop == 1.75\n\n\ndef test_custom_caster_destruction():\n    """"""Tests that returning a pointer to a type that gets converted with a custom type caster gets\n    destroyed when the function has py::return_value_policy::take_ownership policy applied.""""""\n\n    cstats = m.destruction_tester_cstats()\n    # This one *doesn\'t* have take_ownership: the pointer should be used but not destroyed:\n    z = m.custom_caster_no_destroy()\n    assert cstats.alive() == 1 and cstats.default_constructions == 1\n    assert z\n\n    # take_ownership applied: this constructs a new object, casts it, then destroys it:\n    z = m.custom_caster_destroy()\n    assert z\n    assert cstats.default_constructions == 2\n\n    # Same, but with a const pointer return (which should *not* inhibit destruction):\n    z = m.custom_caster_destroy_const()\n    assert z\n    assert cstats.default_constructions == 3\n\n    # Make sure we still only have the original object (from ..._no_destroy()) alive:\n    assert cstats.alive() == 1\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_modules.py,0,"b'from pybind11_tests import modules as m\nfrom pybind11_tests.modules import subsubmodule as ms\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_nested_modules():\n    import pybind11_tests\n    assert pybind11_tests.__name__ == ""pybind11_tests""\n    assert pybind11_tests.modules.__name__ == ""pybind11_tests.modules""\n    assert pybind11_tests.modules.subsubmodule.__name__ == ""pybind11_tests.modules.subsubmodule""\n    assert m.__name__ == ""pybind11_tests.modules""\n    assert ms.__name__ == ""pybind11_tests.modules.subsubmodule""\n\n    assert ms.submodule_func() == ""submodule_func()""\n\n\ndef test_reference_internal():\n    b = ms.B()\n    assert str(b.get_a1()) == ""A[1]""\n    assert str(b.a1) == ""A[1]""\n    assert str(b.get_a2()) == ""A[2]""\n    assert str(b.a2) == ""A[2]""\n\n    b.a1 = ms.A(42)\n    b.a2 = ms.A(43)\n    assert str(b.get_a1()) == ""A[42]""\n    assert str(b.a1) == ""A[42]""\n    assert str(b.get_a2()) == ""A[43]""\n    assert str(b.a2) == ""A[43]""\n\n    astats, bstats = ConstructorStats.get(ms.A), ConstructorStats.get(ms.B)\n    assert astats.alive() == 2\n    assert bstats.alive() == 1\n    del b\n    assert astats.alive() == 0\n    assert bstats.alive() == 0\n    assert astats.values() == [\'1\', \'2\', \'42\', \'43\']\n    assert bstats.values() == []\n    assert astats.default_constructions == 0\n    assert bstats.default_constructions == 1\n    assert astats.copy_constructions == 0\n    assert bstats.copy_constructions == 0\n    # assert astats.move_constructions >= 0  # Don\'t invoke any\n    # assert bstats.move_constructions >= 0  # Don\'t invoke any\n    assert astats.copy_assignments == 2\n    assert bstats.copy_assignments == 0\n    assert astats.move_assignments == 0\n    assert bstats.move_assignments == 0\n\n\ndef test_importing():\n    from pybind11_tests.modules import OD\n    from collections import OrderedDict\n\n    assert OD is OrderedDict\n    assert str(OD([(1, \'a\'), (2, \'b\')])) == ""OrderedDict([(1, \'a\'), (2, \'b\')])""\n\n\ndef test_pydoc():\n    """"""Pydoc needs to be able to provide help() for everything inside a pybind11 module""""""\n    import pybind11_tests\n    import pydoc\n\n    assert pybind11_tests.__name__ == ""pybind11_tests""\n    assert pybind11_tests.__doc__ == ""pybind11 test module""\n    assert pydoc.text.docmodule(pybind11_tests)\n\n\ndef test_duplicate_registration():\n    """"""Registering two things with the same name""""""\n\n    assert m.duplicate_registration() == []\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_multiple_inheritance.py,0,"b'import pytest\nfrom pybind11_tests import ConstructorStats\nfrom pybind11_tests import multiple_inheritance as m\n\n\ndef test_multiple_inheritance_cpp():\n    mt = m.MIType(3, 4)\n\n    assert mt.foo() == 3\n    assert mt.bar() == 4\n\n\ndef test_multiple_inheritance_mix1():\n    class Base1:\n        def __init__(self, i):\n            self.i = i\n\n        def foo(self):\n            return self.i\n\n    class MITypePy(Base1, m.Base2):\n        def __init__(self, i, j):\n            Base1.__init__(self, i)\n            m.Base2.__init__(self, j)\n\n    mt = MITypePy(3, 4)\n\n    assert mt.foo() == 3\n    assert mt.bar() == 4\n\n\ndef test_multiple_inheritance_mix2():\n\n    class Base2:\n        def __init__(self, i):\n            self.i = i\n\n        def bar(self):\n            return self.i\n\n    class MITypePy(m.Base1, Base2):\n        def __init__(self, i, j):\n            m.Base1.__init__(self, i)\n            Base2.__init__(self, j)\n\n    mt = MITypePy(3, 4)\n\n    assert mt.foo() == 3\n    assert mt.bar() == 4\n\n\ndef test_multiple_inheritance_python():\n\n    class MI1(m.Base1, m.Base2):\n        def __init__(self, i, j):\n            m.Base1.__init__(self, i)\n            m.Base2.__init__(self, j)\n\n    class B1(object):\n        def v(self):\n            return 1\n\n    class MI2(B1, m.Base1, m.Base2):\n        def __init__(self, i, j):\n            B1.__init__(self)\n            m.Base1.__init__(self, i)\n            m.Base2.__init__(self, j)\n\n    class MI3(MI2):\n        def __init__(self, i, j):\n            MI2.__init__(self, i, j)\n\n    class MI4(MI3, m.Base2):\n        def __init__(self, i, j):\n            MI3.__init__(self, i, j)\n            # This should be ignored (Base2 is already initialized via MI2):\n            m.Base2.__init__(self, i + 100)\n\n    class MI5(m.Base2, B1, m.Base1):\n        def __init__(self, i, j):\n            B1.__init__(self)\n            m.Base1.__init__(self, i)\n            m.Base2.__init__(self, j)\n\n    class MI6(m.Base2, B1):\n        def __init__(self, i):\n            m.Base2.__init__(self, i)\n            B1.__init__(self)\n\n    class B2(B1):\n        def v(self):\n            return 2\n\n    class B3(object):\n        def v(self):\n            return 3\n\n    class B4(B3, B2):\n        def v(self):\n            return 4\n\n    class MI7(B4, MI6):\n        def __init__(self, i):\n            B4.__init__(self)\n            MI6.__init__(self, i)\n\n    class MI8(MI6, B3):\n        def __init__(self, i):\n            MI6.__init__(self, i)\n            B3.__init__(self)\n\n    class MI8b(B3, MI6):\n        def __init__(self, i):\n            B3.__init__(self)\n            MI6.__init__(self, i)\n\n    mi1 = MI1(1, 2)\n    assert mi1.foo() == 1\n    assert mi1.bar() == 2\n\n    mi2 = MI2(3, 4)\n    assert mi2.v() == 1\n    assert mi2.foo() == 3\n    assert mi2.bar() == 4\n\n    mi3 = MI3(5, 6)\n    assert mi3.v() == 1\n    assert mi3.foo() == 5\n    assert mi3.bar() == 6\n\n    mi4 = MI4(7, 8)\n    assert mi4.v() == 1\n    assert mi4.foo() == 7\n    assert mi4.bar() == 8\n\n    mi5 = MI5(10, 11)\n    assert mi5.v() == 1\n    assert mi5.foo() == 10\n    assert mi5.bar() == 11\n\n    mi6 = MI6(12)\n    assert mi6.v() == 1\n    assert mi6.bar() == 12\n\n    mi7 = MI7(13)\n    assert mi7.v() == 4\n    assert mi7.bar() == 13\n\n    mi8 = MI8(14)\n    assert mi8.v() == 1\n    assert mi8.bar() == 14\n\n    mi8b = MI8b(15)\n    assert mi8b.v() == 3\n    assert mi8b.bar() == 15\n\n\ndef test_multiple_inheritance_python_many_bases():\n\n    class MIMany14(m.BaseN1, m.BaseN2, m.BaseN3, m.BaseN4):\n        def __init__(self):\n            m.BaseN1.__init__(self, 1)\n            m.BaseN2.__init__(self, 2)\n            m.BaseN3.__init__(self, 3)\n            m.BaseN4.__init__(self, 4)\n\n    class MIMany58(m.BaseN5, m.BaseN6, m.BaseN7, m.BaseN8):\n        def __init__(self):\n            m.BaseN5.__init__(self, 5)\n            m.BaseN6.__init__(self, 6)\n            m.BaseN7.__init__(self, 7)\n            m.BaseN8.__init__(self, 8)\n\n    class MIMany916(m.BaseN9, m.BaseN10, m.BaseN11, m.BaseN12, m.BaseN13, m.BaseN14, m.BaseN15,\n                    m.BaseN16):\n        def __init__(self):\n            m.BaseN9.__init__(self, 9)\n            m.BaseN10.__init__(self, 10)\n            m.BaseN11.__init__(self, 11)\n            m.BaseN12.__init__(self, 12)\n            m.BaseN13.__init__(self, 13)\n            m.BaseN14.__init__(self, 14)\n            m.BaseN15.__init__(self, 15)\n            m.BaseN16.__init__(self, 16)\n\n    class MIMany19(MIMany14, MIMany58, m.BaseN9):\n        def __init__(self):\n            MIMany14.__init__(self)\n            MIMany58.__init__(self)\n            m.BaseN9.__init__(self, 9)\n\n    class MIMany117(MIMany14, MIMany58, MIMany916, m.BaseN17):\n        def __init__(self):\n            MIMany14.__init__(self)\n            MIMany58.__init__(self)\n            MIMany916.__init__(self)\n            m.BaseN17.__init__(self, 17)\n\n    # Inherits from 4 registered C++ classes: can fit in one pointer on any modern arch:\n    a = MIMany14()\n    for i in range(1, 4):\n        assert getattr(a, ""f"" + str(i))() == 2 * i\n\n    # Inherits from 8: requires 1/2 pointers worth of holder flags on 32/64-bit arch:\n    b = MIMany916()\n    for i in range(9, 16):\n        assert getattr(b, ""f"" + str(i))() == 2 * i\n\n    # Inherits from 9: requires >= 2 pointers worth of holder flags\n    c = MIMany19()\n    for i in range(1, 9):\n        assert getattr(c, ""f"" + str(i))() == 2 * i\n\n    # Inherits from 17: requires >= 3 pointers worth of holder flags\n    d = MIMany117()\n    for i in range(1, 17):\n        assert getattr(d, ""f"" + str(i))() == 2 * i\n\n\ndef test_multiple_inheritance_virtbase():\n\n    class MITypePy(m.Base12a):\n        def __init__(self, i, j):\n            m.Base12a.__init__(self, i, j)\n\n    mt = MITypePy(3, 4)\n    assert mt.bar() == 4\n    assert m.bar_base2a(mt) == 4\n    assert m.bar_base2a_sharedptr(mt) == 4\n\n\ndef test_mi_static_properties():\n    """"""Mixing bases with and without static properties should be possible\n     and the result should be independent of base definition order""""""\n\n    for d in (m.VanillaStaticMix1(), m.VanillaStaticMix2()):\n        assert d.vanilla() == ""Vanilla""\n        assert d.static_func1() == ""WithStatic1""\n        assert d.static_func2() == ""WithStatic2""\n        assert d.static_func() == d.__class__.__name__\n\n        m.WithStatic1.static_value1 = 1\n        m.WithStatic2.static_value2 = 2\n        assert d.static_value1 == 1\n        assert d.static_value2 == 2\n        assert d.static_value == 12\n\n        d.static_value1 = 0\n        assert d.static_value1 == 0\n        d.static_value2 = 0\n        assert d.static_value2 == 0\n        d.static_value = 0\n        assert d.static_value == 0\n\n\n@pytest.unsupported_on_pypy\ndef test_mi_dynamic_attributes():\n    """"""Mixing bases with and without dynamic attribute support""""""\n\n    for d in (m.VanillaDictMix1(), m.VanillaDictMix2()):\n        d.dynamic = 1\n        assert d.dynamic == 1\n\n\ndef test_mi_unaligned_base():\n    """"""Returning an offset (non-first MI) base class pointer should recognize the instance""""""\n\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    c = m.I801C()\n    d = m.I801D()\n    # + 4 below because we have the two instances, and each instance has offset base I801B2\n    assert ConstructorStats.detail_reg_inst() == n_inst + 4\n    b1c = m.i801b1_c(c)\n    assert b1c is c\n    b2c = m.i801b2_c(c)\n    assert b2c is c\n    b1d = m.i801b1_d(d)\n    assert b1d is d\n    b2d = m.i801b2_d(d)\n    assert b2d is d\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 4  # no extra instances\n    del c, b1c, b2c\n    assert ConstructorStats.detail_reg_inst() == n_inst + 2\n    del d, b1d, b2d\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n\ndef test_mi_base_return():\n    """"""Tests returning an offset (non-first MI) base class pointer to a derived instance""""""\n\n    n_inst = ConstructorStats.detail_reg_inst()\n\n    c1 = m.i801c_b1()\n    assert type(c1) is m.I801C\n    assert c1.a == 1\n    assert c1.b == 2\n\n    d1 = m.i801d_b1()\n    assert type(d1) is m.I801D\n    assert d1.a == 1\n    assert d1.b == 2\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 4\n\n    c2 = m.i801c_b2()\n    assert type(c2) is m.I801C\n    assert c2.a == 1\n    assert c2.b == 2\n\n    d2 = m.i801d_b2()\n    assert type(d2) is m.I801D\n    assert d2.a == 1\n    assert d2.b == 2\n\n    assert ConstructorStats.detail_reg_inst() == n_inst + 8\n\n    del c2\n    assert ConstructorStats.detail_reg_inst() == n_inst + 6\n    del c1, d1, d2\n    assert ConstructorStats.detail_reg_inst() == n_inst\n\n    # Returning an unregistered derived type with a registered base; we won\'t\n    # pick up the derived type, obviously, but should still work (as an object\n    # of whatever type was returned).\n    e1 = m.i801e_c()\n    assert type(e1) is m.I801C\n    assert e1.a == 1\n    assert e1.b == 2\n\n    e2 = m.i801e_b2()\n    assert type(e2) is m.I801B2\n    assert e2.b == 2\n\n\ndef test_diamond_inheritance():\n    """"""Tests that diamond inheritance works as expected (issue #959)""""""\n\n    # Issue #959: this shouldn\'t segfault:\n    d = m.D()\n\n    # Make sure all the various distinct pointers are all recognized as registered instances:\n    assert d is d.c0()\n    assert d is d.c1()\n    assert d is d.b()\n    assert d is d.c0().b()\n    assert d is d.c1().b()\n    assert d is d.c0().c1().b().c0().b()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_numpy_array.py,0,"b'import pytest\nfrom pybind11_tests import numpy_array as m\n\npytestmark = pytest.requires_numpy\n\nwith pytest.suppress(ImportError):\n    import numpy as np\n\n\n@pytest.fixture(scope=\'function\')\ndef arr():\n    return np.array([[1, 2, 3], [4, 5, 6]], \'=u2\')\n\n\ndef test_array_attributes():\n    a = np.array(0, \'f8\')\n    assert m.ndim(a) == 0\n    assert all(m.shape(a) == [])\n    assert all(m.strides(a) == [])\n    with pytest.raises(IndexError) as excinfo:\n        m.shape(a, 0)\n    assert str(excinfo.value) == \'invalid axis: 0 (ndim = 0)\'\n    with pytest.raises(IndexError) as excinfo:\n        m.strides(a, 0)\n    assert str(excinfo.value) == \'invalid axis: 0 (ndim = 0)\'\n    assert m.writeable(a)\n    assert m.size(a) == 1\n    assert m.itemsize(a) == 8\n    assert m.nbytes(a) == 8\n    assert m.owndata(a)\n\n    a = np.array([[1, 2, 3], [4, 5, 6]], \'u2\').view()\n    a.flags.writeable = False\n    assert m.ndim(a) == 2\n    assert all(m.shape(a) == [2, 3])\n    assert m.shape(a, 0) == 2\n    assert m.shape(a, 1) == 3\n    assert all(m.strides(a) == [6, 2])\n    assert m.strides(a, 0) == 6\n    assert m.strides(a, 1) == 2\n    with pytest.raises(IndexError) as excinfo:\n        m.shape(a, 2)\n    assert str(excinfo.value) == \'invalid axis: 2 (ndim = 2)\'\n    with pytest.raises(IndexError) as excinfo:\n        m.strides(a, 2)\n    assert str(excinfo.value) == \'invalid axis: 2 (ndim = 2)\'\n    assert not m.writeable(a)\n    assert m.size(a) == 6\n    assert m.itemsize(a) == 2\n    assert m.nbytes(a) == 12\n    assert not m.owndata(a)\n\n\n@pytest.mark.parametrize(\'args, ret\', [([], 0), ([0], 0), ([1], 3), ([0, 1], 1), ([1, 2], 5)])\ndef test_index_offset(arr, args, ret):\n    assert m.index_at(arr, *args) == ret\n    assert m.index_at_t(arr, *args) == ret\n    assert m.offset_at(arr, *args) == ret * arr.dtype.itemsize\n    assert m.offset_at_t(arr, *args) == ret * arr.dtype.itemsize\n\n\ndef test_dim_check_fail(arr):\n    for func in (m.index_at, m.index_at_t, m.offset_at, m.offset_at_t, m.data, m.data_t,\n                 m.mutate_data, m.mutate_data_t):\n        with pytest.raises(IndexError) as excinfo:\n            func(arr, 1, 2, 3)\n        assert str(excinfo.value) == \'too many indices for an array: 3 (ndim = 2)\'\n\n\n@pytest.mark.parametrize(\'args, ret\',\n                         [([], [1, 2, 3, 4, 5, 6]),\n                          ([1], [4, 5, 6]),\n                          ([0, 1], [2, 3, 4, 5, 6]),\n                          ([1, 2], [6])])\ndef test_data(arr, args, ret):\n    from sys import byteorder\n    assert all(m.data_t(arr, *args) == ret)\n    assert all(m.data(arr, *args)[(0 if byteorder == \'little\' else 1)::2] == ret)\n    assert all(m.data(arr, *args)[(1 if byteorder == \'little\' else 0)::2] == 0)\n\n\n@pytest.mark.parametrize(\'dim\', [0, 1, 3])\ndef test_at_fail(arr, dim):\n    for func in m.at_t, m.mutate_at_t:\n        with pytest.raises(IndexError) as excinfo:\n            func(arr, *([0] * dim))\n        assert str(excinfo.value) == \'index dimension mismatch: {} (ndim = 2)\'.format(dim)\n\n\ndef test_at(arr):\n    assert m.at_t(arr, 0, 2) == 3\n    assert m.at_t(arr, 1, 0) == 4\n\n    assert all(m.mutate_at_t(arr, 0, 2).ravel() == [1, 2, 4, 4, 5, 6])\n    assert all(m.mutate_at_t(arr, 1, 0).ravel() == [1, 2, 4, 5, 5, 6])\n\n\ndef test_mutate_readonly(arr):\n    arr.flags.writeable = False\n    for func, args in (m.mutate_data, ()), (m.mutate_data_t, ()), (m.mutate_at_t, (0, 0)):\n        with pytest.raises(ValueError) as excinfo:\n            func(arr, *args)\n        assert str(excinfo.value) == \'array is not writeable\'\n\n\ndef test_mutate_data(arr):\n    assert all(m.mutate_data(arr).ravel() == [2, 4, 6, 8, 10, 12])\n    assert all(m.mutate_data(arr).ravel() == [4, 8, 12, 16, 20, 24])\n    assert all(m.mutate_data(arr, 1).ravel() == [4, 8, 12, 32, 40, 48])\n    assert all(m.mutate_data(arr, 0, 1).ravel() == [4, 16, 24, 64, 80, 96])\n    assert all(m.mutate_data(arr, 1, 2).ravel() == [4, 16, 24, 64, 80, 192])\n\n    assert all(m.mutate_data_t(arr).ravel() == [5, 17, 25, 65, 81, 193])\n    assert all(m.mutate_data_t(arr).ravel() == [6, 18, 26, 66, 82, 194])\n    assert all(m.mutate_data_t(arr, 1).ravel() == [6, 18, 26, 67, 83, 195])\n    assert all(m.mutate_data_t(arr, 0, 1).ravel() == [6, 19, 27, 68, 84, 196])\n    assert all(m.mutate_data_t(arr, 1, 2).ravel() == [6, 19, 27, 68, 84, 197])\n\n\ndef test_bounds_check(arr):\n    for func in (m.index_at, m.index_at_t, m.data, m.data_t,\n                 m.mutate_data, m.mutate_data_t, m.at_t, m.mutate_at_t):\n        with pytest.raises(IndexError) as excinfo:\n            func(arr, 2, 0)\n        assert str(excinfo.value) == \'index 2 is out of bounds for axis 0 with size 2\'\n        with pytest.raises(IndexError) as excinfo:\n            func(arr, 0, 4)\n        assert str(excinfo.value) == \'index 4 is out of bounds for axis 1 with size 3\'\n\n\ndef test_make_c_f_array():\n    assert m.make_c_array().flags.c_contiguous\n    assert not m.make_c_array().flags.f_contiguous\n    assert m.make_f_array().flags.f_contiguous\n    assert not m.make_f_array().flags.c_contiguous\n\n\ndef test_make_empty_shaped_array():\n    m.make_empty_shaped_array()\n\n\ndef test_wrap():\n    def assert_references(a, b, base=None):\n        from distutils.version import LooseVersion\n        if base is None:\n            base = a\n        assert a is not b\n        assert a.__array_interface__[\'data\'][0] == b.__array_interface__[\'data\'][0]\n        assert a.shape == b.shape\n        assert a.strides == b.strides\n        assert a.flags.c_contiguous == b.flags.c_contiguous\n        assert a.flags.f_contiguous == b.flags.f_contiguous\n        assert a.flags.writeable == b.flags.writeable\n        assert a.flags.aligned == b.flags.aligned\n        if LooseVersion(np.__version__) >= LooseVersion(""1.14.0""):\n            assert a.flags.writebackifcopy == b.flags.writebackifcopy\n        else:\n            assert a.flags.updateifcopy == b.flags.updateifcopy\n        assert np.all(a == b)\n        assert not b.flags.owndata\n        assert b.base is base\n        if a.flags.writeable and a.ndim == 2:\n            a[0, 0] = 1234\n            assert b[0, 0] == 1234\n\n    a1 = np.array([1, 2], dtype=np.int16)\n    assert a1.flags.owndata and a1.base is None\n    a2 = m.wrap(a1)\n    assert_references(a1, a2)\n\n    a1 = np.array([[1, 2], [3, 4]], dtype=np.float32, order=\'F\')\n    assert a1.flags.owndata and a1.base is None\n    a2 = m.wrap(a1)\n    assert_references(a1, a2)\n\n    a1 = np.array([[1, 2], [3, 4]], dtype=np.float32, order=\'C\')\n    a1.flags.writeable = False\n    a2 = m.wrap(a1)\n    assert_references(a1, a2)\n\n    a1 = np.random.random((4, 4, 4))\n    a2 = m.wrap(a1)\n    assert_references(a1, a2)\n\n    a1t = a1.transpose()\n    a2 = m.wrap(a1t)\n    assert_references(a1t, a2, a1)\n\n    a1d = a1.diagonal()\n    a2 = m.wrap(a1d)\n    assert_references(a1d, a2, a1)\n\n    a1m = a1[::-1, ::-1, ::-1]\n    a2 = m.wrap(a1m)\n    assert_references(a1m, a2, a1)\n\n\ndef test_numpy_view(capture):\n    with capture:\n        ac = m.ArrayClass()\n        ac_view_1 = ac.numpy_view()\n        ac_view_2 = ac.numpy_view()\n        assert np.all(ac_view_1 == np.array([1, 2], dtype=np.int32))\n        del ac\n        pytest.gc_collect()\n    assert capture == """"""\n        ArrayClass()\n        ArrayClass::numpy_view()\n        ArrayClass::numpy_view()\n    """"""\n    ac_view_1[0] = 4\n    ac_view_1[1] = 3\n    assert ac_view_2[0] == 4\n    assert ac_view_2[1] == 3\n    with capture:\n        del ac_view_1\n        del ac_view_2\n        pytest.gc_collect()\n        pytest.gc_collect()\n    assert capture == """"""\n        ~ArrayClass()\n    """"""\n\n\n@pytest.unsupported_on_pypy\ndef test_cast_numpy_int64_to_uint64():\n    m.function_taking_uint64(123)\n    m.function_taking_uint64(np.uint64(123))\n\n\ndef test_isinstance():\n    assert m.isinstance_untyped(np.array([1, 2, 3]), ""not an array"")\n    assert m.isinstance_typed(np.array([1.0, 2.0, 3.0]))\n\n\ndef test_constructors():\n    defaults = m.default_constructors()\n    for a in defaults.values():\n        assert a.size == 0\n    assert defaults[""array""].dtype == np.array([]).dtype\n    assert defaults[""array_t<int32>""].dtype == np.int32\n    assert defaults[""array_t<double>""].dtype == np.float64\n\n    results = m.converting_constructors([1, 2, 3])\n    for a in results.values():\n        np.testing.assert_array_equal(a, [1, 2, 3])\n    assert results[""array""].dtype == np.int_\n    assert results[""array_t<int32>""].dtype == np.int32\n    assert results[""array_t<double>""].dtype == np.float64\n\n\ndef test_overload_resolution(msg):\n    # Exact overload matches:\n    assert m.overloaded(np.array([1], dtype=\'float64\')) == \'double\'\n    assert m.overloaded(np.array([1], dtype=\'float32\')) == \'float\'\n    assert m.overloaded(np.array([1], dtype=\'ushort\')) == \'unsigned short\'\n    assert m.overloaded(np.array([1], dtype=\'intc\')) == \'int\'\n    assert m.overloaded(np.array([1], dtype=\'longlong\')) == \'long long\'\n    assert m.overloaded(np.array([1], dtype=\'complex\')) == \'double complex\'\n    assert m.overloaded(np.array([1], dtype=\'csingle\')) == \'float complex\'\n\n    # No exact match, should call first convertible version:\n    assert m.overloaded(np.array([1], dtype=\'uint8\')) == \'double\'\n\n    with pytest.raises(TypeError) as excinfo:\n        m.overloaded(""not an array"")\n    assert msg(excinfo.value) == """"""\n        overloaded(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: numpy.ndarray[float64]) -> str\n            2. (arg0: numpy.ndarray[float32]) -> str\n            3. (arg0: numpy.ndarray[int32]) -> str\n            4. (arg0: numpy.ndarray[uint16]) -> str\n            5. (arg0: numpy.ndarray[int64]) -> str\n            6. (arg0: numpy.ndarray[complex128]) -> str\n            7. (arg0: numpy.ndarray[complex64]) -> str\n\n        Invoked with: \'not an array\'\n    """"""\n\n    assert m.overloaded2(np.array([1], dtype=\'float64\')) == \'double\'\n    assert m.overloaded2(np.array([1], dtype=\'float32\')) == \'float\'\n    assert m.overloaded2(np.array([1], dtype=\'complex64\')) == \'float complex\'\n    assert m.overloaded2(np.array([1], dtype=\'complex128\')) == \'double complex\'\n    assert m.overloaded2(np.array([1], dtype=\'float32\')) == \'float\'\n\n    assert m.overloaded3(np.array([1], dtype=\'float64\')) == \'double\'\n    assert m.overloaded3(np.array([1], dtype=\'intc\')) == \'int\'\n    expected_exc = """"""\n        overloaded3(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: numpy.ndarray[int32]) -> str\n            2. (arg0: numpy.ndarray[float64]) -> str\n\n        Invoked with: """"""\n\n    with pytest.raises(TypeError) as excinfo:\n        m.overloaded3(np.array([1], dtype=\'uintc\'))\n    assert msg(excinfo.value) == expected_exc + repr(np.array([1], dtype=\'uint32\'))\n    with pytest.raises(TypeError) as excinfo:\n        m.overloaded3(np.array([1], dtype=\'float32\'))\n    assert msg(excinfo.value) == expected_exc + repr(np.array([1.], dtype=\'float32\'))\n    with pytest.raises(TypeError) as excinfo:\n        m.overloaded3(np.array([1], dtype=\'complex\'))\n    assert msg(excinfo.value) == expected_exc + repr(np.array([1. + 0.j]))\n\n    # Exact matches:\n    assert m.overloaded4(np.array([1], dtype=\'double\')) == \'double\'\n    assert m.overloaded4(np.array([1], dtype=\'longlong\')) == \'long long\'\n    # Non-exact matches requiring conversion.  Since float to integer isn\'t a\n    # save conversion, it should go to the double overload, but short can go to\n    # either (and so should end up on the first-registered, the long long).\n    assert m.overloaded4(np.array([1], dtype=\'float32\')) == \'double\'\n    assert m.overloaded4(np.array([1], dtype=\'short\')) == \'long long\'\n\n    assert m.overloaded5(np.array([1], dtype=\'double\')) == \'double\'\n    assert m.overloaded5(np.array([1], dtype=\'uintc\')) == \'unsigned int\'\n    assert m.overloaded5(np.array([1], dtype=\'float32\')) == \'unsigned int\'\n\n\ndef test_greedy_string_overload():\n    """"""Tests fix for #685 - ndarray shouldn\'t go to std::string overload""""""\n\n    assert m.issue685(""abc"") == ""string""\n    assert m.issue685(np.array([97, 98, 99], dtype=\'b\')) == ""array""\n    assert m.issue685(123) == ""other""\n\n\ndef test_array_unchecked_fixed_dims(msg):\n    z1 = np.array([[1, 2], [3, 4]], dtype=\'float64\')\n    m.proxy_add2(z1, 10)\n    assert np.all(z1 == [[11, 12], [13, 14]])\n\n    with pytest.raises(ValueError) as excinfo:\n        m.proxy_add2(np.array([1., 2, 3]), 5.0)\n    assert msg(excinfo.value) == ""array has incorrect number of dimensions: 1; expected 2""\n\n    expect_c = np.ndarray(shape=(3, 3, 3), buffer=np.array(range(3, 30)), dtype=\'int\')\n    assert np.all(m.proxy_init3(3.0) == expect_c)\n    expect_f = np.transpose(expect_c)\n    assert np.all(m.proxy_init3F(3.0) == expect_f)\n\n    assert m.proxy_squared_L2_norm(np.array(range(6))) == 55\n    assert m.proxy_squared_L2_norm(np.array(range(6), dtype=""float64"")) == 55\n\n    assert m.proxy_auxiliaries2(z1) == [11, 11, True, 2, 8, 2, 2, 4, 32]\n    assert m.proxy_auxiliaries2(z1) == m.array_auxiliaries2(z1)\n\n\ndef test_array_unchecked_dyn_dims(msg):\n    z1 = np.array([[1, 2], [3, 4]], dtype=\'float64\')\n    m.proxy_add2_dyn(z1, 10)\n    assert np.all(z1 == [[11, 12], [13, 14]])\n\n    expect_c = np.ndarray(shape=(3, 3, 3), buffer=np.array(range(3, 30)), dtype=\'int\')\n    assert np.all(m.proxy_init3_dyn(3.0) == expect_c)\n\n    assert m.proxy_auxiliaries2_dyn(z1) == [11, 11, True, 2, 8, 2, 2, 4, 32]\n    assert m.proxy_auxiliaries2_dyn(z1) == m.array_auxiliaries2(z1)\n\n\ndef test_array_failure():\n    with pytest.raises(ValueError) as excinfo:\n        m.array_fail_test()\n    assert str(excinfo.value) == \'cannot create a pybind11::array from a nullptr\'\n\n    with pytest.raises(ValueError) as excinfo:\n        m.array_t_fail_test()\n    assert str(excinfo.value) == \'cannot create a pybind11::array_t from a nullptr\'\n\n    with pytest.raises(ValueError) as excinfo:\n        m.array_fail_test_negative_size()\n    assert str(excinfo.value) == \'negative dimensions are not allowed\'\n\n\ndef test_initializer_list():\n    assert m.array_initializer_list1().shape == (1,)\n    assert m.array_initializer_list2().shape == (1, 2)\n    assert m.array_initializer_list3().shape == (1, 2, 3)\n    assert m.array_initializer_list4().shape == (1, 2, 3, 4)\n\n\ndef test_array_resize(msg):\n    a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=\'float64\')\n    m.array_reshape2(a)\n    assert(a.size == 9)\n    assert(np.all(a == [[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n\n    # total size change should succced with refcheck off\n    m.array_resize3(a, 4, False)\n    assert(a.size == 64)\n    # ... and fail with refcheck on\n    try:\n        m.array_resize3(a, 3, True)\n    except ValueError as e:\n        assert(str(e).startswith(""cannot resize an array""))\n    # transposed array doesn\'t own data\n    b = a.transpose()\n    try:\n        m.array_resize3(b, 3, False)\n    except ValueError as e:\n        assert(str(e).startswith(""cannot resize this array: it does not own its data""))\n    # ... but reshape should be fine\n    m.array_reshape2(b)\n    assert(b.shape == (8, 8))\n\n\n@pytest.unsupported_on_pypy\ndef test_array_create_and_resize(msg):\n    a = m.create_and_resize(2)\n    assert(a.size == 4)\n    assert(np.all(a == 42.))\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_numpy_dtypes.py,0,"b'import re\nimport pytest\nfrom pybind11_tests import numpy_dtypes as m\n\npytestmark = pytest.requires_numpy\n\nwith pytest.suppress(ImportError):\n    import numpy as np\n\n\n@pytest.fixture(scope=\'module\')\ndef simple_dtype():\n    ld = np.dtype(\'longdouble\')\n    return np.dtype({\'names\': [\'bool_\', \'uint_\', \'float_\', \'ldbl_\'],\n                     \'formats\': [\'?\', \'u4\', \'f4\', \'f{}\'.format(ld.itemsize)],\n                     \'offsets\': [0, 4, 8, (16 if ld.alignment > 4 else 12)]})\n\n\n@pytest.fixture(scope=\'module\')\ndef packed_dtype():\n    return np.dtype([(\'bool_\', \'?\'), (\'uint_\', \'u4\'), (\'float_\', \'f4\'), (\'ldbl_\', \'g\')])\n\n\ndef dt_fmt():\n    from sys import byteorder\n    e = \'<\' if byteorder == \'little\' else \'>\'\n    return (""{{\'names\':[\'bool_\',\'uint_\',\'float_\',\'ldbl_\'],""\n            "" \'formats\':[\'?\',\'"" + e + ""u4\',\'"" + e + ""f4\',\'"" + e + ""f{}\'],""\n            "" \'offsets\':[0,4,8,{}], \'itemsize\':{}}}"")\n\n\ndef simple_dtype_fmt():\n    ld = np.dtype(\'longdouble\')\n    simple_ld_off = 12 + 4 * (ld.alignment > 4)\n    return dt_fmt().format(ld.itemsize, simple_ld_off, simple_ld_off + ld.itemsize)\n\n\ndef packed_dtype_fmt():\n    from sys import byteorder\n    return ""[(\'bool_\', \'?\'), (\'uint_\', \'{e}u4\'), (\'float_\', \'{e}f4\'), (\'ldbl_\', \'{e}f{}\')]"".format(\n        np.dtype(\'longdouble\').itemsize, e=\'<\' if byteorder == \'little\' else \'>\')\n\n\ndef partial_ld_offset():\n    return 12 + 4 * (np.dtype(\'uint64\').alignment > 4) + 8 + 8 * (\n        np.dtype(\'longdouble\').alignment > 8)\n\n\ndef partial_dtype_fmt():\n    ld = np.dtype(\'longdouble\')\n    partial_ld_off = partial_ld_offset()\n    return dt_fmt().format(ld.itemsize, partial_ld_off, partial_ld_off + ld.itemsize)\n\n\ndef partial_nested_fmt():\n    ld = np.dtype(\'longdouble\')\n    partial_nested_off = 8 + 8 * (ld.alignment > 8)\n    partial_ld_off = partial_ld_offset()\n    partial_nested_size = partial_nested_off * 2 + partial_ld_off + ld.itemsize\n    return ""{{\'names\':[\'a\'], \'formats\':[{}], \'offsets\':[{}], \'itemsize\':{}}}"".format(\n        partial_dtype_fmt(), partial_nested_off, partial_nested_size)\n\n\ndef assert_equal(actual, expected_data, expected_dtype):\n    np.testing.assert_equal(actual, np.array(expected_data, dtype=expected_dtype))\n\n\ndef test_format_descriptors():\n    with pytest.raises(RuntimeError) as excinfo:\n        m.get_format_unbound()\n    assert re.match(\'^NumPy type info missing for .*UnboundStruct.*$\', str(excinfo.value))\n\n    ld = np.dtype(\'longdouble\')\n    ldbl_fmt = (\'4x\' if ld.alignment > 4 else \'\') + ld.char\n    ss_fmt = ""^T{?:bool_:3xI:uint_:f:float_:"" + ldbl_fmt + "":ldbl_:}""\n    dbl = np.dtype(\'double\')\n    partial_fmt = (""^T{?:bool_:3xI:uint_:f:float_:"" +\n                   str(4 * (dbl.alignment > 4) + dbl.itemsize + 8 * (ld.alignment > 8)) +\n                   ""xg:ldbl_:}"")\n    nested_extra = str(max(8, ld.alignment))\n    assert m.print_format_descriptors() == [\n        ss_fmt,\n        ""^T{?:bool_:I:uint_:f:float_:g:ldbl_:}"",\n        ""^T{"" + ss_fmt + "":a:^T{?:bool_:I:uint_:f:float_:g:ldbl_:}:b:}"",\n        partial_fmt,\n        ""^T{"" + nested_extra + ""x"" + partial_fmt + "":a:"" + nested_extra + ""x}"",\n        ""^T{3s:a:3s:b:}"",\n        ""^T{(3)4s:a:(2)i:b:(3)B:c:1x(4, 2)f:d:}"",\n        \'^T{q:e1:B:e2:}\',\n        \'^T{Zf:cflt:Zd:cdbl:}\'\n    ]\n\n\ndef test_dtype(simple_dtype):\n    from sys import byteorder\n    e = \'<\' if byteorder == \'little\' else \'>\'\n\n    assert m.print_dtypes() == [\n        simple_dtype_fmt(),\n        packed_dtype_fmt(),\n        ""[(\'a\', {}), (\'b\', {})]"".format(simple_dtype_fmt(), packed_dtype_fmt()),\n        partial_dtype_fmt(),\n        partial_nested_fmt(),\n        ""[(\'a\', \'S3\'), (\'b\', \'S3\')]"",\n        (""{{\'names\':[\'a\',\'b\',\'c\',\'d\'], "" +\n         ""\'formats\':[(\'S4\', (3,)),(\'"" + e + ""i4\', (2,)),(\'u1\', (3,)),(\'"" + e + ""f4\', (4, 2))], "" +\n         ""\'offsets\':[0,12,20,24], \'itemsize\':56}}"").format(e=e),\n        ""[(\'e1\', \'"" + e + ""i8\'), (\'e2\', \'u1\')]"",\n        ""[(\'x\', \'i1\'), (\'y\', \'"" + e + ""u8\')]"",\n        ""[(\'cflt\', \'"" + e + ""c8\'), (\'cdbl\', \'"" + e + ""c16\')]""\n    ]\n\n    d1 = np.dtype({\'names\': [\'a\', \'b\'], \'formats\': [\'int32\', \'float64\'],\n                   \'offsets\': [1, 10], \'itemsize\': 20})\n    d2 = np.dtype([(\'a\', \'i4\'), (\'b\', \'f4\')])\n    assert m.test_dtype_ctors() == [np.dtype(\'int32\'), np.dtype(\'float64\'),\n                                    np.dtype(\'bool\'), d1, d1, np.dtype(\'uint32\'), d2]\n\n    assert m.test_dtype_methods() == [np.dtype(\'int32\'), simple_dtype, False, True,\n                                      np.dtype(\'int32\').itemsize, simple_dtype.itemsize]\n\n    assert m.trailing_padding_dtype() == m.buffer_to_dtype(np.zeros(1, m.trailing_padding_dtype()))\n\n\ndef test_recarray(simple_dtype, packed_dtype):\n    elements = [(False, 0, 0.0, -0.0), (True, 1, 1.5, -2.5), (False, 2, 3.0, -5.0)]\n\n    for func, dtype in [(m.create_rec_simple, simple_dtype), (m.create_rec_packed, packed_dtype)]:\n        arr = func(0)\n        assert arr.dtype == dtype\n        assert_equal(arr, [], simple_dtype)\n        assert_equal(arr, [], packed_dtype)\n\n        arr = func(3)\n        assert arr.dtype == dtype\n        assert_equal(arr, elements, simple_dtype)\n        assert_equal(arr, elements, packed_dtype)\n\n        if dtype == simple_dtype:\n            assert m.print_rec_simple(arr) == [\n                ""s:0,0,0,-0"",\n                ""s:1,1,1.5,-2.5"",\n                ""s:0,2,3,-5""\n            ]\n        else:\n            assert m.print_rec_packed(arr) == [\n                ""p:0,0,0,-0"",\n                ""p:1,1,1.5,-2.5"",\n                ""p:0,2,3,-5""\n            ]\n\n    nested_dtype = np.dtype([(\'a\', simple_dtype), (\'b\', packed_dtype)])\n\n    arr = m.create_rec_nested(0)\n    assert arr.dtype == nested_dtype\n    assert_equal(arr, [], nested_dtype)\n\n    arr = m.create_rec_nested(3)\n    assert arr.dtype == nested_dtype\n    assert_equal(arr, [((False, 0, 0.0, -0.0), (True, 1, 1.5, -2.5)),\n                       ((True, 1, 1.5, -2.5), (False, 2, 3.0, -5.0)),\n                       ((False, 2, 3.0, -5.0), (True, 3, 4.5, -7.5))], nested_dtype)\n    assert m.print_rec_nested(arr) == [\n        ""n:a=s:0,0,0,-0;b=p:1,1,1.5,-2.5"",\n        ""n:a=s:1,1,1.5,-2.5;b=p:0,2,3,-5"",\n        ""n:a=s:0,2,3,-5;b=p:1,3,4.5,-7.5""\n    ]\n\n    arr = m.create_rec_partial(3)\n    assert str(arr.dtype) == partial_dtype_fmt()\n    partial_dtype = arr.dtype\n    assert \'\' not in arr.dtype.fields\n    assert partial_dtype.itemsize > simple_dtype.itemsize\n    assert_equal(arr, elements, simple_dtype)\n    assert_equal(arr, elements, packed_dtype)\n\n    arr = m.create_rec_partial_nested(3)\n    assert str(arr.dtype) == partial_nested_fmt()\n    assert \'\' not in arr.dtype.fields\n    assert \'\' not in arr.dtype.fields[\'a\'][0].fields\n    assert arr.dtype.itemsize > partial_dtype.itemsize\n    np.testing.assert_equal(arr[\'a\'], m.create_rec_partial(3))\n\n\ndef test_array_constructors():\n    data = np.arange(1, 7, dtype=\'int32\')\n    for i in range(8):\n        np.testing.assert_array_equal(m.test_array_ctors(10 + i), data.reshape((3, 2)))\n        np.testing.assert_array_equal(m.test_array_ctors(20 + i), data.reshape((3, 2)))\n    for i in range(5):\n        np.testing.assert_array_equal(m.test_array_ctors(30 + i), data)\n        np.testing.assert_array_equal(m.test_array_ctors(40 + i), data)\n\n\ndef test_string_array():\n    arr = m.create_string_array(True)\n    assert str(arr.dtype) == ""[(\'a\', \'S3\'), (\'b\', \'S3\')]""\n    assert m.print_string_array(arr) == [\n        ""a=\'\',b=\'\'"",\n        ""a=\'a\',b=\'a\'"",\n        ""a=\'ab\',b=\'ab\'"",\n        ""a=\'abc\',b=\'abc\'""\n    ]\n    dtype = arr.dtype\n    assert arr[\'a\'].tolist() == [b\'\', b\'a\', b\'ab\', b\'abc\']\n    assert arr[\'b\'].tolist() == [b\'\', b\'a\', b\'ab\', b\'abc\']\n    arr = m.create_string_array(False)\n    assert dtype == arr.dtype\n\n\ndef test_array_array():\n    from sys import byteorder\n    e = \'<\' if byteorder == \'little\' else \'>\'\n\n    arr = m.create_array_array(3)\n    assert str(arr.dtype) == (\n        ""{{\'names\':[\'a\',\'b\',\'c\',\'d\'], "" +\n        ""\'formats\':[(\'S4\', (3,)),(\'"" + e + ""i4\', (2,)),(\'u1\', (3,)),(\'{e}f4\', (4, 2))], "" +\n        ""\'offsets\':[0,12,20,24], \'itemsize\':56}}"").format(e=e)\n    assert m.print_array_array(arr) == [\n        ""a={{A,B,C,D},{K,L,M,N},{U,V,W,X}},b={0,1},"" +\n        ""c={0,1,2},d={{0,1},{10,11},{20,21},{30,31}}"",\n        ""a={{W,X,Y,Z},{G,H,I,J},{Q,R,S,T}},b={1000,1001},"" +\n        ""c={10,11,12},d={{100,101},{110,111},{120,121},{130,131}}"",\n        ""a={{S,T,U,V},{C,D,E,F},{M,N,O,P}},b={2000,2001},"" +\n        ""c={20,21,22},d={{200,201},{210,211},{220,221},{230,231}}"",\n    ]\n    assert arr[\'a\'].tolist() == [[b\'ABCD\', b\'KLMN\', b\'UVWX\'],\n                                 [b\'WXYZ\', b\'GHIJ\', b\'QRST\'],\n                                 [b\'STUV\', b\'CDEF\', b\'MNOP\']]\n    assert arr[\'b\'].tolist() == [[0, 1], [1000, 1001], [2000, 2001]]\n    assert m.create_array_array(0).dtype == arr.dtype\n\n\ndef test_enum_array():\n    from sys import byteorder\n    e = \'<\' if byteorder == \'little\' else \'>\'\n\n    arr = m.create_enum_array(3)\n    dtype = arr.dtype\n    assert dtype == np.dtype([(\'e1\', e + \'i8\'), (\'e2\', \'u1\')])\n    assert m.print_enum_array(arr) == [\n        ""e1=A,e2=X"",\n        ""e1=B,e2=Y"",\n        ""e1=A,e2=X""\n    ]\n    assert arr[\'e1\'].tolist() == [-1, 1, -1]\n    assert arr[\'e2\'].tolist() == [1, 2, 1]\n    assert m.create_enum_array(0).dtype == dtype\n\n\ndef test_complex_array():\n    from sys import byteorder\n    e = \'<\' if byteorder == \'little\' else \'>\'\n\n    arr = m.create_complex_array(3)\n    dtype = arr.dtype\n    assert dtype == np.dtype([(\'cflt\', e + \'c8\'), (\'cdbl\', e + \'c16\')])\n    assert m.print_complex_array(arr) == [\n        ""c:(0,0.25),(0.5,0.75)"",\n        ""c:(1,1.25),(1.5,1.75)"",\n        ""c:(2,2.25),(2.5,2.75)""\n    ]\n    assert arr[\'cflt\'].tolist() == [0.0 + 0.25j, 1.0 + 1.25j, 2.0 + 2.25j]\n    assert arr[\'cdbl\'].tolist() == [0.5 + 0.75j, 1.5 + 1.75j, 2.5 + 2.75j]\n    assert m.create_complex_array(0).dtype == dtype\n\n\ndef test_signature(doc):\n    assert doc(m.create_rec_nested) == \\\n        ""create_rec_nested(arg0: int) -> numpy.ndarray[NestedStruct]""\n\n\ndef test_scalar_conversion():\n    n = 3\n    arrays = [m.create_rec_simple(n), m.create_rec_packed(n),\n              m.create_rec_nested(n), m.create_enum_array(n)]\n    funcs = [m.f_simple, m.f_packed, m.f_nested]\n\n    for i, func in enumerate(funcs):\n        for j, arr in enumerate(arrays):\n            if i == j and i < 2:\n                assert [func(arr[k]) for k in range(n)] == [k * 10 for k in range(n)]\n            else:\n                with pytest.raises(TypeError) as excinfo:\n                    func(arr[0])\n                assert \'incompatible function arguments\' in str(excinfo.value)\n\n\ndef test_register_dtype():\n    with pytest.raises(RuntimeError) as excinfo:\n        m.register_dtype()\n    assert \'dtype is already registered\' in str(excinfo.value)\n\n\n@pytest.requires_numpy\ndef test_compare_buffer_info():\n    assert all(m.compare_buffer_info())\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_numpy_vectorize.py,0,"b'import pytest\nfrom pybind11_tests import numpy_vectorize as m\n\npytestmark = pytest.requires_numpy\n\nwith pytest.suppress(ImportError):\n    import numpy as np\n\n\ndef test_vectorize(capture):\n    assert np.isclose(m.vectorized_func3(np.array(3 + 7j)), [6 + 14j])\n\n    for f in [m.vectorized_func, m.vectorized_func2]:\n        with capture:\n            assert np.isclose(f(1, 2, 3), 6)\n        assert capture == ""my_func(x:int=1, y:float=2, z:float=3)""\n        with capture:\n            assert np.isclose(f(np.array(1), np.array(2), 3), 6)\n        assert capture == ""my_func(x:int=1, y:float=2, z:float=3)""\n        with capture:\n            assert np.allclose(f(np.array([1, 3]), np.array([2, 4]), 3), [6, 36])\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=3)\n            my_func(x:int=3, y:float=4, z:float=3)\n        """"""\n        with capture:\n            a = np.array([[1, 2], [3, 4]], order=\'F\')\n            b = np.array([[10, 20], [30, 40]], order=\'F\')\n            c = 3\n            result = f(a, b, c)\n            assert np.allclose(result, a * b * c)\n            assert result.flags.f_contiguous\n        # All inputs are F order and full or singletons, so we the result is in col-major order:\n        assert capture == """"""\n            my_func(x:int=1, y:float=10, z:float=3)\n            my_func(x:int=3, y:float=30, z:float=3)\n            my_func(x:int=2, y:float=20, z:float=3)\n            my_func(x:int=4, y:float=40, z:float=3)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 3, 5], [7, 9, 11]]), np.array([[2, 4, 6], [8, 10, 12]]), 3\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=3)\n            my_func(x:int=3, y:float=4, z:float=3)\n            my_func(x:int=5, y:float=6, z:float=3)\n            my_func(x:int=7, y:float=8, z:float=3)\n            my_func(x:int=9, y:float=10, z:float=3)\n            my_func(x:int=11, y:float=12, z:float=3)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 2, 3], [4, 5, 6]]), np.array([2, 3, 4]), 2\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=2)\n            my_func(x:int=2, y:float=3, z:float=2)\n            my_func(x:int=3, y:float=4, z:float=2)\n            my_func(x:int=4, y:float=2, z:float=2)\n            my_func(x:int=5, y:float=3, z:float=2)\n            my_func(x:int=6, y:float=4, z:float=2)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), 2\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=2)\n            my_func(x:int=2, y:float=2, z:float=2)\n            my_func(x:int=3, y:float=2, z:float=2)\n            my_func(x:int=4, y:float=3, z:float=2)\n            my_func(x:int=5, y:float=3, z:float=2)\n            my_func(x:int=6, y:float=3, z:float=2)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 2, 3], [4, 5, 6]], order=\'F\'), np.array([[2], [3]]), 2\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=2)\n            my_func(x:int=2, y:float=2, z:float=2)\n            my_func(x:int=3, y:float=2, z:float=2)\n            my_func(x:int=4, y:float=3, z:float=2)\n            my_func(x:int=5, y:float=3, z:float=2)\n            my_func(x:int=6, y:float=3, z:float=2)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 2, 3], [4, 5, 6]])[::, ::2], np.array([[2], [3]]), 2\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=2)\n            my_func(x:int=3, y:float=2, z:float=2)\n            my_func(x:int=4, y:float=3, z:float=2)\n            my_func(x:int=6, y:float=3, z:float=2)\n        """"""\n        with capture:\n            a, b, c = np.array([[1, 2, 3], [4, 5, 6]], order=\'F\')[::, ::2], np.array([[2], [3]]), 2\n            assert np.allclose(f(a, b, c), a * b * c)\n        assert capture == """"""\n            my_func(x:int=1, y:float=2, z:float=2)\n            my_func(x:int=3, y:float=2, z:float=2)\n            my_func(x:int=4, y:float=3, z:float=2)\n            my_func(x:int=6, y:float=3, z:float=2)\n        """"""\n\n\ndef test_type_selection():\n    assert m.selective_func(np.array([1], dtype=np.int32)) == ""Int branch taken.""\n    assert m.selective_func(np.array([1.0], dtype=np.float32)) == ""Float branch taken.""\n    assert m.selective_func(np.array([1.0j], dtype=np.complex64)) == ""Complex float branch taken.""\n\n\ndef test_docs(doc):\n    assert doc(m.vectorized_func) == """"""\n        vectorized_func(arg0: numpy.ndarray[int32], arg1: numpy.ndarray[float32], arg2: numpy.ndarray[float64]) -> object\n    """"""  # noqa: E501 line too long\n\n\ndef test_trivial_broadcasting():\n    trivial, vectorized_is_trivial = m.trivial, m.vectorized_is_trivial\n\n    assert vectorized_is_trivial(1, 2, 3) == trivial.c_trivial\n    assert vectorized_is_trivial(np.array(1), np.array(2), 3) == trivial.c_trivial\n    assert vectorized_is_trivial(np.array([1, 3]), np.array([2, 4]), 3) == trivial.c_trivial\n    assert trivial.c_trivial == vectorized_is_trivial(\n        np.array([[1, 3, 5], [7, 9, 11]]), np.array([[2, 4, 6], [8, 10, 12]]), 3)\n    assert vectorized_is_trivial(\n        np.array([[1, 2, 3], [4, 5, 6]]), np.array([2, 3, 4]), 2) == trivial.non_trivial\n    assert vectorized_is_trivial(\n        np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), 2) == trivial.non_trivial\n    z1 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=\'int32\')\n    z2 = np.array(z1, dtype=\'float32\')\n    z3 = np.array(z1, dtype=\'float64\')\n    assert vectorized_is_trivial(z1, z2, z3) == trivial.c_trivial\n    assert vectorized_is_trivial(1, z2, z3) == trivial.c_trivial\n    assert vectorized_is_trivial(z1, 1, z3) == trivial.c_trivial\n    assert vectorized_is_trivial(z1, z2, 1) == trivial.c_trivial\n    assert vectorized_is_trivial(z1[::2, ::2], 1, 1) == trivial.non_trivial\n    assert vectorized_is_trivial(1, 1, z1[::2, ::2]) == trivial.c_trivial\n    assert vectorized_is_trivial(1, 1, z3[::2, ::2]) == trivial.non_trivial\n    assert vectorized_is_trivial(z1, 1, z3[1::4, 1::4]) == trivial.c_trivial\n\n    y1 = np.array(z1, order=\'F\')\n    y2 = np.array(y1)\n    y3 = np.array(y1)\n    assert vectorized_is_trivial(y1, y2, y3) == trivial.f_trivial\n    assert vectorized_is_trivial(y1, 1, 1) == trivial.f_trivial\n    assert vectorized_is_trivial(1, y2, 1) == trivial.f_trivial\n    assert vectorized_is_trivial(1, 1, y3) == trivial.f_trivial\n    assert vectorized_is_trivial(y1, z2, 1) == trivial.non_trivial\n    assert vectorized_is_trivial(z1[1::4, 1::4], y2, 1) == trivial.f_trivial\n    assert vectorized_is_trivial(y1[1::4, 1::4], z2, 1) == trivial.c_trivial\n\n    assert m.vectorized_func(z1, z2, z3).flags.c_contiguous\n    assert m.vectorized_func(y1, y2, y3).flags.f_contiguous\n    assert m.vectorized_func(z1, 1, 1).flags.c_contiguous\n    assert m.vectorized_func(1, y2, 1).flags.f_contiguous\n    assert m.vectorized_func(z1[1::4, 1::4], y2, 1).flags.f_contiguous\n    assert m.vectorized_func(y1[1::4, 1::4], z2, 1).flags.c_contiguous\n\n\ndef test_passthrough_arguments(doc):\n    assert doc(m.vec_passthrough) == (\n        ""vec_passthrough("" + "", "".join([\n            ""arg0: float"",\n            ""arg1: numpy.ndarray[float64]"",\n            ""arg2: numpy.ndarray[float64]"",\n            ""arg3: numpy.ndarray[int32]"",\n            ""arg4: int"",\n            ""arg5: m.numpy_vectorize.NonPODClass"",\n            ""arg6: numpy.ndarray[float64]""]) + "") -> object"")\n\n    b = np.array([[10, 20, 30]], dtype=\'float64\')\n    c = np.array([100, 200])  # NOT a vectorized argument\n    d = np.array([[1000], [2000], [3000]], dtype=\'int\')\n    g = np.array([[1000000, 2000000, 3000000]], dtype=\'int\')  # requires casting\n    assert np.all(\n        m.vec_passthrough(1, b, c, d, 10000, m.NonPODClass(100000), g) ==\n        np.array([[1111111, 2111121, 3111131],\n                  [1112111, 2112121, 3112131],\n                  [1113111, 2113121, 3113131]]))\n\n\ndef test_method_vectorization():\n    o = m.VectorizeTestClass(3)\n    x = np.array([1, 2], dtype=\'int\')\n    y = np.array([[10], [20]], dtype=\'float32\')\n    assert np.all(o.method(x, y) == [[14, 15], [24, 25]])\n\n\ndef test_array_collapse():\n    assert not isinstance(m.vectorized_func(1, 2, 3), np.ndarray)\n    assert not isinstance(m.vectorized_func(np.array(1), 2, 3), np.ndarray)\n    z = m.vectorized_func([1], 2, 3)\n    assert isinstance(z, np.ndarray)\n    assert z.shape == (1, )\n    z = m.vectorized_func(1, [[[2]]], 3)\n    assert isinstance(z, np.ndarray)\n    assert z.shape == (1, 1, 1)\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_opaque_types.py,0,"b'import pytest\nfrom pybind11_tests import opaque_types as m\nfrom pybind11_tests import ConstructorStats, UserType\n\n\ndef test_string_list():\n    lst = m.StringList()\n    lst.push_back(""Element 1"")\n    lst.push_back(""Element 2"")\n    assert m.print_opaque_list(lst) == ""Opaque list: [Element 1, Element 2]""\n    assert lst.back() == ""Element 2""\n\n    for i, k in enumerate(lst, start=1):\n        assert k == ""Element {}"".format(i)\n    lst.pop_back()\n    assert m.print_opaque_list(lst) == ""Opaque list: [Element 1]""\n\n    cvp = m.ClassWithSTLVecProperty()\n    assert m.print_opaque_list(cvp.stringList) == ""Opaque list: []""\n\n    cvp.stringList = lst\n    cvp.stringList.push_back(""Element 3"")\n    assert m.print_opaque_list(cvp.stringList) == ""Opaque list: [Element 1, Element 3]""\n\n\ndef test_pointers(msg):\n    living_before = ConstructorStats.get(UserType).alive()\n    assert m.get_void_ptr_value(m.return_void_ptr()) == 0x1234\n    assert m.get_void_ptr_value(UserType())  # Should also work for other C++ types\n    assert ConstructorStats.get(UserType).alive() == living_before\n\n    with pytest.raises(TypeError) as excinfo:\n        m.get_void_ptr_value([1, 2, 3])  # This should not work\n    assert msg(excinfo.value) == """"""\n        get_void_ptr_value(): incompatible function arguments. The following argument types are supported:\n            1. (arg0: capsule) -> int\n\n        Invoked with: [1, 2, 3]\n    """"""  # noqa: E501 line too long\n\n    assert m.return_null_str() is None\n    assert m.get_null_str_value(m.return_null_str()) is not None\n\n    ptr = m.return_unique_ptr()\n    assert ""StringList"" in repr(ptr)\n    assert m.print_opaque_list(ptr) == ""Opaque list: [some value]""\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_operator_overloading.py,0,"b'import pytest\nfrom pybind11_tests import operators as m\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_operator_overloading():\n    v1 = m.Vector2(1, 2)\n    v2 = m.Vector(3, -1)\n    assert str(v1) == ""[1.000000, 2.000000]""\n    assert str(v2) == ""[3.000000, -1.000000]""\n\n    assert str(v1 + v2) == ""[4.000000, 1.000000]""\n    assert str(v1 - v2) == ""[-2.000000, 3.000000]""\n    assert str(v1 - 8) == ""[-7.000000, -6.000000]""\n    assert str(v1 + 8) == ""[9.000000, 10.000000]""\n    assert str(v1 * 8) == ""[8.000000, 16.000000]""\n    assert str(v1 / 8) == ""[0.125000, 0.250000]""\n    assert str(8 - v1) == ""[7.000000, 6.000000]""\n    assert str(8 + v1) == ""[9.000000, 10.000000]""\n    assert str(8 * v1) == ""[8.000000, 16.000000]""\n    assert str(8 / v1) == ""[8.000000, 4.000000]""\n    assert str(v1 * v2) == ""[3.000000, -2.000000]""\n    assert str(v2 / v1) == ""[3.000000, -0.500000]""\n\n    v1 += 2 * v2\n    assert str(v1) == ""[7.000000, 0.000000]""\n    v1 -= v2\n    assert str(v1) == ""[4.000000, 1.000000]""\n    v1 *= 2\n    assert str(v1) == ""[8.000000, 2.000000]""\n    v1 /= 16\n    assert str(v1) == ""[0.500000, 0.125000]""\n    v1 *= v2\n    assert str(v1) == ""[1.500000, -0.125000]""\n    v2 /= v1\n    assert str(v2) == ""[2.000000, 8.000000]""\n\n    assert hash(v1) == 4\n\n    cstats = ConstructorStats.get(m.Vector2)\n    assert cstats.alive() == 2\n    del v1\n    assert cstats.alive() == 1\n    del v2\n    assert cstats.alive() == 0\n    assert cstats.values() == [\'[1.000000, 2.000000]\', \'[3.000000, -1.000000]\',\n                               \'[4.000000, 1.000000]\', \'[-2.000000, 3.000000]\',\n                               \'[-7.000000, -6.000000]\', \'[9.000000, 10.000000]\',\n                               \'[8.000000, 16.000000]\', \'[0.125000, 0.250000]\',\n                               \'[7.000000, 6.000000]\', \'[9.000000, 10.000000]\',\n                               \'[8.000000, 16.000000]\', \'[8.000000, 4.000000]\',\n                               \'[3.000000, -2.000000]\', \'[3.000000, -0.500000]\',\n                               \'[6.000000, -2.000000]\']\n    assert cstats.default_constructions == 0\n    assert cstats.copy_constructions == 0\n    assert cstats.move_constructions >= 10\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n\ndef test_operators_notimplemented():\n    """"""#393: need to return NotSupported to ensure correct arithmetic operator behavior""""""\n\n    c1, c2 = m.C1(), m.C2()\n    assert c1 + c1 == 11\n    assert c2 + c2 == 22\n    assert c2 + c1 == 21\n    assert c1 + c2 == 12\n\n\ndef test_nested():\n    """"""#328: first member in a class can\'t be used in operators""""""\n\n    a = m.NestA()\n    b = m.NestB()\n    c = m.NestC()\n\n    a += 10\n    assert m.get_NestA(a) == 13\n    b.a += 100\n    assert m.get_NestA(b.a) == 103\n    c.b.a += 1000\n    assert m.get_NestA(c.b.a) == 1003\n    b -= 1\n    assert m.get_NestB(b) == 3\n    c.b -= 3\n    assert m.get_NestB(c.b) == 1\n    c *= 7\n    assert m.get_NestC(c) == 35\n\n    abase = a.as_base()\n    assert abase.value == -2\n    a.as_base().value += 44\n    assert abase.value == 42\n    assert c.b.a.as_base().value == -2\n    c.b.a.as_base().value += 44\n    assert c.b.a.as_base().value == 42\n\n    del c\n    pytest.gc_collect()\n    del a  # Shouldn\'t delete while abase is still alive\n    pytest.gc_collect()\n\n    assert abase.value == 42\n    del abase, b\n    pytest.gc_collect()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_pickling.py,0,"b'import pytest\nfrom pybind11_tests import pickling as m\n\ntry:\n    import cPickle as pickle  # Use cPickle on Python 2.7\nexcept ImportError:\n    import pickle\n\n\n@pytest.mark.parametrize(""cls_name"", [""Pickleable"", ""PickleableNew""])\ndef test_roundtrip(cls_name):\n    cls = getattr(m, cls_name)\n    p = cls(""test_value"")\n    p.setExtra1(15)\n    p.setExtra2(48)\n\n    data = pickle.dumps(p, 2)  # Must use pickle protocol >= 2\n    p2 = pickle.loads(data)\n    assert p2.value() == p.value()\n    assert p2.extra1() == p.extra1()\n    assert p2.extra2() == p.extra2()\n\n\n@pytest.unsupported_on_pypy\n@pytest.mark.parametrize(""cls_name"", [""PickleableWithDict"", ""PickleableWithDictNew""])\ndef test_roundtrip_with_dict(cls_name):\n    cls = getattr(m, cls_name)\n    p = cls(""test_value"")\n    p.extra = 15\n    p.dynamic = ""Attribute""\n\n    data = pickle.dumps(p, pickle.HIGHEST_PROTOCOL)\n    p2 = pickle.loads(data)\n    assert p2.value == p.value\n    assert p2.extra == p.extra\n    assert p2.dynamic == p.dynamic\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_pytypes.py,0,"b'import pytest\nimport sys\n\nfrom pybind11_tests import pytypes as m\nfrom pybind11_tests import debug_enabled\n\n\ndef test_list(capture, doc):\n    with capture:\n        lst = m.get_list()\n        assert lst == [""overwritten""]\n\n        lst.append(""value2"")\n        m.print_list(lst)\n    assert capture.unordered == """"""\n        Entry at position 0: value\n        list item 0: overwritten\n        list item 1: value2\n    """"""\n\n    assert doc(m.get_list) == ""get_list() -> list""\n    assert doc(m.print_list) == ""print_list(arg0: list) -> None""\n\n\ndef test_set(capture, doc):\n    s = m.get_set()\n    assert s == {""key1"", ""key2"", ""key3""}\n\n    with capture:\n        s.add(""key4"")\n        m.print_set(s)\n    assert capture.unordered == """"""\n        key: key1\n        key: key2\n        key: key3\n        key: key4\n    """"""\n\n    assert doc(m.get_list) == ""get_list() -> list""\n    assert doc(m.print_list) == ""print_list(arg0: list) -> None""\n\n\ndef test_dict(capture, doc):\n    d = m.get_dict()\n    assert d == {""key"": ""value""}\n\n    with capture:\n        d[""key2""] = ""value2""\n        m.print_dict(d)\n    assert capture.unordered == """"""\n        key: key, value=value\n        key: key2, value=value2\n    """"""\n\n    assert doc(m.get_dict) == ""get_dict() -> dict""\n    assert doc(m.print_dict) == ""print_dict(arg0: dict) -> None""\n\n    assert m.dict_keyword_constructor() == {""x"": 1, ""y"": 2, ""z"": 3}\n\n\ndef test_str(doc):\n    assert m.str_from_string().encode().decode() == ""baz""\n    assert m.str_from_bytes().encode().decode() == ""boo""\n\n    assert doc(m.str_from_bytes) == ""str_from_bytes() -> str""\n\n    class A(object):\n        def __str__(self):\n            return ""this is a str""\n\n        def __repr__(self):\n            return ""this is a repr""\n\n    assert m.str_from_object(A()) == ""this is a str""\n    assert m.repr_from_object(A()) == ""this is a repr""\n\n    s1, s2 = m.str_format()\n    assert s1 == ""1 + 2 = 3""\n    assert s1 == s2\n\n\ndef test_bytes(doc):\n    assert m.bytes_from_string().decode() == ""foo""\n    assert m.bytes_from_str().decode() == ""bar""\n\n    assert doc(m.bytes_from_str) == ""bytes_from_str() -> {}"".format(\n        ""bytes"" if sys.version_info[0] == 3 else ""str""\n    )\n\n\ndef test_capsule(capture):\n    pytest.gc_collect()\n    with capture:\n        a = m.return_capsule_with_destructor()\n        del a\n        pytest.gc_collect()\n    assert capture.unordered == """"""\n        creating capsule\n        destructing capsule\n    """"""\n\n    with capture:\n        a = m.return_capsule_with_destructor_2()\n        del a\n        pytest.gc_collect()\n    assert capture.unordered == """"""\n        creating capsule\n        destructing capsule: 1234\n    """"""\n\n    with capture:\n        a = m.return_capsule_with_name_and_destructor()\n        del a\n        pytest.gc_collect()\n    assert capture.unordered == """"""\n        created capsule (1234, \'pointer type description\')\n        destructing capsule (1234, \'pointer type description\')\n    """"""\n\n\ndef test_accessors():\n    class SubTestObject:\n        attr_obj = 1\n        attr_char = 2\n\n    class TestObject:\n        basic_attr = 1\n        begin_end = [1, 2, 3]\n        d = {""operator[object]"": 1, ""operator[char *]"": 2}\n        sub = SubTestObject()\n\n        def func(self, x, *args):\n            return self.basic_attr + x + sum(args)\n\n    d = m.accessor_api(TestObject())\n    assert d[""basic_attr""] == 1\n    assert d[""begin_end""] == [1, 2, 3]\n    assert d[""operator[object]""] == 1\n    assert d[""operator[char *]""] == 2\n    assert d[""attr(object)""] == 1\n    assert d[""attr(char *)""] == 2\n    assert d[""missing_attr_ptr""] == ""raised""\n    assert d[""missing_attr_chain""] == ""raised""\n    assert d[""is_none""] is False\n    assert d[""operator()""] == 2\n    assert d[""operator*""] == 7\n    assert d[""implicit_list""] == [1, 2, 3]\n    assert all(x in TestObject.__dict__ for x in d[""implicit_dict""])\n\n    assert m.tuple_accessor(tuple()) == (0, 1, 2)\n\n    d = m.accessor_assignment()\n    assert d[""get""] == 0\n    assert d[""deferred_get""] == 0\n    assert d[""set""] == 1\n    assert d[""deferred_set""] == 1\n    assert d[""var""] == 99\n\n\ndef test_constructors():\n    """"""C++ default and converting constructors are equivalent to type calls in Python""""""\n    types = [str, bool, int, float, tuple, list, dict, set]\n    expected = {t.__name__: t() for t in types}\n    assert m.default_constructors() == expected\n\n    data = {\n        str: 42,\n        bool: ""Not empty"",\n        int: ""42"",\n        float: ""+1e3"",\n        tuple: range(3),\n        list: range(3),\n        dict: [(""two"", 2), (""one"", 1), (""three"", 3)],\n        set: [4, 4, 5, 6, 6, 6],\n        memoryview: b\'abc\'\n    }\n    inputs = {k.__name__: v for k, v in data.items()}\n    expected = {k.__name__: k(v) for k, v in data.items()}\n\n    assert m.converting_constructors(inputs) == expected\n    assert m.cast_functions(inputs) == expected\n\n    # Converting constructors and cast functions should just reference rather\n    # than copy when no conversion is needed:\n    noconv1 = m.converting_constructors(expected)\n    for k in noconv1:\n        assert noconv1[k] is expected[k]\n\n    noconv2 = m.cast_functions(expected)\n    for k in noconv2:\n        assert noconv2[k] is expected[k]\n\n\ndef test_implicit_casting():\n    """"""Tests implicit casting when assigning or appending to dicts and lists.""""""\n    z = m.get_implicit_casting()\n    assert z[\'d\'] == {\n        \'char*_i1\': \'abc\', \'char*_i2\': \'abc\', \'char*_e\': \'abc\', \'char*_p\': \'abc\',\n        \'str_i1\': \'str\', \'str_i2\': \'str1\', \'str_e\': \'str2\', \'str_p\': \'str3\',\n        \'int_i1\': 42, \'int_i2\': 42, \'int_e\': 43, \'int_p\': 44\n    }\n    assert z[\'l\'] == [3, 6, 9, 12, 15]\n\n\ndef test_print(capture):\n    with capture:\n        m.print_function()\n    assert capture == """"""\n        Hello, World!\n        1 2.0 three True -- multiple args\n        *args-and-a-custom-separator\n        no new line here -- next print\n        flush\n        py::print + str.format = this\n    """"""\n    assert capture.stderr == ""this goes to stderr""\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.print_failure()\n    assert str(excinfo.value) == ""make_tuple(): unable to convert "" + (\n        ""argument of type \'UnregisteredType\' to Python object""\n        if debug_enabled else\n        ""arguments to Python object (compile in debug mode for details)""\n    )\n\n\ndef test_hash():\n    class Hashable(object):\n        def __init__(self, value):\n            self.value = value\n\n        def __hash__(self):\n            return self.value\n\n    class Unhashable(object):\n        __hash__ = None\n\n    assert m.hash_function(Hashable(42)) == 42\n    with pytest.raises(TypeError):\n        m.hash_function(Unhashable())\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_sequences_and_iterators.py,0,"b'import pytest\nfrom pybind11_tests import sequences_and_iterators as m\nfrom pybind11_tests import ConstructorStats\n\n\ndef isclose(a, b, rel_tol=1e-05, abs_tol=0.0):\n    """"""Like math.isclose() from Python 3.5""""""\n    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n\n\ndef allclose(a_list, b_list, rel_tol=1e-05, abs_tol=0.0):\n    return all(isclose(a, b, rel_tol=rel_tol, abs_tol=abs_tol) for a, b in zip(a_list, b_list))\n\n\ndef test_generalized_iterators():\n    assert list(m.IntPairs([(1, 2), (3, 4), (0, 5)]).nonzero()) == [(1, 2), (3, 4)]\n    assert list(m.IntPairs([(1, 2), (2, 0), (0, 3), (4, 5)]).nonzero()) == [(1, 2)]\n    assert list(m.IntPairs([(0, 3), (1, 2), (3, 4)]).nonzero()) == []\n\n    assert list(m.IntPairs([(1, 2), (3, 4), (0, 5)]).nonzero_keys()) == [1, 3]\n    assert list(m.IntPairs([(1, 2), (2, 0), (0, 3), (4, 5)]).nonzero_keys()) == [1]\n    assert list(m.IntPairs([(0, 3), (1, 2), (3, 4)]).nonzero_keys()) == []\n\n    # __next__ must continue to raise StopIteration\n    it = m.IntPairs([(0, 0)]).nonzero()\n    for _ in range(3):\n        with pytest.raises(StopIteration):\n            next(it)\n\n    it = m.IntPairs([(0, 0)]).nonzero_keys()\n    for _ in range(3):\n        with pytest.raises(StopIteration):\n            next(it)\n\n\ndef test_sequence():\n    cstats = ConstructorStats.get(m.Sequence)\n\n    s = m.Sequence(5)\n    assert cstats.values() == [\'of size\', \'5\']\n\n    assert ""Sequence"" in repr(s)\n    assert len(s) == 5\n    assert s[0] == 0 and s[3] == 0\n    assert 12.34 not in s\n    s[0], s[3] = 12.34, 56.78\n    assert 12.34 in s\n    assert isclose(s[0], 12.34) and isclose(s[3], 56.78)\n\n    rev = reversed(s)\n    assert cstats.values() == [\'of size\', \'5\']\n\n    rev2 = s[::-1]\n    assert cstats.values() == [\'of size\', \'5\']\n\n    it = iter(m.Sequence(0))\n    for _ in range(3):  # __next__ must continue to raise StopIteration\n        with pytest.raises(StopIteration):\n            next(it)\n    assert cstats.values() == [\'of size\', \'0\']\n\n    expected = [0, 56.78, 0, 0, 12.34]\n    assert allclose(rev, expected)\n    assert allclose(rev2, expected)\n    assert rev == rev2\n\n    rev[0::2] = m.Sequence([2.0, 2.0, 2.0])\n    assert cstats.values() == [\'of size\', \'3\', \'from std::vector\']\n\n    assert allclose(rev, [2, 56.78, 2, 0, 2])\n\n    assert cstats.alive() == 4\n    del it\n    assert cstats.alive() == 3\n    del s\n    assert cstats.alive() == 2\n    del rev\n    assert cstats.alive() == 1\n    del rev2\n    assert cstats.alive() == 0\n\n    assert cstats.values() == []\n    assert cstats.default_constructions == 0\n    assert cstats.copy_constructions == 0\n    assert cstats.move_constructions >= 1\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n\ndef test_map_iterator():\n    sm = m.StringMap({\'hi\': \'bye\', \'black\': \'white\'})\n    assert sm[\'hi\'] == \'bye\'\n    assert len(sm) == 2\n    assert sm[\'black\'] == \'white\'\n\n    with pytest.raises(KeyError):\n        assert sm[\'orange\']\n    sm[\'orange\'] = \'banana\'\n    assert sm[\'orange\'] == \'banana\'\n\n    expected = {\'hi\': \'bye\', \'black\': \'white\', \'orange\': \'banana\'}\n    for k in sm:\n        assert sm[k] == expected[k]\n    for k, v in sm.items():\n        assert v == expected[k]\n\n    it = iter(m.StringMap({}))\n    for _ in range(3):  # __next__ must continue to raise StopIteration\n        with pytest.raises(StopIteration):\n            next(it)\n\n\ndef test_python_iterator_in_cpp():\n    t = (1, 2, 3)\n    assert m.object_to_list(t) == [1, 2, 3]\n    assert m.object_to_list(iter(t)) == [1, 2, 3]\n    assert m.iterator_to_list(iter(t)) == [1, 2, 3]\n\n    with pytest.raises(TypeError) as excinfo:\n        m.object_to_list(1)\n    assert ""object is not iterable"" in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        m.iterator_to_list(1)\n    assert ""incompatible function arguments"" in str(excinfo.value)\n\n    def bad_next_call():\n        raise RuntimeError(""py::iterator::advance() should propagate errors"")\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.iterator_to_list(iter(bad_next_call, None))\n    assert str(excinfo.value) == ""py::iterator::advance() should propagate errors""\n\n    lst = [1, None, 0, None]\n    assert m.count_none(lst) == 2\n    assert m.find_none(lst) is True\n    assert m.count_nonzeros({""a"": 0, ""b"": 1, ""c"": 2}) == 2\n\n    r = range(5)\n    assert all(m.tuple_iterator(tuple(r)))\n    assert all(m.list_iterator(list(r)))\n    assert all(m.sequence_iterator(r))\n\n\ndef test_iterator_passthrough():\n    """"""#181: iterator passthrough did not compile""""""\n    from pybind11_tests.sequences_and_iterators import iterator_passthrough\n\n    assert list(iterator_passthrough(iter([3, 5, 7, 9, 11, 13, 15]))) == [3, 5, 7, 9, 11, 13, 15]\n\n\ndef test_iterator_rvp():\n    """"""#388: Can\'t make iterators via make_iterator() with different r/v policies """"""\n    import pybind11_tests.sequences_and_iterators as m\n\n    assert list(m.make_iterator_1()) == [1, 2, 3]\n    assert list(m.make_iterator_2()) == [1, 2, 3]\n    assert not isinstance(m.make_iterator_1(), type(m.make_iterator_2()))\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_smart_ptr.py,0,"b'import pytest\nfrom pybind11_tests import smart_ptr as m\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_smart_ptr(capture):\n    # Object1\n    for i, o in enumerate([m.make_object_1(), m.make_object_2(), m.MyObject1(3)], start=1):\n        assert o.getRefCount() == 1\n        with capture:\n            m.print_object_1(o)\n            m.print_object_2(o)\n            m.print_object_3(o)\n            m.print_object_4(o)\n        assert capture == ""MyObject1[{i}]\\n"".format(i=i) * 4\n\n    for i, o in enumerate([m.make_myobject1_1(), m.make_myobject1_2(), m.MyObject1(6), 7],\n                          start=4):\n        print(o)\n        with capture:\n            if not isinstance(o, int):\n                m.print_object_1(o)\n                m.print_object_2(o)\n                m.print_object_3(o)\n                m.print_object_4(o)\n            m.print_myobject1_1(o)\n            m.print_myobject1_2(o)\n            m.print_myobject1_3(o)\n            m.print_myobject1_4(o)\n        assert capture == ""MyObject1[{i}]\\n"".format(i=i) * (4 if isinstance(o, int) else 8)\n\n    cstats = ConstructorStats.get(m.MyObject1)\n    assert cstats.alive() == 0\n    expected_values = [\'MyObject1[{}]\'.format(i) for i in range(1, 7)] + [\'MyObject1[7]\'] * 4\n    assert cstats.values() == expected_values\n    assert cstats.default_constructions == 0\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0 # Doesn\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n    # Object2\n    for i, o in zip([8, 6, 7], [m.MyObject2(8), m.make_myobject2_1(), m.make_myobject2_2()]):\n        print(o)\n        with capture:\n            m.print_myobject2_1(o)\n            m.print_myobject2_2(o)\n            m.print_myobject2_3(o)\n            m.print_myobject2_4(o)\n        assert capture == ""MyObject2[{i}]\\n"".format(i=i) * 4\n\n    cstats = ConstructorStats.get(m.MyObject2)\n    assert cstats.alive() == 1\n    o = None\n    assert cstats.alive() == 0\n    assert cstats.values() == [\'MyObject2[8]\', \'MyObject2[6]\', \'MyObject2[7]\']\n    assert cstats.default_constructions == 0\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0 # Doesn\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n    # Object3\n    for i, o in zip([9, 8, 9], [m.MyObject3(9), m.make_myobject3_1(), m.make_myobject3_2()]):\n        print(o)\n        with capture:\n            m.print_myobject3_1(o)\n            m.print_myobject3_2(o)\n            m.print_myobject3_3(o)\n            m.print_myobject3_4(o)\n        assert capture == ""MyObject3[{i}]\\n"".format(i=i) * 4\n\n    cstats = ConstructorStats.get(m.MyObject3)\n    assert cstats.alive() == 1\n    o = None\n    assert cstats.alive() == 0\n    assert cstats.values() == [\'MyObject3[9]\', \'MyObject3[8]\', \'MyObject3[9]\']\n    assert cstats.default_constructions == 0\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0 # Doesn\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n    # Object\n    cstats = ConstructorStats.get(m.Object)\n    assert cstats.alive() == 0\n    assert cstats.values() == []\n    assert cstats.default_constructions == 10\n    assert cstats.copy_constructions == 0\n    # assert cstats.move_constructions >= 0 # Doesn\'t invoke any\n    assert cstats.copy_assignments == 0\n    assert cstats.move_assignments == 0\n\n    # ref<>\n    cstats = m.cstats_ref()\n    assert cstats.alive() == 0\n    assert cstats.values() == [\'from pointer\'] * 10\n    assert cstats.default_constructions == 30\n    assert cstats.copy_constructions == 12\n    # assert cstats.move_constructions >= 0 # Doesn\'t invoke any\n    assert cstats.copy_assignments == 30\n    assert cstats.move_assignments == 0\n\n\ndef test_smart_ptr_refcounting():\n    assert m.test_object1_refcounting()\n\n\ndef test_unique_nodelete():\n    o = m.MyObject4(23)\n    assert o.value == 23\n    cstats = ConstructorStats.get(m.MyObject4)\n    assert cstats.alive() == 1\n    del o\n    assert cstats.alive() == 1  # Leak, but that\'s intentional\n\n\ndef test_large_holder():\n    o = m.MyObject5(5)\n    assert o.value == 5\n    cstats = ConstructorStats.get(m.MyObject5)\n    assert cstats.alive() == 1\n    del o\n    assert cstats.alive() == 0\n\n\ndef test_shared_ptr_and_references():\n    s = m.SharedPtrRef()\n    stats = ConstructorStats.get(m.A)\n    assert stats.alive() == 2\n\n    ref = s.ref  # init_holder_helper(holder_ptr=false, owned=false)\n    assert stats.alive() == 2\n    assert s.set_ref(ref)\n    with pytest.raises(RuntimeError) as excinfo:\n        assert s.set_holder(ref)\n    assert ""Unable to cast from non-held to held instance"" in str(excinfo.value)\n\n    copy = s.copy  # init_holder_helper(holder_ptr=false, owned=true)\n    assert stats.alive() == 3\n    assert s.set_ref(copy)\n    assert s.set_holder(copy)\n\n    holder_ref = s.holder_ref  # init_holder_helper(holder_ptr=true, owned=false)\n    assert stats.alive() == 3\n    assert s.set_ref(holder_ref)\n    assert s.set_holder(holder_ref)\n\n    holder_copy = s.holder_copy  # init_holder_helper(holder_ptr=true, owned=true)\n    assert stats.alive() == 3\n    assert s.set_ref(holder_copy)\n    assert s.set_holder(holder_copy)\n\n    del ref, copy, holder_ref, holder_copy, s\n    assert stats.alive() == 0\n\n\ndef test_shared_ptr_from_this_and_references():\n    s = m.SharedFromThisRef()\n    stats = ConstructorStats.get(m.B)\n    assert stats.alive() == 2\n\n    ref = s.ref  # init_holder_helper(holder_ptr=false, owned=false, bad_wp=false)\n    assert stats.alive() == 2\n    assert s.set_ref(ref)\n    assert s.set_holder(ref)  # std::enable_shared_from_this can create a holder from a reference\n\n    bad_wp = s.bad_wp  # init_holder_helper(holder_ptr=false, owned=false, bad_wp=true)\n    assert stats.alive() == 2\n    assert s.set_ref(bad_wp)\n    with pytest.raises(RuntimeError) as excinfo:\n        assert s.set_holder(bad_wp)\n    assert ""Unable to cast from non-held to held instance"" in str(excinfo.value)\n\n    copy = s.copy  # init_holder_helper(holder_ptr=false, owned=true, bad_wp=false)\n    assert stats.alive() == 3\n    assert s.set_ref(copy)\n    assert s.set_holder(copy)\n\n    holder_ref = s.holder_ref  # init_holder_helper(holder_ptr=true, owned=false, bad_wp=false)\n    assert stats.alive() == 3\n    assert s.set_ref(holder_ref)\n    assert s.set_holder(holder_ref)\n\n    holder_copy = s.holder_copy  # init_holder_helper(holder_ptr=true, owned=true, bad_wp=false)\n    assert stats.alive() == 3\n    assert s.set_ref(holder_copy)\n    assert s.set_holder(holder_copy)\n\n    del ref, bad_wp, copy, holder_ref, holder_copy, s\n    assert stats.alive() == 0\n\n    z = m.SharedFromThisVirt.get()\n    y = m.SharedFromThisVirt.get()\n    assert y is z\n\n\ndef test_move_only_holder():\n    a = m.TypeWithMoveOnlyHolder.make()\n    stats = ConstructorStats.get(m.TypeWithMoveOnlyHolder)\n    assert stats.alive() == 1\n    del a\n    assert stats.alive() == 0\n\n\ndef test_holder_with_addressof_operator():\n    # this test must not throw exception from c++\n    a = m.TypeForHolderWithAddressOf.make()\n    a.print_object_1()\n    a.print_object_2()\n    a.print_object_3()\n    a.print_object_4()\n\n    stats = ConstructorStats.get(m.TypeForHolderWithAddressOf)\n    assert stats.alive() == 1\n\n    np = m.TypeForHolderWithAddressOf.make()\n    assert stats.alive() == 2\n    del a\n    assert stats.alive() == 1\n    del np\n    assert stats.alive() == 0\n\n    b = m.TypeForHolderWithAddressOf.make()\n    c = b\n    assert b.get() is c.get()\n    assert stats.alive() == 1\n\n    del b\n    assert stats.alive() == 1\n\n    del c\n    assert stats.alive() == 0\n\n\ndef test_move_only_holder_with_addressof_operator():\n    a = m.TypeForMoveOnlyHolderWithAddressOf.make()\n    a.print_object()\n\n    stats = ConstructorStats.get(m.TypeForMoveOnlyHolderWithAddressOf)\n    assert stats.alive() == 1\n\n    a.value = 42\n    assert a.value == 42\n\n    del a\n    assert stats.alive() == 0\n\n\ndef test_smart_ptr_from_default():\n    instance = m.HeldByDefaultHolder()\n    with pytest.raises(RuntimeError) as excinfo:\n        m.HeldByDefaultHolder.load_shared_ptr(instance)\n    assert ""Unable to load a custom holder type from a default-holder instance"" in str(excinfo)\n\n\ndef test_shared_ptr_gc():\n    """"""#187: issue involving std::shared_ptr<> return value policy & garbage collection""""""\n    el = m.ElementList()\n    for i in range(10):\n        el.add(m.ElementA(i))\n    pytest.gc_collect()\n    for i, v in enumerate(el.get()):\n        assert i == v.value()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_stl.py,0,"b'import pytest\n\nfrom pybind11_tests import stl as m\nfrom pybind11_tests import UserType\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_vector(doc):\n    """"""std::vector <-> list""""""\n    lst = m.cast_vector()\n    assert lst == [1]\n    lst.append(2)\n    assert m.load_vector(lst)\n    assert m.load_vector(tuple(lst))\n\n    assert m.cast_bool_vector() == [True, False]\n    assert m.load_bool_vector([True, False])\n\n    assert doc(m.cast_vector) == ""cast_vector() -> List[int]""\n    assert doc(m.load_vector) == ""load_vector(arg0: List[int]) -> bool""\n\n    # Test regression caused by 936: pointers to stl containers weren\'t castable\n    assert m.cast_ptr_vector() == [""lvalue"", ""lvalue""]\n\n\ndef test_array(doc):\n    """"""std::array <-> list""""""\n    lst = m.cast_array()\n    assert lst == [1, 2]\n    assert m.load_array(lst)\n\n    assert doc(m.cast_array) == ""cast_array() -> List[int[2]]""\n    assert doc(m.load_array) == ""load_array(arg0: List[int[2]]) -> bool""\n\n\ndef test_valarray(doc):\n    """"""std::valarray <-> list""""""\n    lst = m.cast_valarray()\n    assert lst == [1, 4, 9]\n    assert m.load_valarray(lst)\n\n    assert doc(m.cast_valarray) == ""cast_valarray() -> List[int]""\n    assert doc(m.load_valarray) == ""load_valarray(arg0: List[int]) -> bool""\n\n\ndef test_map(doc):\n    """"""std::map <-> dict""""""\n    d = m.cast_map()\n    assert d == {""key"": ""value""}\n    d[""key2""] = ""value2""\n    assert m.load_map(d)\n\n    assert doc(m.cast_map) == ""cast_map() -> Dict[str, str]""\n    assert doc(m.load_map) == ""load_map(arg0: Dict[str, str]) -> bool""\n\n\ndef test_set(doc):\n    """"""std::set <-> set""""""\n    s = m.cast_set()\n    assert s == {""key1"", ""key2""}\n    s.add(""key3"")\n    assert m.load_set(s)\n\n    assert doc(m.cast_set) == ""cast_set() -> Set[str]""\n    assert doc(m.load_set) == ""load_set(arg0: Set[str]) -> bool""\n\n\ndef test_recursive_casting():\n    """"""Tests that stl casters preserve lvalue/rvalue context for container values""""""\n    assert m.cast_rv_vector() == [""rvalue"", ""rvalue""]\n    assert m.cast_lv_vector() == [""lvalue"", ""lvalue""]\n    assert m.cast_rv_array() == [""rvalue"", ""rvalue"", ""rvalue""]\n    assert m.cast_lv_array() == [""lvalue"", ""lvalue""]\n    assert m.cast_rv_map() == {""a"": ""rvalue""}\n    assert m.cast_lv_map() == {""a"": ""lvalue"", ""b"": ""lvalue""}\n    assert m.cast_rv_nested() == [[[{""b"": ""rvalue"", ""c"": ""rvalue""}], [{""a"": ""rvalue""}]]]\n    assert m.cast_lv_nested() == {\n        ""a"": [[[""lvalue"", ""lvalue""]], [[""lvalue"", ""lvalue""]]],\n        ""b"": [[[""lvalue"", ""lvalue""], [""lvalue"", ""lvalue""]]]\n    }\n\n    # Issue #853 test case:\n    z = m.cast_unique_ptr_vector()\n    assert z[0].value == 7 and z[1].value == 42\n\n\ndef test_move_out_container():\n    """"""Properties use the `reference_internal` policy by default. If the underlying function\n    returns an rvalue, the policy is automatically changed to `move` to avoid referencing\n    a temporary. In case the return value is a container of user-defined types, the policy\n    also needs to be applied to the elements, not just the container.""""""\n    c = m.MoveOutContainer()\n    moved_out_list = c.move_list\n    assert [x.value for x in moved_out_list] == [0, 1, 2]\n\n\n@pytest.mark.skipif(not hasattr(m, ""has_optional""), reason=\'no <optional>\')\ndef test_optional():\n    assert m.double_or_zero(None) == 0\n    assert m.double_or_zero(42) == 84\n    pytest.raises(TypeError, m.double_or_zero, \'foo\')\n\n    assert m.half_or_none(0) is None\n    assert m.half_or_none(42) == 21\n    pytest.raises(TypeError, m.half_or_none, \'foo\')\n\n    assert m.test_nullopt() == 42\n    assert m.test_nullopt(None) == 42\n    assert m.test_nullopt(42) == 42\n    assert m.test_nullopt(43) == 43\n\n    assert m.test_no_assign() == 42\n    assert m.test_no_assign(None) == 42\n    assert m.test_no_assign(m.NoAssign(43)) == 43\n    pytest.raises(TypeError, m.test_no_assign, 43)\n\n    assert m.nodefer_none_optional(None)\n\n\n@pytest.mark.skipif(not hasattr(m, ""has_exp_optional""), reason=\'no <experimental/optional>\')\ndef test_exp_optional():\n    assert m.double_or_zero_exp(None) == 0\n    assert m.double_or_zero_exp(42) == 84\n    pytest.raises(TypeError, m.double_or_zero_exp, \'foo\')\n\n    assert m.half_or_none_exp(0) is None\n    assert m.half_or_none_exp(42) == 21\n    pytest.raises(TypeError, m.half_or_none_exp, \'foo\')\n\n    assert m.test_nullopt_exp() == 42\n    assert m.test_nullopt_exp(None) == 42\n    assert m.test_nullopt_exp(42) == 42\n    assert m.test_nullopt_exp(43) == 43\n\n    assert m.test_no_assign_exp() == 42\n    assert m.test_no_assign_exp(None) == 42\n    assert m.test_no_assign_exp(m.NoAssign(43)) == 43\n    pytest.raises(TypeError, m.test_no_assign_exp, 43)\n\n\n@pytest.mark.skipif(not hasattr(m, ""load_variant""), reason=\'no <variant>\')\ndef test_variant(doc):\n    assert m.load_variant(1) == ""int""\n    assert m.load_variant(""1"") == ""std::string""\n    assert m.load_variant(1.0) == ""double""\n    assert m.load_variant(None) == ""std::nullptr_t""\n\n    assert m.load_variant_2pass(1) == ""int""\n    assert m.load_variant_2pass(1.0) == ""double""\n\n    assert m.cast_variant() == (5, ""Hello"")\n\n    assert doc(m.load_variant) == ""load_variant(arg0: Union[int, str, float, None]) -> str""\n\n\ndef test_vec_of_reference_wrapper():\n    """"""#171: Can\'t return reference wrappers (or STL structures containing them)""""""\n    assert str(m.return_vec_of_reference_wrapper(UserType(4))) == \\\n        ""[UserType(1), UserType(2), UserType(3), UserType(4)]""\n\n\ndef test_stl_pass_by_pointer(msg):\n    """"""Passing nullptr or None to an STL container pointer is not expected to work""""""\n    with pytest.raises(TypeError) as excinfo:\n        m.stl_pass_by_pointer()  # default value is `nullptr`\n    assert msg(excinfo.value) == """"""\n        stl_pass_by_pointer(): incompatible function arguments. The following argument types are supported:\n            1. (v: List[int]=None) -> List[int]\n\n        Invoked with:\n    """"""  # noqa: E501 line too long\n\n    with pytest.raises(TypeError) as excinfo:\n        m.stl_pass_by_pointer(None)\n    assert msg(excinfo.value) == """"""\n        stl_pass_by_pointer(): incompatible function arguments. The following argument types are supported:\n            1. (v: List[int]=None) -> List[int]\n\n        Invoked with: None\n    """"""  # noqa: E501 line too long\n\n    assert m.stl_pass_by_pointer([1, 2, 3]) == [1, 2, 3]\n\n\ndef test_missing_header_message():\n    """"""Trying convert `list` to a `std::vector`, or vice versa, without including\n    <pybind11/stl.h> should result in a helpful suggestion in the error message""""""\n    import pybind11_cross_module_tests as cm\n\n    expected_message = (""Did you forget to `#include <pybind11/stl.h>`? Or <pybind11/complex.h>,\\n""\n                        ""<pybind11/functional.h>, <pybind11/chrono.h>, etc. Some automatic\\n""\n                        ""conversions are optional and require extra headers to be included\\n""\n                        ""when compiling your pybind11 module."")\n\n    with pytest.raises(TypeError) as excinfo:\n        cm.missing_header_arg([1.0, 2.0, 3.0])\n    assert expected_message in str(excinfo.value)\n\n    with pytest.raises(TypeError) as excinfo:\n        cm.missing_header_return()\n    assert expected_message in str(excinfo.value)\n\n\ndef test_stl_ownership():\n    cstats = ConstructorStats.get(m.Placeholder)\n    assert cstats.alive() == 0\n    r = m.test_stl_ownership()\n    assert len(r) == 1\n    del r\n    assert cstats.alive() == 0\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_stl_binders.py,0,"b'import pytest\nimport sys\nfrom pybind11_tests import stl_binders as m\n\nwith pytest.suppress(ImportError):\n    import numpy as np\n\n\ndef test_vector_int():\n    v_int = m.VectorInt([0, 0])\n    assert len(v_int) == 2\n    assert bool(v_int) is True\n\n    v_int2 = m.VectorInt([0, 0])\n    assert v_int == v_int2\n    v_int2[1] = 1\n    assert v_int != v_int2\n\n    v_int2.append(2)\n    v_int2.insert(0, 1)\n    v_int2.insert(0, 2)\n    v_int2.insert(0, 3)\n    v_int2.insert(6, 3)\n    assert str(v_int2) == ""VectorInt[3, 2, 1, 0, 1, 2, 3]""\n    with pytest.raises(IndexError):\n        v_int2.insert(8, 4)\n\n    v_int.append(99)\n    v_int2[2:-2] = v_int\n    assert v_int2 == m.VectorInt([3, 2, 0, 0, 99, 2, 3])\n    del v_int2[1:3]\n    assert v_int2 == m.VectorInt([3, 0, 99, 2, 3])\n    del v_int2[0]\n    assert v_int2 == m.VectorInt([0, 99, 2, 3])\n\n\n# related to the PyPy\'s buffer protocol.\n@pytest.unsupported_on_pypy\ndef test_vector_buffer():\n    b = bytearray([1, 2, 3, 4])\n    v = m.VectorUChar(b)\n    assert v[1] == 2\n    v[2] = 5\n    mv = memoryview(v)  # We expose the buffer interface\n    if sys.version_info.major > 2:\n        assert mv[2] == 5\n        mv[2] = 6\n    else:\n        assert mv[2] == \'\\x05\'\n        mv[2] = \'\\x06\'\n    assert v[2] == 6\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.create_undeclstruct()  # Undeclared struct contents, no buffer interface\n    assert ""NumPy type info missing for "" in str(excinfo.value)\n\n\n@pytest.unsupported_on_pypy\n@pytest.requires_numpy\ndef test_vector_buffer_numpy():\n    a = np.array([1, 2, 3, 4], dtype=np.int32)\n    with pytest.raises(TypeError):\n        m.VectorInt(a)\n\n    a = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]], dtype=np.uintc)\n    v = m.VectorInt(a[0, :])\n    assert len(v) == 4\n    assert v[2] == 3\n    ma = np.asarray(v)\n    ma[2] = 5\n    assert v[2] == 5\n\n    v = m.VectorInt(a[:, 1])\n    assert len(v) == 3\n    assert v[2] == 10\n\n    v = m.get_vectorstruct()\n    assert v[0].x == 5\n    ma = np.asarray(v)\n    ma[1][\'x\'] = 99\n    assert v[1].x == 99\n\n    v = m.VectorStruct(np.zeros(3, dtype=np.dtype([(\'w\', \'bool\'), (\'x\', \'I\'),\n                                                   (\'y\', \'float64\'), (\'z\', \'bool\')], align=True)))\n    assert len(v) == 3\n\n\ndef test_vector_bool():\n    import pybind11_cross_module_tests as cm\n\n    vv_c = cm.VectorBool()\n    for i in range(10):\n        vv_c.append(i % 2 == 0)\n    for i in range(10):\n        assert vv_c[i] == (i % 2 == 0)\n    assert str(vv_c) == ""VectorBool[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]""\n\n\ndef test_vector_custom():\n    v_a = m.VectorEl()\n    v_a.append(m.El(1))\n    v_a.append(m.El(2))\n    assert str(v_a) == ""VectorEl[El{1}, El{2}]""\n\n    vv_a = m.VectorVectorEl()\n    vv_a.append(v_a)\n    vv_b = vv_a[0]\n    assert str(vv_b) == ""VectorEl[El{1}, El{2}]""\n\n\ndef test_map_string_double():\n    mm = m.MapStringDouble()\n    mm[\'a\'] = 1\n    mm[\'b\'] = 2.5\n\n    assert list(mm) == [\'a\', \'b\']\n    assert list(mm.items()) == [(\'a\', 1), (\'b\', 2.5)]\n    assert str(mm) == ""MapStringDouble{a: 1, b: 2.5}""\n\n    um = m.UnorderedMapStringDouble()\n    um[\'ua\'] = 1.1\n    um[\'ub\'] = 2.6\n\n    assert sorted(list(um)) == [\'ua\', \'ub\']\n    assert sorted(list(um.items())) == [(\'ua\', 1.1), (\'ub\', 2.6)]\n    assert ""UnorderedMapStringDouble"" in str(um)\n\n\ndef test_map_string_double_const():\n    mc = m.MapStringDoubleConst()\n    mc[\'a\'] = 10\n    mc[\'b\'] = 20.5\n    assert str(mc) == ""MapStringDoubleConst{a: 10, b: 20.5}""\n\n    umc = m.UnorderedMapStringDoubleConst()\n    umc[\'a\'] = 11\n    umc[\'b\'] = 21.5\n\n    str(umc)\n\n\ndef test_noncopyable_containers():\n    # std::vector\n    vnc = m.get_vnc(5)\n    for i in range(0, 5):\n        assert vnc[i].value == i + 1\n\n    for i, j in enumerate(vnc, start=1):\n        assert j.value == i\n\n    # std::deque\n    dnc = m.get_dnc(5)\n    for i in range(0, 5):\n        assert dnc[i].value == i + 1\n\n    i = 1\n    for j in dnc:\n        assert(j.value == i)\n        i += 1\n\n    # std::map\n    mnc = m.get_mnc(5)\n    for i in range(1, 6):\n        assert mnc[i].value == 10 * i\n\n    vsum = 0\n    for k, v in mnc.items():\n        assert v.value == 10 * k\n        vsum += v.value\n\n    assert vsum == 150\n\n    # std::unordered_map\n    mnc = m.get_umnc(5)\n    for i in range(1, 6):\n        assert mnc[i].value == 10 * i\n\n    vsum = 0\n    for k, v in mnc.items():\n        assert v.value == 10 * k\n        vsum += v.value\n\n    assert vsum == 150\n\n\ndef test_map_delitem():\n    mm = m.MapStringDouble()\n    mm[\'a\'] = 1\n    mm[\'b\'] = 2.5\n\n    assert list(mm) == [\'a\', \'b\']\n    assert list(mm.items()) == [(\'a\', 1), (\'b\', 2.5)]\n    del mm[\'a\']\n    assert list(mm) == [\'b\']\n    assert list(mm.items()) == [(\'b\', 2.5)]\n\n    um = m.UnorderedMapStringDouble()\n    um[\'ua\'] = 1.1\n    um[\'ub\'] = 2.6\n\n    assert sorted(list(um)) == [\'ua\', \'ub\']\n    assert sorted(list(um.items())) == [(\'ua\', 1.1), (\'ub\', 2.6)]\n    del um[\'ua\']\n    assert sorted(list(um)) == [\'ub\']\n    assert sorted(list(um.items())) == [(\'ub\', 2.6)]\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_virtual_functions.py,0,"b'import pytest\n\nfrom pybind11_tests import virtual_functions as m\nfrom pybind11_tests import ConstructorStats\n\n\ndef test_override(capture, msg):\n    class ExtendedExampleVirt(m.ExampleVirt):\n        def __init__(self, state):\n            super(ExtendedExampleVirt, self).__init__(state + 1)\n            self.data = ""Hello world""\n\n        def run(self, value):\n            print(\'ExtendedExampleVirt::run(%i), calling parent..\' % value)\n            return super(ExtendedExampleVirt, self).run(value + 1)\n\n        def run_bool(self):\n            print(\'ExtendedExampleVirt::run_bool()\')\n            return False\n\n        def get_string1(self):\n            return ""override1""\n\n        def pure_virtual(self):\n            print(\'ExtendedExampleVirt::pure_virtual(): %s\' % self.data)\n\n    class ExtendedExampleVirt2(ExtendedExampleVirt):\n        def __init__(self, state):\n            super(ExtendedExampleVirt2, self).__init__(state + 1)\n\n        def get_string2(self):\n            return ""override2""\n\n    ex12 = m.ExampleVirt(10)\n    with capture:\n        assert m.runExampleVirt(ex12, 20) == 30\n    assert capture == """"""\n        Original implementation of ExampleVirt::run(state=10, value=20, str1=default1, str2=default2)\n    """"""  # noqa: E501 line too long\n\n    with pytest.raises(RuntimeError) as excinfo:\n        m.runExampleVirtVirtual(ex12)\n    assert msg(excinfo.value) == \'Tried to call pure virtual function ""ExampleVirt::pure_virtual""\'\n\n    ex12p = ExtendedExampleVirt(10)\n    with capture:\n        assert m.runExampleVirt(ex12p, 20) == 32\n    assert capture == """"""\n        ExtendedExampleVirt::run(20), calling parent..\n        Original implementation of ExampleVirt::run(state=11, value=21, str1=override1, str2=default2)\n    """"""  # noqa: E501 line too long\n    with capture:\n        assert m.runExampleVirtBool(ex12p) is False\n    assert capture == ""ExtendedExampleVirt::run_bool()""\n    with capture:\n        m.runExampleVirtVirtual(ex12p)\n    assert capture == ""ExtendedExampleVirt::pure_virtual(): Hello world""\n\n    ex12p2 = ExtendedExampleVirt2(15)\n    with capture:\n        assert m.runExampleVirt(ex12p2, 50) == 68\n    assert capture == """"""\n        ExtendedExampleVirt::run(50), calling parent..\n        Original implementation of ExampleVirt::run(state=17, value=51, str1=override1, str2=override2)\n    """"""  # noqa: E501 line too long\n\n    cstats = ConstructorStats.get(m.ExampleVirt)\n    assert cstats.alive() == 3\n    del ex12, ex12p, ex12p2\n    assert cstats.alive() == 0\n    assert cstats.values() == [\'10\', \'11\', \'17\']\n    assert cstats.copy_constructions == 0\n    assert cstats.move_constructions >= 0\n\n\ndef test_alias_delay_initialization1(capture):\n    """"""`A` only initializes its trampoline class when we inherit from it\n\n    If we just create and use an A instance directly, the trampoline initialization is\n    bypassed and we only initialize an A() instead (for performance reasons).\n    """"""\n    class B(m.A):\n        def __init__(self):\n            super(B, self).__init__()\n\n        def f(self):\n            print(""In python f()"")\n\n    # C++ version\n    with capture:\n        a = m.A()\n        m.call_f(a)\n        del a\n        pytest.gc_collect()\n    assert capture == ""A.f()""\n\n    # Python version\n    with capture:\n        b = B()\n        m.call_f(b)\n        del b\n        pytest.gc_collect()\n    assert capture == """"""\n        PyA.PyA()\n        PyA.f()\n        In python f()\n        PyA.~PyA()\n    """"""\n\n\ndef test_alias_delay_initialization2(capture):\n    """"""`A2`, unlike the above, is configured to always initialize the alias\n\n    While the extra initialization and extra class layer has small virtual dispatch\n    performance penalty, it also allows us to do more things with the trampoline\n    class such as defining local variables and performing construction/destruction.\n    """"""\n    class B2(m.A2):\n        def __init__(self):\n            super(B2, self).__init__()\n\n        def f(self):\n            print(""In python B2.f()"")\n\n    # No python subclass version\n    with capture:\n        a2 = m.A2()\n        m.call_f(a2)\n        del a2\n        pytest.gc_collect()\n        a3 = m.A2(1)\n        m.call_f(a3)\n        del a3\n        pytest.gc_collect()\n    assert capture == """"""\n        PyA2.PyA2()\n        PyA2.f()\n        A2.f()\n        PyA2.~PyA2()\n        PyA2.PyA2()\n        PyA2.f()\n        A2.f()\n        PyA2.~PyA2()\n    """"""\n\n    # Python subclass version\n    with capture:\n        b2 = B2()\n        m.call_f(b2)\n        del b2\n        pytest.gc_collect()\n    assert capture == """"""\n        PyA2.PyA2()\n        PyA2.f()\n        In python B2.f()\n        PyA2.~PyA2()\n    """"""\n\n\n# PyPy: Reference count > 1 causes call with noncopyable instance\n# to fail in ncv1.print_nc()\n@pytest.unsupported_on_pypy\n@pytest.mark.skipif(not hasattr(m, ""NCVirt""), reason=""NCVirt test broken on ICPC"")\ndef test_move_support():\n    class NCVirtExt(m.NCVirt):\n        def get_noncopyable(self, a, b):\n            # Constructs and returns a new instance:\n            nc = m.NonCopyable(a * a, b * b)\n            return nc\n\n        def get_movable(self, a, b):\n            # Return a referenced copy\n            self.movable = m.Movable(a, b)\n            return self.movable\n\n    class NCVirtExt2(m.NCVirt):\n        def get_noncopyable(self, a, b):\n            # Keep a reference: this is going to throw an exception\n            self.nc = m.NonCopyable(a, b)\n            return self.nc\n\n        def get_movable(self, a, b):\n            # Return a new instance without storing it\n            return m.Movable(a, b)\n\n    ncv1 = NCVirtExt()\n    assert ncv1.print_nc(2, 3) == ""36""\n    assert ncv1.print_movable(4, 5) == ""9""\n    ncv2 = NCVirtExt2()\n    assert ncv2.print_movable(7, 7) == ""14""\n    # Don\'t check the exception message here because it differs under debug/non-debug mode\n    with pytest.raises(RuntimeError):\n        ncv2.print_nc(9, 9)\n\n    nc_stats = ConstructorStats.get(m.NonCopyable)\n    mv_stats = ConstructorStats.get(m.Movable)\n    assert nc_stats.alive() == 1\n    assert mv_stats.alive() == 1\n    del ncv1, ncv2\n    assert nc_stats.alive() == 0\n    assert mv_stats.alive() == 0\n    assert nc_stats.values() == [\'4\', \'9\', \'9\', \'9\']\n    assert mv_stats.values() == [\'4\', \'5\', \'7\', \'7\']\n    assert nc_stats.copy_constructions == 0\n    assert mv_stats.copy_constructions == 1\n    assert nc_stats.move_constructions >= 0\n    assert mv_stats.move_constructions >= 0\n\n\ndef test_dispatch_issue(msg):\n    """"""#159: virtual function dispatch has problems with similar-named functions""""""\n    class PyClass1(m.DispatchIssue):\n        def dispatch(self):\n            return ""Yay..""\n\n    class PyClass2(m.DispatchIssue):\n        def dispatch(self):\n            with pytest.raises(RuntimeError) as excinfo:\n                super(PyClass2, self).dispatch()\n            assert msg(excinfo.value) == \'Tried to call pure virtual function ""Base::dispatch""\'\n\n            p = PyClass1()\n            return m.dispatch_issue_go(p)\n\n    b = PyClass2()\n    assert m.dispatch_issue_go(b) == ""Yay..""\n\n\ndef test_override_ref():\n    """"""#392/397: overriding reference-returning functions""""""\n    o = m.OverrideTest(""asdf"")\n\n    # Not allowed (see associated .cpp comment)\n    # i = o.str_ref()\n    # assert o.str_ref() == ""asdf""\n    assert o.str_value() == ""asdf""\n\n    assert o.A_value().value == ""hi""\n    a = o.A_ref()\n    assert a.value == ""hi""\n    a.value = ""bye""\n    assert a.value == ""bye""\n\n\ndef test_inherited_virtuals():\n    class AR(m.A_Repeat):\n        def unlucky_number(self):\n            return 99\n\n    class AT(m.A_Tpl):\n        def unlucky_number(self):\n            return 999\n\n    obj = AR()\n    assert obj.say_something(3) == ""hihihi""\n    assert obj.unlucky_number() == 99\n    assert obj.say_everything() == ""hi 99""\n\n    obj = AT()\n    assert obj.say_something(3) == ""hihihi""\n    assert obj.unlucky_number() == 999\n    assert obj.say_everything() == ""hi 999""\n\n    for obj in [m.B_Repeat(), m.B_Tpl()]:\n        assert obj.say_something(3) == ""B says hi 3 times""\n        assert obj.unlucky_number() == 13\n        assert obj.lucky_number() == 7.0\n        assert obj.say_everything() == ""B says hi 1 times 13""\n\n    for obj in [m.C_Repeat(), m.C_Tpl()]:\n        assert obj.say_something(3) == ""B says hi 3 times""\n        assert obj.unlucky_number() == 4444\n        assert obj.lucky_number() == 888.0\n        assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    class CR(m.C_Repeat):\n        def lucky_number(self):\n            return m.C_Repeat.lucky_number(self) + 1.25\n\n    obj = CR()\n    assert obj.say_something(3) == ""B says hi 3 times""\n    assert obj.unlucky_number() == 4444\n    assert obj.lucky_number() == 889.25\n    assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    class CT(m.C_Tpl):\n        pass\n\n    obj = CT()\n    assert obj.say_something(3) == ""B says hi 3 times""\n    assert obj.unlucky_number() == 4444\n    assert obj.lucky_number() == 888.0\n    assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    class CCR(CR):\n        def lucky_number(self):\n            return CR.lucky_number(self) * 10\n\n    obj = CCR()\n    assert obj.say_something(3) == ""B says hi 3 times""\n    assert obj.unlucky_number() == 4444\n    assert obj.lucky_number() == 8892.5\n    assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    class CCT(CT):\n        def lucky_number(self):\n            return CT.lucky_number(self) * 1000\n\n    obj = CCT()\n    assert obj.say_something(3) == ""B says hi 3 times""\n    assert obj.unlucky_number() == 4444\n    assert obj.lucky_number() == 888000.0\n    assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    class DR(m.D_Repeat):\n        def unlucky_number(self):\n            return 123\n\n        def lucky_number(self):\n            return 42.0\n\n    for obj in [m.D_Repeat(), m.D_Tpl()]:\n        assert obj.say_something(3) == ""B says hi 3 times""\n        assert obj.unlucky_number() == 4444\n        assert obj.lucky_number() == 888.0\n        assert obj.say_everything() == ""B says hi 1 times 4444""\n\n    obj = DR()\n    assert obj.say_something(3) == ""B says hi 3 times""\n    assert obj.unlucky_number() == 123\n    assert obj.lucky_number() == 42.0\n    assert obj.say_everything() == ""B says hi 1 times 123""\n\n    class DT(m.D_Tpl):\n        def say_something(self, times):\n            return ""DT says:"" + (\' quack\' * times)\n\n        def unlucky_number(self):\n            return 1234\n\n        def lucky_number(self):\n            return -4.25\n\n    obj = DT()\n    assert obj.say_something(3) == ""DT says: quack quack quack""\n    assert obj.unlucky_number() == 1234\n    assert obj.lucky_number() == -4.25\n    assert obj.say_everything() == ""DT says: quack 1234""\n\n    class DT2(DT):\n        def say_something(self, times):\n            return ""DT2: "" + (\'QUACK\' * times)\n\n        def unlucky_number(self):\n            return -3\n\n    class BT(m.B_Tpl):\n        def say_something(self, times):\n            return ""BT"" * times\n\n        def unlucky_number(self):\n            return -7\n\n        def lucky_number(self):\n            return -1.375\n\n    obj = BT()\n    assert obj.say_something(3) == ""BTBTBT""\n    assert obj.unlucky_number() == -7\n    assert obj.lucky_number() == -1.375\n    assert obj.say_everything() == ""BT -7""\n\n\ndef test_issue_1454():\n    # Fix issue #1454 (crash when acquiring/releasing GIL on another thread in Python 2.7)\n    m.test_gil()\n    m.test_gil_from_thread()\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tools/libsize.py,0,"b'from __future__ import print_function, division\nimport os\nimport sys\n\n# Internal build script for generating debugging test .so size.\n# Usage:\n#     python libsize.py file.so save.txt -- displays the size of file.so and, if save.txt exists, compares it to the\n#                                           size in it, then overwrites save.txt with the new size for future runs.\n\nif len(sys.argv) != 3:\n    sys.exit(""Invalid arguments: usage: python libsize.py file.so save.txt"")\n\nlib = sys.argv[1]\nsave = sys.argv[2]\n\nif not os.path.exists(lib):\n    sys.exit(""Error: requested file ({}) does not exist"".format(lib))\n\nlibsize = os.path.getsize(lib)\n\nprint(""------"", os.path.basename(lib), ""file size:"", libsize, end=\'\')\n\nif os.path.exists(save):\n    with open(save) as sf:\n        oldsize = int(sf.readline())\n\n    if oldsize > 0:\n        change = libsize - oldsize\n        if change == 0:\n            print("" (no change)"")\n        else:\n            print("" (change of {:+} bytes = {:+.2%})"".format(change, change / oldsize))\nelse:\n    print()\n\nwith open(save, \'w\') as sf:\n    sf.write(str(libsize))\n\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tools/mkdoc.py,0,"b'#!/usr/bin/env python3\n#\n#  Syntax: mkdoc.py [-I<path> ..] [.. a list of header files ..]\n#\n#  Extract documentation from C++ header files to use it in Python bindings\n#\n\nimport os\nimport sys\nimport platform\nimport re\nimport textwrap\n\nfrom clang import cindex\nfrom clang.cindex import CursorKind\nfrom collections import OrderedDict\nfrom threading import Thread, Semaphore\nfrom multiprocessing import cpu_count\n\nRECURSE_LIST = [\n    CursorKind.TRANSLATION_UNIT,\n    CursorKind.NAMESPACE,\n    CursorKind.CLASS_DECL,\n    CursorKind.STRUCT_DECL,\n    CursorKind.ENUM_DECL,\n    CursorKind.CLASS_TEMPLATE\n]\n\nPRINT_LIST = [\n    CursorKind.CLASS_DECL,\n    CursorKind.STRUCT_DECL,\n    CursorKind.ENUM_DECL,\n    CursorKind.ENUM_CONSTANT_DECL,\n    CursorKind.CLASS_TEMPLATE,\n    CursorKind.FUNCTION_DECL,\n    CursorKind.FUNCTION_TEMPLATE,\n    CursorKind.CONVERSION_FUNCTION,\n    CursorKind.CXX_METHOD,\n    CursorKind.CONSTRUCTOR,\n    CursorKind.FIELD_DECL\n]\n\nCPP_OPERATORS = {\n    \'<=\': \'le\', \'>=\': \'ge\', \'==\': \'eq\', \'!=\': \'ne\', \'[]\': \'array\',\n    \'+=\': \'iadd\', \'-=\': \'isub\', \'*=\': \'imul\', \'/=\': \'idiv\', \'%=\':\n    \'imod\', \'&=\': \'iand\', \'|=\': \'ior\', \'^=\': \'ixor\', \'<<=\': \'ilshift\',\n    \'>>=\': \'irshift\', \'++\': \'inc\', \'--\': \'dec\', \'<<\': \'lshift\', \'>>\':\n    \'rshift\', \'&&\': \'land\', \'||\': \'lor\', \'!\': \'lnot\', \'~\': \'bnot\',\n    \'&\': \'band\', \'|\': \'bor\', \'+\': \'add\', \'-\': \'sub\', \'*\': \'mul\', \'/\':\n    \'div\', \'%\': \'mod\', \'<\': \'lt\', \'>\': \'gt\', \'=\': \'assign\', \'()\': \'call\'\n}\n\nCPP_OPERATORS = OrderedDict(\n    sorted(CPP_OPERATORS.items(), key=lambda t: -len(t[0])))\n\njob_count = cpu_count()\njob_semaphore = Semaphore(job_count)\n\noutput = []\n\ndef d(s):\n    return s.decode(\'utf8\')\n\n\ndef sanitize_name(name):\n    name = re.sub(r\'type-parameter-0-([0-9]+)\', r\'T\\1\', name)\n    for k, v in CPP_OPERATORS.items():\n        name = name.replace(\'operator%s\' % k, \'operator_%s\' % v)\n    name = re.sub(\'<.*>\', \'\', name)\n    name = \'\'.join([ch if ch.isalnum() else \'_\' for ch in name])\n    name = re.sub(\'_$\', \'\', re.sub(\'_+\', \'_\', name))\n    return \'__doc_\' + name\n\n\ndef process_comment(comment):\n    result = \'\'\n\n    # Remove C++ comment syntax\n    leading_spaces = float(\'inf\')\n    for s in comment.expandtabs(tabsize=4).splitlines():\n        s = s.strip()\n        if s.startswith(\'/*\'):\n            s = s[2:].lstrip(\'*\')\n        elif s.endswith(\'*/\'):\n            s = s[:-2].rstrip(\'*\')\n        elif s.startswith(\'///\'):\n            s = s[3:]\n        if s.startswith(\'*\'):\n            s = s[1:]\n        if len(s) > 0:\n            leading_spaces = min(leading_spaces, len(s) - len(s.lstrip()))\n        result += s + \'\\n\'\n\n    if leading_spaces != float(\'inf\'):\n        result2 = """"\n        for s in result.splitlines():\n            result2 += s[leading_spaces:] + \'\\n\'\n        result = result2\n\n    # Doxygen tags\n    cpp_group = \'([\\w:]+)\'\n    param_group = \'([\\[\\w:\\]]+)\'\n\n    s = result\n    s = re.sub(r\'\\\\c\\s+%s\' % cpp_group, r\'``\\1``\', s)\n    s = re.sub(r\'\\\\a\\s+%s\' % cpp_group, r\'*\\1*\', s)\n    s = re.sub(r\'\\\\e\\s+%s\' % cpp_group, r\'*\\1*\', s)\n    s = re.sub(r\'\\\\em\\s+%s\' % cpp_group, r\'*\\1*\', s)\n    s = re.sub(r\'\\\\b\\s+%s\' % cpp_group, r\'**\\1**\', s)\n    s = re.sub(r\'\\\\ingroup\\s+%s\' % cpp_group, r\'\', s)\n    s = re.sub(r\'\\\\param%s?\\s+%s\' % (param_group, cpp_group),\n               r\'\\n\\n$Parameter ``\\2``:\\n\\n\', s)\n    s = re.sub(r\'\\\\tparam%s?\\s+%s\' % (param_group, cpp_group),\n               r\'\\n\\n$Template parameter ``\\2``:\\n\\n\', s)\n\n    for in_, out_ in {\n        \'return\': \'Returns\',\n        \'author\': \'Author\',\n        \'authors\': \'Authors\',\n        \'copyright\': \'Copyright\',\n        \'date\': \'Date\',\n        \'remark\': \'Remark\',\n        \'sa\': \'See also\',\n        \'see\': \'See also\',\n        \'extends\': \'Extends\',\n        \'throw\': \'Throws\',\n        \'throws\': \'Throws\'\n    }.items():\n        s = re.sub(r\'\\\\%s\\s*\' % in_, r\'\\n\\n$%s:\\n\\n\' % out_, s)\n\n    s = re.sub(r\'\\\\details\\s*\', r\'\\n\\n\', s)\n    s = re.sub(r\'\\\\brief\\s*\', r\'\', s)\n    s = re.sub(r\'\\\\short\\s*\', r\'\', s)\n    s = re.sub(r\'\\\\ref\\s*\', r\'\', s)\n\n    s = re.sub(r\'\\\\code\\s?(.*?)\\s?\\\\endcode\',\n               r""```\\n\\1\\n```\\n"", s, flags=re.DOTALL)\n\n    # HTML/TeX tags\n    s = re.sub(r\'<tt>(.*?)</tt>\', r\'``\\1``\', s, flags=re.DOTALL)\n    s = re.sub(r\'<pre>(.*?)</pre>\', r""```\\n\\1\\n```\\n"", s, flags=re.DOTALL)\n    s = re.sub(r\'<em>(.*?)</em>\', r\'*\\1*\', s, flags=re.DOTALL)\n    s = re.sub(r\'<b>(.*?)</b>\', r\'**\\1**\', s, flags=re.DOTALL)\n    s = re.sub(r\'\\\\f\\$(.*?)\\\\f\\$\', r\'$\\1$\', s, flags=re.DOTALL)\n    s = re.sub(r\'<li>\', r\'\\n\\n* \', s)\n    s = re.sub(r\'</?ul>\', r\'\', s)\n    s = re.sub(r\'</li>\', r\'\\n\\n\', s)\n\n    s = s.replace(\'``true``\', \'``True``\')\n    s = s.replace(\'``false``\', \'``False``\')\n\n    # Re-flow text\n    wrapper = textwrap.TextWrapper()\n    wrapper.expand_tabs = True\n    wrapper.replace_whitespace = True\n    wrapper.drop_whitespace = True\n    wrapper.width = 70\n    wrapper.initial_indent = wrapper.subsequent_indent = \'\'\n\n    result = \'\'\n    in_code_segment = False\n    for x in re.split(r\'(```)\', s):\n        if x == \'```\':\n            if not in_code_segment:\n                result += \'```\\n\'\n            else:\n                result += \'\\n```\\n\\n\'\n            in_code_segment = not in_code_segment\n        elif in_code_segment:\n            result += x.strip()\n        else:\n            for y in re.split(r\'(?: *\\n *){2,}\', x):\n                wrapped = wrapper.fill(re.sub(r\'\\s+\', \' \', y).strip())\n                if len(wrapped) > 0 and wrapped[0] == \'$\':\n                    result += wrapped[1:] + \'\\n\'\n                    wrapper.initial_indent = \\\n                        wrapper.subsequent_indent = \' \' * 4\n                else:\n                    if len(wrapped) > 0:\n                        result += wrapped + \'\\n\\n\'\n                    wrapper.initial_indent = wrapper.subsequent_indent = \'\'\n    return result.rstrip().lstrip(\'\\n\')\n\n\ndef extract(filename, node, prefix):\n    if not (node.location.file is None or\n            os.path.samefile(d(node.location.file.name), filename)):\n        return 0\n    if node.kind in RECURSE_LIST:\n        sub_prefix = prefix\n        if node.kind != CursorKind.TRANSLATION_UNIT:\n            if len(sub_prefix) > 0:\n                sub_prefix += \'_\'\n            sub_prefix += d(node.spelling)\n        for i in node.get_children():\n            extract(filename, i, sub_prefix)\n    if node.kind in PRINT_LIST:\n        comment = d(node.raw_comment) if node.raw_comment is not None else \'\'\n        comment = process_comment(comment)\n        sub_prefix = prefix\n        if len(sub_prefix) > 0:\n            sub_prefix += \'_\'\n        if len(node.spelling) > 0:\n            name = sanitize_name(sub_prefix + d(node.spelling))\n            global output\n            output.append((name, filename, comment))\n\n\nclass ExtractionThread(Thread):\n    def __init__(self, filename, parameters):\n        Thread.__init__(self)\n        self.filename = filename\n        self.parameters = parameters\n        job_semaphore.acquire()\n\n    def run(self):\n        print(\'Processing ""%s"" ..\' % self.filename, file=sys.stderr)\n        try:\n            index = cindex.Index(\n                cindex.conf.lib.clang_createIndex(False, True))\n            tu = index.parse(self.filename, self.parameters)\n            extract(self.filename, tu.cursor, \'\')\n        finally:\n            job_semaphore.release()\n\nif __name__ == \'__main__\':\n    parameters = [\'-x\', \'c++\', \'-std=c++11\']\n    filenames = []\n\n    if platform.system() == \'Darwin\':\n        dev_path = \'/Applications/Xcode.app/Contents/Developer/\'\n        lib_dir = dev_path + \'Toolchains/XcodeDefault.xctoolchain/usr/lib/\'\n        sdk_dir = dev_path + \'Platforms/MacOSX.platform/Developer/SDKs\'\n        libclang = lib_dir + \'libclang.dylib\'\n\n        if os.path.exists(libclang):\n            cindex.Config.set_library_path(os.path.dirname(libclang))\n\n        if os.path.exists(sdk_dir):\n            sysroot_dir = os.path.join(sdk_dir, next(os.walk(sdk_dir))[1][0])\n            parameters.append(\'-isysroot\')\n            parameters.append(sysroot_dir)\n\n    for item in sys.argv[1:]:\n        if item.startswith(\'-\'):\n            parameters.append(item)\n        else:\n            filenames.append(item)\n\n    if len(filenames) == 0:\n        print(\'Syntax: %s [.. a list of header files ..]\' % sys.argv[0])\n        exit(-1)\n\n    print(\'\'\'/*\n  This file contains docstrings for the Python bindings.\n  Do not edit! These were automatically extracted by mkdoc.py\n */\n\n#define __EXPAND(x)                                      x\n#define __COUNT(_1, _2, _3, _4, _5, _6, _7, COUNT, ...)  COUNT\n#define __VA_SIZE(...)                                   __EXPAND(__COUNT(__VA_ARGS__, 7, 6, 5, 4, 3, 2, 1))\n#define __CAT1(a, b)                                     a ## b\n#define __CAT2(a, b)                                     __CAT1(a, b)\n#define __DOC1(n1)                                       __doc_##n1\n#define __DOC2(n1, n2)                                   __doc_##n1##_##n2\n#define __DOC3(n1, n2, n3)                               __doc_##n1##_##n2##_##n3\n#define __DOC4(n1, n2, n3, n4)                           __doc_##n1##_##n2##_##n3##_##n4\n#define __DOC5(n1, n2, n3, n4, n5)                       __doc_##n1##_##n2##_##n3##_##n4##_##n5\n#define __DOC6(n1, n2, n3, n4, n5, n6)                   __doc_##n1##_##n2##_##n3##_##n4##_##n5##_##n6\n#define __DOC7(n1, n2, n3, n4, n5, n6, n7)               __doc_##n1##_##n2##_##n3##_##n4##_##n5##_##n6##_##n7\n#define DOC(...)                                         __EXPAND(__EXPAND(__CAT2(__DOC, __VA_SIZE(__VA_ARGS__)))(__VA_ARGS__))\n\n#if defined(__GNUG__)\n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored ""-Wunused-variable""\n#endif\n\'\'\')\n\n    output.clear()\n    for filename in filenames:\n        thr = ExtractionThread(filename, parameters)\n        thr.start()\n\n    print(\'Waiting for jobs to finish ..\', file=sys.stderr)\n    for i in range(job_count):\n        job_semaphore.acquire()\n\n    name_ctr = 1\n    name_prev = None\n    for name, _, comment in list(sorted(output, key=lambda x: (x[0], x[1]))):\n        if name == name_prev:\n            name_ctr += 1\n            name = name + ""_%i"" % name_ctr\n        else:\n            name_prev = name\n            name_ctr = 1\n        print(\'\\nstatic const char *%s =%sR""doc(%s)doc"";\' %\n              (name, \'\\n\' if \'\\n\' in comment else \' \', comment))\n\n    print(\'\'\'\n#if defined(__GNUG__)\n#pragma GCC diagnostic pop\n#endif\n\'\'\')\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_cmake_build/test.py,0,"b'import sys\nimport test_cmake_build\n\nassert test_cmake_build.add(1, 2) == 3\nprint(""{} imports, runs, and adds: 1 + 2 = 3"".format(sys.argv[1]))\n'"
flink-ml-framework/python/lib/pybind11-2.2.4/tests/test_embed/test_interpreter.py,0,"b'from widget_module import Widget\n\n\nclass DerivedWidget(Widget):\n    def __init__(self, message):\n        super(DerivedWidget, self).__init__(message)\n\n    def the_answer(self):\n        return 42\n'"
