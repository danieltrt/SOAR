file_path,api_count,code
setup.py,0,"b""import os\nfrom setuptools import setup, find_packages\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname)).read()\n\nsetup(\n    name='kgp',\n    version='0.3.2',\n    packages=find_packages(),\n    description='GP models for Keras.',\n    long_description=read('README.md'),\n    author='Maruan Al-Shedivat',\n    author_email='maruan@alshedivat.com',\n    url='https://github.com/alshedivat/keras-gp',\n    license='MIT',\n    install_requires=[\n        'numpy>=1.11', 'keras==2.1.3', 'tensorflow>=1.0', 'pyyaml', 'six',\n    ],\n    extras_require = {\n        'matlab_engine':  ['matlab'],\n        'octave_engine':  ['oct2py==3.9.2'],\n    },\n    package_data = {\n        '': ['*.yaml', '*.m'],\n    }\n)\n"""
examples/gp_lstm_actuator.py,0,"b'""""""\nGP-LSTM regression on Actuator data.\n""""""\nfrom __future__ import print_function\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.optimizers import Adagrad, Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\n# Dataset interfaces\nfrom kgp.datasets.sysid import load_data\nfrom kgp.datasets.data_utils import data_to_seq, standardize_data\n\n# Model assembling and executing\nfrom kgp.utils.assemble import load_NN_configs, load_GP_configs, assemble\nfrom kgp.utils.experiment import train\n\n# Metrics & losses\nfrom kgp.losses import gen_gp_loss\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef main():\n    # Load data\n    X, y = load_data(\'actuator\', use_targets=False)\n    X_seq, y_seq = data_to_seq(X, y,\n        t_lag=32, t_future_shift=1, t_future_steps=1, t_sw_step=1)\n\n    # Split\n    train_end = int((45. / 100.) * len(X_seq))\n    test_end = int((90. / 100.) * len(X_seq))\n    X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n    X_test, y_test = X_seq[train_end:test_end], y_seq[train_end:test_end]\n    X_valid, y_valid = X_seq[test_end:], y_seq[test_end:]\n\n    data = {\n        \'train\': [X_train, y_train],\n        \'valid\': [X_valid, y_valid],\n        \'test\': [X_test, y_test],\n    }\n\n    # Re-format targets\n    for set_name in data:\n        y = data[set_name][1]\n        y = y.reshape((-1, 1, np.prod(y.shape[1:])))\n        data[set_name][1] = [y[:,:,i] for i in xrange(y.shape[2])]\n\n    # Model & training parameters\n    nb_train_samples = data[\'train\'][0].shape[0]\n    input_shape = list(data[\'train\'][0].shape[1:])\n    nb_outputs = len(data[\'train\'][1])\n    gp_input_shape = (1,)\n    batch_size = 128\n    epochs = 5\n\n    nn_params = {\n        \'H_dim\': 16,\n        \'H_activation\': \'tanh\',\n        \'dropout\': 0.1,\n    }\n    gp_params = {\n        \'cov\': \'SEiso\',\n        \'hyp_lik\': -2.0,\n        \'hyp_cov\': [[-0.7], [0.0]],\n        \'opt\': {},\n    }\n\n    # Retrieve model config\n    nn_configs = load_NN_configs(filename=\'lstm.yaml\',\n                                 input_shape=input_shape,\n                                 output_shape=gp_input_shape,\n                                 params=nn_params)\n    gp_configs = load_GP_configs(filename=\'gp.yaml\',\n                                 nb_outputs=nb_outputs,\n                                 batch_size=batch_size,\n                                 nb_train_samples=nb_train_samples,\n                                 params=gp_params)\n\n    # Construct & compile the model\n    model = assemble(\'GP-LSTM\', [nn_configs[\'1H\'], gp_configs[\'GP\']])\n    loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n    model.compile(optimizer=Adam(1e-2), loss=loss)\n\n    # Callbacks\n    callbacks = [EarlyStopping(monitor=\'val_mse\', patience=10)]\n\n    # Train the model\n    history = train(model, data, callbacks=callbacks, gp_n_iter=5,\n                    checkpoint=\'lstm\', checkpoint_monitor=\'val_mse\',\n                    epochs=epochs, batch_size=batch_size, verbose=2)\n\n    # Finetune the model\n    model.finetune(*data[\'train\'],\n                   batch_size=batch_size,\n                   gp_n_iter=100,\n                   verbose=0)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test predict RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/lstm_actuator.py,0,"b'""""""\nLSTM regression on Actuator data.\n""""""\nfrom __future__ import print_function\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.optimizers import Adagrad, Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\n# Dataset interfaces\nfrom kgp.datasets.sysid import load_data\nfrom kgp.datasets.data_utils import data_to_seq, standardize_data\n\n# Model assembling and executing\nfrom kgp.utils.assemble import load_NN_configs, assemble\nfrom kgp.utils.experiment import train\n\n# Metrics\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef main():\n    # Load data\n    X, y = load_data(\'actuator\', use_targets=False)\n    X_seq, y_seq = data_to_seq(X, y,\n        t_lag=32, t_future_shift=1, t_future_steps=1, t_sw_step=1)\n\n    # Split\n    train_end = int((45. / 100.) * len(X_seq))\n    test_end = int((90. / 100.) * len(X_seq))\n    X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n    X_test, y_test = X_seq[train_end:test_end], y_seq[train_end:test_end]\n    X_valid, y_valid = X_seq[test_end:], y_seq[test_end:]\n\n    data = {\n        \'train\': [X_train, y_train],\n        \'valid\': [X_valid, y_valid],\n        \'test\': [X_test, y_test],\n    }\n\n    # Model & training parameters\n    input_shape = list(data[\'train\'][0].shape[1:])\n    output_shape = list(data[\'train\'][1].shape[1:])\n    batch_size = 16\n    epochs = 5\n\n    nn_params = {\n        \'H_dim\': 32,\n        \'H_activation\': \'tanh\',\n        \'dropout\': 0.1,\n    }\n\n    # Retrieve model config\n    configs = load_NN_configs(filename=\'lstm.yaml\',\n                              input_shape=input_shape,\n                              output_shape=output_shape,\n                              params=nn_params)\n\n    # Construct & compile the model\n    model = assemble(\'LSTM\', configs[\'1H\'])\n    model.compile(optimizer=Adam(1e-1), loss=\'mse\')\n\n    # Callbacks\n    callbacks = [EarlyStopping(monitor=\'val_loss\', patience=10)]\n\n    # Train the model\n    history = train(model, data, callbacks=callbacks,\n                    checkpoint=\'lstm\', checkpoint_monitor=\'val_loss\',\n                    epochs=epochs, batch_size=batch_size, verbose=2)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/mlp_kin40k.py,0,"b'""""""\nMLP regression on Kin40k data.\nAttains about 0.14 RMSE within 150 epochs.\nReference: https://arxiv.org/abs/1511.02222\n""""""\nfrom __future__ import print_function\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping\n\n# Dataset interfaces\nfrom kgp.datasets.kin40k import load_data\n\n# Model assembling and executing\nfrom kgp.utils.experiment import train\n\n# Metrics\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef standardize_data(X_train, X_test, X_valid):\n    X_mean = np.mean(X_train, axis=0)\n    X_std = np.std(X_train, axis=0)\n\n    X_train -= X_mean\n    X_train /= X_std\n    X_test -= X_mean\n    X_test /= X_std\n    X_valid -= X_mean\n    X_valid /= X_std\n\n    return X_train, X_test, X_valid\n\n\ndef assemble_mlp(input_shape, output_shape):\n    """"""Assemble a simple MLP model.\n    """"""\n    inputs = Input(shape=input_shape)\n    hidden = Dense(1024, activation=\'relu\', name=\'dense1\')(inputs)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(512, activation=\'relu\', name=\'dense2\')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(64, activation=\'relu\', name=\'dense3\')(hidden)\n    hidden = Dropout(0.25)(hidden)\n    hidden = Dense(2, activation=\'relu\', name=\'dense4\')(hidden)\n    outputs = Dense(1, activation=\'linear\')(hidden)\n    return Model(inputs=inputs, outputs=outputs)\n\n\ndef main():\n    # Load data\n    X_train, y_train = load_data(stop=90.)\n    X_test, y_test = load_data(start=90.)\n    X_valid, y_valid = X_test, y_test\n    X_train, X_test, X_valid = standardize_data(X_train, X_test, X_valid)\n    data = {\n        \'train\': (X_train, y_train),\n        \'valid\': (X_valid, y_valid),\n        \'test\': (X_test, y_test),\n    }\n\n    # Model & training parameters\n    input_shape = data[\'train\'][0].shape[1:]\n    output_shape = data[\'train\'][1].shape[1:]\n    batch_size = 128\n    epochs = 100\n\n    # Construct & compile the model\n    model = assemble_mlp(input_shape, output_shape)\n    model.compile(optimizer=Adam(1e-4), loss=\'mse\')\n    model.load_weights(\'checkpoints/mlp_kin40k.h5\', by_name=True)\n\n    # Callbacks\n    # callbacks = [\n    #     EarlyStopping(monitor=\'val_loss\', patience=10),\n    # ]\n    callbacks = []\n\n    # Train the model\n    history = train(model, data, callbacks=callbacks,\n                    checkpoint=\'mlp_kin40k\', checkpoint_monitor=\'val_loss\',\n                    epochs=epochs, batch_size=batch_size, verbose=1)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/msgp_lstm_actuator.py,0,"b'""""""MSGP-LSTM regression on Actuator data.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.optimizers import Adagrad, Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\n# Dataset interfaces\nfrom kgp.datasets.sysid import load_data\nfrom kgp.datasets.data_utils import data_to_seq, standardize_data\n\n# Model assembling and executing\nfrom kgp.utils.assemble import load_NN_configs, load_GP_configs, assemble\nfrom kgp.utils.experiment import train\n\n# Metrics & losses\nfrom kgp.losses import gen_gp_loss\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef main():\n    # Load data\n    X, y = load_data(\'actuator\', use_targets=False)\n    X_seq, y_seq = data_to_seq(X, y,\n        t_lag=32, t_future_shift=1, t_future_steps=1, t_sw_step=1)\n\n    # Split\n    train_end = int((45. / 100.) * len(X_seq))\n    test_end = int((90. / 100.) * len(X_seq))\n    X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n    X_test, y_test = X_seq[train_end:test_end], y_seq[train_end:test_end]\n    X_valid, y_valid = X_seq[test_end:], y_seq[test_end:]\n\n    data = {\n        \'train\': [X_train, y_train],\n        \'valid\': [X_valid, y_valid],\n        \'test\': [X_test, y_test],\n    }\n\n    # Re-format targets\n    for set_name in data:\n        y = data[set_name][1]\n        y = y.reshape((-1, 1, np.prod(y.shape[1:])))\n        data[set_name][1] = [y[:,:,i] for i in xrange(y.shape[2])]\n\n    # Model & training parameters\n    nb_train_samples = data[\'train\'][0].shape[0]\n    input_shape = data[\'train\'][0].shape[1:]\n    nb_outputs = len(data[\'train\'][1])\n    gp_input_shape = (1,)\n    batch_size = 128\n    epochs = 5\n\n    nn_params = {\n        \'H_dim\': 16,\n        \'H_activation\': \'tanh\',\n        \'dropout\': 0.1,\n    }\n    gp_params = {\n        \'cov\': \'SEiso\',\n        \'hyp_lik\': -2.0,\n        \'hyp_cov\': [[-0.7], [0.0]],\n        \'opt\': {\'cg_maxit\': 500, \'cg_tol\': 1e-4},\n        \'grid_kwargs\': {\'eq\': 1, \'k\': 1e2},\n        \'update_grid\': True,\n    }\n\n    # Retrieve model config\n    nn_configs = load_NN_configs(filename=\'lstm.yaml\',\n                                 input_shape=input_shape,\n                                 output_shape=gp_input_shape,\n                                 params=nn_params)\n    gp_configs = load_GP_configs(filename=\'gp.yaml\',\n                                 nb_outputs=nb_outputs,\n                                 batch_size=batch_size,\n                                 nb_train_samples=nb_train_samples,\n                                 params=gp_params)\n\n    # Construct & compile the model\n    model = assemble(\'GP-LSTM\', [nn_configs[\'1H\'], gp_configs[\'MSGP\']])\n    loss = [gen_gp_loss(gp) for gp in model.output_layers]\n    model.compile(optimizer=Adam(1e-2), loss=loss)\n\n    # Callbacks\n    callbacks = [EarlyStopping(monitor=\'val_mse\', patience=10)]\n\n    # Train the model\n    history = train(model, data, callbacks=callbacks, gp_n_iter=5,\n                    checkpoint=\'lstm\', checkpoint_monitor=\'val_mse\',\n                    epochs=epochs, batch_size=batch_size, verbose=2)\n\n    # Finetune the model\n    model.finetune(*data[\'train\'],\n                   batch_size=batch_size,\n                   gp_n_iter=100,\n                   verbose=0)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test predict RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/msgp_lstm_actuator_manual_grid.py,0,"b'""""""MSGP-LSTM regression on Actuator data with manually specified grid.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.optimizers import Adagrad, Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\n# Dataset interfaces\nfrom kgp.datasets.sysid import load_data\nfrom kgp.datasets.data_utils import data_to_seq, standardize_data\n\n# Model assembling and executing\nfrom kgp.utils.assemble import load_NN_configs, load_GP_configs, assemble\nfrom kgp.utils.experiment import train\n\n# Metrics & losses\nfrom kgp.losses import gen_gp_loss\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef main():\n    # Load data\n    X, y = load_data(\'actuator\', use_targets=False)\n    X_seq, y_seq = data_to_seq(X, y,\n        t_lag=32, t_future_shift=1, t_future_steps=1, t_sw_step=1)\n\n    # Split\n    train_end = int((45. / 100.) * len(X_seq))\n    test_end = int((90. / 100.) * len(X_seq))\n    X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n    X_test, y_test = X_seq[train_end:test_end], y_seq[train_end:test_end]\n    X_valid, y_valid = X_seq[test_end:], y_seq[test_end:]\n\n    data = {\n        \'train\': [X_train, y_train],\n        \'valid\': [X_valid, y_valid],\n        \'test\': [X_test, y_test],\n    }\n\n    # Re-format targets\n    for set_name in data:\n        y = data[set_name][1]\n        y = y.reshape((-1, 1, np.prod(y.shape[1:])))\n        data[set_name][1] = [y[:,:,i] for i in xrange(y.shape[2])]\n\n    # Model & training parameters\n    nb_train_samples = data[\'train\'][0].shape[0]\n    input_shape = data[\'train\'][0].shape[1:]\n    nb_outputs = len(data[\'train\'][1])\n    gp_input_shape = (1,)\n    batch_size = 128\n    epochs = 5\n\n    nn_params = {\n        \'H_dim\': 16,\n        \'H_activation\': \'tanh\',\n        \'dropout\': 0.1,\n    }\n    gp_params = {\n        \'cov\': \'SEiso\',\n        \'hyp_lik\': -2.0,\n        \'hyp_cov\': [[-0.7], [0.0]],\n        \'opt\': {\'cg_maxit\': 500, \'cg_tol\': 1e-4},\n        \'grid_kwargs\': {\'eq\': 1, \'k\': 1e2},\n        \'update_grid\': False,  # when using manual grid, turn off grid updates\n    }\n\n    # Retrieve model config\n    nn_configs = load_NN_configs(filename=\'lstm.yaml\',\n                                 input_shape=input_shape,\n                                 output_shape=gp_input_shape,\n                                 params=nn_params)\n    gp_configs = load_GP_configs(filename=\'gp.yaml\',\n                                 nb_outputs=nb_outputs,\n                                 batch_size=batch_size,\n                                 nb_train_samples=nb_train_samples,\n                                 params=gp_params)\n\n    # Specify manual grid for MSGP (100 equidistant points per input dimension).\n    # Note: each np.ndarray in the xg must be a column vector.\n    gp_configs[\'MSGP\'][\'config\'][\'grid_kwargs\'][\'xg\'] = (\n            gp_input_shape[0] * [np.linspace(-1.0, 1.0, 100)[:, None]])\n\n    # Construct & compile the model\n    model = assemble(\'GP-LSTM\', [nn_configs[\'1H\'], gp_configs[\'MSGP\']])\n    loss = [gen_gp_loss(gp) for gp in model.output_layers]\n    model.compile(optimizer=Adam(1e-2), loss=loss)\n\n    # Callbacks\n    callbacks = [EarlyStopping(monitor=\'val_mse\', patience=10)]\n\n    # Train the model\n    history = train(model, data, callbacks=callbacks, gp_n_iter=5,\n                    checkpoint=\'lstm\', checkpoint_monitor=\'val_mse\',\n                    epochs=epochs, batch_size=batch_size, verbose=2)\n\n    # Finetune the model\n    model.finetune(*data[\'train\'],\n                   batch_size=batch_size,\n                   gp_n_iter=100,\n                   verbose=0)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test predict RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/msgp_mlp_kin40k.py,0,"b'""""""\nMSGP-MLP regression on Kin40k data.\nReference: https://arxiv.org/abs/1511.02222\n\nThis example showcases semi-stochastic training\nof GP-MLP model from scratch. Note that the original\npaper used full-batch pretraining-finetuning scheme.\n""""""\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping\n\n# KGP\nfrom kgp.models import Model\nfrom kgp.layers import GP\n\n# Dataset interfaces\nfrom kgp.datasets.kin40k import load_data\n\n# Model assembling and executing\nfrom kgp.utils.experiment import train\n\n# Metrics & losses\nfrom kgp.losses import gen_gp_loss\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef standardize_data(X_train, X_test, X_valid):\n    X_mean = np.mean(X_train, axis=0)\n    X_std = np.std(X_train, axis=0)\n\n    X_train -= X_mean\n    X_train /= X_std\n    X_test -= X_mean\n    X_test /= X_std\n    X_valid -= X_mean\n    X_valid /= X_std\n\n    return X_train, X_test, X_valid\n\n\ndef assemble_mlp(input_shape, output_shape, batch_size, nb_train_samples):\n    """"""Assemble a simple MLP model.\n    """"""\n    inputs = Input(shape=input_shape)\n    hidden = Dense(1024, activation=\'relu\', name=\'dense1\')(inputs)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(512, activation=\'relu\', name=\'dense2\')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(64, activation=\'relu\', name=\'dense3\')(hidden)\n    hidden = Dropout(0.25)(hidden)\n    hidden = Dense(2, activation=\'relu\', name=\'dense4\')(hidden)\n    gp = GP(hyp={\n                \'lik\': np.log(0.3),\n                \'mean\': [],\n                \'cov\': [[0.5], [1.0]],\n            },\n            inf=\'infGrid\', dlik=\'dlikGrid\',\n            opt={\'cg_maxit\': 2000, \'cg_tol\': 1e-6},\n            mean=\'meanZero\', cov=\'covSEiso\',\n            update_grid=1,\n            grid_kwargs={\'eq\': 1, \'k\': 70.},\n            batch_size=batch_size,\n            nb_train_samples=nb_train_samples)\n    outputs = [gp(hidden)]\n    return Model(inputs=inputs, outputs=outputs)\n\n\ndef main():\n    # Load data\n    X_train, y_train = load_data(stop=90.)\n    X_test, y_test = load_data(start=90.)\n    X_valid, y_valid = X_test, y_test\n    X_train, X_test, X_valid = standardize_data(X_train, X_test, X_valid)\n    data = {\n        \'train\': (X_train, y_train),\n        \'valid\': (X_valid, y_valid),\n        \'test\': (X_test, y_test),\n    }\n\n    # Model & training parameters\n    input_shape = data[\'train\'][0].shape[1:]\n    output_shape = data[\'train\'][1].shape[1:]\n    batch_size = 2**10\n    epochs = 500\n\n    # Construct & compile the model\n    model = assemble_mlp(input_shape, output_shape, batch_size,\n                         nb_train_samples=len(X_train))\n    loss = [gen_gp_loss(gp) for gp in model.output_layers]\n    model.compile(optimizer=Adam(1e-4), loss=loss)\n\n    # Load saved weights (if exist)\n    if os.path.isfile(\'checkpoints/msgp_mlp_kin40k.h5\'):\n        model.load_weights(\'checkpoints/msgp_mlp_kin40k.h5\', by_name=True)\n\n    # Train the model\n    history = train(model, data, callbacks=[], gp_n_iter=5,\n                    checkpoint=\'msgp_mlp_kin40k\', checkpoint_monitor=\'val_mse\',\n                    epochs=epochs, batch_size=batch_size, verbose=1)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/msgp_sm_kernel_mlp_kin40k.py,0,"b'""""""\nMSGP-MLP regression on Kin40k data using Spectral Mixture Kernel.\n""""""\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\nnp.random.seed(42)\n\n# Keras\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping\n\n# KGP\nfrom kgp.models import Model\nfrom kgp.layers import GP\n\n# Dataset interfaces\nfrom kgp.datasets.kin40k import load_data\n\n# Model assembling and executing\nfrom kgp.utils.experiment import train\n\n# Metrics & losses\nfrom kgp.losses import gen_gp_loss\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef standardize_data(X_train, X_test, X_valid):\n    X_mean = np.mean(X_train, axis=0)\n    X_std = np.std(X_train, axis=0)\n\n    X_train -= X_mean\n    X_train /= X_std\n    X_test -= X_mean\n    X_test /= X_std\n    X_valid -= X_mean\n    X_valid /= X_std\n\n    return X_train, X_test, X_valid\n\ndef initCovSM(Q, D):\n    w0 = np.log(np.ones((Q,1)))\n    mu = np.log(np.maximum(0.05*np.random.rand(Q*D,1),1e-8))\n    v = np.log(np.abs(np.random.randn(Q*D,1) + 1))\n    return [[w0], [mu], [v]]\n    \ndef assemble_mlp(input_shape, output_shape, batch_size, nb_train_samples):\n    """"""Assemble a simple MLP model.\n    """"""\n    inputs = Input(shape=input_shape)\n    hidden = Dense(1024, activation=\'relu\', name=\'dense1\')(inputs)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(512, activation=\'relu\', name=\'dense2\')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(64, activation=\'relu\', name=\'dense3\')(hidden)\n    hidden = Dropout(0.25)(hidden)\n    hidden = Dense(2, activation=\'relu\', name=\'dense4\')(hidden)\n    \n    gp = GP(hyp={\n                \'lik\': np.log(0.3),\n                \'mean\': np.zeros((2,1)).tolist() + [[0]],\n                \'cov\': initCovSM(6,1),\n            },\n            inf=\'infGrid\', dlik=\'dlikGrid\',\n            opt={\'cg_maxit\': 2000, \'cg_tol\': 1e-6},\n            mean=\'meanSum\', cov=\'covSM\',\n            update_grid=1,\n            grid_kwargs={\'eq\': 1, \'k\': 70.},\n            cov_args=[6],\n            mean_args=[\'{@meanLinear, @meanConst}\'],\n            batch_size=batch_size,\n            nb_train_samples=nb_train_samples)\n    outputs = [gp(hidden)]\n    return Model(inputs=inputs, outputs=outputs)\n\n\ndef main():\n    # Load data\n    X_train, y_train = load_data(stop=90.)\n    X_test, y_test = load_data(start=90.)\n    X_valid, y_valid = X_test, y_test\n    X_train, X_test, X_valid = standardize_data(X_train, X_test, X_valid)\n    data = {\n        \'train\': (X_train, y_train),\n        \'valid\': (X_valid, y_valid),\n        \'test\': (X_test, y_test),\n    }\n\n    # Model & training parameters\n    input_shape = data[\'train\'][0].shape[1:]\n    output_shape = data[\'train\'][1].shape[1:]\n    batch_size = 2**10\n    epochs = 500\n\n    # Construct & compile the model\n    model = assemble_mlp(input_shape, output_shape, batch_size,\n                         nb_train_samples=len(X_train))\n    loss = [gen_gp_loss(gp) for gp in model.output_layers]\n    model.compile(optimizer=Adam(1e-4), loss=loss)\n\n    # Load saved weights (if exist)\n    if os.path.isfile(\'checkpoints/msgp_sm_kernel_mlp_kin40k.h5\'):\n        model.load_weights(\'checkpoints/msgp_sm_kernel_mlp_kin40k.h5\', by_name=True)\n\n    # Train the model\n    history = train(model, data, callbacks=[], gp_n_iter=5,\n                    checkpoint=\'msgp_sm_kernel_mlp_kin40k\', checkpoint_monitor=\'val_mse\',\n                    epochs=epochs, batch_size=batch_size, verbose=1)\n\n    # Test the model\n    X_test, y_test = data[\'test\']\n    y_preds = model.predict(X_test)\n    rmse_predict = RMSE(y_test, y_preds)\n    print(\'Test RMSE:\', rmse_predict)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
kgp/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom . import backend\nfrom . import datasets\nfrom . import layers\nfrom . import utils\nfrom . import callbacks\nfrom . import metrics\nfrom . import models\nfrom . import losses\n\n__version__ = '0.3.2'\n"""
kgp/callbacks.py,0,"b'""""""Gaussian Process callbacks for Keras.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport warnings\n\nimport numpy as np\n\nfrom contextlib import contextmanager\nfrom timeit import default_timer\n\nfrom keras.callbacks import Callback\n\nfrom .metrics import mean_squared_error as MSE\n\n\n@contextmanager\ndef elapsed_timer():\n    start = default_timer()\n    elapsed = lambda: default_timer() - start\n    yield lambda: elapsed()\n    end = default_timer()\n    elapsed = lambda: end - start\n\n\nclass UpdateGP(Callback):\n    """"""Performs GP updates at the beginning of each epoch during training.\n    """"""\n    def __init__(self, ins, val_ins=None,\n                 batch_size=128, gp_n_iter=1, verbose=0):\n        self.training_data = ins\n        self.validation_data = val_ins\n        self.batch_size = batch_size\n        self.gp_n_iter = gp_n_iter\n        self.verbose = verbose\n\n    def set_params(self, params):\n        self.params = params\n        if self.validation_data is not None:\n            self.params[\'metrics\'] += [\'val_\' + m for m in params[\'metrics\']]\n\n    def on_epoch_begin(self, epoch, logs=None):\n        logs = logs if logs is not None else {}\n\n        X, Y = self.training_data\n\n        # Do forward pass\n        H = self.model.transform(X, self.batch_size, learning_phase=1.)\n\n        # Update GPs\n        gp_update_elapsed = []\n        for gp, h, y in zip(self.model.output_gp_layers, H, Y):\n            # Update GP data (and grid if necessary)\n            gp.backend.update_data(\'tr\', h, y, verbose=self.verbose)\n            if gp.update_grid and (epoch % gp.update_grid == 0):\n                gp.backend.update_grid(\'tr\', verbose=self.verbose)\n\n            # Train GP & get derivatives\n            with elapsed_timer() as elapsed:\n                gp.hyp = gp.backend.train(self.gp_n_iter, verbose=self.verbose)\n                gp.dlik_dh = gp.backend.get_dlik_dx(\'tr\', verbose=self.verbose)\n            gp_update_elapsed.append(elapsed())\n\n            # Compute MSE and NLML\n            nlml, preds = gp.backend.eval_predict(h)\n            gp.nlml, gp.mse = nlml, MSE(y, preds)\n        logs[\'gp_update_elapsed\'] = np.mean(gp_update_elapsed)\n\n    def on_batch_begin(self, batch, logs=None):\n        logs = logs if logs is not None else {}\n\n        # Update the batch ids for the current batch\n        for gp in self.model.output_gp_layers:\n            gp.batch_sz = int(logs[\'size\'])\n            pad_size = self.batch_size - logs[\'size\']\n            gp.batch_ids = np.pad(logs[\'ids\'], (0, pad_size), \'constant\')\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs if logs is not None else {}\n\n        # Do validation (if necessary)\n        if self.validation_data is not None:\n            X_val, Y_val = self.validation_data\n            H_val = self.model.transform(X_val, self.batch_size)\n            for gp, h, y in zip(self.model.output_gp_layers, H_val, Y_val):\n                nlml, preds = gp.backend.eval_predict(h, verbose=self.verbose)\n                logs[\'val_nlml\'], logs[\'val_mse\'] = nlml, MSE(y, preds)\n\n\nclass Timer(Callback):\n    """"""Simply records time for each batch and epoch.\n    """"""\n    def on_epoch_begin(self, epoch, logs=None):\n        self._epoch_start = default_timer()\n\n    def on_batch_begin(self, batch, logs=None):\n        self._batch_start = default_timer()\n        self._batch_elapsed = []\n\n    def on_batch_end(self, batch, logs=None):\n        batch_elapsed = default_timer() - self._batch_start\n        self._batch_elapsed.append(batch_elapsed)\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs if logs is not None else {}\n        logs[\'epoch_elapsed\'] = default_timer() - self._epoch_start\n        logs[\'batch_elapsed_avg\'] = np.mean(self._batch_elapsed)\n'"
kgp/layers.py,0,"b'""""""Gaussian Process layers for Keras.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom keras.engine import InputSpec\nfrom keras.engine import Layer\n\n# Backend for neural networks\nfrom keras import backend as K\n\n# Backend for Gaussian processes\nfrom .backend import GP_BACKEND\n\n\nclass GP(Layer):\n    """"""Gaussian Process layer.\n\n    Encapsulates GP backend functionality and provides a Keras-like interface\n    for working with Gaussian processes.\n\n    Arguments:\n    ----------\n        hyp : dict\n            GP kernel hyper-parameters.\n        batch_size : uint\n\n        nb_train_samples : uint\n            The size of the training data. GP needs to know this a priory.\n        opt : dict (default: {})\n            GP inference / training options.\n        inf : str (default: \'infExact\')\n            Inference function name (should match one from GPML).\n        lik : str (default: \'likGauss\')\n            Likelihood name (should match one from GPML).\n        mean : str (default: \'meanZero\')\n            Mean function name (should match one from GPML).\n        cov : str (default: \'covSEiso\')\n            Covariance function name (should match one from GPML).\n        dlik : str (default: \'dlikExact\')\n            Derivative of the likelihood w.r.t. covariance kernel.\n        update_grid : uint (default: 0)\n            The frequency of grid updates in epochs.\n            0 means the grid is being created once and fixed.\n        gpml_path : str or None (default: None)\n            Path to GPML. If None, the backend will attempt to use `GPML_PATH`\n            environment variable and raises ValueError if couldn\'t find.\n    """"""\n    def __init__(self, hyp, batch_size, nb_train_samples, opt=None,\n                 inf=\'infExact\', lik=\'likGauss\', dlik=\'dlikExact\',\n                 mean=\'meanZero\', cov=\'covSEiso\',\n                 grid_kwargs=None, \n                 cov_args=None, mean_args=None,\n                 update_grid=0,\n                 engine=None, engine_kwargs=None,\n                 gpml_path=None, verbose=1):\n        self.hyp = hyp\n        self.batch_size = batch_size\n        self.nb_train_samples = nb_train_samples\n        self.backend = GP_BACKEND(engine, engine_kwargs, gpml_path)\n        self.backend_config = {\n            \'opt\': opt or {},\n            \'inf\': inf,\n            \'lik\': lik,\n            \'dlik\': dlik,\n            \'mean\': mean,\n            \'cov\': cov,\n            \'cov_args\': cov_args, \n            \'mean_args\': mean_args,\n            \'grid_kwargs\': grid_kwargs,\n            \'verbose\': verbose,\n        }\n        self.update_grid = update_grid\n        super(GP, self).__init__(trainable=False)\n\n    @property\n    def hyp(self):\n        return self._hyp\n\n    @hyp.setter\n    def hyp(self, value):\n        self._hyp = value\n\n    @property\n    def dlik_dh(self):\n        return self._dlik_dh\n\n    @dlik_dh.setter\n    def dlik_dh(self, value):\n        K.set_value(self._dlik_dh, value)\n\n    @property\n    def batch_ids(self):\n        return self._batch_ids\n\n    @batch_ids.setter\n    def batch_ids(self, value):\n        K.set_value(self._batch_ids, value)\n\n    @property\n    def batch_sz(self):\n        return self._batch_sz\n\n    @batch_sz.setter\n    def batch_sz(self, value):\n        K.set_value(self._batch_sz, value)\n\n    @property\n    def nlml(self):\n        return self._nlml\n\n    @nlml.setter\n    def nlml(self, value):\n        K.set_value(self._nlml, value)\n\n    @property\n    def mse(self):\n        return self._mse\n\n    @mse.setter\n    def mse(self, value):\n        K.set_value(self._mse, value)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], 1)\n\n    def build(self, input_shape):\n        """"""Create the internal variables for communication with GP backend.\n\n        Arguments:\n        ----------\n            input_shape: Keras tensor (future input to layer)\n                or list/tuple of Keras tensors to reference\n                for weight shape computations.\n        """"""\n        assert len(input_shape) == 2\n        input_dim = input_shape[-1]\n        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n\n        # Configure GP backend\n        self.backend.configure(input_dim, self.hyp, **self.backend_config)\n\n        # Internal shared variables\n        self._dlik_dh = K.zeros((self.nb_train_samples, input_dim))\n        self._batch_ids = K.variable(np.zeros(self.batch_size), dtype=\'int32\')\n        self._batch_sz = K.variable(self.batch_size, dtype=\'int32\')\n\n        # Internal metrics\n        self._nlml = K.variable(0.)\n        self._mse = K.variable(0.)\n\n        self.built = True\n'"
kgp/losses.py,0,"b'""""""\nObjectives for Gaussian Process layers.\n""""""\nfrom keras import backend as K\n\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n\n\ndef gen_gp_loss(gp):\n    """"""Generate an internal objective, `dlik_dh * H`, for a given GP layer.\n    """"""\n    def loss(_, H):\n        dlik_dh_times_H = H * K.gather(gp.dlik_dh, gp.batch_ids[:gp.batch_sz])\n        return K.sum(dlik_dh_times_H, axis=1, keepdims=True)\n    return loss\n\n\n# Aliases\n\nrmse = RMSE = root_mean_squared_error\n'"
kgp/metrics.py,0,"b'import numpy as np\n\ndef mean_squared_error(y_true, y_pred):\n    return np.mean((np.ravel(y_true) - np.ravel(y_pred))**2)\n\ndef root_mean_squared_error(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n'"
kgp/models.py,0,"b'""""""Gaussian Process models for Keras 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom keras.models import Model as KerasModel\nfrom keras.engine.topology import _to_list\nfrom keras.engine.training import _standardize_input_data\n\nfrom .callbacks import UpdateGP\n\n\nclass Model(KerasModel):\n    """"""Model that supports arbitrary structure with GP output layers.\n\n    This class extends `keras.models.Model` and allows using Gaussian Processes\n    as output layers. The model completely inherits the function interface\n    of Keras and is constructed in a standard way.\n\n    On training, GPs are optimized using empirical Bayes (log marginal\n    likelihood maximization) using semi-stochastic alternating scheme with\n    delayed kernel matrix updates [1]. Non-GP output layers can use one the\n    standard Keras objectives, e.g., the mean squared error.\n    """"""\n    def __init__(self, inputs, outputs, name=None):\n        super(Model, self).__init__(inputs, outputs, name)\n\n        # List all output GP layers\n        self.output_gp_layers = [layer for layer in self.output_layers\n                                 if layer.name.startswith(\'gp\')]\n\n    def compile(self, optimizer, loss, metrics=None, loss_weights=None,\n                sample_weight_mode=None, **kwargs):\n        super(Model, self).compile(optimizer, loss, metrics, loss_weights,\n                                   sample_weight_mode, **kwargs)\n\n        # Remove the metrics meaningless for GP output layers\n        self.metrics_tensors = [\n            mt for mt, mn in zip(self.metrics_tensors, self.metrics_names[1:])\n            if not (mn.startswith(\'gp\') and mn.endswith(\'loss\'))\n        ]\n        self.metrics_names = [\n            mn for mn in self.metrics_names\n            if not (mn.startswith(\'gp\') and mn.endswith(\'loss\'))\n        ]\n\n        # Add MSE and NLML metrics for each output GP\n        for gp in self.output_gp_layers:\n            self.metrics_tensors.extend([gp.mse, gp.nlml])\n            self.metrics_names.extend([gp.name + \'_mse\', gp.name + \'_nlml\'])\n\n        # Add cumulative MSE & NLML metrics\n        self.mse = sum([gp.mse for gp in self.output_gp_layers])\n        self.nlml = sum([gp.nlml for gp in self.output_gp_layers])\n        self.metrics_tensors.extend([self.mse, self.nlml])\n        self.metrics_names.extend([\'mse\', \'nlml\'])\n\n    def transform(self, x, batch_size=32, learning_phase=0., verbose=0):\n        h = super(Model, self).predict(x, batch_size, learning_phase, verbose)\n        return _to_list(h)\n\n    def fit(self, X, Y,\n            batch_size=32,\n            epochs=1,\n            gp_n_iter=1,\n            verbose=1,\n            callbacks=None,\n            validation_split=0.,\n            validation_data=None,\n            shuffle=True,\n            class_weight=None,\n            sample_weight=None,\n            initial_epoch=0,\n            **kwargs):\n        """"""Trains the model for a fixed number of epochs (iterations on a dataset).\n\n        For argument details, refer to `keras.engine.training.Model.fit`.\n\n        Notes:\n            The following arguments are currently unsupported by models with GP\n            output layers:\n            - validation_split\n            - class_weight\n            - sample_weight\n        """"""\n        # Validate user data\n        X, Y, _ = self._standardize_user_data(\n            X, Y,\n            sample_weight=None,\n            class_weight=None,\n            check_batch_axis=False,\n            batch_size=batch_size)\n        if validation_data is not None:\n            X_val, Y_val, _ = self._standardize_user_data(\n                *validation_data,\n                sample_weight=None,\n                class_weight=None,\n                check_batch_axis=False,\n                batch_size=batch_size)\n            validation_data = (X_val, Y_val)\n\n        # Setup GP updates\n        update_gp = UpdateGP(ins=(X, Y),\n                             val_ins=validation_data,\n                             batch_size=batch_size,\n                             gp_n_iter=gp_n_iter,\n                             verbose=verbose)\n        callbacks = [update_gp] + (callbacks or [])\n\n        return super(Model, self).fit(\n            X, Y,\n            batch_size=batch_size,\n            epochs=epochs,\n            verbose=verbose,\n            callbacks=callbacks,\n            shuffle=shuffle,\n            initial_epoch=initial_epoch,\n            **kwargs)\n\n    def finetune(self, X, Y, batch_size=32, gp_n_iter=1, verbose=1):\n        """"""Finetune the output GP layers assuming the network is pre-trained.\n\n        Arguments:\n        ----------\n            X : np.ndarray or list of np.ndarrays\n            Y : np.ndarray or list of np.ndarrays\n            batch_size : uint (default: 128)\n                Batch size used for data streaming through the network.\n            gp_n_iter : uint (default: 100)\n                Number of iterations for GP training.\n            verbose : uint (default: 1)\n                Verbosity mode, 0 or 1.\n        """"""\n        # Validate user data\n        X = _standardize_input_data(\n            X, self._feed_input_names, self._feed_input_shapes,\n            check_batch_axis=False, exception_prefix=\'input\')\n\n        H = self.transform(X, batch_size=batch_size)\n\n        if verbose:\n            print(""Finetuning output GPs..."")\n\n        for gp, h, y in zip(self.output_gp_layers, H, Y):\n            # Update GP data (and grid if necessary)\n            gp.backend.update_data(\'tr\', h, y)\n            if gp.update_grid:\n                gp.backend.update_grid(\'tr\')\n\n            # Train GP\n            gp.hyp = gp.backend.train(gp_n_iter, verbose=verbose)\n\n        if verbose:\n            print(""Done."")\n\n    def evaluate(self, X, Y, batch_size=32, verbose=0):\n        """"""Compute NLML on the given data.\n\n        Arguments:\n        ----------\n            X : np.ndarray or list of np.ndarrays\n            Y : np.ndarray or list of np.ndarrays\n            batch_size : uint (default: 128)\n            verbose : uint (default: 0)\n                Verbosity mode, 0 or 1.\n\n        Returns:\n        --------\n            nlml : float\n        """"""\n        # Validate user data\n        X, Y, _ = self._standardize_user_data(\n            X, Y,\n            sample_weight=None,\n            class_weight=None,\n            check_batch_axis=False,\n            batch_size=batch_size)\n\n        H = self.transform(X, batch_size=batch_size)\n\n        nlml = 0.\n        for gp, h, y in zip(self.output_gp_layers, H, Y):\n            nlml += gp.backend.evaluate(\'tmp\', h, y)\n\n        return nlml\n\n    def predict(self, X, X_tr=None, Y_tr=None,\n                batch_size=32, return_var=False, verbose=0):\n        """"""Generate output predictions for the input samples batch by batch.\n\n        Arguments:\n        ----------\n            X : np.ndarray or list of np.ndarrays\n            batch_size : uint (default: 128)\n            return_var : bool (default: False)\n                Whether predictive variance is returned.\n            verbose : uint (default: 0)\n                Verbosity mode, 0 or 1.\n\n        Returns:\n        --------\n            preds : a list or a tuple of lists\n                Lists of output predictions and variance estimates.\n        """"""\n        # Update GP data if provided (and grid if necessary)\n        if X_tr is not None and Y_tr is not None:\n            X_tr, Y_tr, _ = self._standardize_user_data(\n                X_tr, Y_tr,\n                sample_weight=None,\n                class_weight=None,\n                check_batch_axis=False,\n                batch_size=batch_size)\n            H_tr = self.transform(X_tr, batch_size=batch_size)\n            for gp, h, y in zip(self.output_gp_layers, H_tr, Y_tr):\n                gp.backend.update_data(\'tr\', h, y)\n                if gp.update_grid:\n                    gp.backend.update_grid(\'tr\')\n\n        # Validate user data\n        X = _standardize_input_data(\n            X, self._feed_input_names, self._feed_input_shapes,\n            check_batch_axis=False, exception_prefix=\'input\')\n\n        H = self.transform(X, batch_size=batch_size)\n\n        preds = []\n        for gp, h in zip(self.output_gp_layers, H):\n            preds.append(gp.backend.predict(h, return_var=return_var))\n\n        if return_var:\n            preds = map(list, zip(*preds))\n\n        return preds\n\n\n# Apply tweaks\nfrom . import tweaks\n'"
kgp/tweaks.py,0,"b'""""""\nTweaks for KGP and original Keras classes or methods.\nTo keep the primary KGP code clean, all tweaks are kept in a separate file.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport keras.callbacks as cbks\nfrom keras.engine.training import _batch_shuffle\nfrom keras.engine.training import _make_batches\nfrom keras.engine.training import _slice_arrays\nfrom keras.engine.training import _standardize_input_data\n\nfrom keras.models import Model\n\nfrom keras import backend as K\n\n\ndef _fit_loop(self, f, ins, out_labels=None, batch_size=32,\n              epochs=100, verbose=1, callbacks=None,\n              val_f=None, val_ins=None, shuffle=True,\n              callback_metrics=None, initial_epoch=0,\n              steps_per_epoch=None, validation_steps=None):\n    """"""Abstract fit function for f(ins).\n    Assume that f returns a list, labeled by out_labels.\n\n    # Arguments\n        f: Keras function returning a list of tensors\n        ins: List of tensors to be fed to `f`\n        out_labels: List of strings, display names of\n            the outputs of `f`\n        batch_size: Integer batch size or None if unknown.\n        epochs: Number of times to iterate over the data\n        verbose: Verbosity mode, 0, 1 or 2\n        callbacks: List of callbacks to be called during training\n        val_f: Keras function to call for validation\n        val_ins: List of tensors to be fed to `val_f`\n        shuffle: Whether to shuffle the data at the beginning of each epoch\n        callback_metrics: List of strings, the display names of the metrics\n            passed to the callbacks. They should be the\n            concatenation of list the display names of the outputs of\n             `f` and the list of display names of the outputs of `f_val`.\n        initial_epoch: Epoch at which to start training\n            (useful for resuming a previous training run)\n        steps_per_epoch: Total number of steps (batches of samples)\n            before declaring one epoch finished and starting the\n            next epoch. Ignored with the default value of `None`.\n        validation_steps: Number of steps to run validation for\n            (only if doing validation from data tensors).\n            Ignored with the default value of `None`.\n\n    # Returns\n        `History` object.\n\n    [A tweaked version.]\n    """"""\n    do_validation = False\n    if val_f and val_ins:\n        do_validation = True\n        if verbose and ins and hasattr(ins[0], \'shape\') and hasattr(val_ins[0], \'shape\'):\n            print(\'Train on %d samples, validate on %d samples\' %\n                  (ins[0].shape[0], val_ins[0].shape[0]))\n    if validation_steps:\n        do_validation = True\n        if steps_per_epoch is None:\n            raise ValueError(\'Can only use `validation_steps` \'\n                             \'when doing step-wise \'\n                             \'training, i.e. `steps_per_epoch` \'\n                             \'must be set.\')\n\n    num_train_samples = self._check_num_samples(ins, batch_size,\n                                                steps_per_epoch,\n                                                \'steps_per_epoch\')\n    if num_train_samples is not None:\n        index_array = np.arange(num_train_samples)\n\n    self.history = cbks.History()\n    callbacks = [cbks.BaseLogger()] + (callbacks or []) + [self.history]\n    if verbose:\n        if steps_per_epoch is not None:\n            count_mode = \'steps\'\n        else:\n            count_mode = \'samples\'\n        callbacks += [cbks.ProgbarLogger(count_mode)]\n    callbacks = cbks.CallbackList(callbacks)\n    out_labels = out_labels or []\n\n    # it\'s possible to callback a different model than self\n    # (used by Sequential models)\n    if hasattr(self, \'callback_model\') and self.callback_model:\n        callback_model = self.callback_model\n    else:\n        callback_model = self\n\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        \'batch_size\': batch_size,\n        \'epochs\': epochs,\n        \'steps\': steps_per_epoch,\n        \'samples\': num_train_samples,\n        \'verbose\': verbose,\n        \'do_validation\': do_validation,\n        \'metrics\': callback_metrics or [],\n    })\n    callbacks.on_train_begin()\n    callback_model.stop_training = False\n    # for cbk in callbacks:\n    #     cbk.validation_data = val_ins\n\n    for epoch in range(initial_epoch, epochs):\n        callbacks.on_epoch_begin(epoch)\n        epoch_logs = {}\n        if steps_per_epoch is not None:\n            for step_index in range(steps_per_epoch):\n                batch_logs = {}\n                batch_logs[\'batch\'] = step_index\n                batch_logs[\'size\'] = 1\n                callbacks.on_batch_begin(step_index, batch_logs)\n                outs = f(ins)\n\n                if not isinstance(outs, list):\n                    outs = [outs]\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(step_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n            if do_validation:\n                val_outs = self._test_loop(val_f, val_ins,\n                                           batch_size=batch_size,\n                                           steps=validation_steps,\n                                           verbose=0)\n                if not isinstance(val_outs, list):\n                    val_outs = [val_outs]\n                # Same labels assumed.\n                for l, o in zip(out_labels, val_outs):\n                    epoch_logs[\'val_\' + l] = o\n        else:\n            if shuffle == \'batch\':\n                index_array = _batch_shuffle(index_array, batch_size)\n            elif shuffle:\n                np.random.shuffle(index_array)\n\n            batches = _make_batches(num_train_samples, batch_size)\n            for batch_index, (batch_start, batch_end) in enumerate(batches):\n                batch_ids = index_array[batch_start:batch_end]\n                try:\n                    if isinstance(ins[-1], float):\n                        # do not slice the training phase flag\n                        ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n                    else:\n                        ins_batch = _slice_arrays(ins, batch_ids)\n                except TypeError:\n                    raise TypeError(\'TypeError while preparing batch. \'\n                                    \'If using HDF5 input data, \'\n                                    \'pass shuffle=""batch"".\')\n                batch_logs = {}\n                batch_logs[\'batch\'] = batch_index\n                batch_logs[\'size\'] = len(batch_ids)\n                batch_logs[\'ids\'] = batch_ids\n                callbacks.on_batch_begin(batch_index, batch_logs)\n                outs = f(ins_batch)\n                if not isinstance(outs, list):\n                    outs = [outs]\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n                if batch_index == len(batches) - 1:  # last batch.\n                    if do_validation:\n                        val_outs = self._test_loop(val_f, val_ins,\n                                                   batch_size=batch_size,\n                                                   verbose=0)\n                        if not isinstance(val_outs, list):\n                            val_outs = [val_outs]\n                        # same labels assumed\n                        for l, o in zip(out_labels, val_outs):\n                            epoch_logs[\'val_\' + l] = o\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        if callback_model.stop_training:\n            break\n    callbacks.on_train_end()\n    return self.history\n\n\ndef predict(self, x,\n            batch_size=None,\n            learning_phase=0.,\n            verbose=0,\n            steps=None):\n        """"""Generates output predictions for the input samples.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: the input data, as a Numpy array\n                (or list of Numpy arrays if the model has multiple outputs).\n            batch_size: integer.\n            verbose: verbosity mode, 0 or 1.\n            steps: Total number of steps (batches of samples)\n                before declaring the prediction round finished.\n                Ignored with the default value of `None`.\n\n        # Returns\n            Numpy array(s) of predictions.\n\n        # Raises\n            ValueError: In case of mismatch between the provided\n                input data and the model\'s expectations,\n                or in case a stateful model receives a number of samples\n                that is not a multiple of the batch size.\n\n        [A tweaked version.]\n        """"""\n        # Backwards compatibility.\n        if batch_size is None and steps is None:\n            batch_size = 32\n        if x is None and steps is None:\n            raise ValueError(\'If predicting from data tensors, \'\n                             \'you should specify the `steps` \'\n                             \'argument.\')\n        # validate user data\n        x = _standardize_input_data(x, self._feed_input_names,\n                                    self._feed_input_shapes,\n                                    check_batch_axis=False)\n        if self.stateful:\n            if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n                raise ValueError(\'In a stateful network, \'\n                                 \'you should only pass inputs with \'\n                                 \'a number of samples that can be \'\n                                 \'divided by the batch size. Found: \' +\n                                 str(x[0].shape[0]) + \' samples. \'\n                                 \'Batch size: \' + str(batch_size) + \'.\')\n\n        # prepare inputs, delegate logic to _predict_loop\n        if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n            ins = x + [learning_phase]\n        else:\n            ins = x\n        self._make_predict_function()\n        f = self.predict_function\n        return self._predict_loop(f, ins,\n                                  batch_size=batch_size, verbose=verbose)\n\n# Monkey-patch keras.models.Model\nModel._fit_loop = _fit_loop\nModel.predict = predict\n\n\ndef _on_batch_end(self, batch, logs=None):\n    logs = logs if logs is not None else {}\n    batch_size = logs.get(\'size\', 0)\n    self.seen += batch_size\n\n    for k, v in logs.items():\n        if k == \'ids\':\n            continue\n        if k in self.totals:\n            self.totals[k] += v * batch_size\n        else:\n            self.totals[k] = v * batch_size\n\n# Monkey-patch keras.callbacks.BaseLogger\ncbks.BaseLogger.on_batch_end = _on_batch_end\n'"
tests/test_callbacks.py,0,"b""import numpy as np\n\nfrom six.moves import xrange\n\nfrom keras.layers import Input, Dense, LSTM\n\nfrom kgp.models import Model\nfrom kgp.layers import GP\nfrom kgp.callbacks import *\nfrom kgp.losses import gen_gp_loss\n\nN = 256\nbatch_size = 64\ninput_shape = (32, 10)\nlstm_dim = 16\ndense_dim = 1\noptimizer = 'adam'\n\ngp_test_config = {\n    'hyp': {'lik': -2.0, 'cov': np.array([[-0.7], [0.0]])},\n    'nb_train_samples': N,\n    'batch_size': batch_size,\n    'opt': {},\n    'inf': 'infExact',\n    'mean': 'meanZero',\n    'cov': 'covSEiso',\n    'lik': 'likGauss',\n    'dlik': 'dlikExact',\n    'verbose': 0,\n}\n\n\ndef build_model(nb_outputs=2):\n    inputs = Input(shape=input_shape)\n\n    # Neural transformations\n    lstm = LSTM(lstm_dim)(inputs)\n    dense = Dense(dense_dim)(lstm)\n\n    # GP outputs\n    outputs = [GP(**gp_test_config)(dense) for _ in xrange(nb_outputs)]\n\n    # Build the model\n    model = Model(inputs=inputs, outputs=outputs)\n\n    return model\n\n\ndef test_update_gp(seed=42):\n    rng = np.random.RandomState(seed)\n\n    for nb_outputs in [1, 2]:\n        # Generate dummy data\n        X_tr = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_tr = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n        X_val = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_val = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n\n        # Build & compile the model\n        model = build_model(nb_outputs)\n        loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n\n        # Setup the callback\n        update_gp_callback = UpdateGP((X_tr, Y_tr),\n                                      val_ins=(X_val, Y_val),\n                                      batch_size=batch_size)\n        update_gp_callback.set_model(model)\n\n        # Test the callback\n        epoch_logs, batch_logs = {}, {}\n        batch_logs['size'] = batch_size\n        batch_logs['ids'] = np.arange(batch_size)\n        update_gp_callback.on_epoch_begin(1, epoch_logs)\n        update_gp_callback.on_batch_begin(1, batch_logs)\n        update_gp_callback.on_epoch_end(1, epoch_logs)\n\n        assert 'gp_update_elapsed' in epoch_logs\n        assert 'val_nlml' in epoch_logs\n        assert 'val_mse' in epoch_logs\n\n\ndef test_timer():\n    # Setup the callback\n    timer_callback = Timer()\n\n    # Test the callback\n    epoch_logs= {}\n    timer_callback.on_epoch_begin(1, epoch_logs)\n    timer_callback.on_batch_begin(1)\n    timer_callback.on_batch_end(1)\n    timer_callback.on_epoch_end(1, epoch_logs)\n\n    assert 'epoch_elapsed' in epoch_logs\n    assert 'batch_elapsed_avg' in epoch_logs\n"""
tests/test_models.py,0,"b""import numpy as np\n\nfrom six.moves import xrange\n\nfrom keras.layers import Input, Dense, LSTM\n\nfrom kgp.models import Model\nfrom kgp.layers import GP\nfrom kgp.losses import gen_gp_loss\n\nN = 256\nbatch_size = 64\ninput_shape = (32, 10)\nlstm_dim = 16\ndense_dim = 1\noptimizer = 'adam'\n\ngp_test_config = {\n    'hyp': {'lik': -2.0, 'cov': np.array([[-0.7], [0.0]])},\n    'nb_train_samples': N,\n    'batch_size': batch_size,\n    'opt': {},\n    'inf': 'infExact',\n    'mean': 'meanZero',\n    'cov': 'covSEiso',\n    'lik': 'likGauss',\n    'dlik': 'dlikExact',\n    'verbose': 0,\n}\n\n\ndef build_model(nb_outputs=2):\n    inputs = Input(shape=input_shape)\n\n    # Neural transformations\n    lstm = LSTM(lstm_dim)(inputs)\n    dense = Dense(dense_dim)(lstm)\n\n    # GP outputs\n    outputs = [GP(**gp_test_config)(dense) for _ in xrange(nb_outputs)]\n\n    # Build the model\n    model = Model(inputs=inputs, outputs=outputs)\n\n    return model\n\n\ndef test_compile():\n    model = build_model()\n\n    # Generate losses for GP outputs\n    loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n\n    # Compile the model\n    model.compile(optimizer=optimizer, loss=loss)\n\n\ndef test_fit(epochs=10, seed=42):\n    rng = np.random.RandomState(seed)\n\n    for nb_outputs in [1, 2]:\n        # Generate dummy data\n        X_tr = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_tr = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n\n        # Build & compile the model\n        model = build_model(nb_outputs)\n        loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n\n        # Train the model\n        model.fit(X_tr, Y_tr,\n                  epochs=epochs,\n                  batch_size=batch_size,\n                  verbose=2)\n\n\ndef test_finetune(gp_n_iter=10, seed=42):\n    rng = np.random.RandomState(seed)\n\n    for nb_outputs in [1, 2]:\n        # Generate dummy data\n        X_tr = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_tr = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n\n        # Build & compile the model\n        model = build_model(nb_outputs)\n        loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n\n        # Finetune the model\n        model.finetune(X_tr, Y_tr,\n                       batch_size=batch_size,\n                       gp_n_iter=gp_n_iter,\n                       verbose=0)\n\n\ndef test_evaluate(seed=42):\n    rng = np.random.RandomState(seed)\n\n    for nb_outputs in [1, 2]:\n        # Generate dummy data\n        X_ts = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_ts = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n\n        # Build & compile the model\n        model = build_model(nb_outputs)\n        loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n\n        # Evaluate the model\n        nlml = model.evaluate(X_ts, Y_ts, batch_size=batch_size, verbose=0)\n\n\ndef test_predict(seed=42):\n    rng = np.random.RandomState(seed)\n\n    for nb_outputs in [1, 2]:\n        # Generate dummy data\n        X_tr = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_tr = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n        X_ts = rng.normal(size=(N, input_shape[0], input_shape[1]))\n        Y_ts = [rng.normal(size=(N, 1)) for _ in xrange(nb_outputs)]\n\n        # Build & compile the model\n        model = build_model(nb_outputs)\n        loss = [gen_gp_loss(gp) for gp in model.output_gp_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n\n        # Predict\n        Y_pr = model.predict(X_ts, X_tr, Y_tr,\n                             batch_size=batch_size, verbose=0)\n        assert type(Y_pr) is list\n        assert len(Y_pr) == len(Y_ts)\n        assert np.all([(yp.shape == yt.shape) for yp, yt in zip(Y_pr, Y_ts)])\n"""
kgp/backend/__init__.py,0,"b""from __future__ import absolute_import\n\nimport os\nimport sys\n\n_BACKEND = 'gpml'\n_ENGINE = 'octave'\n\nif 'GP_BACKEND' in os.environ:\n    _BACKEND = os.environ['GP_BACKEND']\n    assert _BACKEND in {'gpml'}\nelse:\n    os.environ['GP_BACKEND'] = _BACKEND\n\nif 'GP_ENGINE' in os.environ:\n    _ENGINE = os.environ['GP_ENGINE']\n    assert _ENGINE in {'matlab', 'octave'}\nelse:\n    os.environ['GP_ENGINE'] = _ENGINE\n\nif _ENGINE == 'matlab':\n    if sys.version_info[0] > 2:\n        raise ImportError('MATLAB engine does not support Python 3.x. '\n                          'Please switch to Octave engine or use Python 2.x.')\n\nif _BACKEND == 'gpml':\n    sys.stderr.write('Using GPML backend with ')\n    sys.stderr.write('MATLAB ' if _ENGINE == 'matlab' else 'Octave ')\n    sys.stderr.write('engine as the default one.\\n')\n    from .gpml import GPML as GP_BACKEND\nelse:\n    raise Exception('Unknown backend: ' + str(_BACKEND))\n"""
kgp/backend/engines.py,0,"b'""""""\nComputational engines used by GP backends.\n""""""\nimport numpy as np\n\n\nclass Engine(object):\n    """"""The base class for computational engines.\n    """"""\n    def addpath(self, path):\n        self._eng.addpath(path)\n\n    def eval(self, expr, verbose):\n        """"""Evaluate an expression.\n        """"""\n        raise NotImplementedError\n\n    def push(self, name, var):\n        """"""Push a variable into the engine session under the given name.\n        """"""\n        raise NotImplementedError\n\n    def pull(self, name):\n        """"""Pull a variable from the engine session.\n        """"""\n        raise NotImplementedError\n\n\nclass MATLABEngine(Engine):\n    def __init__(self):\n        import matlab.engine\n        from matlab import double as matdouble\n\n        from StringIO import StringIO\n\n        self._matarray = matdouble\n        self._eng = matlab.engine.start_matlab()\n        self._devnull = StringIO()\n\n    def push(self, name, var):\n        # Convert np.ndarrays into matlab.doubles and push into the workspace\n        if type(var) is np.ndarray:\n            self._eng.workspace[name] = self._matarray(var.tolist())\n        elif type(var) is dict:\n            var_copy = var.copy()\n            for k, v in var_copy.iteritems():\n                if type(v) is np.ndarray:\n                    var_copy[k] = self._matarray(v.tolist())\n            self._eng.workspace[name] = var_copy\n        elif type(var) in {list, int, float}:\n            self._eng.workspace[name] = var\n        else:\n            raise ValueError(""Unknown type (%s) variable being pushed ""\n                             ""into the MATLAB session."" % type(var))\n\n    def pull(self, name):\n        var = self._eng.workspace[name]\n        if type(var) is self._matarray:\n            var = np.asarray(var)\n        elif type(var) is dict:\n            for k, v in var.iteritems():\n                if type(v) is self._matarray:\n                    var[k] = np.asarray(v)\n        return var\n\n    def eval(self, expr, verbose=0):\n        assert type(expr) is str\n        stdout = None if verbose else self._devnull\n        self._eng.eval(expr, nargout=0, stdout=stdout)\n\n\nclass OctaveEngine(Engine):\n    def __init__(self, jit_enable=True):\n        from oct2py import Oct2Py\n        from oct2py import Struct\n\n        self._struct = Struct\n        self._eng = Oct2Py()\n        if jit_enable:\n            self._eng.eval(\'jit_enable(1)\', verbose=0)\n        self._eng.eval(\'pkg load statistics\', verbose=0)\n\n    def push(self, name, var):\n        if type(var) is np.ndarray and var.dtype == \'float32\':\n            # Octave does not support `sparse matrix * dense matrix` operations\n            # for float32 type, hence we cast `var` to float64 before pushing\n            # into the Octave session\n            var = var.astype(\'float64\')\n        self._eng.push(name, var)\n\n    def pull(self, name):\n        var = self._eng.pull(name)\n        if type(var) is self._struct:\n            var = dict(var)\n        return var\n\n    def eval(self, expr, verbose=0):\n        assert type(expr) is str\n        self._eng.eval(expr, verbose=verbose)\n\n'"
kgp/backend/gpml.py,0,"b'""""""GPML backend for Gaussian processes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\n\nfrom pprint import pprint\n\n# MATLAB scripts\n_gp_train_epoch = """"""\nhyp = minimize(hyp, @gp, -{n_iter:d}, {inf}, {mean}, {cov}, {lik}, X_tr, y_tr);\n""""""\n_gp_evaluate = """"""\n[nlZ dnlZ post     ] = gp(hyp, {inf}, {mean}, {cov}, {lik}, {X}, {y});\n""""""\n_gp_predict = """"""\n[ymu ys2 fmu fs2   ] = gp(hyp, {inf}, {mean}, {cov}, {lik}, X_tr, y_tr, X_tst);\n""""""\n_gp_predict_grid = """"""\n[post nlZ dnlZ     ] = infGrid(hyp, {mean}, {cov}, {lik}, X_tr, y_tr, opt);\n[fmu fs2 ymu ys2   ] = post.predict(X_tst);\n""""""\n_gp_dlik = """"""\n[dlik_dx           ] = dlik(hyp, {mean}, {cov}, {lik}, {dcov}, {X}, {y});\n""""""\n_gp_create_grid = """"""\nxg = covGrid(\'create\', {X}, eq, k);\n""""""\n\n\nclass GPML(object):\n    """"""Class that implements backend functionality for Gaussian processes.\n\n    Arguments:\n    ----------\n        engine : str (either \'matlab\' or \'octave\')\n        gpml_path : str or None\n    """"""\n    def __init__(self, engine=None, engine_kwargs=None, gpml_path=None):\n        if engine is None:\n            if \'GP_ENGINE\' in os.environ:\n                engine = os.environ[\'GP_ENGINE\']\n            else:\n                raise ValueError(""GP_ENGINE is neither provided nor available ""\n                                 ""in the environment."")\n        if engine == \'matlab\':\n            from .engines import MATLABEngine as Engine\n        elif engine == \'octave\':\n            from .engines import OctaveEngine as Engine\n        else:\n            raise ValueError(\'Unknown GP_ENGINE: %s\' % engine)\n\n        if gpml_path is None:\n            if \'GPML_PATH\' in os.environ:\n                gpml_path = os.environ[\'GPML_PATH\']\n            else:\n                current_dir = os.path.dirname(os.path.realpath(__file__))\n                gpml_path = os.path.join(current_dir, \'gpml\')\n                if not os.path.isfile(os.path.join(gpml_path, \'startup.m\')):\n                    raise ValueError(\n                        ""Neither GPML_PATH is provided nor GPML library is ""\n                        ""available directly from keras-gp. ""\n                        ""Please make sure you cloned keras-gp *recursively*."")\n\n        engine_kwargs = engine_kwargs or {}\n        self.eng = Engine(**engine_kwargs)\n        self.eng.addpath(gpml_path)\n        self.eng.eval(\'startup\', verbose=0)\n\n        utils_path = os.path.join(os.path.dirname(__file__), \'utils\')\n        self.eng.addpath(utils_path)\n\n    def configure(self, input_dim, hyp, opt, inf, mean, cov, lik, dlik,\n                  grid_kwargs=None, cov_args=None, mean_args=None, verbose=1):\n        """"""Configure GPML-based Guassian process.\n\n        Arguments:\n        ----------\n            input_dim : uint\n                The dimension of the GP inputs.\n            hyp : dict\n                A dictionary of GP hyperparameters.\n            opt : dict\n                GPML inference/training options (see GPML doc for details).\n            inf : str\n                Name of the inference method (see GPML doc for details).\n            mean : str\n                Name of the mean function (see GPML doc for details).\n            cov : str\n                Name of the covariance function (see GPML doc for details).\n            lik : str\n                Name of the likelihood function (see GPML doc for details).\n            dlik : str\n                Name of the function that computes dlik/dx.\n            grid_kwargs : dict\n                \'eq\' : uint\n                    Whether to enforce an equispaced grid.\n                \'k\' : uint or float in (0, 1]\n                    Number of inducing points per dimension.\n                \'xg\' : list\n                    Manually specified grid. Must be represented in the form of\n                    a list of np.ndarrays that specify the grid points for each\n                    dimension.\n        """"""\n        self.config = {}\n        self.config[\'lik\']  = ""{@%s}"" % lik\n\n        if mean_args is None:\n            self.config[\'mean\'] = ""{@%s}"" % mean\n        else:\n            self.config[\'mean\'] = \'{@%s, %s}\' % (\n                mean, \', \'.join(str(e) for e in mean_args))\n        \n        self.config[\'inf\']  = ""{@(varargin) %s(varargin{:}, opt)}"" % inf\n        self.config[\'dlik\'] = ""@(varargin) %s(varargin{:}, opt)"" % dlik\n\n        if inf == \'infGrid\':\n            assert grid_kwargs is not None, (\n                ""GPML: No arguments provided for grid generation for infGrid."")\n            self.eng.push(\'k\', float(grid_kwargs[\'k\']))\n            self.eng.push(\'eq\', float(grid_kwargs[\'eq\']))\n            if \'xg\' in grid_kwargs:\n                self.eng.push(\'xg\', grid_kwargs[\'xg\'])\n            if cov_args is None:\n                cov = \',\'.join(input_dim * [\'{@%s}\' % cov])\n            else:\n                cov = \',\'.join(input_dim * [\n                    \'{@%s, %s}\' % (cov, \', \'.join(str(e) for e in cov_args))])\n            if input_dim > 1:\n                cov = \'{\' + cov + \'}\'\n            hyp[\'cov\'] = np.tile(hyp[\'cov\'], (1, input_dim))\n            self.config[\'cov\'] = ""{@covGrid, %s, xg}"" % cov\n            self.config[\'dcov\'] = ""[]""\n            self.using_grid = True\n        else:\n            hyp[\'cov\'] = np.asarray(hyp[\'cov\'])\n            self.config[\'cov\'] = ""{@%s}"" % cov\n            self.config[\'dcov\'] = ""@d%s"" % cov\n            self.using_grid = False\n\n        self.eng.push(\'hyp\', hyp)\n        self.eng.push(\'opt\', opt)\n        self.eng.eval(""dlik = %s;"" % self.config[\'dlik\'], verbose=0)\n\n        if verbose:\n            print(""GP configuration:"")\n            pprint(self.config)\n\n    def update_data(self, which_set, X, y=None, verbose=0):\n        """"""Update data in GP backend.\n        """"""\n        assert which_set in {\'tr\', \'tst\', \'val\', \'tmp\'}\n        self.eng.push(\'X_\' + which_set, X)\n        if y is not None:\n            self.eng.push(\'y_\' + which_set, y)\n\n    def update_grid(self, which_set, verbose=0):\n        """"""Update grid for grid-based GP inference.\n        """"""\n        assert which_set in {\'tr\', \'tst\', \'val\', \'tmp\'}\n        self.config.update({\'X\': \'X_\' + which_set, \'y\': None})\n        self.eng.eval(_gp_create_grid.format(**self.config), verbose=verbose)\n\n    def evaluate(self, which_set, X=None, y=None, verbose=0):\n        """"""Evaluate GP for given X and y.\n        Return negative log marginal likelihood.\n        """"""\n        assert which_set in {\'tr\', \'tst\', \'val\', \'tmp\'}\n        if X is not None and y is not None:\n            self.update_data(which_set, X, y)\n        X_name, y_name = \'X_\' + which_set, \'y_\' + which_set\n        self.config.update({\'X\': X_name, \'y\': y_name})\n        self.eng.eval(_gp_evaluate.format(**self.config), verbose=verbose)\n        nlZ = self.eng.pull(\'nlZ\')\n        return nlZ\n\n    def predict(self, X, X_tr=None, y_tr=None, return_var=False, verbose=0):\n        """"""Predict ymu and ys2 for a given X. Return ymu and ys2.\n        """"""\n        self.update_data(\'tst\', X)\n        if X_tr is not None and y_tr is not None:\n            self.update_data(\'tr\', X_tr, y_tr)\n        if self.using_grid:\n            self.eng.eval(_gp_predict_grid.format(**self.config),\n                          verbose=verbose)\n        else:\n            self.eng.eval(_gp_predict.format(**self.config),\n                          verbose=verbose)\n        preds = self.eng.pull(\'ymu\')\n        if return_var:\n            preds = (preds, self.eng.pull(\'ys2\'))\n        return preds\n\n    def train(self, n_iter, X_tr=None, y_tr=None, verbose=0):\n        """"""Train GP for `n_iter` iterations. Return a dict of hyperparameters.\n        """"""\n        if X_tr is not None and y_tr is not None:\n            self.update_data(\'tr\', X_tr, y_tr)\n        self.config.update({\'n_iter\': n_iter})\n        self.eng.eval(_gp_train_epoch.format(**self.config), verbose=verbose)\n        hyp = self.eng.pull(\'hyp\')\n        return hyp\n\n    def get_dlik_dx(self, which_set, verbose=0):\n        """"""Get derivative of the log marginal likelihood w.r.t. the kernel.\n        """"""\n        assert which_set in {\'tr\', \'tst\', \'val\', \'tmp\'}\n        X_name, y_name = \'X_\' + which_set, \'y_\' + which_set\n        self.config.update({\'X\': X_name, \'y\': y_name})\n        self.eng.eval(_gp_dlik.format(**self.config), verbose=verbose)\n        dlik_dx = self.eng.pull(\'dlik_dx\')\n        return dlik_dx\n\n    def eval_predict(self, X, X_tr=None, y_tr=None, verbose=0):\n        """"""Use the grid-specific fast evaluation and prediction.\n        """"""\n        self.update_data(\'tst\', X)\n        if X_tr is not None and y_tr is not None:\n            self.update_data(\'tr\', X_tr, y_tr)\n        if self.using_grid:\n            self.eng.eval(_gp_predict_grid.format(**self.config),\n                          verbose=verbose)\n        else:\n            self.eng.eval(_gp_evaluate.format(**self.config), verbose=verbose)\n            self.eng.eval(_gp_predict.format(**self.config), verbose=verbose)\n        return self.eng.pull(\'nlZ\'), self.eng.pull(\'ymu\')\n'"
kgp/datasets/__init__.py,0,b''
kgp/datasets/car.py,0,"b'""""""\nInterface for the self-driving car sensors dataset.\n\nDescription of the data:\n""times""                 : seconds from start\n""fiber_accel""           : m/s**2\n""fiber_compass""         : this is a concatenation of x_y_z below\n""fiber_compass_x""       : magnetic north (we are not sure actually ;))\n""fiber_compass_y""       : orthogonal to magnetic north in car plane\n""fiber_compass_z""       : orthogonal to x and y\n""fiber_gyro""            : deg/s - roll, pitch, yaw in car-centric frame\n""gps_1_pos""             : ECEF coordinates\n""gps_1_vel""             : ECEF velocity\n""gps_2_pos""             : ECEF coordinates (another GPS sensor)\n""gps_2_vel""             : ECEF velocity (another GPS sensor)\n""imu_compass""           : same as compass above\n""imu_gyro""              : deg/s - roll, pitch, yaw in car-centric frame\n""speed""                 : m/s\n""speed_abs""             : m/s\n""steering_angle""        : deg/m\n""velodyne_gps""          : ECEF\n""velodyne_imu""          : deg/s\n""left_lanes""            : 4 + 4 coefficients of the cubic polynomials (x and y)\n""right_lanes""           : 4 + 4 coefficients of the cubic polynomials (x and y)\n""radar_leads""           : x-y (front-left) coordinates of a detected vehicle\n\nAbout the self-driving car:\nhttp://www.bloomberg.com/features/2015-george-hotz-self-driving-car/\n""""""\nimport os\nimport sys\nimport h5py\n\nimport numpy as np\nimport pandas as pd\n\n\ndef load_data(start=0., stop=100., t_step=1, set_name=\'full\',\n              ins=[\'gps_1_vel\', \'fiber_gyro\'], outs=[\'speed\'],\n              gps_to_enu=False,\n              pts_per_lane=7,\n              verbose=1):\n    """"""Load the car sensors data.\n\n    Arguments:\n    ----------\n        t_step : uint\n            Take data points t_step apart from each other in time.\n        start : float in [0., 100.)\n        stop : float in (0., 100.]\n        ins : list of str\n            Names of the fields to use as inputs.\n        outs : list of str\n            Names of the fields to use as outputs.\n        verbose : uint (default: 1)\n    """"""\n    if \'DATA_PATH\' not in os.environ:\n        raise Exception(""Cannot find DATA_PATH variable in the environment. ""\n                        ""DATA_PATH should be the folder that contains ""\n                        ""`self-driving/` directory with car sensors data. ""\n                        ""Please export DATA_PATH before loading the data."")\n\n    datadir = os.path.join(os.environ[\'DATA_PATH\'], \'self-driving\')\n    dataset_name = \'car_sensors{}.h5\'.format(\n        \'_{}\'.format(set_name) if set_name != \'full\' else \'\')\n    dataset_path = os.path.join(datadir, dataset_name)\n    if not os.path.exists(dataset_path):\n        raise Exception(""Cannot find data: %s"" % dataset_path)\n\n    if verbose:\n        print(\'Loading data from %s...\' % os.path.basename(dataset_path))\n        sys.stdout.flush()\n\n    f = h5py.File(dataset_path, \'r\')\n\n    # Compute the data slice\n    N = len(f[\'times\'][:])\n    start_t = int((start/100.) * N)\n    stop_t = int((stop/100.) * N)\n    idx = slice(start_t, stop_t, t_step)\n\n    # Read & preprocess inputs\n    input_nnz, input_vars = None, []\n    for name in ins:\n        X = f[name][idx]\n        if gps_to_enu and name.startswith(\'gps\'):\n            assert X.shape[1] == 3\n            x0, y0, z0 = X[0, 0], X[0, 1], X[0, 2]\n            x, y, z = X[:, 0], X[:, 1], X[:, 2]\n            X = ecef2enu(x, y, z, x0, y0, z0)\n        if name.endswith(\'lanes\'):\n            if verbose:\n                print(\'...constructing %s\' % name)\n                sys.stdout.flush()\n            X, nnz = construct_lanes(X, pts_per_lane)\n            input_nnz = nnz if input_nnz is None \\\n                        else np.logical_and(input_nnz, nnz)\n        if name == \'radar_leads\':\n            X[X[:, 0] < 0] = np.nan\n            X_df = pd.DataFrame(X).fillna(method=\'ffill\')\n            X = X_df.values\n            # X[X[:, 0] < 0., 0], X[X[:, 1] < 0., 1] = 200., 0.\n        if len(X.shape) == 1:\n            input_vars.append(X[:, None])\n        else:\n            assert len(X.shape) == 2\n            input_vars.append(X)\n\n    # Read & preprocess targets\n    target_nnz, target_vars = None, []\n    for name in outs:\n        Y = f[name][idx]\n        if gps_to_enu and name.startswith(\'gps\'):\n            assert Y.shape[1] == 3\n            x0, y0, z0 = Y[0, :]\n            x, y, z = Y[:, 0], Y[:, 1], Y[:, 2]\n            Y = ecef2enu(x, y, z, x0, y0, z0)\n        if name.endswith(\'lanes\'):\n            if verbose:\n                print(\'...constructing %s\' % name)\n                sys.stdout.flush()\n            Y, nnz = construct_lanes(Y, pts_per_lane)\n            target_nnz = nnz if target_nnz is None \\\n                         else np.logical_and(target_nnz, nnz)\n        if name == \'radar_leads\':\n            Y[Y[:, 0] < 0] = np.nan\n            Y_df = pd.DataFrame(Y).fillna(method=\'ffill\')\n            Y = Y_df.values\n            # Y[Y[:, 0] < 0., 0], Y[Y[:, 1] < 0., 1] = 200., 0.\n        if len(Y.shape) == 1:\n            target_vars.append(Y[:, None])\n        else:\n            assert len(Y.shape) == 2\n            target_vars.append(Y)\n\n    f.close()\n\n    X = np.nan_to_num(np.concatenate(input_vars, axis=1)) if input_vars \\\n        else None\n    Y = np.nan_to_num(np.concatenate(target_vars, axis=1)) if target_vars \\\n        else None\n\n    # Make sure inputs and targets have the same indexes\n    nnz = None\n    if (input_nnz is not None) and (target_nnz is not None):\n        nnz = np.logical_and(input_nnz, target_nnz)\n    elif input_nnz is not None:\n        nnz = input_nnz\n    elif target_nnz is not None:\n        nnz = target_nnz\n\n    if (X is not None) and (nnz is not None):\n        X = X[nnz]\n    if (Y is not None) and (nnz is not None):\n        Y = Y[nnz]\n\n    if verbose:\n        print(\'Done.\')\n        print(\'# of loaded points: %d\' % len(X))\n        if X is not None:\n            print(\'Inputs shape: %s\' % X.shape[1:])\n        if Y is not None:\n            print(\'Targets shape: %s\' % Y.shape[1:])\n\n    return X, Y\n\n\ndef construct_lanes(data, pts_per_lane):\n    poly_x, poly_y = data[:,:4], data[:,4:]\n    lane_x = np.vstack(\n        [np.polyval(poly_x[t], np.linspace(0, 50, pts_per_lane))\n         for t in xrange(poly_x.shape[0])])\n    lane_y = np.vstack(\n        [np.polyval(poly_y[t], np.linspace(0, 50, pts_per_lane))\n         for t in xrange(poly_y.shape[0])])\n    lane = np.hstack([lane_x, lane_y])\n    nnz = lane_x.sum(axis=1) > 0.\n    return lane, nnz\n\n\ndef ecef2enu(x, y, z, x0, y0, z0):\n    """"""Convert ECEF to local coordinates.\n    """"""\n    lat0, lon0, alt0 = ecef2geodetic(x0, y0, z0)\n\n    u, v, w = x - x0, y - y0, z - z0\n    t     =  np.cos(lon0) * u + np.sin(lon0) * v\n    East  = -np.sin(lon0) * u + np.cos(lon0) * v\n    North = -np.sin(lat0) * t + np.cos(lat0) * w\n    Up    =  np.cos(lat0) * t + np.sin(lat0) * w\n\n    return np.vstack([East, North, Up]).T\n\n\ndef ecef2geodetic(x, y, z):\n    """"""http://www.astro.uni.torun.pl/~kb/Papers/geod/Geod-BG.htm\n\n    This algorithm provides a converging solution to the latitude equation in\n    terms of the parametric or reduced latitude form (v).\n    This algorithm provides a uniform solution over all latitudes as it does\n    not involve division by cos(phi) or sin(phi).\n    """"""\n    ell = {}\n    ell[\'a\'] = 6378137.\n    ell[\'f\'] = 1. / 298.2572235630\n    ell[\'b\'] = ell[\'a\'] * (1 - ell[\'f\'])\n\n    ea = ell[\'a\']\n    eb = ell[\'b\']\n    rad = np.hypot(x, y)\n    # Constant required for Latitude equation\n    rho = np.arctan2(eb * z, ea * rad)\n    #Constant required for latitude equation\n    c = (ea**2 - eb**2) / np.hypot(ea * rad, eb * z)\n    # Starter for the Newtons Iteration Method\n    vnew = np.arctan2(ea * z, eb * rad)\n    # Initializing the parametric latitude\n    v = 0\n    count = 0\n    while (v != vnew).any() and count < 5:\n        v = vnew.copy()\n        # Newtons Method for computing iterations\n        vnew = v - ((2 * np.sin(v - rho) - c * np.sin(2 * v)) /\n                    (2 * (np.cos(v - rho) - c * np.cos(2 * v))))\n        count += 1\n\n    # Computing latitude from the root of the latitude equation\n    lat = np.arctan2(ea * np.tan(vnew), eb)\n    lon = np.arctan2(y, x)\n    alt = ((rad - ea * np.cos(vnew)) * np.cos(lat) +\n           (z - eb * np.sin(vnew)) * np.sin(lat))\n\n    return lat, lon, alt\n'"
kgp/datasets/data_utils.py,0,"b'""""""\nData utility functions.\n""""""\nimport numpy as np\n\nfrom six.moves import xrange\n\n\ndef standardize_data(data):\n    if type(data) is not dict:\n        raise ValueError(""Data should be a dict."")\n    if \'train\' not in data:\n        raise ValueError(""Cannot find training set in the data."")\n\n    # Compute mean and std on the train set\n    X, y = data[\'train\']\n    X_mean = X.reshape((-1, X.shape[-1])).mean(axis=0)\n    X_std = X.reshape((-1, X.shape[-1])).std(axis=0)\n    y_mean = y.reshape((-1, y.shape[-1])).mean(axis=0)\n    y_std = y.reshape((-1, y.shape[-1])).std(axis=0)\n\n    # Standardize the data\n    for set_name, (X, y) in data.iteritems():\n        X = (X - X_mean) / X_std\n        y = (y - y_mean) / y_std\n        if len(y.shape) == 1:\n            y = y[:, None]\n        data[set_name] = [X, y]\n\n    return data\n\n\ndef data_to_seq(X, Y,\n                t_lag=8,\n                t_future_shift=1,\n                t_future_steps=1,\n                t_sw_step=1,\n                X_pad_with=None):\n    """"""Slice X and Y into sequences using a sliding window.\n\n    Arguments:\n    ----------\n        X : np.ndarray with ndim == 2\n        Y : np.ndarray with ndim == 2\n        t_sw_step : uint (default: 1)\n            Time step of the sliding window.\n        t_lag : uint (default: 8)\n            (t_lag - 1) past time steps used to construct a sequence of inputs.\n        t_future_shift : uint (default: 0)\n            How far in the future predictions are supposed to be made.\n        t_future_steps : uint (default: 1)\n            How many steps to be predicted from t + t_future_shift.\n            The sequences are constructed in a way that the model can be\n            trained to predict Y[t_future:t_future+t_future_steps]\n            from X[t-t_lag:t] where t_future = t + t_future_shift.\n    """"""\n    # Assume that provided X and Y are matrices and are aligned in time\n    assert X.ndim == 2 and Y.ndim == 2\n    assert len(X) == len(Y)\n\n    # Pad X sequence from the beginning\n    X_padding_left = np.zeros((t_lag - 1, X.shape[1]))\n    X = np.vstack([X_padding_left, X])\n\n    # The future steps of X should be skipped, hence padded with zeros\n    # X_padding_right = np.zeros((t_future_shift+t_future_steps-1, X.shape[1]))\n\n    nb_t_steps = 1 + len(X) - (t_future_shift + (t_future_steps - 1))\n    X_seq, Y_seq = [], []\n    for t in xrange(t_lag, nb_t_steps, t_sw_step):\n        t_past = t - t_lag\n        t_future = t_past + t_future_shift\n        # X_seq.append(np.vstack([X[t_past:t], X_padding_right]))\n        X_seq.append(X[t_past:t])\n        Y_seq.append(Y[t_future:t_future+t_future_steps])\n    X_seq = np.asarray(X_seq)\n    Y_seq = np.asarray(Y_seq)\n\n    return [X_seq, Y_seq]\n'"
kgp/datasets/gef2012_power.py,0,"b'""""""\nInterface for the data from GEF 2012 power forecasting Kaggle competition.\n""""""\nimport os\nimport sys\n\nimport numpy as np\n\nfrom six.moves import cPickle as pkl\n\ndef load_data(start=0., stop=100., t_step=1, average_load=True, verbose=1):\n    """"""Load the GEF-power data.\n\n    Arguments:\n    ----------\n        t_step : uint\n            Take data points t_step apart from each other in time.\n        start : float in [0., 100.)\n        stop : float in (0., 100.]\n        average_load : bool (default: True)\n            Whether to use hourly power load averaged across 20 stations, or\n            separate power loads for each station.\n        verbose : uint (default: 1)\n    """"""\n    if \'DATA_PATH\' not in os.environ:\n        raise Exception(""Cannot find DATA_PATH variable in the environment. ""\n                        ""DATA_PATH should be the folder that contains ""\n                        ""`GEF/power/` directory with the GEF data. ""\n                        ""Please export DATA_PATH before loading the data."")\n\n    dataset_path = os.path.join(os.environ[\'DATA_PATH\'], \'GEF\', \'power\',\n                                \'temperature_load_history.pkl\')\n    if not os.path.exists(dataset_path):\n        raise Exception(""Cannot find data: %s"" % dataset_path)\n\n    if verbose:\n        sys.stdout.write(\'Loading data from %s...\' %\n                         os.path.basename(dataset_path))\n        sys.stdout.flush()\n\n    with open(dataset_path) as fp:\n        data = pkl.load(fp)\n\n    # Select the data points\n    start = int((start/100.) * len(data[\'X\']))\n    stop = int((stop/100.) * len(data[\'X\']))\n\n    X = data[\'X\'][start:stop:t_step,:]\n    Y = data[\'Y\'][start:stop:t_step,:]\n\n    if average_load:\n        Y = np.mean(Y, axis=1, keepdims=True)\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n        print(\'# of loaded points: %d\' % len(X))\n\n    return X, Y\n'"
kgp/datasets/gef2012_wind.py,0,"b'""""""\nInterface for the data from GEF 2012 wind forecasting Kaggle competition.\n""""""\nimport os\nimport sys\n\nimport numpy as np\n\nfrom six.moves import cPickle as pkl\n\ndef load_data(start=0., stop=100., t_step=1, wf=[1], t_decay=None, verbose=1):\n    """"""Load the GEF-wind data.\n\n    Arguments:\n    ----------\n        t_step : uint\n            Take data points t_step apart from each other in time.\n        start : float in [0., 100.)\n        stop : float in (0., 100.]\n        wf : list of uints in [1, 7]\n            The list of wind farm ids to use data from.\n        t_decay : float in (0., 1.] or None (default: None)\n            At each time step `t`, we are given 4 forecasts of the wind\n            parameters. Each of these forecasts was made at some point\n            `t_forecast` during the past 48 hours with a step of 12 hours\n            between the forecasts. We average these forecasts with coefficients\n            proportional to `t_dacay^forecast_n` so that older forecasts\n            contribute less than the fresh ones. If None, all forecasts are\n            used as independent features (no averaging is done).\n        verbose : uint (default: 1)\n    """"""\n    if \'DATA_PATH\' not in os.environ:\n        raise Exception(""Cannot find DATA_PATH variable in the environment. ""\n                        ""DATA_PATH should be the folder that contains ""\n                        ""`GEF/wind/` directory with the GEF data. ""\n                        ""Please export DATA_PATH before loading the data."")\n\n    hours_path = os.path.join(os.environ[\'DATA_PATH\'],\n                              \'GEF\', \'wind\', \'hours.npy\')\n    targets_path = os.path.join(os.environ[\'DATA_PATH\'],\n                                \'GEF\', \'wind\', \'targets.npy\')\n    forecast_paths = [\n        os.path.join(os.environ[\'DATA_PATH\'], \'GEF\', \'wind\',\n                     \'windforecasts_wf%d.npy\' % i)\n        for i in wf]\n\n    if not os.path.exists(hours_path):\n        raise Exception(""Cannot find data: %s"" % hours_path)\n    if not os.path.exists(targets_path):\n        raise Exception(""Cannot find data: %s"" % targets_path)\n    for path in forecast_paths:\n        if not os.path.exists(path):\n            raise Exception(""Cannot find data: %s"" % path)\n\n    if verbose:\n        sys.stdout.write(\'Loading data...\')\n        sys.stdout.flush()\n\n    hours = np.hstack(len(wf) * [np.load(hours_path)])\n    targets = np.load(targets_path)[:, [i - 1 for i in wf]]\n    forecasts = np.vstack([np.load(path) for path in forecast_paths])\n\n    if t_decay is not None:\n        decay = t_decay**np.arange(forecasts.shape[1])\n        decay /= decay.sum()\n        forecasts = (forecasts * decay).sum(axis=1)\n    else:\n        forecasts = forecasts.reshape(len(forecasts), -1)\n\n    X = np.hstack([hours[:, None], forecasts])\n    Y = targets.T.reshape(-1, 1)\n\n    # Select the data points\n    start = int((start/100.) * len(X))\n    stop = int((stop/100.) * len(X))\n\n    X = X[start:stop:t_step,:]\n    Y = Y[start:stop:t_step,:]\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n        print(\'# of loaded points: %d\' % len(X))\n\n    return X, Y\n'"
kgp/datasets/kin40k.py,0,"b'""""""\nInterface for kin40k UCI dataset.\n""""""\nimport os\nimport sys\nimport numpy as np\n\n\ndef load_data(start=0., stop=100., verbose=1):\n    """"""Load the Kin40k data.\n\n    Arguments:\n    ----------\n        start : float in [0., 100.) (default: 0.)\n        stop : float in (0., 100.] (default: 100.)\n        verbose : uint (default: 1)\n    """"""\n    if \'DATA_PATH\' not in os.environ:\n        raise Exception(""Cannot find DATA_PATH variable in the environment. ""\n                        ""DATA_PATH should be the folder that contains ""\n                        ""`kin40k/` directory with the data. ""\n                        ""Please export DATA_PATH before loading the data."")\n\n    dataset_path = os.path.join(os.environ[\'DATA_PATH\'],\'kin40k\',\'kin40k.npz\')\n    if not os.path.exists(dataset_path):\n        raise Exception(""Cannot find data: %s"" % dataset_path)\n\n    if verbose:\n        sys.stdout.write(\'Loading data...\')\n        sys.stdout.flush()\n\n    data = np.load(dataset_path)\n\n    start = int((start/100.) * len(data[\'X\']))\n    stop = int((stop/100.) * len(data[\'X\']))\n\n    X = data[\'X\'][start:stop,:]\n    Y = data[\'Y\'][start:stop,:]\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n        print(\'# of loaded points: %d\' % len(X))\n\n    return X, Y\n'"
kgp/datasets/quandl_ustycr.py,0,"b'""""""\nInterface for the U.S. Treasury Yield Curve Rates data.\nSource: https://www.quandl.com/data/USTREASURY/YIELD-Treasury-Yield-Curve-Rates\n\nDESCRIPTION\nThese rates are commonly referred to as ""Constant Maturity Treasury"" rates, or\nCMTs. Yields are interpolated by the Treasury from the daily yield curve. This\ncurve, which relates the yield on a security to its time to maturity is based\non the closing market bid yields on actively traded Treasury securities in the\nover-the-counter market. These market yields are calculated from composites of\nquotations obtained by the Federal Reserve Bank of New York. The yield values\nare read from the yield curve at fixed maturities, currently 1, 3 and 6 months\nand 1, 2, 3, 5, 7, 10, 20, and 30 years. This method provides a yield for a 10\nyear maturity, for example, even if no outstanding security has exactly 10\nyears remaining to maturity.\n\nAvailable time series that correspond to fixed times to maturity:\n\'1 MO\', \'3 MO\', \'6 MO\',\n\'1 YR\', \'2 YR\', \'3YR\', \'5 YR\', \'7 YR\' ,\'10 YR\', \'20 YR\', \'30 YR\'.\n""""""\nimport os\nimport sys\n\nimport numpy as np\nimport pandas as pd\n\nimport Quandl\n\ndef load_data(start=0., stop=100., t_step=1,\n              ins=[\'6 MO\', \'1 YR\', \'3 YR\', \'5 YR\'], outs=[\'10 YR\'], verbose=1):\n    """"""Load the U.S. Treasury Yield Curve Rates data.\n\n    Arguments:\n    ----------\n        t_step : uint (default: 1)\n            Take data points t_step days from each other.\n        start : float in [0., 100.)\n        stop : float in (0., 100.]\n        ins : list of str (default: [\'3 MO\', \'6 MO\', \'1 YR\', \'3 YR\'])\n            Names of the fields (times to maturity) to use as inputs.\n        outs : list of str (default: [\'10 YR\'])\n            Names of the fields (times to maturity) to use as outputs.\n        verbose : uint (default: 1)\n    """"""\n    if \'DATA_PATH\' not in os.environ:\n        raise Exception(""Cannot find DATA_PATH variable in the environment. ""\n                        ""DATA_PATH should be the folder that contains ""\n                        ""`quandl/` directory where the data is cached. ""\n                        ""Please export DATA_PATH before using `load_data`."")\n\n    # Create a directory for Quandl data if it does not exist\n    quandl_data_path = os.path.join(os.environ[\'DATA_PATH\'], \'quandl\')\n    if not os.path.isdir(quandl_data_path):\n        os.makedirs(quandl_data_path)\n\n    if verbose:\n        sys.stdout.write(\'Loading data...\')\n        sys.stdout.flush()\n\n    # Load the data\n    dataset_path = os.path.join(quandl_data_path, \'USTYCR.pkl\')\n    if not os.path.exists(dataset_path):\n        USTYCR = Quandl.get(""USTREASURY/YIELD"")\n        USTYCR.to_pickle(dataset_path)\n    else:\n        USTYCR = pd.read_pickle(dataset_path)\n\n    X = USTYCR[ins].as_matrix()\n    Y = USTYCR[outs].as_matrix()\n\n    start = int((start/100.) * len(X))\n    stop = int((stop/100.) * len(X))\n\n    # Select the data points\n    X = X[start:stop:t_step,:]\n    Y = Y[start:stop:t_step,:]\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n        print(\'# of loaded points: %d\' % len(X))\n\n    return X, Y\n'"
kgp/datasets/sysid.py,0,"b'""""""\nInterface for system identification data (Actuator and Drives).\n""""""\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport zipfile\nimport warnings\n\nimport numpy as np\nimport scipy.io as sio\n\nfrom six.moves import urllib\nfrom six.moves import cPickle as pkl\n\nSOURCE_URLS = {\n    \'actuator\': \'https://www.cs.cmu.edu/~mshediva/assets/data/actuator.mat\',\n    \'drives\': \'https://www.cs.cmu.edu/~mshediva/assets/data/NonlinearData.zip\',\n}\n\n\ndef maybe_download(data_path, dataset_name, verbose=1):\n    source_url = SOURCE_URLS[dataset_name]\n    datadir_path = os.path.join(data_path, \'sysid\')\n    dataset_path = os.path.join(datadir_path, dataset_name + \'.mat\')\n\n    # Create directories (if necessary)\n    if not os.path.isdir(datadir_path):\n        os.makedirs(datadir_path)\n\n    # Download & extract the data (if necessary)\n    if not os.path.isfile(dataset_path):\n        if dataset_name == \'actuator\':\n            urllib.request.urlretrieve(source_url, dataset_path)\n        if dataset_name == \'drives\':\n            assert source_url.endswith(\'.zip\')\n            archive_path = os.path.join(datadir_path, \'tmp.zip\')\n            urllib.request.urlretrieve(source_url, archive_path)\n            with zipfile.ZipFile(archive_path, \'r\') as zfp:\n                zfp.extract(\'DATAPRBS.MAT\', datadir_path)\n            os.rename(os.path.join(datadir_path, \'DATAPRBS.MAT\'), dataset_path)\n            os.remove(archive_path)\n        if verbose:\n            print(""Successfully downloaded `%s` dataset from %s."" %\n                  (dataset_name, source_url))\n\n    return dataset_path\n\n\ndef load_data(dataset_name, t_step=1, start=0., stop=100.,\n              use_targets=True, batch_size=None, verbose=1):\n    """"""Load the system identification data.\n\n    Arguments:\n    ----------\n        t_step : uint (default: 1)\n            Take data points t_step apart from each other in time.\n        start : float in [0., 100.) (default: 0.)\n        stop : float in (0., 100.] (default: 100.)\n        use_targets : bool (default: True)\n        batch_size : uint or None (default: None)\n        verbose : uint (default: 1)\n    """"""\n    if dataset_name not in {\'actuator\', \'drives\'}:\n        raise ValueError(""Unknown dataset: %s"" % dataset_name)\n\n    if \'DATA_PATH\' not in os.environ:\n        warnings.warn(""Cannot find DATA_PATH variable in the environment. ""\n                      ""Using <current_working_directory>/data/ instead."")\n        DATA_PATH = os.path.join(os.getcwd(), \'data\')\n    else:\n        DATA_PATH = os.environ[\'DATA_PATH\']\n\n    dataset_path = maybe_download(DATA_PATH, dataset_name, verbose=verbose)\n    if not os.path.exists(dataset_path):\n        raise Exception(""Cannot find data: %s"" % dataset_path)\n\n    if verbose:\n        sys.stdout.write(\'Loading data...\')\n        sys.stdout.flush()\n\n    data_mat = sio.loadmat(dataset_path)\n    if dataset_name == \'actuator\':\n        X, Y = data_mat[\'u\'], data_mat[\'p\']\n    if dataset_name == \'drives\':\n        X, Y = data_mat[\'u1\'], data_mat[\'z1\']\n\n    start = int((start/100.) * len(X))\n    stop = int((stop/100.) * len(X))\n\n    X = X[start:stop:t_step,:]\n    Y = Y[start:stop:t_step,:]\n\n    if use_targets:\n        X = np.hstack([X, Y])\n\n    if batch_size:\n        nb_examples = (len(X) // batch_size) * batch_size\n        X = X[:nb_examples]\n        Y = Y[:nb_examples]\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n        print(\'# of loaded points: %d\' % len(X))\n\n    return X, Y\n'"
kgp/utils/__init__.py,0,b''
kgp/utils/assemble.py,0,"b'""""""\nUtility functions for assembling Keras and KGP models.\n""""""\nimport os\nimport sys\nimport yaml\n\nimport numpy as np\n\nfrom six.moves import xrange\n\nimport keras\nfrom keras import layers, regularizers\nfrom keras.models import Model as KerasModel\n\nimport kgp\nfrom kgp.models import Model\nfrom kgp.layers import GP\n\n\nNN_DEFAULT_PARAMS = {\n    \'H_dim\': 32,\n    \'H_activation\': \'relu\',\n    \'dropout\': 0.5,\n    \'dropout_W\': 0.0,\n    \'dropout_U\': 0.0,\n    \'batch_norm\': \'null\',\n    \'stateful\': False,\n}\n\n\nGP_DEFAULT_PARAMS = {\n    \'cov\': \'SEiso\',\n    \'hyp_lik\': -2.0,\n    \'hyp_cov\': [[-0.7], [0.0]],\n    \'opt\': {},\n    \'grid_kwargs\': {\'eq\': 1, \'k\': 1e2},\n    \'update_grid\': True,\n}\n\n\ndef load_NN_configs(filename, input_shape, output_shape,\n                    params=None, verbose=0):\n    path = os.path.join(kgp.__path__[0], \'configs\', filename)\n    with open(path) as fp:\n        config_templates = fp.read()\n    if params is None:\n        params = NN_DEFAULT_PARAMS\n    else:\n        temp = NN_DEFAULT_PARAMS.copy()\n        temp.update(params)\n        params = temp\n    configs = config_templates.format(input_shape=list(input_shape),\n                                      output_shape=list(output_shape),\n                                      **params)\n    if verbose:\n        print(configs)\n    return yaml.load(configs)\n\n\ndef load_GP_configs(filename, nb_outputs, batch_size, nb_train_samples,\n                    params=None, verbose=0):\n    path = os.path.join(kgp.__path__[0], \'configs\', filename)\n    with open(path) as fp:\n        config_templates = fp.read()\n    if params is None:\n        params = GP_DEFAULT_PARAMS\n    else:\n        temp = GP_DEFAULT_PARAMS.copy()\n        temp.update(params)\n        params = temp\n    configs = config_templates.format(nb_outputs=nb_outputs,\n                                      batch_size=batch_size,\n                                      nb_train_samples=nb_train_samples,\n                                      **params)\n    if verbose:\n        print(configs)\n    return yaml.load(configs)\n\n\ndef assemble(name, params):\n    if name == \'NARX\':\n        return assemble_narx(params)\n    elif name == \'GP-NARX\':\n        return assemble_gpnarx(*params)\n    elif name == \'RNN\':\n        return assemble_rnn(params)\n    elif name == \'GP-RNN\':\n        return assemble_gprnn(*params)\n    elif name == \'LSTM\':\n        return assemble_rnn(params)\n    elif name == \'GP-LSTM\':\n        return assemble_gprnn(*params)\n    elif name == \'GRU\':\n        return assemble_rnn(params)\n    elif name == \'GP-GRU\':\n        return assemble_gprnn(*params)\n    else:\n        raise ValueError(""Unknown name of the model: %s."" % name)\n\n\ndef assemble_narx(params, final_reshape=True):\n    """"""Construct a NARX model of the form: X-[H1-H2-...-HN]-Y.\n    All the H-layers are Dense and optional, i.e., depend on whether they are\n    specified in the params dictionary. Here, X is a sequence.\n    """"""\n    # Input layer\n    input_shape = params[\'input_shape\']\n    inputs = layers.Input(shape=input_shape)\n\n    # Flatten the time dimension\n    target_shape = (np.prod(input_shape), )\n    previous = layers.Reshape(target_shape)(inputs)\n\n    # Hidden layers\n    for layer in params[\'hidden_layers\']:\n        Layer = layers.deserialize(\n            {\'class_name\': layer[\'name\'], \'config\': layer[\'config\']})\n        previous = Layer(previous)\n        if \'dropout\' in layer and layer[\'dropout\'] is not None:\n            previous = layers.Dropout(layer[\'dropout\'])(previous)\n        if \'batch_norm\' in layer and layer[\'batch_norm\'] is not None:\n            previous = layers.BatchNormalization(**layer[\'batch_norm\'])(previous)\n\n    # Output layer\n    output_shape = params[\'output_shape\']\n    output_dim = np.prod(output_shape)\n    outputs = layers.Dense(output_dim)(previous)\n\n    if final_reshape:\n        outputs = layers.Reshape(output_shape)(outputs)\n\n    return KerasModel(inputs=inputs, outputs=outputs)\n\n\ndef assemble_gpnarx(nn_params, gp_params):\n    """"""Construct an GP-NARX model of the form: X-[H1-H2-...-HN]-GP-Y.\n    """"""\n    # Assemble NARX\n    NARX = assemble_narx(nn_params, final_reshape=False)\n\n    # Inputs and NARX layer\n    inputs = NARX.inputs\n    narx = NARX(inputs)\n\n    # Output GP layers\n    outputs = [GP(**gp_params[\'config\'])(narx)\n               for _ in xrange(gp_params[\'nb_outputs\'])]\n\n    return Model(inputs=inputs, outputs=outputs)\n\n\ndef assemble_rnn(params, final_reshape=True):\n    """"""Construct an RNN/LSTM/GRU model of the form: X-[H1-H2-...-HN]-Y.\n    All the H-layers are optional recurrent layers and depend on whether they\n    are specified in the params dictionary.\n    """"""\n    # Input layer\n    input_shape = params[\'input_shape\']\n    inputs = layers.Input(shape=input_shape)\n    # inputs = layers.Input(batch_shape=[20] + list(input_shape))\n\n    # Masking layer\n    previous = layers.Masking(mask_value=0.0)(inputs)\n\n    # Hidden layers\n    for layer in params[\'hidden_layers\']:\n        Layer = layers.deserialize(\n            {\'class_name\': layer[\'name\'], \'config\': layer[\'config\']})\n        previous = Layer(previous)\n        if \'dropout\' in layer and layer[\'dropout\'] is not None:\n            previous = layers.Dropout(layer[\'dropout\'])(previous)\n        if \'batch_norm\' in layer and layer[\'batch_norm\'] is not None:\n            previous = layers.BatchNormalization(**layer[\'batch_norm\'])(previous)\n\n    # Output layer\n    output_shape = params[\'output_shape\']\n    output_dim = np.prod(output_shape)\n    outputs = layers.Dense(output_dim)(previous)\n\n    if final_reshape:\n        outputs = layers.Reshape(output_shape)(outputs)\n\n    return KerasModel(inputs=inputs, outputs=outputs)\n\n\ndef assemble_gprnn(nn_params, gp_params):\n    """"""Construct an GP-RNN/LSTM/GRU model of the form: X-[H1-H2-...-HN]-GP-Y\n    """"""\n    # Assemble RNN\n    RNN = assemble_rnn(nn_params, final_reshape=False)\n\n    # Inputs and RNN layer\n    inputs = RNN.inputs\n    rnn = RNN(inputs)\n\n    # Output GP layers\n    outputs = [GP(**gp_params[\'config\'])(rnn)\n               for _ in xrange(gp_params[\'nb_outputs\'])]\n\n    return Model(inputs=inputs, outputs=outputs)\n'"
kgp/utils/experiment.py,0,"b'""""""\nUtility functions for running experiments, saving results, etc.\n""""""\nimport os\nimport sys\nimport warnings\n\nimport numpy as np\n\nfrom keras.callbacks import ModelCheckpoint\n\nfrom kgp.metrics import root_mean_squared_error as RMSE\n\n\ndef train(model, data,\n          epochs=100,\n          batch_size=128,\n          callbacks=None,\n          checkpoint=None,\n          checkpoint_monitor=\'val_loss\',\n          verbose=1,\n          **fit_kwargs):\n    """"""Train the model on the data.\n\n    Arguments:\n    ----------\n        model : Model\n            Assumes the model has been already compiled.\n        data : dict\n        epochs : uint (default: 100)\n        batch_size : uint (default: 128)\n        callbacks : list (default: None)\n        checkpoint : str (default: None)\n        verbose : uint (default: 1)\n\n    Returns:\n    --------\n        history : training history\n    """"""\n    X_train, y_train = data[\'train\']\n    X_test, y_test = data[\'test\']\n    validation_data = data[\'valid\'] if \'valid\' in data else None\n    callbacks = callbacks or []\n\n    # Make sure the checkpoints directory exists\n    if checkpoint is not None:\n        if not os.path.isdir(\'checkpoints/\'):\n            os.makedirs(\'checkpoints/\')\n\n    # Update list of callbacks\n    if checkpoint is not None:\n        callbacks += [\n            ModelCheckpoint(\'checkpoints/%s.h5\' % checkpoint,\n                            monitor=checkpoint_monitor,\n                            save_weights_only=True,\n                            save_best_only=True)\n        ]\n\n    # Train the model\n    if verbose:\n        sys.stdout.write(""Training...\\n"")\n        sys.stdout.flush()\n\n    history = model.fit(X_train, y_train, validation_data=validation_data,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=callbacks, verbose=verbose,\n                        **fit_kwargs)\n\n    if verbose:\n        sys.stdout.write(\'Done.\\n\')\n\n    # Test the model\n    if checkpoint is not None:\n        if os.path.isfile(\'checkpoints/%s.h5\' % checkpoint):\n            model.load_weights(\'checkpoints/%s.h5\' % checkpoint)\n        else:\n            warnings.warn(\'Checkpoint file was specified, but no models were \'\n                          \'saved by the monitor. Make sure the validation \'\n                          \'dataset is specified and the monitoring channel \'\n                          \'is set correctly.\')\n\n    return history\n\n'"
tests/backend/test_gpml.py,0,"b'""""""\nTests for GPML backend.\n""""""\nimport os\nimport sys\nimport pytest\nimport numpy as np\n\nfrom kgp.backend import GP_BACKEND\n\nAVAILABLE_ENGINES = set()\ntry:\n    import oct2py\n    AVAILABLE_ENGINES.add(\'octave\')\nexcept ImportError:\n    pass\ntry:\n    import matlab.engine\n    AVAILABLE_ENGINES.add(\'matlab\')\nexcept ImportError:\n    pass\nif not AVAILABLE_ENGINES:\n    sys.stderr.write(""No computational engines is available. ""\n                     ""GPML tests will be skipped.\\n"")\n\ngp_test_config = {\n    \'input_dim\': 1,\n    \'hyp\': {\'lik\': -2.0, \'mean\': [], \'cov\': np.array([[-0.7], [0.0]])},\n    \'opt\': {},\n    \'inf\': \'infExact\',\n    \'mean\': \'meanZero\',\n    \'cov\': \'covSEiso\',\n    \'lik\': \'likGauss\',\n    \'dlik\': \'dlikExact\',\n    \'verbose\': 0,\n}\n\nmsgp_test_config = {\n    \'input_dim\': 1,\n    \'hyp\': {\'lik\': -2.0, \'mean\': [], \'cov\': np.array([[-0.7], [0.0]])},\n    \'opt\': {\'cg_maxit\': 500, \'cg_tol\': 1e-4},\n    \'inf\': \'infGrid\',\n    \'mean\': \'meanZero\',\n    \'cov\': \'covSEiso\',\n    \'lik\': \'likGauss\',\n    \'dlik\': \'dlikGrid\',\n    \'grid_kwargs\': {\'eq\': 1, \'k\': 100},\n    \'verbose\': 0,\n}\n\ndef configure(engine):\n    gp = GP_BACKEND(engine)\n\n    gp.configure(**gp_test_config)\n    gp.configure(**msgp_test_config)\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_update_data(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X = rng.normal(size=(N, D))\n    y = rng.normal(size=(N, D))\n\n    # Test updating data\n    gp.configure(**gp_test_config)\n    gp.update_data(\'tmp\', X, y)\n    gp.eng.eval(""assert(exist(\'X_tmp\',\'var\') ~= 0)"")\n    gp.eng.eval(""assert(exist(\'y_tmp\',\'var\') ~= 0)"")\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_update_grid(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X = rng.normal(size=(N, D))\n\n    # Test updating the grid\n    gp.configure(**msgp_test_config)\n    gp.update_data(\'tmp\', X)\n    gp.update_grid(\'tmp\')\n    gp.eng.eval(""assert(exist(\'xg\',\'var\') ~= 0)"")\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_evaluate(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X = rng.normal(size=(N, D))\n    y = rng.normal(size=(N, D))\n    gp.update_data(\'tmp\', X, y)\n\n    # Evaluate GP\n    gp.configure(**gp_test_config)\n    nlZ = gp.evaluate(\'tmp\')\n\n    # Evaluate MSGP\n    gp.configure(**msgp_test_config)\n    gp.update_grid(\'tmp\')\n    nlZ = gp.evaluate(\'tmp\')\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_predict(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X_tr = rng.normal(size=(N, D))\n    y_tr = rng.normal(size=(N, D))\n    X_tst = rng.normal(size=(N, D))\n    gp.update_data(\'tr\', X_tr, y_tr)\n\n    # Predict using GP\n    gp.configure(**gp_test_config)\n    ymu, ys2 = gp.predict(X_tst, return_var=True)\n\n    assert type(ymu) is np.ndarray and type(ys2) is np.ndarray\n    assert ymu.shape == ys2.shape\n\n    # Predict using MSGP\n    gp.configure(**msgp_test_config)\n    gp.update_grid(\'tr\')\n    ymu, ys2 = gp.predict(X_tst, return_var=True)\n\n    assert type(ymu) is np.ndarray and type(ys2) is np.ndarray\n    assert ymu.shape == ys2.shape\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_train(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X_tr = rng.normal(size=(N, D))\n    y_tr = rng.normal(size=(N, D))\n\n    # Train a GP\n    gp.configure(**gp_test_config)\n    hyp = gp.train(5, X_tr, y_tr)\n\n    assert type(hyp) is dict\n    assert set(hyp.keys()) == set(gp_test_config[\'hyp\'].keys())\n\n    # Train an MSGP\n    gp.configure(**msgp_test_config)\n    gp.update_data(\'tr\', X_tr, y_tr)\n    gp.update_grid(\'tr\')\n    hyp = gp.train(5)\n\n    assert type(hyp) is dict\n    assert set(hyp.keys()) == set(gp_test_config[\'hyp\'].keys())\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_get_dlik_dx(engine, N=100, D=1, seed=42):\n    gp = GP_BACKEND(engine)\n\n    rng = np.random.RandomState(seed)\n    X = rng.normal(size=(N, D))\n    y = rng.normal(size=(N, D))\n    gp.update_data(\'tmp\', X, y)\n\n    # GP\n    gp.configure(**gp_test_config)\n    dlik_dx = gp.get_dlik_dx(\'tmp\')\n    assert type(dlik_dx) is np.ndarray\n    assert dlik_dx.shape == (N, D)\n\n    # MSGP\n    gp.configure(**msgp_test_config)\n    gp.update_grid(\'tmp\')\n    dlik_dx = gp.get_dlik_dx(\'tmp\')\n    assert type(dlik_dx) is np.ndarray\n    assert dlik_dx.shape == (N, D)\n\n@pytest.mark.parametrize(\'engine\', AVAILABLE_ENGINES)\ndef test_grad_checks(engine):\n    gp = GP_BACKEND(engine)\n\n    dir_path = os.path.dirname(os.path.abspath(__file__))\n    grad_checks_path = os.path.join(dir_path, \'matlab\')\n    gp.eng.addpath(grad_checks_path)\n\n    gp.eng.eval(\'grad_check_covSEiso\')\n    gp.eng.eval(\'grad_check_covSEard\')\n    gp.eng.eval(\'grad_check_likExact\')\n    gp.eng.eval(\'grad_check_likGrid\')\n'"
tests/datasets/test_data_utils.py,0,"b'""""""\nTests for datasets and their utility functions.\n""""""\nimport numpy as np\n\nfrom six.moves import xrange\n\nfrom kgp.datasets.data_utils import *\n\n\nX_dim = 10\nY_dim = 3\n\nt_lag = 5\nt_future_shift = 1\nt_future_steps = 2\nt_sw_step = 1\n\n\ndef test_data_to_seq():\n    N = 10\n    X = np.arange(N * X_dim).reshape(N, X_dim)\n    Y = np.arange(N * Y_dim).reshape(N, Y_dim)\n    X_seq, Y_seq = data_to_seq(X, Y,\n                               t_lag=t_lag,\n                               t_future_shift=t_future_shift,\n                               t_future_steps=t_future_steps,\n                               t_sw_step=t_sw_step)\n\n    t_future = t_future_shift + t_future_steps - 1\n    expected_num_seq = N - t_future\n    # assert X_seq.shape == (expected_num_seq, t_lag + t_future, X_dim)\n    assert X_seq.shape == (expected_num_seq, t_lag, X_dim)\n    assert Y_seq.shape == (expected_num_seq, t_future_steps, Y_dim)\n'"
tests/utils/test_assemble.py,0,"b""from six.moves import xrange\n\nimport numpy as np\n\nimport kgp\nfrom kgp.losses import gen_gp_loss\nfrom kgp.utils.assemble import *\n\n# Test parameters\nN = 256\nbatch_size = 64\ninput_shape = (32, 10)\noutput_shape = (2, )\noptimizer = 'rmsprop'\n\n# Load configs\nnarx_configs = load_NN_configs(filename='narx.yaml',\n                               input_shape=input_shape,\n                               output_shape=output_shape)\nlstm_configs = load_NN_configs(filename='lstm.yaml',\n                               input_shape=input_shape,\n                               output_shape=output_shape)\nrnn_configs = load_NN_configs(filename='rnn.yaml',\n                              input_shape=input_shape,\n                              output_shape=output_shape)\ngru_configs = load_NN_configs(filename='gru.yaml',\n                              input_shape=input_shape,\n                              output_shape=output_shape)\ngp_configs = load_GP_configs(filename='gp.yaml',\n                             nb_outputs=np.prod(output_shape),\n                             batch_size=batch_size,\n                             nb_train_samples=N)\n\ndef test_assemble_narx():\n    for i in xrange(3):\n        model = assemble('NARX', narx_configs[str(i) + 'H'])\n        model.compile(optimizer=optimizer, loss='mse')\n        assert model.built\n\ndef test_assemble_gpnarx():\n    for gp_type in ['GP', 'MSGP']:\n        model = assemble('GP-NARX', [narx_configs['1H'], gp_configs[gp_type]])\n        loss = [gen_gp_loss(gp) for gp in model.output_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n        assert model.built\n\ndef test_assemble_rnn():\n    for i in xrange(1, 3):\n        model = assemble('RNN', rnn_configs[str(i) + 'H'])\n        model.compile(optimizer=optimizer, loss='mse')\n        assert model.built\n\ndef test_assemble_gprnn():\n    for gp_type in ['GP', 'MSGP']:\n        model = assemble('GP-RNN', [rnn_configs['1H'], gp_configs[gp_type]])\n        loss = [gen_gp_loss(gp) for gp in model.output_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n        assert model.built\n\ndef test_assemble_lstm():\n    for i in xrange(1, 3):\n        model = assemble('LSTM', lstm_configs[str(i) + 'H'])\n        model.compile(optimizer=optimizer, loss='mse')\n        assert model.built\n\ndef test_assemble_gplstm():\n    for gp_type in ['GP', 'MSGP']:\n        model = assemble('GP-LSTM', [lstm_configs['1H'], gp_configs[gp_type]])\n        loss = [gen_gp_loss(gp) for gp in model.output_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n        assert model.built\n\ndef test_assemble_gru():\n    for i in xrange(1, 3):\n        model = assemble('GRU', gru_configs[str(i) + 'H'])\n        model.compile(optimizer=optimizer, loss='mse')\n        assert model.built\n\ndef test_assemble_gpgru():\n    for gp_type in ['GP', 'MSGP']:\n        model = assemble('GP-GRU', [gru_configs['1H'], gp_configs[gp_type]])\n        loss = [gen_gp_loss(gp) for gp in model.output_layers]\n        model.compile(optimizer=optimizer, loss=loss)\n        assert model.built\n"""
