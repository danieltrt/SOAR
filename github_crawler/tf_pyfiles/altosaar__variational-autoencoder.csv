file_path,api_count,code
data.py,0,"b'""""""Get the binarized MNIST dataset and convert to hdf5.\nFrom https://github.com/yburda/iwae/blob/master/datasets.py\n""""""\nimport urllib.request\nimport os\nimport numpy as np\nimport h5py\n\n\ndef parse_binary_mnist(data_dir):\n  def lines_to_np_array(lines):\n    return np.array([[int(i) for i in line.split()] for line in lines])\n  with open(os.path.join(data_dir, \'binarized_mnist_train.amat\')) as f:\n    lines = f.readlines()\n  train_data = lines_to_np_array(lines).astype(\'float32\')\n  with open(os.path.join(data_dir, \'binarized_mnist_valid.amat\')) as f:\n    lines = f.readlines()\n  validation_data = lines_to_np_array(lines).astype(\'float32\')\n  with open(os.path.join(data_dir, \'binarized_mnist_test.amat\')) as f:\n    lines = f.readlines()\n  test_data = lines_to_np_array(lines).astype(\'float32\')\n  return train_data, validation_data, test_data\n\n\ndef download_binary_mnist(fname):\n  data_dir = \'/tmp/\'\n  subdatasets = [\'train\', \'valid\', \'test\']\n  for subdataset in subdatasets:\n    filename = \'binarized_mnist_{}.amat\'.format(subdataset)\n    url = \'http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_{}.amat\'.format(\n        subdataset)\n    local_filename = os.path.join(data_dir, filename)\n    urllib.request.urlretrieve(url, local_filename)\n\n  train, validation, test = parse_binary_mnist(data_dir)\n  \n  data_dict = {\'train\': train, \'valid\': validation, \'test\': test}\n  f = h5py.File(fname, \'w\')\n  f.create_dataset(\'train\', data=data_dict[\'train\'])\n  f.create_dataset(\'valid\', data=data_dict[\'valid\'])\n  f.create_dataset(\'test\', data=data_dict[\'test\'])\n  f.close()\n  print(f\'Saved binary MNIST data to: {fname}\')\n'"
flow.py,0,"b'""""""Credit: mostly based on Ilya\'s excellent implementation here: https://github.com/ikostrikov/pytorch-flows""""""\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass InverseAutoregressiveFlow(nn.Module):\n  """"""Inverse Autoregressive Flows with LSTM-type update. One block.\n  \n  Eq 11-14 of https://arxiv.org/abs/1606.04934\n  """"""\n  def __init__(self, num_input, num_hidden, num_context):\n    super().__init__()\n    self.made = MADE(num_input=num_input, num_output=num_input * 2,\n                     num_hidden=num_hidden, num_context=num_context)\n    # init such that sigmoid(s) is close to 1 for stability\n    self.sigmoid_arg_bias = nn.Parameter(torch.ones(num_input) * 2)\n    self.sigmoid = nn.Sigmoid()\n    self.log_sigmoid = nn.LogSigmoid()\n\n  def forward(self, input, context=None):\n    m, s = torch.chunk(self.made(input, context), chunks=2, dim=-1)\n    s = s + self.sigmoid_arg_bias \n    sigmoid = self.sigmoid(s)\n    z = sigmoid * input + (1 - sigmoid) * m\n    return z, -self.log_sigmoid(s)\n\n\nclass FlowSequential(nn.Sequential):\n  """"""Forward pass.""""""\n\n  def forward(self, input, context=None):\n    total_log_prob = torch.zeros_like(input, device=input.device)\n    for block in self._modules.values():\n      input, log_prob = block(input, context)\n      total_log_prob += log_prob\n    return input, total_log_prob\n\n\nclass MaskedLinear(nn.Module):\n  """"""Linear layer with some input-output connections masked.""""""\n  def __init__(self, in_features, out_features, mask, context_features=None, bias=True):\n    super().__init__()\n    self.linear = nn.Linear(in_features, out_features, bias)\n    self.register_buffer(""mask"", mask)\n    if context_features is not None:\n      self.cond_linear = nn.Linear(context_features, out_features, bias=False)\n\n  def forward(self, input, context=None):\n    output =  F.linear(input, self.mask * self.linear.weight, self.linear.bias)\n    if context is None:\n      return output\n    else:\n      return output + self.cond_linear(context)\n\n\nclass MADE(nn.Module):\n  """"""Implements MADE: Masked Autoencoder for Distribution Estimation.\n\n  Follows https://arxiv.org/abs/1502.03509\n\n  This is used to build MAF: Masked Autoregressive Flow (https://arxiv.org/abs/1705.07057).\n  """"""\n  def __init__(self, num_input, num_output, num_hidden, num_context):\n    super().__init__()\n    # m corresponds to m(k), the maximum degree of a node in the MADE paper\n    self._m = []\n    self._masks = []\n    self._build_masks(num_input, num_output, num_hidden, num_layers=3)\n    self._check_masks()\n    modules = []\n    self.input_context_net = MaskedLinear(num_input, num_hidden, self._masks[0], num_context)\n    modules.append(nn.ReLU())\n    modules.append(MaskedLinear(num_hidden, num_hidden, self._masks[1], context_features=None))\n    modules.append(nn.ReLU())\n    modules.append(MaskedLinear(num_hidden, num_output, self._masks[2], context_features=None))\n    self.net = nn.Sequential(*modules)\n\n  def _build_masks(self, num_input, num_output, num_hidden, num_layers):\n    """"""Build the masks according to Eq 12 and 13 in the MADE paper.""""""\n    rng = np.random.RandomState(0)\n    # assign input units a number between 1 and D\n    self._m.append(np.arange(1, num_input + 1))\n    for i in range(1, num_layers + 1):\n      # randomly assign maximum number of input nodes to connect to\n      if i == num_layers:\n        # assign output layer units a number between 1 and D\n        m = np.arange(1, num_input + 1)\n        assert num_output % num_input == 0, ""num_output must be multiple of num_input""\n        self._m.append(np.hstack([m for _ in range(num_output // num_input)]))\n      else:\n        # assign hidden layer units a number between 1 and D-1\n        self._m.append(rng.randint(1, num_input, size=num_hidden))\n        #self._m.append(np.arange(1, num_hidden + 1) % (num_input - 1) + 1)\n      if i == num_layers:\n        mask = self._m[i][None, :] > self._m[i - 1][:, None]\n      else:\n        # input to hidden & hidden to hidden\n        mask = self._m[i][None, :] >= self._m[i - 1][:, None]\n      # need to transpose for torch linear layer, shape (num_output, num_input)\n      self._masks.append(torch.from_numpy(mask.astype(np.float32).T))\n\n  def _check_masks(self):\n    """"""Check that the connectivity matrix between layers is lower triangular.""""""\n    # (num_input, num_hidden)\n    prev = self._masks[0].t()\n    for i in range(1, len(self._masks)):\n      # num_hidden is second axis\n      prev = prev @ self._masks[i].t()\n    final = prev.numpy()\n    num_input = self._masks[0].shape[1]\n    num_output = self._masks[-1].shape[0]\n    assert final.shape == (num_input, num_output)\n    if num_output == num_input:\n      assert np.triu(final).all() == 0\n    else:\n      for submat in np.split(final, \n                             indices_or_sections=num_output // num_input,\n                             axis=1):\n        assert np.triu(submat).all() == 0\n\n  def forward(self, input, context=None):\n    # first hidden layer receives input and context\n    hidden = self.input_context_net(input, context)\n    # rest of the network is conditioned on both input and context\n    return self.net(hidden)\n\n\n\nclass Reverse(nn.Module):\n  """""" An implementation of a reversing layer from\n  Density estimation using Real NVP\n  (https://arxiv.org/abs/1605.08803).\n\n  From https://github.com/ikostrikov/pytorch-flows/blob/master/main.py\n  """"""\n\n  def __init__(self, num_input):\n    super(Reverse, self).__init__()\n    self.perm = np.array(np.arange(0, num_input)[::-1])\n    self.inv_perm = np.argsort(self.perm)\n\n  def forward(self, inputs, context=None, mode=\'forward\'):\n    if mode == ""forward"":\n      return inputs[:, :, self.perm], torch.zeros_like(inputs, device=inputs.device)\n    elif mode == ""inverse"":\n      return inputs[:, :, self.inv_perm], torch.zeros_like(inputs, device=inputs.device)\n    else:\n      raise ValueError(""Mode must be one of {forward, inverse}."")\n\n    \n'"
train_variational_autoencoder_pytorch.py,0,"b'""""""Fit a variational autoencoder to MNIST. \n\nNotes:\n  - run https://github.com/altosaar/proximity_vi/blob/master/get_binary_mnist.py to download binary MNIST file\n  - batch size is the innermost dimension, then the sample dimension, then latent dimension\n""""""\nimport torch\nimport torch.utils\nimport torch.utils.data\nfrom torch import nn\nimport nomen\nimport yaml\nimport numpy as np\nimport logging\nimport pathlib\nimport h5py\nimport random\nimport data\nimport flow\n\nconfig = """"""\nlatent_size: 128\nvariational: flow\nflow_depth: 2\ndata_size: 784\nlearning_rate: 0.001\nbatch_size: 128\ntest_batch_size: 512\nmax_iterations: 100000\nlog_interval: 10000\nearly_stopping_interval: 5\nn_samples: 128\nuse_gpu: true\ntrain_dir: $TMPDIR\ndata_dir: $TMPDIR\nseed: 582838\n""""""\n\nclass Model(nn.Module):\n  """"""Bernoulli model parameterized by a generative network with Gaussian latents for MNIST.""""""\n  def __init__(self, latent_size, data_size):\n    super().__init__()\n    self.register_buffer(\'p_z_loc\', torch.zeros(latent_size))\n    self.register_buffer(\'p_z_scale\', torch.ones(latent_size))\n    self.log_p_z = NormalLogProb()\n    self.log_p_x = BernoulliLogProb()\n    self.generative_network = NeuralNetwork(input_size=latent_size,\n                                            output_size=data_size, \n                                            hidden_size=latent_size * 2)\n\n  def forward(self, z, x):\n    """"""Return log probability of model.""""""\n    log_p_z = self.log_p_z(self.p_z_loc, self.p_z_scale, z).sum(-1, keepdim=True)\n    logits = self.generative_network(z)\n    # unsqueeze sample dimension\n    logits, x = torch.broadcast_tensors(logits, x.unsqueeze(1))\n    log_p_x = self.log_p_x(logits, x).sum(-1, keepdim=True)\n    return log_p_z + log_p_x\n\n    \nclass VariationalMeanField(nn.Module):\n  """"""Approximate posterior parameterized by an inference network.""""""\n  def __init__(self, latent_size, data_size):\n    super().__init__()\n    self.inference_network = NeuralNetwork(input_size=data_size, \n                                           output_size=latent_size * 2, \n                                           hidden_size=latent_size*2)\n    self.log_q_z = NormalLogProb()\n    self.softplus = nn.Softplus()\n\n  def forward(self, x, n_samples=1):\n    """"""Return sample of latent variable and log prob.""""""\n    loc, scale_arg = torch.chunk(self.inference_network(x).unsqueeze(1), chunks=2, dim=-1)\n    scale = self.softplus(scale_arg)\n    eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n    z = loc + scale * eps  # reparameterization\n    log_q_z = self.log_q_z(loc, scale, z).sum(-1, keepdim=True)\n    return z, log_q_z\n\n\nclass VariationalFlow(nn.Module):\n  """"""Approximate posterior parameterized by a flow (https://arxiv.org/abs/1606.04934).""""""\n  def __init__(self, latent_size, data_size, flow_depth):\n    super().__init__()\n    hidden_size = latent_size * 2\n    self.inference_network = NeuralNetwork(input_size=data_size, \n                                           # loc, scale, and context\n                                           output_size=latent_size * 3, \n                                           hidden_size=hidden_size)\n    modules = []\n    for _ in range(flow_depth):\n      modules.append(flow.InverseAutoregressiveFlow(num_input=latent_size,\n                                                    num_hidden=hidden_size,\n                                                    num_context=latent_size))\n      modules.append(flow.Reverse(latent_size))\n    self.q_z_flow = flow.FlowSequential(*modules)\n    self.log_q_z_0 = NormalLogProb()\n    self.softplus = nn.Softplus()\n\n  def forward(self, x, n_samples=1):\n    """"""Return sample of latent variable and log prob.""""""\n    loc, scale_arg, h = torch.chunk(self.inference_network(x).unsqueeze(1), chunks=3, dim=-1)\n    scale = self.softplus(scale_arg)\n    eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n    z_0 = loc + scale * eps  # reparameterization\n    log_q_z_0 = self.log_q_z_0(loc, scale, z_0)\n    z_T, log_q_z_flow = self.q_z_flow(z_0, context=h)\n    log_q_z = (log_q_z_0 + log_q_z_flow).sum(-1, keepdim=True)\n    return z_T, log_q_z\n\n\n\nclass NeuralNetwork(nn.Module):\n  def __init__(self, input_size, output_size, hidden_size):\n    super().__init__()\n    modules = [nn.Linear(input_size, hidden_size),\n               nn.ReLU(),\n               nn.Linear(hidden_size, hidden_size),\n               nn.ReLU(),\n               nn.Linear(hidden_size, output_size)]\n    self.net = nn.Sequential(*modules)\n    \n  def forward(self, input):\n    return self.net(input)\n\n\nclass NormalLogProb(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, loc, scale, z):\n    var = torch.pow(scale, 2)\n    return -0.5 * torch.log(2 * np.pi * var) - torch.pow(z - loc, 2) / (2 * var)\n\n\nclass BernoulliLogProb(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.bce_with_logits = nn.BCEWithLogitsLoss(reduction=\'none\')\n\n  def forward(self, logits, target):\n    # bernoulli log prob is equivalent to negative binary cross entropy\n    return -self.bce_with_logits(logits, target)\n\n\ndef cycle(iterable):\n  while True:\n    for x in iterable:\n      yield x\n\n\ndef load_binary_mnist(cfg, **kwcfg):\n  fname = cfg.data_dir / \'binary_mnist.h5\'\n  if not fname.exists():\n    print(\'Downloading binary MNIST data...\')\n    data.download_binary_mnist(fname)\n  f = h5py.File(pathlib.os.path.join(pathlib.os.environ[\'DAT\'], \'binary_mnist.h5\'), \'r\')\n  x_train = f[\'train\'][::]\n  x_val = f[\'valid\'][::]\n  x_test = f[\'test\'][::]\n  train = torch.utils.data.TensorDataset(torch.from_numpy(x_train))\n  train_loader = torch.utils.data.DataLoader(train, batch_size=cfg.batch_size, shuffle=True, **kwcfg)\n  validation = torch.utils.data.TensorDataset(torch.from_numpy(x_val))\n  val_loader = torch.utils.data.DataLoader(validation, batch_size=cfg.test_batch_size, shuffle=False)\n  test = torch.utils.data.TensorDataset(torch.from_numpy(x_test))\n  test_loader = torch.utils.data.DataLoader(test, batch_size=cfg.test_batch_size, shuffle=False)\n  return train_loader, val_loader, test_loader\n\n\ndef evaluate(n_samples, model, variational, eval_data):\n  model.eval()\n  total_log_p_x = 0.0\n  total_elbo = 0.0\n  for batch in eval_data:\n    x = batch[0].to(next(model.parameters()).device)\n    z, log_q_z = variational(x, n_samples)\n    log_p_x_and_z = model(z, x)\n    # importance sampling of approximate marginal likelihood with q(z)\n    # as the proposal, and logsumexp in the sample dimension\n    elbo = log_p_x_and_z - log_q_z\n    log_p_x = torch.logsumexp(elbo, dim=1) - np.log(n_samples)\n    # average over sample dimension, sum over minibatch\n    total_elbo += elbo.cpu().numpy().mean(1).sum()\n    # sum over minibatch\n    total_log_p_x += log_p_x.cpu().numpy().sum()\n  n_data = len(eval_data.dataset)\n  return total_elbo / n_data, total_log_p_x / n_data\n  \n  \nif __name__ == \'__main__\':\n  dictionary = yaml.load(config)\n  cfg = nomen.Config(dictionary)\n  cfg.parse_args()\n  device = torch.device(""cuda:0"" if cfg.use_gpu else ""cpu"")\n  torch.manual_seed(cfg.seed)\n  np.random.seed(cfg.seed)\n  random.seed(cfg.seed)\n\n  model = Model(latent_size=cfg.latent_size, \n                data_size=cfg.data_size)\n  if cfg.variational == \'flow\':\n    variational = VariationalFlow(latent_size=cfg.latent_size,\n                                  data_size=cfg.data_size,\n                                  flow_depth=cfg.flow_depth)\n  elif cfg.variational == \'mean-field\':\n    variational = VariationalMeanField(latent_size=cfg.latent_size,\n                                       data_size=cfg.data_size)\n  else:\n    raise ValueError(\'Variational distribution not implemented: %s\' % cfg.variational)\n\n  model.to(device)\n  variational.to(device)\n\n  optimizer = torch.optim.RMSprop(list(model.parameters()) +\n                                  list(variational.parameters()),\n                                  lr=cfg.learning_rate,\n                                  centered=True)\n\n  kwargs = {\'num_workers\': 4, \'pin_memory\': True} if cfg.use_gpu else {}\n  train_data, valid_data, test_data = load_binary_mnist(cfg, **kwargs)\n\n  best_valid_elbo = -np.inf\n  num_no_improvement = 0\n\n  for step, batch in enumerate(cycle(train_data)):\n    x = batch[0].to(device)\n    model.zero_grad()\n    variational.zero_grad()\n    z, log_q_z = variational(x, n_samples=1)\n    log_p_x_and_z = model(z, x)\n    # average over sample dimension\n    elbo = (log_p_x_and_z - log_q_z).mean(1)\n    # sum over batch dimension\n    loss = -elbo.sum(0)\n    loss.backward()\n    optimizer.step()\n\n    if step % cfg.log_interval == 0:\n      print(f\'step:\\t{step}\\ttrain elbo: {elbo.detach().cpu().numpy().mean():.2f}\')\n      with torch.no_grad():\n        valid_elbo, valid_log_p_x = evaluate(cfg.n_samples, model, variational, valid_data)\n      print(f\'step:\\t{step}\\t\\tvalid elbo: {valid_elbo:.2f}\\tvalid log p(x): {valid_log_p_x:.2f}\')\n      if valid_elbo > best_valid_elbo:\n        num_no_improvement = 0\n        best_valid_elbo = valid_elbo\n        states = {\'model\': model.state_dict(),\n                  \'variational\': variational.state_dict()}\n        torch.save(states, cfg.train_dir / \'best_state_dict\')\n      else:\n        num_no_improvement += 1\n\n      if num_no_improvement > cfg.early_stopping_interval:\n        checkpoint = torch.load(cfg.train_dir / \'best_state_dict\')\n        model.load_state_dict(checkpoint[\'model\'])\n        variational.load_state_dict(checkpoint[\'variational\'])\n        with torch.no_grad():\n          test_elbo, test_log_p_x = evaluate(cfg.n_samples, model, variational, test_data)\n        print(f\'step:\\t{step}\\t\\ttest elbo: {test_elbo:.2f}\\ttest log p(x): {test_log_p_x:.2f}\')\n        break\n'"
train_variational_autoencoder_tensorflow.py,16,"b'import itertools\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nimport tensorflow.contrib.slim as slim\nimport time\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nfrom imageio import imwrite\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\ntfkl = tfk.layers\ntfc = tf.compat.v1\n\nflags = tf.app.flags\nflags.DEFINE_string(\'data_dir\', \'/tmp/dat/\', \'Directory for data\')\nflags.DEFINE_string(\'logdir\', \'/tmp/log/\', \'Directory for logs\')\nflags.DEFINE_integer(\'latent_dim\', 100, \'Latent dimensionality of model\')\nflags.DEFINE_integer(\'batch_size\', 64, \'Minibatch size\')\nflags.DEFINE_integer(\'n_samples\', 1, \'Number of samples to save\')\nflags.DEFINE_integer(\'print_every\', 1000, \'Print every n iterations\')\nflags.DEFINE_integer(\'hidden_size\', 200, \'Hidden size for neural networks\')\nflags.DEFINE_integer(\'n_iterations\', 100000, \'number of iterations\')\n\nFLAGS = flags.FLAGS\n\n\ndef inference_network(x, latent_dim, hidden_size):\n  """"""Construct an inference network parametrizing a Gaussian.\n\n  Args:\n    x: A batch of MNIST digits.\n    latent_dim: The latent dimensionality.\n    hidden_size: The size of the neural net hidden layers.\n\n  Returns:\n    mu: Mean parameters for the variational family Normal\n    sigma: Standard deviation parameters for the variational family Normal\n  """"""\n  inference_net = tfk.Sequential([\n    tfkl.Flatten(),\n    tfkl.Dense(hidden_size, activation=tf.nn.relu),\n    tfkl.Dense(hidden_size, activation=tf.nn.relu),\n    tfkl.Dense(latent_dim * 2, activation=None)\n    ])\n  gaussian_params = inference_net(x)\n  # The mean parameter is unconstrained\n  mu = gaussian_params[:, :latent_dim]\n  # The standard deviation must be positive. Parametrize with a softplus\n  sigma = tf.nn.softplus(gaussian_params[:, latent_dim:])\n  return mu, sigma\n\n\ndef generative_network(z, hidden_size):\n  """"""Build a generative network parametrizing the likelihood of the data\n\n  Args:\n    z: Samples of latent variables\n    hidden_size: Size of the hidden state of the neural net\n\n  Returns:\n    bernoulli_logits: logits for the Bernoulli likelihood of the data\n  """"""\n  generative_net = tfk.Sequential([\n    tfkl.Dense(hidden_size, activation=tf.nn.relu),\n    tfkl.Dense(hidden_size, activation=tf.nn.relu),\n    tfkl.Dense(28 * 28, activation=None)\n    ])\n  bernoulli_logits = generative_net(z)\n  return tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n\n\ndef train():\n  # Train a Variational Autoencoder on MNIST\n\n  # Input placeholders\n  with tf.name_scope(\'data\'):\n    x = tfc.placeholder(tf.float32, [None, 28, 28, 1])\n    tfc.summary.image(\'data\', x)\n\n  with tfc.variable_scope(\'variational\'):\n    q_mu, q_sigma = inference_network(x=x,\n                                      latent_dim=FLAGS.latent_dim,\n                                      hidden_size=FLAGS.hidden_size)\n    # The variational distribution is a Normal with mean and standard\n    # deviation given by the inference network\n    q_z = tfp.distributions.Normal(loc=q_mu, scale=q_sigma)\n    assert q_z.reparameterization_type == tfp.distributions.FULLY_REPARAMETERIZED\n\n  with tfc.variable_scope(\'model\'):\n    # The likelihood is Bernoulli-distributed with logits given by the\n    # generative network\n    p_x_given_z_logits = generative_network(z=q_z.sample(),\n                                            hidden_size=FLAGS.hidden_size)\n    p_x_given_z = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n    posterior_predictive_samples = p_x_given_z.sample()\n    tfc.summary.image(\'posterior_predictive\',\n                     tf.cast(posterior_predictive_samples, tf.float32))\n\n  # Take samples from the prior\n  with tfc.variable_scope(\'model\', reuse=True):\n    p_z = tfp.distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n                               scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n    p_z_sample = p_z.sample(FLAGS.n_samples)\n    p_x_given_z_logits = generative_network(z=p_z_sample,\n                                            hidden_size=FLAGS.hidden_size)\n    prior_predictive = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n    prior_predictive_samples = prior_predictive.sample()\n    tfc.summary.image(\'prior_predictive\',\n                     tf.cast(prior_predictive_samples, tf.float32))\n\n  # Take samples from the prior with a placeholder\n  with tfc.variable_scope(\'model\', reuse=True):\n    z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n    p_x_given_z_logits = generative_network(z=z_input,\n                                            hidden_size=FLAGS.hidden_size)\n    prior_predictive_inp = tfp.distributions.Bernoulli(logits=p_x_given_z_logits)\n    prior_predictive_inp_sample = prior_predictive_inp.sample()\n\n  # Build the evidence lower bound (ELBO) or the negative loss\n  kl = tf.reduce_sum(tfp.distributions.kl_divergence(q_z, p_z), 1)\n  expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n                                          [1, 2, 3])\n\n  elbo = tf.reduce_sum(expected_log_likelihood - kl, 0)\n  optimizer = tfc.train.RMSPropOptimizer(learning_rate=0.001)\n  train_op = optimizer.minimize(-elbo)\n\n  # Merge all the summaries\n  summary_op = tfc.summary.merge_all()\n\n  init_op = tfc.global_variables_initializer()\n\n  # Run training\n  sess = tfc.InteractiveSession()\n  sess.run(init_op)\n\n  mnist_data = tfds.load(name=\'binary_mnist\', split=\'train\', shuffle_files=False)\n  dataset = mnist_data.repeat().shuffle(buffer_size=1024).batch(FLAGS.batch_size)\n\n  print(\'Saving TensorBoard summaries and images to: %s\' % FLAGS.logdir)\n  train_writer = tfc.summary.FileWriter(FLAGS.logdir, sess.graph)\n\n  t0 = time.time()\n  for i, batch in enumerate(tfds.as_numpy(dataset)):\n    np_x = batch[\'image\']\n    sess.run(train_op, {x: np_x})\n    if i % FLAGS.print_every == 0:\n      np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x})\n      train_writer.add_summary(summary_str, i)\n      print(\'Iteration: {0:d} ELBO: {1:.3f} s/iter: {2:.3e}\'.format(\n          i,\n          np_elbo / FLAGS.batch_size,\n          (time.time() - t0) / FLAGS.print_every))\n      # Save samples\n      np_posterior_samples, np_prior_samples = sess.run(\n          [posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n      for k in range(FLAGS.n_samples):\n        f_name = os.path.join(\n            FLAGS.logdir, \'iter_%d_posterior_predictive_%d_data.jpg\' % (i, k))\n        imwrite(f_name, np_x[k, :, :, 0].astype(np.uint8))\n        f_name = os.path.join(\n            FLAGS.logdir, \'iter_%d_posterior_predictive_%d_sample.jpg\' % (i, k))\n        imwrite(f_name, np_posterior_samples[k, :, :, 0].astype(np.uint8))\n        f_name = os.path.join(\n            FLAGS.logdir, \'iter_%d_prior_predictive_%d.jpg\' % (i, k))\n        imwrite(f_name, np_prior_samples[k, :, :, 0].astype(np.uint8))\n      t0 = time.time()\n\nif __name__ == \'__main__\':\n  train()\n'"
